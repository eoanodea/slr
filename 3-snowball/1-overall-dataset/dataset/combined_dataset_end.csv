DOI,round_5,round_4,round_3,round_2,round_1,record_id,Author,Title,Publication Title,ISBN,ISSN,Url,Date,Series,Series Number,Series Text,Series Title,Publisher,Place,Abstract,ISBNs,Funding Information,PDF Link,Author Keywords,Mesh_Terms,Article Citation Count,Patent Citation Count,Reference Count,License,Issue Date,Meeting Date,Author full names,Author(s) ID,Year,Source title,Art. No.,Page start,Page end,Page count,Cited by,Link,Publication Stage,Open Access,Source,EID,Keywords,asreview_prior,exported_notes_1,asreview_ranking
10.1007/978-3-030-27878-6_16,0,0,0,0,0,293,"Kozhevnikov, Sergey; Skobelev, Petr; Pribyl, Ondrej; Svítek, Miroslav",Development of Resource-Demand Networks for Smart Cities 5.0,"Industrial Applications of Holonic and Multi-Agent Systems: 9th International Conference, HoloMAS 2019, Linz, Austria, August 26–29, 2019, Proceedings",978-3-030-27877-9,,https://doi.org/10.1007/978-3-030-27878-6_16,2019,,,,,Springer-Verlag,"Berlin, Heidelberg","In the paper, the new vision of “Smart City 5.0” is presented. It is based on a previously developed model of Smart City 4.0 and implementing the concept of the complex adaptive system for balancing conflict interests of different city actors. These actors can include business, transport, energy and water supply providers, entertainment and other services and can be unified based on resource and demand model.The paper describes the general principals, functionality and the architecture of the digital multi-agent platform for creating eco-system of “Smart City 5.0”. It is designed as holonic p2p network of smart services and technological components for supporting demand-resource relations.It is shown that in proposed eco-system smart services can interact both vertically and horizontally supporting competition and cooperation behavior on the basis of specialized protocols of p2p network. In the future, each smart service is considered as an autonomous cyber-physical multi-agent system which can be decomposed on a lower level of smaller services recursively.The first prototypes of smart services and their interaction are presented, the next steps for future research work are outlined.",,,,,,,,,,,,,,,,,,,,,,,,,,Artificial intelligence; Digital eco-system; Holonic system; Multi-agent technology; Smart City 5.0; Smart services,,,388
10.1007/978-3-030-30856-8_5,0,0,0,0,0,137,"Moghaddam, Mahyar Tourchi; Muccini, Henry",Fault-Tolerant IoT: A Systematic Mapping Study,"Software Engineering for Resilient Systems: 11th International Workshop, SERENE 2019, Naples, Italy, September 17, 2019, Proceedings",978-3-030-30855-1,,https://doi.org/10.1007/978-3-030-30856-8_5,2019,,,,,Springer-Verlag,"Berlin, Heidelberg","A failure may occur at all architectural levels of the Internet of Things (IoT) applications: sensor and actuator nodes can be missed, network links can be down, and processing and storage components can fail to perform properly. That is the reason for which fault-tolerance (FT) has become a crucial concern for IoT systems.Our study aims at identifying and classifying the existing FT mechanisms that can tolerate the IoT systems failure. In line with a systematic mapping study selection procedure, we picked out 60 papers among over 2300 candidate studies. To this end, we applied a rigorous classification and extraction framework to select and analyze the most influential domain-related information. Our analysis revealed the following main findings: (i) whilst researchers tend to study fault-tolerant IoT (FT-IoT) in cloud level only, several studies extend the application to fog and edge computing; (ii) there is a growing scientific interest on using the microservices architecture to address FT in IoT systems; (iii) the IoT components distribution, collaboration and intelligent elements location impact the system resiliency. This study gives a foundation to classify the existing and future approaches for fault-tolerant IoT, by classifying a set of methods, techniques and architectures that are potentially capable to reduce IoT systems failure.",,,,,,,,,,,,,,,,,,,,,,,,,,Internet of Things; Fault-tolerance; Software architecture; Systematic mapping study,,,684
10.1007/978-3-030-39306-9_4,0,0,0,0,0,286,"Capizzi, Antonio; Distefano, Salvatore; Mazzara, Manuel",From DevOps to DevDataOps: Data Management in DevOps Processes,"Software Engineering Aspects of Continuous Development and New Paradigms of Software Production and Deployment: Second International Workshop, DEVOPS 2019, Château de Villebrumier, France, May 6–8, 2019, Revised Selected Papers",978-3-030-39305-2,,https://doi.org/10.1007/978-3-030-39306-9_4,2019,,,,,Springer-Verlag,"Berlin, Heidelberg","DevOps is a quite effective approach for managing software development and operation, as confirmed by plenty of success stories in real applications and case studies. DevOps is now becoming the main-stream solution adopted by the software industry in development, able to reduce the time to market and costs while improving quality and ensuring evolvability and adaptability of the resulting software architecture. Among the aspects to take into account in a DevOps process, data is assuming strategic importance, since it allows to gain insights from the operation directly into the development, the main objective of a DevOps approach. Data can be therefore considered as the fuel of the DevOps process, requiring proper solutions for its management. Based on the amount of data generated, its variety, velocity, variability, value and other relevant features, DevOps data management can be mainly framed into the BigData category. This allows exploiting BigData solutions for the management of DevOps data generated throughout the process, including artefacts, code, documentation, logs and so on. This paper aims at investigating data management in DevOps processes, identifying related issues, challenges and potential solutions taken from the BigData world as well as from new trends adopting and adapting DevOps approaches in data management, i.e. DataOps.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,485
10.1007/978-3-030-42051-2_16,0,0,0,0,0,295,"Karagiannis, Ioannis; Mavrogiannis, Konstantinos; Soldatos, John; Drakoulis, Dimitris; Troiano, Ernesto; Polyviou, Ariana",Blockchain Based Sharing of Security Information for Critical Infrastructures of the Finance Sector,"Computer Security: ESORICS 2019 International Workshops, IOSec, MSTEC, and FINSEC, Luxembourg City, Luxembourg, September 26–27, 2019, Revised Selected Papers",978-3-030-42050-5,,https://doi.org/10.1007/978-3-030-42051-2_16,2019,,,,,Springer-Verlag,"Berlin, Heidelberg","Recent security incidents in the finance sector have demonstrated the importance of sharing security information across financial institutions, as a means of mitigating risks and boosting the early preparedness against relevant attacks. However, financial institutions are in several cases reluctant to share security information beyond what is imposed by applicable regulations. In this paper, we introduce a blockchain-based solution for sharing security information in a decentralized way, which boosts security and trust in the information sharing process. We also illustrate how the information that is shared across financial institutions can serve as a basis for collaborative security services such as risk assessment.",,,,,,,,,,,,,,,,,,,,,,,,,,Security; Blockchain; Cyber security; Critical infrastructure protection; Finance; Information sharing,,,1425
10.1007/978-3-030-45989-5_8,0,0,0,0,0,77,"Carrusca, André; Gomes, Maria Cecília; Leitão, João",Microservices Management on Cloud/Edge Environments,"Service-Oriented Computing – ICSOC 2019 Workshops: WESOACS, ASOCA, ISYCC, TBCE, and STRAPS, Toulouse, France, October 28–31, 2019, Revised Selected Papers",978-3-030-45988-8,,https://doi.org/10.1007/978-3-030-45989-5_8,2019,,,,,Springer-Verlag,"Berlin, Heidelberg","The microservices architecture is a promising approach for application development, deployment, and evolution, both on cloud and emerging fog/edge platforms. Microservices’ single functionality, small size, and independent development/deployment support faster and cheaper scaling of pressing functionalities on cloud systems. They support applications’ evolution via service reuse and smooth service modification/inclusion. Individual or sets of inter-related services may also be dynamically deployed onto resource-restricted nodes closer to end devices and data sources, which are typical of fog/edge computational platforms. The resulting system is very complex and impossible to be adequately managed manually. This work presents an automatic solution for microservices’ deployment/replication in the fog/edge, adapting the system according to the runtime evaluation of client accesses and resource usage. The evaluation validates the adaptability and performance gains.",,,,,,,,,,,,,,,,,,,,,,,,,,Cloud and fog/edge computing; Microservices architecture; Self-adaptable applications,0.0,EC3,1811
10.1007/978-3-030-47679-3_28,0,0,0,0,0,88,"Balicki, Jerzy; Balicka, Honorata; Dryja, Piotr; Tyszka, Maciej",Multi-criteria Differential Evolution for Optimization of Virtual Machine Resources in Smart City Cloud,"Computer Information Systems and Industrial Management: 19th International Conference, CISIM 2020, Bialystok, Poland, October 16–18, 2020, Proceedings",978-3-030-47678-6,,https://doi.org/10.1007/978-3-030-47679-3_28,2020,,,,,Springer-Verlag,"Berlin, Heidelberg","In a smart city, artificial intelligence tools support citizens and urban services. From the user point of view, smart applications should bring computing to the edge of the cloud, closer to citizens with short latency. However, from the cloud designer point of view, the trade-off between cost, energy and time criteria requires the Pareto solutions. Therefore, the proposed multi-criteria differential evolution can optimize virtual machine resources in smart city clouds to find compromises between preferences of citizens and designers. In this class of distributed computer systems, smart mobile devices share computing workload with the set of virtual machines that can be migrated among the nodes of the cloud. Finally, some numerical results are studied for the laboratory cloud GUT-WUT.",,,,,,,,,,,,,,,,,,,,,,,,,,Artificial intelligence; Differential evolution; Smart city,,,260
10.1007/978-3-030-48340-1_10,0,0,0,0,0,84,"Gaggero, Massimo; Busonera, Giovanni; Pireddu, Luca; Zanetti, Gianluigi",TDM Edge Gateway: A Flexible Microservice-Based Edge Gateway Architecture for Heterogeneous Sensors,"Euro-Par 2019: Parallel Processing Workshops: Euro-Par 2019 International Workshops, Göttingen, Germany, August 26–30, 2019, Revised Selected Papers",978-3-030-48339-5,,https://doi.org/10.1007/978-3-030-48340-1_10,2019,,,,,Springer-Verlag,"Berlin, Heidelberg","How to effectively handle heterogeneous data sources is one of the main challenges in the design of large-scale research computing platforms to collect, analyze and integrate data from IoT sensors. The platform must seamlessly support the integration of myriads of data formats and communication protocols, many being introduced after the platform has been deployed. Edge gateways, devices deployed at the edge of the network near the sensors, communicate with measurement stations using their proper protocol, receive and translate the messages to a standardized format, forward the data to the processing platform and provide local data buffering and preprocessing. In this work we present the TDM Edge Gateway architecture, which we have developed to be used in research contexts to meet the requirements of being self-built, low-cost, and compatible with current or future connected sensors.The architecture is based on a microservice-oriented design implemented with software containerization and leverages publish/subscribe Inter Process Communication to ensure modularity and resiliency. Costs and construction simplicity are ensured by adopting the popular Raspberry Pi Single Board Computer. The resulting platform is lean, flexible and easy to expand and integrate. It does not pose constraints on programming languages to use and relies on standard protocols and data models.",,,,,,,,,,,,,,,,,,,,,,,,,,Edge computing; Embedded; FIWARE; Sensor networks,0.0,EC3,2920
10.1007/978-3-030-48340-1_36,0,0,0,0,0,204,"Shi, Zeshun; Zhou, Huan; Hu, Yang; Koulouzis, Spiros; Rubia, Carlos; Zhao, Zhiming",Co-located and Orchestrated Network Fabric (CONF): An Automated Cloud Virtual Infrastructure for Social Network Applications,"Euro-Par 2019: Parallel Processing Workshops: Euro-Par 2019 International Workshops, Göttingen, Germany, August 26–30, 2019, Revised Selected Papers",978-3-030-48339-5,,https://doi.org/10.1007/978-3-030-48340-1_36,2019,,,,,Springer-Verlag,"Berlin, Heidelberg","Cloud environments can provide virtualized, elastic, controllable and high-quality on-demand infrastructure services for supporting complex distributed applications. However, existing IaaS (Infrastructure-as-a-Service) solutions mainly focus on the automated integration or deployment of generic applications; they lack flexible infrastructure planning and provisioning solutions and do not have rich support for the high service quality and trustworthiness required by social network applications. This paper introduces an automated cloud virtual infrastructure solution for social network applications, called Co-located and Orchestrated Network Fabric (CONF), which was conducted in a recently funded EU H2020 project ARTICONF. CONF aims to improve the existing infrastructure support in the DevOps lifecycle of social network applications to optimize QoS performance metrics as well as ensure fast recovery in the presence of faults or performance drops.",,,,,,,,,,,,,,,,,,,,,,,,,,Cloud; Social network applications; Virtual infrastructure,,,292
10.1007/978-3-030-49435-3_5,0,0,0,0,0,160,"Liebenberg, Martin; Jarke, Matthias",Information Systems Engineering with Digital Shadows: Concept and Case Studies: An Exploratory Paper,"Advanced Information Systems Engineering: 32nd International Conference, CAiSE 2020, Grenoble, France, June 8–12, 2020, Proceedings",978-3-030-49434-6,,https://doi.org/10.1007/978-3-030-49435-3_5,2020,,,,,Springer-Verlag,"Berlin, Heidelberg","The production sector has faced many difficulties in taking full advantage of opportunities found in other web application domains. Production research has focused on sophisticated mathematical models ranging from molecular materials modeling to efficient production control to inter-company supply network logistics. Often, these models have no closed-form solutions; this led to intense simulation research for individual modeling viewpoints, often labeled “Digital Twins”.However, the complexity of the overall system precludes Digital Twins covering more than just a few system perspectives, especially if near-realtime performance is required. Moreover, the wide variety of individual situations and behaviors is usually captured only as statistical uncertainty. In order to achieve better performance and more context adaptation, the interdisciplinary research cluster “Internet of Production” at RWTH Aachen University is exploring the concept of “Digital Shadows”. Digital Shadows can be understood as compact views on dynamic processes, usually combining condensed measurement data with highly efficient simplified mathematical models. In this exploratory paper, we argue based on a couple of initial case studies that Digital Shadows are not just valuable carriers of deep engineering knowledge but due to their small size also help in reducing network congestion and enabling edge computing. These properties could make Digital Shadows an interesting solution to address resilience in other information-intensive dynamic systems.",,,,,,,,,,,,,,,,,,,,,,,,,,Collaborative information systems; Database views; Digital shadows; Internet of Production (IoP); World Wide Lab (WWL),,,789
10.1007/978-3-030-49461-2_36,0,0,0,0,0,300,"Dibowski, Henrik; Massa Gray, Francesco",Applying Knowledge Graphs as Integrated Semantic Information Model for the Computerized Engineering of Building Automation Systems,"The Semantic Web: 17th International Conference, ESWC 2020, Heraklion, Crete, Greece, May 31–June 4, 2020, Proceedings",978-3-030-49460-5,,https://doi.org/10.1007/978-3-030-49461-2_36,2020,,,,,Springer-Verlag,"Berlin, Heidelberg","During the life cycle of a smart building, an extensive amount of heterogeneous information is required to plan, construct, operate and maintain the building and its technical systems. Traditionally, there is an information gap between the different phases and stakeholders, leading to information being exchanged, processed and stored in a variety of mostly human-readable documents. This paper shows how a knowledge graph can be established as integrated information model that can provide the required information for all phases in a machine-interpretable way. The knowledge graph describes and connects all relevant information, which allows combining and applying it in a holistic way. This makes the knowledge graph a key enabler for a variety of advanced, computerized engineering tasks, ranging from the planning and design phases over the commissioning and the operation of a building. The computerized engineering of building automation systems (BAS) with an advanced software tool chain is presented as such a use case in more detail. The knowledge graph is based on standard semantic web technologies and builds on existing ontologies, such as the Brick and QUDT ontologies, with various novel extensions presented in this paper. Special attention is given to the rich semantic definition of the entities, such as the equipment and the typically thousands of datapoints in a BAS, which can be achieved as a combination of contextual modeling and semantic tagging.",,,,,,,,,,,,,,,,,,,,,,,,,,Knowledge graph; Analytics; Building automation system; Building controls; Information model; Semantic definition; Semantic tagging,0.0,EC3,3012
10.1007/978-3-030-49760-6_7,0,0,0,0,0,86,"Matz, Alexandra; Götz, Clarissa","Designing Human-Centered Interactions for Smart Environments Based on Heterogeneous, Interrelated Systems: A User Research Method for the “Age of Services” (URSERVe)","Design, User Experience, and Usability. Design for Contemporary Interactive Environments: 9th International Conference, DUXU 2020, Held as Part of the 22nd HCI International Conference, HCII 2020, Copenhagen, Denmark, July 19–24, 2020, Proceedings, Part II",978-3-030-49759-0,,https://doi.org/10.1007/978-3-030-49760-6_7,2020,,,,,Springer-Verlag,"Berlin, Heidelberg","Heterogeneous, interrelated systems leveraging the Internet of Things (IoT) in smart environments such as a smart factory or smart city, pose many challenges for software design and development. Capturing and addressing the human needs is one of them and designing the interactions within smart applications is another. There are several reasons for this, for example diverse personas for both human as well as digital actors, a large variety of usage scenarios, a vast number of data exchange protocols, as well as data privacy and ethical concerns.On top of this, the service-based architecture of smart environments has widened the gap between software development teams and their end users. Services are highly modular, decoupled units of application logic, developed for specific task units and as such they are one or even several hierarchy layers removed from the business logic of main applications, and, in consequence, from the applications’ users. Therefore, in our experience, service development groups often refrain from identifying their users and doing any kind of user research at all. The reasoning being that, because their services will only be used by applications or other services, and not directly by human or non-human users, these users cannot be identified at all or are of no importance. This paper proposes a bidirectional, complementary method to approach user research for smart environments, such as smart cities. It describes how service development teams can reach beyond the applications which consume their service to the users of these consuming apps and how, at the same time, the smart city projects can reap the benefits of the technological expertise that resides in the service development groups. Ultimately, the method helps to create a shared knowledge base which arguably fosters innovation within an organization responsible for the design, development and operation of smart environments.",,,,,,,,,,,,,,,,,,,,,,,,,,Microservices; Smart cities; Services; Human-centered design; Knowledge management; Smart environments; Smart factory; User centered design; User experience; User research,0.0,EC3,3037
10.1007/978-3-030-58628-7_6,0,0,0,0,0,223,"Cardellini, Valeria; Lo Presti, Francesco; Nardelli, Matteo; Rossi, Fabiana",Self-adaptive Container Deployment in the Fog: A Survey,"Algorithmic Aspects of Cloud Computing: 5th International Symposium, ALGOCLOUD 2019, Munich, Germany, September 10, 2019, Revised Selected Papers",978-3-030-58627-0,,https://doi.org/10.1007/978-3-030-58628-7_6,2019,,,,,Springer-Verlag,"Berlin, Heidelberg","The fast increasing presence of Internet-of-Things and fog computing resources exposes new challenges due to heterogeneity and non-negligible network delays among resources as well as the dynamism of operating conditions. Such a variable computing environment leads the applications to adopt an elastic and decentralized execution. To simplify the application deployment and run-time management, containers are widely used nowadays. The deployment of a container-based application over a geo-distributed computing infrastructure is a key task that has a significant impact on the application non-functional requirements (e.g., performance, security, cost). In this survey, we first develop a taxonomy based on the goals, the scope, the actions, and the methodologies considered to adapt at run-time the application deployment. Then, we use it to classify some of the existing research results. Finally, we identify some open challenges that arise for the application deployment in the fog. In literature, we can find many different approaches for adapting the containers deployment, each tailored for optimizing a specific objective, such as the application response time, its deployment cost, or the efficient utilization of the available computing resources. However, although several solutions for deploying containers exist, those explicitly considering the distinctive features of fog computing are at the early stages: indeed, existing solutions scale containers without considering their placement, or do not consider the heterogeneity, the geographic distribution, and mobility of fog resources.",,,,,,,,,,,,,,,,,,,,,,,,,,Fog computing; Containers; Elasticity; Placement; Self-adaptive systems,0.0,EC3,1863
10.1007/978-3-030-58811-3_73,0,0,0,0,0,71,"Siavvas, Miltiadis; Tsoukalas, Dimitrios; Marantos, Charalampos; Tsintzira, Angeliki-Agathi; Jankovic, Marija; Soudris, Dimitrios; Chatzigeorgiou, Alexander; Kehagias, Dionysios",The SDK4ED Platform for Embedded Software Quality Improvement - Preliminary Overview,"Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part IV",978-3-030-58810-6,,https://doi.org/10.1007/978-3-030-58811-3_73,2020,,,,,Springer-Verlag,"Berlin, Heidelberg","Maintaining high level of quality with respect to important quality attributes is critical for the success of modern software applications. Hence, appropriate tooling is required to help developers and project managers monitor and optimize software quality throughout the overall Software Development Lifecycle (SDLC). Moreover, embedded software engineers and developers need support to manage complex interdependencies and inherent trade-offs between design and run-time qualities. To this end, in an attempt to address these issues, we are developing the SDK4ED Platform as part of the ongoing EU-funded SDK4ED project, a software quality system that enables the monitoring and optimization of software quality, with emphasis on embedded software. The purpose of this technical paper is to provide an overview of the SDK4ED Platform and present the main novel functionalities that have been implemented within the platform until today.",,,,,,,,,,,,,,,,,,,,,,,,,,Software quality; Verification and validation,0.0,EC3,3087
10.1007/978-3-030-58923-3_4,0,0,0,0,0,56,"Caporuscio, Mauro; D’Angelo, Mirko; Grassi, Vincenzo; Mirandola, Raffaela",Decentralized Architecture for Energy-Aware Service Assembly,"Software Architecture: 14th European Conference, ECSA 2020, L'Aquila, Italy, September 14–18, 2020, Proceedings",978-3-030-58922-6,,https://doi.org/10.1007/978-3-030-58923-3_4,2020,,,,,Springer-Verlag,"Berlin, Heidelberg","Contemporary application domains make more and more appealing the vision of applications built as a dynamic and opportunistic assembly of autonomous and independent resources. However, the adoption of such paradigm is challenged by: (i) the openness and scalability needs of the operating environment, which rule out approaches based on centralized architectures and, (ii) the increasing concern for sustainability issues, which makes particularly relevant, in addition to QoS constraints, the goal of reducing the application energy footprint. In this context, we contribute by proposing a decentralized architecture to build a fully functional assembly of distributed services, able to optimize its energy consumption, paying also attention to issues concerning the delivered quality of service. We suggest suitable indexes to measure from different perspectives the energy efficiency of the resulting assembly, and present the results of extensive simulation experiments to assess the effectiveness of our approach.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,EC3,1933
10.1007/978-3-030-59824-2_2,1,1,1,0,1,100,"Kaneko, Yu; Yokoyama, Yuhei; Monma, Nobuyuki; Terashima, Yoshiki; Teramoto, Keiichi; Kishimoto, Takuya; Saito, Takeshi",A Microservice-Based Industrial Control System Architecture Using Cloud and&nbsp;MEC,"Edge Computing – EDGE 2020: 4th International Conference, Held as Part of the Services Conference Federation, SCF 2020, Honolulu, HI, USA, September 18-20, 2020, Proceedings",978-3-030-59823-5,,https://doi.org/10.1007/978-3-030-59824-2_2,2020,,,,,Springer-Verlag,"Berlin, Heidelberg","Cloud computing has been adapted for various application areas. Several research projects are underway to migrate Industrial Control Systems (ICSs) to the public cloud. Some functions of ICSs require real-time processing that is difficult to migrate to the public cloud because network latency of the internet is unpredictable. Fog computing is a new computing paradigm that could address this latency issue. In particular, Multi-access Edge Computing (MEC) is a fog computing environment integrated with the 5G network, and therefore the real-time processing requirement of ICSs could be satisfied by using MEC. In this paper, we propose a microservice-based ICS architecture using the cloud and fog computing. In the architecture, each function of an ICS is implemented as a microservice and its execution locations are determined by an algorithm minimizing the total usage fee for cloud and fog computing while satisfying the real-time processing requirement. The proposed architecture and placement algorithm are evaluated by simulation under the scenario of a virtual power plant that manages distributed energy resources. The simulation result shows the proposed placement algorithm suppresses VM usage fee while satisfying the requirement of a real-time control function.",,,,,,,,,,,,,,,,,,,,,,,,,,5G; Cloud; Fog; Industrial Control System; MEC,0.0,IC4,49
10.1007/978-3-030-62522-1_28,0,0,0,0,0,172,"Dalibor, Manuela; Michael, Judith; Rumpe, Bernhard; Varga, Simon; Wortmann, Andreas",Towards a Model-Driven Architecture for Interactive Digital Twin Cockpits,"Conceptual Modeling: 39th International Conference, ER 2020, Vienna, Austria, November 3–6, 2020, Proceedings",978-3-030-62521-4,,https://doi.org/10.1007/978-3-030-62522-1_28,2020,,,,,Springer-Verlag,"Berlin, Heidelberg","Digital twins promise tremendous potential to reduce time and cost in the smart manufacturing of Industry 4.0. Engineering and monitoring interactive digital twins currently demands integrating different piecemeal technologies that effectively hinders their application and deployment. Current research on digital twins focuses on specific implementations or abstract models on how digital twins could be conceived. We propose model-driven software engineering to realize interactive digital twins and user-specific cockpits to interact with the digital twin by generating the infrastructure from common data structure models. To this end, we present a model-driven architecture for digital twins, its integration with an interactive cockpit, and a systematic method of realizing both. Through this, modeling, deploying, and monitoring interactive digital twins becomes more feasible and fosters their successful application in smart manufacturing.",,,,,,,,,,,,,,,,,,,,,,,,,,Digital Twins; Information systems; Model-driven software engineering; Smart manufacturing,,,1242
10.1007/978-3-030-86960-1_37,0,0,0,0,0,262,"Costantini, Alessandro; Duma, Doina Cristina; Martelli, Barbara; Antonacci, Marica; Galletti, Matteo; Tisbeni, Simone Rossi; Bellavista, Paolo; Di Modica, Giuseppe; Nehls, Daniel; Ahouangonou, Jean-Christian; Delamarre, Cedric; Cesini, Daniele",A Cloud-Edge Orchestration Platform for the Innovative Industrial Scenarios of the IoTwins Project,"Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part II",978-3-030-86959-5,,https://doi.org/10.1007/978-3-030-86960-1_37,2021,,,,,Springer-Verlag,"Berlin, Heidelberg","The concept of digital twins has growing more and more interest not only in the academic field but also among industrial environments thanks to the fact that the Internet of Things has enabled its cost-effective implementation. Digital twins (or digital models) refer to a virtual representation of a physical product or process that integrate data from various sources such as data APIs, historical data, embedded sensors and open data, giving to the manufacturers an unprecedented view into how their products are performing. The EU-funded IoTwins project plans to build testbeds for digital twins in order to run real-time computation as close to the data origin as possible (e.g., IoT Gateway or Edge nodes), and whilst batch-wise tasks such as Big Data analytics and Machine Learning model training are advised to run on the Cloud, where computing resources are abundant. In this paper, the basic concepts of the IoTwins project, its reference architecture, functionalities and components have been presented and discussed.",,,,,,,,,,,,,,,,,,,,,,,,,,IoT; Edge; Cloud; Digital twins; Digital models,,,735
10.1007/978-3-030-87007-2_20,0,0,0,0,0,186,"Kumari, Anisha; Sahoo, Bibhudatta; Behera, Ranjan Kumar; Misra, Sanjay; Sharma, Mayank Mohan",Evaluation of Integrated Frameworks for Optimizing QoS in Serverless Computing,"Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part VII",978-3-030-87006-5,,https://doi.org/10.1007/978-3-030-87007-2_20,2021,,,,,Springer-Verlag,"Berlin, Heidelberg","Serverless computing is an emerging cloud deployment model where developers can concentrate on developing application logic without worrying about the underlying architecture. It is similar to the platform as a service (PaaS) but at the functional level. Applications are usually deployed in the form of a set of functions independently and each function may be executed at separate servers thus also named as function as a service (FaaS). Serverless at the edge can handle thousands of concurrent functions invocations to process various kinds of events generated from resources like database, system logs, and other storage units, etc. A number of serverless frameworks like Openfaas, Openwhisk, Microsoft Azure, Amazon AWS allow dynamic scaling to handle the parallel request of stateless functions from the client-side. A separate container manager may be provisioned to handle distributed load for data processing. In this paper, we have evaluated the performance of serverless frameworks for parallel loads in terms of response time and throughput. In this paper, we have shown that the serverless framework is suitable for handling dynamic applications that can be executed on a number of stateless functions. An extensive comparison of the performance of serverless frameworks in handling concurrent invocations in terms of response time and throughput is also presented. It has been observed that Openwhisk is found to be the better serverless framework in terms of elasticity and scalability.",,,,,,,,,,,,,,,,,,,,,,,,,,Serverless computing; Orchestration; Function-as-a-service; Openfaas; OpenWhisk,0.0,EC3,1904
10.1007/978-3-030-91431-8_34,0,0,0,0,0,271,"Duan, Kaiyue; Li, Yusen; Marbach, Trent G.; Wang, Gang; Liu, Xiaoguang",Improving Load Balancing for Modern Data Centers Through Resource Equivalence Classes,"Service-Oriented Computing: 19th International Conference, ICSOC 2021, Virtual Event, November 22–25, 2021, Proceedings",978-3-030-91430-1,,https://doi.org/10.1007/978-3-030-91431-8_34,2021,,,,,Springer-Verlag,"Berlin, Heidelberg","Load balancing is one of the most significant concerns for data center (DC) management, and the basic method is reassigning applications from overloaded servers to underloaded servers. However, to ensure the service availability, during the reassignment of an application, some resources (i.e., transient resources) are consumed simultaneously on its initial server and its target server, which imposes a challenge for load balancing. The latest research has proposed a concept called resource equivalence class (REC: a set of resource configurations such that a latency-critical (LC) application running with any one of them can meet the QoS target). In this paper, we use the REC to improve the load balancing for a DC where multiple LC applications have already been co-located on servers with the service availability and QoS requirements. We formulate the proposed load rebalancing problem as a multi-objective constrained programming model. To solve the proposed problem, we propose to use a machine learning-based classification model to construct the RECs for applications, and we develop a local search (LS) algorithm to approximate the optimal solution. We evaluate the proposed algorithm via simulated experiments using real LC applications. To our knowledge, it is the first time to use REC for improving load balancing.",,,,,,,,,,,,,,,,,,,,,,,,,,Load rebalancing; Local search; Resource equivalence class,0.0,EC3,2329
10.1007/978-3-030-93747-8_10,0,0,0,0,0,97,"Dimitrakos, Theo; Dilshener, Tezcan; Kravtsov, Alexander; Marra, Antonio La; Martinelli, Fabio; Rizos, Athanasios; Rosetti, Alessandro",Handling Meta Attribute Information in Usage Control Policies (Short Paper),"Emerging Technologies for Authorization and Authentication: 4th International Workshop, ETAA 2021, Darmstadt, Germany, October 8, 2021, Revised Selected Papers",978-3-030-93746-1,,https://doi.org/10.1007/978-3-030-93747-8_10,2021,,,,,Springer-Verlag,"Berlin, Heidelberg","This work builds on top of an architecture and prototype implementation of a novel trust-aware continuous authorization technology that targets consumer Internet of Things (IoT), e.g., Smart Home to introduce a novel trust algorithm and meta attribute evaluation. Our approach extends previous work in two complementary ways: (1) By introducing a novel set of meta attributes that characterize the values of condition attributes such as Time To Live. This set of meta attributes serves as additional information that can be used by the system in order to proper caching attribute values or deciding whether or not to use an attribute already retrieved or to ask for a fresh one. (2) By minimizing the network consumption related to requesting additional and fresh attributes to sensor in IoT environments. Network is the source of major energy consumption in IoT devices, therefore being able to minimize network consumption is beneficial for the whole system.",,,,,,,,,,,,,,,,,,,,,,,,,,IoT; ABAC; Authorization; Trust; Usage control,0.0,EC3,2435
10.1007/978-3-030-95388-1_44,0,0,0,0,0,290,"Wang, Lei; Chen, Haiming; Qin, Wei",NBUFlow: A Dataflow Based Universal Task Orchestration and&nbsp;Offloading Platform for&nbsp;Low-Cost Development of&nbsp;IoT Systems with&nbsp;Cloud-Edge-Device Collaborative Computing,"Algorithms and Architectures for Parallel Processing: 21st International Conference, ICA3PP 2021, Virtual Event, December 3–5, 2021, Proceedings, Part II",978-3-030-95387-4,,https://doi.org/10.1007/978-3-030-95388-1_44,2021,,,,,Springer-Verlag,"Berlin, Heidelberg","With the development of intelligent hardware technology, the heterogeneity of IoT devices in the edge and end layers, and diversity of application scenarios bring unprecedented challenges to the growth of IoT systems, mainly including a large amount of code be written for task construction in a cloud-edge-device collaborative environment, the inflexible configuration of offloading policies, and the high overhead in task scheduling/offloading. A dataflow based low-cost task orchestration and offloading platform, named as NBUFlow, is proposed to solve these problems. It is featured by defining tasks based on visualized dashboard of Node-RED to realize convenient and low-cost development of IoT systems, and multiple offload strategies of deployment and multi-task parallel processing based on dataflow migration. The performance of the platform in terms of complete time of task deployment are verified through experiments. Results show that the completion time of task offloading can be reduced by three times compared with the container migration-based offloading method.",,,,,,,,,,,,,,,,,,,,,,,,,,Internet of Things (IoT); Edge computing; Node-RED,0.0,EC3,2240
10.1007/978-3-031-04580-6_23,0,0,0,0,0,18,"Lamprakos, Christos P.; Marantos, Charalampos; Papadopoulos, Lazaros; Soudris, Dimitrios",The Known Unknowns: Discovering Trade-Offs Between Heterogeneous Code Changes: Invited Paper,"Embedded Computer Systems: Architectures, Modeling, and Simulation: 21st International Conference, SAMOS 2021, Virtual Event, July 4–8, 2021, Proceedings",978-3-031-04579-0,,https://doi.org/10.1007/978-3-031-04580-6_23,2021,,,,,Springer-Verlag,"Berlin, Heidelberg","Software projects must adhere to a variety of non-functional requirements, also known as software qualities. Automated evaluation with respect to such requirements can be conducted thanks to a wide array of available tools. Each tool usually focuses on a specific quality, since heterogeneous analyses are needed for each non-functional requirement. Apart from an overall index expressing the project’s performance in terms of the software quality they specialize on, many tools recommend code changes that are expected to improve the aforementioned index. Thus, a development team that cares for more than one non-functional requirement is facing the problem of unknown trade-offs; besides improving the quality on which the tool that generated each suggestion focuses, how would this code change, if implemented, affect the rest of the non-functional requirements? We present a framework for dealing with this problem. We pick energy efficiency, technical debt and software security as our qualities of interest, and use three respective tools for the analysis of several open-source projects with regard to these qualities. We develop an extensible empirical model, based on fuzzy sets, for the characterization of each suggestion’s trade-offs. Finally, we present an intuitive visualization of said trade-offs, and suggest a method of utilizing them towards reliable decision-making.",,,,,,,,,,,,,,,,,,,,,,,,,,Software quality; Trade-off analysis; Decision-making,0.0,EC3,2599
10.1007/978-3-031-04718-3_8,0,0,0,0,0,174,"Mock, Markus; Arlt, Stefan",Upilio: Leveraging the&nbsp;Serverless Paradigm for&nbsp;Building a&nbsp;Versatile IoT Application,"Service-Oriented and Cloud Computing: 9th IFIP WG 6.12 European Conference, ESOCC 2022, Wittenberg, Germany, March 22–24, 2022, Proceedings",978-3-031-04717-6,,https://doi.org/10.1007/978-3-031-04718-3_8,2022,,,,,Springer-Verlag,"Berlin, Heidelberg","Serverless computing has emerged over the last couple of years as a flexible paradigm for deploying cloud-based applications and allowing developers to focus on their applications and reduce application maintenance costs over the lifetime of an application. However, there has not been an examination of whether a complex application can be built and operated with high performance and low operating cost relying entirely on the serverless paradigm. This paper presents the design, implementation, performance, and cost evaluation of what we believe to be a representative kind of IoT application, a cloud-based energy data management system named Upilio. Upilio is a versatile data collection and analysis platform for IoT sensor data. Upilio’s functionality is implemented entirely using AWS Lambda serverless functions and managed services to store data, and even the graphical user interface does not need a dedicated web server. Our empirical evaluation shows that the system, including its serverless online analytics (OLAP) functionality, is cost-effective, requiring only a fraction of the server cost necessary for operating such a system using on-premise hardware. Thus, Upilio demonstrates that complex IoT system scenarios can be implemented successfully with good performance and cost characteristics leveraging the serverless paradigm.",,,,,,,,,,,,,,,,,,,,,,,,,,IoT; FaaS; Serverless Computing,0.0,EC3,1887
10.1007/978-3-031-13832-4_10,0,0,0,0,0,109,"Yu, Yangming; Zha, Zhiyong; Jin, Bo; Wu, Geng; Dong, Chenxi",Research on Exchange and Management Platform of Enterprise Power Data Uniftcation Summit,"Intelligent Computing Methodologies: 18th International Conference, ICIC 2022, Xi'an, China, August 7–11, 2022, Proceedings, Part III",978-3-031-13831-7,,https://doi.org/10.1007/978-3-031-13832-4_10,2022,,,,,Springer-Verlag,"Berlin, Heidelberg","In recent years, due to the continuous expansion of the government, enterprises, and social users for power data sharing applications, as well as the rapid expansion of data service product scope and data service access business volume, along with the continuous advancement of power data sharing interactive services, it is necessary to improve data sharing operation management and control mechanism. At the same time, on the one hand, in order to support the efficient development of the daily operation and maintenance management of data products, standardize the product operation and maintenance, including data catalog management, data product management, product drop-off, etc.; on the other hand, in order to strengthen the operation of the data sharing and interaction process, etc. This paper proposes the design of the data sharing service platform, which meets the following requirements: First, establish access service security guarantee, unified access to government, enterprise and other customers’ access requests in different ways The second is to establish a standardized data service demand management, to provide data catalog query services for customers such as governments and enterprises, and to accept the data needs of these customers, forward the business department for processing, and give feedback to customers on data demand processing opinions; The third is to provide unified data Product access services, including data product query, data product application, and data product online access services; Fourth, provide unified data interface services, including data interface query services. The platform guarantees the normal operation of external power data sharing services, establishes an internal data sharing management and control mechanism, and supports the company’s data service operation management personnel to carry out data product operation management, data service upload and removal, data service authority control, data service operation monitoring and analysis, etc. Data sharing operation management and control.",,,,,,,,,,,,,,,,,,,,,,,,,,Data sharing; Enterprise power data; Management platform,,,375
10.1007/978-3-031-15565-9_4,0,0,0,0,0,131,"Malhi, Avleen; Apopei, Vlad; Madhikermi, Manik; Mandeep; Främling, Kary",Smartphone Based Grape Leaf Disease Diagnosis and&nbsp;Remedial System Assisted with&nbsp;Explanations,"Explainable and Transparent AI and Multi-Agent Systems: 4th International Workshop, EXTRAAMAS 2022, Virtual Event, May 9–10, 2022, Revised Selected Papers",978-3-031-15564-2,,https://doi.org/10.1007/978-3-031-15565-9_4,2022,,,,,Springer-Verlag,"Berlin, Heidelberg","Plant diseases are one of the biggest challenges faced by the agricultural sector due to the damage and economic losses in crops. Despite the importance, crop disease diagnosis is challenging because of the limited-resources farmers have. Subsequently, the early diagnosis of plant diseases results in considerable improvement in product quality. The aim of the proposed work is to design an ML-powered mobile-based system to diagnose and provide an explanation based remedy for the diseases in grape leaves using image processing and explainable artificial intelligence. The proposed system will employ the computer vision empowered with Machine Learning (ML) for plant disease recognition and explains the predictions while providing remedy for it. The developed system uses Convolutional Neural networks (CNN) as an underlying machine/deep learning engine for classifying the top disease categories and Contextual Importance and Utility (CIU) for localizing the disease areas based on prediction. The user interface is developed as an IOS mobile app, allowing farmers to capture a photo of the infected grape leaves. The system has been evaluated using various performance metrics such as classification accuracy and processing time by comparing with different state-of-the-art algorithms. The proposed system is highly compatible with the Apple ecosystem by developing IOS app with high prediction and response time. The proposed system will act as a prototype for the plant disease detector robotic system.",,,,,,,,,,,,,,,,,,,,,,,,,,Machine learning; Agriculture; Grape leaf detection; Mobile app,,,704
10.1007/978-3-031-18192-4_1,1,1,1,1,0,226,"Akasiadis, Charilaos; Iatrakis, Georgios; Spanoudakis, Nikolaos; Chalkiadakis, Georgios",An Open MAS/IoT-Based Architecture for&nbsp;Large-Scale V2G/G2V,"Advances in Practical Applications of Agents, Multi-Agent Systems, and Complex Systems Simulation. The PAAMS Collection: 20th International Conference, PAAMS 2022, L'Aquila, Italy, July 13–15, 2022, Proceedings",978-3-031-18191-7,,https://doi.org/10.1007/978-3-031-18192-4_1,2022,,,,,Springer-Verlag,"Berlin, Heidelberg","In this paper we put forward an open multi-agent systems (MAS) architecture for the important and challenging to engineer vehicle-to-grid (V2G) and grid-to-vehicle (G2V) energy transfer problem domains. To promote scalability, our solution is provided in the form of modular microservices that are interconnected using a multi-protocol Internet of Things (IoT) platform. On the one hand, the low-level modularity of Smart Grid services allows the seamless integration of different agent strategies, pricing mechanisms and algorithms; and on the other, the IoT-based implementation offers both direct applicability in real-world settings, as well as advanced analytics capabilities by enabling digital twins models for Smart Grid ecosystems. We describe our MAS/IoT-based architecture and present results from simulations that incorporate large numbers of heterogeneous Smart Grid agents, which might follow different strategies for their decision making tasks. Our framework enables the testing of various schemes in simulation mode, and can also be used as the basis for the implementation of real-world prototypes for the delivery of large-scale V2G/G2V services.",,,,,,,,,,,,,,,,,,,,,,,,,,Internet of things; Smart grid; Open multi-agent systems,,,302
10.1007/978-3-031-19433-7_46,0,0,0,0,0,183,"Ploennigs, Joern; Semertzidis, Konstantinos; Lorenzi, Fabio; Mihindukulasooriya, Nandana",Scaling Knowledge Graphs for&nbsp;Automating AI of&nbsp;Digital Twins,"The Semantic Web – ISWC 2022: 21st International Semantic Web Conference, Virtual Event, October 23–27, 2022, Proceedings",978-3-031-19432-0,,https://doi.org/10.1007/978-3-031-19433-7_46,2022,,,,,Springer-Verlag,"Berlin, Heidelberg",Digital Twins are digital representations of systems in the Internet of Things (IoT) that are often based on AI models that are trained on data from those systems. Semantic models are used increasingly to link these datasets from different stages of the IoT systems life-cycle together and to automatically configure the AI modelling pipelines. This combination of semantic models with AI pipelines running on external datasets raises unique challenges particular if rolled out at scale. Within this paper we will discuss the unique requirements of applying semantic graphs to automate Digital Twins in different practical use cases. We will introduce the benchmark dataset DTBM that reflects these characteristics and look into the scaling challenges of different knowledge graph technologies. Based on these insights we will propose a reference architecture that is in-use in multiple products in IBM and derive lessons learned for scaling knowledge graphs for configuring AI models for Digital Twins.,,,,,,,,,,,,,,,,,,,,,,,,,,Internet of Things; Machine learning; Scalability; Digital twins; Knowledge graphs; Semantic models,,,675
10.1007/978-3-031-20984-0_38,1,1,1,1,0,45,"N. Toosi, Adel; Agarwal, Chayan; Mashayekhy, Lena; Moghaddam, Sara K.; Mahmud, Redowan; Tari, Zahir",GreenFog: A Framework for&nbsp;Sustainable Fog Computing,"Service-Oriented Computing: 20th International Conference, ICSOC 2022, Seville, Spain, November 29 – December 2, 2022, Proceedings",978-3-031-20983-3,,https://doi.org/10.1007/978-3-031-20984-0_38,2022,,,,,Springer-Verlag,"Berlin, Heidelberg","The alarming rate of increase in energy demand and carbon footprint of Fog environments has become a critical issue. It is, therefore, necessary to reduce the percentage of brown energy consumption in these systems and integrate renewable energy use into Fog. Renewables, however, are prone to availability fluctuations due to their variable and intermittent nature. In this paper, we propose a new Fog framework and design various optimization techniques, including linear programming optimization, linear regression estimation, and Multi-Armed Bandit (MAB) learning to optimize renewable energy use in the Fog based on a novel idea of load shaping with adaptive Quality of Service (QoS). The proposed framework, along with the optimization techniques, are tested on a real-world micro data center (Fog environment) powered by solar energy sources connected to multiple IoT devices. The results show that our proposed framework significantly reduces the difference between renewable energy generation and total energy consumption while efficiently adjusting the QoS of applications.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,EC3,1912
10.1007/978-3-031-22698-4_12,0,0,0,0,0,285,"Spišaková, Viktória; Klusáček, Dalibor; Hejtmánek, Lukáš",Using Kubernetes in&nbsp;Academic Environment: Problems and&nbsp;Approaches,"Job Scheduling Strategies for Parallel Processing: 25th International Workshop, JSSPP 2022, Virtual Event, June 3, 2022, Revised Selected Papers",978-3-031-22697-7,,https://doi.org/10.1007/978-3-031-22698-4_12,2022,,,,,Springer-Verlag,"Berlin, Heidelberg","In this work, we discuss our experience when utilizing the Kubernetes orchestrator (K8s) to efficiently allocate resources in a heterogeneous and dynamic academic environment. In the commercial world, the “pay per use” model is a strong regulating factor for efficient resource usage. In the academic environment, resources are usually provided “for free” to the end-users, thus they often lack a clear motivation to plan their use efficiently. In this paper, we show three major sources of inefficiencies. One is the users’ requirement to have interactive computing environments, where the users need resources for their application as soon as possible. Users do not appreciate waiting for interactive environments, but constantly keeping some resources available for interactive tasks is inefficient. The second phenomenon is observable in both interactive and batch workloads; users tend to overestimate necessary limits for their computations, thus wasting resources. Finally, Kubernetes does not support fair-sharing functionality (dynamic user priorities) which hampers the efforts when developing a fair scheme for Pod/job scheduling and/or eviction. We discuss various approaches to deal with these problems such as scavenger jobs, placeholder jobs, Kubernetes-specific resource allocation policies, separate clusters, priority classes, and novel hybrid cloud approach. We also show that all these proposals open interesting scheduling-related questions that are hard to answer with existing Kubernetes tools and policies. Last but not least, we provide a real workload trace from our installation to the scheduling community which captures these phenomena.",,,,,,,,,,,,,,,,,,,,,,,,,,HPC; Kubernetes; Resource management; Cloud; Scheduling,0.0,EC3,2893
10.1007/978-3-031-23470-5_7,0,0,0,0,0,279,"Luo, Min",Future Lies with Edge Computing,"Edge Computing – EDGE 2022: 6th International Conference, Held as Part of the Services Conference Federation, SCF 2022, Honolulu, HI, USA, December 10–14, 2022, Proceedings",978-3-031-23469-9,,https://doi.org/10.1007/978-3-031-23470-5_7,2022,,,,,Springer-Verlag,"Berlin, Heidelberg","This paper presents a relatively comprehensive survey of recent advances in edge computing, including key drivers and business benefits for typical business use cases. A reference architecture with a supporting service model is proposed that can be used as the starting point for any organization to adopt edge computing. A list of challenges and future work is also assembled for both the academic and industrial research and development.",,,,,,,,,,,,,,,,,,,,,,,,,,Security; Internet of Things (IoT); Edge computing; Artificial intelligence; Reliability; Cloud datacenter; Distributed DBMS/SQL; Distributed systems; Edge datacenter; Machine learning; Scalability; Tiered architecture,,,1028
10.1007/978-3-031-26422-1_43,0,0,0,0,0,2,"Müller, Benedikt Tobias; Ender, Marvin; Swiadek, Jan Erik; Jin, Mengcheng; Winkel, Simon; Niedziela, Dominik; Li, Bin; Hüntelmann, Jelle; Müller, Emmanuel","ADEPT: Anomaly Detection, Explanation and Processing for Time Series with a Focus on Energy Consumption Data","Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2022, Grenoble, France, September 19–23, 2022, Proceedings, Part VI",978-3-031-26421-4,,https://doi.org/10.1007/978-3-031-26422-1_43,2022,,,,,Springer-Verlag,"Berlin, Heidelberg","Anomaly detection techniques are applicable for recognizing excessive energy consumption and device failure, thereby contributing to the maintenance of operational and sustainable energy supply systems. In this context, human decision makers can benefit from receiving explanation attempts for detected anomalies as part of a semi-automated software solution. Therefore we introduce the framework ADEPT, which comprises interfaces for processing user-supplied time series data and interactively visualizing explanatory anomaly information. Our framework features several shallow and deep machine learning algorithms for anomaly detection and explanation. We demonstrate ADEPT using energy consumption data collected from our university campus.",,,,,,,,,,,,,,,,,,,,,,,,,,Anomaly detection; Energy consumption; Explainability,,,398
10.1007/978-3-031-26507-5_15,0,0,0,0,0,20,"Cañete, Angel; Rodríguez, Alberto; Amor, Mercedes; Fuentes, Lidia",Energy-Aware Placement of&nbsp;Network Functions in&nbsp;Edge-Based Infrastructures with&nbsp;Open Source MANO and&nbsp;Kubernetes,"Service-Oriented Computing – ICSOC 2022 Workshops: ASOCA, AI-PA, FMCIoT, WESOACS 2022, Sevilla, Spain, November 29 – December 2, 2022 Proceedings",978-3-031-26506-8,,https://doi.org/10.1007/978-3-031-26507-5_15,2022,,,,,Springer-Verlag,"Berlin, Heidelberg","The virtualization of network functions aims to replace traditional network functions running on proprietary middleboxes with software instances running on general-purpose virtualization solutions, looking for more flexible, scalable and sustainable networks. However, despite the availability of platforms and technologies that enable its realisation, there are still technical challenges that have to be addressed to obtain those benefits. One of the challenges is the efficient placement of network functions, considering, for instance, the energy footprint among other constraints and available resources. This paper proposes an energy-aware virtual network function placement and resource-allocation solution for heterogeneous edge infrastructures that considers the computation and communication delays according to the virtual network functions’ location in the infrastructure. The solution has been integrated with the ETSI-sponsored project Open Source Management and Orchestration (OSM) as an extension that allows the configuration of virtual network functions and their subsequent resource allocation and deployment at the edge, minimizing energy consumption and ensuring the quality of service. Applied to the deployment of augmented reality services in different scenarios, the results show up to a 51% reduction in energy consumption compared to the default OSM placement and quality of service compliance in all scenarios considered.",,,,,,,,,,,,,,,,,,,,,,,,,,Energy efficiency; Edge computing; B5G; Feature models; Open source mano; VNF placement,0.0,EC3,1766
10.1007/978-3-031-26507-5_20,0,0,0,0,0,207,"Serrano-Gutierrez, Pablo; Ayala, Inmaculada; Fuentes, Lidia",FUSPAQ: A Function Selection Platform to Adjust QoS in a FaaS Application,"Service-Oriented Computing – ICSOC 2022 Workshops: ASOCA, AI-PA, FMCIoT, WESOACS 2022, Sevilla, Spain, November 29 – December 2, 2022 Proceedings",978-3-031-26506-8,,https://doi.org/10.1007/978-3-031-26507-5_20,2022,,,,,Springer-Verlag,"Berlin, Heidelberg","Function as a Service (FaaS) development has numerous benefits for application deployment, management, and maintenance. However, the lack of control over the infrastructure and, often, over the FaaS platform itself, makes it necessary to look for external solutions that allow the operation of the application to be adapted to different requirements or changing execution conditions. In a FaaS application, the quality of service (QoS) is determined by the characteristics of the functions executed to perform each workflow operation. Deciding the most suitable functions providing a QoS is a complex process due to the high variability of possible function implementations, each giving different qualities. Leaving this task in the hands of the developer is not a good solution and makes it difficult to program the application. We present FUSPAQ, a framework for working with serverless architectures, which can automatically select the best functions executed at runtime to satisfy specific QoS requirements. With this objective, a Software Product Line approach is used, modeling the application’s tasks and operations using Feature Models that specify the variability of functions that can perform the same operation as a family of functions. We use Z3, a cross-platform satisfiability modulo theories (SMT) solver, to generate optimal configurations. As requirements can change over time, the system automatically adapts to these changes to continue maintaining the desired QoS. We test our approach with different QoS parameters, and analyse the value added to serverless frameworks.",,,,,,,,,,,,,,,,,,,,,,,,,,FaaS; Serverless; QoS; Feature model; Software product line,0.0,EC3,1878
10.1007/978-3-031-29315-3_8,0,0,0,0,0,148,"Poleggi, Marco Emilio; Abdennadher, Nabil; Dupuis, Raoul; Mendonça, Francisco",Edge-to-Cloud Solutions for&nbsp;Self-adaptive Machine Learning-Based IoT Applications: A Cost Comparison,"Economics of Grids, Clouds, Systems, and Services: 19th International Conference, GECON 2022, Izola, Slovenia, September 13–15, 2022, Proceedings",978-3-031-29314-6,,https://doi.org/10.1007/978-3-031-29315-3_8,2022,,,,,Springer-Verlag,"Berlin, Heidelberg","Large-scale IoT applications based on machine learning (ML) demand both edge and cloud processing for, respectively, AI inference and ML training tasks. Context-aware applications also need self-adaptive intelligence which makes their architecture even more complex. Estimating the costs of operating such edge-to-cloud deployments is challenging. To this purpose, we propose a reference service-oriented event-driven system architecture for IoT/edge applications comprising a minimal set of components, mapped on available cloud services. We then propose a resource consumption model for estimating the cost of deploying and running self-adaptive AI-assisted IoT applications on selected edge-to-cloud platforms. The model is evaluated in two scenarios: Road Traffic Management and Smart Grid. We finally provide some estimates showing how the expenditure breakdown varies significantly depending on the adopted platform: storage costs are dominant in Road Traffic Management for all providers, whereas either messaging or edge management costs may dominate the Smart Grid scenario, and, surprisingly, computing costs are almost negligible in all cases.",,,,,,,,,,,,,,,,,,,,,,,,,,IoT; Edge; Cloud; Cost Model; PaaS,0.0,EC3,1931
10.1007/978-3-031-36889-9_21,0,0,0,0,0,277,"Matevska, Jasminka; Soldin, Marvin",Enabling IoT Connectivity and&nbsp;Interoperability by&nbsp;Using Automated Gateways,"Software Architecture. ECSA 2022 Tracks and Workshops: Prague, Czech Republic, September 19–23, 2022, Revised Selected Papers",978-3-031-36888-2,,https://doi.org/10.1007/978-3-031-36889-9_21,2022,,,,,Springer-Verlag,"Berlin, Heidelberg","As an essential part of the Industry 4.0 strategy, the Internet of Things is developing to “Internet of Everything”. The number of interconnected devices and the amount of data produced increases constantly. Various devices are communicating using various protocols, exchanging data using various data formats and connecting to various software applications. In an IoT-Architecture, a gateway is a fundamental component needed for enabling device interoperability. While a great deal of research has already been done on IoT in cloud computing, fog computing, and edge computing, there is still no intensive activity in the field of gateways in particular. Even the basic gateways can act as a proxy between low-end IoT devices and data centres, automated gateways can provide significantly higher functionality to solve the problems of diversity of protocols, data formats and the custom needs of various devices including used applications. This paper presents a concept of an automated gateway dealing with the problems of protocol conversion, device management, middleware abstraction, resource management and traffic optimisation. The gateway is designed as a modular plug-and-play architecture and was evaluated for MQTT, ZigBee, WebSocket and Amazon WebServices. The architecture can be extended by including additional modules to support further protocols and services. Finally, the gateway defines its protocol and translates incoming messages into an optional uniform format, which can be used by the client to enable more complex message flows. Thus, it provides a solid foundation for further development towards standardization of communication interfaces and interoperability of IoT devices.",,,,,,,,,,,,,,,,,,,,,,,,,,Internet of Things; Automated Gateway; Communication Protocols; Connectivity; Industry 4.0; Interoperability; IoT Architecture,0.0,EC3,3054
10.1007/978-3-031-36889-9_9,0,0,0,0,0,149,"Abdullah, Milad",Controlling Automatic Experiment-Driven Systems Using Statistics and&nbsp;Machine Learning,"Software Architecture. ECSA 2022 Tracks and Workshops: Prague, Czech Republic, September 19–23, 2022, Revised Selected Papers",978-3-031-36888-2,,https://doi.org/10.1007/978-3-031-36889-9_9,2022,,,,,Springer-Verlag,"Berlin, Heidelberg","Experiments are used in many modern systems to optimize their operation. Such experiment-driven systems are used in various fields, such as web-based systems, smart-* systems, and various self-adaptive systems. There is a class of these systems that derive their data from running simulations or another type of computation, such as in digital twins, online planning using probabilistic model-checking, or performance benchmarking. To obtain statistically significant results, these systems must repeat the experiments multiple times. As a result, they consume extensive computation resources. The GraalVM benchmarking project detects performance changes in the GraalVM compiler. However, the benchmarking project has an extensive usage of computational resources and time. The doctoral research project proposed in this paper focuses on controlling the experiments with the goal of reducing computation costs. The plan is to use statistical and machine learning approaches to predict the outcomes of experiments and select the experiments yielding more useful information. As an evaluation, we are applying these methods to the GraalVM benchmarking project; the initial results confirm that these methods have the potential to significantly reduce computation costs.",,,,,,,,,,,,,,,,,,,,,,,,,,Machine Learning; Experiment-Driven Systems; Statistics,0.0,EC3,2439
10.1007/978-3-031-39764-6_9,0,0,0,0,0,220,"Kotilainen, Pyry; Heikkilä, Ville; Systä, Kari; Mikkonen, Tommi",Towards Liquid AI in&nbsp;IoT with&nbsp;WebAssembly: A Prototype Implementation,"Mobile Web and Intelligent Information Systems: 19th International Conference, MobiWIS 2023, Marrakech, Morocco, August 14–16, 2023, Proceedings",978-3-031-39763-9,,https://doi.org/10.1007/978-3-031-39764-6_9,2023,,,,,Springer-Verlag,"Berlin, Heidelberg","An Internet of Things (IoT) system typically comprises numerous subsystems and devices, such as sensors, actuators, gateways for internet connectivity, cloud services, end-user applications, and analytics. Currently, these subsystems are built using a wide range of programming technologies and tools, posing challenges in migrating functionality between them. In our previous work, we have proposed so-called liquid software, where different subsystems are developed using a consistent set of technologies and functions can flow from one computer to another. In this paper, we introduce a prototype implementation of liquid artificial intelligence features, which can be flexibly deployed at the cloud-edge continuum.",,,,,,,,,,,,,,,,,,,,,,,,,,Internet of Things; AI; ML; IoT; Artificial intelligence; Machine learning; WoT; Isomorphic software; Liquid software; Web of Things,,,605
10.1007/978-3-031-42592-9_15,0,0,0,0,0,161,"Spalazzese, Romina; De Sanctis, Martina; Alkhabbas, Fahed; Davidsson, Paul",Shaping IoT Systems Together: The&nbsp;User-System Mixed-Initiative Paradigm and&nbsp;Its Challenges,"Software Architecture: 17th European Conference, ECSA 2023, Istanbul, Turkey, September 18–22, 2023, Proceedings",978-3-031-42591-2,,https://doi.org/10.1007/978-3-031-42592-9_15,2023,,,,,Springer-Verlag,"Berlin, Heidelberg","Internet of Things (IoT) systems are often complex and have to deal with many challenges at the same time, both from a human and technical perspective. In this vision paper, we (i) describe IoT-Together , the Mixed-initiative Paradigm that we devise for IoT user-system collaboration and (ii)&nbsp;critically analyze related architectural challenges.",,,,,,,,,,,,,,,,,,,,,,,,,,Goal-driven IoT Systems; Intelligent IoT Systems; Mixed-initiative paradigm; Novel Experiences; User-System Collaboration,,,961
10.1007/978-3-031-43185-2_21,0,0,0,0,0,222,"Carnevali, Laura; Paolieri, Marco; Picano, Benedetta; Reali, Riccardo; Scommegna, Leonardo; Vicario, Enrico",A Quantitative Approach to&nbsp;Coordinated Scaling of&nbsp;Resources in&nbsp;Complex Cloud Computing Workflows,"Computer Performance Engineering and Stochastic Modelling: 19th European Workshop, EPEW 2023, and 27th International Conference, ASMTA 2023, Florence, Italy, June 20–23, 2023, Proceedings",978-3-031-43184-5,,https://doi.org/10.1007/978-3-031-43185-2_21,2023,,,,,Springer-Verlag,"Berlin, Heidelberg","Resource scaling is widely employed in cloud computing to adapt system operation to internal (i.e.,&nbsp;application) and external (i.e.,&nbsp;environment) changes. We present a quantitative approach for coordinated vertical scaling of resources in cloud computing workflows, aimed at satisfying an agreed Service Level Objective&nbsp;(SLO) by improving the workflow end-to-end (e2e) response time distribution. Workflows consist of IaaS services running on dedicated clusters, statically reserved before execution. Services are composed through sequence, choice/merge, and balanced split/join blocks, and have generally distributed (i.e.,&nbsp;non-Markovian) durations possibly over bounded supports, facilitating fitting of analytical distributions from observed data. Resource allocation is performed through an efficient heuristics guided by the mean makespans of sub-workflows. The heuristics performs a top-down visit of the hierarchy of services, and it exploits an efficient compositional method to derive the response time distribution and the mean makespan of each sub-workflow. Experimental results on a workflow with high concurrency degree appear promising for feasibility and effectiveness of the approach.",,,,,,,,,,,,,,,,,,,,,,,,,,Cloud computing; complex workflow structure; coordinated scaling; end-to-end response time distribution; stochastic workflow,0.0,EC3,2630
10.1007/978-3-031-44836-2_3,1,1,1,1,1,8,"Nam, T. B.; Khiem, H. G.; Triet, M. N.; Hong, K. V.; Khoa, T. D.; Bao, Q. T.; Phuc, N. T.; Hieu, M. D.; Loc, V. C. P.; Quy, T. L.; Anh, N. T.; Hien, Q. N.; Bang, L. K.; Trong, D. P. N.; Ngan, N. T. K.; Son, H.; Luong, H. H.",SPaMeR: Securing Patient Medical Records in&nbsp;the&nbsp;Cloud - A Microservice and&nbsp;Brokerless Architecture Approach,"Web Services – ICWS 2023: 30th International Conference, Held as Part of the Services Conference Federation, SCF 2023, Honolulu, HI, USA, September 23–26, 2023, Proceedings",978-3-031-44835-5,,https://doi.org/10.1007/978-3-031-44836-2_3,2023,,,,,Springer-Verlag,"Berlin, Heidelberg","The expansion of Internet of Things (IoT) technologies has revolutionized various sectors, one of the most critical being healthcare. The effective management of Patient Medical Records (PMRs) is an area where IoT plays a significant role, and its integration with Cloud Computing offers an enormous opportunity to enhance data accessibility, efficiency, and cost-effectiveness. However, the challenge of securing PMRs in the cloud remains a key concern. This paper introduces SPaMeR, an innovative IoT platform based on microservice and brokerless architecture, tailored to address this challenge and the specific requirements of healthcare environments. SPaMeR platform incorporates and extends the core functionalities of the IoT platform designed in our previous work - data collection, device and user management, and remote device control - while specifically addressing six critical issues for healthcare data: a) secure and reliable transmission of medical data, b) energy efficiency for healthcare devices, c) high-speed and accurate data collection from medical devices, d) robust security mechanisms to protect sensitive patient information, e) scalability to accommodate the ever-growing number of patients and medical devices, and f) compliance with healthcare data regulations and standards. To demonstrate the effectiveness and feasibility of SPaMeR, we provide a comprehensive evaluation with two distinct healthcare scenarios. Our results indicate significant improvements in the areas of data security, energy efficiency, and system scalability compared to traditional healthcare platforms.",,,,,,,,,,,,,,,,,,,,,,,,,,microservice; micro-service; Internet of Things; Kafka; brokerless; gRPC; Medical record; RBAC; Single Sign-On,0.0,IC4,107
10.1007/978-3-031-46077-7_27,0,0,0,0,0,107,"Kokkinis, Argyris; Nanos, Annastasios; Siozios, Kostas",Enabling an&nbsp;Isolated and&nbsp;Energy-Aware Deployment of&nbsp;Computationally Intensive Kernels on&nbsp;Multi-tenant Environments,"Embedded Computer Systems: Architectures, Modeling, and Simulation: 23rd International Conference, SAMOS 2023, Samos, Greece, July 2–6, 2023, Proceedings",978-3-031-46076-0,,https://doi.org/10.1007/978-3-031-46077-7_27,2023,,,,,Springer-Verlag,"Berlin, Heidelberg","Nowadays, hardware acceleration can be used as a service for maximizing the applications’ performance and achieve significant speedup in time-critical scenarios. FPGA devices inherently consume less power than GPUs and HPC systems and are candidate solutions for performing low-energy yet high-performance computations. However, hardware acceleration services require a private, isolated and flexible execution of the accelerators in multi-tenant environments without compromising the platform’s energy and performance efficiency. In this paper we aim to address this issue by proving an end-to-end methodology for the generation, virtualization and deployment of High-Level Synthesis accelerators in multi-tenant environments. We leverage approximate computing techniques and utilize the vAccel framework. Our proposed methodology was evaluated on the Xilinx Alveo U50 acceleration card, achieving energy savings up to 5.2× compared to the initial non energy optimized and non virtualized designs.",,,,,,,,,,,,,,,,,,,,,,,,,,virtualization; FPGA; Approximate Computing; High-Level Synthesis,0.0,EC3,2008
10.1007/978-3-031-46235-1_14,0,0,0,0,0,74,"Moussa, Mohamad; Glass, Philippe; Abdennahder, Nabil; Di Marzo Serugendo, Giovanna; Couturier, Raphaël",Towards a&nbsp;Decentralised Federated Learning Based Compute Continuum Framework,"Service-Oriented and Cloud Computing: 10th IFIP WG 6.12 European Conference, ESOCC 2023, Larnaca, Cyprus, October 24–25, 2023, Proceedings",978-3-031-46234-4,,https://doi.org/10.1007/978-3-031-46235-1_14,2023,,,,,Springer-Verlag,"Berlin, Heidelberg","The proliferation of sensing device technologies, and the growing demand for data intensive IoT applications calls for a seamless interconnection of IoT, edge and cloud resources in one computing system, to form a Compute Continuum, also referred to as edge-to-cloud.This paper targets self-adaptive Machine Learning applications that rely on data coming from IoT sensors. These applications are often “context-aware”, with high context sensitivity, different physical settings and complex usage patterns. Their intelligence, deployed on the edge, is updated on the fly.We present two Compute Continuum strategies for the deployment of such applications: (1) a centralised approach, which involves training a model on a centralised server, and (2) a decentralised approach using Federated Learning. The former approach involves centralising data from multiple sources onto a single server, while the latter locally decentralises both the training process and the aggregation and communication tasks across edge devices. In both cases the inference model is deployed on edge devices close to the collected data. The decentralised architecture relies on a coordination platform favouring self-adaptation and decentralised Federated Learning. Results show that the decentralised Federated Learning approach offers networking performances and privacy-preserving advantages compared to non-private centralised models, with a slight trade-off in prediction accuracy. According to our simulations, the deployment cost of the decentralised architecture is much lower than that of deployment on the centralised architecture.",,,,,,,,,,,,,,,,,,,,,,,,,,Edge; Cloud; Compute Continuum; Coordination model; Federated Learning; Self-Adaptive IoT applications,0.0,EC3,2786
10.1007/978-3-031-46235-1_17,0,0,0,0,0,159,"Chondrogiannis, Efthymios; Karanastasis, Efstathios; Andronikou, Vassiliki; Spătaru, Adrian; Nanos, Anastassios; Kretsis, Aristotelis; Kokkinos, Panagiotis",Intent-Based AI-Enhanced Service Orchestration for Application Deployment and Execution in the Cloud Continuum,"Service-Oriented and Cloud Computing: 10th IFIP WG 6.12 European Conference, ESOCC 2023, Larnaca, Cyprus, October 24–25, 2023, Proceedings",978-3-031-46234-4,,https://doi.org/10.1007/978-3-031-46235-1_17,2023,,,,,Springer-Verlag,"Berlin, Heidelberg","Given the complexity of contemporary applications, the varying goals and intents of their owners, and the availability of resources with fundamentally different characteristics and capabilities, the optimal deployment and execution of applications and their internal components is a rather challenging subject in the Cloud Continuum era. This includes the selection and the configuration of the resources to adequately cover the set technological and business requirements and constraints from the side of both application owners and resource providers. The aforementioned process is often and to a great extent, done manually and hence not optimally, with direct impact to the execution of an application and the usage or the available resources. In this work, we present the approach followed for the design and development of a Service Orchestrator equipped with AI techniques and the underlying multi-layered abstraction model enabling its functionality. These components were incorporated in a platform for infrastructure-agnostic deployment of data-intensive applications and tested in real-life scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,Edge Computing; Abstraction Models; Cloud Continuum; Intent-based Application Deployment and Execution; Machine Learning Techniques; Service Orchestration,0.0,EC3,2012
10.1007/978-3-031-47969-4_17,0,0,0,0,0,212,"Soumplis, Polyzois; Kokkinos, Panagiotis; Varvarigos, Emmanouel",Efficient Resource Provisioning in Critical Infrastructures Based on Multi-Agent Rollout Enabled by Deep Q-Learning,"Advances in Visual Computing: 18th International Symposium, ISVC 2023, Lake Tahoe, NV, USA, October 16–18, 2023, Proceedings, Part I",978-3-031-47968-7,,https://doi.org/10.1007/978-3-031-47969-4_17,2023,,,,,Springer-Verlag,"Berlin, Heidelberg","Next-generation smart environments, an integral part of our modern lives, integrate computing and networking technologies to enrich our experiences. Harnessing cutting-edge technologies like the Internet of Things, Artificial Intelligence, and Edge Computing, they function under the control of critical infrastructures often processing complex computer vision tasks such as object recognition and image segmentation in real-time. These infrastructures manage vast volumes of data with intensive computational demands. In response to these challenges, infrastructures have evolved to distributed that consists of resources of different capabilities and different operators. Within these environments, the security and communication among different domains are fundamental. Each domain potentially has different levels of security requirements and may use various protocols for communication. As data travels across these domains, it is exposed to a variety of threats, including data breaches, cyberattacks, and unauthorized access. In such a environment, where multiple domains co-exist, each with its own unique resources and security specifications, communication constraints across them further complicate the resource allocation process. This complexity is further increased by the diverse computing and networking constraints imposed by applications. In this work, we propose a multi-Agent Deep Reinforcement Learning mechanism that operates based on multi-Agent Rollout and deep Q-learning in order to serve the different applications’ requirements. The proposed optimization mechanism considers multiple objectives during the resource allocation process and tries to fulfill the specific constraints set by the demands and the broader objectives set by the infrastructure operator. Through rigorous evaluations, we showcase the effectiveness and efficiency of our proposed mechanisms in accommodating the heterogeneous and stringent workload requirements, whilst optimizing the use of infrastructure resources. Our simulation experiments confirm that the proposed mechanism can substantially enhance the efficiency of resource allocation in critical infrastructures.",,,,,,,,,,,,,,,,,,,,,,,,,,Reinforcement Learning; critical infrastructure; Deep Q-Learning; multi-Agent Rollout,0.0,EC3,2220
10.1007/978-3-031-48421-6_13,1,1,1,1,1,14,"Dinga, Madalina; Malavolta, Ivano; Giamattei, Luca; Guerriero, Antonio; Pietrantuono, Roberto",An Empirical Evaluation of&nbsp;the&nbsp;Energy and&nbsp;Performance Overhead of&nbsp;Monitoring Tools on&nbsp;Docker-Based Systems,"Service-Oriented Computing: 21st International Conference, ICSOC 2023, Rome, Italy, November 28 – December 1, 2023, Proceedings, Part I",978-3-031-48420-9,,https://doi.org/10.1007/978-3-031-48421-6_13,2023,,,,,Springer-Verlag,"Berlin, Heidelberg","Context. Energy efficiency is gaining importance in the design of software systems, but is still marginally addressed in the area of microservice-based systems. Energy-related aspects often get neglected in favor of other software quality attributes, such as performance, service composition, maintainability, and security.Goal. The aim of this study is to identify, synthesize and empirically evaluate the energy and performance overhead of monitoring tools employed in the microservices and DevOps context.Method. We selected four representative monitoring tools in the microservices and DevOps context. These were evaluated via a controlled experiment on an open-source Docker-based microservice benchmark system.Results. The results highlight: i) the specific frequency and workload conditions under which energy consumption and performance metrics are impacted by the tools; ii) the differences between the tools; iii) the relation between energy and performance overhead.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,IC4,73
10.1007/978-3-031-48421-6_8,0,0,0,0,0,43,"He, Xiang; Shao, Zihao; Wang, Teng; Shi, Haomai; Chen, Yin; Wang, Zhongjie",Predicting Effect and Cost of Microservice System Evolution Using Graph Neural Network,"Service-Oriented Computing: 21st International Conference, ICSOC 2023, Rome, Italy, November 28 – December 1, 2023, Proceedings, Part I",978-3-031-48420-9,,https://doi.org/10.1007/978-3-031-48421-6_8,2023,,,,,Springer-Verlag,"Berlin, Heidelberg","With the increasing prevalence of microservice technology, the architectural flexibility and scalability of software systems have witnessed notable advancements. However, this progress has also brought about a challenge in meeting the frequent changes in user requirements, thereby adversely affecting the quality of the system. It is crucial for microservice systems to undergo evolution through the modification of system configurations to adapt to changing requirements, and various methods for system evolution have been proposed. However, the evolution schemes generated by these methods vary in terms of the degree of improvement in quality and the cost required for evolution, such as time and money, i.e., different evolution effect and evolution cost. Considering the above, it is necessary to predict effect and cost before applying these schemes to real systems. Existing physical methods possess drawbacks such as high expenses and time-consuming setup procedures. Conversely, simulation methods, which are based on mathematical models, necessitate certain simplifications, resulting in disparities between the outcomes and the actual results. To overcome these challenges, this paper introduces a prediction method for microservice system evolution. By employing Graph Neural Network techniques to learn from historical data, this method enables precise prediction of the effects and costs associated with various microservice evolution schemes. And based on the above algorithm, an online prediction system is implemented, independent of the microservice system for long-term prediction. Experimental results validate the accuracy and robustness of the proposed prediction method.",,,,,,,,,,,,,,,,,,,,,,,,,,Prediction; Evolution Cost; Evolution Effect; Microservice system,0.0,EC3,2124
10.1007/978-3-031-48652-4_13,0,0,0,0,0,51,"Grafenhorst, Simon; Förderer, Kevin; Hagenmeyer, Veit",Distribution Grid Monitoring Based on&nbsp;Widely Available Smart Plugs,"Energy Informatics: Third Energy Informatics Academy Conference, EI.A 2023, Campinas, Brazil, December 6–8, 2023, Proceedings, Part II",978-3-031-48651-7,,https://doi.org/10.1007/978-3-031-48652-4_13,2023,,,,,Springer-Verlag,"Berlin, Heidelberg","The growing popularity of e-mobility, heat pumps, and renewable generation such as photovoltaics is leading to scenarios which the distribution grid was not originally designed for. Moreover, parts of the distribution grid are only sparsely instrumented, leaving the distribution system operator unaware of possible bottlenecks resulting from the introduction of such loads and renewable generation. To overcome this lack of information, we propose the use of widely available smart home devices, such as smart plugs, for grid monitoring. We detail the aggregation and storage of smart plug measurements for distribution grid monitoring and examine the accuracy of the measurements. A case study shows how the average monitoring error in a distribution grid area decreases the more measurement devices are installed. Hence, simple smart plugs can help with distribution grid monitoring and provide valuable information to the DSO.",,,,,,,,,,,,,,,,,,,,,,,,,,Distribution Grid Monitoring; Smart Grid; Smart Home Measurement Device,,,1090
10.1007/978-3-031-48803-0_6,0,0,0,0,0,247,"Valdez, Wilson; Baniata, Hamza; Markus, Andras; Kertesz, Attila",Towards a&nbsp;Simulation as&nbsp;a&nbsp;Service Platform for&nbsp;the&nbsp;Cloud-to-Things Continuum,"Euro-Par 2023: Parallel Processing Workshops: Euro-Par 2023 International Workshops, Limassol, Cyprus, August 28 – September 1, 2023, Revised Selected Papers, Part II",978-3-031-48802-3,,https://doi.org/10.1007/978-3-031-48803-0_6,2023,,,,,Springer-Verlag,"Berlin, Heidelberg","In the past years, we have seen an unprecedented pace of technological development in smart applications. Smart Systems incorporate securely connected sensors, actuators, and data processing resources to provide digital services. They provide a wide range of smart applications using emerging technologies that address governmental or industrial processes or citizen life in smart cities, and many of them have been affected by the COVID-19 pandemic which involved a general lack of trust. Integrating Blockchain-based data management into smart systems can enhance the performance, trust, and privacy of their applications, which are getting more and more crucial. In this paper, we propose a vision for a unified Simulation as a Service platform, which will be able to model and investigate Blockchain-based smart systems exploiting IoT, Fog, and Cloud Computing infrastructures.",,,,,,,,,,,,,,,,,,,,,,,,,,Internet of Things; Cloud Computing; Blockchain; Fog Computing; Simulation,0.0,EC3,2574
10.1007/978-3-031-49361-4_5,0,0,0,0,0,240,"Garlisi, Domenico; Restuccia, Gabriele; Tinnirello, Ilenia; Cuomo, Francesca; Chatzigiannakis, Ioannis",Real-Time Leakage Zone Detection in&nbsp;Water Distribution Networks: A Machine Learning-Based Stream Processing Algorithm,"Algorithmic Aspects of Cloud Computing: 8th International Symposium, ALGOCLOUD 2023, Amsterdam, The Netherlands, September 5, 2023, Revised Selected Papers",978-3-031-49360-7,,https://doi.org/10.1007/978-3-031-49361-4_5,2023,,,,,Springer-Verlag,"Berlin, Heidelberg","This work presents LEAKSTREAM, a novel approach for detecting leakages in water distribution networks by leveraging the power of clustering and stream processing techniques, combined with advanced machine learning approaches. Given the critical importance of efficient water management and the significant economic and environmental consequences of undetected leaks, it is crucial to develop innovative strategies that can accurately and promptly identify anomalies within distribution networks.Our proposed method focuses on an algorithm that creates clusters of nodes in the water distribution network, taking into account their spatial proximity and hydraulic characteristics. We employ stream processing to efficiently handle large-scale real-time data generated from meters installed at consumer locations throughout the network.The results of our study indicate that the proposed leakage detection algorithm effectively employs a generalization technique to detect events not included in the training data. We show performance in terms of precision and loss in the case of 75 different leakages positions in a water distribution network which extends in an area of 7×3.5sqkm. Furthermore, its capacity to process data at the edge and in real-time enables prompt responses and mitigation measures, thereby reducing the overall impact of leaks on both the environment and infrastructure.Our results attain an average accuracy of detecting and localizing the zone of the leakages of about 98.6% when leakages are not present during the training of the ML models.",,,,,,,,,,,,,,,,,,,,,,,,,,IoT; stream processing; neural network; k-means clustering; leakage detection; LoRaWAN; water supply system; WDN,0.0,EC3,2727
10.1007/978-3-031-49435-2_1,0,0,0,0,0,203,"Sukhoroslov, Oleg; Gorokhovskii, Maksim",Benchmarking DAG Scheduling Algorithms on&nbsp;Scientific Workflow Instances,"Supercomputing: 9th Russian Supercomputing Days, RuSCDays 2023, Moscow, Russia, September 25–26, 2023, Revised Selected Papers, Part II",978-3-031-49434-5,,https://doi.org/10.1007/978-3-031-49435-2_1,2023,,,,,Springer-Verlag,"Berlin, Heidelberg","Many real-world applications executed on distributed computing systems are organized as directed acyclic graphs (DAGs) of tasks. The algorithm employed for scheduling these tasks across the system has a substantial impact on the achieved performance. Despite the numerous DAG scheduling algorithms proposed by researchers, there is a lack of benchmarks that evaluate the performance of such algorithms on a set of real application instances in realistic conditions. Thus developers of runtime systems often resort to the use of simple but inefficient algorithms. In this work we aim to fill this gap by proposing a benchmark for evaluating DAG scheduling algorithms based on a set of 150 real-world workflow instances with up to 1695 tasks and 10 realistic cluster configurations with multi-core machines. We apply this benchmark for evaluation of 16 scheduling algorithms including the well-known static algorithms and the commonly used in practice dynamic algorithms. The obtained results demonstrate that the proposed benchmark allows to clearly separate and compare the algorithms performance from different angles and to make important observations.",,,,,,,,,,,,,,,,,,,,,,,,,,Benchmark; Simulation; Distributed computing; DAG scheduling; Task graph; Workflow,0.0,EC3,2116
10.1007/978-3-031-61063-9_19,0,0,0,0,0,217,"Hu, Juncheng; Zhou, Youtian; Mo, Chunbao",Design and Optimization Strategy of a CNN Aided Pre-hospital Diagnosis System for Myocardial Infarction,"Digital Human Modeling and Applications in Health, Safety, Ergonomics and Risk Management: 15th International Conference, DHM 2024, Held as Part of the 26th HCI International Conference, HCII 2024, Washington, DC, USA, June 29–July 4, 2024, Proceedings, Part II",978-3-031-61062-2,,https://doi.org/10.1007/978-3-031-61063-9_19,2024,,,,,Springer-Verlag,"Berlin, Heidelberg","Myocardial Infarction (MI), commonly known as a heart attack, is a prevalent acute cardiovascular disease and one of the deadliest types of cardiovascular diseases in China. Electrocardiogram (ECG) serves as the primary tool for analysis and diagnosis, recording the heart’s electrical activity to detect abnormal patterns. ECG exhibits nonlinear and unstable characteristics, with its noise being random. Therefore, the extraction and identification of ECG features related to MI are crucial for intelligent assisted diagnosis of heart attacks. In this paper, a MI assisted pre-hospital diagnosis system based on CNN (Convolution Neural Network, CNN) is designed, to assist healthcare professionals in improving the accuracy and efficiency of MI diagnosis. The MI assisted diagnosis system, based on the CNN MI intelligent diagnostic model, is established with the goal of intelligent identification of MI in the context of computer-aided diagnosis systems. It aims to address key issues such as automatic feature extraction and analysis diagnosis, focusing on ECG research and exploring MI intelligent identification methods based on CNN. This system is dedicated to providing assistance and support for clinical diagnosis by healthcare professionals, enhancing diagnostic accuracy and efficiency to ensure early diagnosis and timely treatment of MI. Additionally, in cases of sudden cardiac emergencies among ECG users, the system is capable of providing timely assistance and treatment plans, offering critical support to ECG users in need.",,,,,,,,,,,,,,,,,,,,,,,,,,Healthcare; Artificial Intelligence &amp; CNN; Diagnosis System; Myocardial Infarction; Smart Services,,,1255
10.1007/978-3-031-62362-2_10,0,0,0,0,0,90,"Dalamagkas, Christos; Georgakis, Angelos; Hrissagis-Chrysagis, Kostas; Papadakis, George",The Open V2X Management Platform,"Web Engineering: 24th International Conference, ICWE 2024, Tampere, Finland, June 17–20, 2024, Proceedings",978-3-031-62361-5,,https://doi.org/10.1007/978-3-031-62362-2_10,2024,,,,,Springer-Verlag,"Berlin, Heidelberg","In this paper, we present an open-source web-based system, called Open V2X Management Platform (O-V2X-MP), which facilitates the management of charging points for electric vehicles with the goal of realizing Vehicle-to-Everything (V2X) scenarios. We describe its backend, which is composed of numerous components through a microservices architecture leveraging Docker containers. Using the C4 model methodology, we present the system context diagram with the main actors that interact with O-V2X-MP, the container diagram with the main communication flows between its components and the component diagram with the modules comprising the core of the platform. We also delve into the value-added services of O-V2X-MP, namely the billing engine, the REST APIs that enable interconnections and integration with external systems, the data analytics capabilities and the cyber-security module. We conclude with our future plans for the platform’s frontend.",,,,,,,,,,,,,,,,,,,,,,,,,,microservices; Charging points management; Electric Vehicles; OCPP protocol; Web platform,,,666
10.1007/978-3-031-62645-6_10,0,0,0,0,0,255,"Rousseaux, Tom; Crochet, Christophe; Aoga, John; Legay, Axel",Network Simulator-Centric Compositional Testing,"Formal Techniques for Distributed Objects, Components, and Systems: 44th IFIP WG 6.1 International Conference, FORTE 2024, Held as Part of the 19th International Federated Conference on Distributed Computing Techniques, DisCoTec 2024, Groningen, The Netherlands, June 17–21, 2024, Proceedings",978-3-031-62644-9,,https://doi.org/10.1007/978-3-031-62645-6_10,2024,,,,,Springer-Verlag,"Berlin, Heidelberg","This article introduces a novel methodology, Network Simulator-centric Compositional Testing (NSCT), to enhance the verification of network protocols with a particular focus on time-varying network properties. NSCT follows a Model-Based Testing (MBT) approach. These approaches usually struggle to test and represent time-varying network properties. NSCT also aims to achieve more accurate and reproducible protocol testing. It is implemented using the Ivy tool and the Shadow network simulator. This enables online debugging of real protocol implementations. A case study on an implementation of QUIC (picoquic) is presented, revealing an error in its compliance with a time-varying specification. This error has subsequently been rectified, highlighting NSCT’s effectiveness in uncovering and addressing real-world protocol implementation issues. The article underscores NSCT’s potential in advancing protocol testing methodologies, offering a notable contribution to the field of network protocol verification.",,,,,,,,,,,,,,,,,,,,,,,,,,Adverse Stimuli; Concrete Implementation; Formal Specifications; Internet protocols; Model-Based Testing; Network Simulator; QUIC; Software verification and validation; Time-varying Network Properties,,,770
10.1007/978-3-031-63783-4_18,0,0,0,0,0,139,"Kalka, Wojciech; Szydlo, Tomasz",μChaos: Moving Chaos Engineering to IoT Devices,"Computational Science – ICCS 2024: 24th International Conference, Malaga, Spain, July 2–4, 2024, Proceedings, Part VII",978-3-031-63785-8,,https://doi.org/10.1007/978-3-031-63783-4_18,2024,,,,,Springer-Verlag,"Berlin, Heidelberg","The concept of the Internet of Things (IoT) has been widely used in many applications. IoT devices can be exposed to various external factors, such as network congestion, signal interference, and limited network bandwidth. This paper proposes an open-source μChaos software tool for the ZephyrOS real-time operating system for embedded devices. The proposed tool intends to inject failures into device’s applications in a controlled manner to improve their error-handling algorithms. The proposed novel framework fills the gap in the chaos engineering tools for edge devices in the cloud-edge continuum. In the paper, we also discuss the typical failures of IoT devices and the potential use cases of the solution.",,,,,,,,,,,,,,,,,,,,,,,,,,IoT; Chaos Engineering; fault injection; Zephyr-OS,,,238
10.1007/978-3-031-65154-0_9,0,0,0,0,0,184,"Bogdanov, Alexander; Khvatov, Valery; Uteshev, Alexei; Shchegoleva, Nadezhda",Virtual Blockchain Network for Secure Financial and Industrial Applications,"Computational Science and Its Applications – ICCSA 2024 Workshops: Hanoi, Vietnam, July 1–4, 2024, Proceedings, Part I",978-3-031-65153-3,,https://doi.org/10.1007/978-3-031-65154-0_9,2024,,,,,Springer-Verlag,"Berlin, Heidelberg","This study introduces blockchain virtualization as a solution to tackle modern challenges. Evaluating the DGT network as a model, experiments probe the impact of virtualization (network-of-networks) on scaling and latency, shedding light on its potential for enhanced performance. Conclusively, the study emphasizes the significance of blockchain virtualization for enhancing blockchain performance and scalability. Future directions of research are discussed, including testing of security and privacy within virtualized environments and the potential of virtualization in shaping cloud-based blockchain-as-a-service solutions. In sum, blockchain virtualization emerges as a pivotal innovation poised to redefine the landscape of blockchain technology and its applications.",,,,,,,,,,,,,,,,,,,,,,,,,,decentralization; DGT; Layer 1 digital platform; multi-party computation over blockchain,,,983
10.1007/978-3-031-65329-2_23,0,0,0,0,0,114,"Bakhsh, Wazeer; Fiori, Chiara; de Luca, Stefano","Literature Review on the Smart Port: Evolution, Technological Development, Performance Indicators of Smart Ports","Computational Science and Its Applications – ICCSA 2024 Workshops: Hanoi, Vietnam, July 1–4, 2024, Proceedings, Part IX",978-3-031-65328-5,,https://doi.org/10.1007/978-3-031-65329-2_23,2024,,,,,Springer-Verlag,"Berlin, Heidelberg","Boost in the global supply chain in the post-industry 4.0 revolution has amplified the demand for maritime ports, triggering their operational capacities, competitiveness, and sustainability concerns. Maritime ports are undergoing a paradigm shift, an evolution from goods loading and unloading terminals to highly intelligent hubs in the logistic supply chain.Existing literature on smart ports focuses on different aspects such as evolution stages, bibliometrics analysis, feasible technologies, and Key Performance Indicators (KPI).This review complements the existing ones by investigating the term “smart port” definition, the technology use, the Key Performance Indicators (KPIs), and providing bibliometric analysis across major databases, sourcing from journals, conferences, official websites, and books in various disciplines.Results show that the definition of “smart port” is still under development. Moreover, the major driver of transformation is the technology use, some technologies are mature enough for implementation and the rest are in the early stages. Importantly, along with innovative technologies, management skills play a crucial role in successful implementation. The Smartness of the port is difficult to answer but an accurate quantification of Key Performance Indicators (KPIs) can assist the ports in gauging their level of “smartness”.From the bibliometric perspective, there is a growing trend of smart port research, particularly in recent years. This trend is interestingly almost uniformly distributed in different geographic locations.The findings underscore the kernel role of technology in shaping smart port evolution, and performance indicators for quantifying the smartness. The investigation of the challenges for ports to implement technologies is a key area for future research.",,,,,,,,,,,,,,,,,,,,,,,,,,Industry 4.0; Container Terminal Digital transformation; Port Sustainability; Smart Port; Smart Port KPIs,,,1297
10.1007/978-3-031-66459-5_2,0,0,0,0,0,83,"Alho, Riku; Raatikainen, Mikko; Myllyaho, Lalli; Nurminen, Jukka K.",On Modularity of&nbsp;Neural Networks: Systematic Review and&nbsp;Open Challenges,"Reuse and Software Quality: 21st International Conference on Software and Systems Reuse, ICSR 2024, Limassol, Cyprus, June 19–20, 2024, Proceedings",978-3-031-66458-8,,https://doi.org/10.1007/978-3-031-66459-5_2,2024,,,,,Springer-Verlag,"Berlin, Heidelberg","Modularity is used to manage the complexity of monolithic software systems and is a de facto practice in software engineering. Similar modularity concepts might also translate beneficially to machine learning, uncovering equivalent benefits, such as reuse opportunities. We address the research problem of modular neural networks’ (MNNs) applicability, operations, and comparability to monolithic solutions. A systematic literature review is used to identify 86 studies that provide information regarding modularity compared to monolithic solutions. The selected studies address many tasks and domains, evidencing broad applicability, although applied modularity operations are limited mainly to splitting. Nearly two-thirds of studies show improvements in task accuracy compared to monolithic solutions. Only 16% of studies report performance values in their comparisons, but 82% report MNN performance benefits in training and inference time, memory, and energy consumption compared to monolithic solutions. However, publication bias can favor MNNs, and most studies were conducted in laboratory environments on focused tasks and static requirements. Nevertheless, we conclude that MNNs perform at least satisfactorily compared to monolithic solutions. Modularity can bring forth benefits for managing complexity and has the potential for development and operations performance efficiency. Therefore, modularity in neural networks opens research avenues for extending software reuse, especially regarding broadening the applicability of advanced solutions and experiences from long-term or industrial applications and use.",,,,,,,,,,,,,,,,,,,,,,,,,,deep learning; systematic literature review; modular neural networks; modularity,,,271
10.1007/978-3-031-66459-5_8,0,0,0,0,0,0,"Serrano-Gutierrez, Pablo; Ayala, Inmaculada",Using Energy Consumption for Self-adaptation in&nbsp;FaaS,"Reuse and Software Quality: 21st International Conference on Software and Systems Reuse, ICSR 2024, Limassol, Cyprus, June 19–20, 2024, Proceedings",978-3-031-66458-8,,https://doi.org/10.1007/978-3-031-66459-5_8,2024,,,,,Springer-Verlag,"Berlin, Heidelberg","One of the programming models that has been developing the most in recent years is Function as a Service (FaaS). The growing concern over data centre energy footprints has driven sustainable software development. In serverless applications, energy consumption depends on the energy consumption of the application’s functions. However, measuring energy proves challenging, and the results’ variability complicates optimisation efforts at runtime. This article addresses this issue by measuring serverless function energy consumption and exploring integration into an optimisation system that selects implementations based on their current energy footprint. For this, we have integrated an energy measurement software into a FaaS system. We have analysed how to properly process the data and how to use them to perform self-adaptation. We present a series of methods and policies that make our system not only capable of detecting variations in the energy consumption of the functions, but it does so taking into account the variability in the measurements that each function may present. Our experiments showcase proper integration in a self-adaptive system, showing a reduction up to 5% in energy consumption due to functions in a test application.",,,,,,,,,,,,,,,,,,,,,,,,,,serverless; sustainability; self-adaptive,0.0,EC3,1666
10.1007/978-3-031-70753-7_7,0,0,0,0,0,168,"Tsutsumi, Keishin; Phung-Duc, Tuan; Truong, Hong-Linh",Queueing Analysis of&nbsp;an&nbsp;Ensemble Machine Learning System,"Analytical and Stochastic Modelling Techniques and Applications: 28th International Conference, ASMTA 2024, Venice, Italy, June 14, 2024, Proceedings",978-3-031-70752-0,,https://doi.org/10.1007/978-3-031-70753-7_7,2024,,,,,Springer-Verlag,"Berlin, Heidelberg","Recent advances in AI/ML technologies have accelerated the development of various ML applications. One of the major trends in AI/ML application development is the increasing use of multiple ML models to support high-accuracy inference in a complex end-to-end ML serving. However, testing the right configuration of multiple ML models is expensive, and the application requirements for ML inferences are highly dependent on various factors like the quality of ML models, computing resource performance, and data quality. In this context, techniques and methods that help to emulate and analyze ML inference characteristics using queueing theory can reduce the development effort and cost for ML services encapsulating ML models but also the entire ML system. In this paper, we modeled and analyzed a queueing model for an ML system that uses ensemble learning as an inference method with a new rule and clarified the impacts of model design in ensemble learning on the system’s performance. As a result, we demonstrate the usefulness of the analysis for understanding possible configurations and their efficiency in the ML system through queueing analysis and simulation.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,EC3,2745
10.1007/978-3-031-70797-1_19,0,0,0,0,0,228,"Dini, Vick; Tamburri, Damian Andrew; Di Nitto, Elisabetta",Electric Vehicle Fast-Charging Software: Architectural Considerations Towards Trustworthiness,"Software Architecture: 18th European Conference, ECSA 2024, Luxembourg City, Luxembourg, September 3–6, 2024, Proceedings",978-3-031-70796-4,,https://doi.org/10.1007/978-3-031-70797-1_19,2024,,,,,Springer-Verlag,"Berlin, Heidelberg","The industry of electric vehicle charging stations is rapidly evolving. As more people purchase electric vehicles, the need for an efficient and reliable charging service value chain becomes paramount. In this paper we perform a literature review and topic modeling in the field to identify the most relevant topics and challenges therein as well as any standard or reference software architectures proposed to date. Subsequently, by means of ethnomethodological research and interview study with industry practitioners from an emerging Charging Point Operator (CPO) we identify major practical architectural concerns. We conclude that typical -Ops issues related to monitoring and autonomous runtime management of fast-charging software architectures are not addressed in the literature so far, and yet are the most critical for practitioners; this inspired a revised augment of state-of-the-art reference architecture frameworks—offered as the first key original contribution of this paper—for practitioners and researchers to use in the future. Finally, evaluating such framework with practitioners’ insights, we elaborated a second key contribution of this work, namely, six architectural considerations to be taken into account towards trustworthy electric vehicle fast-charging software architectures of the future.",,,,,,,,,,,,,,,,,,,,,,,,,,software architecture; software engineering; charging station; cloud-edge computing; electric vehicle; fast charging,,,842
10.1007/978-3-031-71707-9_18,0,0,0,0,0,191,"Stentoumis, Christos; Katsiokalis, Minas; Bikiris, Panagiotis; Karantakis, Nikos",Engineering a&nbsp;BIM-Based Mixed Reality Application for&nbsp;the&nbsp;Life-Cycle Management of&nbsp;Buildings,"Extended Reality: International Conference, XR Salento 2024, Lecce, Italy, September 4–7, 2024, Proceedings, Part I",978-3-031-71706-2,,https://doi.org/10.1007/978-3-031-71707-9_18,2024,,,,,Springer-Verlag,"Berlin, Heidelberg","In this research and innovation work, we propose an open and interoperable approach for exploiting the various building data collected across the different phases of a building’s life cycle via a BIM-based Mixed Reality (MR) platform. Building information modelling (BIM) has gradually gained attention in recent decades and is widely considered a key to digitising and optimising the building life cycle. Yet, there are significant challenges in the standardization of methodologies, interoperability of solutions, and data exchange, as well as the usability of BIM-related solutions. Moreover, modern or renovated buildings produce extensive real-time data on top of the static data organised in BIM, and it is commonly accepted that most professionals in the field do not have access to the critical static or dynamic data that accompany a building project. Here, we exploit MR as an advanced user interface for enhanced interaction with building static and dynamic data instantiated by a BIM model, while we address a seamless conversion of BIM data to an online visual digital twin that supports the MR mobile application.",,,,,,,,,,,,,,,,,,,,,,,,,,BIM; building life-cycle management; Mixed Reality,0.0,EC3,2862
10.1007/978-3-031-73741-1_15,0,0,0,0,0,180,"Brandon, Colm; Singh, Amandeep; Margaria, Tiziana",Model Driven Development for&nbsp;AI-Based Healthcare Systems: A Review,"Bridging the Gap Between AI and Reality: First International Conference, AISoLA 2023, Crete, Greece, October 23–28, 2023, Selected Papers",978-3-031-73740-4,,https://doi.org/10.1007/978-3-031-73741-1_15,2023,,,,,Springer-Verlag,"Berlin, Heidelberg","We review our experience with integrating Artificial Intelligence (AI) into healthcare systems following the Model-Driven Development (MDD) approach. At a time when AI has the potential to instigate&nbsp;a paradigm shift in the health sector, better integrating healthcare experts in the development of these technologies is of paramount importance. We see MDD as a useful way to better&nbsp;embed non-technical stakeholders in the development process. The main&nbsp;goal of this review is to reflect on our experiences to date&nbsp;with MDD and AI in the context of developing healthcare systems. Four case studies that fall within that scope but&nbsp;have different profiles are introduced and summarised: the&nbsp;MyMM application for Multiple Myeloma diagnosis; CNN-HAR, that studies the ability to do AI on the edge for IoT-supported human activity recognition; the HIPPP web based portal for patient information&nbsp;in public health; and Cinco de Bio, a new model driven platform&nbsp;used for the first time to support a better cell-level understanding&nbsp;of diseases. Based on the aforementioned case studies we discuss&nbsp;the characteristics, the challenges faced and the postive outcomes achieved.",,,,,,,,,,,,,,,,,,,,,,,,,,Artificial Intelligence Applications; Digital Thread; Health Informatics; Low-code/no-code; Model Driven Development,,,695
10.1007/978-3-031-74738-0_12,0,0,0,0,0,136,"Clausen, Christian Skafte Beck; Lehnhoff, Sebastian; Schwarz, Jan Sören; Jørgensen, Bo Nørregaard; Ma, Zheng Grace",Challenges in Transitioning from Co-simulation to Practical Application: A Case Study on Economic Emission Dispatch in a Greenhouse Compartment,"Energy Informatics: 4th Energy Informatics Academy Conference, EI.A 2024, Kuta, Bali, Indonesia, October 23–25, 2024, Proceedings, Part I",978-3-031-74737-3,,https://doi.org/10.1007/978-3-031-74738-0_12,2024,,,,,Springer-Verlag,"Berlin, Heidelberg","Co-simulation is a widely applied method used to analyze the behavior of complex, interdisciplinary, and integrated cyber-physical control systems. Despite its prevalence, the transition from co-simulated control systems into practical applications is not discussed as much in the literature. This leaves a gap in the literature because practitioners may not be aware of these challenges. This paper aims to uncover and discuss some of the challenges that arise in the transition from a co-simulated control system to a practical application.A case study on economic emission dispatch in a Danish industrial greenhouse compartment serves as the fundament in studying these challenges. Economic emission dispatch is a method that can be used in a closed-loop arrangement to decrease costs and emissions of multiple energy production units. The case study is first implemented as a co-simulation which is subject to a subsequent practical implementation. The co-simulation implementation is governed by the open-source framework mosaik that is used extensively in smart grid applications. In contrast, the practical implementation is not governed by mosaik due to architectural design discrepancies. A key feature of the study is the use of software-in-the-loop, which means that the controller being tested is the actual software intended for deployment.The highlighted challenges include that the core abstractions (master algorithm, scenario-script, and protocol) of the co-simulation framework cannot be transferred to an operational context due to design discrepancies. Despite these challenges, the co-simulation can still serve as a baseline for comparing functional performance metrics during the transition.",,,,,,,,,,,,,,,,,,,,,,,,,,challenges; co-simulation; cyber-physical system; Economic dispatch; greenhouse; SIL; simulation-to-reality gap; software-in-the-loop; transition,0.0,EC3,2300
10.1007/978-3-031-78386-9_31,0,0,0,0,0,11,"Steidl, Monika; Leitner, Michael; Urbanke, Pirmin; Gattringer, Marko; Felderer, Michael; Ristov, Sashko",Understanding Microservice Runtime Monitoring Data for&nbsp;Anomaly Detection with&nbsp;Structural Equation Modeling,"Product-Focused Software Process Improvement: 25th International Conference, PROFES 2024, Tartu, Estonia, December 2–4, 2024, Proceedings",978-3-031-78385-2,,https://doi.org/10.1007/978-3-031-78386-9_31,2024,,,,,Springer-Verlag,"Berlin, Heidelberg","Microservices reliability is critical, but runtime anomalies are increasingly common due to system complexity. Rule-based and AI-based anomaly detection methods assist practitioners in analyzing runtime monitoring data (logs, traces, metrics) to identify anomalies. However, these methods rely&nbsp;on high-quality datasets and deep domain knowledge to deliver accurate results. Thus, a significant challenge lies in the lack of consensus on which runtime monitoring parameters effectively represent&nbsp;the system and microservices, reliably indicate anomalies,&nbsp;or distinguish deviations that genuinely signal anomalies. A thorough understanding of the dataset, key monitoring parameters,&nbsp;and microservice dependencies is crucial to minimize bias and&nbsp;false positives, ultimately improving the effectiveness of anomaly detection methods.Thus, we investigate whether structural equation modeling&nbsp;can describe the system’s or microservices’ behavior via indicators extracted from runtime monitoring data and identify their causal relationships. We used EvoMaster to simulate user behavior&nbsp;in TrainTicket and extract runtime monitoring data to test our model. Our results show that the identified indicators effectively describe microservices’ behavior, but network indicators alone&nbsp;are insufficient for describing the whole system’s behavior. The&nbsp;model can also identify microservices that significantly influence&nbsp;the whole system’s behavior.",,,,,,,,,,,,,,,,,,,,,,,,,,Metrics; Anomaly detection during runtime; Logs; Microservice; Monitoring data; PLS-SEM; Structural equation model; Traces,0.0,EC3,2122
10.1007/978-3-031-81375-7_15,0,0,0,0,0,151,"Faria, Bruno; Abreu, David Perez; Velasquez, Karima; Curado, Marília",Self-organising Approach to&nbsp;Anomaly Mitigation in&nbsp;the&nbsp;Cloud-to-Edge Continuum,"Cooperative Information Systems: 30th International Conference, CoopIS 2024, Porto, Portugal, November 19–21, 2024, Proceedings",978-3-031-81374-0,,https://doi.org/10.1007/978-3-031-81375-7_15,2025,,,,,Springer-Verlag,"Berlin, Heidelberg","The cloud-to-edge continuum paradigm has permeated various application domains, including critical urban-city safety systems. In these contexts, anomalies can compromise public safety, for example, by disrupting the communication between smart city infrastructure and vehicles, which aims to prevent accidents at pedestrian crossings. Given these environments’ heterogeneous and large-scale nature, manual recovery from anomalies is not feasible. Machine Learning techniques have emerged as an alternative, supporting a zero-touch approach that enables self-organising and self-healing solutions for anomaly prediction, detection, and mitigation. This paper proposes an Artificial Intelligence-driven, self-organising approach for anomaly management in the cloud-to-edge continuum, integrating both reactive and proactive mechanisms. We evaluate different Machine Learning models, including Random Forest Classifiers, Neural Networks, and Convolutional Neural Networks, to predict node performance anomalies. The simulation results obtained using the COSCO framework showcase the effectiveness of our method. It achieves an F1 score of 73% for multiclass classification, predicting different levels of anomaly severity, and 87% for binary classification, distinguishing between normal and abnormal states.",,,,,,,,,,,,,,,,,,,,,,,,,,Machine learning; Cloud-to-edge continuum; Anomaly management; Self-X; Time-series classification; Zero-touch,,,493
10.1007/978-3-031-81542-3_11,0,0,0,0,0,60,"Akmal, Muhammad Uzair; Asif, Saara; Koval, Leonid; Mathias, Selvine G.; Knollmeyer, Simon; Grossmann, Daniel",Layered Data-Centric AI to&nbsp;Streamline Data Quality Practices for&nbsp;Enhanced Automation,"Artificial Intelligence: Methodology, Systems, and Applications: 19th International Conference, AIMSA 2024, Varna, Bulgaria, September 18–20, 2024, Proceedings",978-3-031-81541-6,,https://doi.org/10.1007/978-3-031-81542-3_11,2025,,,,,Springer-Verlag,"Berlin, Heidelberg","Most artificial intelligence (AI) applications are designed under the model-centric AI (MCAI) approach, where data scientists aim to optimize the machine learning (ML) models starting with fixed, preprocessed data. However, businesses often struggle with limited datasets, changes in data over time, and limited ML knowledge, making it difficult to maintain data quality. One potential approach is data-centric AI (DCAI), which systematically improves the data quality used to build AI systems. However, rapid growth in data volume leads to challenges in selecting the most suitable operations for enhancing data quality thereby maintaining data accuracy, completeness, consistency, and reliability in real-world applications. To address this gap, we propose a novel framework, namely Layered Data-Centric AI (LDCAI) by expanding upon the existing DCAI Pipeline. LDCAI employs a three-layered approach, starting with the DCAI layer which generates the initial training data. This data is then analyzed and refined in the Data Analysis (DA) layer by a team of data scientists and domain experts. The refined data is later sent to the Data Quality Control (DQC) layer for additional quality checks and improvements by data scientists. The entire process can be iterated as needed to continuously improve data quality, which can be fed back into the DCAI layer’s model to achieve better predictions and outcomes. The implementation is illustrated through a scalable and customizable architecture designed for optimizing data quality within AI systems.",,,,,,,,,,,,,,,,,,,,,,,,,,Artificial intelligence; Data analysis; Data quality; Data-centric,,,353
10.1007/978-3-319-69035-3_17,0,0,0,0,0,110,"HoseinyFarahabady, MohammadReza; Lee, Young Choon; Zomaya, Albert Y.; Tari, Zahir",A QoS-Aware Resource Allocation Controller for Function as a Service (FaaS) Platform,"Service-Oriented Computing: 15th International Conference, ICSOC 2017, Malaga, Spain, November 13–16, 2017, Proceedings",978-3-319-69034-6,,https://doi.org/10.1007/978-3-319-69035-3_17,2017,,,,,Springer-Verlag,"Berlin, Heidelberg","Function as a Service (FaaS) is a recent event-driven serverless paradigm that allows enterprises to build their applications in a fault tolerant distributed manner. Having been considered as an attractive replacement of traditional Service Oriented Architecture (SOA), the FaaS platform leverages the management of massive data sets or the handling of event streams. However, the realization of such leverage is largely dependent on the effective exploitation of FaaS elasticity/scalability.In this paper, we present a closed-loop resource allocation controller to dynamically scale resources by predicting the future rate of incoming events and by considering the Quality of Service (QoS) enforcements requested by end-users. The performance evaluation is carried out by comparing the proposed controller with some well-known heuristics such as round robin and best-effort strategies. Experimental results confirm that the proposed controller increases the overall resource utilization by 21% on average, while reducing QoS violations by a factor of almost 3.",,,,,,,,,,,,,,,,,,,,,,,,,,Microservices; Dynamic resource allocation; Function as a Service (FaaS); Serverless Lambda Platform,0.0,EC3,2790
10.1007/978-3-319-69035-3_28,0,0,0,0,0,252,"Ravindra, Pushkara; Khochare, Aakash; Reddy, Siva Prakash; Sharma, Sarthak; Varshney, Prateeksha; Simmhan, Yogesh",: An Adaptive Orchestration Platform for Hybrid Dataflows across Cloud and Edge,"Service-Oriented Computing: 15th International Conference, ICSOC 2017, Malaga, Spain, November 13–16, 2017, Proceedings",978-3-319-69034-6,,https://doi.org/10.1007/978-3-319-69035-3_28,2017,,,,,Springer-Verlag,"Berlin, Heidelberg","The Internet of Things (IoT) is offering unprecedented observational data that are used for managing Smart City utilities. Edge and Fog gateway devices are an integral part of IoT deployments to acquire real-time data and enact controls. Recently, Edge-computing is emerging as first-class paradigm to complement Cloud-centric analytics. But a key limitation is the lack of a platform-as-a-service for applications spanning Edge and Cloud. Here, we propose , an orchestration platform for dataflows across distributed resources. ’s hybrid dataflow composition can operate on diverse data models – streams, micro-batches and files, and interface with native runtime engines like TensorFlow and Storm to execute them. It manages the application’s lifecycle, including container-based deployment and a registry for state management. can schedule the dataflow on different Edge, Fog&nbsp;and&nbsp;Cloud resources, and also perform dynamic task migration between resources. We validate the platform for executing video analytics and sensor streams for Smart Traffic and Smart Utility applications on Raspberry Pi, NVidia TX1, ARM64 and Azure Cloud VM resources, and present our results.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,EC3,2534
10.1007/978-981-96-0808-9_6,0,0,0,0,0,35,"Sedlak, Boris; Morichetta, Andrea; Wang, Yuhao; Fei, Yang; Wang, Liang; Dustdar, Schahram; Qu, Xiaobo",SLO-Aware Task Offloading Within Collaborative Vehicle Platoons,"Service-Oriented Computing: 22nd International Conference, ICSOC 2024, Tunis, Tunisia, December 3–6, 2024, Proceedings, Part II",978-981-96-0807-2,,https://doi.org/10.1007/978-981-96-0808-9_6,2024,,,,,Springer-Verlag,"Berlin, Heidelberg","In the context of autonomous vehicles (AVs), offloading is essential for guaranteeing the execution of perception tasks, e.g., mobile mapping or object detection. While existing work on offloading focused extensively on minimizing inter-vehicle networking latency, vehicle platoons (e.g., heavy-duty transport) present numerous other objectives, such as energy efficiency or data quality. To optimize these Service Level Objectives (SLOs) during operation, this work presents a purely Vehicle-to-Vehicle approach (V2V) for collaborative services offloading within a vehicle platoon. By training and using a Bayesian Network (BN), services can proactively decide to offload whenever this promises to improve platoon-wide SLO fulfillment; therefore, vehicles estimate how both sides would be impacted by offloading a service. In particular, this considers resource heterogeneity within the platoon to avoid overloading more restricted devices. We evaluate our approach in a physical setup, where vehicles in a platoon continuously (i.e., every 500 ms) interpret the SLOs of three perception services. Our probabilistic, predictive method shows promising results in handling large AV platoons; within seconds, it detects and resolves SLO violations through offloading.",,,,,,,,,,,,,,,,,,,,,,,,,,Microservices; Edge Computing; Bayesian Networks; Intelligent Transportation; Offloading; Service Level Objectives,,,885
10.1109/ASE.2019.00123,0,0,0,0,0,246,"Reichelt, David Georg; Kühne, Stefan; Hasselbring, Wilhelm",PeASS: a tool for identifying performance changes at code level,Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering,978-1-7281-2508-4,,https://doi.org/10.1109/ASE.2019.00123,2020,ASE '19,,,,IEEE Press,"San Diego, California","We present PeASS (Performance Analysis of Software System versions), a tool for detecting performance changes at source code level that occur between different code versions. By using PeASS, it is possible to identify performance regressions that happened in the past to fix them.PeASS measures the performance of unit tests in different source code versions. To achieve statistic rigor, measurements are repeated and analyzed using an agnostic t-test. To execute a minimal amount of tests, PeASS uses a regression test selection.We evaluate PeASS on a selection of Apache Commons projects and show that 81% of all unit test covered performance changes can be found by PeASS. A video presentation is available at https://www.youtube.com/watch?v=RORFEGSCh6Y and PeASS can be downloaded from https://github.com/DaGeRe/peass.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1279
10.1109/ICSE-Companion.2019.00086,0,0,0,0,0,208,"Zhao, Yixue",Mobile-app analysis and instrumentation techniques reimagined with DECREE,Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings,,,https://doi.org/10.1109/ICSE-Companion.2019.00086,2019,ICSE '19,,,,IEEE Press,"Montreal, Quebec, Canada","A large number of mobile-app analysis and instrumentation techniques have emerged in the past decade. However, those techniques' components are difficult to extract and reuse outside their original tools, their evaluation results are hard to reproduce, and the tools themselves are hard to compare. This paper introduces DECREE, an infrastructure intended to guide such techniques to be reproducible, practical, reusable, and easy to adopt in practice. DECREE allows researchers and developers to easily discover existing solutions to their needs, enables unbiased and reproducible evaluation, and supports easy construction and execution of replication studies. The paper describes DECREE's three modules and its potential to fundamentally alter how research is conducted in this area.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,664
10.1109/ICSE-SEET58685.2023.00015,0,0,0,0,0,234,"Lanubile, Filippo; Martínez-Fernández, Silverio; Quaranta, Luigi",Teaching MLOps in Higher Education through Project-Based Learning,Proceedings of the 45th International Conference on Software Engineering: Software Engineering Education and Training,979-8-3503-2259-0,,https://doi.org/10.1109/ICSE-SEET58685.2023.00015,2023,ICSE-SEET '23,,,,IEEE Press,"Melbourne, Australia","Building and maintaining production-grade ML-enabled components is a complex endeavor that goes beyond the current approach of academic education, focused on the optimization of ML model performance in the lab. In this paper, we present a project-based learning approach to teaching MLOps, focused on the demonstration and experience with emerging practices and tools to automatize the construction of ML-enabled components. We examine the design of a course based on this approach, including laboratory sessions that cover the end-to-end ML component life cycle, from model building to production deployment. Moreover, we report on preliminary results from the first edition of the course. During the present year, an updated version of the same course is being delivered in two independent universities; the related learning outcomes will be evaluated to analyze the effectiveness of project-based learning for this specific subject.",,,,,,,,,,,,,,,,,,,,,,,,,,machine learning; data science; model deployment; reproducibility; software engineering for AI,0.0,EC3,3053
10.1109/ICSE-SEIP58684.2023.00038,0,0,0,0,0,113,"Sampaio, Adalberto R.; Beschastnikh, Ivan; Maier, Daryl; Bourne, Don; Sundaresen, Vijay",Auto-Tuning Elastic Applications in Production,Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice,979-8-3503-0037-6,,https://doi.org/10.1109/ICSE-SEIP58684.2023.00038,2023,ICSE-SEIP '23,,,,IEEE Press,"Melbourne, Australia","Modern cloud applications must be tuned for high performance. Yet, a single static configuration is insufficient since a cloud application must deal with changes in workload, varying numbers of replicas due to auto-scaling, and upgrades to the environment and the application code itself. These dynamics can only be observed altogether during the application execution and affects different layers of the application stack. In this paper, we describe SmartTuning, a technique and tool to auto-tune cloud applications on the fly, improving resource utilization and performance under dynamic workloads.SmartTuning reacts to different workloads over time and automatically explores and adapts the application's configuration through Bayesian Optimization. SmartTuning searches for configurations that better use resources when the application is subject to auto-scaling and dynamic workloads. It minimizes the need for the operations team to instrument code or manually try out configurations in testing environments. Our evaluation of three industrial applications indicates that SmartTuning can, on average, improve application efficiency by 58% and reduce cost by 27%.",,,,,,,,,,,,,,,,,,,,,,,,,,microservices; kubernetes; auto-tuning; elastic cloud environments,0.0,EC3,2822
10.1109/ISCA.2018.00012,0,0,0,0,0,251,"Fowers, Jeremy; Ovtcharov, Kalin; Papamichael, Michael; Massengill, Todd; Liu, Ming; Lo, Daniel; Alkalay, Shlomi; Haselman, Michael; Adams, Logan; Ghandi, Mahdi; Heil, Stephen; Patel, Prerak; Sapek, Adam; Weisz, Gabriel; Woods, Lisa; Lanka, Sitaram; Reinhardt, Steven K.; Caulfield, Adrian M.; Chung, Eric S.; Burger, Doug",A configurable cloud-scale DNN processor for real-time AI,Proceedings of the 45th Annual International Symposium on Computer Architecture,978-1-5386-5984-7,,https://doi.org/10.1109/ISCA.2018.00012,2018,ISCA '18,,,,IEEE Press,"Los Angeles, California","Interactive AI-powered services require low-latency evaluation of deep neural network (DNN) models—aka ""realtime AI"". The growing demand for computationally expensive, state-of-the-art DNNs, coupled with diminishing performance gains of general-purpose architectures, has fueled an explosion of specialized Neural Processing Units (NPUs). NPUs for interactive services should satisfy two requirements: (1) execution of DNN models with low latency, high throughput, and high efficiency, and (2) flexibility to accommodate evolving state-of-the-art models (e.g., RNNs, CNNs, MLPs) without costly silicon updates.This paper describes the NPU architecture for Project Brainwave, a production-scale system for real-time AI. The Brainwave NPU achieves more than an order of magnitude improvement in latency and throughput over state-of-the-art GPUs on large RNNs at a batch size of 1. The NPU attains this performance using a single-threaded SIMD ISA paired with a distributed microarchitecture capable of dispatching over 7M operations from a single instruction. The spatially distributed microarchitecture, scaled up to 96,000 multiply-accumulate units, is supported by hierarchical instruction decoders and schedulers coupled with thousands of independently addressable high-bandwidth on-chip memories, and can transparently exploit many levels of fine-grain SIMD parallelism. When targeting an FPGA, microarchitectural parameters such as native datapaths and numerical precision can be ""synthesis specialized"" to models at compile time, enabling high FPGA performance competitive with hardened NPUs. When running on an Intel Stratix 10 280 FPGA, the Brainwave NPU achieves performance ranging from ten to over thirty-five teraflops, with no batching, on large, memory-intensive RNNs.",,,,,,,,,,,,,,,,,,,,,,,,,,accelerator architectures; field programmable gate arrays; neural network hardware,0.0,EC3,2904
10.1109/ISCA45697.2020.00014,0,0,0,0,0,273,"Adiga, Narasimha; Bonanno, James; Collura, Adam; Heizmann, Matthias; Prasky, Brian R.; Saporito, Anthony",The IBM zl5 high frequency mainframe branch predictor,Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture,978-1-7281-4661-4,,https://doi.org/10.1109/ISCA45697.2020.00014,2020,ISCA '20,,,,IEEE Press,Virtual Event,"The design of the modern, enterprise-class IBM z15 branch predictor is described. Implemented as a multi-level look-ahead structure, the branch predictor is capable of predicting branch direction and target addresses, augmented with multiple auxiliary direction, target, and power predictors. Predictions are made asynchronously, and later integrated into the processor pipeline. The design is optimized for the unique workloads executed on these enterprise-class systems, including compute intensive and both large instruction and data footprint workloads. This paper highlights the major operations and functions of the IBM z15 branch predictor, including its pipeline, prediction structures and verification methodology. Explanations as to how the design matured to its current state are also provided.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,713
10.1109/ISCA45697.2020.00084,0,0,0,0,0,237,"Gupta, Udit; Hsia, Samuel; Saraph, Vikram; Wang, Xiaodong; Reagen, Brandon; Wei, Gu-Yeon; Lee, Hsien-Hsin S.; Brooks, David; Wu, Carole-Jean",DeepRecSys: a system for optimizing end-to-end at-scale neural recommendation inference,Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture,978-1-7281-4661-4,,https://doi.org/10.1109/ISCA45697.2020.00084,2020,ISCA '20,,,,IEEE Press,Virtual Event,"Neural personalized recommendation is the cornerstone of a wide collection of cloud services and products, constituting significant compute demand of cloud infrastructure. Thus, improving the execution efficiency of recommendation directly translates into infrastructure capacity saving. In this paper, we propose DeepRecSched, a recommendation inference scheduler that maximizes latency-bounded throughput by taking into account characteristics of inference query size and arrival patterns, model architectures, and underlying hardware systems. By carefully optimizing task versus data-level parallelism, DeepRecSched improves system throughput on server class CPUs by 2x across eight industry-representative models. Next, we deploy and evaluate this optimization in an at-scale production datacenter which reduces end-to-end tail latency across a wide variety of recommendation models by 30%. Finally, DeepRecSched demonstrates the role and impact of specialized AI hardware in optimizing system level performance (QPS) and power efficiency (QPS/watt) of recommendation inference.In order to enable the design space exploration of customized recommendation systems shown in this paper, we design and validate an end-to-end modeling infrastructure, DeepRecInfra. DeepRecInfra enables studies over a variety of recommendation use cases, taking into account at-scale effects, such as query arrival patterns and recommendation query sizes, observed from a production datacenter, as well as industry-representative models and tail latency targets.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,EC3,2430
10.1109/MICRO56248.2022.00040,1,1,1,1,1,15,"Khairy, Mahmoud; Alawneh, Ahmad; Barnes, Aaron; Rogers, Timothy G.",SIMR: Single Instruction Multiple Request Processing for Energy-Efficient Data Center Microservices,Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture,978-1-6654-6272-3,,https://doi.org/10.1109/MICRO56248.2022.00040,2023,MICRO '22,,,,IEEE Press,"Chicago, Illinois, USA","Contemporary data center servers process thousands of similar, independent requests per minute. In the interest of programmer productivity and ease of scaling, workloads in data centers have shifted from single monolithic processes toward a micro and nanoservice software architecture. As a result, single servers are now packed with many threads executing the same, relatively small task on different data.State-of-the-art data centers run these microservices on multi-core CPUs. However, the flexibility offered by traditional CPUs comes at an energy-efficiency cost. The Multiple Instruction Multiple Data execution model misses opportunities to aggregate the similarity in contemporary microservices. We observe that the Single Instruction Multiple Thread execution model, employed by GPUs, provides better thread scaling and has the potential to reduce frontend and memory system energy consumption. However, contemporary GPUs are ill-suited for the latency-sensitive microservice space.To exploit the similarity in contemporary microservices, while maintaining acceptable latency, we propose the Request Processing Unit (RPU). The RPU combines elements of out-of-order CPUs with lockstep thread aggregation mechanisms found in GPUs to execute microservices in a Single Instruction Multiple Request (SIMR) fashion. To complement the RPU, we also propose a SIMR-aware software stack that uses novel mechanisms to batch requests based on their predicted control-flow, split batches based on predicted latency divergence and map per-request memory allocations to maximize coalescing opportunities. Our resulting RPU system processes 5.7× more requests/joule than multi-core CPUs, while increasing single thread latency by only 1.44×.",,,,,,,,,,,,,,,,,,,,,,,,,,data center; GPU; microservices; SIMT,0.0,IC4,87
10.1109/MICRO56248.2022.00063,1,1,1,0,0,73,"Yahya, Jawad Haj; Volos, Haris; Bartolini, Davide B.; Antoniou, Georgia; Kim, Jeremie S.; Wang, Zhe; Kalaitzidis, Kleovoulos; Rollet, Tom; Chen, Zhirui; Geng, Ye; Mutlu, Onur; Sazeides, Yiannakis",AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for Latency-Sensitive Server Applications,Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture,978-1-6654-6272-3,,https://doi.org/10.1109/MICRO56248.2022.00063,2023,MICRO '22,,,,IEEE Press,"Chicago, Illinois, USA","User-facing applications running in modern datacenters exhibit irregular request patterns and are implemented using a multitude of services with tight latency requirements (30–250μs). These characteristics render existing energy-conserving techniques ineffective when processors are idle due to the long transition time (order of 100μs) from a deep CPU core idle power state (C-state). While prior works propose management techniques to mitigate this inefficiency, we tackle it at its root with AgileWatts (AW): a new deep CPU core C-state architecture optimized for datacenter server processors targeting latency-sensitive applications.AW drastically reduces the transition latency from deep CPU core idle power states while retaining most of their power savings based on three key ideas. First, AW eliminates the latency (several microseconds) of saving/restoring the core context when powering-off/-on the core in a deep idle state by i) implementing medium-grained power-gates, carefully distributed across the CPU core, and ii) retaining context in the power-ungated domain. Second, AW eliminates the flush latency (several tens of microseconds) of the L1/L2 caches when entering a deep idle state by keeping L1/L2 content power-ungated. A small control logic also remains ungated to serve cache coherence traffic. AW implements cache sleep-mode and leakage reduction for the power-ungated domain by lowering a core's voltage to the minimum operational level. Third, using a state-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock generator, AW keeps the PLL active and locked during the idle state, cutting microseconds of wake-up latency at negligible power cost.Our evaluation with an accurate industrial-grade simulator calibrated against an Intel Skylake server shows that AW reduces the energy consumption of Memcached by up to 71% (35% on average) with &lt;1% end-to-end performance degradation. We observe similar trends for other evaluated services (MySQL and Kafka). AW's new deep C-states C6A and C6AE reduce transition-time by up to 900× as compared to the deepest existing idle state C6, while consuming only 7% and 5% of the active state (C0) power, respectively.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,EC3,2321
10.1109/MICRO56248.2022.00065,1,1,1,0,1,48,"Antoniou, Georgia; Volos, Haris; Bartolini, Davide B.; Rollet, Tom; Sazeides, Yiannakis; Yahya, Jawad Haj",AgilePkgC: An Agile System Idle State Architecture for Energy Proportional Datacenter Servers,Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture,978-1-6654-6272-3,,https://doi.org/10.1109/MICRO56248.2022.00065,2023,MICRO '22,,,,IEEE Press,"Chicago, Illinois, USA","Modern user-facing applications deployed in data-centers use a distributed system architecture that exacerbates the latency requirements of their constituent microservices (30–250μs). Existing CPU power-saving techniques degrade the performance of these applications due to the long transition latency (order of 100μs) to wake up from a deep CPU idle state (C-state). For this reason, server vendors recommend only enabling shallow core C-states (e.g., CC1) for idle CPU cores, thus preventing the system from entering deep package C-states (e.g., PC6) when all CPU cores are idle. This choice, however, impairs server energy proportionality since power-hungry resources (e.g., IOs, uncore, DRAM) remain active even when there is no active core to use them. As we show, it is common for all cores to be idle due to the low average utilization (e.g., 5 – 20%) of datacenter servers running user-facing applications.We propose to reap this opportunity with AgilePkgC (APC), a new package C-state architecture that improves the energy proportionality of server processors running latency-critical applications. APC implements PC1A (package C1 agile), a new deep package C-state that a system can enter once all cores are in a shallow C-state (i.e., CC1) and has a nanosecond-scale transition latency. PC1A is based on four key techniques. First, a hardware-based agile power management unit (APMU) rapidly detects when all cores enter a shallow core C-state (CC1) and triggers the system-level power savings control flow. Second, an IO Standby Mode (IOSM) places IO interfaces (e.g., PCIe, DMI, UPI, DRAM) in shallow (nanosecond-scale transition latency) low-power modes. Third, a CLM Retention (CLMR) mode rapidly reduces the CLM (Cache-and-home-agent, Last-level-cache, and Mesh network-on-chip) domain's voltage to its retention level, drastically reducing its power consumption. Fourth, APC keeps all system PLLs active in PC1A to allow nanosecond-scale exit latency by avoiding PLL re-locking overhead.Combining these techniques enables significant power savings while requiring less than 200ns transition latency, &gt;250× faster than existing deep package C-states (e.g., PC6), making PC1A practical for datacenter servers. Our evaluation based on an Intel Skylake-based server shows that APC reduces the energy consumption of Memcached by up to 41% (25% on average) with &lt;0.1% performance degradation. APC provides similar benefits for other representative workloads.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,IC4,121
10.1109/SC41406.2024.00103,0,0,0,0,0,19,"Ghosh, Anyesha; Yadwadkar, Neeraja J.; Erez, Mattan",Fast and Efficient Scaling for Microservices with SurgeGuard,"Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis",979-8-3503-5291-7,,https://doi.org/10.1109/SC41406.2024.00103,2024,SC '24,,,,IEEE Press,"Atlanta, GA, USA","The microservice architecture is increasingly popular for flexible, large-scale online applications. However, existing resource management mechanisms incur high latency in detecting Quality of Service (QoS) violations, and hence, fail to allocate resources effectively under commonly-observed varying load conditions. This results in over-allocation coupled with a late response that increase both the total cost of ownership and the magnitude of each QoS violation event. We present SurgeGuard, a decentralized resource controller for microservice applications specifically designed to guard application QoS during surges in load and network latency. SurgeGuard uses the key insight that for rapid detection and effective management of QoS violations, the controller must be aware of any available slack in latency and communication patterns between microservices within a task-graph. Our experiments show that for the workloads in DeathStarBench, SurgeGuard on average reduces the combined violation magnitude and duration by 61.1%% and 93.7%%, respectively, compared to the well-known Parties and Caladan algorithms, and requires 8% fewer resources than Parties.",,,,,,,,,,,,,,,,,,,,,,,,,,Cloud computing; microservices; datacenters; quality-of-service; resource management; serverless,0.0,EC3,2208
10.1109/SCW63240.2024.00071,0,0,0,0,0,260,"Pan, Haochen; Chard, Ryan; Zhou, Sicheng; Kamatar, Alok; Vescovi, Rafael; Hayot-Sasson, Valérie; Bauer, André; Gonthier, Maxime; Chard, Kyle; Foster, Ian",Octopus: Experiences with a Hybrid Event-Driven Architecture for Distributed Scientific Computing,"Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis",979-8-3503-5554-3,,https://doi.org/10.1109/SCW63240.2024.00071,2025,SC-W '24,,,,IEEE Press,"Atlanta, GA, USA","Scientific research increasingly relies on distributed computational resources, storage systems, networks, and instruments, ranging from HPC and cloud systems to edge devices. Event-driven architecture (EDA) benefits applications targeting distributed research infrastructures by enabling the organization, communication, processing, reliability, and security of events generated from many sources. To support the development of scientific EDA, we introduce Octopus, a hybrid, cloud-to-edge event fabric designed to link many local event producers and consumers with cloud-hosted brokers, and to provide a fabric for developing resilient applications. Octopus can be scaled to meet demand, permits the deployment of highly available Triggers for automatic event processing, and enforces fine-grained access control. We identify requirements in self-driving laboratories, scientific data automation, online task scheduling, epidemic modeling, and dynamic workflow management use cases, and present results demonstrating Octopus' ability to meet those requirements. Octopus supports producing and consuming events at a rate of over 4.2 M and 9.6 M events per second, respectively, from distributed clients.",,,,,,,,,,,,,,,,,,,,,,,,,,Event-driven architecture; research automation; scientific computing,,,499
10.1109/SCW63240.2024.00197,0,0,0,0,0,126,"Shilpika; Lusch, Bethany; Vishwanath, Venkatram; Papka, Michael E.","An Incremental Multi-Level, Multi-Scale Approach to Assessment of Multifidelity HPC Systems","Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis",979-8-3503-5554-3,,https://doi.org/10.1109/SCW63240.2024.00197,2025,SC-W '24,,,,IEEE Press,"Atlanta, GA, USA","With the growing complexity in architecture and the size of large-scale computing systems, monitoring and analyzing system behavior and events has become daunting. Monitoring data amounting to terabytes per day are collected by sensors housed in these massive systems at multiple fidelity levels and varying temporal resolutions. In this work, we develop an incremental version of multiresolution dynamic mode decomposition (mrDMD), which converts high-dimensional data to spatial-temporal patterns at varied frequency ranges. Our incremental implementation of the mrDMD algorithm (I-mrDMD) promptly reveals valuable information in the massive environment log dataset, which is then visually aligned with the processed hardware and job log datasets through our generalizable rack visualization using D3 visualization integrated into the Jupyter Notebook interface. We demonstrate the efficacy of our approach with two use scenarios on a real-world dataset from a Cray XC40 supercomputer, Theta.",,,,,,,,,,,,,,,,,,,,,,,,,,scalability; Dynamic mode decomposition; high performance computing; incremental analysis; mrDMD; online analysis; visualization,1.0,,1634
10.1109/SERP4IoT.2019.00009,0,0,0,0,0,230,"Lepekhin, Aleksandr; Borremans, Alexandra; Ilin, Igor; Jantunen, Sami",A systematic mapping study on internet of things challenges,Proceedings of the 1st International Workshop on Software Engineering Research &amp; Practices for the Internet of Things,,,https://doi.org/10.1109/SERP4IoT.2019.00009,2019,SERP4IoT '19,,,,IEEE Press,"Montreal, Quebec, Canada","The challenge of developing IoT-based systems has been found to be a complex problem It is influenced by number of factors: heterogeneous devices/resources, various perception-action cycles and widely distributed devices and computing resources. Increasing complexity and immaturity to deal with it have resulted in growing range of problems and challenges in IoT development. This paper identifies essential IoT-related challenges by conducting a systematic mapping study of existing IoT literature. To this end, we distil information with respect to IoT-related: 1) challenges, 2) experimental studies, and 3) recommendations for future research. We then discuss our findings in order to understand better the general state of IoT research, potential gaps in research, and implications for future research.",,,,,,,,,,,,,,,,,,,,,,,,,,IoT; internet of things; IoT challenges; IoT development; systematic mapping study,0.0,EC3,3106
10.1109/TNET.2019.2958367,0,0,0,0,0,319,"Pham, Minh; Hoang, Doan B.; Chaczko, Zenon",Congestion-Aware and Energy-Aware Virtual Network Embedding,IEEE/ACM Trans. Netw.,,1063-6692,https://doi.org/10.1109/TNET.2019.2958367,2020-02,,,,,,,"Network virtualization is an inherent component of future internet architectures. Network resources are virtualized from the underlying substrate and elastically provisioned and offered to customers on-demand. Optimal allocation of network resources in terms of utilization, quality of service, and energy consumption has been a challenge. Existing solutions consider congestion control in a single-objective virtual network embedding (VNE) problem. This paper defines a multiple-objective VNE problem called the congestion-aware, energy-aware VNE (CEVNE). The aim is to seek a solution that saves cost, saves energy and avoids network congestion simultaneously. CEVNE modelling techniques and solution approaches apply both the weighting method and the constraint method to search for pareto-optimal solutions that produce the best compromised solutions for all three objectives. Solving VNE problem is, however, NP-hard. A heuristic solution is proposed involving a two-stage coordinated CEVNE. The node-mapping algorithm searches for the sub-optimal solutions for three objectives. The link mapping process is an SDN-based heuristic algorithm that deploys a path service and a resource monitoring application on an SDN controller. The solution is realized using SDN, Segment Routing, and open network operating system platform (ONOS) technologies. The energy minimization is implemented with a registry that keeps track of active nodes and sets inactive nodes to sleep mode. The evaluation results showed that the multiple-objective CEVNE approach is feasible and achieves its goals of optimizing the resource allocation, improving the runtime, saving the energy consumption and controlling the network congestion.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,EC3,2007
10.1109/TNET.2020.3019098,0,0,0,0,0,423,"Luu, Quang-Trung; Kerboeuf, Sylvaine; Mouradian, Alexandre; Kieffer, Michel",A Coverage-Aware Resource Provisioning Method for Network Slicing,IEEE/ACM Trans. Netw.,,1063-6692,https://doi.org/10.1109/TNET.2020.3019098,2020-12,,,,,,,"With network slicing in 5G networks, Mobile Network Operators can create various slices for Service Providers (SPs) to accommodate customized services. Usually, the various Service Function Chains (SFCs) belonging to a slice are deployed on a best-effort basis. Nothing ensures that the Infrastructure Provider (InP) will be able to allocate enough resources to cope with the increasing demands of some SP. Moreover, in many situations, slices have to be deployed over some geographical area: coverage as well as minimum per-user rate constraints have then to be taken into account. This paper takes the InP perspective and proposes a slice resource &lt;italic&gt;provisioning&lt;/italic&gt; approach to cope with multiple slice demands in terms of computing, storage, coverage, and rate constraints. The resource requirements of the various SFCs within a slice are aggregated within a graph of Slice Resource Demands (SRD). Infrastructure nodes and links have then to be provisioned so as to satisfy all SRDs. This problem leads to a Mixed Integer Linear Programming formulation. A two-step approach is considered, with several variants, depending on whether the constraints of each slice to be provisioned are taken into account sequentially or jointly. Once provisioning has been performed, any slice deployment strategy may be considered on the reduced-size infrastructure graph on which resources have been provisioned. Simulation results demonstrate the effectiveness of the proposed approach compared to a more classical direct slice embedding approach.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,344
10.1109/TNET.2023.3324704,0,0,0,0,0,361,"Li, Jing; Guo, Song; Liang, Weifa; Wang, Jianping; Chen, Quan; Xu, Zichuan; Xu, Wenzheng",AoI-Aware User Service Satisfaction Enhancement in Digital Twin-Empowered Edge Computing,IEEE/ACM Trans. Netw.,,1063-6692,https://doi.org/10.1109/TNET.2023.3324704,2023-10,,,,,,,"The emerging digital twin technique enhances the network management efficiency and provides comprehensive insights on network performance, through mapping physical objects to their digital twins. The user satisfaction on digital twin-enabled service relies on the freshness of digital twin data, which is measured by the Age of Information (AoI). Due to long service delays, the use of the remote cloud for delay-sensitive service provisioning faces serious challenges. Mobile Edge Computing (MEC), as an ideal paradigm for delay-sensitive services, is able to realize real-time data communication between physical objects and their digital twins at the network edge. However, the mobility of physical objects and dynamics of user query arrivals make seamless service provisioning in MEC become challenging. In this paper, we investigate dynamic digital twin placements for improving user service satisfaction in MEC environments, by introducing a novel metric to measure user service satisfaction based on the AoI concept and formulating two user service satisfaction enhancement problems: the static and dynamic utility maximization problems under static and dynamic digital twin placement schemes. To this end, we first formulate an Integer Linear Programming (ILP) solution to the static utility maximization problem when the problem size is small; otherwise, we propose a performance-guaranteed approximation algorithm. We then propose an online algorithm with a provable competitive ratio for the dynamic utility maximization problem, by considering dynamic user query services. Finally, we evaluate the performance of the proposed algorithms via simulations. Simulation results demonstrate that the proposed algorithms outperform the comparison baseline algorithms, improving the algorithm performance by at least 10.7%, compared to the baseline algorithms.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,EC3,2357
10.1109/TNET.2024.3400953,0,0,0,0,0,310,"Wang, Ziliang; Zhu, Shiyi; Li, Jianguo; Jiang, Wei; Ramakrishnan, K. K.; Yan, Meng; Zhang, Xiaohong; Liu, Alex X.",DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems,IEEE/ACM Trans. Netw.,,1063-6692,https://doi.org/10.1109/TNET.2024.3400953,2024-05,,,,,,,"Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,EC3,2413
10.1145/2742854.2747279,0,0,0,0,0,258,"Salapura, Valentina; Beaty, Kirk A.; Bivens, Alan; Kim, Minkyong; Li, Min",Towards building an analytics platform in the cloud,Proceedings of the 12th ACM International Conference on Computing Frontiers,978-1-4503-3358-0,,https://doi.org/10.1145/2742854.2747279,2015,CF '15,,,,Association for Computing Machinery,"New York, NY, USA","Recently enterprises have been able to leverage two revolutionary new tools for gaining a competitive advantage for their business – cloud computing and analytic applications. Cloud computing unburdens them from running and maintaining their compute resources, whereas analytic applications comb through their big data to provide new insights for a competitive advantage in the market. Analytic applications are carefully tailored to their target problems. While there is a lot of work published on both the mechanics of cloud computing as well as analytic methods for distilling insights from a variety of data, there is little work available about the cloud influence on the analytics platforms which aim at lowering the barrier for the creation, deployment, scaling and maintenance of next generation analytic workloads. This paper discusses the challenges we are facing today in order to provide an analytics platform to reduce cost and increase performance of analytics applications in the cloud computing environment.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; agile cluster provisioning; analytics application; analytics platform; cluster sizing; in memory computing; MapReduce; spark,0.0,EC3,2988
10.1145/2786805.2786826,0,0,0,0,0,303,"Cito, Jürgen; Leitner, Philipp; Fritz, Thomas; Gall, Harald C.",The making of cloud applications: an empirical study on software development for the cloud,Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,978-1-4503-3675-8,,https://doi.org/10.1145/2786805.2786826,2015,ESEC/FSE 2015,,,,Association for Computing Machinery,"New York, NY, USA","Cloud computing is gaining more and more traction as a deployment and provisioning model for software. While a large body of research already covers how to optimally operate a cloud system, we still lack insights into how professional software engineers actually use clouds, and how the cloud impacts development practices. This paper reports on the first systematic study on how software developers build applications for the cloud. We conducted a mixed-method study, consisting of qualitative interviews of 25 professional developers and a quantitative survey with 294 responses. Our results show that adopting the cloud has a profound impact throughout the software development process, as well as on how developers utilize tools and data in their daily work. Among other things, we found that (1) developers need better means to anticipate runtime problems and rigorously define metrics for improved fault localization and (2) the cloud offers an abundance of operational data, however, developers still often rely on their experience and intuition rather than utilizing metrics. From our findings, we extracted a set of guidelines for cloud development and identified challenges for researchers and tool vendors.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; user study,0.0,EC3,2283
10.1145/2834791.2834794,0,0,0,0,0,268,"Liechti, Olivier; Prévost, Laurent; Delaye, Valentin; Hennebert, Jean; Grivel, Vincent; Rey, Jean-Philippe; Depraz, Jonathan; Sommer, Marc",Enabling reactive cities with the iFLUX middleware,Proceedings of the 6th International Workshop on the Web of Things,978-1-4503-4045-8,,https://doi.org/10.1145/2834791.2834794,2015,WoT '15,,,,Association for Computing Machinery,"New York, NY, USA","This paper presents the iFLUX middleware, designed to provide a lightweight integration solution for Smart City applications. Based on three core abstractions, namely event sources, action targets and rules, iFLUX makes it very easy to expose sensors and actuators through REST APIs so that they can be integrated in application-level workflows. Sensors and actuators can be smart objects integrating hardware and software, but can also be pure software services. In the paper, we introduce the iFLUX programming model and describe how it has been implemented in a middleware platform. We also report on how the platform has been used and evaluated in various contexts. While iFLUX has been initially designed in the context of Smart City applications, it is generic and applicable to other domains where hardware and software components are connected through the Web.",,,,,,,,,,,,,,,,,,,,,,,,,,smart city; integration middleware; rule engine,,,210
10.1145/2897829.2897831,0,0,0,0,0,264,"Reiss, Steven P.; Xin, Qi","Building dynamic, long-running systems",Proceedings of the 4th International Workshop on Software Engineering for Systems-of-Systems,978-1-4503-4172-1,,https://doi.org/10.1145/2897829.2897831,2016,SESoS '16,,,,Association for Computing Machinery,"New York, NY, USA","Complex applications that are effectively systems-of-systems are becoming more common and more useful. Our goal is to devise new ways of architecting such systems that will make their programming easier. We take a component oriented approach. A component's interface, which we call an outerface, includes not only the syntax of the component, but also its semantics and constraints on its use. Implementations of outerfaces are defined separately. Our underlying framework, TAIGA, lets the user code directly to the outerface and automatically finds, validates and binds an appropriate implementation. The framework handles component evolution and failure by detected changes and dynamically revalidating and rebinding possibly new implementations to existing outerfaces while maintaining the running system. We are currently working on extending this framework to handle modern, distributed systems-of-systems.",,,,,,,,,,,,,,,,,,,,,,,,,,distributed systems; evolution; interfaces,,,896
10.1145/2961111.2962597,0,0,0,0,0,292,"Carver, Jeffrey C.; Dieste, Oscar; Kraft, Nicholas A.; Lo, David; Zimmermann, Thomas",How Practitioners Perceive the Relevance of ESEM Research,Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement,978-1-4503-4427-2,,https://doi.org/10.1145/2961111.2962597,2016,ESEM '16,,,,Association for Computing Machinery,"New York, NY, USA","Background: The relevance of ESEM research to industry practitioners is key to the long-term health of the conference. Aims: The goal of this work is to understand how ESEM research is perceived within the practitioner community and provide feedback to the ESEM community ensure our research remains relevant. Method: To understand how practitioners perceive ESEM research, we replicated previous work by sending a survey to several hundred industry practitioners at a number of companies around the world. We asked the survey participants to rate the relevance of the research described in 156 ESEM papers published between 2011 and 2015. Results: We received 9,941 ratings by 437 practitioners who labeled ideas as Essential, Worth-while, Unimportant, or Unwise. The results showed that overall, industrial practitioners find the work published in ESEM to be valuable: 67% of all ratings were essential or worthwhile. We found no correlation between citation count and perceived relevance of the papers. Through a qualitative analysis, we also identified a number of research themes on which practitioners would like to see an increased research focus. Conclusions: The work published in ESEM is generally relevant to industrial practitioners. There are a number of topics for which those practitioners would like to see additional research undertaken.",,,,,,,,,,,,,,,,,,,,,,,,,,Survey; ESEM Conference; Industrial Relevance,,,1250
10.1145/2993236.2993256,0,0,0,0,0,188,"Boussaa, Mohamed; Barais, Olivier; Baudry, Benoit; Sunyé, Gerson",Automatic non-functional testing of code generators families,Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences,978-1-4503-4446-3,,https://doi.org/10.1145/2993236.2993256,2016,GPCE 2016,,,,Association for Computing Machinery,"New York, NY, USA","The intensive use of generative programming techniques provides an elegant engineering solution to deal with the heterogeneity of platforms and technological stacks. The use of domain-specific languages for example, leads to the creation of numerous code generators that automatically translate highlevel system specifications into multi-target executable code. Producing correct and efficient code generator is complex and error-prone. Although software designers provide generally high-level test suites to verify the functional outcome of generated code, it remains challenging and tedious to verify the behavior of produced code in terms of non-functional properties. This paper describes a practical approach based on a runtime monitoring infrastructure to automatically check the potential inefficient code generators. This infrastructure, based on system containers as execution platforms, allows code-generator developers to evaluate the generated code performance. We evaluate our approach by analyzing the performance of Haxe, a popular high-level programming language that involves a set of cross-platform code generators. Experimental results show that our approach is able to detect some performance inconsistencies that reveal real issues in Haxe code generators.",,,,,,,,,,,,,,,,,,,,,,,,,,code generator; code quality; non-functional properties; testing,0.0,EC3,3069
10.1145/2993412.3004852,0,0,0,0,0,142,"Díaz, Jessica; Pérez, Jennifer; Pérez, Jorge; Garbajosa, Juan",Conceptualizing a framework for cyber-physical systems of systems development and deployment,Proccedings of the 10th European Conference on Software Architecture Workshops,978-1-4503-4781-5,,https://doi.org/10.1145/2993412.3004852,2016,ECSAW '16,,,,Association for Computing Machinery,"New York, NY, USA","Cyber-physical systems (CPS) refer to the next generation of embedded ICT systems that are interconnected, collaborative and that provide users and businesses with a wide range of smart applications and services. Software in CPS applications ranges from small systems to large systems, aka. Systems of Systems (SoS), such as smart grids and cities. CPSoS require managing massive amounts of data, being aware of their emerging behavior, and scaling out to progressively evolve and add new systems. Cloud computing supports processing and storing massive amounts of data, hosting and delivering services, and configuring self-provisioned resources. Therefore, cloud computing is the natural candidate to solve CPSoS needs. However, the diversity of platforms and the low-level cloud programming models make difficult to find a common solution for the development and deployment of CPSoS. This paper presents the architectural foundations of a cloud-centric framework for automating the development and deployment of CPSoS service applications to converge towards a common open service platform for CPSoS applications. This framework relies on the well-known qualities of the microservices architecture style, the autonomic computing paradigm, and the model-driven software development approach. Its implementation and validation is on-going at two European and national projects.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; microservices; software architecture; cyber-physical systems; model-driven development,0.0,EC3,2636
10.1145/2996890.3007878,0,0,0,0,0,26,"Carrega, A.; Repetto, M.",Exploiting novel software development paradigms to increase the sustainability of data centers,Proceedings of the 9th International Conference on Utility and Cloud Computing,978-1-4503-4616-0,,https://doi.org/10.1145/2996890.3007878,2016,UCC '16,,,,Association for Computing Machinery,"New York, NY, USA","The application of effective energy management strategies in data centers is often hindered by the substantial conflict between the interests of cloud users and infrastructure owners. As a matter of fact, cloud users require that the service level they are paying for is tightly met, whereas data center owners try to cut down their operational expenses.In this paper, we propose a novel consolidation algorithm that exploits emerging software development paradigms. Our approach enables cloud users to indicate their willingness to apply energy saving mechanisms to some of their virtual resources, hence giving infrastructure managers the ability to apply more efficient workload consolidation and to switch their hardware to very low-power states. The result is an optimal trade-off between energy consumption and performance.",,,,,,,,,,,,,,,,,,,,,,,,,,energy efficiency; consolidation algorithms; green data centers; software development paradigms,0.0,EC3,1943
10.1145/3018896.3025168,0,0,0,0,0,218,"Cloete, A. H.; Booysen, M. J.; Sandell, R. C.; van der Merwe, A. B.",Smart electric water heaters: a system architecture proposal for scalable IoT,"Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing",978-1-4503-4774-7,,https://doi.org/10.1145/3018896.3025168,2017,ICC '17,,,,Association for Computing Machinery,"New York, NY, USA","Energy and water are scarce resources in South Africa, and the management thereof is receiving increased attention. One rare place where energy and water coincide is in water heaters, which consume over 30% of household energy in South Africa and collectively pass over 500 billion litres of water annually. These resource-hungry devices are not well managed and tend to be physically inaccessible and difficult to understand for users. The advent of wireless and cloud computing technology brought expectations of simple management of a multitude of devices and the so-called Internet of Things. A few solutions have been proposed for remote water heater management, but these are not scalable, and have not been benchmarked for scalability above tens of units. This paper proposes an end-to-end architecture to monitor and control EWHs based on MQTT, and specific emphasis is placed on the technologies suitable for large-scale cross-domain interoperability. The performance of the system with 10,000 connections and the results of a pilot deployment are presented.",,,,,,,,,,,,,,,,,,,,,,,,,,internet of things; smart cities; electric water heaters,0.0,EC3,3132
10.1145/3030207.3030245,0,0,0,0,0,280,"Mohamed, Saleh; Forshaw, Matthew; Thomas, Nigel; Dinn, Andrew",Performance and Dependability Evaluation of Distributed Event-based Systems: A Dynamic Code-injection Approach,Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering,978-1-4503-4404-3,,https://doi.org/10.1145/3030207.3030245,2017,ICPE '17,,,,Association for Computing Machinery,"New York, NY, USA","Distributed stream processing and event-based systems are an increasingly critical component in contemporary large-scale data processing applications, and are often subject to strict latency and reliability requirements. However, to achieve scalability demands, they are often deployed on distributed clusters of heterogeneous nodes, causing unpredictable runtime performance and complex fault characteristics.The behaviour of these systems is poorly understood, and existing performance and dependability evaluation techniques are ill-equipped to handle the challenges introduced by the complex and distributed nature of event-based systems.We develop a dynamic code-injection approach to evaluate the performance and dependability of stream processing and event-based systems. Our approach supports fine-grained instrumentation of applications and their runtime infrastructure, and the dynamic injection of code mutations and faults into a production system at runtime. We demonstrate the proposed approach by performing instrumentation and code injection on a distributed Apache Spark cluster.",,,,,,,,,,,,,,,,,,,,,,,,,,performance; dependability; fault injection; event-based systems,0.0,EC3,2569
10.1145/3050748.3050758,0,0,0,0,0,132,"Nitu, Vlad; Olivier, Pierre; Tchana, Alain; Chiba, Daniel; Barbalace, Antonio; Hagimont, Daniel; Ravindran, Binoy",Swift Birth and Quick Death: Enabling Fast Parallel Guest Boot and Destruction in the Xen Hypervisor,Proceedings of the 13th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments,978-1-4503-4948-2,,https://doi.org/10.1145/3050748.3050758,2017,VEE '17,,,,Association for Computing Machinery,"New York, NY, USA","The ability to quickly set up and tear down a virtual machine is critical for today's cloud elasticity, as well as in numerous other scenarios: guest migration/consolidation, event-driven invocation of micro-services, dynamically adaptive unikernel-based applications, micro-reboots for security or stability, etc.In this paper, we focus on the process of setting up/freeing the hypervisor and host control layer data structures at boot/destruction time, showing that it does not scale in current virtualization solutions. In addition to the direct overhead of long VM set-up/destruction times, we demonstrate by experimentation the indirect costs on real world auto scaling systems. Focusing on the popular Xen hypervisor, we identify three critical issues hindering the scalability of the boot and destruction processes: serialized boot, unscalable interactions with the Xenstore at guest creation time, and remote NUMA memory scrubbing at destruction time. For each of these issues we present the design and implementation of a solution in the Xen infrastructure: parallel boot with fine-grained locking, caching of Xenstore data, and local NUMA scrubbing. We evaluate these solutions using micro-benchmarks, macro-benchmarks, and real world datacenter traces. Results show that our work improves the current Xen implementation by a significant factor, for example macro-benchmarks indicate a speedup of more than 4X in high-load scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,Cloud Computing; Elasticity; Scalability; Virtualization,,,298
10.1145/3077839.3081675,0,0,0,0,0,239,"Fiebig, Florian; Kochanneck, Sebastian; Mauser, Ingo; Schmeck, Hartmut",Detecting Occupancy in Smart Buildings by Data Fusion from Low-cost Sensors: Poster Description,Proceedings of the Eighth International Conference on Future Energy Systems,978-1-4503-5036-5,,https://doi.org/10.1145/3077839.3081675,2017,e-Energy '17,,,,Association for Computing Machinery,"New York, NY, USA",This poster presents ongoing work on an occupancy detection system based on data fusion of various low-cost sensors within a real smart building and compares the results of different classification approaches.,,,,,,,,,,,,,,,,,,,,,,,,,,Information Fusion; Occupancy Detection; Smart Building,,,694
10.1145/3084100.3084107,0,0,0,0,0,294,"Rosenberg, Doug; Boehm, Barry; Wang, Bo; Qi, Kan","Rapid, evolutionary, reliable, scalable system and software development: the resilient agile process",Proceedings of the 2017 International Conference on Software and System Process,978-1-4503-5270-3,,https://doi.org/10.1145/3084100.3084107,2017,ICSSP '17,,,,Association for Computing Machinery,"New York, NY, USA","The increasing pace of change in competition, technology, and complexity of software-intensive systems has increased the demand for rapid, reliable, scalable, and evolvable processes. Agile methods have made significant contributions to speeding up software development, but often encounter problems with reliability, scalability, and evolvability. Over the past 3 years, we have been experimenting with an approach called Resilient Agile (RA), which addresses these problems while also speeding up development by finding enablers for parallel systems engineering, development, and test. This paper summarizes our experience in defining and evolving RA by applying it to three representative emergent-technology applications: Location-Based Advertising, Picture Sharing, and Bad Driver Reporting. In comparison with the mainstream Architected Agile process that we had been using on similar systems, the RA process achieved fewer defects and significant speedups in system development and evolution. The paper summarizes the overall challenge of software schedule compression; identifies managed parallel development as generally the most powerful but least-practiced strategy for schedule compression; summarizes the key elements required to support parallelism, including specific model-driven system development techniques, automatic generation of key elements and realistic schedule and effort estimation. It then summarizes the three successful Resilient Agile projects to date, provides criteria for selecting a Resilient Agile process, and summarizes the key techniques for scaling up Resilient Agile, using a previous million-line command and control project as an example.",,,,,,,,,,,,,,,,,,,,,,,,,,microservice architecture; code generation; agile development; ICSM; MVC; NoSQL; parallel development; rapid delivery; resilient software; REST; scalable software development; schedule compression; UML modeling; use case driven development,,,905
10.1145/3093742.3093928,0,0,0,0,0,236,"Antonić, Aleksandar; Marjanović, Martina; Žarko, Ivana Podnar",Modeling Aggregate Input Load of Interoperable Smart City Services,Proceedings of the 11th ACM International Conference on Distributed and Event-Based Systems,978-1-4503-5065-5,,https://doi.org/10.1145/3093742.3093928,2017,DEBS '17,,,,Association for Computing Machinery,"New York, NY, USA","The Internet of Things (IoT) is expanding and reaching the maturity level beyond initial deployments. An integrative and interoperable IoT platform proves to be a suitable execution environment for Smart City services because users simultaneously use multiple services, while an IoT platform enables cross-service data sharing. A large number of various IoT and mobile devices as well as the corresponding services can generate tremendous input load on an underlying IoT platform. Thus, it is crucial to analyze the overall input rate on Smart City services to ensure predefined quality of service (e.g., low latency required by some IoT services). An aggregate input rate which characterizes a real world deployment can be used to check if a platform is able to adequately support multiple services running in parallel and to evaluate its overall performance.In this paper we review IoT-based Smart City services to identify key applications characterizing the domain, e.g., smart mobility, smart utilities, and citizen-driven mobile crowd sensing services. Next, we analyze the potential load which such applications pose on IoT services that continuously process the generated data streams. The analysis is used to create a model estimating an aggregate load generated by Smart City applications. We simulate a number of characteristic application compositions to provide insight about the aggregate input load and its potential impact on the performance of Smart City services. The proposed model is a first step towards predicting the processing load of Smart City services to facilitate the assessment and planning of required resources for continuous processing of sensor data in the context of Smart City services.",,,,,,,,,,,,,,,,,,,,,,,,,,Internet of Things; input load; model; Smart City services,0.0,EC3,2622
10.1145/3093899,0,0,0,0,0,370,"Liu, Xuanzhe; Yu, Meihua; Ma, Yun; Huang, Gang; Mei, Hong; Liu, Yunxin",i-Jacob: An Internetware-Oriented Approach to Optimizing Computation-Intensive Mobile Web Browsing,ACM Trans. Internet Technol.,,1533-5399,https://doi.org/10.1145/3093899,2018-03,,,,,,,"Web browsing is always a key requirement of Internet users. Current mobile Web apps can contain computation-intensive JavaScript logics and thus affect browsing performance. Learning from our over-decade research and development experiences of the Internetware paradigm, we present the novel and generic i-Jacob approach to improving the performance of mobile Web browsing with effective JavaScript-code offloading. Our approach proposes a programming abstraction to make mobile Web situational and adaptive to contexts, by specifying the computation-intensive and “ offloadable ” code, and develops a platform-independent lightweight runtime spanning the mobile devices and the cloud. We demonstrate the efficiency of i-Jacob with some typical computation-intensive tasks over various combinations of hardware, operating systems, browsers, and network connections. The improvements can reach up to 49× speed-up in response time and 90% saving in energy.",,,,,,,,,,,,,,,,,,,,,,,,,,performance; energy; JavaScript offloading; Mobile Web,0.0,EC3,3034
10.1145/3110355.3110359,1,1,1,1,0,221,"Tsiachri Renta, Pelagia; Sotiriadis, Stelios; Petrakis, Euripides G.M.",Healthcare Sensor Data Management on the Cloud,Proceedings of the 2017 Workshop on Adaptive Resource Management and Scheduling for Cloud Computing,978-1-4503-5116-4,,https://doi.org/10.1145/3110355.3110359,2017,ARMS-CC '17,,,,Association for Computing Machinery,"New York, NY, USA","The quality of medical services can be significantly improved by supporting health care procedures with new technologies such as Cloud computing and Internet of Things (IoTs). The need to monitor patient's health remotely and in real time becomes more and more a vital requirement, especially for chronic patients and elderly. In this work, we focus on the management of health care related data stored on the Cloud produced by Bluetooth low energy devices. We present a Cloud based IoT Management System that collects vital user data (e.g. cardiac pulse rate and blood oxygen saturation) on real time. Our solution enables sensor data collection and processing fast and efficient, while users such as medical personnel can subscribe to patient's data and get notifications. The system is designed based on microservices and includes a notification service for both health care providers and patients minimizing the risk of late response to emergency conditions. Alerts are produced according to predefined rules and on patient specific reaction plans. We present an experimental study where we evaluate our system based on real world sensors, while we generate a synthetic dataset for simulating thousands of users. The results are prosperous, as the system responds close to real time even under heavy loads binding to the limits of the web server that receives the service request. The heaviest workload simulates 2000 user requests (while 80 are executed concurrently) is completed in less than 13 seconds when the system deployed in a virtual machine of 2GB RAM, 1 VCPU and 20GB Disk.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; internet of things,0.0,IC4 but not relevant,2304
10.1145/3125719.3125727,0,0,0,0,0,302,"Król, Michał; Psaras, Ioannis",NFaaS: named function as a service,Proceedings of the 4th ACM Conference on Information-Centric Networking,978-1-4503-5122-5,,https://doi.org/10.1145/3125719.3125727,2017,ICN '17,,,,Association for Computing Machinery,"New York, NY, USA","In the past, the Information-centric networking (ICN) community has focused on issues mainly pertaining to traditional content delivery (e.g., routing and forwarding scalability, congestion control and in-network caching). However, to keep up with future Internet architectural trends the wider area of future Internet paradigms, there is a pressing need to support edge/fog computing environments, where cloud functionality is available more proximate to where the data is generated and needs processing.With this goal in mind, we propose Named Function as a Service (NFaaS), a framework that extends the Named Data Networking architecture to support in-network function execution. In contrast to existing works, NFaaSbuilds on very lightweight VMs and allows for dynamic execution of custom code. Functions can be downloaded and run by any node in the network. Functions can move between nodes according to user demand, making resolution of moving functions a first-class challenge. NFaaSincludes a Kernel Store component, which is responsible not only for storing functions, but also for making decisions on which functions to run locally. NFaaSincludes a routing protocol and a number of forwarding strategies to deploy and dynamically migrate functions within the network. We validate our design through extensive simulations, which show that delay-sensitive functions are deployed closer to the edge, while less delay-sensitive ones closer to the core.",,,,,,,,,,,,,,,,,,,,,,,,,,mobile edge computing; function migration; information centric networking; network architectures; networks,0.0,EC3,2280
10.1145/3127479.3132252,0,0,0,0,0,245,"Le, Yanfang; Chang, Hyunseok; Mukherjee, Sarit; Wang, Limin; Akella, Aditya; Swift, Michael M.; Lakshman, T. V.",UNO: uniflying host and smart NIC offload for flexible packet processing,Proceedings of the 2017 Symposium on Cloud Computing,978-1-4503-5028-0,,https://doi.org/10.1145/3127479.3132252,2017,SoCC '17,,,,Association for Computing Machinery,"New York, NY, USA","Increasingly, smart Network Interface Cards (sNICs) are being used in data centers to offload networking functions (NFs) from host processors thereby making these processors available for tenant applications. Modern sNICs have fully programmable, energy-efficient multi-core processors on which many packet processing functions, including a full-blown programmable switch, can run. However, having multiple switch instances deployed across the host hypervisor and the attached sNICs makes controlling them difficult and data plane operations more complex.This paper proposes a generalized SDN-controlled NF offload architecture called UNO. It can transparently offload dynamically selected host processors' packet processing functions to sNICs by using multiple switches in the host while keeping the data centerwide network control and management planes unmodified. UNO exposes a single virtual control plane to the SDN controller and hides dynamic NF offload behind a unified virtual management plane. This enables UNO to make optimal use of host's and sNIC's combined packet processing capabilities with local optimization based on locally observed traffic patterns and resource consumption, and without central controller involvement. Experimental results based on a real UNO prototype in realistic scenarios show promising results: it can save processing worth up to 8 CPU cores, reduce power usage by up to 2x, and reduce the control plane overhead by more than 50%.",,,,,,,,,,,,,,,,,,,,,,,,,,networking and SDNs; virtualization and containers,0.0,EC3,2779
10.1145/3127499,0,0,0,0,0,402,"Ferroni, Matteo; Corna, Andrea; Damiani, Andrea; Brondolin, Rolando; Kubiatowicz, John D.; Sciuto, Donatella; Santambrogio, Marco D.",MARC: A Resource Consumption Modeling Service for Self-Aware Autonomous Agents,ACM Trans. Auton. Adapt. Syst.,,1556-4665,https://doi.org/10.1145/3127499,2017-11,,,,,,,"Autonomicity is a golden feature when dealing with a high level of complexity. This complexity can be tackled partitioning huge systems in small autonomous modules, i.e., agents. Each agent then needs to be capable of extracting knowledge from its environment and to learn from it, in order to fulfill its goals: this could not be achieved without proper modeling techniques that allow each agent to gaze beyond its sensors. Unfortunately, the simplicity of agents and the complexity of modeling do not fit together, thus demanding for a third party to bridge the gap.Given the opportunities in the field, the main contributions of this work are twofold: (1) we propose a general methodology to model resource consumption trends and (2) we implemented it into MARC, a Cloud-service platform that produces Models-as-a-Service, thus relieving self-aware agents from the burden of building their custom modeling framework. In order to validate the proposed methodology, we set up a custom simulator to generate a wide spectrum of controlled traces: this allowed us to verify the correctness of our framework from a general and comprehensive point of view.",,,,,,,,,,,,,,,,,,,,,,,,,,autoregressive with exogenous variable models; discrete Markov models; Model-as-a-service; resource consumption,0.0,EC3,2821
10.1145/3129790.3129818,0,0,0,0,1,30,"Munoz, Daniel-Jesus; Pinto, Mónica; Fuentes, Lidia",Green software development and research with the HADAS toolkit,Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings,978-1-4503-5217-8,,https://doi.org/10.1145/3129790.3129818,2017,ECSA '17,,,,Association for Computing Machinery,"New York, NY, USA","Energy is a critical resource, and designing a sustainable software architecture is a non-trivial task. Developers require energy metrics that support sustainable software architectures reflecting quality attributes such as security, reliability, performance, etc., identifying what are the concerns that impact more in the energy consumption. A variability model of different designs and implementations of an energy model should exist for this task, as well as a service that stores and compares the experimentation results of energy and time consumption of each concern, finding out what is the most eco-efficient solution. The experimental measurements are performed by energy experts and researchers that share the energy model and metrics in a collaborative repository. HADAS confronts these tasks modelling and reasoning with the variability of energy consuming concerns for different energy contexts, connecting HADAS variability model with its energy efficiency collaborative repository, establishing a Software Product Line (SPL) service. Our main goal is to help developers to perform sustainability analyses finding out the eco-friendliest architecture configurations. A HADAS toolkit prototype is implemented based on a Clafer model and Choco solver, and it has been tested with several case studies.",,,,,,,,,,,,,,,,,,,,,,,,,,energy efficiency; variability; optimisation; clafer; CVL; metrics; repository; software product line,0.0,EC3 but maybe useful,93
10.1145/3132211.3134459,0,0,0,0,0,138,"Yi, Shanhe; Hao, Zijiang; Zhang, Qingyang; Zhang, Quan; Shi, Weisong; Li, Qun",LAVEA: latency-aware video analytics on edge computing platform,Proceedings of the Second ACM/IEEE Symposium on Edge Computing,978-1-4503-5087-7,,https://doi.org/10.1145/3132211.3134459,2017,SEC '17,,,,Association for Computing Machinery,"New York, NY, USA","Along the trend pushing computation from the network core to the edge where the most of data are generated, edge computing has shown its potential in reducing response time, lowering bandwidth usage, improving energy efficiency and so on. At the same time, low-latency video analytics is becoming more and more important for applications in public safety, counter-terrorism, self-driving cars, VR/AR, etc. As those tasks are either computation intensive or bandwidth hungry, edge computing fits in well here with its ability to flexibly utilize computation and bandwidth from and between each layer. In this paper, we present LAVEA, a system built on top of an edge computing platform, which offloads computation between clients and edge nodes, collaborates nearby edge nodes, to provide low-latency video analytics at places closer to the users. We have utilized an edge-first design and formulated an optimization problem for offloading task selection and prioritized offloading requests received at the edge node to minimize the response time. In case of a saturating workload on the front edge node, we have designed and compared various task placement schemes that are tailed for inter-edge collaboration. We have implemented and evaluated our system. Our results reveal that the client-edge configuration has a speedup ranging from 1.3x to 4x (1.2x to 1.7x) against running in local (client-cloud configuration). The proposed shortest scheduling latency first scheme outputs the best overall task placement performance for inter-edge collaboration.",,,,,,,,,,,,,,,,,,,,,,,,,,edge computing; computation offloading,0.0,EC3,2453
10.1145/3136273.3136283,0,0,0,0,0,257,"Gusev, Marjan; Patel, Shushma; Tasic, Jurij",Stimulating Intellectual Activity with Adaptive Environment (SMILE),Proceedings of the 8th Balkan Conference in Informatics,978-1-4503-5285-7,,https://doi.org/10.1145/3136273.3136283,2017,BCI '17,,,,Association for Computing Machinery,"New York, NY, USA","As of 2016, 47.5 million people have dementia worldwide according to the World Health Organization, and it is expected that this number will nearly double every 20 years, reaching 100 million sooner than 2030. The goal of the SMILE concept is to help the elderly to live independently and to prevent/delay dementia. It should improve the quality of life of elderly by introducing healthy habits and a lifelong involvement in mentally and socially stimulating activities. It should reduce social isolation, i.e. high degree of loneliness, dissatisfaction with social contacts, and decreased social network. The focus is on prevention of gradual decline in memory function and planning ability, as well. Applying social network concepts provides easier social interaction for older people. Using stimulating and interactive environment enhances and maintains brain plasticity through social interaction. So far there are no published articles or projects that are targeted at the same goal to develop a self-care social interactive TV system as a Service, which is the aim of the SMILE concept. There are a lot of projects and papers about the relevant technologies, however not the integrated approach, which is the key innovation in this concept.",,,,,,,,,,,,,,,,,,,,,,,,,,Adaptive environment; Avatars; elderly support; Self-health Care; Social Networks; Tablets; wearable telemedicine sensors,,,1277
10.1145/3137133.3137151,0,0,0,0,0,158,"Andersen, Michael P; Kolb, John; Chen, Kaifei; Culler, David E.; Katz, Randy",Democratizing authority in the built environment,Proceedings of the 4th ACM International Conference on Systems for Energy-Efficient Built Environments,978-1-4503-5544-5,,https://doi.org/10.1145/3137133.3137151,2017,BuildSys '17,,,,Association for Computing Machinery,"New York, NY, USA","Operating systems and applications in the built environment have relied upon central authorization and management mechanisms which restrict their scalability, especially with respect to administrative overhead. We propose a new set of primitives encompassing syndication, security, and service execution that unifies the management of applications and services across the built environment, while enabling participants to individually delegate privilege across multiple administrative domains with no loss of security or manageability. We show how to leverage a decentralized authorization syndication platform to extend the design of building operating systems beyond the single administrative domain of a building. The authorization system leveraged is based on blockchain smart contracts to permit decentralized and democratized delegation of authorization without central trust. Upon this, a publish/subscribe syndication tier and a containerized service execution environment are constructed. Combined, these mechanisms solve problems of delegation, federation, device protection and service execution that arise throughout the built environment. We leverage a high-fidelity city-scale emulation to verify the scalability of the authorization tier, and briefly describe a prototypical democratized operating system for the built environment using this foundation.",,,,,,,,,,,,,,,,,,,,,,,,,,microservices; federation; syndication; built environment,,,667
10.1145/3139290,1,1,1,1,0,424,"Fokaefs, Marios; Barna, Cornel; Litoiu, Marin",From DevOps to BizOps: Economic Sustainability for Scalable Cloud Applications,ACM Trans. Auton. Adapt. Syst.,,1556-4665,https://doi.org/10.1145/3139290,2017-11,,,,,,,"Virtualization of resources in cloud computing has enabled developers to commission and recommission resources at will and on demand. This virtualization is a coin with two sides. On one hand, the flexibility in managing virtual resources has enabled developers to efficiently manage their costs; they can easily remove unnecessary resources or add resources temporarily when the demand increases. On the other hand, the volatility of such environment and the velocity with which changes can occur may have a greater impact on the economic position of a stakeholder and the business balance of the overall ecosystem. In this work, we recognise the business ecosystem of cloud computing as an economy of scale and explore the effect of this fact on decisions concerning scaling the infrastructure of web applications to account for fluctuations in demand. The goal is to reveal and formalize opportunities for economically optimal scaling that takes into account not only the cost of infrastructure but also the revenue from service delivery and eventually the profit of the service provider. The end product is a scaling mechanism that makes decisions based on both performance and economic criteria and takes adaptive actions to optimize both performance and profitability for the system.",,,,,,,,,,,,,,,,,,,,,,,,,,Cloud computing; DevOps; self-adaptive systems; software engineering economics,0.0,EC3,2046
10.1145/3147213.3147227,1,1,1,1,1,40,"Hasan, MD Sabbir; Alvares, Frederico; Ledoux, Thomas",GPaaScaler: Green Energy Aware Platform Scaler for Interactive Cloud Application,Proceedings of The10th International Conference on Utility and Cloud Computing,978-1-4503-5149-2,,https://doi.org/10.1145/3147213.3147227,2017,UCC '17,,,,Association for Computing Machinery,"New York, NY, USA","Recently, smart usage of renewable energy has been a hot topic in the Cloud community. In this vein, we have recently proposed the creation of green energy awareness around Interactive Cloud Applications, but in static amount of underlying resources. This paper adds to previous ones as it considers elastic underlying infrastructure, that is, we propose a PaaS solution which efficiently utilize the elasticity nature at both infrastructure and application levels, by leveraging adaptation in facing to changing condition i.e., workload burst, performance degradation, quality of energy, etc. While applications are adapted by dynamically re-configuring their service level based on performance and/or green energy availability, the infrastructure takes care of addition/removal of resources based on application's resource demand. Both adaptive behaviors are implemented in separated modules and are coordinated in a sequential manner.We validate our approach by extensive experiments and results obtained over Grid'5000 test bed. Results show that, application can reduce significant amount of brown energy consumption by 35% and daily instance hour cost by 37% compared to a baseline approach when green energy aware adaptation is considered.",,,,,,,,,,,,,,,,,,,,,,,,,,energy consumption; autonomic computing; green it; interactive cloud application; paas; sustainable computing.,0.0,IC3,67
10.1145/3147213.3147230,0,0,0,0,0,209,"Sampaio, Lilia; Silva, Fábio; Souza, Amanda; Brito, Andrey; Felber, Pascal",Secure and Privacy-Aware Data Dissemination for Cloud-Based Applications,Proceedings of The10th International Conference on Utility and Cloud Computing,978-1-4503-5149-2,,https://doi.org/10.1145/3147213.3147230,2017,UCC '17,,,,Association for Computing Machinery,"New York, NY, USA","In this paper we propose a data dissemination platform that supports data security and different privacy levels even when the platform and the data are hosted by untrusted infrastructures. The proposed system aims at enabling an application ecosystem that uses off-the-shelf trusted platforms (in this case, Intel SGX), so that users may allow or disallow third parties to access the live data stream with a specific sensitivity-level. Moreover, this approach does not require users to manage the encryption keys directly. Our experiments show that such an approach is indeed practical for medium scale systems, where participants disseminate small volumes of data at a time, such as in smart grids and IoT environments.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; iot; data security; privacy; sgx; trusted execution,,,226
10.1145/3147234.3148092,0,0,0,0,0,66,"Martins, Lucas M. C. e; Filho, Francisco L. de Caldas; Júnior, Rafael T. de Sousa; Giozza, William F.; da Costa, João Paulo C.L.",Increasing the Dependability of IoT Middleware with Cloud Computing and Microservices,Companion Proceedings of The10th International Conference on Utility and Cloud Computing,978-1-4503-5195-9,,https://doi.org/10.1145/3147234.3148092,2017,UCC '17 Companion,,,,Association for Computing Machinery,"New York, NY, USA","Internet of Things (IoT) has left the experimental field and is reaching the final consumer in areas such as residential automation, health, transportation and government support. Since these are applications intrinsically linked to the physical world, they require more attention in aspects related to their dependability. Focusing on its availability and reliability, we present a proposal to apply the Microservices architectural pattern in an IoT middleware with web services in the monolithic architecture. We describe the reengineering that must be done in this middleware and, finally, we analyze the advantages and disadvantages of this approach, highlighting the availability improvement, optimization of the infrastructure resources, the ease of maintenance and evolution, as well as the inclination to the elasticity that the architecture allows.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; microservices; dependability; internet of things (iot); middleware; soa.,0.0,EC3,1928
10.1145/3154273.3154347,0,0,0,0,0,143,"Mahmud, Redowan; Koch, Fernando Luiz; Buyya, Rajkumar",Cloud-Fog Interoperability in IoT-enabled Healthcare Solutions,Proceedings of the 19th International Conference on Distributed Computing and Networking,978-1-4503-6372-3,,https://doi.org/10.1145/3154273.3154347,2018,ICDCN '18,,,,Association for Computing Machinery,"New York, NY, USA","The issue of utilizing Internet of Things (IoT) in Healthcare solutions relates to the problems of latency sensitivity, uneven data load, diverse user expectations and heterogeneity of the applications. Current explorations consider Cloud Computing as the base stone to create IoT-Enable solution. Nonetheless, this environment entails limitations in terms of multi-hop distance from the data source, geographical centralized architecture, economical aspects, etc. To address these limitations, there is a surge of solutions that apply Fog Computing as an approach to bring computing resources closer to the data sources. This approach is being fomented by the growing availability of powerful edge computing at lower cost and commercial developments in the area. Nonetheless, the implementation of Cloud-Fog interoperability and integration implies in complex coordination of applications and services and the demand for intelligent service orchestrations so that solutions can make the best use of distributed resources without compromising stability, quality of services, and security. In this paper, we introduce a Fog-based IoT-Healthcare solution structure and explore the integration of Cloud-Fog services in interoperable Healthcare solutions extended upon the traditional Cloud-based structure. The scenarios are evaluated through simulations using the iFogSim simulator and the results analyzed in relation to distributed computing, reduction of latency, optimization of data communication, and power consumption. The experimental results point towards improvement in instance cost, network delay and energy usage.",,,,,,,,,,,,,,,,,,,,,,,,,,Cloud computing; Fog computing; Internet of Things; HealthCare; Interoperable architecture,0.0,EC3,1792
10.1145/3184407.3184414,0,0,0,0,0,68,"Tesfatsion, Selome Kostentinos; Klein, Cristian; Tordsson, Johan","Virtualization Techniques Compared: Performance, Resource, and Power Usage Overheads in Clouds",Proceedings of the 2018 ACM/SPEC International Conference on Performance Engineering,978-1-4503-5095-2,,https://doi.org/10.1145/3184407.3184414,2018,ICPE '18,,,,Association for Computing Machinery,"New York, NY, USA","Virtualization solutions based on hypervisors or containers are enabling technologies for scalable, flexible, and cost-effective resource sharing. As the fundamental limitations of each technology are yet to be understood, they need to be regularly reevaluated to better understand the trade-off provided by latest technological advances. This paper presents an in-depth quantitative analysis of virtualization overheads in these two groups of systems and their gaps relative to native environments based on a diverse set of workloads that stress CPU, memory, storage, and networking resources. KVM and XEN are used to represent hypervisor-based virtualization, and LXC and Docker for container-based platforms. The systems were evaluated with respect to several cloud resource management dimensions including performance, isolation, resource usage, energy efficiency, start-up time, and density. Our study is useful both to practitioners to understand the current state of the technology in order to make the right decision in the selection, operation and/or design of platforms and to scholars to illustrate how these technologies evolved over time.",,,,,,,,,,,,,,,,,,,,,,,,,,performance; containers; power; isolation; resource usage; virtualization overhead; VMS,0.0,EC3,2734
10.1145/3184407.3184431,0,0,0,0,0,296,"Knoche, Holger; Eichelberger, Holger",Using the Raspberry Pi and Docker for Replicable Performance Experiments: Experience Paper,Proceedings of the 2018 ACM/SPEC International Conference on Performance Engineering,978-1-4503-5095-2,,https://doi.org/10.1145/3184407.3184431,2018,ICPE '18,,,,Association for Computing Machinery,"New York, NY, USA","Replicating software performance experiments is difficult. A common obstacle to replication is that recreating the hardware and software environments is often impractical. As researchers usually run their experiments on the hardware and software that happens to be available to them, recreating the experiments would require obtaining identical hardware, which can lead to high costs. Recreating the software environment is also difficult, as software components such as particular library versions might no longer be available. Cheap, standardized hardware components like the Raspberry Pi and portable software containers like the ones provided by Docker are a potential solution to meet the challenge of replicability. In this paper, we report on experiences from replicating performance experiments on Raspberry Pi devices with and without Docker and show that good replication results can be achieved for microbenchmarks such as JMH. Replication of macrobenchmarks like SPECjEnterprise 2010 proves to be much more difficult, as they are strongly affected by (non-standardized) peripherals. Inspired by previous microbenchmarking experiments on the Pi platform, we furthermore report on a systematic analysis of response time fluctuations, and present lessons learned on dos and don»ts for replicable performance experiments.",,,,,,,,,,,,,,,,,,,,,,,,,,performance benchmark; Raspberry pi; replicability; single-board computer,0.0,EC3,3101
10.1145/3197385,0,0,0,0,0,418,"Calegari, Patrice; Levrier, Marc; Balczyński, Paweł",Web Portals for High-performance Computing: A Survey,ACM Trans. Web,,1559-1131,https://doi.org/10.1145/3197385,2019-02,,,,,,,"This article addresses web interfaces for High-performance Computing (HPC) simulation software. First, it presents a brief history, starting in the 1990s with Java applets, of web interfaces used for accessing and making best possible use of remote HPC resources. It introduces HPC web-based portal use cases. Then it identifies and discusses the key features, among functional and non-functional requirements, that characterize such portals. A brief state of the art is then presented. The design and development of Bull extreme factory Computing Studio v3 (XCS3) is chosen as a common thread for showing how the identified key features can all be implemented in one software: multi-tenancy, multi-scheduler compatibility, complete control through an HTTP RESTful API, customizable user interface with Responsive Web Design, HPC application template framework, remote visualization, and access through the Authentication, Authorization, and Accounting security framework with the Role-Based Access Control permission model. Non-functional requirements (security, usability, performance, reliability) are discussed, and the article concludes by giving perspective for future work.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; HPC; application templates; customizable GUI; dashboards; High-performance computing; HPC-as-a-service; HPCaaS; job management; RESTful API; SaaS; science gateway; service-oriented architectures; software-as-a-service; user interface; web portal,0.0,EC3,2908
10.1145/3199665,0,0,0,0,0,359,"Andersen, Michael P.; Kolb, John; Chen, Kaifei; Fierro, Gabe; Culler, David E.; Katz, Randy",Democratizing Authority in the Built Environment,ACM Trans. Sen. Netw.,,1550-4859,https://doi.org/10.1145/3199665,2018-12,,,,,,,"Operating systems and applications in the built environment have relied upon central authorization and management mechanisms that restrict their scalability, especially with respect to administrative overhead. We propose a new set of primitives encompassing syndication, security, and service execution that unifies the management of applications and services across the built environment, while enabling participants to individually delegate privilege across multiple administrative domains with no loss of security or manageability. We show how to leverage a decentralized authorization syndication platform to extend the design of building operating systems beyond the single administrative domain of a building. The authorization system leveraged is based on blockchain smart contracts to permit decentralized and democratized delegation of authorization without central trust. Upon this, a publish/subscribe syndication tier and a containerized service execution environment are constructed. Combined, these mechanisms solve problems of delegation, federation, device protection and service execution that arise throughout the built environment. We leverage a high-fidelity city-scale emulation to verify the scalability of the authorization tier, and briefly describe a prototypical democratized operating system for the built environment using this foundation.This is an extension of work presented in Ref.&nbsp;[3].",,,,,,,,,,,,,,,,,,,,,,,,,,microservices; Built environment; federation; syndication,,,618
10.1145/3210284.3210285,0,0,0,0,0,272,"Meißner, Echo; Erb, Benjamin; Kargl, Frank; Tichy, Matthias",Retro-λ: An Event-sourced Platform for Serverless Applications with Retroactive Computing Support,Proceedings of the 12th ACM International Conference on Distributed and Event-Based Systems,978-1-4503-5782-1,,https://doi.org/10.1145/3210284.3210285,2018,DEBS '18,,,,Association for Computing Machinery,"New York, NY, USA","State changes over time are inherent characteristics of stateful applications. So far, there are almost no attempts to make the past application history programmatically accessible or even modifiable. This is primarily due to the complexity of temporal changes and a difficult alignment with prevalent programming primitives and persistence strategies. Retroactive computing enables powerful capabilities though, including computations and predictions of alternate application timelines, post-hoc bug fixes, or retroactive state explorations. We propose an event-driven programming model that is oriented towards serverless computing and applies retroaction to the event sourcing paradigm. Our model is deliberately restrictive, but therefore keeps the complexity of retroactive operations in check. We introduce retro-λ, a runtime platform that implements the model and provides retroactive capabilites to its applications. While retro-λ only shows negligible performance overheads compared to similar solutions for running regular applications, it enables its users to execute retroactive computations on the application histories as part of its programming model.",,,,,,,,,,,,,,,,,,,,,,,,,,serverless computing; event processing; event sourcing; event-driven architecture; retroaction; retroactive computing,,,796
10.1145/3210284.3219770,0,0,0,0,0,283,"Meißner, Echo; Erb, Benjamin; Kargl, Frank",Performance Engineering in Distributed Event-sourced Systems,Proceedings of the 12th ACM International Conference on Distributed and Event-Based Systems,978-1-4503-5782-1,,https://doi.org/10.1145/3210284.3219770,2018,DEBS '18,,,,Association for Computing Machinery,"New York, NY, USA","Distributed event-sourced systems adopt a fairly new architectural style for data-intensive applications that maintains the full history of the application state. However, the performance implications of such systems are not yet well explored, let alone how the performance of these systems can be improved. A central issue is the lack of systematic performance engineering approaches that take into account the specific characteristics of these systems. To address this problem, we suggest a methodology for performance engineering and performance analysis of distributed event-sourced systems based on specific measurements and subsequent, targeted optimizations. The methodology blends in well into existing software engineering processes and helps developers to identify bottlenecks and to resolve performance issues. Using our structured approach, we improved an existing event-sourced system prototype and increased its performance considerably.",,,,,,,,,,,,,,,,,,,,,,,,,,event processing; event sourcing; performance engineering,,,180
10.1145/3219104.3219147,0,0,0,0,0,69,"Sampedro, Zebula; Holt, Aaron; Hauser, Thomas",Continuous Integration and Delivery for HPC: Using Singularity and Jenkins,Proceedings of the Practice and Experience on Advanced Research Computing: Seamless Creativity,978-1-4503-6446-1,,https://doi.org/10.1145/3219104.3219147,2018,PEARC '18,,,,Association for Computing Machinery,"New York, NY, USA","Continuous integration, delivery, and deployment (CICD) is widely used in DevOps communities, as it allows for teams of all sizes to deploy rapidly-changing hardware and software resources quickly and confidently. In this paper, we will describe how University of Colorado Boulder Research Computing has adopted these practices on the RMACC Summit supercomputer [17] to allow system engineers and researchers alike to capitalize on the benefits of CICD-centric development workflows. We will introduce the topic of CICD at a high level and describe how such practices can ease common software management challenges for High-Performance Computing (HPC) resources. We will then document the infrastructure deployed for Summit, and explain how software such as Jenkins and Singularity enabled adaptation for an HPC environment. We will conclude with two case studies discussing the use of our CICD infrastructure: one case study from the perspective of a system engineer maintaining user-facing resources, and the other case study from the perspective of a researcher developing, maintaining, and using the MFiX-Exa codebase.",,,,,,,,,,,,,,,,,,,,,,,,,,containers; continuous delivery; continuous deployment; continuous integration; MFIX-Exa; Singularity; software automation; software builds,,,1007
10.1145/3229631.3236092,0,0,0,0,0,289,"Ciobanu, Cătălin Bogdan; Stramondo, Giulio; Varbanescu, Ana Lucia; Brokalakis, Andreas; Nikitakis, Antonis; Tucci, Lorenzo Di; Rabozzi, Marco; Stornaiuolo, Luca; Santambrogio, Marco; Chrysos, Grigorios; Vatsolakis, Charalampos; Georgios, Charitopoulos; Pnevmatikatos, Dionisios",EXTRA: an open platform for reconfigurable architectures,"Proceedings of the 18th International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation",978-1-4503-6494-2,,https://doi.org/10.1145/3229631.3236092,2018,SAMOS '18,,,,Association for Computing Machinery,"New York, NY, USA","Reconfigurable hardware is becoming increasingly mainstream, evolving to a valid alternative to Graphics Processing Units-based hardware accelerators. However, several major challenges remain for migrating existing software to heterogeneous reconfigurable architectures. The EXTRA project aims to develop an integrated environment for developing and programming reconfigurable architectures. The EXTRA platform enables the joint optimization of architecture, tools, and reconfiguration technology, and targets the future High Performance Computing hardware nodes. In this paper, we present four innovative EXTRA technologies: (1) a hardware-software co-design framework; (2) a parallel memory system; (3) a decoupled access execute framework for reconfigurable technology; and (4) transparent access and virtualization of reconfigurable hardware accelerators. Moreover, we describe how the EXTRA technologies targeting the Amazon F1 cloud compute instances can be used in medical applications such as the retinal image segmentation.",,,,,,,,,,,,,,,,,,,,,,,,,,FPGA; CAOS; DAER; exascale; PolyMem; polymorphic; RACOS,,,384
10.1145/3232162,0,0,0,0,0,323,"Azimi, Reza; Fox, Tyler; Gonzalez, Wendy; Reda, Sherief",Scale-Out vs Scale-Up: A Study of ARM-based SoCs on Server-Class Workloads,ACM Trans. Model. Perform. Eval. Comput. Syst.,,2376-3639,https://doi.org/10.1145/3232162,2018-08,,,,,,,"ARM 64-bit processing has generated enthusiasm to develop ARM-based servers that are targeted for both data centers and supercomputers. In addition to the server-class components and hardware advancements, the ARM software environment has grown substantially over the past decade. Major development ecosystems and libraries have been ported and optimized to run on ARM, making ARM suitable for server-class workloads. There are two trends in available ARM SoCs: mobile-class ARM SoCs that rely on the heterogeneous integration of a mix of CPU cores, GPGPU streaming multiprocessors (SMs), and other accelerators, and the server-class SoCs that instead rely on integrating a larger number of CPU cores with no GPGPU support and a number of IO accelerators. For scaling the number of processing cores, there are two different paradigms: mobile-class SoCs that use scale-out architecture in the form of a cluster of simpler systems connected over a network, and server-class ARM SoCs that use the scale-up solution and leverage symmetric multiprocessing to pack a large number of cores on the chip. In this article, we present ScaleSoC cluster, which is a scale-out solution based on mobile class ARM SoCs. ScaleSoC leverages fast network connectivity and GPGPU acceleration to improve performance and energy efficiency compared to previous ARM scale-out clusters. We consider a wide range of modern server-class parallel workloads to study both scaling paradigms, including latency-sensitive transactional workloads, MPI-based CPU and GPGPU-accelerated scientific applications, and emerging artificial intelligence workloads. We study the performance and energy efficiency of ScaleSoC compared to server-class ARM SoCs and discrete GPGPUs in depth. We quantify the network overhead on the performance of ScaleSoC and show that packing a large number of ARM cores on a single chip does not necessarily guarantee better performance, due to the fact that shared resources, such as last-level cache, become performance bottlenecks. We characterize the GPGPU accelerated workloads and demonstrate that for applications that can leverage the better CPU-GPGPU balance of the ScaleSoC cluster, performance and energy efficiency improve compared to discrete GPGPUs.",,,,,,,,,,,,,,,,,,,,,,,,,,ARM computing; GPGPU acceleration; scale-out clusters,,,190
10.1145/3234152.3234196,0,0,0,0,0,63,"Li, Fei; Gelbke, Lars",Microservice architecture in industrial software delivery on edge devices,Proceedings of the 19th International Conference on Agile Software Development: Companion,978-1-4503-6422-5,,https://doi.org/10.1145/3234152.3234196,2018,XP '18,,,,Association for Computing Machinery,"New York, NY, USA","Production in Industry 4.0 calls for flexible configurations, customizable features, frequent changes, and above all, short time to market, which poses significant challenges to the practices of industrial software development and delivery. However, under strict regulations and non-negotiable requirements on safety, reliability and compliance, industrial software is still dominated by monolithic architecture that usually requires planned downtime and manual installation for each update. The result is a typical release cycle of 12-18 months with several patches during the release cycle.This paper reports our experiences in applying microservice architecture, and delivery pipeline to achieve continuous deployment for industrial control software on edge devices. They are characterized by their physical proximity to industrial equipment, limited computational resources, high avail-ability, and highly heterogeneous software environments in order to accommodate various industrial protocols and processes.",,,,,,,,,,,,,,,,,,,,,,,,,,microservice; edge computing; industrial software,0.0,EC3,2638
10.1145/3236332,0,0,0,0,0,392,"Herbst, Nikolas; Bauer, André; Kounev, Samuel; Oikonomou, Giorgos; Eyk, Erwin Van; Kousiouris, George; Evangelinou, Athanasia; Krebs, Rouven; Brecht, Tim; Abad, Cristina L.; Iosup, Alexandru","Quantifying Cloud Performance and Dependability: Taxonomy, Metric Design, and Emerging Challenges",ACM Trans. Model. Perform. Eval. Comput. Syst.,,2376-3639,https://doi.org/10.1145/3236332,2018-08,,,,,,,"In only a decade, cloud computing has emerged from a pursuit for a service-driven information and communication technology (ICT), becoming a significant fraction of the ICT market. Responding to the growth of the market, many alternative cloud services and their underlying systems are currently vying for the attention of cloud users and providers. To make informed choices between competing cloud service providers, permit the cost-benefit analysis of cloud-based systems, and enable system DevOps to evaluate and tune the performance of these complex ecosystems, appropriate performance metrics, benchmarks, tools, and methodologies are necessary. This requires re-examining old system properties and considering new system properties, possibly leading to the re-design of classic benchmarking metrics such as expressing performance as throughput and latency (response time). In this work, we address these requirements by focusing on four system properties: (i) elasticity of the cloud service, to accommodate large variations in the amount of service requested, (ii)&nbsp;performance isolation between the tenants of shared cloud systems and resulting performance variability, (iii)&nbsp;availability of cloud services and systems, and (iv) the operational risk of running a production system in a cloud environment. Focusing on key metrics for each of these properties, we review the state-of-the-art, then select or propose new metrics together with measurement approaches. We see the presented metrics as a foundation toward upcoming, future industry-standard cloud benchmarks.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud; Metrics; benchmarking; availability; elasticity; operational risk; performance isolation; performance variability,0.0,EC3,2100
10.1145/3241403.3241424,0,0,0,0,0,78,"Muccini, Henry; Spalazzese, Romina; Moghaddam, Mahyar T.; Sharaf, Mohammad",Self-adaptive IoT architectures: an emergency handling case study,Proceedings of the 12th European Conference on Software Architecture: Companion Proceedings,978-1-4503-6483-6,,https://doi.org/10.1145/3241403.3241424,2018,ECSA '18,,,,Association for Computing Machinery,"New York, NY, USA","Along with the rapid growth of IoT technologies and devices, their solutions are currently being applied on various domains such as health-care, transportation and agriculture, but mainly on crowd monitoring and emergency handling. The latter is a safety critical IoT system based on collecting and analyzing the real-time data to perform proper actuation. In order to engineer such a high quality IoT application, a proper software architecture should be designed. In order for the software architecture to be able to optimize critical requirements such as fault-tolerance, performance and energy consumption, it ought to: i) adapt itself to real-time environment transformation, ii) be designed in a proper level of elements distribution. In this paper, we critically analyze a set of IoT distribution and self-adaptation patterns to identify their suitable architectural combinations. Further, we use our IoT modeling framework (CAPS) to model an emergency handling system. Based on these, we design two quality driven architectures to be used for a forest monitoring and evacuation example and qualitatively evaluate and compare them.",,,,,,,,,,,,,,,,,,,,,,,,,,self-adaptation; IoT; architectural patterns and styles; quality attributes; urban security,0.0,EC3,2106
10.1145/3241737,0,0,0,0,0,344,"Buyya, Rajkumar; Srirama, Satish Narayana; Casale, Giuliano; Calheiros, Rodrigo; Simmhan, Yogesh; Varghese, Blesson; Gelenbe, Erol; Javadi, Bahman; Vaquero, Luis Miguel; Netto, Marco A. S.; Toosi, Adel Nadjaran; Rodriguez, Maria Alejandra; Llorente, Ignacio M.; Vimercati, Sabrina De Capitani Di; Samarati, Pierangela; Milojicic, Dejan; Varela, Carlos; Bahsoon, Rami; Assuncao, Marcos Dias De; Rana, Omer; Zhou, Wanlei; Jin, Hai; Gentzsch, Wolfgang; Zomaya, Albert Y.; Shen, Haiying",A Manifesto for Future Generation Cloud Computing: Research Directions for the Next Decade,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3241737,2018-11,,,,,,,"The Cloud computing paradigm has revolutionised the computer science horizon during the past decade and has enabled the emergence of computing as the fifth utility. It has captured significant attention of academia, industries, and government bodies. Now, it has emerged as the backbone of modern economy by offering subscription-based services anytime, anywhere following a pay-as-you-go model. This has instigated (1) shorter establishment times for start-ups, (2) creation of scalable global enterprise applications, (3) better cost-to-value associativity for scientific and high-performance computing applications, and (4) different invocation/execution models for pervasive and ubiquitous applications. The recent technological developments and paradigms such as serverless computing, software-defined networking, Internet of Things, and processing at network edge are creating new opportunities for Cloud computing. However, they are also posing several new challenges and creating the need for new approaches and research strategies, as well as the re-evaluation of the models that were developed to address issues such as scalability, elasticity, reliability, security, sustainability, and application models. The proposed manifesto addresses them by identifying the major open challenges in Cloud computing, emerging trends, and impact areas. It then offers research directions for the next decade, thus helping in the realisation of Future Generation Cloud Computing.",,,,,,,,,,,,,,,,,,,,,,,,,,Cloud computing; Fog computing; serverless computing; sustainability; scalability; application development; Cloud economics; data management; InterCloud,0.0,EC3,2719
10.1145/3267195.3267200,0,0,0,0,0,232,"Siddiqui, Sarah Ali; Mahmood, Adnan",Towards Fog-based Next Generation Internet of Vehicles Architecture,Proceedings of the 1st International Workshop on Communication and Computing in Connected Vehicles and Platooning,978-1-4503-5925-2,,https://doi.org/10.1145/3267195.3267200,2018,C3VP '18,,,,Association for Computing Machinery,"New York, NY, USA","Internet of Vehicles (IoV) is an integral component of the Intelligent Transportation System (ITS) with the vision to acquire and process data for improving traffic safety. In IoV, vehicles communicate by exchanging messages with other vehicles and roadside infrastructure subsequently generating a large volume of data often referred to as big data, and which demands a massive storage and high computational capability with minimal possible delay. The aim of this paper is to take a step forward in outlining the current state-of-the-art in IoV and analyzing the improved data processing and storage features by utilizing big data analytics and fog computing paradigm. We further presented a preliminary simulation result to demonstrate that the introduction of fog improves the overall system performance by ensuring low-latency critical for especially vehicular safety applications. Moreover, this paper briefly discusses a fog computing based IoV crime assistant and highlight some of its challenges which needs to be addressed in order to enhance the performance of such systems.",,,,,,,,,,,,,,,,,,,,,,,,,,fog computing; big data analytics; intelligent transportation systems; internet of vehicles,0.0,EC3,2664
10.1145/3277593.3277599,0,0,0,0,0,150,"Garzon, Sandro Rodriguez; Walther, Sebastian; Pang, Shaoning; Deva, Bersant; Küpper, Axel",Urban air pollution alert service for smart cities,Proceedings of the 8th International Conference on the Internet of Things,978-1-4503-6564-2,,https://doi.org/10.1145/3277593.3277599,2018,IOT '18,,,,Association for Computing Machinery,"New York, NY, USA","With the advent of small, energy-efficient, interconnected and cheap air pollution sensors, communities all over the world have begun to drive forward the expansion of city-wide sensor networks in order to get a holistic picture of the air pollution level that the citizens are exposed to in their daily life. Unfortunately, the resulting insights about the current air quality need to be actively queried by the citizens, leading to situations in which the citizen is exposed to a high air pollution without being aware of it. In this article, we introduce a context-aware air pollution monitoring and alert service that proactively notifies citizens via mobile devices about highly relevant air quality information once they enter an area with an air pollution that exceeds a user-defined threshold. The service continuously determines the concentration of particulate matter throughout a given urban area, detects closed areas with a high air pollution and shares this information with the citizen's mobile devices which are in turn responsible to regularly compare a citizen's position against the areas of poor air quality. As a proof of concept, we prototypically implemented and evaluated the service by means of real world measurements taken on consecutive days within an urban area in New Zealand.",,,,,,,,,,,,,,,,,,,,,,,,,,air quality; context-aware computing; geofencing; location-based services; smart city; urban sensing,,,975
10.1145/3286978.3286997,0,0,0,0,0,242,"Akkermans, Sven; Crispo, Bruno; Joosen, Wouter; Hughes, Danny","Polyglot CerberOS: Resource Security, Interoperability and Multi-Tenancy for IoT Services on a Multilingual Platform","Proceedings of the 15th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services",978-1-4503-6093-7,,https://doi.org/10.1145/3286978.3286997,2018,MobiQuitous '18,,,,Association for Computing Machinery,"New York, NY, USA","The Internet of Things (IoT) promises to tackle a range of environmental challenges and deliver large efficiency gains in industry by embedding computational intelligence, sensing and control in our physical environment. Multiple independent parties are increasingly seeking to leverage shared IoT infrastructure, using a similar model to the cloud, and thus require constrained IoT devices to become microservice-hosting platforms that can securely and concurrently execute their code and interoperate. This vision demands that heterogeneous services, peripherals and platforms are provided with an expanded set of security guarantees to prevent third-party services from hijacking the platform, resource-level access control and accounting, and strong isolation between running processes to prevent unauthorized access to third-party services and data. This paper introduces Polyglot CerberOS, a resource-secure operating system for multi-tenant IoT devices that is realised through a reconfigurable virtual machine which can simultaneously execute interoperable services, written in different languages. We evaluate Polyglot CerberOS on IETF Class-1 devices running both Java and C services. The results show that interoperability and strong security guarantees for multilingual services on multi-tenant commodity IoT devices are feasible, in terms of performance and memory overhead, and transparent for developers.",,,,,,,,,,,,,,,,,,,,,,,,,,Internet of Things; Security; Interoperability; Services,,,242
10.1145/3287921.3287966,0,0,0,0,0,37,"Vu, Tran Hoang; Dai, Nghia Hoang Ba; Khanh, Hong Nguyen Thi; Quang, Dong Nguyen Vo",Overall Structural System Solution for Supporting Services and Tourists Management Oriented on Smart City in Viet Nam,Proceedings of the 9th International Symposium on Information and Communication Technology,978-1-4503-6539-0,,https://doi.org/10.1145/3287921.3287966,2018,SoICT '18,,,,Association for Computing Machinery,"New York, NY, USA","Recently, ""Smart tourism"" has appeared as a new term to describe the application of the technological advancements that rely on sensors, big data processing technique, open data, open API, new way of connecting and exchanging between humans and machines and multi-device (such as IoT, RFID, and NFC) in tourism. When these technologies are utilized, the digital data become practical and valuable products. Besides, the mobile revolution, and specifically the role of the smartphone and its many opportunities to support travel experiences, is especially worth mentioning in this context. In addition, new management tool for the government, new business opportunities for travel agencies, as well as new experiences for tourists are created. Therefore, in this paper, a model for developing sustainable and intelligent tourism is studied and then an overall architecture model of the information system that can provide supporting services and tools for visitor management (Smart Tourism Service Centre – STSC) is proposed for fostering the smart cities in Vietnam, in general, and Danang, in particular.",,,,,,,,,,,,,,,,,,,,,,,,,,IoT; Smart city; Micro-Services; NFC; Smart Tourism,,,800
10.1145/3288599.3288634,0,0,0,0,0,181,"Fang, Zhou; Lin, Jeng-Hau; Srivastava, Mani B.; Gupta, Rajesh K.",Multi-tenant mobile offloading systems for real-time computer vision applications,Proceedings of the 20th International Conference on Distributed Computing and Networking,978-1-4503-6094-4,,https://doi.org/10.1145/3288599.3288634,2019,ICDCN '19,,,,Association for Computing Machinery,"New York, NY, USA","Offloading techniques enable many emerging computer vision applications on mobile platforms by executing compute-intensive tasks on resource-rich servers. Although there have been a significant amount of research efforts devoted in optimizing mobile offloading frameworks, most previous works are evaluated in a single-tenant setting, that is, a server is assigned to a single client. However, in a practical scenario that servers must handle tasks from many clients running diverse applications, contention on shared server resources may degrade application performance. In this work, we study scheduling techniques to improve serving performance in multi-tenant mobile offloading systems, for computer vision algorithms running on CPUs and deep neural networks (DNNs) running on GPUs. For CPU workloads, we present methods to mitigate resource contention and to improve delay using a Plan-Schedule approach. The planning phase predicts future workloads from all clients, estimates contention, and adjusts future task start times to remove or reduce contention. The scheduling phase dispatches arriving offloaded tasks to the server that minimizes contention. For DNN workloads running on GPUs, we propose adaptive batching algorithms using information of batch size, model complexity and system load to achieve the best Quality of Service (QoS), which are measured from accuracy and delay of DNN tasks. We demonstrate the improvement of serving performance using several real-world applications with different server deployments.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,EC3,2158
10.1145/3295500.3356169,0,0,0,0,0,301,"Li, Ang; Geng, Tong; Wang, Tianqi; Herbordt, Martin; Song, Shuaiwen Leon; Barker, Kevin",BSTC: a novel binarized-soft-tensor-core design for accelerating bit-based approximated neural nets,"Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",978-1-4503-6229-0,,https://doi.org/10.1145/3295500.3356169,2019,SC '19,,,,Association for Computing Machinery,"New York, NY, USA","Binarized neural networks (or BNNs) promise tremendous performance improvement over traditional DNNs through simplified bit-level computation and significantly reduced memory access/storage cost. In addition, it has advantages of low-cost, low-energy, and high-robustness, showing great potential in resources-constrained, volatile, and latency-critical applications, which are critical for future HPC, cloud, and edge applications. However, the promised significant performance gain of BNN inference has never been fully demonstrated on general-purpose processors, particularly on GPUs, due to: (i) the challenge of extracting and leveraging sufficient finegrained bit-level-parallelism to saturate GPU cores when the batch size is small; (ii) the fundamental design conflict between bit-based BNN algorithm and word-based architecture; and (iii) architecture &amp; performance unfriendly to BNN network design. To address (i) and (ii), we propose a binarized-soft-tensor-core as a software-hardware codesign approach to construct bit-manipulation capability for modern GPUs and thereby effectively harvest bit-level-parallelism (BLP). To tackle (iii), we propose intra- and inter-layer fusion techniques so that the entire BNN inference execution can be packed into a single GPU kernel, and so avoid the high-cost of frequent launching and releasing. Experiments show that our Singular-Binarized-Neural-Network (SBNN) design can achieve over 1000X speedup for raw inference latency over the state-of-the-art full-precision BNN inference for AlexNet on GPUs. Comparisons with CPU, GPU, FPGA and Xeon-Phi demonstrate the effectiveness of our design. SBNN is opensourced and available at https://github.com/uuudown/SBNN.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,636
10.1145/3297280.3297427,0,0,0,0,0,80,"Neatu, Darius-Florentin; Stochitoiu, Radu-Dumitru; Postoaca, Andrei-Vlad; Filip, Ion-Dorinel; Pop, Florin",My cloudy time machine: a scalable microservice-based platform for data processing in cloud-edge systems: a proof of concept for the ROBIN-cloud project,Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing,978-1-4503-5933-7,,https://doi.org/10.1145/3297280.3297427,2019,SAC '19,,,,Association for Computing Machinery,"New York, NY, USA","As Cloud computing is a very well developed domain, many companies tend to move their entire activity in Cloud. At the same time, there is a tendency to move some of the data processing from Cloud to Edge as close as possible to the end devices. One main advantage of this approach is minimizing the latency in communication between the end devices and Cloud. Better usage of on premise devices is also a good achievement of the Edge offload. In this paper, we propose an architecture for applications that are connected to ROBIN-Cloud or to a general Cloud. We present how we have encapsulated Python-based microservices in Docker containers. We provide an implementation for the My Cloudy Time Machine application - GIGEL (Guided Intelligent GEared Legend), a nearby autonomous assistive robot. We use this prototype to evaluate the scalability of the proposed architecture. We also present results that show how to gain high performance by tuning a container-based embedded system.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; microservices; edge computing; auto-scalability; data processing,0.0,EC3,1970
10.1145/3301418.3313947,0,0,0,0,0,259,"Karhula, Pekka; Janak, Jan; Schulzrinne, Henning",Checkpointing and Migration of IoT Edge Functions,"Proceedings of the 2nd International Workshop on Edge Systems, Analytics and Networking",978-1-4503-6275-7,,https://doi.org/10.1145/3301418.3313947,2019,EdgeSys '19,,,,Association for Computing Machinery,"New York, NY, USA","The serverless and functions as a service (FaaS) paradigms are currently trending among cloud providers and are now increasingly being applied to the network edge, and to the Internet of Things (IoT) devices. The benefits include reduced latency for communication, less network traffic and increased privacy for data processing. However, there are challenges as IoT devices have limited resources for running multiple simultaneous containerized functions, and also FaaS does not typically support long-running functions. Our implementation utilizes Docker and CRIU for checkpointing and suspending long-running blocking functions. The results show that checkpointing is slightly slower than regular Docker pause, but it saves memory and allows for more long-running functions to be run on an IoT device. Furthermore, the resulting checkpoint files are small, hence they are suitable for live migration and backing up stateful functions, therefore improving availability and reliability of the system.",,,,,,,,,,,,,,,,,,,,,,,,,,serverless; Internet of Things; checkpointing; function as a service; light-weight virtualization,,,334
10.1145/3301443,0,0,0,0,0,376,"Puliafito, Carlo; Mingozzi, Enzo; Longo, Francesco; Puliafito, Antonio; Rana, Omer",Fog Computing for the Internet of Things: A Survey,ACM Trans. Internet Technol.,,1533-5399,https://doi.org/10.1145/3301443,2019-04,,,,,,,"Research in the Internet of Things (IoT) conceives a world where everyday objects are connected to the Internet and exchange, store, process, and collect data from the surrounding environment. IoT devices are becoming essential for supporting the delivery of data to enable electronic services, but they are not sufficient in most cases to host application services directly due to their intrinsic resource constraints. Fog Computing (FC) can be a suitable paradigm to overcome these limitations, as it can coexist and cooperate with centralized Cloud systems and extends the latter toward the network edge. In this way, it is possible to distribute resources and services of computing, storage, and networking along the Cloud-to-Things continuum. As such, FC brings all the benefits of Cloud Computing (CC) closer to end (user) devices. This article presents a survey on the employment of FC to support IoT devices and services. The principles and literature characterizing FC are described, highlighting six IoT application domains that may benefit from the use of this paradigm. The extension of Cloud systems towards the network edge also creates new challenges and can have an impact on existing approaches employed in Cloud-based deployments. Research directions being adopted by the community are highlighted, with an indication of which of these are likely to have the greatest impact. An overview of existing FC software and hardware platforms for the IoT is also provided, along with the standardisation efforts in this area initiated by the OpenFog Consortium (OFC).",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; Fog computing; internet of things; topological proximity,0.0,EC3,2999
10.1145/3304109.3306221,0,0,0,0,0,122,"Fang, Zhou; Hong, Dezhi; Gupta, Rajesh K.",Serving deep neural networks at the cloud edge for vision applications on mobile platforms,Proceedings of the 10th ACM Multimedia Systems Conference,978-1-4503-6297-9,,https://doi.org/10.1145/3304109.3306221,2019,MMSys '19,,,,Association for Computing Machinery,"New York, NY, USA","The proliferation of high resolution cameras on embedded devices along with the growing maturity of deep neural networks (DNNs) has spawned powerful mobile vision applications. To enable applications on mobile devices, the offloading approach processes live video streams using DNNs on server-class GPU accelerators. However, their use in latency constrained applications is particularly challenging because of the large and unpredictable round-trip latency from mobile devices to the cloud computing resources. As a consequence, system designers routinely look for ways to offload to local servers at the cloud edge, known as the cloudlet. This paper explores the potential of serving multiple DNNs using the cloudlet model to implement complex vision applications on mobile devices. We present DeepQuery, a new mobile offloading system that is capable to serve DNNs with different structures for a wide range of tasks including object detection and tracking, scene graph detection, and video description. DeepQuery provides application programming interfaces to offload applications programed as Directed Acyclic Graphs of DNN queries, and employs data parallelization and input batching techniques to reduce processing delays. To improve GPU utilization, it co-locates real-time and delay-tolerant tasks on shared GPUs, and exploits a predictive and plan-ahead approach to alleviate resource contention caused by co-locating. We evaluate DeepQuery and demonstrate its effectiveness using several real world applications.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; edge computing; computer vision; deep neural networks; mobile computing,0.0,EC3,3065
10.1145/3307630.3342385,0,0,0,0,0,93,"Munoz, Daniel-Jesus; Pinto, Mónica; Fuentes, Lidia",HADAS: Analysing Quality Attributes of Software Configurations,Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B,978-1-4503-6668-7,,https://doi.org/10.1145/3307630.3342385,2019,SPLC '19,,,,Association for Computing Machinery,"New York, NY, USA","Software Product Lines (SPLs) are highly configurable systems. Automatic analyses of SPLs rely on solvers to navigate complex dependencies among features and find legal solutions. Variability analysis tools are complex due to the diversity of products and domain-specific knowledge. On that, while there are experimental studies that analyse quality attributes, the knowledge is not easily accessible for developers, and its appliance is not trivial. Aiming to allow the industry to quality-explore SPL design spaces, we developed the HADAS assistant that: (1) models systems and collects quality attributes metrics in a cloud repository, and (2) reasons about it helping developers with quality attributes requirements.",,,,,,,,,,,,,,,,,,,,,,,,,,variability; model; software product line; attribute; NFQA; numerical,0.0,EC3,2518
10.1145/3309705,0,0,0,0,0,369,"Merlino, Giovanni; Dautov, Rustem; Distefano, Salvatore; Bruneo, Dario","Enabling Workload Engineering in Edge, Fog, and Cloud Computing through OpenStack-based Middleware",ACM Trans. Internet Technol.,,1533-5399,https://doi.org/10.1145/3309705,2019-04,,,,,,,"To enable and support smart environments, a recent ICT trend promotes pushing computation from the remote Cloud as close to data sources as possible, resulting in the emergence of the Fog and Edge computing paradigms. Together with Cloud computing, they represent a stacked architecture, in which raw datasets are first pre-processed locally at the Edge and then vertically offloaded to the Fog and/or the Cloud. However, as hardware is becoming increasingly powerful, Edge devices are seen as candidates for offering data processing capabilities, able to pool and share computing resources to achieve better performance at a lower network latency—a pattern that can be also applied to Fog nodes. In these circumstances, it is important to enable efficient, intelligent, and balanced allocation of resources, as well as their further orchestration, in an elastic and transparent manner. To address such a requirement, this article proposes an OpenStack-based middleware platform through which resource containers at the Edge, Fog, and Cloud levels can be discovered, combined, and provisioned to end users and applications, thereby facilitating and orchestrating offloading processes. As demonstrated through a proof of concept on an intelligent surveillance system, by converging the Edge, Fog, and Cloud, the proposed architecture has the potential to enable faster data processing, as compared to processing at the Edge, Fog, or Cloud levels separately. This also allows architects to combine different offloading patterns in a flexible and fine-grained manner, thus providing new workload engineering patterns. Measurements demonstrated the effectiveness of such patterns, even outperforming edge clusters.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; edge; IoT; Fog; big data; offloading; smart environment; Stack4Things,0.0,EC3,1881
10.1145/3318216.3363298,0,0,0,0,0,21,"Nguyen, Chanh; Mehta, Amardeep; Klein, Cristian; Elmroth, Erik",Why cloud applications are not ready for the edge (yet),Proceedings of the 4th ACM/IEEE Symposium on Edge Computing,978-1-4503-6733-2,,https://doi.org/10.1145/3318216.3363298,2019,SEC '19,,,,Association for Computing Machinery,"New York, NY, USA","Mobile Edge Clouds (MECs) are distributed platforms in which distant data-centers are complemented with computing and storage capacity located at the edge of the network. Their wide resource distribution enables MECs to fulfill the need of low latency and high bandwidth to offer an improved user experience.As modern cloud applications are increasingly architected as collections of small, independently deployable services, they can be flexibly deployed in various configurations that combines resources from both centralized datacenters and edge locations. In principle, such applications should therefore be well-placed to exploit the advantages of MECs so as to reduce service response times.In this paper, we quantify the benefits of deploying such cloud micro-service applications on MECs. Using two popular benchmarks, we show that, against conventional wisdom, end-to-end latency does not improve significantly even when most application services are deployed in the edge location. We developed a profiler to better understand this phenomenon, allowing us to develop recommendations for adapting applications to MECs. Further, by quantifying the gains of those recommendations, we show that the performance of an application can be made to reach the ideal scenario, in which the latency between an edge datacenter and a remote datacenter has no impact on the application performance.This work thus presents ways of adapting cloud-native applications to take advantage of MECs and provides guidance for developing MEC-native applications. We believe that both these elements are necessary to drive MEC adoption.",,,,,,,,,,,,,,,,,,,,,,,,,,edge latency; micro-service; mobile application development; mobile edge clouds; profiling,0.0,EC3,2155
10.1145/3318216.3363306,0,0,0,0,0,118,"Boubin, Jayson G.; Babu, Naveen T. R.; Stewart, Christopher; Chumley, John; Zhang, Shiqi",Managing edge resources for fully autonomous aerial systems,Proceedings of the 4th ACM/IEEE Symposium on Edge Computing,978-1-4503-6733-2,,https://doi.org/10.1145/3318216.3363306,2019,SEC '19,,,,Association for Computing Machinery,"New York, NY, USA","Fully autonomous aerial systems (FAAS) fly complex missions guided wholly by software. If users choose software, compute hardware and aircraft well, FAAS can complete missions faster and safer than unmanned aerial systems piloted by humans. On the other hand, poorly managed edge resources slow down missions, waste energy and inflate costs. This paper presents a model-driven approach to manage FAAS. We fly real FAAS missions, profile compute and aircraft resource usage and model expected demands. Naive profiling approaches use traces from previous flights to infer resource usage. However, edge resources can affect where FAAS fly and which data they sense. Usage profiles can diverge greatly across edge management policies. Instead of using traces, we characterize whole flight areas to accurately model resource usage for any flight path. We combine expected resource demands to model mission throughput, i.e., missions completed per fully charged battery. We validated our model by creating FAAS, measuring mission throughput across many system settings. Our FAAS benchmarks, released through our open source FAAS suite SoftwarePilot, execute realistic missions: autonomous photography, search and rescue, and agricultural scouting using well-known software. Our model predicted throughput with 4% error across mission, software and hardware settings. Competing approaches yielded 10–24% error. We used our SoftwarePilot benchmarks to study (1) GPU acceleration, scale up, and scale out, (2) onboard, edge and cloud computing, (3) energy and monetary budgets, and (4) software driven GPU management. We found that model-driven management can boost mission throughput by 10X and reduce costs by 87%.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,744
10.1145/3318216.3363314,0,0,0,0,0,287,"Wolski, Rich; Krintz, Chandra; Bakir, Fatih; George, Gareth; Lin, Wei-Tsung","CSPOT: portable, multi-scale functions-as-a-service for IoT",Proceedings of the 4th ACM/IEEE Symposium on Edge Computing,978-1-4503-6733-2,,https://doi.org/10.1145/3318216.3363314,2019,SEC '19,,,,Association for Computing Machinery,"New York, NY, USA","In this paper, we present CSPOT, a distributed runtime system implementing a functions-as-service (FaaS) programming model for the ""Internet of Things"" (IoT). With FaaS, developers express arbitrary computations as simple functions that are automatically invoked and managed by a cloud platform in response to events. We extend this FaaS model so that it is suitable for use in all tiers of scale for IoT - sensors, edge devices, and cloud - to facilitate robust, portable, and low-latency IoT application development and deployment.To enable this, we combine the use of Linux containers and namespaces for isolation and portability, an append-only object store for robust persistence, and a causal event log for triggering functions and tracking event dependencies. We present the design and implementation of CSPOT, detail its abstractions and APIs, and overview examples of its use. We empirically evaluate the performance of CSPOT using different devices and applications and find that it implements function invocation with significantly lower latency than other FaaS offerings, while providing portability across tiers and similar data durability characteristics.",,,,,,,,,,,,,,,,,,,,,,,,,,serverless; IoT; distributed systems; append-only; cloud functions; portability,,,737
10.1145/3318216.3363335,0,0,0,0,0,119,"Tourani, Reza; Bos, Austin; Misra, Satyajayant; Esposito, Flavio",Towards security-as-a-service in multi-access edge,Proceedings of the 4th ACM/IEEE Symposium on Edge Computing,978-1-4503-6733-2,,https://doi.org/10.1145/3318216.3363335,2019,SEC '19,,,,Association for Computing Machinery,"New York, NY, USA","The prevailing network security measures are often implemented on proprietary appliances that are deployed at fixed network locations with constant capacity. Such a rigid deployment is sometimes necessary, but undermines the flexibility of security services in meeting the demands of emerging applications, such as augmented/virtual reality, autonomous driving, and 5G for industry 4.0, which are provoked by the evolution of connected and smart devices, their heterogeneity, and integration with cloud and edge computing infrastructures.To loosen these rigid security deployments, in this paper, we propose a data-centric SECurity-as-a-Service (SECaaS) framework for elastic deployment and provisioning of security services at the Multi-Access Edge Computing (MEC) infrastructure. In particular, we discuss three security services that are suitable for edge deployment: (i) an intrusion detection and prevention system (IDPS), (ii) an access control enforcement system (ACE), and (iii) a communication anonymization service (CA). We benchmark the common security microservices along with the design and implementation of a proof of concept communication anonymization application.",,,,,,,,,,,,,,,,,,,,,,,,,,security; microservice; edge computing; virtualization; NDN,0.0,EC3,2694
10.1145/3329007.3329009,0,0,0,0,0,299,"Fleischmann, Albert; Stary, Christian","Dependable data sharing in dynamic IoT-systems: subject-oriented process design, complex event processing, and blockchains",Proceedings of the 11th International Conference on Subject-Oriented Business Process Management,978-1-4503-6250-4,,https://doi.org/10.1145/3329007.3329009,2019,S-BPM ONE '19,,,,Association for Computing Machinery,"New York, NY, USA","Internet of Things (IoT) applications are increasingly used in domains critical for society and adapted to individual needs. Thereby, various physical entities, humans and computer programs are dynamically networked while being situated in different locations. For system stakeholders, reliable operation requires intelligible design and adaptability at run time. The more users can adjust IoT systems dynamically, the more transparent has to be how specific requirements can be met. Executable and role-specific process models are a viable starting point, as they represent the behavior of components and the flow of control. However, they need to be extended with effective data exchange mechanisms between IoT system elements. In this paper, we utilize the subject-oriented concept and notation for role-specific process modeling and communication design of IoT systems and enrich them with a data sharing facility, so-called shared input pools. In order to avoid a single point of failure in system operation, we propose to implement them as blockchains. IoT-elements can then interact and exchange data in a semantically correct and robust way, in particular when behavior changes may occur dynamically.",,,,,,,,,,,,,,,,,,,,,,,,,,internet of things; events; reliable communications; S-BPM; shared input pool; subject orientation,0.0,EC3,2772
10.1145/3329786,0,0,0,0,0,408,"Or-Meir, Ori; Nissim, Nir; Elovici, Yuval; Rokach, Lior",Dynamic Malware Analysis in the Modern Era—A State of the Art Survey,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3329786,2019-09,,,,,,,"Although malicious software (malware) has been around since the early days of computers, the sophistication and innovation of malware has increased over the years. In particular, the latest crop of ransomware has drawn attention to the dangers of malicious software, which can cause harm to private users as well as corporations, public services (hospitals and transportation systems), governments, and security institutions. To protect these institutions and the public from malware attacks, malicious activity must be detected as early as possible, preferably before it conducts its harmful acts. However, it is not always easy to know what to look for—especially when dealing with new and unknown malware that has never been seen. Analyzing a suspicious file by static or dynamic analysis methods can provide relevant and valuable information regarding a file's impact on the hosting system and help determine whether the file is malicious or not, based on the method's predefined rules. While various techniques (e.g., code obfuscation, dynamic code loading, encryption, and packing) can be used by malware writers to evade static analysis (including signature-based anti-virus tools), dynamic analysis is robust to these techniques and can provide greater understanding regarding the analyzed file and consequently can lead to better detection capabilities. Although dynamic analysis is more robust than static analysis, existing dynamic analysis tools and techniques are imperfect, and there is no single tool that can cover all aspects of malware behavior. The most recent comprehensive survey performed in this area was published in 2012. Since that time, the computing environment has changed dramatically with new types of malware (ransomware, cryptominers), new analysis methods (volatile memory forensics, side-channel analysis), new computing environments (cloud computing, IoT devices), new machine-learning algorithms, and more. The goal of this survey is to provide a comprehensive and up-to-date overview of existing methods used to dynamically analyze malware, which includes a description of each method, its strengths and weaknesses, and its resilience against malware evasion techniques. In addition, we include an overview of prominent studies presenting the usage of machine-learning methods to enhance dynamic malware analysis capabilities aimed at detection, classification, and categorization.",,,,,,,,,,,,,,,,,,,,,,,,,,behavioral analysis; detection; Dynamic analysis; evasion; malware,,,481
10.1145/3331052.3332473,0,0,0,0,0,62,"Kaur, Kuljeet; Garg, Sahil; Kaddoum, Georges; Ahmed, Syed Hassan; Jayakody, Dushantha Nalin K.",En-OsCo: Energy-aware Osmotic Computing Framework using Hyper-heuristics,Proceedings of the ACM MobiHoc Workshop on Pervasive Systems in the IoT Era,978-1-4503-6805-6,,https://doi.org/10.1145/3331052.3332473,2019,PERSIST-IoT '19,,,,Association for Computing Machinery,"New York, NY, USA","The proliferation of the Internet of Things (IoT) has paved the way for many cloud based applications such as smart grid, healthcare, traffic management, finance, etc. In this vein, the need of transferring large data-streams to remote data centers is a key concern for modern Cloud-based IoT paradigms. This disrupts the remote Cloud Computing model, moving applications, data and computing resources to the logical extremes of the network. Thus, to handle streaming data in IoT environments, an efficient IoT-based computing model that can dynamically handle the interplay between Cloud and Edge data centers is required. In this direction, a recent paradigm, popularly known as Osmotic Computing, has emerged to ensure the acceptable performance of widely dispersed services. However, the burden of data-offloading across multiple data centers usually leads to a consequent increase in their energy consumption which in-turn will affect the overall Quality of Service (QoS) of the IoT-based applications. Keeping focus on all these issues, a consolidated decision making framework for Osmotic Computing, i.e., En-OsCo, is designed to ensure the energy-aware dynamic management of resources. The proposed framework incorporates four significant contributions: i) Resource monitoring of Edge data centers using Extended Kalman Filter, ii) Optimal dispatch of incoming services to the Edge/Cloud setup using Hyper-heuristics, iii) Minimizing the energy consumption of underlying data centers and reducing the service latency, and iv) Reducing the search space of Hyper-heuristics by keeping track of previously made decisions using Universal Streaming Monitoring. Further, in order to validate the efficacy of the proposed En-OsCo framework, ContainerCloudSim has been used in combination of HyFlex on PlanetLab datasets. The obtained results validate the purpose of the proposed scheme in minimizing the overall energy consumption of the computing setup while considerably reducing the latency.",,,,,,,,,,,,,,,,,,,,,,,,,,Cloud Computing; Edge Computing; and Osmotic Computing; Energy Minimization; Extended Kalman Filter; Hyper-heuristics; Latency Minimization,0.0,EC3,1957
10.1145/3331453.3361637,0,0,0,0,0,163,"Cao, Suzhi; Han, Hao; Wei, Junyong; Zhao, Yi; Yang, Shuling; Yan, Lei","Space Cloud-Fog Computing: Architecture, Application and Challenge",Proceedings of the 3rd International Conference on Computer Science and Application Engineering,978-1-4503-6294-8,,https://doi.org/10.1145/3331453.3361637,2019,CSAE '19,,,,Association for Computing Machinery,"New York, NY, USA","With the development of space technology, it is an inevitable trend to build a flexible and efficient computing system for space-based information network. Traditional limited computing in single satellite cannot meet the increasing needs and cannot make effective use of the resources. This paper proposes a space-based cloud-fog computing architecture, which applies cloud-fog computing system to the space-based information network, and then we introduce the deployment of the architecture, and describe its typical application. At last, we analyze the challenges and future development of space cloud-fog computing.",,,,,,,,,,,,,,,,,,,,,,,,,,Edge Computing; Virtualization; Cloud-fog computing system; Space-based information network,0.0,EC3,2503
10.1145/3335484.3335546,0,0,0,0,0,5,"Lin, Pingrong; Lin, Zheyuan; Shi, Xiaoquan",Research on Optimization of Course Selection System Based on Micro service and Dynamic Resource Extension,Proceedings of the 4th International Conference on Big Data and Computing,978-1-4503-6278-8,,https://doi.org/10.1145/3335484.3335546,2019,ICBDC '19,,,,Association for Computing Machinery,"New York, NY, USA","In view of the fact that the course selection system in Colleges and universities has short burst access, large concurrent business and complex business, and combined with the existing course selection system of South China Institute of Software Engineering of Guangzhou University, an optimization scheme based on micro service and dynamic resource extension is constructed. The scheme can solve the problem of low performance of course selection system by building a micro-service architecture, splitting the business services of course selection system, scaling up the cluster of servers, and expanding the resources dynamically. The performance comparison before and after optimization shows that the performance of the course selection system has been greatly improved, and the scheme is feasible and effective.",,,,,,,,,,,,,,,,,,,,,,,,,,micro-service; course selection system; dynamic resource expansion,,,309
10.1145/3337821.3337857,0,0,0,0,0,76,"Hou, Xiaofeng; Liu, Jiacheng; Li, Chao; Guo, Minyi",Unleashing the Scalability Potential of Power-Constrained Data Center in the Microservice Era,Proceedings of the 48th International Conference on Parallel Processing,978-1-4503-6295-5,,https://doi.org/10.1145/3337821.3337857,2019,ICPP '19,,,,Association for Computing Machinery,"New York, NY, USA","Recent scale-out cloud services have undergone a shift from monolithic applications to microservices by putting each functionality into lightweight software containers. Although traditional data center power optimization frameworks excel at per-server or per-rack management, they can hardly make informed decisions when facing microservices that have different QoS requirements on a per-service basis. In a power-constrained data center, blindly budgeting power usage could lead to a power unbalance issue: microservices on the critical path may not receive adequate power budget. This unavoidably hinders the growth of cloud productivity.To unleash the performance potential of cloud in the microservice era, this paper investigates microservice-aware data center resource management. We model microservice using a bipartite graph and propose a metric called microservice criticality factor (MCF) to measure the overall impact of performance scaling on a microservice from the whole application's perspective. We further devise ServiceFridge, a novel system framework that leverages MCF to jointly orchestrate software containers and control hardware power demand. Our detailed case study on a practical microservice application demonstrates that ServiceFridge allows data center to reduce its dynamic power by 25% with slight performance loss. It improves the mean response time by 25.2% and improves the 90th tail latency by 18.0% compared with existing schemes.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,EC3,1689
10.1145/3341105.3374016,0,0,0,0,0,91,"Mendonça, Júlio; Cho, Jin-Hee; Moore, Terrence J.; Nelson, Frederica F.; Lim, Hyuk; Zimmermann, Armin; Kim, Dong Seong",Performability analysis of services in a software-defined networking adopting time-based moving target defense mechanisms,Proceedings of the 35th Annual ACM Symposium on Applied Computing,978-1-4503-6866-7,,https://doi.org/10.1145/3341105.3374016,2020,SAC '20,,,,Association for Computing Machinery,"New York, NY, USA","Moving target defense (MTD) has been developed as an emerging technology to enhance system/network security by randomly and continuously changing attack surface. Despite the significant progress of recent efforts in analyzing the security effectiveness of MTD mechanisms, critical gaps still exist in terms of the impact of running MTD mechanisms on system performance and dependability, exposing a critical design tradeoff between security and performance. To investigate the tradeoff, we propose performability models for evaluating services hosted in software-defined networks with a time-based MTD mechanism being deployed. We developed analytical models for evaluating key performability metrics, in terms of response time, throughput, availability, host utilization, a number of requests lost, and cost (i.e., energy consumption plus profits lost due to dropped jobs). Our results showed that using the time-based MTD mechanism can (1) improve service response time and host utilization; (2) introduce a higher number of requests lost and higher overall cost; and (3) reduce service availability while still handling most of the jobs without much performance degradation.",,,,,,,,,,,,,,,,,,,,,,,,,,moving target defense; performability; petri nets; software-defined networking,0.0,EC3,2425
10.1145/3341105.3374046,0,0,0,0,0,140,"Balteanu, Vasile-Daniel; Neculai, Alexandru; Negru, Catalin; Pop, Florin; Stoica, Adrian",Near real-time scheduling in cloud-edge platforms,Proceedings of the 35th Annual ACM Symposium on Applied Computing,978-1-4503-6866-7,,https://doi.org/10.1145/3341105.3374046,2020,SAC '20,,,,Association for Computing Machinery,"New York, NY, USA","As Cloud-Edge architectures are becoming more and more popular, due to their improvement on the battery life of the IoT devices and the high availability of data from the Cloud, this approach also creates new problems. As data gathered from the Edge has to be transferred to the Cloud in order to be processed, the result will be a decreased responsiveness of the system. Also, devices might have to process data by themselves, as the Cloud could be unreachable at random moments in time, resulting in a reduction in battery life. Therefore, we propose an architecture that solves these problems, by introducing an intermediate layer, called Fog, which uses a task scheduling algorithm to send data received from Edge to another device that has enough resources and the required hardware and software to complete the task. In addition, the architecture is based on microservices, hence improving scalability and flexibility. In the performance analysis, we used different values to find the best node that should receive the data for processing. In addition, we compared the microservice based architecture with a monolithic one in order to see how the throughput and responsiveness of the system are affected.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; task scheduling; edge computing,0.0,EC3,1704
10.1145/3342103,0,0,0,0,0,420,"Wu, Caesar; Buyya, Rajkumar; Ramamohanarao, Kotagiri","Cloud Pricing Models: Taxonomy, Survey, and Interdisciplinary Challenges",ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3342103,2019-10,,,,,,,"This article provides a systematic review of cloud pricing in an interdisciplinary approach. It examines many historical cases of pricing in practice and tracks down multiple roots of pricing in research. The aim is to help both cloud service provider (CSP) and cloud customers to capture the essence of cloud pricing when they need to make a critical decision either to achieve competitive advantages or to manage cloud resource effectively. Currently, the number of available pricing schemes in the cloud market is overwhelming. It is an intricate issue to understand these schemes and associated pricing models clearly due to involving several domains of knowledge, such as cloud technologies, microeconomics, operations research, and value theory. Some earlier studies have introduced this topic unsystematically. Their approaches inevitably lead to much confusion for many cloud decision-makers. To address their weaknesses, we present a comprehensive taxonomy of cloud pricing, which is driven by a framework of three fundamental pricing strategies that are built on nine cloud pricing categories. These categories can be further mapped onto a total of 60 pricing models. Many of the pricing models have been already adopted by CSPs. Others have been widespread across in other industries. We give descriptions of these model categories and highlight both advantages and disadvantages. Moreover, this article offers an extensive survey of many cloud pricing models that were proposed by many researchers during the past decade. Based on the survey, we identify four trends of cloud pricing and the general direction, which is moving from intrinsic value per physical box to extrinsic value per serverless sandbox. We conclude that hyper-converged cloud resources pool supported by cloud orchestration, virtual machine, Open Application Programming Interface, and serverless sandbox will drive the future of cloud pricing.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud price model; Cloud services provider (CSP); cost-based pricing; market-based pricing; value-based pricing,,,1073
10.1145/3342195.3387534,0,0,0,0,0,171,"Zhao, Laiping; Yang, Yanan; Zhang, Kaixuan; Zhou, Xiaobo; Qiu, Tie; Li, Keqiu; Bao, Yungang",Rhythm: component-distinguishable workload deployment in datacenters,Proceedings of the Fifteenth European Conference on Computer Systems,978-1-4503-6882-7,,https://doi.org/10.1145/3342195.3387534,2020,EuroSys '20,,,,Association for Computing Machinery,"New York, NY, USA","Cloud service providers improve resource utilization by co-locating latency-critical (LC) workloads with best-effort batch (BE) jobs in datacenters. However, they usually treat an LC workload as a whole when allocating resources to BE jobs and neglect the different features of components of an LC workload. This kind of coarse-grained co-location method leaves a significant room for improvement in resource utilization.Based on the observation of the inconsistent interference tolerance abilities of different LC components, we propose a new abstraction called Servpod, which is a collection of a LC parts that are deployed on the same physical machine together, and show its merits on building a fine-grained co-location framework. The key idea is to differentiate the BE throughput launched with each LC Servpod, i.e., Servpod with high interference tolerance ability can be deployed along with more BE jobs. Based on Servpods, we present Rhythm, a co-location controller that maximizes the resource utilization while guaranteeing LC service's tail latency requirement. It quantifies the interference tolerance ability of each servpod through the analysis of tail-latency contribution. We evaluate Rhythm using LC services in forms of containerized processes and microservices, and find that it can improve the system throughput by 31.7%, CPU utilization by 26.2%, and memory bandwidth utilization by 34% while guaranteeing the SLA (service level agreement).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,280
10.1145/3344341.3368803,0,0,0,0,0,291,"Yussupov, Vladimir; Breitenbücher, Uwe; Leymann, Frank; Wurster, Michael",A Systematic Mapping Study on Engineering Function-as-a-Service Platforms and Tools,Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing,978-1-4503-6894-0,,https://doi.org/10.1145/3344341.3368803,2019,UCC'19,,,,Association for Computing Machinery,"New York, NY, USA","Function-as-a-Service (FaaS) is a novel cloud service model allowing to develop fine-grained, provider-managed cloud applications. In this work, we investigate which challenges motivate researchers to introduce or enhance FaaS platforms and tools. We use a systematic mapping study method to collect and analyze the relevant scientific literature, which helps us answering the three clearly-defined research questions. We design our study using well-established guidelines and systematically apply it to 62 selected publications. The collected and synthesized data provides useful insights into the main challenges that motivate researchers to work on this topic and can be helpful in identifying research gaps for future research.",,,,,,,,,,,,,,,,,,,,,,,,,,serverless; systematic mapping study; function-as-a-service; faas,,,437
10.1145/3350768.3353813,0,0,0,0,0,4,"Duarte, Lucio Mauro; da Silva Alves, Danilo; Toresan, Bruno Ramos; Maia, Paulo Henrique; Silva, Davi",A Model-based Framework for the Analysis of Software Energy Consumption,Proceedings of the XXXIII Brazilian Symposium on Software Engineering,978-1-4503-7651-8,,https://doi.org/10.1145/3350768.3353813,2019,SBES '19,,,,Association for Computing Machinery,"New York, NY, USA","Software is present in all types of devices, some of them with restrictions as to the amount of energy they can spend to execute software applications. For this reason, energy costs are becoming an important factor during software development and evolution. However, there is still little support for creating energy-efficient software. In this work, we introduce a possible framework for software energy costs evaluation based on model analysis. We model software as Labelled Transitions Systems (LTS) and annotate these models with energy costs, which can be obtained using existing tools. We can then apply graph-based algorithms to traverse the models to obtain information about energy consumption related to software behaviour, such as its most/least costly execution, the cost of a specific execution, and the average cost of executing the software. No existing tool currently provides all the necessary analyses, even though they are essential for energy-consumption evaluation. We have conducted a small experiment with our framework where we employed jRAPL to measure energy costs. We annotated the models with the collected energy costs using an extended version of the LoTuS tool, where we have also implemented some of the desired analyses. Based on this support and on our initial results, we believe developers could create software more energy-efficient and consider possible trade-offs related to time, space, and energy costs when producing new versions of their systems.",,,,,,,,,,,,,,,,,,,,,,,,,,Embedded Software; Energy Consumption Evaluation; Labelled Transition System; Model-based Analysis,0.0,EC3,1691
10.1145/3355399,0,0,0,0,0,348,"Liu, Xunyun; Buyya, Rajkumar","Resource Management and Scheduling in Distributed Stream Processing Systems: A Taxonomy, Review, and Future Directions",ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3355399,2020-05,,,,,,,"Stream processing is an emerging paradigm to handle data streams upon arrival, powering latency-critical application such as fraud detection, algorithmic trading, and health surveillance. Though there are a variety of Distributed Stream Processing Systems (DSPSs) that facilitate the development of streaming applications, resource management and task scheduling is not automatically handled by the DSPS middleware and requires a laborious process to tune toward specific deployment targets. As the advent of cloud computing has supported renting resources on-demand, it is of great interest to review the research progress of hosting streaming systems in clouds under certain Service Level Agreements (SLA) and cost constraints. In this article, we introduce the hierarchical structure of streaming systems, define the scope of the resource management problem, and present a comprehensive taxonomy in this context covering critical research topics such as resource provisioning, operator parallelisation, and task scheduling. The literature is then reviewed following the taxonomy structure, facilitating a deeper understanding of the research landscape through classification and comparison of existing works. Finally, we discuss the open issues and future research directions toward realising an automatic, SLA-aware resource management framework.",,,,,,,,,,,,,,,,,,,,,,,,,,task scheduling; distributed stream processing systems; Resource management; stream processing,0.0,EC3,2860
10.1145/3357223.3362737,0,0,0,0,0,193,"Golestani, Hossein; Mirhosseini, Amirhossein; Wenisch, Thomas F.",Software Data Planes: You Can't Always Spin to Win,Proceedings of the ACM Symposium on Cloud Computing,978-1-4503-6973-2,,https://doi.org/10.1145/3357223.3362737,2019,SoCC '19,,,,Association for Computing Machinery,"New York, NY, USA","Today's datacenters demand high-performance, energy-efficient software data planes, which are widely used in many areas including fast network packet processing, network function virtualization, high-speed data transfer in storage systems, and I/O virtualization. Modern software data planes bypass OS I/O stacks and rely on cores spinning on user-level queues as a fast notification mechanism. Whereas spin-polling can improve latency and throughput, it entails significant shortcomings, especially when scaling to large numbers of cores/queues. In this paper, we pinpoint and quantify challenges of spin-polling–based software data planes using Intel's Data Plane Development Kit (DPDK) as a representative infrastructure. We characterize four scalability issues of software data planes: (1) Full-tilt spinning cores perform more (useless) polling work when there is less work pending in the queues; (2) Spin-polling scales poorly with the number of polled queues due to processor cache capacity constraints, especially when traffic is unbalanced; (3) Operation rate limits (transactions per second) as well as a Polling Tax (the overhead of polling, which is considerable even when operating at saturation throughput) result in poor core scalability. (4) Whereas shared queues can mitigate load imbalance and head-of-line-blocking, synchronization overheads limit their potential benefits. We identify root causes of these issues and discuss solution directions to improve hardware and software abstractions for better performance, efficiency, and scalability in software data planes.",,,,,,,,,,,,,,,,,,,,,,,,,,data planes; spin-polling; user-level queues,,,478
10.1145/3357390.3361027,0,0,0,0,0,106,"Hartley, Tim; Zakkak, Foivos S.; Kotselidis, Christos; Luján, Mikel",An analysis of call-site patching without strong hardware support for self-modifying-code,Proceedings of the 16th ACM SIGPLAN International Conference on Managed Programming Languages and Runtimes,978-1-4503-6977-0,,https://doi.org/10.1145/3357390.3361027,2019,MPLR 2019,,,,Association for Computing Machinery,"New York, NY, USA","With micro-services continuously gaining popularity and low-power processors making their way into data centers, efficient execution of managed runtime systems on low-power architectures is also gaining interest. Apart from the inherent performance differences between high and low power processors, porting a managed runtime system to a low-power architecture may result in spuriously introducing additional overheads and design trade-offs. In this work we investigate how the lack of strong hardware support for Self Modifying Code (SMC) in low-power architectures, influences Just-In-Time (JIT) compilation and execution in modern virtual machines. In particular, we examine how low-power architectures, with no or limited hardware support for SMC, impose restrictions on call-site implementations, when the latter need to be patchable by the runtime system. We present four different memory-safe implementations for call-site generation and discuss their advantages and disadvantages in the absence of strong hardware support for SMC. Finally, we evaluate each technique on different workloads using micro-benchmarks and we evaluate the best two techniques on the Dacapo benchmark suite showcasing performance differences up to 15%.",,,,,,,,,,,,,,,,,,,,,,,,,,AArch64; JIT compilation; RISC; Self modifying code,0.0,EC3,2098
10.1145/3360774.3360790,0,0,0,0,0,75,"Bosak, T.; Jantos, P.; Boryczko, K.",Simulation of a big number of microservices in a highly distributed vast network,"Proceedings of the 16th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services",978-1-4503-7283-1,,https://doi.org/10.1145/3360774.3360790,2020,MobiQuitous '19,,,,Association for Computing Machinery,"New York, NY, USA","The rapidly increasing number of instances and types of services, especially microservices, and the growing complexity of systems create uncertainty if the existing algorithms and solutions for such problems as service discovery or load balancing are still the case. Building a huge infrastructure for the verification purposes is often too costly and impractical in the production environment therefore it is necessary to find other ways to test architectural approaches and algorithms. One of them is simulators. For simulating very big systems like a telecommunications radio access network the simulator should have certain features: capability of simulating a huge number of services, possibility to model heterogenic and vast network, facility to add a new type of services and logic. This paper presents a concept of such a simulator of services.",,,,,,,,,,,,,,,,,,,,,,,,,,microservices; service discovery; simulation; edge cloud,0.0,EC3,2553
10.1145/3361149.3361174,0,0,0,0,0,55,"Dobaj, Jürgen; Schuss, Markus; Krisper, Michael; Boano, Carlo Alberto; Macher, Georg",Dependable mesh networking patterns,Proceedings of the 24th European Conference on Pattern Languages of Programs,978-1-4503-6206-1,,https://doi.org/10.1145/3361149.3361174,2019,EuroPLop '19,,,,Association for Computing Machinery,"New York, NY, USA","In our daily life, we are increasingly relying on connected systems ranging from smart health care devices to industrial and intelligent transportation systems, as well as smart homes and cities. The unavailability or malfunctioning of these systems could threaten human life, cause environmental damage, and significant financial loss. To prevent such large scale and mission-critical systems from malfunctioning, it is of utmost importance to establish and guaranty reliable connections to attain a dependable networked system. Generally, mesh networking technologies are used for building such systems since mesh networks provide the best performance characteristics regarding fault-tolerance, throughput, resource usage, and service level flexibility.In this paper, we summarize the major challenges in dependable network design, to subsequently present three patterns that approach redundancy on the hardware level, software-defined networking, and cross-cutting concerns like monitoring and service discovery within distributed networked systems. These three patterns should help designers and engineers in choosing the appropriate technologies for building dependable networked systems at all scales. Since dependable network engineering requires a holistic system-wide design and engineering approach, we also present a pattern map guiding to complementary and closely related patterns. System architects and system engineers responsible for building mixed-criticality systems, internet-of-things (IoT), and industrial Internet-of-Things (IIoT) systems are the target audience of the patterns presented in this paper.",,,,,,,,,,,,,,,,,,,,,,,,,,microservices; cloud architecture; dependability; IaaS; IIoT; industry 4.0; infrastructure-as-a-service; IoT; logical mesh pattern; network architecture; network design; physical mesh pattern; service mesh pattern; service-oriented architecture; SoA,0.0,EC3,2151
10.1145/3361525.3361538,0,0,0,0,0,249,"Jayaram, K. R.; Muthusamy, Vinod; Dube, Parijat; Ishakian, Vatche; Wang, Chen; Herta, Benjamin; Boag, Scott; Arroyo, Diana; Tantawi, Asser; Verma, Archit; Pollok, Falk; Khalaf, Rania",FfDL: A Flexible Multi-tenant Deep Learning Platform,Proceedings of the 20th International Middleware Conference,978-1-4503-7009-7,,https://doi.org/10.1145/3361525.3361538,2019,Middleware '19,,,,Association for Computing Machinery,"New York, NY, USA","Deep learning (DL) is becoming increasingly popular in several application domains and has made several new application features involving computer vision, speech recognition and synthesis, self-driving automobiles, drug design, etc. feasible and accurate. As a result, large scale ""on-premise"" and ""cloud-hosted"" deep learning platforms have become essential infrastructure in many organizations. These systems accept, schedule, manage and execute DL training jobs at scale.This paper describes the design, implementation and our experiences with FfDL, a DL platform used at IBM. We describe how our design balances dependability with scalability, elasticity, flexibility and efficiency. We examine FfDL qualitatively through a retrospective look at the lessons learned from building, operating, and supporting FfDL; and quantitatively through a detailed empirical evaluation of FfDL, including the overheads introduced by the platform for various DL models, the load and performance observed in a real case study using FfDL within our organization, the frequency of various faults observed including faults that we did not anticipate, and experiments demonstrating the benefits of various scheduling policies. FfDL has been open-sourced.",,,,,,,,,,,,,,,,,,,,,,,,,,cluster scheduling; deep learning platform; fault tolerance; gang scheduling,,,366
10.1145/3362031,0,0,0,0,0,362,"Ren, Ju; Zhang, Deyu; He, Shiwen; Zhang, Yaoxue; Li, Tao","A Survey on End-Edge-Cloud Orchestrated Network Computing Paradigms: Transparent Computing, Mobile Edge Computing, Fog Computing, and Cloudlet",ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3362031,2019-10,,,,,,,"Sending data to the cloud for analysis was a prominent trend during the past decades, driving cloud computing as a dominant computing paradigm. However, the dramatically increasing number of devices and data traffic in the Internet-of-Things (IoT) era are posing significant burdens on the capacity-limited Internet and uncontrollable service delay. It becomes difficult to meet the delay-sensitive and context-aware service requirements of IoT applications by using cloud computing alone. Facing these challenges, computing paradigms are shifting from the centralized cloud computing to distributed edge computing. Several new computing paradigms, including Transparent Computing, Mobile Edge Computing, Fog Computing, and Cloudlet, have emerged to leverage the distributed resources at network edge to provide timely and context-aware services. By integrating end devices, edge servers, and cloud, they form a hierarchical IoT architecture, i.e., End-Edge-Cloud orchestrated architecture to improve the performance of IoT systems. This article presents a comprehensive survey of these emerging computing paradigms from the perspective of end-edge-cloud orchestration. Specifically, we first introduce and compare the architectures and characteristics of different computing paradigms. Then, a comprehensive survey is presented to discuss state-of-the-art research in terms of computation offloading, caching, security, and privacy. Finally, some potential research directions are envisioned for fostering continuous research efforts.",,,,,,,,,,,,,,,,,,,,,,,,,,fog computing; cloudlet; End-edge-cloud orchestration; mobile edge computing; network computing; transparent computing,0.0,EC3,2205
10.1145/3365871.3365892,0,0,0,0,0,28,"Mehran, Narges; Kimovski, Dragi; Prodan, Radu",MAPO: A Multi-Objective Model for IoT Application Placement in a Fog Environment,Proceedings of the 9th International Conference on the Internet of Things,978-1-4503-7207-7,,https://doi.org/10.1145/3365871.3365892,2019,IoT '19,,,,Association for Computing Machinery,"New York, NY, USA","The emergence of the Fog computing paradigm that leverages innetwork virtualized resources raises important challenges in terms of resource and IoT application management in a heterogeneous environment with limited computing resources. In this work, we propose a novel Pareto-based approach for application placement close to the data sources called Multi-objective IoT Application Placement in fOg (MAPO). MAPO models applications based on a finite state machine using three conflicting optimization objectives, completion time, energy consumption, and economic cost, and considering both the computation and communication aspects. In contrast to existing solutions that optimize a single objective, MAPO enables multi-objective energy and cost-aware application placement. To evaluate the quality of the MAPO placements, we created both simulated and real-world testbeds tailored for a set of medical IoT application case studies. Compared to the state-of-the-art approaches, MAPO reduces the economic cost by 28%, while decreasing the energy requirements by 29-64% on average, and improves the completion time by a factor of six.",,,,,,,,,,,,,,,,,,,,,,,,,,energy consumption; Fog computing; IoT application placement; multi-objective optimization,0.0,EC3,2318
10.1145/3366030.3366093,0,0,0,0,0,225,"Markoska, Elena; Lazarova-Molnar, Sanja",The Synergy of Simulation and Time Series Forecasting for Live Performance Testing of Smart Buildings,Proceedings of the 21st International Conference on Information Integration and Web-Based Applications &amp; Services,978-1-4503-7179-7,,https://doi.org/10.1145/3366030.3366093,2020,iiWAS2019,,,,Association for Computing Machinery,"New York, NY, USA","Differences in requirements for reliability in buildings imply the different needs for calculation of expected building behaviour. In this paper we examine four techniques for calculating expected behaviour of buildings. Two of them are simulation techniques, namely, a white box EnergyPlus model and a æ static tool as per the requirements of the Danish government. The other two are machine learning techniques, namely an ARIMA model, and an long short-term memory artificial recurrent neural network, used in deep learning. We compare and contrast these four techniques based on their accuracy of forecast, as well as execution time to forecast a new data point. Furthermore, we provide an algorithm for selection of forecasting technique based on terms such as availability, accuracy, and execution time requirements, to facilitate real time threshold generation in light of building performance testing.",,,,,,,,,,,,,,,,,,,,,,,,,,deep learning; smart buildings; performance testing; Real-time performance evaluation; threshold discovery,0.0,EC3,2685
10.1145/3366623.3368134,0,0,0,0,0,61,"Huber, Florian; Körber, Nikolai; Mock, Markus",Selena: a Serverless Energy Management System,Proceedings of the 5th International Workshop on Serverless Computing,978-1-4503-7038-7,,https://doi.org/10.1145/3366623.3368134,2019,WOSC '19,,,,Association for Computing Machinery,"New York, NY, USA","Reduction of CO2 emissions has become a significant challenge faced by humanity today. Energy management systems try to contribute to addressing this challenge by enabling an intelligent use and combination of different energy sources by capturing and visualizing energy usage and production data to enable energy efficiency improvement measures. In this paper, we present Selena, a prototypical energy management system that is implemented using the serverless computing paradigm. Essential design goals for Selena are both extensibility, so that many different data sources and providers (e.g., measurement systems) can be integrated easily, as well as efficient scalability, so that the system can be used from small (e.g., one building) to large installations (potentially entire neighborhoods) with deployment cost commensurate with the installation size. Our initial experiences with Selena indicate that the serverless paradigm is very well suited to capture and process energy-related data reliably and has excellent scaling properties due to the elastic compute platform that it is built upon.",,,,,,,,,,,,,,,,,,,,,,,,,,microservices; IoT; AWS Lambda; energy management systems; FaaS; serverless computing,0.0,EC3,2187
10.1145/3368036,0,0,0,0,0,387,"Hilman, Muhammad H.; Rodriguez, Maria A.; Buyya, Rajkumar",Multiple Workflows Scheduling in Multi-tenant Distributed Systems: A Taxonomy and Future Directions,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3368036,2020-02,,,,,,,"Workflows are an application model that enables the automated execution of multiple interdependent and interconnected tasks. They are widely used by the scientific community to manage the distributed execution and dataflow of complex simulations and experiments. As the popularity of scientific workflows continue to rise, and their computational requirements continue to increase, the emergence and adoption of multi-tenant computing platforms that offer the execution of these workflows as a service becomes widespread. This article discusses the scheduling and resource provisioning problems particular to this type of platform. It presents a detailed taxonomy and a comprehensive survey of the current literature and identifies future directions to foster research in the field of multiple workflow scheduling in multi-tenant distributed computing systems.",,,,,,,,,,,,,,,,,,,,,,,,,,multi-tenant platforms; multiple workflows scheduling; Scientific workflows,,,146
10.1145/3368235.3368838,0,0,0,0,0,276,"Harsh, Piyush; Ribera Laszkowski, Juan Francisco; Edmonds, Andy; Quang Thanh, Tran; Pauls, Michael; Vlaskovski, Radoslav; Avila-García, Orlando; Pages, Enric; Gortázar Bellas, Francisco; Gallego Carrillo, Micael",Cloud Enablers For Testing Large-Scale Distributed Applications,Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing Companion,978-1-4503-7044-8,,https://doi.org/10.1145/3368235.3368838,2019,UCC '19 Companion,,,,Association for Computing Machinery,"New York, NY, USA","Testing large-scale distributed systems (also known as testing in the large) is a challenge that spreads across different technical domains and areas of expertise. Current methods and tools provide some minimal guarantees in relation to the correctness of their functional properties and have serious limitations when evaluating their extra-functional properties in realistic conditions, such as scalability, availability and performance efficiency. Cloud Testing and more specifically ""testing in the cloud” has arisen to tackle those challenges. In this new paradigm, cloud-based environment and infrastructure are used to run realistic end-to-end and/or system-level tests, collect test data and analyse them. In this paper we present a set of cloud-native services to take from the tester the responsibility of managing the resources and complementary services required to simulate realistic operational conditions and production environments. Specifically, they provide cloud testing capabilities such as logs and measurements collection from both testing jobs and system under test; test data analytics and visualization; provisioning and operation of additional services and processes to replicate realistic production ecosystems; support to scalability and diversity of underlying testing infrastructure; and replication of the operational conditions of the software under test through its instrumentation. We present the architecture of the cloud testing solution and the detailed design of each of the services; we also evaluate their relative contribution to satisfy different needs in the context of test execution.",,,,,,,,,,,,,,,,,,,,,,,,,,scalability; testing; continuous integration; cloud testing; continuous testing; large-scale distributed systems; reliability,0.0,EC3,2175
10.1145/3368235.3368847,0,0,0,0,0,177,"Abdul Majeed, Ayesha; Kilpatrick, Peter; Spence, Ivor; Varghese, Blesson",Performance Estimation of Container-Based Cloud-to-Fog Offloading,Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing Companion,978-1-4503-7044-8,,https://doi.org/10.1145/3368235.3368847,2019,UCC '19 Companion,,,,Association for Computing Machinery,"New York, NY, USA","Fog computing offloads latency critical services of a Cloud application onto resources located at the edge of the network that are in close proximity to end-user devices. The research in this paper is motivated towards characterising and estimating the time taken to offload a service using containers, which is investigated in the context of the 'Save and Load' container migration technique. To this end, the research addresses questions such as whether fog offloading can be accurately modelled and which system and network related parameters influence offloading. These are addressed by exploring a catalogue of 21 different metrics both at the system and process levels that is used as input to four estimation techniques using a collective model and individual models to predict the time taken for offloading. The study is pursued by collecting over 1.1 million data points and the preliminary results indicate that offloading can be modelled accurately.",,,,,,,,,,,,,,,,,,,,,,,,,,containers; edge computing; fog computing; offloading,0.0,EC3,2594
10.1145/3368235.3368848,0,0,0,0,0,127,"Chegini, Hossein; Mahanti, Aniket",A Framework of Automation on Context-Aware Internet of Things (IoT) Systems,Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing Companion,978-1-4503-7044-8,,https://doi.org/10.1145/3368235.3368848,2019,UCC '19 Companion,,,,Association for Computing Machinery,"New York, NY, USA","An ever-increasing number of different types of objects are connecting to the Internet, and this phenomenon is called the Internet of Things(IoT). Processing the IoT generated data by Cloud Computing causes high latency. Fog Computing is a new motivation for resolving the latency issue, which is a hosting environment between the IoT and the Cloud layers. IoT applications are faced with three significant challenges: big data, device heterogeneity, and Fog resiliency. With the motivation of resolving the challenges, this proposal introduces a Microservice software framework for implementing automatic functions in the IoT-Fog-Cloud ecosystem. The proposed Microservice framework will also enable the development of IoT-based context-aware intelligent decision-making systems. We describe the functionality and contribution of each automatic function in the paper.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; microservice; fog computing; iot; automation; complex event processing; context-aware system; machine-to-machine(m2m),0.0,EC3,2273
10.1145/3369740.3369793,0,0,0,0,0,219,"Ramisetty, Rajeswara Rao; Qu, Chengyi; Aktar, Rumana; Wang, Songjie; Calyam, Prasad; Palaniappan, Kannappan",Dynamic Computation Off-loading and Control based on Occlusion Detection in Drone Video Analytics,Proceedings of the 21st International Conference on Distributed Computing and Networking,978-1-4503-7751-5,,https://doi.org/10.1145/3369740.3369793,2020,ICDCN '20,,,,Association for Computing Machinery,"New York, NY, USA","Unmanned Aerial Vehicles (UAVs) or drones equipped with cameras are extensively used in different scenarios such as surveillance of hazardous locations, disaster response and crime fighting. The related video streaming/analytics requires real-time drone-to-Ground Control Station (GCS) communication and computation co-ordination for desired user Quality of Experience (QoE). In situations where the quality of the video can be affected by occlusions (e.g., image distortion, frame stalling) due to network bottlenecks, there is a need to dynamically make decisions on the computation offloading and networking protocols in order to properly handle the video data for real world application purposes. In this paper, we propose a novel function-centric computing approach that helps a user to perform drone video analytics to assess a wide-area scene to chart a plan of action. Our approach involves handling network impairments affecting the switching between high resolution/low resolution video capture, or change of camera direction for assessment of the scene effectively. It also features a novel video quality enhancing algorithm based on occlusion-detection that adapts to video impairments related to image distortion and frame stalling. Our experiment results from a realistic testbed show that our approach can efficiently choose the suitable networking protocols (i.e., TCP/HTTP, UDP/RTP, QUIC) and orchestrate both the camera control on the drone, and the computation off-loading of the video analytics over limited edge computing resources. The performance improvements for computation off-loading involving our video quality enhancing algorithm are shown for different network conditions in terms of occlusion rate and processing times.",,,,,,,,,,,,,,,,,,,,,,,,,,Computation off-loading; Drone video analytics; Multimedia networking protocols; Network management; Occlusion detection,,,385
10.1145/3373087.3375305,0,0,0,0,0,124,"Nguyen, Tuan D. A.; Kumar, Akash",Maximizing the Serviceability of Partially Reconfigurable FPGA Systems in Multi-tenant Environment,Proceedings of the 2020 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,978-1-4503-7099-8,,https://doi.org/10.1145/3373087.3375305,2020,FPGA '20,,,,Association for Computing Machinery,"New York, NY, USA","In cloud computing, software is transitioning from monolithic to microservices architecture to improve the maintainability, upgradability and the flexibility of the applications. They are able to request a service with different implementations of the same functionality, including hardware accelerator, depending on cost and performance. This model opens up a new opportunity to integrate reconfigurable hardware, specifically, FPGA, in the cloud to offer such services. There are many research works discussing solutions for this problem but they focus primarily on the high-level aspects of resource manager, hypervisor or hardware architecture. The low-level physical design choices of FPGA to maximize the accelerator allocation success rate (called serviceability) is largely untouched. In this paper, we propose a design space exploration algorithm to determine the best configuration of partially reconfigurable regions (PRRs) to host the accelerators. Besides, the algorithm is capable of estimating the actual resources occupied by the PRRs on the FPGA even before floorplanning. We systematically study the effects of having more PRRs on the system in various aspects, i.e., serviceability, waiting time and resource wastage. The experiments show that at a certain number of PRRs, upto 91% serviceability can be achieved for 12 concurrent users. It is a significant improvement from 52% without our approach. The average amount of time that each request has to wait to be served is also reduced by 6.3X. Furthermore, the cumulative unused FPGA resources is reduced almost by half.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud; microservice; design automation; design space exploration; floorplan; fpga; partial reconfiguration,0.0,EC3,2793
10.1145/3375714,0,0,0,0,0,385,"Li, Yuhao; Sun, Dan; Lee, Benjamin C.",Dynamic Colocation Policies with Reinforcement Learning,ACM Trans. Archit. Code Optim.,,1544-3566,https://doi.org/10.1145/3375714,2020-03,,,,,,,"We draw on reinforcement learning frameworks to design and implement an adaptive controller for managing resource contention. During runtime, the controller observes the dynamic system conditions and optimizes control policies that satisfy latency targets yet improve server utilization. We evaluate a physical prototype that guarantees 95th percentile latencies for a search engine and improves server utilization by up to 70%, compared to exclusively reserving servers for interactive services, for varied batch workloads in machine learning.",,,,,,,,,,,,,,,,,,,,,,,,,,machine learning; adaptive control; Resource contention,0.0,EC3,2942
10.1145/3385896,0,0,0,0,0,394,"Macedo, Ricardo; Paulo, João; Pereira, José; Bessani, Alysson",A Survey and Classification of Software-Defined Storage Systems,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3385896,2020-05,,,,,,,"The exponential growth of digital information is imposing increasing scale and efficiency demands on modern storage infrastructures. As infrastructure complexity increases, so does the difficulty in ensuring quality of service, maintainability, and resource fairness, raising unprecedented performance, scalability, and programmability challenges. Software-Defined Storage (SDS) addresses these challenges by cleanly disentangling control and data flows, easing management, and improving control functionality of conventional storage systems. Despite its momentum in the research community, many aspects of the paradigm are still unclear, undefined, and unexplored, leading to misunderstandings that hamper the research and development of novel SDS technologies. In this article, we present an in-depth study of SDS systems, providing a thorough description and categorization of each plane of functionality. Further, we propose a taxonomy and classification of existing SDS solutions according to different criteria. Finally, we provide key insights about the paradigm and discuss potential future research directions for the field.",,,,,,,,,,,,,,,,,,,,,,,,,,distributed storage; Software-defined storage; storage infrastructures,0.0,EC3,2663
10.1145/3388922,0,0,0,0,0,380,"Welsh, Thomas; Benkhelifa, Elhadj",On Resilience in Cloud Computing: A Survey of Techniques across the Cloud Domain,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3388922,2020-05,,,,,,,"Cloud infrastructures are highly favoured as a computing delivery model worldwide, creating a strong societal dependence. It is therefore vital to enhance their resilience, providing persistent service delivery under a variety of conditions. Cloud environments are highly complex and continuously evolving. Additionally, the plethora of use-cases ensures requirements for persistent service delivery vary. As a contribution to knowledge, this work surveys resilience techniques for cloud environments. We apply a novel perspective using a layered model of traditional and emerging cloud paradigms. Works are then classified according to the Resilinets model. For each layer, the most common techniques with limitations are derived including an actor’s strength in influencing resilience in the cloud with each technique. We conclude with some future challenges to the field of resilient cloud computing.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud; edge; survey; fog; Resilience,,,1249
10.1145/3395363.3397365,0,0,0,0,0,281,"Kadron, undefinedsmet Burak; Rosner, Nicolás; Bultan, Tevfik",Feedback-driven side-channel analysis for networked applications,Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis,978-1-4503-8008-9,,https://doi.org/10.1145/3395363.3397365,2020,ISSTA 2020,,,,Association for Computing Machinery,"New York, NY, USA","Information leakage in software systems is a problem of growing importance. Networked applications can leak sensitive information even when they use encryption. For example, some characteristics of network packets, such as their size, timing and direction, are visible even for encrypted traffic. Patterns in these characteristics can be leveraged as side channels to extract information about secret values accessed by the application. In this paper, we present a new tool called AutoFeed for detecting and quantifying information leakage due to side channels in networked software applications. AutoFeed profiles the target system and automatically explores the input space, explores the space of output features that may leak information, quantifies the information leakage, and identifies the top-leaking features. Given a set of input mutators and a small number of initial inputs provided by the user, AutoFeed iteratively mutates inputs and periodically updates its leakage estimations to identify the features that leak the greatest amount of information about the secret of interest. AutoFeed uses a feedback loop for incremental profiling, and a stopping criterion that terminates the analysis when the leakage estimation for the top-leaking features converges. AutoFeed also automatically assigns weights to mutators in order to focus the search of the input space on exploring dimensions that are relevant to the leakage quantification. Our experimental evaluation on the benchmarks shows that AutoFeed is effective in detecting and quantifying information leaks in networked applications.",,,,,,,,,,,,,,,,,,,,,,,,,,dynamic program analysis; input generation; network traffic analysis; Side-channel analysis,,,1287
10.1145/3396374,0,0,0,0,0,417,"Ali, Isra Mohamed; Caprolu, Maurantonio; Pietro, Roberto Di","Foundations, Properties, and Security Applications of Puzzles: A Survey",ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3396374,2020-08,,,,,,,"Cryptographic algorithms have been used not only to create robust ciphertexts but also to generate cryptograms that, contrary to the classic goal of cryptography, are meant to be broken. These cryptograms, generally called puzzles, require the use of a certain amount of resources to be solved, hence introducing a cost that is often regarded as a time delay—though it could involve other metrics as well, such as bandwidth. These powerful features have made puzzles the core of many security protocols, acquiring increasing importance in the IT security landscape. The concept of a puzzle has subsequently been extended to other types of schemes that do not use cryptographic functions, such as CAPTCHAs, which are used to discriminate humans from machines. Overall, puzzles have experienced a renewed interest with the advent of Bitcoin, which uses a CPU-intensive puzzle as proof of work. In this article, we provide a comprehensive study of the most important puzzle construction schemes available in the literature, categorizing them according to several attributes, such as resource type, verification type, and applications. We have redefined the term puzzle by collecting and integrating the scattered notions used in different works, to cover all the existing applications. Moreover, we provide an overview of the possible applications, identifying key requirements and different design approaches. Finally, we highlight the features and limitations of each approach, providing a useful guide for the future development of new puzzle schemes.",,,,,,,,,,,,,,,,,,,,,,,,,,Bandwidth-bound puzzle; Bitcoin; CAPTCHA; CPU-bound puzzle; Cryptocurrency; Cryptographic puzzle; Human-bound puzzle; Memory-bound puzzle; Network-bound puzzle; Proof-of-Work,,,918
10.1145/3397022,0,0,0,0,0,397,"Fei, Xincai; Liu, Fangming; Zhang, Qixia; Jin, Hai; Hu, Hongxin","Paving the Way for NFV Acceleration: A Taxonomy, Survey and Future Directions",ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3397022,2020-08,,,,,,,"As a recent innovation, network functions virtualization (NFV)—with its core concept of replacing hardware middleboxes with software network functions (NFs) implemented in commodity servers—promises cost savings and flexibility benefits. However, transitioning NFs from special-purpose hardware to commodity servers has turned out to be more challenging than expected, as it inevitably incurs performance penalties due to bottlenecks in both software and hardware. To achieve performance comparable to hardware middleboxes, there is a strong demand for a speedup in NF processing, which plays a crucial role in the success of NFV. In this article, we study the performance challenges that exist in general-purpose servers and simultaneously summarize the typical performance bottlenecks in NFV. Through reviewing the progress in the field of NFV acceleration, we present a new taxonomy of the state-of-the-art efforts according to various acceleration approaches. We discuss the surveyed works and identify the respective advantages and disadvantages in each category. We then discuss the products, solutions, and projects emerged in industry. We also present a gap analysis to improve current solutions and highlight promising research trends that can be explored in the future.",,,,,,,,,,,,,,,,,,,,,,,,,,high performance; Network functions virtualization; NFV acceleration,,,559
10.1145/3399579.3399863,0,0,0,0,0,288,"Singhal, Rekha; Chahal, Dheeraj; Kunde, Shruti; Mishra, Mayank; Nambiar, Manoj",A Vision on Accelerating Enterprise IT System 2.0,Proceedings of the Fourth International Workshop on Data Management for End-to-End Machine Learning,978-1-4503-8023-2,,https://doi.org/10.1145/3399579.3399863,2020,DEEM '20,,,,Association for Computing Machinery,"New York, NY, USA","The proliferation of commodity based big data platforms and an exponential increase in the research in machine learning techniques lead to a change in application development paradigm from traditional control-flow Software 1.0 to data-flow Software 2.0 programming paradigm e.g. use of machine learning based models over customer-scoring methods for generating recommendations. The Software 2.0 paradigm is a data-driven programming that requires specialized data management to get clean, governed and unbiased data sets, well defined neural network architectures for building a model, efficient model training, extensive testing and high performance deployment. Unlike Software 1.0 paradigm, a Software 2.0 program's output is probabilistic in nature as the correctness is highly dependent on the size and quality of the input data, however the program's performance is deterministic. This has led to the research in specialized hardware and high performance architectures for deep-learning algorithms. Also, the nature of Software 2.0 paradigm brings in heterogeneity in the whole life cycle starting from an application development until its deployment in the production environment and hence posing numerous architecture and performance challenges.In this paper, we outline the research problems that will emerge due to migration of apart of Software 1.0 to Software 2.0. We present the challenges and the approaches to address them, for accelerating the development and deployment of Software 2.0 programs. We also envision evolution of existing enterprise IT systems to the data-driven enterprise IT systems, referred to as EIT 2.0. We have compared a conventional development life-cycle of applications with that in EIT 2.0. We address research problems and approaches with the related state-of-art in the performance engineering of modern enterprise applications during its life cycle in EIT 2.0.",,,,,,,,,,,,,,,,,,,,,,,,,,Accelerating EIT 2.0; data driven IT; Enterprise IT 2.0; Software 2.0,0.0,EC3,2107
10.1145/3401895.3402082,0,0,0,0,0,94,"Souza, Layse Santos; de Almeida Neto, João Ribeiro; Melo, Thiago José Sandes; Soares, Michel dos Santos; Rocha, Fabio Gomes; Nascimento, Rogério Patrício Chagas do",An investigation into technology solutions for urban public monitoring through ubiquitous devices in the context of smart cities,Proceedings of the 10th Euro-American Conference on Telematics and Information Systems,978-1-4503-7711-9,,https://doi.org/10.1145/3401895.3402082,2021,EATIS '20,,,,Association for Computing Machinery,"New York, NY, USA","Context: Socioeconomic development has accelerated the urbanization process and the advance of Information and Communication Technologies (ICT). In this context, population growth in urban centers manifested challenges in the economic and cultural aspects of society. Issues such as mobility and sustainability come up. ICT plays an important role in this scenario, as it enables innovative services to citizens to improve their quality of life. Objective: To identify and analyze technological solutions (tools, services, applications or products) of urban public monitoring in order to characterize them, with respect to smart cities and ubiquitous devices, from the point of view of researchers and public agents in the field. context of theoretical and applied research in the literature. Method: A Systematic Literature Mapping (SLM) was performed to characterize the research in the area. Results: It was found that most studies treat as important and global trend urban public monitoring through ubiquitous devices in the context of smart cities. Conclusion: Urban public monitoring can be performed through appropriate ICT infrastructures, using devices such as sensors to capture and transmit data in real-time. This communication takes place without the use of wires and it is necessary to worry about the energy efficiency and accuracy of these ubiquitous devices. This way, the city can become more efficient and attractive to its citizens.",,,,,,,,,,,,,,,,,,,,,,,,,,smart city; mobile devices; public monitoring; ubiquitous devices; wearables,,,909
10.1145/3403955,0,0,0,0,0,338,"Mahmud, Redowan; Ramamohanarao, Kotagiri; Buyya, Rajkumar","Application Management in Fog Computing Environments: A Taxonomy, Review and Future Directions",ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3403955,2020-07,,,,,,,"The Internet of Things (IoT) paradigm is being rapidly adopted for the creation of smart environments in various domains. The IoT-enabled cyber-physical systems associated with smart city, healthcare, Industry 4.0 and Agtech handle a huge volume of data and require data processing services from different types of applications in real time. The Cloud-centric execution of IoT applications barely meets such requirements as the Cloud datacentres reside at a multi-hop distance from the IoT devices. Fog computing, an extension of Cloud at the edge network, can execute these applications closer to data sources. Thus, Fog computing can improve application service delivery time and resist network congestion. However, the Fog nodes are highly distributed and heterogeneous, and most of them are constrained in resources and spatial sharing. Therefore, efficient management of applications is necessary to fully exploit the capabilities of Fog nodes. In this work, we investigate the existing application management strategies in Fog computing and review them in terms of architecture, placement and maintenance. Additionally, we propose a comprehensive taxonomy and highlight the research gaps in Fog-based application management. We also discuss a perspective model and provide future research directions for further improvement of application management in Fog computing.",,,,,,,,,,,,,,,,,,,,,,,,,,Fog computing; application placement; Internet of Things; application architecture; application maintenance,0.0,EC3,1879
10.1145/3406208,1,1,1,0,1,307,"Lin, Weiwei; Shi, Fang; Wu, Wentai; Li, Keqin; Wu, Guangxin; Mohammed, Al-Alas",A Taxonomy and Survey of Power Models and Power Modeling for Cloud Servers,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3406208,2020-09,,,,,,,"Due to the increasing demand of cloud resources, the ever-increasing number and scale of cloud data centers make their massive power consumption a prominent issue today. Evidence reveals that the behaviors of cloud servers make the major impact on data centers’ power consumption. Although extensive research can be found in this context, a systematic review of the models and modeling methods for the entire hierarchy (from underlying hardware components to the upper-layer applications) of the cloud server is still missing, which is supposed to cover the relevant studies on physical and virtual cloud server instances, server components, and cloud applications. In this article, we summarize a broad range of relevant studies from three perspectives: power data acquisition, power models, and power modeling methods for cloud servers (including bare-metal, virtual machine (VM), and container instances). We present a comprehensive taxonomy on the collection methods of server-level power data, the existing mainstream power models at multiple levels from hardware to software and application, and commonly used methods for modeling power consumption including classical regression analysis and emerging methods like reinforcement learning. Throughout the work, we introduce a variety of models and methods, illustrating their implementation, usability, and applicability while discussing the limitations of existing approaches and possible ways of improvement. Apart from reviewing existing studies on server power models and modeling methods, we further figure out several open challenges and possible research directions, such as the study on modeling the power consumption of lightweight virtual units like unikernel and the necessity of further explorations toward empowering server power estimation/prediction with machine learning. As power monitoring is drawing increasing attention from cloud service providers (CSPs), this survey provides useful guidelines on server power modeling and can be inspiring for further research on energy-efficient data centers.",,,,,,,,,,,,,,,,,,,,,,,,,,data center; Cloud server; power consumption; power model; power modeling,0.0,IC4,91
10.1145/3408314,0,0,0,0,0,398,"Davoudian, Ali; Liu, Mengchi",Big Data Systems: A Software Engineering Perspective,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3408314,2020-09,,,,,,,"Big Data Systems (BDSs) are an emerging class of scalable software technologies whereby massive amounts of heterogeneous data are gathered from multiple sources, managed, analyzed (in batch, stream or hybrid fashion), and served to end-users and external applications. Such systems pose specific challenges in all phases of software development lifecycle and might become very complex by evolving data, technologies, and target value over time. Consequently, many organizations and enterprises have found it difficult to adopt BDSs. In this article, we provide insight into three major activities of software engineering in the context of BDSs as well as the choices made to tackle them regarding state-of-the-art research and industry efforts. These activities include the engineering of requirements, designing and constructing software to meet the specified requirements, and software/data quality assurance. We also disclose some open challenges of developing effective BDSs, which need attention from both researchers and practitioners.",,,,,,,,,,,,,,,,,,,,,,,,,,Big Data; Big Data systems; quality assurance; requirements engineering; software engineering; software reference architecture,,,502
10.1145/3410670.3410857,0,0,0,0,0,87,"Nakamura, Kiyoshy; Manzoni, Pietro; Zennaro, Marco; Cano, Juan-Carlos; Calafate, Carlos T.; Cecilia, José M.",FUDGE: a frugal edge node for advanced IoT solutions in contexts with limited resources,Proceedings of the 1st Workshop on Experiences with the Design and Implementation of Frugal Smart Objects,978-1-4503-8078-2,,https://doi.org/10.1145/3410670.3410857,2020,FRUGALTHINGS'20,,,,Association for Computing Machinery,"New York, NY, USA","The growing connection between the Internet of Things (IoT) and Artificial Intelligence (AI) poses many challenges that require novel approaches and even a rethinking of the entire communication and processing architecture to meet new requirements for latency, reliability, power consumption and resource usage. Edge computing is a promising approach to meet these challenges that can also be beneficial in delivering advanced AI-based IoT solutions in areas where connectivity is scarce and resources are generally limited.In this paper, we introduce an edge/fog generic architecture to allow the adoption of edge solutions in IoT deployments in poorly connected and resource limited scenarios. To this end, we integrate, using microservices, an MQTT based system that can collect ingress data, handle their persistency, and coordinate data integration with the cloud using a specific service called aggregator. The edge stations have a dedicated channel with the aggregator based on LoRa to enable long-range transmissions with low power consumption. Some details of the implementation aspects are described along with some preliminary results. Initial testing of the architecture indicates that it is flexible and robust enough to become an alternative for the deployment of advanced IoT services in resource-constrained contexts.",,,,,,,,,,,,,,,,,,,,,,,,,,AI; IoT; Edge computing; LoRa,0.0,EC3,1795
10.1145/3410992.3410999,0,0,0,0,0,215,"Krivic, Petar; Guberovic, Emanuel; Zarko, Ivana Podnar; Cavrak, Igor",Evaluation of selected technologies for the implementation of meter data management system,Proceedings of the 10th International Conference on the Internet of Things,978-1-4503-8758-3,,https://doi.org/10.1145/3410992.3410999,2020,IoT '20,,,,Association for Computing Machinery,"New York, NY, USA","Implementation of an efficient meter data management system presents a challenging task since it has to process and store a large number of incoming time-series data entries in almost real-time. A number of solutions for efficient processing and storage of big data streams are today available as open source or commercial software. However, the choice of the most applicable solution highly depends on the requirements of a specific use case scenario since the performance of the aforementioned solutions often vary depending on the specific use-case parameters (e.g. incoming data frequency, average size of a single data entry, etc.). Thus, in this paper we examine different platforms adequate for the implementation of a smart metering data acquisition system, to identify the most efficient ones among the considered candidates. The most important requirement of our meter data management system is to offer a stable solution that processes and stores high volumes of continuously incoming data readings with minimal loss-rate. For this purpose we propose a modular solution where components communicate over a message-queuing system, while the ultimate data repository is a NoSQL database. After carrying out all the specifically designed performance tests, we identify the following platforms as the most promising ones to implement our smart metering solution: Kafka as the messaging broker and time-series database InfluxDB. Finally, we verified that our MDMS successfully processes and stores 2.5 M data entries in a time period under eight minutes which confirms its targeted performance efficiency.",,,,,,,,,,,,,,,,,,,,,,,,,,Kafka; InfluxDB; MDMS; smart metering; time-series data,0.0,EC3,2583
10.1145/3412841.3441894,0,0,0,0,0,189,"Gartziandia, Aitor; Arrieta, Aitor; Agirre, Aitor; Sagardui, Goiuria; Arratibel, Maite",Using regression learners to predict performance problems on software updates: a case study on elevators dispatching algorithms,Proceedings of the 36th Annual ACM Symposium on Applied Computing,978-1-4503-8104-8,,https://doi.org/10.1145/3412841.3441894,2021,SAC '21,,,,Association for Computing Machinery,"New York, NY, USA","Remote software deployment and updating has long been commonplace in many different fields, but now, the increasing expansion of IoT and CPSoS (Cyber-Physcal System of Systems) has highlighted the need for additional mechanisms in these systems, to ensure the correct behaviour of the deployed software version after deployment. In this sense, this paper investigates the use of Machine Learning algorithms to predict acceptable behaviour in system performance of a new software release. By monitoring the real performance, eventual unexpected problems can be identified. Based on previous knowledge and actual run-time information, the proposed approach predicts the response time that can be considered acceptable for the new software release, and this information is used to identify problematic releases. The mechanism has been applied to the post-deployment monitoring of traffic algorithms in elevator systems. To evaluate the approach, we have used performance mutation testing, obtaining good results. This paper makes two contributions. First, it proposes several regression learners that have been trained with different types of traffic profiles to efficiently predict response time of the traffic dispatching algorithm. This prediction is then compared with the actual response time of the new algorithm release, and provides a verdict about its performance. Secondly, a comparison of the different learners is performed.",,,,,,,,,,,,,,,,,,,,,,,,,,machine learning; cyber-physical systems; performance bugs,0.0,EC3,2544
10.1145/3415580,0,0,0,0,0,351,"Do, Jaeyoung; Ferreira, Victor C.; Bobarshad, Hossein; Torabzadehkashi, Mahdi; Rezaei, Siavash; Heydarigorji, Ali; Souza, Diego; Goldstein, Brunno F.; Santiago, Leandro; Kim, Min Soo; Lima, Priscila M. V.; França, Felipe M. G.; Alves, Vladimir","Cost-effective, Energy-efficient, and Scalable Storage Computing for Large-scale AI Applications",ACM Trans. Storage,,1553-3077,https://doi.org/10.1145/3415580,2020-10,,,,,,,"The growing volume of data produced continuously in the Cloud and at the Edge poses significant challenges for large-scale AI applications to extract and learn useful information from the data in a timely and efficient way. The goal of this article is to explore the use of computational storage to address such challenges by distributed near-data processing. We describe Newport, a high-performance and energy-efficient computational storage developed for realizing the full potential of in-storage processing. To the best of our knowledge, Newport is the first commodity SSD that can be configured to run a server-like operating system, greatly minimizing the effort for creating and maintaining applications running inside the storage. We analyze the benefits of using Newport by running complex AI applications such as image similarity search and object tracking on a large visual dataset. The results demonstrate that data-intensive AI workloads can be efficiently parallelized and offloaded, even to a small set of Newport drives with significant performance gains and energy savings. In addition, we introduce a comprehensive taxonomy of existing computational storage solutions together with a realistic cost analysis for high-volume production, giving a good big picture of the economic feasibility of the computational storage technology.",,,,,,,,,,,,,,,,,,,,,,,,,,Computational storage; in-storage processing; neural network; object tracking; similarity search; solid-state drive,0.0,EC3,2203
10.1145/3416010.3423227,0,0,0,0,0,112,"Mendula, Matteo; Khodadadeh, Siavash; Bacanli, Salih Safa; Zehtabian, Sharare; Sheikh, Hassam Ullah; Bölöni, Ladislau; Turgut, Damla; Bellavista, Paolo",Interaction and Behaviour Evaluation for Smart Homes: Data Collection and Analytics in the ScaledHome Project,"Proceedings of the 23rd International ACM Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems",978-1-4503-8117-8,,https://doi.org/10.1145/3416010.3423227,2020,MSWiM '20,,,,Association for Computing Machinery,"New York, NY, USA","The smart home concept can significantly benefit from predictive models that take proactive management operations on home actuators, based on users' behavior evaluation. In this paper, we use a small-scale physical model, the ScaledHome-2 testbed, to experiment with the evolution of measurements in a suburban home under different environmental scenarios. We start from the observation that, for a home to become smart, in addition to IoT sensors and actuators, we also need a predictive model of how actions taken by inhabitants and home actuators affect the internal environment of the home, reflected in the sensor readings. In this paper, we propose a technique to create such a predictive model through machine learning in various simulated weather scenarios. This paper also contributes to the literature in the field by quantitatively comparing several machine learning algorithms (K-nearest neighbor, regression trees, Support Vector Machine regression, and Long Short Term Memory deep neural networks) in their ability to create accurate and generalizable predictive models for smart homes.",,,,,,,,,,,,,,,,,,,,,,,,,,energy; iot; optimization; scaling; simulation,0.0,EC3,2776
10.1145/3416028.3416046,0,0,0,0,0,194,"Ergasheva, Shokhista; Gudkov, Mikhail; Kruglov, Artem; Succi, Giancarlo; Vasques, Xavier; Zagidullina, Aliya",Obtaining Data from the Third-Party Systems for Software Development Process Analysis,Proceedings of the 3rd International Conference on Information Management and Management Science,978-1-4503-7546-7,,https://doi.org/10.1145/3416028.3416046,2020,IMMS '20,,,,Association for Computing Machinery,"New York, NY, USA","Organizations nowadays have plenty number of choices to improve their software processes with the help of several third-party software process evaluation applications. Basically, for any kind of evaluation method, the software process metrics should be measured. Mainly it can be organized in different ways, as far as metrics collection from developers' devices is also not an easy task. This paper introduces our proposed framework to track and analyze the software development process, which gives important insights into the process and the product. In addition, integration of the external third-party agents for development management tools like Trello and SonarQube. The detailed architecture of the framework and it's agent relations are given with the precise metrics lists that are retrieved from the mentioned data sources.",,,,,,,,,,,,,,,,,,,,,,,,,,Development tools; management utilities; productivity; software development tracking,0.0,EC3,2755
10.1145/3422604.3425929,0,0,0,0,0,263,"Qiu, Yiming; Kang, Qiao; Liu, Ming; Chen, Ang",Clara: Performance Clarity for SmartNIC Offloading,Proceedings of the 19th ACM Workshop on Hot Topics in Networks,978-1-4503-8145-1,,https://doi.org/10.1145/3422604.3425929,2020,HotNets '20,,,,Association for Computing Machinery,"New York, NY, USA","The gap between CPU and networking speeds has motivated the development of SmartNICs for near-network processing. Recent work has shown that many network functions can benefit from SmartNIC offloading, but identifying the best porting strategy requires hand-tuning and workload-specific optimizations. The developer has no easy way to understand the ported performance beforehandWe are developing a tool called Clara, whose goal is to provide performance clarity for SmartNIC offloading. Clara can analyze an unported NF in its original form, and predict its performance when ported to a SmartNIC target. This automated workflow enables the developer to easily customize offloading strategies, obtain performance insights, and identify suitable SmartNIC models for her workloads. Clara's key technical roadmap is to emulate a compiler, lowering an unported program to a SmartNIC target logically, without performing code generation. This results in a mapping from core NF logic to SmartNIC hardware resources, and Clara then plugs in NIC parameters to predict the performance for specific workloads. We describe our progress so far, and report initial validation results with Netronome hardware.",,,,,,,,,,,,,,,,,,,,,,,,,,network functions; performance prediction; smartnics,,,828
10.1145/3423211.3425683,1,1,1,1,1,82,"Gunasekaran, Jashwant Raj; Thinakaran, Prashanth; Nachiappan, Nachiappan C.; Kandemir, Mahmut Taylan; Das, Chita R.",Fifer: Tackling Resource Underutilization in the Serverless Era,Proceedings of the 21st International Middleware Conference,978-1-4503-8153-6,,https://doi.org/10.1145/3423211.3425683,2020,Middleware '20,,,,Association for Computing Machinery,"New York, NY, USA","Datacenters are witnessing a rapid surge in the adoption of serverless functions for microservices-based applications. A vast majority of these microservices typically span less than a second, have strict SLO requirements, and are chained together as per the requirements of an application. The aforementioned characteristics introduce a new set of challenges, especially in terms of container provisioning and management, as the state-of-the-art resource management frameworks, employed in serverless platforms, tend to look at microservice-based applications similar to conventional monolithic applications. Hence, these frameworks suffer from microservice agnostic scheduling and colossal container over-provisioning, especially during workload fluctuations, thereby resulting in poor resource utilization.In this work, we quantify the above shortcomings using a variety of workloads on a multi-node cluster managed by the Kubernetes and Brigade serverless framework. To address them, we propose Fifer — an adaptive resource management framework to efficiently manage function-chains on serverless platforms. The key idea is to make Fifer (i) utilization conscious by efficiently bin packing jobs to fewer containers using function-aware container scaling and intelligent request batching, and (ii) at the same time, SLO-compliant by proactively spawning containers to avoid cold-starts, thus minimizing the overall response latency. Combining these benefits, Fifer improves container utilization and cluster-wide energy consumption by 4× and 31%, respectively, without compromising on SLO's, when compared to the state-of-the-art schedulers employed by serverless platforms.",,,,,,,,,,,,,,,,,,,,,,,,,,scheduling; serverless; resource-management; queuing,0.0,IC4,80
10.1145/3424771.3424804,0,0,0,0,0,79,"Dias, João Pedro; Sousa, Tiago Boldt; Restivo, André; Ferreira, Hugo Sereno",A Pattern-Language for Self-Healing Internet-of-Things Systems,Proceedings of the European Conference on Pattern Languages of Programs 2020,978-1-4503-7769-0,,https://doi.org/10.1145/3424771.3424804,2020,EuroPLoP '20,,,,Association for Computing Machinery,"New York, NY, USA","Internet-of-Things systems are assemblies of highly-distributed and heterogeneous parts that, in orchestration, work to provide valuable services to end-users in many scenarios. These systems depend on the correct operation of sensors, actuators, and third-party services, and the failure of a single one can hinder the proper functioning of the whole system, making error detection and recovery of paramount importance, but often overlooked. By drawing inspiration from other research areas, such as cloud, embedded, and mission-critical systems, we present a set of patterns for self-healing IoT systems. We discuss how their implementation can improve system reliability by providing error detection, error recovery, and health mechanisms maintenance.",,,,,,,,,,,,,,,,,,,,,,,,,,fault-tolerance; internet-of-things; patterns; self-healing,0.0,EC3,2808
10.1145/3424771.3424822,0,0,0,0,0,59,"Zimmermann, Olaf; Lübke, Daniel; Zdun, Uwe; Pautasso, Cesare; Stocker, Mirko",Interface Responsibility Patterns: Processing Resources and Operation Responsibilities,Proceedings of the European Conference on Pattern Languages of Programs 2020,978-1-4503-7769-0,,https://doi.org/10.1145/3424771.3424822,2020,EuroPLoP '20,,,,Association for Computing Machinery,"New York, NY, USA","Remote Application Programming Interfaces (APIs), as for instance offered in microservices architectures, are used in almost any distributed system today and are thus enablers for many digitalization efforts. It is hard to design such APIs so that they are easy and effective to use; maintaining their runtime qualities while preserving backward compatibility is equally challenging. Finding well suited granularities in terms of the architectural capabilities of endpoints and the read-write semantics of their operations are particularly important design concerns. Existing pattern languages have dealt with local APIs in object-oriented programming, with remote objects, with queue-based messaging and with service-oriented computing platforms. However, patterns or equivalent guidances for the architectural design of API endpoints, operations and their request and response message structures are still missing. In this paper, we extend our microservice API pattern language (MAP) and introduce endpoint role and operation responsibility patterns, namely Processing Resource, Computation Function, State Creation Operation, Retrieval Operation, and State Transition Operation. Known uses and examples of the patterns are drawn from public Web APIs, as well as application development and system integration projects the authors have been involved in.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,EC3,2088
10.1145/3424978.3425133,0,0,0,0,0,202,"Shen, Jian; Dai, Yang; Dong, Xiaofeng",Research on Grid Edge Agent Based on Enhanced Analysis of Distribution Network Information Processing,Proceedings of the 4th International Conference on Computer Science and Application Engineering,978-1-4503-7772-0,,https://doi.org/10.1145/3424978.3425133,2020,CSAE '20,,,,Association for Computing Machinery,"New York, NY, USA","In the deepening construction of the strong smart grid and the rapid development of the ubiquitous IoT distribution network, issues such as lack of management and poor sharing of data during the mutual penetration and fusion of multiple types of data in the electric power Internet of Things are highlighted. Based on the analysis of the current situation of data acquisition and control of middle and low voltage distribution network, the information processing system of distribution network based on grid edge agent is proposed in this paper. The data of distribution network are analyzed and mined from multiple levels and angles, and the autonomic computing function similar to human brain is endowed to the distribution grid. In this paper, a comprehensive behavioral atlas of distribution network is constructed, and intelligent assisted strategy technology is proposed and successfully applied in the power system of a city in Jiangsu. This article introduces the system design concept, overall architecture and main functions in detail, and prospects its core technology and potential application range. The pilot application proved the rationality of the technical concept.",,,,,,,,,,,,,,,,,,,,,,,,,,Event evolution graph; Low-voltage distribution network; Network edge agent; Regional autonomy; Ubiquitous power IoT,0.0,EC3,2811
10.1145/3427921.3450246,0,0,0,0,0,205,"Avritzer, Alberto; Britto, Ricardo; Trubiani, Catia; Russo, Barbara; Janes, Andrea; Camilli, Matteo; van Hoorn, André; Heinrich, Robert; Rapp, Martina; Henß, Jörg",A Multivariate Characterization and Detection of Software Performance Antipatterns,Proceedings of the ACM/SPEC International Conference on Performance Engineering,978-1-4503-8194-9,,https://doi.org/10.1145/3427921.3450246,2021,ICPE '21,,,,Association for Computing Machinery,"New York, NY, USA","Context. Software Performance Antipatterns (SPAs) research has focused on algorithms for the characterization, detection, and solution of antipatterns. However, existing algorithms are based on the analysis of runtime behavior to detect trends on several monitored variables (e.g., response time, CPU utilization, and number of threads) using pre-defined thresholds. Objective. In this paper, we introduce a new approach for SPA characterization and detection designed to support continuous integration/delivery/deployment (CI/CDD) pipelines, with the goal of addressing the lack of computationally efficient algorithms.Method. Our approach includes SPA statistical characterization using a multivariate analysis approach of load testing experimental results to identify the services that have the largest impact on system scalability.More specifically, we introduce a layered decomposition approach that implements statistical analysis based on response time to characterize load testing experimental results. A distance function is used to match experimental results to SPAs.Results. We have instantiated the introduced methodology by applying it to a large complex telecom system. We were able to automatically identify the top five services that are scalability choke points. In addition, we were able to automatically identify one SPA. We have validated the engineering aspects of our methodology and the expected benefits by means of a domain experts' survey.Conclusion. We contribute to the state-of-the-art by introducing a novel approach to support computationally efficient SPA characterization and detection in large complex systems using performance testing results. We have compared the computational efficiency of the proposed approach with state-of-the-art heuristics. We have found that the approach introduced in this paper grows linearly, which is a significant improvement over existing techniques.",,,,,,,,,,,,,,,,,,,,,,,,,,multivariate analysis; software performance antipattern characterization; software performance antipattern detection,0.0,EC3,2499
10.1145/3437378.3444366,0,0,0,0,0,101,"Buzachis, Alina; Boruta, Daiana; Villari, Massimo; Spillner, Josef",Modeling and Emulation of an Osmotic Computing Ecosystem using OsmoticToolkit,Proceedings of the 2021 Australasian Computer Science Week Multiconference,978-1-4503-8956-3,,https://doi.org/10.1145/3437378.3444366,2021,ACSW '21,,,,Association for Computing Machinery,"New York, NY, USA","Digital services are increasingly becoming cyber-physical and osmotic, combining Cloud resources with Fog, Edge, and IoT devices. This trend can be observed in the e-health domain or in smart city applications where the location of software deployments and data processing matters. Before such applications go live, careful planning with real system emulation is necessary. We claim that the OsmoticToolkit, although in the early stages, is the first emulation environment designed to address this challenge. In this paper, we introduce the emulator’s functionalities and validate experimentally with an e-health scenario, using a reference deployment of a microservice-based hospital application. The experimental results carried out show its effectiveness providing valuable support for understanding the impact on resources, workloads, and Quality of Service requirements within Cloud-Edge/Fog-IoT scenarios while preserving the users’ Service Level Agreements (SLAs).",,,,,,,,,,,,,,,,,,,,,,,,,,Microservices; Deployment; Cloud Computing; Orchestration; SDN; Emulation; Osmotic Computing,0.0,EC3,2256
10.1145/3437378.3444367,0,0,0,0,0,254,"Aslanpour, Mohammad S.; Toosi, Adel N.; Cicconetti, Claudio; Javadi, Bahman; Sbarski, Peter; Taibi, Davide; Assuncao, Marcos; Gill, Sukhpal Singh; Gaire, Raj; Dustdar, Schahram",Serverless Edge Computing: Vision and Challenges,Proceedings of the 2021 Australasian Computer Science Week Multiconference,978-1-4503-8956-3,,https://doi.org/10.1145/3437378.3444367,2021,ACSW '21,,,,Association for Computing Machinery,"New York, NY, USA","Born from a need for a pure “pay-per-use” model and highly scalable platform, the “Serverless” paradigm emerged and has the potential to become a dominant way of building cloud applications. Although it was originally designed for cloud environments, Serverless is finding its position in the Edge Computing landscape, aiming to bring computational resources closer to the data source. That is, Serverless is crossing cloud borders to assess its merits in Edge computing, whose principal partner will be the Internet of Things (IoT) applications. This move sounds promising as Serverless brings particular benefits such as eliminating always-on services causing high electricity usage, for instance. However, the community is still hesitant to uptake Serverless Edge Computing because of the cloud-driven design of current Serverless platforms, and distinctive characteristics of edge landscape and IoT applications. In this paper, we evaluate both sides to shed light on the Serverless new territory. Our in-depth analysis promotes a broad vision for bringing Serverless to the Edge Computing. It also issues major challenges for Serverless to be met before entering Edge computing.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,EC3,2206
10.1145/3440749.3442594,0,0,0,0,0,121,"Khakimov, Abdukodir; Loborchuk, Aleksandr; Ibodullokhodzha, Ibodulaev; Poluektov, Dmitry; Elgendy, Ibrahim A.; Muthanna, Ammar",Edge Computing Resource Allocation Orchestration System for Autonomous Vehicles,Proceedings of the 4th International Conference on Future Networks and Distributed Systems,978-1-4503-8886-3,,https://doi.org/10.1145/3440749.3442594,2021,ICFNDS '20,,,,Association for Computing Machinery,"New York, NY, USA","Edge computing is the key to building 5G Networks and Future 2030 Networks. Edge computing extends the cloud computing paradigm by placing resources close to the network edges to cope with the upcoming growth of connected devices. Future applications: health monitoring and predictive services within the framework of the Smart City, Internet of things (IoT), vehicular ad hoc network, autonomous vehicles present a new set of strict requirements, such as low latency. In this paper, we develop a set of methods for managing and orchestrating new intelligent services in a new network and computing infrastructure. In addition, we consider a new prototype using an orchestration system for managing the autonomous vehicles’ resources in comparison with the existing approaches to the design of high-load networks. This orchestration platform is based on independent Docker containers that running the orchestration system. The main goal of our proposed system is to build an efficient network architecture with a minimum delay to process the information based on neural networks. Finally, simulation results proved that the proposed system can significantly not only reduce the overall network load but also increase the quality of the transmitted stream across the network in comparison with traditional network architectures.",,,,,,,,,,,,,,,,,,,,,,,,,,Edge Computing; V2X; MEC; Network,0.0,EC3,1715
10.1145/3442442.3452057,0,0,0,0,0,81,"Gilgur, Alexander; Coutinho, Brian; Narayanan, Iyswarya; Malani, Parth",Transitive Power Modeling for Improving Resource Efficiency in a Hyperscale Datacenter,Companion Proceedings of the Web Conference 2021,978-1-4503-8313-4,,https://doi.org/10.1145/3442442.3452057,2021,WWW '21,,,,Association for Computing Machinery,"New York, NY, USA","Maintaining efficient utilization of allocated compute resources and controlling their capital and operating expenditure is important for running a hyperscale datacenter infrastructure. Power is one of the most constrained and difficult to manage resources in datacenters. Accurate accounting of power usage across clients of multi-tenant web services can improve budgeting, planning and provisioning of compute resources. In this work, we propose a queuing theory based transitive power modeling framework that estimates the total power cost of a client request across the stack of shared services running in Facebook datacenters. By capturing the non-linearity of power vs load relation, our model is able to estimate marginal change in power consumption of a system upon serving a request with a mean error of less than 4% when applied on production services. In view of the fact that datacenter capacity is planned for peak demand, we test this model at peak load to report up to 2x improvement in accuracy compared to a mathematical model. We further leverage this framework along with a distributed tracing system to estimate power demand shift for serving particular product features within fraction of a percentage and guide the decision to shift their computation at off-peak time.",,,,,,,,,,,,,,,,,,,,,,,,,,cpu; power; scalability; time shifting; transitive; utilization,0.0,EC3,2297
10.1145/3444692,0,0,0,0,0,342,"Varghese, Blesson; Wang, Nan; Bermbach, David; Hong, Cheol-Ho; Lara, Eyal De; Shi, Weisong; Stewart, Christopher",A Survey on Edge Performance Benchmarking,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3444692,2021-04,,,,,,,"Edge computing is the next Internet frontier that will leverage computing resources located near users, sensors, and data stores to provide more responsive services. Therefore, it is envisioned that a large-scale, geographically dispersed, and resource-rich distributed system will emerge and play a key role in the future Internet. However, given the loosely coupled nature of such complex systems, their operational conditions are expected to change significantly over time. In this context, the performance characteristics of such systems will need to be captured rapidly, which is referred to as performance benchmarking, for application deployment, resource orchestration, and adaptive decision-making. Edge performance benchmarking is a nascent research avenue that has started gaining momentum over the past five years. This article first reviews articles published over the past three decades to trace the history of performance benchmarking from tightly coupled to loosely coupled systems. It then systematically classifies previous research to identify the system under test, techniques analyzed, and benchmark runtime in edge performance benchmarking.",,,,,,,,,,,,,,,,,,,,,,,,,,benchmark runtime; Edge computing; edge performance benchmarking; system under test; techniques analyzed,0.0,EC3,2438
10.1145/3445968.3452090,0,0,0,0,0,141,"Hakiri, Akram; Dezfouli, Behnam",Towards a Blockchain-SDN Architecture for Secure and Trustworthy 5G Massive IoT Networks,Proceedings of the 2021 ACM International Workshop on Software Defined Networks &amp; Network Function Virtualization Security,978-1-4503-8318-9,,https://doi.org/10.1145/3445968.3452090,2021,SDN-NFV Sec'21,,,,Association for Computing Machinery,"New York, NY, USA","The emerging 5G mobile network is a prominent technology for addressing networking related challenges of Internet of Things (IoT). The forthcoming 5G is expected to allow low-power massive IoT devices to produce high volumes of data that can be transmitted over ultra-reliable, low-latency wireless communication services. However, IoT systems encounter several security and privacy issues to prevent unauthorized access to IoT nodes. To address these challenges, this paper introduces a novel blockchain-based architecture that leverages Software Defined Network (SDN) and Network Function Virtualization (NFV) for securing IoT transactions. A novel security appliance is introduced in a form of Virtualized Network Functions (VNFs) for improving the scalability and performance of IoT networks. Then, we introduce a novel consensus algorithm to detect and report suspected IoT nodes and mitigate malicious traffic. We evaluate and compare our proposed solution against three well-known consensus algorithms, i.e., Proof of Work (PoW), Proof of Elapsed Time (PoET), and Proof of Stake (PoS). We demonstrate that the proposed solution provides substantially lower latency and higher throughput as well as trustworthy IoT communication.",,,,,,,,,,,,,,,,,,,,,,,,,,security; IoT; NFV; SDN; blockchain; trust and confidence,0.0,EC3,2519
10.1145/3447548.3467146,0,0,0,0,0,190,"Liu, Hao; Gao, Qian; Li, Jiang; Liao, Xiaochao; Xiong, Hao; Chen, Guangxing; Wang, Wenlin; Yang, Guobao; Zha, Zhiwei; Dong, Daxiang; Dou, Dejing; Xiong, Haoyi",JIZHI: A Fast and Cost-Effective Model-As-A-Service System for Web-Scale Online Inference at Baidu,Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining,978-1-4503-8332-5,,https://doi.org/10.1145/3447548.3467146,2021,KDD '21,,,,Association for Computing Machinery,"New York, NY, USA","In modern internet industries, deep learning based recommender systems have became an indispensable building block for a wide spectrum of applications, such as search engine, news feed, and short video clips. However, it remains challenging to carry the well-trained deep models for online real-time inference serving, with respect to the time-varying web-scale traffics from billions of users, in a cost-effective manner. In this work, we present JIZHI - a Model-as-a-Service system - that per second handles hundreds of millions of online inference requests to huge deep models with more than trillions of sparse parameters, for over twenty real-time recommendation services at Baidu, Inc. In JIZHI, the inference workflow of every recommendation request is transformed to a Staged Event-Driven Pipeline (SEDP), where each node in the pipeline refers to a staged computation or I/O intensive task processor. With traffics of real-time inference requests arrived, each modularized processor can be run in a fully asynchronized way and managed separately. Besides, JIZHI introduces the heterogeneous and hierarchical storage to further accelerate the online inference process by reducing unnecessary computations and potential data access latency induced by ultra-sparse model parameters. Moreover, an intelligent resource manager has been deployed to maximize the throughput of JIZHI over the shared infrastructure by searching the optimal resource allocation plan from historical logs and fine-tuning the load shedding policies over intermediate system feedback. Extensive experiments have been done to demonstrate the advantages of JIZHI from the perspectives of end-to-end service latency, system-wide throughput, and resource consumption. Since launched in July 2019, JIZHI has helped Baidu saved more than ten million US dollars in hardware and utility costs per year while handling 200% more traffics without sacrificing the inference efficiency.",,,,,,,,,,,,,,,,,,,,,,,,,,intelligent resource management; MLOPS; online inference; recommendation system,,,613
10.1145/3447548.3467368,0,0,0,0,0,233,"Madhyastha, Meghana; Lillaney, Kunal; Browne, James; Vogelstein, Joshua T.; Burns, Randal",BLOCKSET (Block-Aligned Serialized Trees): Reducing Inference Latency for Tree ensemble Deployment,Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining,978-1-4503-8332-5,,https://doi.org/10.1145/3447548.3467368,2021,KDD '21,,,,Association for Computing Machinery,"New York, NY, USA","We present methods to serialize and deserialize gradient-boosted trees and random forests that optimize inference latency when models are not loaded into memory. This arises when models are larger than memory, but also systematically when models are deployed on low-resource devices in the Internet of Things or run as cloud microservices where resources are allocated on demand. Block-Aligned Serialized Trees (BLOCKSET) introduce the concept of selective access for random forests and gradient boosted trees in which only the parts of the model needed for inference are deserialized and loaded into memory. %BLOCKSET combines concepts from external memory algorithms and data-parallel %layouts of random forests that maximize I/O-density for in-memory models. Using principles from external memory algorithms, we block-align the serialization format in order to minimize the number of I/Os. For gradient boosted trees, this results in a more than five time reduction in inference latency over layouts that do not perform selective access and a 2 times latency reduction over techniques that are selective, but do not encode I/O block boundaries in the layout.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,886
10.1145/3447868,0,0,0,0,0,390,"Michel, Oliver; Bifulco, Roberto; Rétvári, Gábor; Schmid, Stefan","The Programmable Data Plane: Abstractions, Architectures, Algorithms, and Applications",ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3447868,2021-05,,,,,,,"Programmable data plane technologies enable the systematic reconfiguration of the low-level processing steps applied to network packets and are key drivers toward realizing the next generation of network services and applications. This survey presents recent trends and issues in the design and implementation of programmable network devices, focusing on prominent abstractions, architectures, algorithms, and applications proposed, debated, and realized over the past years. We elaborate on the trends that led to the emergence of this technology and highlight the most important pointers from the literature, casting different taxonomies for the field, and identifying avenues for future research.",,,,,,,,,,,,,,,,,,,,,,,,,,in-network computation; network programmability; packet processing; Programmable data planes; programmable switches,,,787
10.1145/3448734.3450869,0,0,0,0,0,243,"Fang, Jianliang; Jiong, Ye; Wei, Jiang; Shangxing, Ye; Yingjie, Xie; Haiqing, Lu; Dong, Dawei; Zhen, Jianfeng",Calculation of Theoretical Line Loss of Full-Voltage Power Grid Based on Multi-Source Data Fusion,The 2nd International Conference on Computing and Data Science,978-1-4503-8957-0,,https://doi.org/10.1145/3448734.3450869,2021,CONF-CDS 2021,,,,Association for Computing Machinery,"New York, NY, USA","Because of the many influencing factors, a large amount of basic data, complex abnormal diagnosis and analysis, and low efficiency of troubleshooting in the current low- and medium-voltage distribution network line loss management, combined with years of line loss management experience, this article focuses on the strategic goals of the energy Internet and is based on Cloud platform applies big data and cloud computing technology to online loss business management, improves the stability and accuracy of theoretical line loss index calculation, and strengthens the platform analysis function to provide relevant line loss lean management work from the perspective of theoretical line loss Business support.",,,,,,,,,,,,,,,,,,,,,,,,,,full voltage level; line loss calculation; Multi-source data fusion; smart grid,,,512
10.1145/3456630,0,0,0,0,0,422,"Oikonomou, Panagiotis; Karanika, Anna; Anagnostopoulos, Christos; Kolomvatsos, Kostas",On the Use of Intelligent Models towards Meeting the Challenges of the Edge Mesh,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3456630,2021-07,,,,,,,"Nowadays, we are witnessing the advent of the Internet of Things (IoT) with numerous devices performing interactions between them or with their environment. The huge number of devices leads to huge volumes of data that demand the appropriate processing. The “legacy” approach is to rely on Cloud where increased computational resources can realize any desired processing. However, the need for supporting real-time applications requires a reduced latency in the provision of outcomes. Edge Computing (EC) comes as the “solver” of the latency problem. Various processing activities can be performed at EC nodes having direct connection with IoT devices. A number of challenges should be met before we conclude a fully automated ecosystem where nodes can cooperate or understand their status to efficiently serve applications. In this article, we perform a survey of the relevant research activities towards the vision of Edge Mesh (EM), i.e., a “cover” of intelligence upon the EC. We present the necessary hardware and discuss research outcomes in every aspect of EC/EM nodes functioning. We present technologies and theories adopted for data, tasks, and resource management while discussing how machine learning and optimization can be adopted in the domain.",,,,,,,,,,,,,,,,,,,,,,,,,,Internet of Things; Machine Learning; Edge Computing; Data Management; Edge Mesh; Resources Management; Tasks Management,0.0,EC3,2778
10.1145/3460866.3461772,0,0,0,0,0,170,"Cérin, Christophe; Andres, Frédéric; Geldwerth-Feniger, Danielle",Towards an emulation tool based on ontologies and data life cycles for studying smart buildings,Proceedings of the International Workshop on Big Data in Emergent Distributed Environments,978-1-4503-8465-0,,https://doi.org/10.1145/3460866.3461772,2021,BiDEDE '21,,,,Association for Computing Machinery,"New York, NY, USA","In this paper, we share our vision to study a complex Information Technology (IT) system handling a massive amount of data in the context of 'smart buildings.' One technique for analyzing complex IT systems relies on emulation, where the final software system is fully deployed on real architectures, and is evaluated in considering ""small"" instances of situations the system is supposed to solve. We propose a software architecture for studying the ecosystem of 'smart buildings'. This software architecture is built: 1) on top of ontologies for the description of smart buildings; 2) on a special tool for mastering the life cycle of data produced by sensors and actuators inside the buildings.We assume that it is equally important to model both the building's components and the flow of data produced inside the building. We use existing software components for both goals and to make real our concerns. According to a translational methodology, we also discuss use cases for illustrating the potential of our approach and the particular challenges associated with making the two main components of our emulation tool inter-operate.Therefore, our main contribution is to propose a comprehensive, ambitious and realistic research plan to guide communities. The paper illustrates how computer scientists and smart buildings domain scientists may communicate to address and solve specific research problems related to Big Data in emergent distributed environments. We are also guessing that experimental results that can demonstrate the practicality of the proposed combination of tools could be devised in the future, based on our broad vision. The paper is, first and foremost, a visionary paper.",,,,,,,,,,,,,,,,,,,,,,,,,,big data tools; data life cycle; emulation principles; ontology; smart buildings; systems and methods,0.0,EC3,2224
10.1145/3464305,0,0,0,0,0,377,"Sobhy, Dalia; Bahsoon, Rami; Minku, Leandro; Kazman, Rick",Evaluation of Software Architectures under Uncertainty: A Systematic Literature Review,ACM Trans. Softw. Eng. Methodol.,,1049-331X,https://doi.org/10.1145/3464305,2021-08,,,,,,,"Context: Evaluating software architectures in uncertain environments raises new challenges, which require continuous approaches. We define continuous evaluation as multiple evaluations of the software architecture that begins at the early stages of the development and is periodically and repeatedly performed throughout the lifetime of the software system. Numerous approaches have been developed for continuous evaluation; to handle dynamics and uncertainties at run-time, over the past years, these approaches are still very few, limited, and lack maturity. Objective: This review surveys efforts on architecture evaluation and provides a unified terminology and perspective on the subject. Method: We conducted a systematic literature review to identify and analyse architecture evaluation approaches for uncertainty including continuous and non-continuous, covering work published between 1990–2020. We examined each approach and provided a classification framework for this field. We present an analysis of the results and provide insights regarding open challenges. Major results and conclusions: The survey reveals that most of the existing architecture evaluation approaches typically lack an explicit linkage between design-time and run-time. Additionally, there is a general lack of systematic approaches on how continuous architecture evaluation can be realised or conducted. To remedy this lack, we present a set of necessary requirements for continuous evaluation and describe some examples.",,,,,,,,,,,,,,,,,,,,,,,,,,Continuous software architecture evaluation; design-time software architecture evaluation; run-time software architecture evaluation; uncertainty,0.0,EC3,2510
10.1145/3465480.3466928,0,0,0,0,0,85,"Ren, Haoyu; Anicic, Darko; Runkler, Thomas A.",The synergy of complex event processing and tiny machine learning in industrial IoT,Proceedings of the 15th ACM International Conference on Distributed and Event-Based Systems,978-1-4503-8555-8,,https://doi.org/10.1145/3465480.3466928,2021,DEBS '21,,,,Association for Computing Machinery,"New York, NY, USA","Focusing on comprehensive networking, the Industrial Internet-of-Things (IIoT) facilitates efficiency and robustness in factory operations. Various intelligent sensors play a central role, as they generate a vast amount of real-time data that can provide insights into manufacturing. Complex event processing (CEP) and machine learning (ML) have been developed actively in the last years in IIoT to identify patterns in heterogeneous data streams and fuse raw data into tangible facts. In a traditional compute-centric paradigm, the raw field data are continuously sent to the cloud and processed centrally. As IIoT devices become increasingly pervasive, concerns are raised since transmitting such an amount of data is energy-intensive, vulnerable to be intercepted, and subjected to high latency. Decentralized on-device ML and CEP provide a solution where data is processed primarily on edge devices. Thus communications can be minimized. However, this is no mean feat because most IIoT edge devices are resource-constrained with low power consumption. This paper proposes a framework that exploits ML and CEP's synergy at the edge in distributed sensor networks. By leveraging tiny ML and μCEP, we now shift the computation from the cloud to the resource-constrained IIoT devices and allow users to adapt on-device ML models and CEP reasoning rules flexibly on the fly. Lastly, we demonstrate the proposed solution and show its effectiveness and feasibility using an industrial use case of machine safety monitoring.",,,,,,,,,,,,,,,,,,,,,,,,,,industrial IoT; complex event processing; tiny machine learning,0.0,EC3,3081
10.1145/3468264.3468610,0,0,0,0,0,298,"Zhang, Qian; Wang, Jiyuan; Kim, Miryung",HeteroFuzz: fuzz testing to detect platform dependent divergence for heterogeneous applications,Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,978-1-4503-8562-6,,https://doi.org/10.1145/3468264.3468610,2021,ESEC/FSE 2021,,,,Association for Computing Machinery,"New York, NY, USA","As specialized hardware accelerators like FPGAs become a prominent part of the current computing landscape, software applications are increasingly constructed to leverage heterogeneous architectures. Such a trend is already happening in the domain of machine learning and Internet-of-Things (IoT) systems built on edge devices. Yet, debugging and testing methods for heterogeneous applications are currently lacking. These applications may look similar to regular C/C++ code but include hardware synthesis details in terms of preprocessor directives. Therefore, their behavior under heterogeneous architectures may diverge significantly from CPU due to hardware synthesis details. Further, the compilation and hardware simulation cycle takes an enormous amount of time, prohibiting frequent invocations required for fuzz testing. We propose a novel fuzz testing technique, called HeteroFuzz, designed to specifically target heterogeneous applications and to detect platform-dependent divergence. The key essence of HeteroFuzz is that it uses a three-pronged approach to reduce the long latency of repetitively invoking a hardware simulator on a heterogeneous application. First, in addition to monitoring code coverage as a fuzzing guidance mechanism, we analyze synthesis pragmas in kernel code and monitor accelerator-relevant value spectra. Second, we design dynamic probabilistic mutations to increase the chance of hitting divergent behavior under different platforms. Third, we memorize the boundaries of seen kernel inputs and skip HLS simulator invocation if it can expose only redundant divergent behavior. We evaluate HeteroFuzz on seven real-world heterogeneous applications with FPGA kernels. HeteroFuzz is 754X faster in exposing the same set of distinct divergence symptoms than naive fuzzing. Probabilistic mutations contribute to 17.5X speed up than the one without. Selective invocation of HLS simulation contributes to 8.8X speed up than the one without.",,,,,,,,,,,,,,,,,,,,,,,,,,Fuzz testing; heterogeneous applications; platform-dependent divergence,,,311
10.1145/3468737.3494097,0,0,0,0,0,248,"Jindal, Anshul; Frielinghaus, Julian; Chadha, Mohak; Gerndt, Michael",Courier: delivering serverless functions within heterogeneous FaaS deployments,Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing,978-1-4503-8564-0,,https://doi.org/10.1145/3468737.3494097,2021,UCC '21,,,,Association for Computing Machinery,"New York, NY, USA","With the advent of serverless computing in different domains, there is a growing need for dynamic adaption to handle diverse and heterogeneous functions. However, serverless computing is currently limited to homogeneous Function-as-a-Service (FaaS) deployments or simply FaaS Deployment (FaaSD) consisting of deployments of serverless functions using a FaaS platform in a region with certain memory configurations. Extending serverless computing to support Heterogeneous FaaS Deployments (HeteroFaaSDs) consisting of multiple FaaSDs with variable configurations (FaaS platform, region, and memory) and dynamically load balancing the invocations of the functions across these FaaSDs within a HeteroFaaSD can provide an optimal way for handling such serverless functions.In this paper, we present a software system called Courier that is responsible for optimally distributing the invocations of the functions (called delivering of serverless functions) within the HeteroFaaSDs based on the execution time of the functions on the FaaSDs comprising the HeteroFaaSDs. To this end, we developed two approaches: Auto Weighted Round-Robin (AWRR) and PerFunction Auto Weighted Round-Robin (PFAWRR) that use functions execution times for delivering serverless functions within a HeteroFaaSD to reduce the overall execution time. We demonstrate and evaluate the functioning of our developed tool on three HeteroFaaSDs using three FaaS platforms: 1) on-premise Open-Whisk, 2) AWS Lambda, and 3) Google Cloud Functions (GCF). We show that Courier can improve the overall performance of the invocations of the functions within a HeteroFaaSD as compared to traditional load balancing algorithms.",,,,,,,,,,,,,,,,,,,,,,,,,,serverless computing; function-as-a-service; functions delivery,,,361
10.1145/3469213.3470417,0,0,0,0,0,278,"Xu, Bin; Liu, Xusheng; Zhao, Wei; Li, Ziqian; Wang, Chenfei; Zhu, Qing",Research on the Architecture and Key Technologies of the Ubiquitous Customer Service Operating System for State Grid,2021 2nd International Conference on Artificial Intelligence and Information Systems,978-1-4503-9020-0,,https://doi.org/10.1145/3469213.3470417,2021,ICAIIS 2021,,,,Association for Computing Machinery,"New York, NY, USA","Traditional customer service systems can hardly satisfy the high quality and efficiency requirements in the digital and intelligent era, to tackle this issue, advanced data and artificial intelligence technology are required to be integrated. Based on the ubiquitous operating system theory and take the State Grid customer service as a business scenario, this paper proposes an intelligent customer service platform. The platform takes the ubiquitous power customer service operating system as its core, manages and information multiplexes the massive data resources systematically to support customer service in a high-efficiency and intelligent manner. This paper studies the architecture of the platform and the key technologies involved, and implemented core applications at the industrial level. The paper also discusses the reliability of the system and proposed a reliability guarantee mechanism for customer service operating system. The demonstration system was verified in the actual production environment of the State Grid, and finally proved that this system can effectively improve the work efficiency of the customer service team and reduce operation and maintenance costs.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,EC3,2762
10.1145/3469440,0,0,0,0,0,379,"Gheibi, Omid; Weyns, Danny; Quin, Federico",Applying Machine Learning in Self-adaptive Systems: A Systematic Literature Review,ACM Trans. Auton. Adapt. Syst.,,1556-4665,https://doi.org/10.1145/3469440,2021-08,,,,,,,"Recently, we have been witnessing a rapid increase in the use of machine learning techniques in self-adaptive systems. Machine learning has been used for a variety of reasons, ranging from learning a model of the environment of a system during operation to filtering large sets of possible configurations before analyzing them. While a body of work on the use of machine learning in self-adaptive systems exists, there is currently no systematic overview of this area. Such an overview is important for researchers to understand the state of the art and direct future research efforts. This article reports the results of a systematic literature review that aims at providing such an overview. We focus on self-adaptive systems that are based on a traditional Monitor-Analyze-Plan-Execute (MAPE)-based feedback loop. The research questions are centered on the problems that motivate the use of machine learning in self-adaptive systems, the key engineering aspects of learning in self-adaptation, and open challenges in this area. The search resulted in 6,709 papers, of which 109 were retained for data collection. Analysis of the collected data shows that machine learning is mostly used for updating adaptation rules and policies to improve system qualities, and managing resources to better balance qualities and resources. These problems are primarily solved using supervised and interactive learning with classification, regression, and reinforcement learning as the dominant methods. Surprisingly, unsupervised learning that naturally fits automation is only applied in a small number of studies. Key open challenges in this area include the performance of learning, managing the effects of learning, and dealing with more complex types of goals. From the insights derived from this systematic literature review, we outline an initial design process for applying machine learning in self-adaptive systems that are based on MAPE feedback loops.",,,,,,,,,,,,,,,,,,,,,,,,,,Self-adaptation; feedback loops; MAPE-K,,,736
10.1145/3470658,0,0,0,0,0,352,"Avasalcai, Cosmin; Tsigkanos, Christos; Dustdar, Schahram",Adaptive Management of Volatile Edge Systems at Runtime With Satisfiability,ACM Trans. Internet Technol.,,1533-5399,https://doi.org/10.1145/3470658,2021-09,,,,,,,"Edge computing offers the possibility of deploying applications at the edge of the network. To take advantage of available devices’ distributed resources, applications often are structured as microservices, often having stringent requirements of low latency and high availability. However, a decentralized edge system that the application may be intended for is characterized by high volatility, due to devices making up the system being unreliable or leaving the network unexpectedly. This makes application deployment and assurance that it will continue to operate under volatility challenging. We propose an adaptive framework capable of deploying and efficiently maintaining a microservice-based application at runtime, by tackling two intertwined problems: (i) finding a microservice placement across device hosts and (ii) deriving invocation paths that serve it. Our objective is to maintain correct functionality by satisfying given requirements in terms of end-to-end latency and availability, in a volatile edge environment. We evaluate our solution quantitatively by considering performance and failure recovery.",,,,,,,,,,,,,,,,,,,,,,,,,,edge computing; Resource management; distributed systems; adaptive systems,0.0,EC3,1798
10.1145/3474554,0,0,0,0,0,414,"Sarwar, Kinza; Yongchareon, Sira; Yu, Jian; Ur Rehman, Saeed",A Survey on Privacy Preservation in Fog-Enabled Internet of Things,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3474554,2021-11,,,,,,,"Despite the rapid growth and advancement in the Internet of Things (IoT), there are critical challenges that need to be addressed before the full adoption of the IoT. Data privacy is one of the hurdles towards the adoption of IoT as there might be potential misuse of users’ data and their identity in IoT applications. Several researchers have proposed different approaches to reduce privacy risks. However, most of the existing solutions still suffer from various drawbacks, such as huge bandwidth utilization and network latency, heavyweight cryptosystems, and policies that are applied on sensor devices and in the cloud. To address these issues, fog computing has been introduced for IoT network edges providing low latency, computation, and storage services. In this survey, we comprehensively review and classify privacy requirements for an in-depth understanding of privacy implications in IoT applications. Based on the classification, we highlight ongoing research efforts and limitations of the existing privacy-preservation techniques and map the existing IoT schemes with Fog-enabled IoT schemes to elaborate on the benefits and improvements that Fog-enabled IoT can bring to preserve data privacy in IoT applications. Lastly, we enumerate key research challenges and point out future research directions.",,,,,,,,,,,,,,,,,,,,,,,,,,Internet of Things; fog computing; Content privacy; context privacy; Fog-enabled IoT; IoT privacy,0.0,EC3,2733
10.1145/3475991,0,0,0,0,0,407,"Ray, Kaustabha; Banerjee, Ansuman",Horizontal Auto-Scaling for Multi-Access Edge Computing Using Safe Reinforcement Learning,ACM Trans. Embed. Comput. Syst.,,1539-9087,https://doi.org/10.1145/3475991,2021-10,,,,,,,"Multi-Access Edge Computing (MEC) has emerged as a promising new paradigm allowing low latency access to services deployed on edge servers to avert network latencies often encountered in accessing cloud services. A key component of the MEC environment is an auto-scaling policy which is used to decide the overall management and scaling of container instances corresponding to individual services deployed on MEC servers to cater to traffic fluctuations. In this work, we propose a Safe Reinforcement Learning (RL)-based auto-scaling policy agent that can efficiently adapt to traffic variations to ensure adherence to service specific latency requirements. We model the MEC environment using a Markov Decision Process (MDP). We demonstrate how latency requirements can be formally expressed in Linear Temporal Logic (LTL). The LTL specification acts as a guide to the policy agent to automatically learn auto-scaling decisions that maximize the probability of satisfying the LTL formula. We introduce a quantitative reward mechanism based on the LTL formula to tailor service specific latency requirements. We prove that our reward mechanism ensures convergence of standard Safe-RL approaches. We present experimental results in practical scenarios on a test-bed setup with real-world benchmark applications to show the effectiveness of our approach in comparison to other state-of-the-art methods in literature. Furthermore, we perform extensive simulated experiments to demonstrate the effectiveness of our approach in large scale scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,auto-scaling; Multi-access edge computing; safe reinforcement learning,0.0,EC3,2692
10.1145/3476513,0,0,0,0,0,395,"Elhoseny, Mohamed; Lakhan, Abdullah; Rashid, Ahmed; Mohammed, Mazin; Abdulkareem, Karrar",Underwater Sensor Multi-Parameter Scheduling for Heterogenous Computing Nodes,ACM Trans. Sen. Netw.,,1550-4859,https://doi.org/10.1145/3476513,2022-09,,,,,,,"Sensor-aware distributed workflow applications are becoming increasingly popular underwater. The apps are marine operations that generate data and process it based on its characteristics. Mobile-fog-cloud paradigms, as well as computing such as sensor nodes, have emerged. As previously stated, the nodes can be combined into a single system to achieve several goals. Many factors are considered, including network contents, workload fluctuation, variable execution durations, deadlines, and bandwidth. As a result, scheduling mobile workflow systems with multiple parameters might be challenging. The study suggests a novel content-efficient decision-aware task scheduling (CATSA) method for defining and adapting to complicated environmental changes. The CATSA consists of several components that work together to perform various benchmarks in the system, including a decision planner, sequencing, and scheduling. As evidenced by test findings during evaluation, the suggested architecture outperforms current studies regarding workflow execution quality of services and improved the makespan 30% and deadline meeting 40% in the study.",,,,,,,,,,,,,,,,,,,,,,,,,,scheduling; content; decision; marine; parameters; sensor; Water,0.0,EC3,2460
10.1145/3478680,0,0,0,0,0,337,"Siqueira, Frank; Davis, Joseph G.","Service Computing for Industry 4.0: State of the Art, Challenges, and Research Opportunities",ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3478680,2021-10,,,,,,,"Recent advances in the large-scale adoption of information and communication technologies in manufacturing processes, known as Industry 4.0 or Smart Manufacturing, provide us a window into how the manufacturing sector will evolve in the coming decades. As a result of these initiatives, manufacturing firms have started to integrate a series of emerging technologies into their processes that will change the way products are designed, manufactured, and consumed. This article provides a comprehensive review of how service-oriented computing is being employed to develop the required software infrastructure for Industry 4.0 and identifies the major challenges and research opportunities that ensue. Particular attention is paid to the microservices architecture, which is increasingly recognized as offering a promising approach for developing innovative industrial applications. This literature review is based on the current state of the art on service computing for Industry 4.0 as described in a large corpus of recently published research papers, which helped us to identify and explore a series of challenges and opportunities for the development of this emerging technology frontier, with the goal of facilitating its widespread adoption.",,,,,,,,,,,,,,,,,,,,,,,,,,digital twins; microservices; industry 4.0; industrial IoT; Service-oriented architecture,0.0,EC3,2525
10.1145/3479241.3486691,0,0,0,0,0,182,"Gerakos, Kostantinos; Panagidi, Kakia; Andreou, Charalampos; Zampouras, Dimitris",MOTIVE - Time-Optimized Contextual Information Flow On Unmanned Vehicles,Proceedings of the 19th ACM International Symposium on Mobility Management and Wireless Access,978-1-4503-9079-8,,https://doi.org/10.1145/3479241.3486691,2021,MobiWac '21,,,,Association for Computing Machinery,"New York, NY, USA","The last decades, an increasing interest has been witnessed on the exploitation of unmanned vehicles in fields such as environmental monitoring, commercial air surveillance, domestic policing, geophysical surveys, disaster relief, scientific research, civilian casualties, search and rescue operations, maritime patrol,traffic management, etc. Regardless of the domain (i.e., aerial, ground or surface) that they belong to, the key elements that distinguish them as the leading edge of their technology, are the provided degree of autonomy (i.e., the ability to make decisions without human intervention) and the endurance and the payload that they can support. The mobile IoT paradigm has been significantly expanded with the proliferation of drones and unmanned robotic devices. End-to-end communication and edge decision support are major challenges when operating with mobile nodes and especially with drones. In this paper, we propose a framework that uses two decision making stochastic optimization models of on-line control unit applied on transmission functionalities of mobile and static nodes adaptive to changes in network quality statistics. This is driven by a novel dynamic suppression control of telemetry process and control messages based on the principles of the Optimal Stopping Theory (OST). MOTIVE's time-optimized control mechanism ensures the optimal delivery of critical information from unmanned vehicles to ground control station and vice versa. The main goal, through this proposal, is to significantly enhance the operation of unmanned vehicles when operating under saturated, high traffic wireless networks.",,,,,,,,,,,,,,,,,,,,,,,,,,apache kafka; change detection; message distributed platform; optimal stopping theory; queue prioritization,,,569
10.1145/3485832.3485907,0,0,0,0,0,42,"Ghorbani, Mohammadmahdi; Moghaddam, Fereydoun Farrahi; Zhang, Mengyuan; Pourzandi, Makan; Nguyen, Kim Khoa; Cheriet, Mohamed",DistAppGaurd: Distributed Application Behaviour Profiling in Cloud-Based Environment,Proceedings of the 37th Annual Computer Security Applications Conference,978-1-4503-8579-4,,https://doi.org/10.1145/3485832.3485907,2021,ACSAC '21,,,,Association for Computing Machinery,"New York, NY, USA","Today, Machine Learning (ML) techniques are increasingly used to detect abnormal behaviours of industrial applications. Since many of these applications are moving to the cloud environments, classical ML approaches are facing new challenges in accurately identifying abnormal behaviours due to the highly dynamic and heterogeneous nature of the cloud. In this paper, we propose a novel framework, DistAppGaurd, for profiling simultaneously the behaviour of all microservice components of a distributed application in the cloud. The framework can therefore, detect complex attacks that are not observable by monitoring a single process or a single microservice. DistAppGaurd utilizes the system calls executed by all the processes of an application to build a graph consisting of data exchanges among different application entities (e.g., processes and files) representing the behaviour of the application. This representation is then used by our novel miroservice-aware Autoencoder model to perform anomaly detection at runtime. The efficiency and feasibility of our approach is shown by implementing several different real-world attacks, which yields high detection rates (94%-97%) at 0.01% false alarm rate.",,,,,,,,,,,,,,,,,,,,,,,,,,anomaly detection; microservice; machine learning; autoencoder; distributed behaviour profiling,0.0,EC3,2684
10.1145/3486221,0,0,0,0,0,333,"Costa, Breno; Bachiega, Joao; de Carvalho, Leonardo Rebouças; Araujo, Aleteia P. F.",Orchestration in Fog Computing: A Comprehensive Survey,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3486221,2022-01,,,,,,,"Fog computing is a paradigm that brings computational resources and services to the network edge in the vicinity of user devices, lowering latency and connecting with cloud computing resources. Unlike cloud computing, fog resources are based on constrained and heterogeneous nodes whose connectivity can be unstable. In this complex scenario, there is a need to define and implement orchestration processes to ensure that applications and services can be provided, considering the settled agreements. Although some publications have dealt with orchestration in fog computing, there are still some diverse definitions and functional intersection with other areas, such as resource management and monitoring. This article presents a systematic review of the literature with focus on orchestration in fog computing. A generic architecture of fog orchestration is presented, created from the consolidation of the analyzed proposals, bringing to light the essential functionalities addressed in the literature. This work also highlights the main challenges and open research questions.",,,,,,,,,,,,,,,,,,,,,,,,,,resource management; Fog computing; orchestration; monitoring,,,164
10.1145/3487664.3487806,0,0,0,0,0,269,"Ilarri, Sergio; Fumanal, Irene; Trillo-Lado, Raquel",An Experience with the Implementation of a Rule-Based Triggering Recommendation Approach for Mobile Devices,The 23rd International Conference on Information Integration and Web Intelligence,978-1-4503-9556-4,,https://doi.org/10.1145/3487664.3487806,2022,iiWAS2021,,,,Association for Computing Machinery,"New York, NY, USA","In the current Big Data era, mobile context-aware recommender systems can play a key role to help citizens and tourists to make good decisions. Ideally, these systems should be proactive, able to detect the right moment and place to offer suggestions of a specific type of item or activity to the user. For this purpose, push-based recommender systems can be used, exploiting context rules to decide when a specific type of recommendation should be triggered. However, experiences regarding the implementation of these types of systems are scarce. Motivated by this, in this paper, we describe our design and implementation efforts focusing on the ability to fire suitable recommendations, without user intervention, whenever it is required. In our proposal, the mobile user can activate, deactivate, parametrize, and define rules in an easy way, to obtain a better user personalization. Besides, the recommendation triggering is performed on the mobile device, which allows minimizing the amount of wireless communications and helps to protect the user’s privacy (as context data is evaluated locally on the device, rather than by an external server). We have analyzed several technological options and evaluated the performance and scalability of our proposal, showing its feasibility.",,,,,,,,,,,,,,,,,,,,,,,,,,data management; mobile devices; mobile computing; context rules; mobile context-aware recommender systems; push-based recommendations,,,445
10.1145/3488247,0,0,0,0,0,349,"Neha, Benazir; Panda, Sanjaya Kumar; Sahu, Pradip Kumar; Sahoo, Kshira Sagar; Gandomi, Amir H.",A Systematic Review on Osmotic Computing,ACM Trans. Internet Things,,,https://doi.org/10.1145/3488247,2022-02,,,,,,,"Osmotic computing in association with related computing paradigms (cloud, fog, and edge) emerges as a promising solution for handling bulk of security-critical as well as latency-sensitive data generated by the digital devices. It is a growing research domain that studies deployment, migration, and optimization of applications in the form of microservices across cloud/edge infrastructure. It presents dynamically tailored microservices in technology-centric environments by exploiting edge and cloud platforms. Osmotic computing promotes digital transformation and furnishes benefits to transportation, smart cities, education, and healthcare. In this article, we present a comprehensive analysis of osmotic computing through a systematic literature review approach. To ensure high-quality review, we conduct an advanced search on numerous digital libraries to extracting related studies. The advanced search strategy identifies 99 studies, from which 29 relevant studies are selected for a thorough review. We present a summary of applications in osmotic computing build on their key features. On the basis of the observations, we outline the research challenges for the applications in this research field. Finally, we discuss the security issues resolved and unresolved in osmotic computing.",,,,,,,,,,,,,,,,,,,,,,,,,,Osmotic computing; edge computing; fog computing,0.0,EC3,2480
10.1145/3488585,0,0,0,0,0,347,"Da Silva, Thiago Pereira; Batista, Thais; Lopes, Frederico; Neto, Aluizio Rocha; Delicato, Flávia C.; Pires, Paulo F.; Da Rocha, Atslands R.",Fog Computing Platforms for Smart City Applications: A Survey,ACM Trans. Internet Technol.,,1533-5399,https://doi.org/10.1145/3488585,2022-12,,,,,,,"Emerging IoT applications with stringent requirements on latency and data processing have posed many challenges to cloud-centric platforms for Smart Cities. Recently, Fog Computing has been advocated as a promising approach to support such new applications and handle the increasing volume of IoT data and devices. The Fog Computing paradigm is characterized by a horizontal system-level architecture where devices close to end-users and IoT devices are used for processing, storage, and networking functions. Fog Computing platforms aim to facilitate the development of applications and systems for Smart Cities by providing services and abstractions designed to integrate data from IoT devices and various information systems deployed in the city. Despite the potential of the Fog Computing paradigm, the literature still lacks a broad, comprehensive overview of what has been investigated on the use of such paradigm in platforms for Smart Cities and open issues to be addressed in future research and development. In this paper, a systematic mapping study was performed and we present a comprehensive understanding of the use of the Fog Computing paradigm in Smart Cities platforms, providing an overview of the current state of research on this topic, and identifying important gaps in the existing approaches and promising research directions.",,,,,,,,,,,,,,,,,,,,,,,,,,Fog computing; edge computing; smart cities,0.0,EC3,2891
10.1145/3491204.3527490,0,0,0,0,0,144,"Tuli, Shreshth; Casale, Giuliano",Optimizing the Performance of Fog Computing Environments Using AI and Co-Simulation,Companion of the 2022 ACM/SPEC International Conference on Performance Engineering,978-1-4503-9159-7,,https://doi.org/10.1145/3491204.3527490,2022,ICPE '22,,,,Association for Computing Machinery,"New York, NY, USA","This tutorial presents a performance engineering approach for optimizing the Quality of Service (QoS) of Edge/Fog/Cloud Computing environments using AI and Coupled-Simulation being developed as part of the Co-Simulation based Container Orchestration (COSCO) framework. It introduces fundamental AI and co-simulation concepts, their importance in QoS optimization and performance engineering challenges in the context of Fog computing. It also discusses how AI models, specifically, deep neural networks (DNNs), can be used in tandem with simulated estimates to take optimal resource management decisions. Additionally, we discuss a few use cases of training DNNs as surrogates to estimate key QoS metrics and utilize such models to build policies for dynamic scheduling in a distributed fog environment. The tutorial demonstrates these concepts using the COSCO framework. Metric monitoring and simulation primitives in COSCO demonstrates the efficacy of an AI and simulation based scheduler on a fog/cloud platform. Finally, we provide AI baselines for resource management problems that arise in the area of fog management.",,,,,,,,,,,,,,,,,,,,,,,,,,fog computing; performance engineering; artificial intelligence; co-simulation.,0.0,EC3,2623
10.1145/3492762,0,0,0,0,0,331,"Sobhy, Dalia; Minku, Leandro; Bahsoon, Rami; Kazman, Rick",Continuous and Proactive Software Architecture Evaluation: An IoT Case,ACM Trans. Softw. Eng. Methodol.,,1049-331X,https://doi.org/10.1145/3492762,2022-03,,,,,,,"Design-time evaluation is essential to build the initial software architecture to be deployed. However, experts’ assumptions made at design-time are unlikely to remain true indefinitely in systems that are characterized by scale, hyperconnectivity, dynamism, and uncertainty in operations (e.g. IoT). Therefore, experts’ design-time decisions can be challenged at run-time. A continuous architecture evaluation that systematically assesses and intertwines design-time and run-time decisions is thus necessary. This paper proposes the first proactive approach to continuous architecture evaluation of the system leveraging the support of simulation. The approach evaluates software architectures by not only tracking their performance over time, but also forecasting their likely future performance through machine learning of simulated instances of the architecture. This enables architects to make cost-effective informed decisions on potential changes to the architecture. We perform an IoT case study to show how machine learning on simulated instances of architecture can fundamentally guide the continuous evaluation process and influence the outcome of architecture decisions. A series of experiments is conducted to demonstrate the applicability and effectiveness of the approach. We also provide the architect with recommendations on how to best benefit from the approach through choice of learners and input parameters, grounded on experimentation and evidence.",,,,,,,,,,,,,,,,,,,,,,,,,,IoT; Continuous evaluation; software architecture evaluation; time series forecasting,0.0,EC3,2082
10.1145/3493229.3493306,0,0,0,0,0,53,"Tzenetopoulos, Achilleas; Marantos, Charalampos; Gavrielides, Giannos; Xydis, Sotirios; Soudris, Dimitrios",FADE: FaaS-inspired application decomposition and Energy-aware function placement on the Edge,Proceedings of the 24th International Workshop on Software and Compilers for Embedded Systems,978-1-4503-9166-5,,https://doi.org/10.1145/3493229.3493306,2021,SCOPES '21,,,,Association for Computing Machinery,"New York, NY, USA","Lately, more and more applications are deployed on heterogeneous, power-constrained edge-computing devices. Bringing computation closer to the data, contributes both to latency and energy consumption reduction due to the elimination of excessive data transfers. However, while the main concern in such environments is the minimization of energy consumption, the heterogeneity in compute resources found at the edge may lead to Quality of Service (QoS) violations. At the same time, Serverless computing, the next frontier of Cloud computing has emerged to offer unprecedented elasticity by utilizing fine-grained, stateless functions. The reduction in the execution time and the modest memory footprint of such decomposed applications, allow for fine-grained resource multiplexing. In this work, we propose a methodology for application decomposition into fine-grained functions and energy-aware function placement on a cluster of edge devices subject to user-specified QoS guarantees.",,,,,,,,,,,,,,,,,,,,,,,,,,resource management; serverless; energy-aware,0.0,EC3,1699
10.1145/3494322.3494338,0,0,0,0,0,274,"Hviid, Jakob; Johansen, Aslak; Caleb Sangogboye, Fisayo; Kjærgaard, Mikkel Baun",OPM: An Ontology-Based Package Manager for Building Operating Systems,Proceedings of the 11th International Conference on the Internet of Things,978-1-4503-8566-4,,https://doi.org/10.1145/3494322.3494338,2022,IoT '21,,,,Association for Computing Machinery,"New York, NY, USA","The energy sector is experiencing new challenges with the move to green energy. One of these challenges is keeping a stable energy grid when transitioning the production to unpredictable energy generation from green sources. Demand Response (DR) can mitigate some of the lack of predictability by influencing the consumer’s load profile. Unfortunately, the cost of implementing DR, and the required infrastructure, vastly overshadows the benefits for the consumer, thereby negating the incentive to invest. Therefore, reducing the initial cost of investment is a critical factor for the success of DR. Building Operating Systems (BOS) is one possible avenue to achieve DR functionality in buildings. This paper seeks to reduce initial investment costs of BOSes, by introducing an ontology-based package manager (OPM), that dynamically resolves dependencies and installs services. An ontology-based approach to dependency resolution allows for loosely defined dependencies but also takes the context of the service into account, as well as requirements in terms of sensor availability and physical layout of the building. The OPM is evaluated by deploying a BOS and accompanying services for occupancy prediction. By significantly reducing deployment complexity, results show considerable time savings, and thereby cost reductions, on deployment and maintenance activities.",,,,,,,,,,,,,,,,,,,,,,,,,,ontology; containerization; deployment; building operating systems; dependency resolution; OWL; Package manager,0.0,EC3,2384
10.1145/3498336,0,0,0,0,0,386,"Berenberg, Anna; Calder, Brad",Deployment Archetypes for Cloud Applications,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3498336,2022-02,,,,,,,"This is a survey article that explores six Cloud-based deployment archetypes for Cloud applications and the tradeoffs between them to achieve high availability, low end-user latency, and acceptable costs. These are (1) Zonal, (2) Regional, (3) Multi-regional, (4) Global, (5) Hybrid, and (6) Multi-cloud deployment archetypes. The goal is to classify cloud applications into a set of deployment archetypes and deployment models that tradeoff their needs around availability, latency, and geographical constraints with a focus on serving applications. This enables application owners to better examine the tradeoffs of each deployment model and what is needed for achieving the availability and latency goals for their application.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud architecture; cloud archetypes; cloud availability; Cloud deployments,,,243
10.1145/3498338,0,0,0,0,0,336,"Li, Huan; Lu, Hua; Jensen, Christian S.; Tang, Bo; Cheema, Muhammad Aamir","Spatial Data Quality in the Internet of Things: Management, Exploitation, and Prospects",ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3498338,2022-02,,,,,,,"With the continued deployment of the Internet of Things (IoT), increasing volumes of devices are being deployed that emit massive spatially referenced data. Due in part to the dynamic, decentralized, and heterogeneous architecture of the IoT, the varying and often low quality of spatial IoT data (SID) presents challenges to applications built on top of this data. This survey aims to provide unique insight to practitioners who intend to develop IoT-enabled applications and to researchers who wish to conduct research that relates to data quality in the IoT setting. The survey offers an inventory analysis of major data quality dimensions in SID and covers significant data characteristics and associated quality considerations. The survey summarizes data quality related technologies from both task and technique perspectives. Organizing the technologies from the task perspective, it covers recent progress in SID quality management, encompassing location refinement, uncertainty elimination, outlier removal, fault correction, data integration, and data reduction; and it covers low-quality SID exploitation, encompassing querying, analysis, and decision-making techniques. Finally, the survey covers emerging trends and open issues concerning the quality of SID.",,,,,,,,,,,,,,,,,,,,,,,,,,Internet of Things; geo-sensory data; location refinement; quality management; spatial computing; spatial queries; spatiotemporal data cleaning; spatiotemporal dependencies,,,471
10.1145/3503222.3507731,0,0,0,0,0,266,"Weiner, Johannes; Agarwal, Niket; Schatzberg, Dan; Yang, Leon; Wang, Hao; Sanouillet, Blaise; Sharma, Bikash; Heo, Tejun; Jain, Mayank; Tang, Chunqiang; Skarlatos, Dimitrios",TMO: transparent memory offloading in datacenters,Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems,978-1-4503-9205-1,,https://doi.org/10.1145/3503222.3507731,2022,ASPLOS '22,,,,Association for Computing Machinery,"New York, NY, USA","The unrelenting growth of the memory needs of emerging datacenter applications, along with ever increasing cost and volatility of DRAM prices, has led to DRAM being a major infrastructure expense. Alternative technologies, such as NVMe SSDs and upcoming NVM devices, offer higher capacity than DRAM at a fraction of the cost and power. One promising approach is to transparently offload colder memory to cheaper memory technologies via kernel or hypervisor techniques. The key challenge, however, is to develop a datacenter-scale solution that is robust in dealing with diverse workloads and large performance variance of different offload devices such as compressed memory, SSD, and NVM. This paper presents TMO, Meta’s transparent memory offloading solution for heterogeneous datacenter environments. TMO introduces a new Linux kernel mechanism that directly measures in realtime the lost work due to resource shortage across CPU, memory, and I/O. Guided by this information and without any prior application knowledge, TMO automatically adjusts how much memory to offload to heterogeneous devices (e.g., compressed memory or SSD) according to the device’s performance characteristics and the application’s sensitivity to memory-access slowdown. TMO holistically identifies offloading opportunities from not only the application containers but also the sidecar containers that provide infrastructure-level functions. To maximize memory savings, TMO targets both anonymous memory and file cache, and balances the swap-in rate of anonymous memory and the reload rate of file pages that were recently evicted from the file cache. TMO has been running in production for more than a year, and has saved between 20-32% of the total memory across millions of servers in our large datacenter fleet. We have successfully upstreamed TMO into the Linux kernel.",,,,,,,,,,,,,,,,,,,,,,,,,,Datacenters; Memory Management; Non-volatile Memory; Operating Systems,0.0,EC3,3109
10.1145/3508042,0,0,0,0,0,373,"Wang, Minhu; Xu, Mingwei; Wu, Jianping",Understanding I/O Direct Cache Access Performance for End Host Networking,Proc. ACM Meas. Anal. Comput. Syst.,,,https://doi.org/10.1145/3508042,2022-02,,,,,,,"Direct Cache Access (DCA) enables a network interface card (NIC) to load and store data directly on the processor cache, as conventional Direct Memory Access (DMA) is no longer suitable as the bridge between NIC and CPU in the era of 100 Gigabit Ethernet. As numerous I/O devices and cores compete for scarce cache resources, making the most of DCA for networking applications with varied objectives and constraints is a challenge, especially given the increasing complexity of modern cache hardware and I/O stacks. In this paper, we reverse engineer details of one commercial implementation of DCA, Intel's Data Direct I/O (DDIO), to explicate the importance of hardware-level investigation into DCA. Based on the learned knowledge of DCA and network I/O stacks, we (1) develop an analytical framework to predict the effectiveness of DCA (i.e., its hit rate) under certain hardware specifications, system configurations, and application properties; (2) measure penalties of the ineffective use of DCA (i.e., its miss penalty) to characterize its benefits; and (3) show that our reverse engineering, measurement, and model contribute to a deeper understanding of DCA, which in turn helps diagnose, optimize, and design end-host networking.",,,,,,,,,,,,,,,,,,,,,,,,,,direct cache access; end host networking,,,1037
10.1145/3510411,0,0,0,0,0,368,"Korala, Harindu; Georgakopoulos, Dimitrios; Jayaraman, Prem Prakash; Yavari, Ali",A Survey of Techniques for Fulfilling the Time-Bound Requirements of Time-Sensitive IoT Applications,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3510411,2022-09,,,,,,,"This article surveys existing techniques for meeting the time-bound requirements of time-sensitive applications in the Internet of Things (IoT). To provide the foundation for identifying and classifying relevant techniques, we present three sample time-sensitive IoT applications and their time-bound requirements, describe the main computation and network resources in IoT that can be used to process such applications, and identify the main challenges in meeting their time-bound requirements. Based on these, the article presents a comprehensive literature review of existing techniques and tools that can help meet application-specific time-bound requirements in IoT. The article also includes a gap analysis in existing research outcomes and proposes research directions for bridging the remaining research gaps in supporting time-sensitive IoT applications.",,,,,,,,,,,,,,,,,,,,,,,,,,Internet of Things (IoT); time sensitive; time-bound,,,322
10.1145/3510611,0,0,0,0,0,374,"Shafiei, Hossein; Khonsari, Ahmad; Mousavi, Payam","Serverless Computing: A Survey of Opportunities, Challenges, and Applications",ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3510611,2022-11,,,,,,,"The emerging serverless computing paradigm has attracted attention from both academia and industry. This paradigm brings benefits such as less operational complexity, a pay-as-you-go pricing model, and an auto-scaling feature. The paradigm opens up new opportunities and challenges for cloud application developers. In this article, we present a comprehensive overview of the past development as well as the recent advances in research areas related to serverless computing. First, we survey serverless applications introduced in the literature. We categorize applications in eight domains and separately discuss the objectives and the viability of the serverless paradigm along with challenges in each of those domains. We then classify those challenges into nine topics and survey the proposed solutions. Finally, we present the areas that need further attention from the research community and identify open problems.",,,,,,,,,,,,,,,,,,,,,,,,,,serverless computing; Cloud services; function-as-a-service (FaaS),0.0,EC3,2642
10.1145/3511094,1,1,1,1,1,305,"Muralidhar, Rajeev; Borovica-Gajic, Renata; Buyya, Rajkumar","Energy Efficient Computing Systems: Architectures, Abstractions and Modeling to Techniques and Standards",ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3511094,2022-09,,,,,,,"Computing systems have undergone a tremendous change in the last few decades with several inflexion points. While Moore’s law guided the semiconductor industry to cram more and more transistors and logic into the same volume, the limits of instruction-level parallelism (ILP) and the end of Dennard’s scaling drove the industry towards multi-core chips. More recently, we have entered the era of domain-specific architectures (DSA) and chips for new workloads like artificial intelligence (AI) and machine learning (ML). These trends continue, arguably with other limits, along with challenges imposed by tighter integration, extreme form factors and increasingly diverse workloads, making systems more complex to architect, design, implement and optimize from an energy efficiency perspective. Energy efficiency has now become a first order design parameter and constraint across the entire spectrum of computing devices.Many research surveys have gone into different aspects of energy efficiency techniques implemented in hardware and microarchitecture across devices, servers, HPC/cloud, data center systems along with improved software, algorithms, frameworks, and modeling energy/thermals. Somewhat in parallel, the semiconductor industry has developed techniques and standards around specification, modeling/simulation, benchmarking and verification of complex chips; these areas have not been addressed in detail by previous research surveys. This survey aims to bring these domains holistically together, present the latest in each of these areas, highlight potential gaps and challenges, and discuss opportunities for the next generation of energy efficient systems. The survey is composed of a systematic categorization of key aspects of building energy efficient systems - (1) specification - the ability to precisely specify the power intent, attributes or properties at different layers (2) modeling and simulation of the entire system or subsystem (hardware or software or both) so as to be able to experiment with possible options and perform what-if analysis, (3) techniques used for implementing energy efficiency at different levels of the stack, (4) verification techniques used to provide guarantees that the functionality of complex designs are preserved, and (5) energy efficiency benchmarks, standards and consortiums that aim to standardize different aspects of energy efficiency, including cross-layer optimizations.",,,,,,,,,,,,,,,,,,,,,,,,,,dynamic power management; Energy efficiency; low power; low power optimizations; modeling; platform-level power management; specification,0.0,IC4,98
10.1145/3512961,0,0,0,0,0,389,"Son, Heesuk; Lee, Chaiwoo; FakhrHosseini, Shabnam; Lee, Shenghung; Coughlin, Joseph F.; Rudnik, John",Reshaping the Smart Home Research and Development in the Pandemic Era: Considerations around Scalable and Easy-to-Install Design,Proc. ACM Hum.-Comput. Interact.,,,https://doi.org/10.1145/3512961,2022-04,,,,,,,"Smart home research has traditionally included visiting participants' homes to build testbed environments and evaluate their experience. However, in-person home deployment poses limitations around scalability and is not a feasible method in the context of the COVID-19 pandemic. The smart home research community is now facing the need to reshape and innovate research methods and design approaches. This study introduces a scalable smart home platform prototype that demonstrates possible solutions to address issues and limitations posed by the pandemic, such as improving package design, enabling user-driven installation, and facilitating remote evaluation and maintenance. The prototype uses off-the-shelf products with specially designed packaging to ensure interoperability as well as ease of shipping and installation. In this study, the prototype kits were shipped to participants' homes to understand and evaluate user perceptions and experiences around installation and initial use. Responses to a post-installation questionnaire and remote monitoring of system status showed that the participants easily completed their self-installation of the prototype without any on-site support. The study also showed potential for a scenario-based evaluation of the prototype using a remote, contactless research procedure.",,,,,,,,,,,,,,,,,,,,,,,,,,scalability; case study; pandemic; research method design; smart home; user experience,,,221
10.1145/3513002,0,0,0,0,0,341,"Jamil, Bushra; Ijaz, Humaira; Shojafar, Mohammad; Munir, Kashif; Buyya, Rajkumar","Resource Allocation and Task Scheduling in Fog Computing and Internet of Everything Environments: A Taxonomy, Review, and Future Directions",ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3513002,2022-09,,,,,,,"The Internet of Everything paradigm is being rapidly adopted in developing applications for different domains like smart agriculture, smart city, big data streaming, and so on. These IoE applications are leveraging cloud computing resources for execution. Fog computing, which emerged as an extension of cloud computing, supports mobility, heterogeneity, geographical distribution, context awareness, and services such as storage, processing, networking, and analytics on nearby fog nodes. The resource-limited, heterogeneous, dynamic, and uncertain fog environment makes task scheduling a great challenge that needs to be investigated. The article is motivated by this consideration and presents a systematic, comprehensive, and detailed comparative study by discussing the merits and demerits of different scheduling algorithms, focused optimization metrics, and evaluation tools in the fog computing and IoE environment. The goal of this survey article is fivefold. First, we review the fog computing and IoE paradigms. Second, we delineate the optimization metric engaged with fog computing and IoE environment. Third, we review, classify, and compare existing scheduling algorithms dealing with fog computing and IoE environment paradigms by leveraging some examples. Fourth, we rationalize the scheduling algorithms and point out the lesson learned from the survey. Fifth, we discuss the open issues and future research directions to improve scheduling in fog computing and the IoE environment.",,,,,,,,,,,,,,,,,,,,,,,,,,Cloud computing; resource allocation; task scheduling; fog computing; Internet of Everything (IoE); Internet of Things (IoT),0.0,EC3,3126
10.1145/3517206.3526266,0,0,0,0,0,22,"Zilic, Josip; De Maio, Vincenzo; Aral, Atakan; Brandic, Ivona",Edge offloading for microservice architectures,"Proceedings of the 5th International Workshop on Edge Systems, Analytics and Networking",978-1-4503-9253-2,,https://doi.org/10.1145/3517206.3526266,2022,EdgeSys '22,,,,Association for Computing Machinery,"New York, NY, USA","Edge offloading is widely used to support the execution of near real-time mobile applications. However, offloading on edge infrastructures can suffer from failures due to the absence of supporting systems and environmental factors. We propose a fault-tolerant offloading method modeled as a Markov Decision Process (MDP) based on predictions performed through Support Vector Regression (SVR). SVR is used to estimate offloading service availability, which is used by MDP for offloading decisions. Our approach is implemented in a real-world test-bed and compared with the default Kubernetes scheduler augmented with hybrid fault-tolerance.",,,,,,,,,,,,,,,,,,,,,,,,,,microservices; kubernetes; containers; edge offloading,0.0,EC3,2651
10.1145/3520304.3533997,1,1,1,1,0,103,"Khalloof, Hatem; Ciftci, Sergen; Shahoud, Shadi; Duepmeier, Clemens; Foerderer, Kevin; Hagenmeyer, Veit",Facilitating the hybridization of parallel evolutionary algorithms in cluster computing environments,Proceedings of the Genetic and Evolutionary Computation Conference Companion,978-1-4503-9268-6,,https://doi.org/10.1145/3520304.3533997,2022,GECCO '22,,,,Association for Computing Machinery,"New York, NY, USA","Evolutionary Algorithms (EAs) need domain-specific adaptations for achieving better results, tend to converge to suboptimal solutions and are computationally expensive when they are applied to complex and large-scale optimization problems. For addressing these challenges, hybridizing EAs with other algorithms and methods and parallelizing them in cluster computing environments represent essential solutions. In the present paper, a new software solution for supporting the hybridizations of parallel EAs is proposed. Unlike other software solutions for hybridizing EAs, the proposed software solution provides a flexible, generic and scalable mechanism for integrating any algorithmic approach like a Machine Learning (ML) algorithm to seed the initial population of parallel EAs. It is designed based on three modern software technologies, namely microservices, container virtualization and the publish/subscribe messaging paradigm. The applicability and generality of the presented software solution is tested by hybridizing the General Learning Evolutionary Algorithm and Method (GLEAM) with ML techniques for solving the problem of scheduling Distributed Energy Resources (DERs). The benchmarking tests are performed in a cluster computing environment. The obtained results show that the new software solution represents a successful approach to facilitate the hybridization of parallel EAs paving the road for future applications of EAs in several domains.",,,,,,,,,,,,,,,,,,,,,,,,,,microservice; scalability; container; global model; hybrid evolutionary algorithms; parallel computing; parallel evolutionary algorithms; scheduling distributed energy resources; virtualization,,,367
10.1145/3522741,0,0,0,0,0,316,"Shi, Yimin; Duan, Haihan; Yang, Lei; Cai, Wei",An Energy-efficient and Privacy-aware Decomposition Framework for Edge-assisted Federated Learning,ACM Trans. Sen. Netw.,,1550-4859,https://doi.org/10.1145/3522741,2022-11,,,,,,,"Deep Learning (DL) is an essential technology for modern intelligent sensor network and interactive multimedia applications, having problems with user data privacy when training on a central cloud. While Federated Learning (FL) motivates to preserve user privacy, it also causes new problems of lower user terminal usability and training efficiency, which caused substantial energy consumption. This article proposes a novel energy-efficient and privacy-aware decomposition framework to improve user-side FL efficiency under pre-defined privacy requirements with the assistance of Mobile Edge Computing (MEC) and Software Decomposition. It takes the propagation of each neural layer as the migrating unit and considers the tradeoff relationship between privacy and efficiency. We also propose an online scheduling algorithm to optimize the framework’s training performance. Furthermore, we summarize eight privacy-sensitive information classes on which existing privacy attacks base and design configurable privacy preservation mechanisms for each class. Simulations and experiments prove the effectiveness of our framework and algorithm in FL efficiency improvement and the effects of different privacy constraints on the overall training efficiency.",,,,,,,,,,,,,,,,,,,,,,,,,,distributed computing; federated learning; Mobile edge computing; software decomposition,0.0,EC3,2615
10.1145/3524106,0,0,0,0,0,396,"Husen, Arif; Chaudary, Muhammad Hasanain; Ahmad, Farooq",A Survey on Requirements of Future Intelligent Networks: Solutions and Future Research Directions,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3524106,2022-11,,,,,,,"The context of this study examines the requirements of Future Intelligent Networks (FIN), solutions, and current research directions through a survey technique. The background of this study is hinged on the applications of Machine Learning (ML) in the networking field. Through careful analysis of literature and real-world reports, we noted that ML has significantly expedited decision-making processes, enhanced intelligent automation, and helped resolve complex problems economically in different fields of life. Various researchers have also envisioned future networks incorporating intelligent functions and operations with ML. Several efforts have been made to automate individual functions and operations in the networking domain; however, most of the existing ML models proposed in the literature lack several vital requirements. Hence, this study aims to present a comprehensive summary of the requirements of FIN and propose a taxonomy of different network functionalities that needs to be equipped with ML techniques. The core objectives of this study are to provide a taxonomy of requirements envisioned for end-to-end FIN, relevant ML techniques, and their analysis to find research gaps, open issues, and future research directions. The real benefit of ML applications in any domain can only be ensured if intelligent capabilities cover all of its components. We observed that future generations of networks are heterogeneous, multi-vendor, and multidimensional, and ML can provide optimal results only if intelligent capabilities are used on a holistic scale. Realizing intelligence on a holistic scale is only possible if the ML algorithms can solve heterogeneous problems in a multi-vendor and multidimensional environment. ML models must be reliable and efficient, support, and possess the capability to learn and share the knowledge across the network layers and administrative domains to solve issues. First, this study ascertains the requirements of the FIN and proposes their taxonomy through reviews on envisioned ideas by various researchers and articles gathered from reputed conferences and standard developing organizations using keyword queries. Second, we have reviewed existing studies on ML applications focusing on coverage, heterogeneity, distributed architecture, and cross-domain knowledge learning and sharing. Our study observed that in the past, ML applications were focused mainly on an individual/isolated level only, and aspects of global and deep holistic learning with cross-layer/cross-domain knowledge sharing with agile ML operations are not explored at large. We recommend that the issues mentioned previously be addressed with improved ML architecture and agile operations and propose an ML pipeline based architecture for FIN. The significant contribution of this study is the impetus for researchers to seek ML models suitable for a modular, distributed, multi-domain, and multi-layer environment and provide decision making on a global or holistic rather than an individual function level.",,,,,,,,,,,,,,,,,,,,,,,,,,cross-administrative domain learning; cross-layer learning; deep holistic learning; feature sharing; Future intelligent networks; global learning; knowledge sharing,0.0,EC3,2986
10.1145/3524458.3547254,0,0,0,0,0,211,"Ayaz, Berk; Slamnik-Kriještorac, Nina; Marquez-Barja, Johann",Data Management Platform For Smart Orchestration of Decentralized and Heterogeneous Vehicular Edge Networks,Proceedings of the 2022 ACM Conference on Information Technology for Social Good,978-1-4503-9284-6,,https://doi.org/10.1145/3524458.3547254,2022,GoodIT '22,,,,Association for Computing Machinery,"New York, NY, USA","Intelligent edge orchestration has become a vital component within next generation communication networks, such as 5G. They offer optimal resource allocation and service distribution, hence allow for full utilization of the opportunities provided by those networks. Orchestrators make use of Machine Learning (ML) techniques to determine the most optimal operational decisions, such as deployment and scaling of services, ensuring the quality of the service performance. The training and validation of these models require significant amount of data. However, in such environments we deal with heterogeneous and distributed data sources, in which the data needs to be collected and pre-processed efficiently, and as such, made ready-to-use for these ML models. Hence, in this paper several state-of-the-art data management technologies suitable for edge computing and orchestration are investigated and compared. After investigating the theoretical features of these technologies, they are deployed and tested on the Smart Highway testbed. The strengths and shortcomings of these systems are presented and compared based on the Quality of Service (QoS) requirements for various vehicular services dealing with highly mobile users, i.e., vehicles, which are considered as quite stringent. This hybrid platform, combining several data management technologies, will be used to assist and enrich the research on smart edge orchestration at IDLab and serve as a reference for other interested parties.",,,,,,,,,,,,,,,,,,,,,,,,,,Edge Computing; ActiveMQ; Data Management; Edge Orchestration; Kafka; Pulsar; V2X; Zenoh,0.0,EC3,1857
10.1145/3524616,0,0,0,0,0,403,"Kundan, Shivam; Marinakis, Theodoros; Anagnostopoulos, Iraklis; Kagaris, Dimitri",A Pressure-Aware Policy for Contention Minimization on Multicore Systems,ACM Trans. Archit. Code Optim.,,1544-3566,https://doi.org/10.1145/3524616,2022-05,,,,,,,"Modern Chip Multiprocessors (CMPs) are integrating an increasing amount of cores to address the continually growing demand for high-application performance. The cores of a CMP share several components of the memory hierarchy, such as Last-Level Cache (LLC) and main memory. This allows for considerable gains in multithreaded applications while also helping to maintain architectural simplicity. However, sharing resources can also result in performance bottleneck due to contention among concurrently executing applications. In this work, we formulate a fine-grained application characterization methodology that leverages Performance Monitoring Counters (PMCs) and Cache Monitoring Technology (CMT) in Intel processors. We utilize this characterization methodology to develop two contention-aware scheduling policies, one static and one dynamic, that co-schedule applications based on their resource-interference profiles. Our approach focuses on minimizing contention on both the main-memory bandwidth and the LLC by monitoring the pressure that each application inflicts on these resources. We achieve performance benefits for diverse workloads, outperforming Linux and three state-of-the-art contention-aware schedulers in terms of system throughput and fairness for both single and multithreaded workloads. Compared with Linux, our policy achieves up to 16% greater throughput for single-threaded and up to 40% greater throughput for multithreaded applications. Additionally, the policies increase fairness by up to 65% for single-threaded and up to 130% for multithreaded ones.",,,,,,,,,,,,,,,,,,,,,,,,,,Chip multiprocessors; contention-aware scheduling; fairness; Linux; pressure minimization; throughput,0.0,EC3,2731
10.1145/3524844.3528052,0,0,0,0,0,164,"Gheibi, Omid; Weyns, Danny",Lifelong self-adaptation: self-adaptation meets lifelong machine learning,Proceedings of the 17th Symposium on Software Engineering for Adaptive and Self-Managing Systems,978-1-4503-9305-8,,https://doi.org/10.1145/3524844.3528052,2022,SEAMS '22,,,,Association for Computing Machinery,"New York, NY, USA","In the past years, machine learning (ML) has become a popular approach to support self-adaptation. While ML techniques enable dealing with several problems in self-adaptation, such as scalable decision-making, they are also subject to inherent challenges. In this paper, we focus on one such challenge that is particularly important for self-adaptation: ML techniques are designed to deal with a set of predefined tasks associated with an operational domain; they have problems to deal with new emerging tasks, such as concept shift in input data that is used for learning. To tackle this challenge, we present lifelong self-adaptation: a novel approach to self-adaptation that enhances self-adaptive systems that use ML techniques with a lifelong ML layer. The lifelong ML layer tracks the running system and its environment, associates this knowledge with the current tasks, identifies new tasks based on differentiations, and updates the learning models of the self-adaptive system accordingly. We present a reusable architecture for lifelong self-adaptation and apply it to the case of concept drift caused by unforeseen changes of the input data of a learning model that is used for decision-making in self-adaptation. We validate lifelong self-adaptation for two types of concept drift using two cases.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1022
10.1145/3524844.3528057,0,0,0,0,0,200,"Dobaj, Jürgen; Riel, Andreas; Krug, Thomas; Seidl, Matthias; Macher, Georg; Egretzberger, Markus",Towards digital twin-enabled DevOps for CPS providing architecture-based service adaptation &amp; verification at runtime,Proceedings of the 17th Symposium on Software Engineering for Adaptive and Self-Managing Systems,978-1-4503-9305-8,,https://doi.org/10.1145/3524844.3528057,2022,SEAMS '22,,,,Association for Computing Machinery,"New York, NY, USA","Background: Industrial Product-Service Systems (IPSS) denote a service-oriented way of providing access to cyber-physical systems' (CPS) capabilities. The design of such systems bears high risk due to uncertainty in requirements related to service function and behavior, operation environments, and evolving customer needs. Such risks and uncertainties are well known in the IT sector, where DevOps principles ensure continuous system improvement through reliable and frequent delivery processes. A modular and service-oriented system architecture complements these processes to facilitate IT system adaptation and evolution.Objective: This work proposes a method to use and extend the Digital Twins (DTs) of IPSS assets for enabling the continuous optimization of CPS service delivery and the latter's adaptation to changing needs and environments. This reduces uncertainty during design and operations by assuring IPSS integrity and availability, especially for design and service adaptations at CPS runtime.Methodology: The method builds on transferring IT DevOps principles to DT-enabled CPS IPSS. The chosen design approach integrates, reuses, and aligns the DT processing and communication resources with DevOps requirements derived from literature.Results: We use these requirements to propose a DT-enabled self-adaptive CPS model, which guides the realization of DT-enabled DevOps in CPS IPSS. We further propose detailed design models for operation-critical DTs that integrate CPS closed-loop control and architecture-based CPS adaptation. This integrated approach enables the implementation of A/B testing as a use case and central concept to enable CPS IPSS service adaptation and reconfiguration.Conclusion: The self-adaptive CPS model and DT design concept have been validated in an evaluation environment for operation-critical CPS IPSS. The demonstrator achieved sub-millisecond cycle times during service A/B testing at runtime without causing CPS operation interferences and downtime.",,,,,,,,,,,,,,,,,,,,,,,,,,self-adaptation; DevOps; CPS; digital twin; deployment; IPSS,,,1137
10.1145/3526060.3535461,0,0,0,0,0,282,"Ossen, Sabra; Brasilino, Lucas R. B.; Dalessandro, Luke; Swany, Martin",Enabling Stateful Functions for Stream Processing in the Programmable Data Plane,Proceedings of the 2nd Workshop on High Performance Serverless Computing,978-1-4503-9311-9,,https://doi.org/10.1145/3526060.3535461,2022,HiPS '22,,,,Association for Computing Machinery,"New York, NY, USA","Sensor-rich environments are crucial components of the Internet of Things ecosystem and benefit from real-time applications. Many applications perform real-time analytics on these IoT workloads by performing continuous stream processing for a window of sequence data elements. However, executing light-weight stateful functions on server CPUs adds to the communication latency of each small message in a high data rate environment, primarily due to messages traveling through a complex network stack to reach the CPU. Thus, we present an in-network function deployment architecture with low latency and low resource footprint by introducing a new compute layer. We propose an FPGA-based Switch/NIC prototype with a compute layer utilizing RISC-V soft cores and High-Level Synthesis modules. We evaluate the design for two microbenchmarks on a Zynq 7000 FPGA each, achieving less than 10 μs in latency and consuming less than 6 % of resources.",,,,,,,,,,,,,,,,,,,,,,,,,,stream processing; programmable networks; fpga; in-network computing; risc-v soft core; stateful faas,0.0,EC3,2740
10.1145/3527312,0,0,0,0,0,421,"Wang, Chengpeng; Yao, Peisen; Tang, Wensheng; Shi, Qingkai; Zhang, Charles",Complexity-guided container replacement synthesis,Proc. ACM Program. Lang.,,,https://doi.org/10.1145/3527312,2022-04,,,,,,,"Containers, such as lists and maps, are fundamental data structures in modern programming languages. However, improper choice of container types may lead to significant performance issues. This paper presents Cres, an approach that automatically synthesizes container replacements to improve runtime performance. The synthesis algorithm works with static analysis techniques to identify how containers are utilized in the program, and attempts to select a method with lower time complexity for each container method call. Our approach can preserve program behavior and seize the opportunity of reducing execution time effectively for general inputs. We implement Cres and evaluate it on 12 real-world Java projects. It is shown that Cres synthesizes container replacements for the projects with 384.2 KLoC in 14 minutes and discovers six categories of container replacements, which can achieve an average performance improvement of 8.1%.",,,,,,,,,,,,,,,,,,,,,,,,,,data structure specification; program optimization; program synthesis,,,817
10.1145/3528535.3565242,1,1,1,1,1,34,"Zandberg, Koen; Baccelli, Emmanuel; Yuan, Shenghao; Besson, Frédéric; Talpin, Jean-Pierre",Femto-containers: lightweight virtualization and fault isolation for small software functions on low-power IoT microcontrollers,Proceedings of the 23rd ACM/IFIP International Middleware Conference,978-1-4503-9340-9,,https://doi.org/10.1145/3528535.3565242,2022,Middleware '22,,,,Association for Computing Machinery,"New York, NY, USA","Low-power operating system runtimes used on IoT microcontrollers typically provide rudimentary APIs, basic connectivity and, sometimes, a (secure) firmware update mechanism. In contrast, on less constrained hardware, networked software has entered the age of serverless, microservices and agility. With a view to bridge this gap, in the paper we design Femto-Containers, a new middleware runtime which can be embedded on heterogeneous low-power IoT devices. Femto-Containers enable the secure deployment, execution and isolation of small virtual software functions on low-power IoT devices, over the network. We implement Femto-Containers, and provide integration in RIOT, a popular open source IoT operating system. We then evaluate the performance of our implementation, which was formally verified for fault-isolation, guaranteeing that RIOT is shielded from logic loaded and executed in a Femto-Container. Our experiments on various popular micro-controller architectures (Arm Cortex-M, ESP32 and RISC-V) show that Femto-Containers offer an attractive trade-off in terms of memory footprint overhead, energy consumption, and security.",,,,,,,,,,,,,,,,,,,,,,,,,,security; IoT; middleware; container; function-as-a-service; low-power; microcontroller; virtual machine,0.0,IC4,130
10.1145/3530692,0,0,0,0,0,410,"Murturi, Ilir; Dustdar, Schahram",DECENT: A Decentralized Configurator for Controlling Elasticity in Dynamic Edge Networks,ACM Trans. Internet Technol.,,1533-5399,https://doi.org/10.1145/3530692,2022-08,,,,,,,"Recent advancements in distributed systems have enabled deploying low-latency and highly resilient edge applications close to the IoT domain at the edge of the network. The broad range of edge application requirements combined with heterogeneous, resource-constrained, and dynamic edge networks make it particularly challenging to configure and deploy them. Besides that, missing elastic capabilities on the edge makes it difficult to operate such applications under dynamic workloads. To this end, this article proposes a lightweight, self-adaptive, and decentralized mechanism (DECENT) for (1) deploying edge applications on edge resources and on premises of Edge-Cloud infrastructure and (2) controlling elasticity requirements. DECENT enables developers to characterize their edge applications by specifying elasticity requirements, which are automatically captured, interpreted, and enforced by our decentralized elasticity interpreters. In response to dynamic workloads, edge applications automatically adapt in compliance with their elasticity requirements. We discuss the architecture, processes of the approach, and the experiment conducted on a real-world testbed to validate its feasibility on low-powered edge devices. Furthermore, we show performance and adaptation aspects through an edge safety application and its evolution in elasticity space (i.e., cost, resource, and quality).",,,,,,,,,,,,,,,,,,,,,,,,,,edge computing; Internet of Things (IoT); Elasticity; edge-cloud,0.0,EC3,2699
10.1145/3533378,0,0,0,0,0,364,"Paleyes, Andrei; Urma, Raoul-Gabriel; Lawrence, Neil D.",Challenges in Deploying Machine Learning: A Survey of Case Studies,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3533378,2022-12,,,,,,,"In recent years, machine learning has transitioned from a field of academic research interest to a field capable of solving real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries, and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. By mapping found challenges to the steps of the machine learning deployment workflow, we show that practitioners face issues at each stage of the deployment process. The goal of this article is to lay out a research agenda to explore approaches addressing these challenges.",,,,,,,,,,,,,,,,,,,,,,,,,,Machine learning applications; sofware deployment,,,449
10.1145/3538637.3538865,0,0,0,0,0,123,"Neupane, Bijay; Siksnys, Laurynas; Pedersen, Torben Bach; Hagensby, Rikke; Aftab, Muhammad; Eck, Bradley; Fusco, Francesco; Gormally, Robert; Purcell, Mark; Tirupathi, Seshu; Cerne, Gregor; Brus, Saso; Papageorgiou, Ioannis; Meindl, Gerhard; Roduit, Pierre","GOFLEX: extracting, aggregating and trading flexibility based on FlexOffers for 500+ prosumers in 3 European cities [operational systems paper]",Proceedings of the Thirteenth ACM International Conference on Future Energy Systems,978-1-4503-9397-3,,https://doi.org/10.1145/3538637.3538865,2022,e-Energy '22,,,,Association for Computing Machinery,"New York, NY, USA","A demand response scheme that uses direct device control to actively exploit prosumer flexibility has been identified as a key remedy to meet the challenge of increased renewable energy sources integration. Although a number of direct control-based demand response solutions exist and have been successfully deployed and demonstrated in the real world, they are typically designed for, and are effective only at small scale and/or target specific types of loads, leading to relatively high cost-of-entry. This prohibits deploying scalable solutions.The H2020 GOFLEX project has addressed this issue and developed a scalable, general, and replicable so-called GOFLEX system, which offers a market-driven approach to solve congestion problems in distribution grids based on aggregated individual flexibilities from a wide range of prosumers, both small (incl. electric vehicles, heat-pumps, boilers, freezers, fridges) and large (incl. factories, water pumping stations, etc.). By encompassing individual prosumers, aggregators, distribution system operators, and energy multi-utilities and retailers. It is a system of systems, where all flexibilities in electricity demand, production, and storage are extracted, (dis)aggregated, optimized, and traded using the powerful and standardized FlexOffer format, yielding a general and replicable solution with low cost-of-entry. The system has been successfully deployed in Switzerland, Germany, and Cyprus where it has controlled loads of 500+ prosumers, with a total of 800MWh flexibility offered on the market, offering up to 64% of adaptability in peak demand. In this paper, we present the overall architecture of the GOFLEX system, its sub-systems, and the interaction between these sub-systems. We then discuss the configurations, observations, and key results of using the GOFLEX system both in the aforementioned 3 demo sites - within the GOFLEX project and after the project.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,625
10.1145/3542929.3563473,0,0,0,0,0,173,"Gain, Gaulthier; Soldani, Cyril; Huici, Felipe; Mathy, Laurent",Want more unikernels? inflate them!,Proceedings of the 13th Symposium on Cloud Computing,978-1-4503-9414-7,,https://doi.org/10.1145/3542929.3563473,2022,SoCC '22,,,,Association for Computing Machinery,"New York, NY, USA","Unikernels are on the rise in the cloud. These lightweight virtual machines (VMs) specialized to a single application offer the same level of isolation as full-blown VMs, while providing performance superior to standard Linux-based VMs or even to containers. However, their inherent specialization renders memory deduplication ineffective, causing unikernels, in practice, to consume more memory than their small memory footprint would suggest. This makes them less advantageous when thousands of SaaS and/or FaaS unikernels instances have to run on the same server.In this paper we introduce a novel approach to build the next generation of networked services and lambda functions by improving unikernel's memory layout so that it is more likely to share identical pages with other unikernels deployed on the system. Our approach supports SaaS and FaaS architectures and can be used with ASLR. Our experiments show that our approach can reduce the amount of physical memory used by a set of unikernels running on the same server by as much as 3x, with next to no overhead on applications performance.",,,,,,,,,,,,,,,,,,,,,,,,,,virtualization; virtual machine; alignment; deduplication; hypervisor; memory; operating systems; unikernels,,,186
10.1145/3543507.3583437,0,0,0,0,0,197,"Savasci, Mehmet; Ali-Eldin, Ahmed; Eker, Johan; Robertsson, Anders; Shenoy, Prashant",DDPC: Automated Data-Driven Power-Performance Controller Design on-the-fly for Latency-sensitive Web Services,Proceedings of the ACM Web Conference 2023,978-1-4503-9416-1,,https://doi.org/10.1145/3543507.3583437,2023,WWW '23,,,,Association for Computing Machinery,"New York, NY, USA","Traditional power reduction techniques such as DVFS or RAPL are challenging to use with web services because they significantly affect the services’ latency and throughput. Previous work suggested the use of controllers based on control theory or machine learning to reduce performance degradation under constrained power. However, generating these controllers is challenging as every web service applications running in a data center requires a power-performance model and a fine-tuned controller. In this paper, we present DDPC, a system for autonomic data-driven controller generation for power-latency management. DDPC automates the process of designing and deploying controllers for dynamic power allocation to manage the power-performance trade-offs for latency-sensitive web applications such as a social network. For each application, DDPC uses system identification techniques to learn an adaptive power-performance model that captures the application’s power-latency trade-offs which is then used to generate and deploy a Proportional-Integral (PI) power controller with gain-scheduling to dynamically manage the power allocation to the server running application using RAPL. We evaluate DDPC with two realistic latency-sensitive web applications under varying load scenarios. Our results show that DDPC is capable of autonomically generating and deploying controllers within a few minutes reducing the active power allocation of a web-server by more than 50% compared to state-of-the-art techniques while maintaining the latency well below the target of the application.",,,,,,,,,,,,,,,,,,,,,,,,,,datacenter; power-management; Web service performance,0.0,EC3,2041
10.1145/3545008.3545094,0,0,0,0,0,120,"Stewart, Christopher; Morris, Nathaniel; Chen, Lydia; Birke, Robert",Performance Modeling for Short-Term Cache Allocation,Proceedings of the 51st International Conference on Parallel Processing,978-1-4503-9733-9,,https://doi.org/10.1145/3545008.3545094,2023,ICPP '22,,,,Association for Computing Machinery,"New York, NY, USA","Short-term cache allocation grants and then revokes access to processor cache lines dynamically. For online services, short-term allocation can speed up targeted query executions and free up cache lines reserved, but normally not needed, for performance. However, in collocated settings, short-term allocation can increase cache contention, slowing down collocated query executions. To offset slowdowns, collocated services may request short-term allocation more often, making the problem worse. Short-term allocation policies manage which queries receive cache allocations and when. In collocated settings, these policies should balance targeted query speedups against slowdowns caused by recurring cache contention. We present a model-driven approach that (1) predicts response time under a given policy, (2) explores competing policies and (3) chooses policies that yield low response time for all collocated services. Our approach profiles cache usage offline, characterizes the effects of cache allocation policies using deep learning techniques and devises novel performance models for short-term allocation with online services. We tested our approach using data processing, cloud, and high-performance computing benchmarks collocated on Intel processors equipped with Cache Allocation Technology. Our models predicted median response time with 11% absolute percent error. Short-term allocation policies found using our approach out performed state-of-the-art shared cache allocation policies by 1.2–2.3X.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,754
10.1145/3550355.3552405,0,0,0,0,0,297,"Li, Jia; Nejati, Shiva; Sabetzadeh, Mehrdad; McCallen, Michael",A domain-specific language for simulation-based testing of IoT edge-to-cloud solutions,Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems,978-1-4503-9466-6,,https://doi.org/10.1145/3550355.3552405,2022,MODELS '22,,,,Association for Computing Machinery,"New York, NY, USA","The Internet of things (IoT) is increasingly prevalent in domains such as emergency response, smart cities and autonomous vehicles. Simulation plays a key role in the testing of IoT systems, noting that field testing of a complete IoT product may be infeasible or prohibitively expensive. In this paper, we propose a domain-specific language (DSL) for generating edge-to-cloud simulators. An edge-to-cloud simulator executes the functionality of a large array of edge devices that communicate with cloud applications. Our DSL, named IoTECS, is the result of a collaborative project with an IoT analytics company, Cheetah Networks. The industrial use case that motivates IoTECS is ensuring the scalability of cloud applications by putting them under extreme loads from IoT devices connected to the edge. We implement IoTECS using Xtext and empirically evaluate its usefulness. We further reflect on the lessons learned.",,,,,,,,,,,,,,,,,,,,,,,,,,IoT; simulation; domain-specific languages; stress testing; xtext,,,244
10.1145/3552326.3587459,0,0,0,0,0,231,"Park, Misun; Bhardwaj, Ketan; Gavrilovska, Ada",Pocket: ML Serving from the Edge,Proceedings of the Eighteenth European Conference on Computer Systems,978-1-4503-9487-1,,https://doi.org/10.1145/3552326.3587459,2023,EuroSys '23,,,,Association for Computing Machinery,"New York, NY, USA","One of the major challenges in serving ML applications is the resource pressure introduced by the underlying ML frameworks. This becomes a bigger problem at resource-constrained, multi-tenant edge server locations, where it is necessary to scale to a larger number of clients with a fixed resource envelope. Naive approaches which simply minimize the resource budget allocation of each application result in performance degradation that voids the benefits expected from operating at the edge.This paper presents Pocket - a new approach for serving ML applications in settings like the edge, based on a shared ML runtime backend as a service and lightweight ML application pocket containers. Key to realizing Pocket is use of lightweight IPC, support for cross-client isolation, and a novel resource amplification method which inlines resource reallocation with IPC. The latter ensures just-in-time assignment of the limited edge resources where they're most needed, thereby reducing contention effects and boosting overall performance and efficiency. Experimental evaluations demonstrate that Pocket can scale to 1.3–20× more clients with the same amount of resources while reducing response time by 20–80% compared to monolithic designs.",,,,,,,,,,,,,,,,,,,,,,,,,,resource management; containers; edge computing; isolation; IPC; ML serving; runtime-as-a-service; visual analytics,0.0,EC3,3146
10.1145/3558819.3558839,0,0,0,0,0,92,"Ma, Xin; Zhang, Shengchun; Xu, Lina; Liu, Chang; Zhao, Ergang; Ding, Yujie",Architecture Analysis of Energy Information Sharing Platform for New Power System,Proceedings of the 7th International Conference on Cyber Security and Information Engineering,978-1-4503-9741-4,,https://doi.org/10.1145/3558819.3558839,2022,ICCSIE '22,,,,Association for Computing Machinery,"New York, NY, USA","Under the trend of new power system construction and digital transformation, the power system generates a large amount of data flow. It is urgent to build an energy power system information sharing platform to promote the efficient allocation of resources and maintain the orderly operation of the power system. This paper studies an energy information sharing platform for new power systems, and designs in detail from the overall architecture, development platform architecture, access architecture, deployment architecture, and functional architecture. The platform is established based on advanced information and communication technologies such as ""Cloud Computing, Internet of Things, Mobile Internet, Big Data"" and cloud-edge collaboration. By integrating functional modules such as coordination and optimization decision-making module, continuous power supply monitoring module, and unified information model, the construction of energy information sharing platform and system integration are realized. The platform integrates ""edge technology + multi-type communication + unified platform + data fusion + business intelligence"", realizes the deep integration of digital technology and physical electrical system, realizes the intelligent interaction and precise control of source, network, load and storage under the new power system.",,,,,,,,,,,,,,,,,,,,,,,,,,Energy information sharing; New power system; Platform architecture,0.0,EC3,1822
10.1145/3563647.3563651,0,0,0,0,0,265,"Bose, Abhik; Kirtikar, Shailendra; Chirumamilla, Shivaji; Shah, Rinku; Vutukuru, Mythili",AccelUPF: accelerating the 5G user plane using programmable hardware,Proceedings of the Symposium on SDN Research,978-1-4503-9892-3,,https://doi.org/10.1145/3563647.3563651,2022,SOSR '22,,,,Association for Computing Machinery,"New York, NY, USA","The latest generation of 5G telecommunication networks are expected to provide high throughput and low latency while catering to diverse applications like mobile broadband, dense IoT, and self-driving cars. A high performance User Plane Function (UPF), the main element in the 5G user plane, is critical to achieving these performance goals. This paper presents AccelUPF, a 5G UPF that offloads functionality to programmable dataplane hardware for performance acceleration. While prior work has proposed accelerating the UPF by offloading its data forwarding functionality to programmable hardware, the Packet Forwarding Control Protocol (PFCP) messages from the control plane that configure the hardware data forwarding rules were still processed in software. We show that only offloading data forwarding and not PFCP message processing leads to suboptimal performance in the UPF for applications like IoT that have a much higher ratio of PFCP messages to data traffic, due to a bottleneck at the software control plane that configures the hardware packet forwarding rules. In contrast to prior work, AccelUPF offloads both PFCP message processing as well as data forwarding to programmable hardware. AccelUPF overcomes several technical challenges pertaining to the processing of the complex variable-sized PFCP messages within the memory and compute constraints of programmable hardware platforms. Our evaluation of AccelUPF implemented over a Netronome programmable NIC and an Intel Tofino programmable switch demonstrates performance gains over the state-of-the-art UPFs for real-world traffic scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,in-network computation; 5G core; 5G user plane; programmable networks,,,653
10.1145/3565010.3569065,1,1,1,1,1,50,"Tootaghaj, Diman Zad; Mercian, Anu; Adarsh, Vivek; Sharifian, Mehrnaz; Sharma, Puneet",SmartNICs at edge for transient compute elasticity,Proceedings of the 3rd International Workshop on Distributed Machine Learning,978-1-4503-9922-7,,https://doi.org/10.1145/3565010.3569065,2022,DistributedML '22,,,,Association for Computing Machinery,"New York, NY, USA","This paper proposes a new architecture that strategically harvests the untapped compute capacity of the SmartNICs to offload transient microservices workload spikes, thereby reducing the SLA violations while providing better performance/energy consumption. This is particularly important for ML workloads at Edge deployments with stringent SLA requirements. Usage of the untapped compute capacity is more favorable than deploying extra servers, as SmartNICs are economically and operationally more desirable. We propose Spike-Offload, a low-cost and scalable platform that leverages machine learning to predict the spikes and orchestrates seamless offloading of generic microservices workloads to the SmartNICs, eliminating the need for pre-deploying expensive host servers and their under-utilization. Our SpikeOffload evaluation shows that SLA violations can be reduced by up to 20% for specific workloads. Furthermore, we demonstrate that for specific workloads our approach can potentially reduce capital expenditure (CAPEX) by more than 40%. Also, performance per unit energy consumption can be improved by upto 2X.",,,,,,,,,,,,,,,,,,,,,,,,,,edge; serverless computing; application offload; SmartNIC,0.0,IC4,133
10.1145/3565387.3565402,0,0,0,0,0,134,"Chen, Lili; Di, Yingqi; Zhang, Lele",Implementation of Cloud-based Urban Rail Big Data Platform,Proceedings of the 6th International Conference on Computer Science and Application Engineering,978-1-4503-9600-4,,https://doi.org/10.1145/3565387.3565402,2022,CSAE '22,,,,Association for Computing Machinery,"New York, NY, USA","Facing the wave of digital transformation of urban rail, it has proposed a cloud-based big data platform construction plan following the innovative urban rail development in the paper. The cloud platform provides virtualized resources and services such as computing, storage, networking, security, and Platform-as-a-Service components for the big data platform. The big data platform works as the Data-as-a-Service layer of the cloud, which integrates multiple business application data, conducts data governance, forms data assets, and provides secure, universal, and flexible data sharing services to various advanced intelligent applications. The low-code historical data interface and real-time data service is an innovative function to meet new demands for intelligent operation. Data modeling and optimization in the face of the characteristics of the Hadoop platform are also mentioned. The solution provides data support for the application of intelligent urban rail and realizes the integration of cloud and big data platforms. Meanwhile, the data is passed from producers to consumers through data reorganization of big data platforms, which could isolate the influence of data errors and model changes.",,,,,,,,,,,,,,,,,,,,,,,,,,Big Data; Cloud Platform; DaaS; Real-time Service; Urban Rail Transit,,,416
10.1145/3567445.3571114,0,0,0,0,0,64,"Buschmann, Philippe; Shorim, Mostafa H. M.; Helm, Max; Bröring, Arne; Carle, Georg",Task Allocation in Industrial Edge Networks with Particle Swarm Optimization and Deep Reinforcement Learning,Proceedings of the 12th International Conference on the Internet of Things,978-1-4503-9665-3,,https://doi.org/10.1145/3567445.3571114,2023,IoT '22,,,,Association for Computing Machinery,"New York, NY, USA","To avoid the disadvantages of a cloud-centric infrastructure, next-generation industrial scenarios focus on using distributed edge networks. Task allocation in distributed edge networks with regards to minimizing the energy consumption is NP-hard and requires considerable computational effort to obtain optimal results with conventional algorithms like Integer Linear Programming (ILP). We extend an existing ILP problem including an ILP heuristic for multi-workflow allocation and propose a Particle Swarm Optimization (PSO) and a Deep Reinforcement Learning (DRL) algorithm. PSO and DRL outperform the ILP heuristic with a median optimality gap of and against . DRL has the lowest upper bound for the optimality gap. It performs better than PSO for problem sizes of more than 25 tasks and PSO fails to find a feasible solution for more than 60 tasks. The execution time of DRL is significantly faster with a maximum of 1 s in comparison to PSO with a maximum of 361 s. In conclusion, our experiments indicate that PSO is more suitable for smaller and DRL for larger sized task allocation problems.",,,,,,,,,,,,,,,,,,,,,,,,,,Internet of Things (IoT); Edge Computing; Deep Reinforcement Learning; Integer Linear Programming; Particle Swarm Optimization; Task Allocation,,,290
10.1145/3567826,0,0,0,0,0,325,"Viola, Roberto; Martín, Ángel; Zorrilla, Mikel; Montalbán, Jon; Angueira, Pablo; Muntean, Gabriel-Miro",A Survey on Virtual Network Functions for Media Streaming: Solutions and Future Challenges,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3567826,2023-02,,,,,,,"Media services must ensure an enhanced user’s perceived quality during content playback to attract and retain audiences, especially while the streams are distributed remotely via networks. Thus, media streaming services rely heavily on good and predictable network performance when delivered to a large number of people. Furthermore, as the quality of media content gets high, the network performance demands are also increasing, and meeting them is challenging. Network functions devoted to improving media streaming services become essential to cope with the high dynamics of network performance and user mobility. Furthermore, new networking paradigms and architectures under the 5G networks umbrella are bringing new possibilities to deploy smart network functions, which monitor the media streaming services through live and objective metrics and boost them in real-time. This survey overviews the state-of-the-art technologies and solutions proposed to apply new network functions for enhancing media streaming.",,,,,,,,,,,,,,,,,,,,,,,,,,Media streaming; network functions; network virtualization,,,765
10.1145/3570361.3592503,0,0,0,0,0,31,"Li, Bolun; Zhao, Qidong; Jiao, Shuyin; Liu, Xu",DroidPerf: Profiling Memory Objects on Android Devices,Proceedings of the 29th Annual International Conference on Mobile Computing and Networking,978-1-4503-9990-6,,https://doi.org/10.1145/3570361.3592503,2023,ACM MobiCom '23,,,,Association for Computing Machinery,"New York, NY, USA","Optimizing performance inefficiencies in memory hierarchies is well-known for native languages, such as C and C++. There are few studies, however, on exploring memory inefficiencies in Android Runtime (ART). Running in ART, managed languages, such as Java and Kotlin, employ various abstractions, such as runtime support, ahead-of-time (AOT) compilation, and garbage collection (GC), which hide important execution details from the plain source code.In this paper, we develop DroidPerf, a lightweight, object-centric memory profiler for ART, which associates memory inefficiencies with objects created and used in Android apps. With such object-level information, DroidPerf is able to guide locality optimization on memory layouts, access patterns, and allocation patterns. Guided by DroidPerf, we optimize a number of popular Android apps and obtain significant performance gains. Many inefficiencies are confirmed by the code authors and optimization patches are under evaluation for upstreaming. As a practical tool, DroidPerf incurs 32% runtime overhead and 14% memory overhead on average. Furthermore, DroidPerf works in the production environment with off-the-shelf hardware, OS, Dalvik virtual machine, ART, and unmodified Android app source code.",,,,,,,,,,,,,,,,,,,,,,,,,,performance; profiling; Android; memory inefficiencies,,,550
10.1145/3571729,0,0,0,0,0,328,"Jeyaraj, Rathinaraja; Balasubramaniam, Anandkumar; M.A., Ajay Kumara; Guizani, Nadra; Paul, Anand",Resource Management in Cloud and Cloud-influenced Technologies for Internet of Things Applications,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3571729,2023-03,,,,,,,"The trend of adopting Internet of Things (IoT) in healthcare, smart cities, Industry 4.0, and so on is increasing by means of cloud computing, which provides on-demand storage and computation facilities over the Internet. To meet specific requirements of IoT applications, the cloud has also shifted its service offering platform to its next-generation models, such as fog, mist, and dew computing. As a result, the cloud and IoT have become part and parcel of smart applications that play significant roles in improving the quality of human life. In addition to the inherent advantages of advanced cloud models, to improve the performance of IoT applications further, it is essential to understand how the resources in the cloud and cloud-influenced platforms are managed to support various phases in the end-to-end IoT deployment. Considering this importance, in this article, we provide a brief description, a systematic review, and possible research directions on every aspect of resource management tasks, such as workload modeling, resource provisioning, workload scheduling, resource allocation, load balancing, energy management, and resource heterogeneity in such advanced platforms, from a cloud perspective. The primary objective of this article is to help early researchers gain insight into the underlying concepts of resource management tasks in the cloud for IoT applications.",,,,,,,,,,,,,,,,,,,,,,,,,,Cloud computing; resource allocation; Internet of Things; edge computing; dew computing; fog computing; load balancing; mist computing; resource heterogeneity; resource provisioning; resource scheduling,0.0,EC3,1855
10.1145/3572901,0,0,0,0,0,371,"Lubbers, Mart; Koopman, Pieter; Ramsingh, Adrian; Singer, Jeremy; Trinder, Phil",Could Tierless Languages Reduce IoT Development Grief?,ACM Trans. Internet Things,,,https://doi.org/10.1145/3572901,2023-02,,,,,,,"Internet of Things (IoT) software is notoriously complex, conventionally comprising multiple tiers. Traditionally an IoT developer must use multiple programming languages and ensure that the components interoperate correctly. A novel alternative is to use a single tierless language with a compiler that generates the code for each component and ensures their correct interoperation.We report a systematic comparative evaluation of two tierless language technologies for IoT stacks: one for resource-rich sensor nodes (Clean with iTask) and one for resource-constrained sensor nodes (Clean with iTask and mTask). The evaluation is based on four implementations of a typical smart campus application: two tierless and two Python-based tiered.(1) We show that tierless languages have the potential to significantly reduce the development effort for IoT systems, requiring 70% less code than the tiered implementations. Careful analysis attributes this code reduction to reduced interoperation (e.g., two embedded domain-specific languages and one paradigm versus seven languages and two paradigms), automatically generated distributed communication, and powerful IoT programming abstractions. (2) We show that tierless languages have the potential to significantly improve the reliability of IoT systems, describing how Clean iTask/mTask maintains type safety, provides higher-order failure management, and simplifies maintainability. (3) We report the first comparison of a tierless IoT codebase for resource-rich sensor nodes with one for resource-constrained sensor nodes. The comparison shows that they have similar code size (within 7%), and functional structure. (4) We present the first comparison of two tierless IoT languages, one for resource-rich sensor nodes and the other for resource-constrained sensor nodes.",,,,,,,,,,,,,,,,,,,,,,,,,,IoT stacks; Tierless languages,,,455
10.1145/3573009,0,0,0,0,0,353,"Lin, Weiwei; Xiong, Chennian; Wu, Wentai; Shi, Fang; Li, Keqin; Xu, Minxian",Performance Interference of Virtual Machines: A Survey,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3573009,2023-03,,,,,,,"The rapid development of cloud computing with virtualization technology has benefited both academia and industry. For any cloud data center at scale, one of the primary challenges is how to effectively orchestrate a large number of virtual machines (VMs) in a performance-aware and cost-effective manner. A key problem here is that the performance interference between VMs can significantly undermine the efficiency of cloud data centers, leading to performance degradation and additional operation cost. To address this issue, extensive studies have been conducted to investigate the problem from different aspects. In this survey, we make a comprehensive investigation into the causes of VM interference and provide an in-depth review of existing research and solutions in the literature. We first categorize existing studies on interference models according to their modeling objectives, metrics used, and modeling methods. Then we revisit interference-aware strategies for scheduling optimization as well as co-optimization-based approaches. Finally, the survey identifies open challenges with respect to VM interference in data centers and discusses possible research directions to provide insights for future research in the area.",,,,,,,,,,,,,,,,,,,,,,,,,,Cloud data center; measuring and modeling; scheduling optimization; VM performance interference,0.0,EC3,2161
10.1145/3573125,0,0,0,0,0,415,"Toczé, Klervie; Fahs, Ali J.; Pierre, Guillaume; Nadjm-Tehrani, Simin",VioLinn: Proximity-aware Edge Placementwith Dynamic and Elastic Resource Provisioning,ACM Trans. Internet Things,,,https://doi.org/10.1145/3573125,2023-02,,,,,,,"Deciding where to handle services and tasks, as well as provisioning an adequate amount of computing resources for this handling, is a main challenge of edge computing systems. Moreover, latency-sensitive services constrain the type and location of edge devices that can provide the needed resources. When available resources are scarce there is a possibility that some resource allocation requests are denied.In this work, we propose the VioLinn system to tackle the joint problems of task placement, service placement, and edge device provisioning. Dealing with latency-sensitive services is achieved through proximity-aware algorithms that ensure the tasks are handled close to the end-user. Moreover, the concept of spare edge device is introduced to handle sudden load variations in time and space without having to continuously overprovision. Several spare device selection algorithms are proposed with different cost/performance tradeoffs.Evaluations are performed both in a Kubernetes-based testbed and using simulations and show the benefit of using spare devices for handling localized load spikes with higher quality of service (QoS) and lower computing resource usage. The study of the different algorithms shows that it is possible to achieve this increase in QoS with different tradeoffs against cost and performance.",,,,,,,,,,,,,,,,,,,,,,,,,,resource management; Kubernetes; elasticity; Edge/fog computing,0.0,EC3,2374
10.1145/3573206,0,0,0,0,0,401,"Picone, Marco; Mamei, Marco; Zambonelli, Franco",A Flexible and Modular Architecture for Edge Digital Twin: Implementation and Evaluation,ACM Trans. Internet Things,,,https://doi.org/10.1145/3573206,2023-02,,,,,,,"IoT systems based on Digital Twins (DTs) — virtual copies of physical objects and systems — can be very effective to enable data-driven services and promote better control and decisions, in particular by exploiting distributed approaches where cloud and edge computing cooperate effectively. In this context, digital twins deployed on the edge represents a new strategic element to design a new wave of distributed cyber-physical applications. Existing approaches are generally focused on fragmented and domain-specific monolithic solutions and are mainly associated to model-driven, simulative or descriptive visions. The idea of extending the DTs role to support last-mile digitalization and interoperability through a set of general purpose and well-defined properties and capabilities is still underinvestigated. In this paper, we present the novel Edge Digital Twins (EDT) architectural model and its implementation, enabling the lightweight replication of physical devices providing an efficient digital abstraction layer to support the autonomous and standard collaboration of things and services. We model the core capabilities with respect to the recent definition of the state of the art, present the software architecture and a prototype implementation. Extensive experimental analysis shows the obtained performance in multiple IoT application contexts and compares them with that of state-of-the-art approaches.",,,,,,,,,,,,,,,,,,,,,,,,,,Internet of Things; edge computing; Digital Twin,0.0,EC3,2144
10.1145/3573428.3573601,0,0,0,0,0,147,"Zhang, Chengting; Ye, Zhihui; Jin, Wenlong; Shi, Dingke",Research on the construction of visual intelligent park management system,Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering,978-1-4503-9714-8,,https://doi.org/10.1145/3573428.3573601,2023,EITCE '22,,,,Association for Computing Machinery,"New York, NY, USA","With the continuous deepening and development of smart city construction, the concept of smart parks has gradually entered the public's field of vision. The construction of smart parks is an important way to realize the modernization and intelligence of park management. Through the construction of smart parks, various resources can not only be utilized and integrated, but also the operation efficiency of the parks can be comprehensively improved. The so-called wisdom not only means to increase the construction of infrastructure, but also to strengthen the construction of informatization. By establishing a set of visual intelligent park management and control platform, the originally independent subsystems can be linked according to the management characteristics of the park, so as to improve The overall management service level of the park. Based on this, the construction plan of the visualized smart park management system is proposed.",,,,,,,,,,,,,,,,,,,,,,,,,,Internet of things; Microservice architecture; Visualization,,,964
10.1145/3575693.3575710,1,1,1,0,1,98,"Switzer, Jennifer; Marcano, Gabriel; Kastner, Ryan; Pannuto, Pat",Junkyard Computing: Repurposing Discarded Smartphones to Minimize Carbon,"Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2",978-1-4503-9916-6,,https://doi.org/10.1145/3575693.3575710,2023,ASPLOS 2023,,,,Association for Computing Machinery,"New York, NY, USA","1.5 billion smartphones are sold annually, and most are decommissioned less than two years later. Most of these unwanted smartphones are neither discarded nor recycled but languish in junk drawers and storage units. This computational stockpile represents a substantial wasted potential: modern smartphones have increasingly high-performance and energy-efficient processors, extensive networking capabilities, and a reliable built-in power supply. This project studies the ability to reuse smartphones as ""junkyard computers."" Junkyard computers grow global computing capacity by extending device lifetimes, which supplants the manufacture of new devices. We show that the capabilities of even decade-old smartphones are within those demanded by modern cloud microservices and discuss how to combine phones to perform increasingly complex tasks. We describe how current operation-focused metrics do not capture the actual carbon costs of compute. We propose Computational Carbon Intensity—a performance metric that balances the continued service of older devices with the superlinear runtime improvements of newer machines. We use this metric to redefine device service lifetime in terms of carbon efficiency. We develop a cloudlet of reused Pixel 3A phones. We analyze the carbon benefits of deploying large, end-to-end microservice-based applications on these smartphones. Finally, we describe system architectures and associated challenges to scale to cloudlets with hundreds and thousands of smartphones.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; sustainability; life cycle assessment,0.0,IC4,142
10.1145/3575879.3575963,0,0,0,0,0,175,"Liakos, Christos; Panagopoulos, Aimilios Christos; Karkazis, Panagiotis",Smart Business Processing in Industry 4.0,Proceedings of the 26th Pan-Hellenic Conference on Informatics,978-1-4503-9854-1,,https://doi.org/10.1145/3575879.3575963,2023,PCI '22,,,,Association for Computing Machinery,"New York, NY, USA","In the context of the Internet of Things and Industry 4.0 innovative technologies have been emerged namely Digital Twins, Business Process Management frameworks, Big Data analysis, and they are now available and ready to be used in order to create new management models. These models implement the interconnection of the structural elements of the production chain (machines, sensors, humans, etc.), by gathering and processing useful information, targeting on automated decisions, problems solving in real-time, and the flexible adjustment of the production process. In the current work, we present and evaluate a management architecture of a business process management flow to identify the alterations on digitally replicated industrial machinery and the impact they have on a production line.",,,,,,,,,,,,,,,,,,,,,,,,,,Digital Twin; Industry 4.0; Industrial Internet of Things; Business Process Management,,,855
10.1145/3578245.3585331,0,0,0,0,1,133,"Farahani, Reza; Kimovski, Dragi; Ristov, Sashko; Iosup, Alexandru; Prodan, Radu",Towards Sustainable Serverless Processing of Massive Graphs on the Computing Continuum,Companion of the 2023 ACM/SPEC International Conference on Performance Engineering,979-8-4007-0072-9,,https://doi.org/10.1145/3578245.3585331,2023,ICPE '23 Companion,,,,Association for Computing Machinery,"New York, NY, USA","With the ever-increasing volume of data and the demand to analyze and comprehend it, graph processing has become an essential approach for solving complex problems in various domains, like social networks, bioinformatics, and finance. Despite the potential benefits of current graph processing platforms, they often encounter difficulties supporting diverse workloads, models, and languages. Moreover, existing platforms suffer from limited portability and interoperability, resulting in redundant efforts and inefficient resource and energy utilization due to vendor and even platform lock-in. To bridge the aforementioned gaps, the Graph-Massivizer project, funded by the Horizon Europe research and innovation program, conducts research and develops a high-performance, scalable, and sustainable platform for information processing and reasoning based on the massive graph (MG) representation of extreme data. In this paper, we briefly introduce the Graph-Massivizer platform. We explore how the emerging serverless computing paradigm can be leveraged to devise a scalable graph analytics tool over a codesigned computing continuum infrastructure. Finally, we sketch seven crucial research questions in our design and outline three ongoing and future research directions for addressing them.",,,,,,,,,,,,,,,,,,,,,,,,,,computing continuum; serverless computing; sustainability; graph processing; massive graph,0.0,EC3,140
10.1145/3579371.3589072,0,0,0,0,0,284,"Liang, Mingyu; Fu, Wenyin; Feng, Louis; Lin, Zhongyi; Panakanti, Pavani; Zheng, Shengbao; Sridharan, Srinivas; Delimitrou, Christina",Mystique: Enabling Accurate and Scalable Generation of Production AI Benchmarks,Proceedings of the 50th Annual International Symposium on Computer Architecture,979-8-4007-0095-8,,https://doi.org/10.1145/3579371.3589072,2023,ISCA '23,,,,Association for Computing Machinery,"New York, NY, USA","Building large AI fleets to support the rapidly growing DL workloads is an active research topic for modern cloud providers. Generating accurate benchmarks plays an essential role in designing the fast-paced software and hardware solutions in this space. Two fundamental challenges to make this scalable are (i) workload representativeness and (ii) the ability to quickly incorporate changes to the fleet into the benchmarks.To overcome these issues, we propose Mystique, an accurate and scalable framework for production AI benchmark generation. It leverages the PyTorch execution trace (ET), a new feature that captures the runtime information of AI models at the granularity of operators, in a graph format, together with their metadata. By sourcing fleet ETs, we can build AI benchmarks that are portable and representative. Mystique is scalable, due to its lightweight data collection, in terms of runtime overhead and instrumentation effort. It is also adaptive because ET composability allows flexible control on benchmark creation.We evaluate our methodology on several production AI models, and show that benchmarks generated with Mystique closely resemble original AI models, both in execution time and system-level metrics. We also showcase the portability of the generated benchmarks across platforms, and demonstrate several use cases enabled by the fine-grained composability of the execution trace.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; benchmarking; artificial intelligence; code generation; performance cloning,0.0,EC3,2867
10.1145/3580305.3599392,0,0,0,0,0,167,"Wang, Dongjie; Chen, Zhengzhang; Fu, Yanjie; Liu, Yanchi; Chen, Haifeng",Incremental Causal Graph Learning for Online Root Cause Analysis,Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,979-8-4007-0103-0,,https://doi.org/10.1145/3580305.3599392,2023,KDD '23,,,,Association for Computing Machinery,"New York, NY, USA","The task of root cause analysis (RCA) is to identify the root causes of system faults/failures by analyzing system monitoring data. Efficient RCA can greatly accelerate system failure recovery and mitigate system damages or financial losses. However, previous research has mostly focused on developing offline RCA algorithms, which often require manually initiating the RCA process, a significant amount of time and data to train a robust model, and then being retrained from scratch for a new system fault.In this paper, we propose CORAL, a novel online RCA framework that can automatically trigger the RCA process and incrementally update the RCA model. CORAL consists of Trigger Point Detection, Incremental Disentangled Causal Graph Learning, and Network Propagation-based Root Cause Localization. The Trigger Point Detection component aims to detect system state transitions automatically and in near-real-time. To achieve this, we develop an online trigger point detection approach based on multivariate singular spectrum analysis and cumulative sum statistics. To efficiently update the RCA model, we propose an incremental disentangled causal graph learning approach to decouple the state-invariant and state-dependent information. After that, CORAL applies a random walk with restarts to the updated causal graph to accurately identify root causes. The online RCA process terminates when the causal graph and the generated root cause list converge. Extensive experiments on three real-world datasets demonstrate the effectiveness and superiority of the proposed framework.",,,,,,,,,,,,,,,,,,,,,,,,,,causal structure learning; disentangled graph learning; incremental learning; root cause analysis; trigger point detection,0.0,EC3,3088
10.1145/3581759,0,0,0,0,0,391,"Hoffpauir, Kyle; Simmons, Jacob; Schmidt, Nikolas; Pittala, Rachitha; Briggs, Isaac; Makani, Shanmukha; Jararweh, Yaser",A Survey on Edge Intelligence and Lightweight Machine Learning Support for Future Applications and Services,J. Data and Information Quality,,1936-1955,https://doi.org/10.1145/3581759,2023-06,,,,,,,"As the number of devices connected to the Internet has grown larger, so too has the intensity of the tasks that these devices need to perform. Modern networks are more frequently working to perform computationally intensive tasks on low-power devices and low-end hardware. Current architectures and platforms tend towards centralized and resource-rich cloud computing approaches to address these deficits. However, edge computing presents a much more viable and flexible alternative. Edge computing refers to a distributed and decentralized network architecture in which demanding tasks such as image recognition, smart city services, and high-intensity data processing tasks can be distributed over a number of integrated network devices. In this article, we provide a comprehensive survey for emerging edge intelligence applications, lightweight machine learning algorithms, and their support for future applications and services. We start by analyzing the rise of cloud computing, discuss its weak points, and identify situations in which edge computing provides advantages over traditional cloud computing architectures. We then divulge details of the survey: the first section identifies opportunities and domains for edge computing growth, the second identifies algorithms and approaches that can be used to enhance edge intelligence implementations, and the third specifically analyzes situations in which edge intelligence can be enhanced using any of the aforementioned algorithms or approaches. In this third section, lightweight machine learning approaches are detailed. A more in-depth analysis and discussion of future developments follows. The primary discourse of this article is in service of an effort to ensure that appropriate approaches are applied adequately to artificial intelligence implementations in edge systems, mainly, the lightweight machine learning approaches.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; quality of service; edge computing; Edge intelligence; artificial intelligence; lightweight machine learning; network services,0.0,EC3,2226
10.1145/3581784.3607091,0,0,0,0,0,111,"Ding, Jianru; Hoffmann, Henry",DPS: Adaptive Power Management for Overprovisioned Systems,"Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",979-8-4007-0109-2,,https://doi.org/10.1145/3581784.3607091,2023,SC '23,,,,Association for Computing Machinery,"New York, NY, USA","Maximizing performance under a power budget is essential for HPC systems and has inspired the development of many power management frameworks. These can be broadly characterized into two groups: model-based and stateless. Model-based frameworks use machine learning to achieve good performance under a power budget but are highly dependent on the quality of the learned model and the data used to train it. Stateless frameworks are more robust and require no training, but are generally lower performance. In this paper, we propose a new framework that does not require a model, but does track a small amount of state in the form of recent power dynamics. We implement this idea and test it on a public cloud running both Spark and HPC jobs. We find when total power demand is low, our framework achieves equivalent performance to prior work, but when power demand is high it achieves mean 8% performance improvement (with no reliance on a learned model).",,,,,,,,,,,,,,,,,,,,,,,,,,resource management; job scheduling; power-efficient design and power-management strategies; system interoperations and energy-aware techniques for large-scale systems,0.0,EC3,2639
10.1145/3584871.3584891,0,0,0,0,0,178,"Zhou, Xin; Wu, Yuxuan",CNDAS-WF: Cloud Native Data Analysis System Based On Workflow Engine,Proceedings of the 2023 6th International Conference on Software Engineering and Information Management,978-1-4503-9823-7,,https://doi.org/10.1145/3584871.3584891,2023,ICSIM '23,,,,Association for Computing Machinery,"New York, NY, USA","With the development of modern big data technology, data size in daily life is expanding rapidly and data relationship is more complex. However, the requirements of data analysis for different resources continuous to surging. Therefore, how to handle a large number of data analysis tasks with complex dependencies efficiently become the challenge. In this paper, we design and implement a cloud native data analysis system based on workflow engine. The system arranges the data analysis tasks, which deployed by containers, with dependency through the workflow engine based on cloud native technology. Flexibility of container cloud makes data analysis procedure effective and efficient. In addition, we designed a workflow engine and an operation and maintenance subsystem for overall system platform anomaly detection. Finally, we verify the effectiveness and efficiency of the system through scientific workflow data. The cloud native data analysis system based on workflow engine has passed all tests and has been applied in small and medium-sized enterprises.",,,,,,,,,,,,,,,,,,,,,,,,,,Kubernetes; workflow; data analysis; cloud native,,,278
10.1145/3586183.3606794,0,0,0,0,0,224,"Chen, Wenqiang; Wang, Ziqi; Quan, Pengrui; Peng, Zhencan; Lin, Shupei; Srivastava, Mani; Matusik, Wojciech; Stankovic, John",Robust Finger Interactions with COTS Smartwatches via Unsupervised Siamese Adaptation,Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology,979-8-4007-0132-0,,https://doi.org/10.1145/3586183.3606794,2023,UIST '23,,,,Association for Computing Machinery,"New York, NY, USA","Wearable devices like smartwatches and smart wristbands have gained substantial popularity in recent years. However, their small interfaces create inconvenience and limit computing functionality. To fill this gap, we propose ViWatch, which enables robust finger interactions under deployment variations, and relies on a single IMU sensor that is ubiquitous in COTS smartwatches. To this end, we design an unsupervised Siamese adversarial learning method. We built a real-time system on commodity smartwatches and tested it with over one hundred volunteers. Results show that the system accuracy is about 97% over a week. In addition, it is resistant to deployment variations such as different hand shapes, finger activity strengths, and smartwatch positions on the wrist. We also developed a number of mobile applications using our interactive system and conducted a user study where all participants preferred our unsupervised approach to supervised calibration. The demonstration of ViWatch is shown at https://youtu.be/N5-ggvy2qfI.",,,,,,,,,,,,,,,,,,,,,,,,,,Finger Interaction; Gesture Recognition; Unsupervised Adversarial Training; Vibration Sensing,,,1223
10.1145/3589227,0,0,0,0,0,381,"Weyns, Danny; Gerostathopoulos, Ilias; Abbas, Nadeem; Andersson, Jesper; Biffl, Stefan; Brada, Premek; Bures, Tomas; Di Salle, Amleto; Galster, Matthias; Lago, Patricia; Lewis, Grace; Litoiu, Marin; Musil, Angelika; Musil, Juergen; Patros, Panos; Pelliccione, Patrizio",Self-Adaptation in Industry: A Survey,ACM Trans. Auton. Adapt. Syst.,,1556-4665,https://doi.org/10.1145/3589227,2023-05,,,,,,,"Computing systems form the backbone of many areas in our society, from manufacturing to traffic control, healthcare, and financial systems. When software plays a vital role in the design, construction, and operation, these systems are referred to as software-intensive systems. Self-adaptation equips a software-intensive system with a feedback loop that either automates tasks that otherwise need to be performed by human operators or deals with uncertain conditions. Such feedback loops have found their way to a variety of practical applications; typical examples are an elastic cloud to adapt computing resources and automated server management to respond quickly to business needs. To gain insight into the motivations for applying self-adaptation in practice, the problems solved using self-adaptation and how these problems are solved, and the difficulties and risks that industry faces in adopting self-adaptation, we performed a large-scale survey. We received 184 valid responses from practitioners spread over 21 countries. Based on the analysis of the survey data, we provide an empirically grounded overview the of state of the practice in the application of self-adaptation. From that, we derive insights for researchers to check their current research with industrial needs, and for practitioners to compare their current practice in applying self-adaptation. These insights also provide opportunities for applying self-adaptation in practice and pave the way for future industry-research collaborations.",,,,,,,,,,,,,,,,,,,,,,,,,,Self-adaptation; survey; industry,,,962
10.1145/3589334.3645434,0,0,0,0,0,250,"Liu, Xu; Hu, Junfeng; Li, Yuan; Diao, Shizhe; Liang, Yuxuan; Hooi, Bryan; Zimmermann, Roger",UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting,Proceedings of the ACM Web Conference 2024,979-8-4007-0171-9,,https://doi.org/10.1145/3589334.3645434,2024,WWW '24,,,,Association for Computing Machinery,"New York, NY, USA","Multivariate time series forecasting plays a pivotal role in contemporary web technologies. In contrast to conventional methods that involve creating dedicated models for specific time series application domains, this research advocates for a unified model paradigm that transcends domain boundaries. However, learning an effective cross-domain model presents the following challenges. First, various domains exhibit disparities in data characteristics, e.g., the number of variables, posing hurdles for existing models that impose inflexible constraints on these factors. Second, the model may encounter difficulties in distinguishing data from various domains, leading to suboptimal performance in our assessments. Third, the diverse convergence rates of time series domains can also result in compromised empirical performance. To address these issues, we propose UniTime for effective cross-domain time series learning. Concretely, UniTime can flexibly adapt to data with varying characteristics. It also uses domain instructions and a Language-TS Transformer to offer identification information and align two modalities. In addition, UniTime employs masking to alleviate domain convergence speed imbalance issues. Our extensive experiments demonstrate the effectiveness of UniTime in advancing state-of-the-art forecasting performance and zero-shot transferability.",,,,,,,,,,,,,,,,,,,,,,,,,,time series forecasting; language models,,,326
10.1145/3592598,0,0,0,0,0,311,"Pallewatta, Samodha; Kostakos, Vassilis; Buyya, Rajkumar",Placement of Microservices-based IoT Applications in Fog Computing: A Taxonomy and Future Directions,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3592598,2023-07,,,,,,,"The Fog computing paradigm utilises distributed, heterogeneous and resource-constrained devices at the edge of the network for efficient deployment of latency-critical and bandwidth-hungry IoT application services. Moreover, MicroService Architecture (MSA) is increasingly adopted to keep up with the rapid development and deployment needs of fast-evolving IoT applications. Due to the fine-grained modularity of the microservices and their independently deployable and scalable nature, MSA exhibits great potential in harnessing Fog and Cloud resources, thus giving rise to novel paradigms like Osmotic computing. The loosely coupled nature of the microservices, aided by the container orchestrators and service mesh technologies, enables the dynamic composition of distributed and scalable microservices to achieve diverse performance requirements of the IoT applications using distributed Fog resources. To this end, efficient placement of microservice plays a vital role, and scalable placement algorithms are required to utilise the said characteristics of the MSA while overcoming novel challenges introduced by the architecture. Thus, we present a comprehensive taxonomy of recent literature on microservices-based IoT applications placement within Fog computing environments. Furthermore, we organise multiple taxonomies to capture the main aspects of the placement problem, analyse and classify related works, identify research gaps within each category, and discuss future research directions.",,,,,,,,,,,,,,,,,,,,,,,,,,Fog computing; application placement; Internet of Things; microservice architecture; Osmotic computing,0.0,EC3,1655
10.1145/3593021,0,0,0,0,0,416,"Will, Newton C.; Maziero, Carlos A.",Intel Software Guard Extensions Applications: A Survey,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3593021,2023-07,,,,,,,"Data confidentiality is a central concern in modern computer systems and services, as sensitive data from users and companies are being increasingly delegated to such systems. Several hardware-based mechanisms have been recently proposed to enforce security guarantees of sensitive information. Hardware-based isolated execution environments are a class of such mechanisms, in which the operating system and other low-level components are removed from the trusted computing base. One of such mechanisms is the Intel Software Guard Extensions (Intel SGX), which creates the concept of enclave to encapsulate sensitive components of applications and their data. Despite being largely applied in several computing areas, SGX has limitations and performance issues that must be addressed for the development of secure solutions. This text brings a categorized literature review of the ongoing research on the Intel SGX architecture, discussing its applications and providing a classification of the solutions that take advantage of SGX mechanisms. We analyze and categorize 293 papers that rely on SGX to provide integrity, confidentiality, and privacy to users and data, regarding different contexts and goals. We also discuss research challenges and provide future directions in the field of enclaved execution, particularly when using SGX.",,,,,,,,,,,,,,,,,,,,,,,,,,confidentiality; Hardware-based security; integrity; processor extensions,,,507
10.1145/3594539,0,0,0,0,0,326,"Lumpp, Francesco; Panato, Marco; Bombieri, Nicola; Fummi, Franco",A Design Flow Based on Docker and Kubernetes for ROS-based Robotic Software Applications,ACM Trans. Embed. Comput. Syst.,,1539-9087,https://doi.org/10.1145/3594539,2024-08,,,,,,,"Human-centered robotic applications are becoming pervasive in the context of robotics and smart manufacturing, and such a pervasiveness is even more expected with the shift to Industry 5.0. The always increasing level of autonomy of modern robotic platforms requires the integration of software applications from different domains to implement artificial intelligence, cognition, and human-robot/robot-robot interaction. Developing and (re)configuring such a multi-domain software to meet functional constraints is a challenging task. Even more challenging is customizing the software to satisfy non-functional requirements such as real-time, reliability, and energy efficiency. In this context, the concept of Edge-Cloud continuum is gaining consensus as a solution to address functional and non-functional constraints in a seamless way. Containerization and orchestration are becoming a standard practice, as they allow for better information flow among different network levels as well as increased modularity in the use of multi-domain software components. Nevertheless, the adoption of such a practice along the design flow, from simulation to the deployment of complex robotic applications by addressing the de facto development standards (e.g., ROS - Robotic Operating System) is still an open problem. We present a design methodology based on Docker and Kubernetes that enables containerization and orchestration of ROS-based robotic SW applications for heterogeneous and hierarchical HW architectures. The methodology aims at (i) integrating and verifying multi-domain components since early in the design flow, (ii) mapping software tasks to containers to minimize the performance and memory footprint overhead, (iii) clustering containers to efficiently distribute load across the edge-cloud architecture by minimizing resource utilization, and (iv) enabling multi-domain verification of functional and non-functional constraints before deployment. The article presents the results obtained with a real case of study, in which the design methodology has been applied to program the mission of a Robotnik RB-Kairos mobile robot in an industrial agile production chain. We have obtained reduced load on the robot’s HW with minimal performance and network overhead, thanks to the optimized distributed system.",,,,,,,,,,,,,,,,,,,,,,,,,,Kubernetes; Docker; ROS; Edge-Cloud computing; K3S; robotic applications,0.0,EC3,2139
10.1145/3594556.3594611,0,0,0,0,0,241,"Samaniego, Mayra; Deters, Ralph",Digital Twins and Blockchain for IoT Management,Proceedings of the 5th ACM International Symposium on Blockchain and Secure Critical Infrastructure,979-8-4007-0198-6,,https://doi.org/10.1145/3594556.3594611,2023,BSCI '23,,,,Association for Computing Machinery,"New York, NY, USA","Security and privacy are primary concerns in IoT management. Security breaches in IoT resources, such as smart sensors, can leak sensitive data and compromise the privacy of individuals. Effective IoT management requires a comprehensive approach to prioritize access security and data privacy protection. Digital twins create virtual representations of IoT resources. Blockchain adds decentralization, transparency, and reliability to IoT systems. This research integrates digital twins and blockchain to manage access to IoT data streaming. Digital twins are used to encapsulate data access and view configurations. Access is enabled on digital twins, not on IoT resources directly. Trust structures programmed as smart contracts are the ones that manage access to digital twins. Consequently, IoT resources are not exposed to third parties, and access security breaches can be prevented. Blockchain has been used to validate digital twins and store their configuration. The research presented in this paper enables multitenant access and customization of data streaming views and abstracts the complexity of data access management. This approach provides access and configuration security and data privacy protection.",,,,,,,,,,,,,,,,,,,,,,,,,,security; IoT; internet of things; distributed systems; blockchain; Digital twins; privacy; access management; consensus; data streaming; data trust; smart contracts; trust,,,1186
10.1145/3595376,0,0,0,0,0,412,"Coullon, Héléne; Henrio, Ludovic; Loulergue, Frédéric; Robillard, Simon",Component-based Distributed Software Reconfiguration:A Verification-oriented Survey,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3595376,2023-08,,,,,,,"Distributed software built from components has become a mainstay of service-oriented applications, which frequently undergo reconfigurations to adapt to changes in their operating environment or their functional requirements. Given the complexity of distributed software and the adverse effects of incorrect reconfigurations, a suitable methodology is needed to ensure the correctness of reconfigurations in component-based systems. This survey gives the reader a global perspective over existing formal techniques that pursue this goal. It distinguishes different ways in which formal methods can improve the reliability of reconfigurations, and lists techniques that contribute to solving each of these particular scientific challenges.",,,,,,,,,,,,,,,,,,,,,,,,,,verification; component-based software engineering; formal methods; Reconfiguration; software adaptation,,,902
10.1145/3597435,0,0,0,0,0,406,"Russo Russo, Gabriele; Cardellini, Valeria; Lo Presti, Francesco",Hierarchical Auto-scaling Policies for Data Stream Processing on Heterogeneous Resources,ACM Trans. Auton. Adapt. Syst.,,1556-4665,https://doi.org/10.1145/3597435,2023-10,,,,,,,"Data Stream Processing (DSP) applications analyze data flows in near real-time by means of operators, which process and transform incoming data. Operators handle high data rates running parallel replicas across multiple processors and hosts. To guarantee consistent performance without wasting resources in the face of variable workloads, auto-scaling techniques have been studied to adapt operator parallelism at run-time. However, most of the effort has been spent under the assumption of homogeneous computing infrastructures, neglecting the complexity of modern environments.We consider the problem of deciding both how many operator replicas should be executed and which types of computing nodes should be acquired. We devise heterogeneity-aware policies by means of a two-layered hierarchy of controllers. While application-level components steer the adaptation process for whole applications, aiming to guarantee user-specified requirements, lower-layer components control auto-scaling of single operators. We tackle the fundamental challenge of performance and workload uncertainty, exploiting Bayesian optimization (BO) and reinforcement learning (RL) to devise policies. The evaluation shows that our approach is able to meet users’ requirements in terms of response time and adaptation overhead, while minimizing the cost due to resource usage, outperforming state-of-the-art baselines. We also demonstrate how partial model information is exploited to reduce training time for learning-based controllers.",,,,,,,,,,,,,,,,,,,,,,,,,,resource management; reinforcement learning; Auto-scaling; Data Stream Processing,0.0,EC3,2799
10.1145/3597926.3598117,0,0,0,0,0,275,"Valle, Pablo; Arrieta, Aitor; Arratibel, Maite",Applying and Extending the Delta Debugging Algorithm for Elevator Dispatching Algorithms (Experience Paper),Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis,979-8-4007-0221-1,,https://doi.org/10.1145/3597926.3598117,2023,ISSTA 2023,,,,Association for Computing Machinery,"New York, NY, USA","Elevator systems are one kind of Cyber-Physical Systems (CPSs), and as such, test cases are usually complex and long in time. This is mainly because realistic test scenarios are employed (e.g., for testing elevator dispatching algorithms, typically a full day of passengers traveling through a system of elevators is used). However, in such a context, when needing to reproduce a failure, it is of high benefit to provide the minimal test input to the software developers. This way, analyzing and trying to localize the root-cause of the failure is easier and more agile. Delta debugging has been found to be an efficient technique to reduce failure-inducing test inputs. In this paper, we enhance this technique by first monitoring the environment at which the CPS operates as well as its physical states. With the monitored information, we search for stable states of the CPS during the execution of the simulation. In a second step, we use such identified stable states to help the delta debugging algorithm isolate the failure-inducing test inputs more efficiently. We report our experience of applying our approach into an industrial elevator dispatching algorithm. An empirical evaluation carried out with real operational data from a real installation of elevators suggests that the proposed environment-wise delta debugging algorithm is between 1.3 to 1.8 times faster than the traditional delta debugging, while producing a larger reduction in the failure-inducing test inputs. The results provided by the different implemented delta debugging algorithm versions are qualitatively assessed with domain experts. This assessment provides new insights and lessons learned, such as, potential applications of the delta debugging algorithm beyond debugging.",,,,,,,,,,,,,,,,,,,,,,,,,,Cyber-Physical Systems; Delta Debugging; Simulation-based Testing,,,659
10.1145/3600006.3613156,0,0,0,0,0,187,"Seemakhupt, Korakit; Stephens, Brent E.; Khan, Samira; Liu, Sihang; Wassel, Hassan; Yeganeh, Soheil Hassas; Snoeren, Alex C.; Krishnamurthy, Arvind; Culler, David E.; Levy, Henry M.",A Cloud-Scale Characterization of Remote Procedure Calls,Proceedings of the 29th Symposium on Operating Systems Principles,979-8-4007-0229-7,,https://doi.org/10.1145/3600006.3613156,2023,SOSP '23,,,,Association for Computing Machinery,"New York, NY, USA","The global scale and challenging requirements of modern cloud applications have led to the development of complex, widely distributed, service-oriented applications. One enabler of such applications is the remote procedure call (RPC), which provides location-independent communication and hides the myriad of cloud communication complexities and requirements within the RPC stack. Understanding RPCs is thus one key to understanding the behavior of cloud applications. While there have been numerous studies of RPCs in distributed systems, as well as attempts to optimize RPC overheads with both software and hardware, there is still a lack of knowledge about the characteristics of RPCs ""in the wild"" in the modern cloud environment.To address this gap, we present, to the best of our knowledge, the first large-scale fleet-wide study of RPCs. Our study is conducted at Google, where we measured the infrastructure supporting Google's user-facing, billion-user web services, such as Google Search, Gmail, Maps, and YouTube, and the information and data management systems that support them. To carry out the study, we examined over 10,000 different RPC methods sampled from over one billion traces, along with statistics collected every 30 minutes over a period of nearly two years. Among other things, we consider the volume, throughput and growth rate of RPCs in the datacenter, the latency of RPCs and their components (the ""RPC latency tax""), and the structure of RPC call chains. Our analysis shows that the characteristics, scope and complexity of RPCs at hyperscale differ significantly from the assumptions made in prior research. Overall, our work provides new insights into RPC usage and characteristics at the largest scale and motivates further research on optimizing the diverse behavior of this crucial communication mechanism.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; distributed computing; communications systems; remote procedure call,,,387
10.1145/3600006.3613208,0,0,0,0,0,169,"Anderson, Patrick; Aranas, Erika Blancada; Assaf, Youssef; Behrendt, Raphael; Black, Richard; Caballero, Marco; Cameron, Pashmina; Canakci, Burcu; De Carvalho, Thales; Chatzieleftheriou, Andromachi; Storan Clarke, Rebekah; Clegg, James; Cletheroe, Daniel; Cooper, Bridgette; Deegan, Tim; Donnelly, Austin; Drevinskas, Rokas; Gaunt, Alexander; Gkantsidis, Christos; Gomez Diaz, Ariel; Haller, Istvan; Hong, Freddie; Ilieva, Teodora; Joshi, Shashidhar; Joyce, Russell; Kunkel, Mint; Lara, David; Legtchenko, Sergey; Liu, Fanglin Linda; Magalhaes, Bruno; Marzoev, Alana; Mcnett, Marvin; Mohan, Jayashree; Myrah, Michael; Nguyen, Trong; Nowozin, Sebastian; Ogus, Aaron; Overweg, Hiske; Rowstron, Antony; Sah, Maneesh; Sakakura, Masaaki; Scholtz, Peter; Schreiner, Nina; Sella, Omer; Smith, Adam; Stefanovici, Ioan; Sweeney, David; Thomsen, Benn; Verkes, Govert; Wainman, Phil; Westcott, Jonathan; Weston, Luke; Whittaker, Charles; Wilke Berenguer, Pablo; Williams, Hugh; Winkler, Thomas; Winzeck, Stefan",Project Silica: Towards Sustainable Cloud Archival Storage in Glass,Proceedings of the 29th Symposium on Operating Systems Principles,979-8-4007-0229-7,,https://doi.org/10.1145/3600006.3613208,2023,SOSP '23,,,,Association for Computing Machinery,"New York, NY, USA","Sustainable and cost-effective long-term storage remains an unsolved problem. The most widely used storage technologies today are magnetic (hard disk drives and tape). They use media that degrades over time and has a limited lifetime, which leads to inefficient, wasteful, and costly solutions for long-lived data. This paper presents Silica: the first cloud storage system for archival data underpinned by quartz glass, an extremely resilient media that allows data to be left in situ indefinitely. The hardware and software of Silica have been co-designed and co-optimized from the media up to the service level with sustainability as a primary objective. The design follows a cloud-first, data-driven methodology underpinned by principles derived from analyzing the archival workload of a large public cloud service. Silica can support a wide range of archival storage workloads and ushers in a new era of sustainable, cost-effective storage.",,,,,,,,,,,,,,,,,,,,,,,,,,data center; sustainability; archival; cloud storage; cold storage; disaggregation; glass; storage,,,807
10.1145/3600061.3600063,0,0,0,0,0,95,"Li, Bojie; Xiang, Zihao; Wang, Xiaoliang; Ruan, Han; Zhou, Jingbin; Tan, Kun",FastWake: Revisiting Host Network Stack for Interrupt-mode RDMA,Proceedings of the 7th Asia-Pacific Workshop on Networking,979-8-4007-0782-7,,https://doi.org/10.1145/3600061.3600063,2023,APNet '23,,,,Association for Computing Machinery,"New York, NY, USA","Polling and interrupt has long been a trade-off in RDMA systems. Polling has lower latency but each CPU core can only run one thread. Interrupt enables time sharing among multiple threads but has higher latency. Many applications such as databases have hundreds of threads, which is much larger than the number of cores. So, they have to use interrupt mode to share cores among threads, and the resulting RDMA latency is much higher than the hardware limits. In this paper, we analyze the root cause of high costs in RDMA interrupt delivery, and present FastWake, a practical redesign of interrupt-mode RDMA host network stack using commodity RDMA hardware, Linux OS, and unmodified applications. Our first approach to fast thread wake-up completely removes interrupts. We design a per-core dispatcher thread to poll all the completion queues of the application threads on the same core, and utilize a kernel fast path to context switch to the thread with an incoming completion event. The approach above would keep CPUs running at 100% utilization, so we design an interrupt-based approach for scenarios with power constraints. Observing that waking up a thread on the same core as the interrupt is much faster than threads on other cores, we dynamically adjust RDMA event queue mappings to improve interrupt core affinity. In addition, we revisit the kernel path of thread wake-up, and remove the overheads in virtual file system (VFS), locking, and process scheduling. Experiments show that FastWake can reduce RDMA latency by 80% on x86 and 77% on ARM at the cost of &lt; 30% higher power utilization than traditional interrupts, and the latency is only 0.3 ∼ 0.4&nbsp;μ s higher than the limits of underlying hardware. When power saving is desired, our interrupt-based approach can still reduce interrupt-mode RDMA latency by 59% on x86 and 52% on ARM.",,,,,,,,,,,,,,,,,,,,,,,,,,Context Switch; Host Network Stack; Interrupt; RDMA,,,435
10.1145/3603166.3632240,0,0,0,0,0,13,"Vahabi, Shahrokh; Righetti, Francesca; Vallati, Carlo; Tonellotto, Nicola",Energy-Efficient Resource Management for Real-Time Applications in FaaS Edge Computing Platforms,Proceedings of the IEEE/ACM 16th International Conference on Utility and Cloud Computing,979-8-4007-0234-1,,https://doi.org/10.1145/3603166.3632240,2024,UCC '23,,,,Association for Computing Machinery,"New York, NY, USA","Edge computing and Function-as-a-Service are two emerging paradigms that enable a timed analysis of data directly in the proximity of cyber-physical systems and users. Function-as-a-service platforms deployed at the edge require mechanisms for resource management and allocation to schedule function execution and to scale the available resources in order to ensure the proper quality of service to applications. Large-scale deployments will also require mechanisms to control the energy consumption of the overall system, to ensure long-term sustainability. In this paper, we propose a technique to schedule function invocations on Edge resources by powering down idle edge nodes during period of low demands. In doing so, our technique aims at reducing the overall energy consumption without incurring in service level agreements violations. Experimental evaluations demonstrate that the proposed approach reduces service level agreement violations by at least 78.1% and energy consumption by at least 62.5% on average using synthetic and real-world datasets w.r.t. different baselines.",,,,,,,,,,,,,,,,,,,,,,,,,,energy efficiency; resource management; edge computing; function-as-a-service; service level agreement,0.0,EC3,1719
10.1145/3603166.3632536,0,0,0,0,0,29,"Reitzl, Marcus; Kimovski, Dragi",Multi-Objective Optimisation of Container Orchestration Systems,Proceedings of the IEEE/ACM 16th International Conference on Utility and Cloud Computing,979-8-4007-0234-1,,https://doi.org/10.1145/3603166.3632536,2024,UCC '23,,,,Association for Computing Machinery,"New York, NY, USA","The ever-increasing global demand for optimized resource utilization, energy efficiency, and rapid response times in cloud computing environments necessitates innovative approaches for resource management. Traditional cloud systems predominantly cater to large-scale infrastructures, often neglecting smaller-scale environments, such as edge computing infrastructures. In response to this gap, this paper introduces a novel Multi-Objective Stochastic Gradient Descent (MOSGD) approach designed to enhance the efficiency of the application placement processes beyond the cloud and closer to the edge of the network. The MOSGD optimisation addresses two crucial objectives: energy consumption and execution time. We meticulously modelled and integrated the two conflicting objectives into a unified cost function to minimise resource consumption and response time.To validate the MOSGD approach, we deployed a real-life environment utilizing the Carinthian Computing Continuum infrastructure as the target platform. The results of this research exhibit significant performance enhancements compared to two conventional methods. The findings indicate an improvement of up to 80% in energy efficiency and up to 30% reduction in execution time. These outcomes underscore the potential of the MOSGD approach to outperform traditional techniques in scenarios where large-scale procedures may exhibit suboptimal performance.",,,,,,,,,,,,,,,,,,,,,,,,,,edge computing; optimisation; pod placement,0.0,EC3,1782
10.1145/3603166.3632540,0,0,0,0,0,12,"Mehran, Narges; Kimovski, Dragi; Hellwagner, Hermann; Roman, Dumitru; Soylu, Ahmet; Prodan, Radu",Scheduling of Distributed Applications on the Computing Continuum: A Survey,Proceedings of the IEEE/ACM 16th International Conference on Utility and Cloud Computing,979-8-4007-0234-1,,https://doi.org/10.1145/3603166.3632540,2024,UCC '23,,,,Association for Computing Machinery,"New York, NY, USA","The demand for distributed applications has significantly increased over the past decade, with improvements in machine learning techniques fueling this growth. These applications predominantly utilize Cloud data centers for high-performance computing and Fog and Edge devices for low-latency communication for small-size machine learning model training and inference. The challenge of executing applications with different requirements on heterogeneous devices requires effective methods for solving NP-hard resource allocation and application scheduling problems. The state-of-the-art techniques primarily investigate conflicting objectives, such as the completion time, energy consumption, and economic cost of application execution on the Cloud, Fog, and Edge computing infrastructure. Therefore, in this work, we review these research works considering their objectives, methods, and evaluation tools. Based on the review, we provide a discussion on the scheduling methods in the Computing Continuum.",,,,,,,,,,,,,,,,,,,,,,,,,,energy efficiency; cloud; computing continuum; edge; placement; scheduling,0.0,EC3,1769
10.1145/3603166.3632555,0,0,0,0,0,115,"Toffetti, Giovanni; Militano, Leonardo; Tharaka, Ratnayake; Straub, Mark",ROS-based Robotic Applications Orchestration in the Compute Continuum: Challenges and Approaches,Proceedings of the IEEE/ACM 16th International Conference on Utility and Cloud Computing,979-8-4007-0234-1,,https://doi.org/10.1145/3603166.3632555,2024,UCC '23,,,,Association for Computing Machinery,"New York, NY, USA","With the adoption of robots growing in several industrial sectors (e.g., logistics, healthcare, agriculture) comes the experience that in ""robotic applications"" robots are but components of larger distributed systems and, despite their specific requirements and assumptions, should be integrated with the other elements.This paper reports on the main challenges of building distributed robotic applications and discusses different approaches in which such applications are orchestrated and managed in the compute continuum from the Cloud to the Edge of the network.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud; edge; IoT; orchestration; fog; cloud robotics; compute continuum,,,651
10.1145/3603955.3603971,0,0,0,0,0,253,"Arin, Ikrar Adinata; Supangkat, Suhono Harso; Gaol, Ford Lumban; Ranti, Benny",Smart Services Implementation in Smart Cities: A Comprehensive Review of State-of-the-art Technologies,Proceedings of the 2023 5th International Conference on Management Science and Industrial Engineering,979-8-4007-0838-1,,https://doi.org/10.1145/3603955.3603971,2023,MSIE '23,,,,Association for Computing Machinery,"New York, NY, USA","This paper discusses comprehensively the advanced technologies used in the implementation of smart services, which are key elements of a smart city. Smart cities are becoming eco-friendly, livable, and sustainable to deliver essential services to their citizens by leveraging internet and communication technology advancements. Starting from a short literature review about smart cities and smart services, a recent trend in the technology used to implement smart services is collected. Smart service can be defined as a service conveyed using intelligent products that possess awareness and connectivity. All kinds of smart services can be connected in smart cities to offer citizens comprehensive solutions. Pre-emptive services, self-services, or highly interactive services, for example, have the potential to be implemented using current technology such as the internet of things, artificial intelligence, cloud computing, big data, machine learning, machine-to-machine communications, and the digital twin. Therefore, there is scope for research and implementation of other technology for smart services in the future.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,791
10.1145/3604930.3605722,0,0,0,0,0,116,"Patel, Pratyush; Gregersen, Theo; Anderson, Thomas",An Agile Pathway Towards Carbon-aware Clouds,Proceedings of the 2nd Workshop on Sustainable Computer Systems,979-8-4007-0242-6,,https://doi.org/10.1145/3604930.3605722,2023,HotCarbon '23,,,,Association for Computing Machinery,"New York, NY, USA","Climate change is a pressing threat to planetary well-being that can be addressed only by rapid near-term actions across all sectors. Yet, the cloud computing sector, with its increasingly large carbon footprint, has initiated only modest efforts to reduce emissions to date; its main approach today relies on cloud providers sourcing renewable energy from a limited global pool of options. We investigate how to accelerate cloud computing's efforts. Our approach tackles carbon reduction from a software standpoint by gradually integrating carbon awareness into the cloud abstraction. Specifically, we identify key bottlenecks to software-driven cloud carbon reduction, including (1) the lack of visibility and disaggregated control between cloud providers and users over infrastructure and applications, (2) the immense overhead presently incurred by application developers to implement carbon-aware application optimizations, and (3) the increasing complexity of carbon-aware resource management due to renewable energy variability and growing hardware heterogeneity. To overcome these barriers, we propose an agile approach that federates the responsibility and tools to achieve carbon awareness across different cloud stakeholders. As a key first step, we advocate leveraging the role of application operators in managing large-scale cloud deployments and integrating carbon efficiency metrics into their cloud usage workflow. We discuss various techniques to help operators reduce carbon emissions, such as carbon budgets, service-level visibility into emissions, and configurable-yet-centralized resource management optimizations.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; sustainability; carbon reduction,0.0,EC3,1865
10.1145/3605098.3635942,0,0,0,0,0,25,"Vaidhyanathan, Karthik; Caporuscio, Mauro; Florio, Stefano; Muccini, Henry",ML-enabled Service Discovery for Microservice Architecture: a QoS Approach,Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing,979-8-4007-0243-3,,https://doi.org/10.1145/3605098.3635942,2024,SAC '24,,,,Association for Computing Machinery,"New York, NY, USA","Microservice architectures have gained enormous popularity due to their ability to be dynamically added/removed, replicated, and updated according to run-time needs. However, the dynamic nature of microservices introduces uncertainty, which in turn can affect the provided Quality of Service (QoS). This calls for novel service discovery mechanisms able to adapt to the variability of the QoS attributes and further perform effective service discovery and selection. To this end, this paper combines machine learning and self-adaptation techniques to perform service discovery and selection by trading off different QoS attributes. The results of our validation on a state-of-the-art microservices exemplar show that our ML-enabled approach can perform service discovery with 35% higher effectiveness with respect to existing baselines.",,,,,,,,,,,,,,,,,,,,,,,,,,machine learning; self-adaptation; service discovery,0.0,EC3,2298
10.1145/3606018,0,0,0,0,0,329,"Shen, Zhishu; Jin, Jiong; Tan, Cheng; Tagami, Atsushi; Wang, Shangguang; Li, Qing; Zheng, Qiushi; Yuan, Jingling",A Survey of Next-generation Computing Technologies in Space-air-ground Integrated Networks,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3606018,2023-08,,,,,,,"Space-air-ground integrated networks (SAGINs) are key elements for facilitating high-speed seamless connectivity to the devices/users in infrastructure-less environments, where the traditional terrestrial networks are critically infeasible or uneconomical to be fully deployed. This article comprehensively surveys the advanced computing technologies that support the utilization of SAGINs for infrastructure-less environments. The advanced computing technologies refer to the emerging computing techniques, tools, and the processes that can be utilized to support SAGINs in handling the increasing computing tasks. The main contents include: (1) background of SAGINs, (2) typical use cases of SAGINs in infrastructure-less environments, (3) advanced computing technologies to assist SAGINs to meet the requirements of various services in infrastructure-less environments, (4) the related practical initiatives, and (5) the open research challenges and the future research directions.",,,,,,,,,,,,,,,,,,,,,,,,,,Internet of Things; computing and networking convergence; infrastructure-less environments; Space-air-ground integrated networks,,,1023
10.1145/3607947.3607969,0,0,0,0,0,227,"Apat, Hemant K; Alkhayyat, Ahmed; Vidyarthi, Ankit; Barik, Rabindra K",Leveraging Towards Serverless Edge Computing Model for Intelligent IoMT Applications,Proceedings of the 2023 Fifteenth International Conference on Contemporary Computing,979-8-4007-0022-4,,https://doi.org/10.1145/3607947.3607969,2023,IC3-2023,,,,Association for Computing Machinery,"New York, NY, USA","Internet of Medical Things(IoMT) devices connect billions of sensors consisting of a large volume of computation-intensive and time-sensitive tasks in the health sector. These tasks need to be processed or executed by different computational devices used in the network infrastructure for getting meaningful information. Due to the resource-constrained nature of IoMT devices, the devices mostly offload these tasks to various edge devices. The IoMT-Cloud paradigm provides different services storage, computation, and communication for the execution of various IoMT applications with elasticity and scalability on the pay-per-use model. Despite the wonderful benefits of cloud computing, there are various issues that remain unsolved like the response time of an IoMT application must satisfy hard deadlines, and Quality of Service(QoS) parameters. In order to improve the response time, and QoS, a decentralized distributed computing paradigm named Edge computing has emerged as a potential solution to provide computing and networking resources at the edge of the network for different IoT application domains. In this article, a serverless edge computing framework is proposed that provides various IoMT services to emerging healthcare applications. It also provides the mathematical derivations of the model along with its client IoMT applications layer, edge layer, and cloud layer.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; FaaS; serverless computing; edge computing; fog computing; QoS; Internet of Medical Things,0.0,EC3,2618
10.1145/3609437.3609448,0,0,0,0,0,70,"Li, Xiaojuan; Zhang, Yu; Zhu, Zhengyan; Yao, Yuan; Zhou, Xingshe",UbiCap: A Capability-based Run-time Model for Heterogeneous Sensors Management in Ubiquitous Operating System,Proceedings of the 14th Asia-Pacific Symposium on Internetware,979-8-4007-0894-7,,https://doi.org/10.1145/3609437.3609448,2023,Internetware '23,,,,Association for Computing Machinery,"New York, NY, USA","The Ubiquitous Operating System(UOS) is a new type of operating system in response to the new patterns and scenarios of future human-cyber-physical ternary ubiquitous computing. Compared with traditional operating systems, one of the fundamental requirements of UOS is to adaptively manage numerous heterogeneous sensors according to dynamic environments and diverse tasks. However, traditional management focuses on the sensors’ parameters and interfaces without highlighting the perception effect that is users’ concern and dynamic changing. It also lacks a unified management approach for heterogeneous sensors. To overcome the limitations, we propose a novel heterogeneous sensors dynamic management model UbiCap, i.e., Ubiquitous Capability, which is based on the capability abstraction and adaptive run-time capability management mechanism. The capability provides a unified abstract for heterogeneous sensors. The adaptive run-time capability management mechanism transfers the management object from low-level hardware sensors to high-level sensing capability. The capability required and the available capability are matched to support run-time adaptive sensors selection. We implement a software prototype iS2ROS(intelligent Sensor Selection Robot Operating System) based on the UbiCap model. We then simulate a forest fire spot monitoring scenario where iS2ROS selects the optimal image sensor during the identification task execution while light or weather condition changes. Experiment results show that the iS2ROS achieves comparative sensing effectiveness through UbiCap with 50% power consumption lower compared to the traditional both-sensors approach.",,,,,,,,,,,,,,,,,,,,,,,,,,adaptive; Capability; Heterogeneous Sensors Management; run-time; Ubiquitous Operating System,0.0,EC3,2945
10.1145/3609504,0,0,0,0,0,345,"Barrios, Carlos; Kumar, Mohan",Service Caching and Computation Reuse Strategies at the Edge: A Survey,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3609504,2023-09,,,,,,,"With the proliferation of connected devices including smartphones, novel network connectivity and management methods are needed to meet user Quality of Experience (QoE) and computational demands of contemporary applications. Service caching and computation reuse techniques are being employed to alleviate challenges due to scalability, interoperability, and mobility, as well as to reduce application latency by enabling caching at the edge. This survey provides a taxonomy for service caching and computation reuse and describes the current state of the research and its challenges. This is the first survey that provides a comprehensive analysis and suggests future research directions on this topic.",,,,,,,,,,,,,,,,,,,,,,,,,,computation deduplication; computation reuse; Service caching; task caching,,,285
10.1145/3611314.3615915,0,0,0,0,0,3,"Simões, Bruno; Del Puy Carretero, María; Martinez Santiago, Jorge; Muñoz Segovia, Sebastian; Alcain, Nieves","TwinArk: A Unified Framework for Digital Twins based on Micro-frontends, Micro-Services, and Web 3D",Proceedings of the 28th International ACM Conference on 3D Web Technology,979-8-4007-0324-9,,https://doi.org/10.1145/3611314.3615915,2023,Web3D '23,,,,Association for Computing Machinery,"New York, NY, USA","The introduction of the Digital Twin (DT) has sparked a great deal of interest in the virtual replication of physical assets and processes. However, to fully realize the potential of DT, companies require robust and scalable front-end solutions. This study proposes harnessing micro-frontend technologies to surmount the limitations of monolithic front-end frameworks, thereby crafting effective presentation layers tailored for industrial companies. By decomposing complex and distributed systems into modular web-based DT, the proposed framework enables better scalability, synergy, and efficient application development. It also tackles the intricacies introduced by complex multi-vendor elements within DT and the orchestration of services and virtual environments. Research is focused on developing an architecture that facilitates seamless connectivity between multiple blocks of DT as well as interactivity and immersive experiences. Our study offers insights into our journey of implementing this framework for various industrial use cases, such as training, monitoring, and control, highlighting the benefits, drawbacks, and challenges. Ultimately, this research aims to accelerate the creation of DT, improve maintainability, and increase efficiency and productivity in industrial environments.",,,,,,,,,,,,,,,,,,,,,,,,,,digital twins; GUI; Web3D,0.0,EC3,2487
10.1145/3612918,0,0,0,0,0,346,"Sun, Danfeng; Hu, Junjie; Wu, Huifeng; Wu, Jia; Yang, Jian; Sheng, Quan Z.; Dustdar, Schahram",A Comprehensive Survey on Collaborative Data-access Enablers in the IIoT,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3612918,2023-09,,,,,,,"The scope of the Industrial Internet of Things (IIoT) has stretched beyond manufacturing to include energy, healthcare, transportation, and all that tomorrow’s smart cities will entail. The realm of IIoT includes smart sensors, actuators, programmable logic controllers, distributed control systems (DCS), embedded devices, supervisory control, and data acquisition systems—all produced by manufacturers for different purposes and with different data structures and formats; designed according to different standards and made to follow different protocols. In this sea of incompatibility, how can we flexibly acquire these heterogeneous data, and how can we uniformly structure them to suit thousands of different applications? In this article, we survey the four pillars of information science that enable collaborative data access in an IIoT—standardization, data acquisition, data fusion, and scalable architecture—to provide an up-to-date audit of current research in the field. Here, standardization in IIoT relies on standards and technologies to make things communicative; data acquisition attempts to transparently collect data through plug-and-play architectures, reconfigurable schemes, or hardware expansion; data fusion refers to the techniques and strategies for overcoming heterogeneity in data formats and sources; and scalable architecture provides basic techniques to support heterogeneous requirements. The article also concludes with an overview of the frontier researches and emerging technologies for supporting or challenging data access from the aspects of 5G, machine learning, blockchain, and semantic web.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,876
10.1145/3614428,0,0,0,0,0,367,"Ma, Xueyan",The Application Research of Distributed Interface Coordination Based on Deep Reinforcement Learning in English MOOC Platform,ACM Trans. Asian Low-Resour. Lang. Inf. Process.,,2375-4699,https://doi.org/10.1145/3614428,2023-10,,,,,,,"Developing information technology and other business management firms is not the same as technological advancement. The only way we can declare that complete technical progress is in place and operating well in the nation is if the education system has been built. The educational system is set up to instruct pupils via wireless technologies and online classes. Advancement in online classes has resulted in the Massive Open Online Courses (MOOC) Platform. The advantage of the MOOC platform is that it is a platform that provides free online courses for anyone to register and continue the course at their own pace. On this MOOC platform, implementing Deep Reinforcement Learning (DRL) aids in choosing the course and learning through an intelligent mechanism. The transmission between students and teachers will significantly rise because of this teaching strategy through the Distributed Interface Coordination (DIC) mechanism where the resources are distributed in the wireless networking environment. Additionally, the registered students should possess privileges to access the resources. This combination of DIC with DRL can be termed as DIC-DRL model for the proposed system. The data was collected using a low packet rate and outdated wireless connection technologies. The suggested model has a 97.67 percent accuracy rate in comparison with the existing model's support vector machine (SVM) mechanism.",,,,,,,,,,,,,,,,,,,,,,,,,,Deep Reinforcement Learning (DRL); DIC-DRL model; Distributed Interface Coordination (DIC); English Learning; intelligent mechanism; Massive Open Online Courses (MOOC) Platform; Support Vector Machine (SVM); Wireless networking environment,,,1003
10.1145/3617591,0,0,0,0,0,321,"Queiroz, Rui; Cruz, Tiago; Mendes, Jérôme; Sousa, Pedro; Simões, Paulo",Container-based Virtualization for Real-time Industrial Systems—A Systematic Review,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3617591,2023-10,,,,,,,"Industrial Automation and Control systems have matured into a stable infrastructure model that has been kept fundamentally unchanged, using discrete embedded systems (such as Programmable Logic Controllers) to implement the first line of sensorization, actuation, and process control and stations and servers providing monitoring, supervision, logging/database and data-sharing capabilities, among others. More recently, with the emergence of the Industry 4.0 paradigm and the need for more flexibility, there has been a steady trend towards virtualizing some of the automation station/server components, first by using virtual machines and, more recently, by using container technology. This trend is pushing for better support for real-time requirements on enabling virtualization technologies such as virtual machines and containers. This article provides a systematic review on the use of container virtualization in real-time environments such as cyber-physical systems, assessing how existing and emerging technologies can fulfill the associated requirements. Starting by reviewing fundamental concepts related to container technology and real-time requirements, it goes on to present the methodology and results of a systematic study of 37 selected papers covering aspects related to the enforcement of real-time constrains within container hosts and the expected task latency on such environments, as well as an overview of container platforms and orchestration mechanisms for RT systems.",,,,,,,,,,,,,,,,,,,,,,,,,,industrial automation control systems; latency; Real-time containers; virtualization of cyber-physical systems,,,204
10.1145/3617992,0,0,0,0,0,366,"Hamissi, Asma; Dhraief, Amine",A Survey on the Unmanned Aircraft System Traffic Management,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3617992,2023-10,,,,,,,"The Unmanned Aircraft System Traffic Management (UTM) system is a set of services offering an automated management of the airspace and thus providing safe and secure Unmanned Aerial Vehicle (UAV) flights in both controlled and uncontrolled airspace. Controlled airspace refers to the portion of the airspace that is under the authority of Air Traffic Control (ATC) and where separation services are offered, while uncontrolled airspace refers to the portion of airspace where aircraft are not regulated by ATC. This article is a comprehensive survey of the existing UTMs development efforts with a focus on the different UTMs architectures, the provided services, the used communication technologies and the decision-making process within UTMs. We firstly review the different UTM architecture and propose a novel UTM taxonomy based on high-level qualitative criteria. Secondly, we detail the services provided by UTMs with an emphasis on the used technologies in the identification, the surveillance, the monitoring, and the deconfliction services. Effective decision-making is crucial, particularly in emergency scenarios such as Air-to-Ground (A2G) communication loss, battery or motor malfunction, or encountering aerial obstacles, among other potential hazards. Despite its significance, the UTM decision-making process is not enough considered in the literature and especially in UTM surveys. We analyze and compare in this article both the centralized and decentralized UTM decision-making. Centralized decision-making is not conducted in real-time and primarily relies on Air-to-Ground (A2G) communication. In the decentralized case, the decision-making process primarily relies on communication and collaboration among UAVs with varying degrees of autonomy. We show in this paper that centralized decision-making may encounter issues with packet loss and imperfect data, which can negatively impact the quality of decision-making. We also highlight that the decentralized decision-making may also face challenges related to security and scalability, which can hinder its effectiveness. Finally, evaluating the performance of UTMs on a real environment raises several challenges and the simulation is a cost-effective alternative. Hence, we provide a summary of the existing UTMs simulators and discuss their main features.",,,,,,,,,,,,,,,,,,,,,,,,,,performance; architecture; services; UAV; communication; decision-making; simulators; UTM,,,517
10.1145/3620665.3640413,1,1,1,0,0,179,"Mahapatra, Rohan; Ghodrati, Soroush; Ahn, Byung Hoon; Kinzer, Sean; Wang, Shu-Ting; Xu, Hanyang; Karthikeyan, Lavanya; Sharma, Hardik; Yazdanbakhsh, Amir; Alian, Mohammad; Esmaeilzadeh, Hadi",In-Storage Domain-Specific Acceleration for Serverless Computing,"Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2",979-8-4007-0385-0,,https://doi.org/10.1145/3620665.3640413,2024,ASPLOS '24,,,,Association for Computing Machinery,"New York, NY, USA","While (I) serverless computing is emerging as a popular form of cloud execution, datacenters are going through major changes: (II) storage dissaggregation in the system infrastructure level and (III) integration of domain-specific accelerators in the hardware level. Each of these three trends individually provide significant benefits; however, when combined the benefits diminish. On the convergence of these trends, the paper makes the observation that for serverless functions, the overhead of accessing dissaggregated storage overshadows the gains from accelerators. Therefore, to benefit from all these trends in conjunction, we propose In-Storage Domain-Specific Acceleration for Serverless Computing (dubbed DSCS-Serverless1). The idea contributes a server-less model that utilizes a programmable accelerator embedded within computational storage to unlock the potential of acceleration in disaggregated datacenters. Our results with eight applications show that integrating a comparatively small accelerator within the storage (DSCS-Serverless) that fits within the storage's power constraints (25 Watts), significantly outperforms a traditional disaggregated system that utilizes NVIDIA RTX 2080 Ti GPU (250 Watts). Further, the work highlights that disaggregation, serverless model, and the limited power budget for computation in storage device require a different design than the conventional practices of integrating microprocessors and FPGAs. This insight is in contrast with current practices of designing computational storage devices that are yet to address the challenges associated with the shifts in datacenters. In comparison with two such conventional designs that use ARM cores or a Xilinx FPGA, DSCS-Serverless provides 3.7× and 1.7× end-to-end application speedup, 4.3× and 1.9× energy reduction, and 3.2× and 2.3× better cost efficiency, respectively.",,,,,,,,,,,,,,,,,,,,,,,,,,serverless computing; accelerator; computational storage drive (CSD); deep neural network (DNN); disaggregated datacenter; domain specific architecture (DSA); in-storage acceleration; large language model (LLM); neural processing unit (NPU); serverless function; storage systems,0.0,EC3,2640
10.1145/3620678.3624644,0,0,0,0,0,16,"Thiede, John; Bashir, Noman; Irwin, David; Shenoy, Prashant",Carbon Containers: A System-level Facility for Managing Application-level Carbon Emissions,Proceedings of the 2023 ACM Symposium on Cloud Computing,979-8-4007-0387-4,,https://doi.org/10.1145/3620678.3624644,2023,SoCC '23,,,,Association for Computing Machinery,"New York, NY, USA","To reduce their environmental impact, cloud datacenters' are increasingly focused on optimizing applications' carbon-efficiency, or work done per mass of carbon emitted. To facilitate such optimizations, we present Carbon Containers, a simple system-level facility, which extends prior work on power containers, that automatically regulates applications' carbon emissions in response to variations in both their work-load's intensity and their energy's carbon-intensity. Specifically, Carbon Containers enable applications to specify a maximum carbon emissions rate (in g.CO2e/hr), and then transparently enforce this rate via a combination of vertical scaling, container migration, and suspend/resume while maximizing either energy-efficiency or performance.Carbon Containers are especially useful for applications that i) must continue running even during high-carbon periods, and ii) execute in regions with few variations in carbon-intensity. These low-variability regions also tend to have high average carbon-intensity, which increases the importance of regulating carbon emissions. We implement a Carbon Container prototype by extending Linux Containers to incorporate the mechanisms above and evaluate it using real workload traces and carbon-intensity data from multiple regions. We compare Carbon Containers with prior work that regulates carbon emissions by suspending/resuming applications during high/low carbon periods. We show that Carbon Containers are more carbon-efficient and improve performance while maintaining similar carbon emissions.",,,,,,,,,,,,,,,,,,,,,,,,,,Carbon-efficiency; energy-efficiency; performance,0.0,EC3,1893
10.1145/3620678.3624652,0,0,0,0,0,67,"Zhu, Xiangfeng; She, Guozhen; Xue, Bowen; Zhang, Yu; Zhang, Yongsu; Zou, Xuan Kelvin; Duan, XiongChun; He, Peng; Krishnamurthy, Arvind; Lentz, Matthew; Zhuo, Danyang; Mahajan, Ratul",Dissecting Overheads of Service Mesh Sidecars,Proceedings of the 2023 ACM Symposium on Cloud Computing,979-8-4007-0387-4,,https://doi.org/10.1145/3620678.3624652,2023,SoCC '23,,,,Association for Computing Machinery,"New York, NY, USA","Service meshes play a central role in the modern application ecosystem by providing an easy and flexible way to connect microservices of a distributed application. However, because of how they interpose on application traffic, they can substantially increase application latency and its resource consumption. We develop a tool called MeshInsight to help developers quantify the overhead of service meshes in deployment scenarios of interest and make informed trade-offs about their functionality vs. overhead. Using MeshInsight, we confirm that service meshes can have high overhead—up to 269% higher latency and up to 163% more virtual CPU cores for our benchmark applications—but the severity is intimately tied to how they are configured and the application workload. IPC (inter-process communication) and socket writes dominate when the service mesh operates as a TCP proxy, but protocol parsing dominates when it operates as an HTTP proxy. MeshInsight also enables us to study the end-to-end impact of optimizations to service meshes. We show that not all seemingly-promising optimizations lead to a notable overhead reduction in realistic settings.",,,,,,,,,,,,,,,,,,,,,,,,,,Cloud computing; Microservices; Service Mesh,0.0,EC3,2641
10.1145/3620678.3624790,0,0,0,0,0,229,"Christofidi, Georgia; Papaioannou, Konstantinos; Doudali, Thaleia Dimitra",Is Machine Learning Necessary for Cloud Resource Usage Forecasting?,Proceedings of the 2023 ACM Symposium on Cloud Computing,979-8-4007-0387-4,,https://doi.org/10.1145/3620678.3624790,2023,SoCC '23,,,,Association for Computing Machinery,"New York, NY, USA","Robust forecasts of future resource usage in cloud computing environments enable high efficiency in resource management solutions, such as autoscaling and overcommitment policies. Production-level systems use lightweight combinations of historical information to enable practical deployments. Recently, Machine Learning (ML) models, in particular Long Short Term Memory (LSTM) neural networks, have been proposed by various works, for their improved predictive capabilities. Following this trend, we train LSTM models and observe high levels of prediction accuracy, even on unseen data. Upon meticulous visual inspection of the results, we notice that although the predicted values seem highly accurate, they are nothing but versions of the original data shifted by one time step into the future. Yet, this clear shift seems to be enough to produce a robust forecast, because the values are highly correlated across time. We investigate time series data of various resource usage metrics (CPU, memory, network, disk I/O) across different cloud providers and levels, such as at the physical or virtual machine-level and at the application job-level. We observe that resource utilization displays very small variations in consecutive time steps. This insight can enable very simple solutions, such as data shifts, to be used for cloud resource forecasting and deliver highly accurate predictions. This is the reason why we ask whether complex machine learning models are even necessary to use. We envision that practical resource management systems need to first identify the extent to which simple solutions can be effective, and resort to using machine learning to the extent that enables its practical use.",,,,,,,,,,,,,,,,,,,,,,,,,,Machine Learning; Cloud Computing; Data Persistence; Forecasting; Long Short Term Memory; Persistent Forecast; Prediction; Resource Usage,0.0,EC3,2070
10.1145/3624062.3624285,0,0,0,0,0,41,"Aldinucci, Marco; Baralis, Elena Maria; Cardellini, Valeria; Colonnelli, Iacopo; Danelutto, Marco; Decherchi, Sergio; Di Modica, Giuseppe; Ferrucci, Luca; Gribaudo, Marco; Iannone, Francesco; Lapegna, Marco; Medic, Doriana; Muscianisi, Giuseppa; Righetti, Francesca; Sciacca, Eva; Tonellotto, Nicola; Tortonesi, Mauro; Trunfio, Paolo; Vardanega, Tullio",A Systematic Mapping Study of Italian Research on Workflows,"Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis",979-8-4007-0785-8,,https://doi.org/10.1145/3624062.3624285,2023,SC-W '23,,,,Association for Computing Machinery,"New York, NY, USA","An entire ecosystem of methodologies and tools revolves around scientific workflow management. They cover crucial non-functional requirements that standard workflow models fail to target, such as interactive execution, energy efficiency, performance portability, Big Data management, and intelligent orchestration in the Computing Continuum. Characterizing and monitoring this ecosystem is crucial to developing an informed view of current and future research directions. This work conducts a systematic mapping study of the Italian workflow research community, analyzing 25 tools and 10 applications from several scientific domains in the context of the ""National Research Centre for HPC, Big Data, and Quantum Computing"" (ICSC). The study aims to outline the main current research directions and determine how they address the critical needs of modern scientific applications. The findings highlight a variegated research ecosystem of tools, with a prominent interest in advanced workflow orchestration and still immature but promising efforts toward energy efficiency.",,,,,,,,,,,,,,,,,,,,,,,,,,Computing Continuum; HPC; Scientific Workflows,,,464
10.1145/3624354.3630584,0,0,0,0,0,125,"Bartolomeo, Giovanni; Cao, Jacky; Su, Xiang; Mohan, Nitinder",Characterizing Distributed Mobile Augmented Reality Applications at the Edge,Companion of the 19th International Conference on Emerging Networking EXperiments and Technologies,979-8-4007-0407-9,,https://doi.org/10.1145/3624354.3630584,2023,CoNEXT 2023,,,,Association for Computing Machinery,"New York, NY, USA","Mobile Augmented Reality (AR) is gaining traction as a compelling application due to recent advancements in hardware and software. Previous studies have suggested that distributing AR services on an edge computing infrastructure can offer significant performance benefits, especially for consolidating concurrent clients. In this study, we shed light on several research challenges directly impacting the effective integration of distributed AR and edge computing. Specifically, we conduct extensive experiments by deploying our distributed stream processing-based AR pipeline, scAtteR, on a representative edge-cloud infrastructure managed by the Oakestra framework. We uncover several unapparent challenges that inhibit the effective marriage of distributed AR when deployed on edge and demonstrate the potential improvements through scAtteR++. We offer valuable insights and best practices to the growing AR research community, specifically those interested in leveraging edge and public cloud technologies for large-scale AR operations.",,,,,,,,,,,,,,,,,,,,,,,,,,containers; orchestration; edge computing; Augmented reality; distributed stream processing,0.0,EC3,2971
10.1145/3624486.3624504,1,1,1,0,0,39,"Chochliouros, Ioannis P.; Pages-Montanera, Enric; Alcázar-Fernández, Aitor; Zahariadis, Theodore; Velivassaki, Terpsichori-Helen; Skianis, Charalabos; Rossini, Rosaria; Belesioti, Maria; Drosos, Nikolaos; Bakiris, Emmanouil; Pedholla, Prashanth Kumar; Karkazis, Panagiotis; Samal, Astik Kumar; Contreras Murillo, Luis Miguel; Del Río, Alberto; Serrano, Javier; Skias, Dimitrios; Segou, Olga E.; Waechter, Sonja",NEMO: Building the Next Generation Meta Operating System,"Proceedings of the 3rd Eclipse Security, AI, Architecture and Modelling Conference on Cloud to Edge Continuum",979-8-4007-0835-0,,https://doi.org/10.1145/3624486.3624504,2023,eSAAM '23,,,,Association for Computing Machinery,"New York, NY, USA","Artificial Intelligence of Things (AIoT) is one of the next big concepts to support societal changes and economic growth, being one of the fastest growing ICT segments. A specific challenge is to leverage existing technology strengths to develop solutions that sustain the European industry and values. The ongoing ΝΕΜΟ (“Next Generation Meta-Operating System”) EU-funded project intends to establish itself as the “game changer” of the AIoT-Edge-Cloud continuum by introducing an open source, modular and cybersecure meta-operating system, leveraging on existing technologies and introducing novel concepts, methods, tools, testing and engagement campaigns.NEMO will bring intelligence closer to the data and make AI-as-a-Service an integral part of network self-organisation and micro-services execution orchestration. Its widespread penetration and massive acceptance will be achieved via new technology, pre-commercial exploitation components and liaison with open-source communities.By defining a modular and adaptable mOS (meta-OS) architecture together with building blocks and plugins the project will “address” current and future technological and business needs.",,,,,,,,,,,,,,,,,,,,,,,,,,Artificial Intelligence of Things; Meta Operating system; The Cloud-to-Thing continuum: opportunities and challenges,,,362
10.1145/3625549.3658829,0,0,0,0,0,129,"Massa, Jacopo",Towards a Comprehensive Approach to Resource and Conflict Management in Cloud-Edge Settings,Proceedings of the 33rd International Symposium on High-Performance Parallel and Distributed Computing,979-8-4007-0413-0,,https://doi.org/10.1145/3625549.3658829,2024,HPDC '24,,,,Association for Computing Machinery,"New York, NY, USA","As the Internet of Things (IoT) gains popularity, the multitude of connected devices accessing computing facilities along the Cloud-IoT continuum has risen rapidly. This growth has prompted an increase in IoT-based applications and the data they process. These applications often carry specific Quality of Service (QoS) requirements, necessitating the research of strategic management for services and data, coupled with routes across the IoT and application components. Concurrently, our research explores the integration of Intent-Based Networking (IBN) principles to manage data flows and resolve conflicts among stakeholders sharing the same infrastructure. This dual-focus approach aims to support the effective placement and management of multi-service applications on Cloud-IoT resources, considering QoS, data characteristics, and resource conflict detection and resolution. This work outlines our primary objectives and shares findings from our ongoing research.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; constraint and logic languages; intent-based networking; network resources allocation,,,459
10.1145/3626232.3653271,0,0,0,0,0,256,"Empl, Philip; Böhm, Fabian; Pernul, Günther",Process-Aware Intrusion Detection in MQTT Networks,Proceedings of the Fourteenth ACM Conference on Data and Application Security and Privacy,979-8-4007-0421-5,,https://doi.org/10.1145/3626232.3653271,2024,CODASPY '24,,,,Association for Computing Machinery,"New York, NY, USA","Intrusion Detection Systems (IDS) allow for detecting malicious activities in organizational networks and hosts. As the Industrial Internet of Things (Industrial IoT) has gained momentum and attackers become process-aware, it elevates the focus on anomaly-based Network Intrusion Detection Systems (NIDS) in IoT. While previous research has primarily concentrated on fortifying SCADA systems with NIDS, keeping track of the latest advancements in resource-efficient messaging (e.g., MQTT, CoAP, and OPC-UA) is paramount. In our work, we straightforwardly derive IoT processes for NIDS using distributed tracing and process mining. We introduce a pioneering framework called MISSION which effectively captures, consolidates, and models MQTT flows, leading to a heightened process awareness in NIDS. Through our prototypical implementation, we demonstrate exceptional performance and high-quality models. Moreover, our experiments provide empirical evidence for rediscovering pre-defined processes and successfully detecting two distinct MQTT attacks in a simulated IoT network.",,,,,,,,,,,,,,,,,,,,,,,,,,distributed tracing; internet of things; ids; mqtt; process mining,,,1020
10.1145/3626246.3653378,0,0,0,0,0,267,"Pavlenko, Anna; Cahoon, Joyce; Zhu, Yiwen; Kroth, Brian; Nelson, Michael; Carter, Andrew; Liao, David; Wright, Travis; Camacho-Rodríguez, Jesús; Saur, Karla",Vertically Autoscaling Monolithic Applications with CaaSPER: Scalable Container-as-a-Service Performance Enhanced Resizing Algorithm for the Cloud,Companion of the 2024 International Conference on Management of Data,979-8-4007-0422-2,,https://doi.org/10.1145/3626246.3653378,2024,SIGMOD/PODS '24,,,,Association for Computing Machinery,"New York, NY, USA","Kubernetes has emerged as a prominent open-source platform for managing cloud applications, including stateful databases. These monolithic applications rely on vertical scaling, adjusting CPU cores based on load fluctuations. However, our analysis of Kubernetes-based Database-as-a-Service (DBaaS) offerings at Microsoft revealed that many customers consistently over-provision resources for peak workloads, neglecting cost-saving opportunities through resource scale-down. We found that there is a gap in the ability of existing vertical autoscaling tools to minimize resource slack and respond promptly to throttling, leading to increased costs and impacting crucial metrics such as throughput and availability.To address this challenge, we propose CaaSPER, a vertical autoscaling algorithm that blends reactive and proactive strategies. By dynamically adjusting CPU resources, CaaSPER minimizes resource slack, maintains optimal CPU utilization, and reduces throttling. Importantly, customers have the flexibility to prioritize either cost savings or high performance based on their preferences. Extensive testing demonstrates that CaaSPER effectively reduces throttling and keeps CPU utilization within target levels. CaaSPER is designed to be application-agnostic and platform-agnostic, with potential for extension to other applications requiring vertical autoscaling.",,,,,,,,,,,,,,,,,,,,,,,,,,kubernetes; containers; resource optimization; vertical auto-scaling,0.0,EC3,2564
10.1145/3627163,0,0,0,0,0,399,"Silva, Samira; Pelliccione, Patrizio; Bertolino, Antonia",Self-Adaptive Testing in the Field,ACM Trans. Auton. Adapt. Syst.,,1556-4665,https://doi.org/10.1145/3627163,2024-02,,,,,,,"We are increasingly surrounded by systems connecting us with the digital world and facilitating our life by supporting our work, leisure, activities at home, health, and so on. These systems are pressed by two forces. On the one side, they operate in environments that are increasingly challenging due to uncertainty and uncontrollability. On the other side, they need to evolve, often in a continuous fashion, to meet changing needs, to offer new functionalities, or also to fix emerging failures. To make the picture even more complex, these systems rarely work in isolation and often need to collaborate with other systems, as well as humans. All such facets call for moving their validation during operation, as offered by approaches called testing in the field.In this article, we observe that even the field-based testing approaches should change over time to follow and adapt to the changes and evolution of collaborating systems or environments or users’ behaviors. We provide a taxonomy of this new category of testing that we call self-adaptive testing in the field (SATF), together with a reference architecture for SATF approaches. To achieve this objective, we surveyed the literature and collected feedback and contributions from experts in the domain via a questionnaire and interviews.",,,,,,,,,,,,,,,,,,,,,,,,,,knowledge gaps; self-adaptive testing; Software testing in the field,,,989
10.1145/3627703.3629587,0,0,0,0,0,27,"Chow, Ka-Ho; Deshpande, Umesh; Deenadayalan, Veera; Seshadri, Sangeetha; Liu, Ling",Atlas: Hybrid Cloud Migration Advisor for Interactive Microservices,Proceedings of the Nineteenth European Conference on Computer Systems,979-8-4007-0437-6,,https://doi.org/10.1145/3627703.3629587,2024,EuroSys '24,,,,Association for Computing Machinery,"New York, NY, USA","Hybrid cloud provides an attractive solution to microservices for better resource elasticity. A subset of application components can be offloaded from the on-premises cluster to the cloud, where they can readily access additional resources. However, the selection of this subset is challenging because of the large number of possible combinations. A poor choice degrades the application performance, disrupts the critical services, and increases the cost to the extent of making the use of hybrid cloud unviable. This paper presents Atlas, a hybrid cloud migration advisor. Atlas uses a data-driven approach to learn how each user-facing API utilizes different components and their network footprints to drive the migration decision. It learns to accelerate the discovery of high-quality migration plans from millions and offers recommendations with customizable trade-offs among three quality indicators: end-to-end latency of user-facing APIs representing application performance, service availability, and cloud hosting costs. Atlas continuously monitors the application even after the migration for proactive recommendations. Our evaluation shows that Atlas can achieve 21% better API performance (latency) and 11% cheaper cost with less service disruption than widely used solutions.",,,,,,,,,,,,,,,,,,,,,,,,,,placement; microservices; machine learning; API; cyberattacks; hybrid cloud,0.0,EC3,2379
10.1145/3628096.3629081,0,0,0,0,0,216,"Ekele, Emeka; Oyewale, Christianah Titilope; Abolade, Akintomiwa Mayowa",Leveraging Human-Computer Interaction (HCI) for Enhancing Military Operations in Nigeria,Proceedings of the 4th African Human Computer Interaction Conference,979-8-4007-0887-9,,https://doi.org/10.1145/3628096.3629081,2024,AfriCHI '23,,,,Association for Computing Machinery,"New York, NY, USA",The Nigerian Military interaction with the HCI-based system in daily operation shows why they can further be assisted in enhancing the HCI-based system managing their activities. The panel discussion is on how they can further leverage the use of the HCI-based system in improving their operations in Nigeria.,,,,,,,,,,,,,,,,,,,,,,,,,,Criminal Activities; Human-Computer Interaction; Nigerian Military; Technological Solutions; UI/UX,,,1353
10.1145/3629527.3651844,1,1,1,1,1,9,"D'Angelo, Andrea; d'Aloisio, Giordano",Grammar-Based Anomaly Detection of Microservice Systems Execution Traces,Companion of the 15th ACM/SPEC International Conference on Performance Engineering,979-8-4007-0445-1,,https://doi.org/10.1145/3629527.3651844,2024,ICPE '24 Companion,,,,Association for Computing Machinery,"New York, NY, USA","Microservice architectures are a widely adopted architectural pattern for large-scale applications. Given the large adoption of these systems, several works have been proposed to detect performance anomalies starting from analysing the execution traces. However, most of the proposed approaches rely on machine learning (ML) algorithms to detect anomalies. While ML methods may be effective in detecting anomalies, the training and deployment of these systems as been shown to be less efficient in terms of time, computational resources, and energy required.In this paper, we propose a novel approach based on Context-free grammar for anomaly detection of microservice systems execution traces. We employ the SAX encoding to transform execution traces into strings. Then, we select strings encoding anomalies, and for each possible anomaly, we build a Context-free grammar using the Sequitur grammar induction algorithm. We test our approach on two real-world datasets and compare it with a Logistic Regression classifier. We show how our approach is more effective in terms of training time of 15 seconds with a minimum loss in effectiveness of 5% compared to the Logistic Regression baseline.",,,,,,,,,,,,,,,,,,,,,,,,,,anomaly detection; context-free grammar; execution traces; micro service system,0.0,IC4,132
10.1145/3629527.3652901,0,0,0,0,0,65,"Farahani, Reza; Loh, Frank; Roman, Dumitru; Prodan, Radu",Serverless Workflow Management on the Computing Continuum: A Mini-Survey,Companion of the 15th ACM/SPEC International Conference on Performance Engineering,979-8-4007-0445-1,,https://doi.org/10.1145/3629527.3652901,2024,ICPE '24 Companion,,,,Association for Computing Machinery,"New York, NY, USA","The growing desire among application providers for a cost model based on pay-per-use, combined with the need for a seamlessly integrated platform to manage the complex workflows of their applications, has spurred the emergence of a promising computing paradigm known as serverless computing. Although serverless computing was initially considered for cloud environments, it has recently been extended to other layers of the computing continuum, i.e., edge and fog. This extension emphasizes that the proximity of computational resources to data sources can further reduce costs and improve performance and energy efficiency. However, orchestrating the computing continuum in complex application workflows, including a set of serverless functions, introduces new challenges. This paper investigates the opportunities and challenges introduced by serverless computing for workflow management systems (WMS) on the computing continuum. In addition, the paper provides a taxonomy of state-of-the-art WMSs and reviews their capabilities.",,,,,,,,,,,,,,,,,,,,,,,,,,serverless computing; edge-cloud continuum; function scheduling; function-as-a-service (faas); service orchestration; sustainability; workflow; workflow management systems (wms),0.0,EC3,2239
10.1145/3630006,0,0,0,0,0,327,"Zhao, Laiping; Cui, Yushuai; Yang, Yanan; Zhou, Xiaobo; Qiu, Tie; Li, Keqiu; Bao, Yungang",Component-distinguishable Co-location and Resource Reclamation for High-throughput Computing,ACM Trans. Comput. Syst.,,0734-2071,https://doi.org/10.1145/3630006,2024-02,,,,,,,"Cloud service providers improve resource utilization by co-locating latency-critical (LC) workloads with best-effort batch (BE) jobs in datacenters. However, they usually treat multi-component LCs as monolithic applications and treat BEs as “second-class citizens” when allocating resources to them. Neglecting the inconsistent interference tolerance abilities of LC components and the inconsistent preemption loss of BE workloads can result in missed co-location opportunities for higher throughput.We present Rhythm, a co-location controller that deploys workloads and reclaims resources rhythmically for maximizing the system throughput while guaranteeing LC service’s tail latency requirement. The key idea is to differentiate the BE throughput launched with each LC component, that is, components with higher interference tolerance can be deployed together with more BE jobs. It also assigns different reclamation priority values to BEs by evaluating their preemption losses into a multi-level reclamation queue. We implement and evaluate Rhythm using workloads in the form of containerized processes and microservices. Experimental results show that it can improve the system throughput by 47.3%, CPU utilization by 38.6%, and memory bandwidth utilization by 45.4% while guaranteeing the tail latency requirement.",,,,,,,,,,,,,,,,,,,,,,,,,,co-locating; Datacenters; resource utilization; tail latency,,,342
10.1145/3631461.3631562,0,0,0,0,0,49,"Holzer, Stefan; Frangoudis, Pantelis; Tsigkanos, Christos; Dustdar, Schahram",SMT-as-a-Service for Fog-Supported Cyber-Physical Systems,Proceedings of the 25th International Conference on Distributed Computing and Networking,979-8-4007-1673-7,,https://doi.org/10.1145/3631461.3631562,2024,ICDCN '24,,,,Association for Computing Machinery,"New York, NY, USA","Various properties related with the safe, correct, and efficient operation of Cyber-Physical Systems (CPS) can be expressed via formal languages and checked at runtime or offline by appropriate verification tools. Such tools operate on monitoring data about the CPS state and functionality, typically collected from IoT devices. A specific approach involves modeling CPS state or operations using Satisfiability Modulo Theories (SMT) formalisms, and using solver software to check whether given CPS properties are satisfied or to derive satisfiable CPS configurations. The computational requirements of this process can however be significant, which challenges its timely execution on IoT/edge devices where input date originate. To address this challenge, we present an architecture that allows the distributed execution of SMT problem solving workloads over the computing continuum as a service. Our design supports arbitrary hierarchies of solver nodes running anywhere from the IoT device to the cloud, each independently executing decision-making logic as to whether to solve an SMT problem instance locally or to recursively offload the task to other nodes in the continuum. We demonstrate the benefits of offloading by implementing and quantitatively evaluating different reinforcement learning-based decision-making strategies addressing latency minimization and energy efficiency goals, and showcase the practicability of our scheme in a fog robotics proof-of-concept.",,,,,,,,,,,,,,,,,,,,,,,,,,IoT; Computing continuum; Computation Offloading; Fog Robotics; Satisfiability Modulo Theories,0.0,EC3,2287
10.1145/3631976,0,0,0,0,0,393,"Cederbladh, Johan; Cicchetti, Antonio; Suryadevara, Jagadish",Early Validation and Verification of System Behaviour in Model-based Systems Engineering: A Systematic Literature Review,ACM Trans. Softw. Eng. Methodol.,,1049-331X,https://doi.org/10.1145/3631976,2024-03,,,,,,,"In the Systems Engineering (SE) domain there has been a paradigm shift from document-based to model-based system development artefacts; in fact, new methodologies are emerging to meet the increasing complexity of current systems and the corresponding growing need of digital workflows. In this regard, Model-Based Systems Engineering (MBSE) is considered as a key enabler by many central players of the SE community. MBSE has reached an adequate level of maturity, and there exist documented success stories in its adoption in industry. In particular, one significant benefit of utilising MBSE when compared to the traditional manual and document-centric workflows is that models are available from early phases of systems development; these enable a multitude of analyses prior any implementation effort together with other relevant capabilities, like the automation of development tasks. Nonetheless, it is noticeable there is a lack of a common understanding for how formal analyses for the verification and validation (V&amp;V) of systems behaviour, specifically in the early phases of development, could be placed in an MBSE setting.In this article, we report on the planning, execution, and results of a systematic literature review regarding the early V&amp;V of systems behaviour in the context of model-based systems engineering. The review aims to provide a structured representation of the state of the art with respect to motivations, proposed solutions, and limitations. From an initial set of potentially relevant 701 peer-reviewed publications we selected 149 primary studies, which we analysed according to a rigorous data extraction, analysis, and synthesis process. Based on our results, early V&amp;V has usually the goal of checking the quality of a system design to avoid discovering flaws when parts are being concretely realised; SysML is a de facto standard for describing the system under study, while the solutions for the analyses tend to be varied; also V&amp;V analyses tend to target varied properties with a slight predominance of functional concerns, and following the variation mentioned so far the proposed solutions are largely context specific; the proposed approaches are usually presented without explicit limitations, while when limitations are discussed, readiness of the solutions, handling of analyses simplifications/assumptions, and languages/tools integration are among the most frequently mentioned issues.Based on the survey results and the standard SE practices, we discuss how the current state-of-the-art MBSE supports early V&amp;V of systems behaviour with a special focus on industrial adoption and identify relevant challenges to be researched further.",,,,,,,,,,,,,,,,,,,,,,,,,,MBSE; system behaviour; systematic literature review; validation; verification,,,861
10.1145/3634769.3634812,0,0,0,0,0,108,"Souza, Abel; Jasoria, Shruti; Chakrabarty, Basundhara; Bridgwater, Alexander; Lundberg, Axel; Skogh, Filip; Ali-Eldin, Ahmed; Irwin, David; Shenoy, Prashant",CASPER: Carbon-Aware Scheduling and Provisioning for Distributed Web Services,Proceedings of the 14th International Green and Sustainable Computing Conference,979-8-4007-1669-0,,https://doi.org/10.1145/3634769.3634812,2024,IGSC '23,,,,Association for Computing Machinery,"New York, NY, USA","There has been a significant societal push towards sustainable practices, including in computing. Modern interactive workloads such as geo-distributed web-services exhibit various spatiotemporal and performance flexibility, enabling the possibility to adapt the location, time, and intensity of processing to align with the availability of renewable and low-carbon energy. An example is a web application hosted across multiple cloud regions, each with varying carbon intensity based on their local electricity mix. Distributed load-balancing enables the exploitation of low-carbon energy through load migration across regions, reducing web applications carbon footprint. In this paper, we present CASPER, a carbon-aware scheduling and provisioning system that primarily minimizes the carbon footprint of distributed web services while also respecting their Service Level Objectives (SLO). We formulate CASPER as an multi-objective optimization problem that considers both the variable carbon intensity and latency constraints of the network. Our evaluation reveals the significant potential of CASPER in achieving substantial reductions in carbon emissions. Compared to baseline methods, CASPER demonstrates improvements of up to 70% with no latency performance degradation.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,EC3,1759
10.1145/3634814.3634838,1,1,1,1,0,17,"Kawalkar, Shreyash N",Geo-Intelligent Architecture for Smart Grid Evolution: Addressing Contemporary Challenges through Spatial AI and Knowledge Integration,Proceedings of the 2023 4th Asia Service Sciences and Software Engineering Conference,979-8-4007-0853-4,,https://doi.org/10.1145/3634814.3634838,2024,ASSE '23,,,,Association for Computing Machinery,"New York, NY, USA","This research explores the challenges of contemporary Smart Grid (SG) technology products and proposes a novel methodology to address them. Through a comprehensive survey, we identified common hurdles SG technologies face, especially with the rapid evolution of technology and the expansion of energy sector assets. Central to our methodology is the development of a conceptual architecture that is adaptive, scalable, and optimized for complex data management. Key features of this architecture include a Modular Architecture with Micro-Services, Serverless 2.0 for Scalability, and the integration of Knowledge Graphs for enhanced data-driven decision-making. At the heart of these solutions lies the synthesis of geospatial intelligence via Geo-Spatial AI and the use of cognitive mapping to bridge micro-services with energy assets, to ensure the grid's responsive adjustment to dynamic energy landscapes. By synthesizing advanced AI technologies and geospatial mapping techniques, our approach promises a leap in efficiency, adaptability, and accuracy for future Smart Grid platforms.",,,,,,,,,,,,,,,,,,,,,,,,,,AI in Modern Energy Infrastructures; AI Solutions for Smart Grid Technology Challenges; Cognitive Mapping of Micro Services to Energy Assets; Context aware Energy Management System; Geo Intelligent Software Architecture; Knowledge Graph Embedded AI; Streamlined Smart Grid Assets and Data Management,,,166
10.1145/3636428,0,0,0,0,0,315,"Gheibi, Omid; Weyns, Danny",Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive Systems Using Lifelong Self-Adaptation,ACM Trans. Auton. Adapt. Syst.,,1556-4665,https://doi.org/10.1145/3636428,2024-02,,,,,,,"Recently, machine learning (ML) has become a popular approach to support self-adaptation. ML has been used to deal with several problems in self-adaptation, such as maintaining an up-to-date runtime model under uncertainty and scalable decision-making. Yet, exploiting ML comes with inherent challenges. In this article, we focus on a particularly important challenge for learning-based self-adaptive systems: drift in adaptation spaces. With adaptation space, we refer to the set of adaptation options a self-adaptive system can select from to adapt at a given time based on the estimated quality properties of the adaptation options. A drift of adaptation spaces originates from uncertainties, affecting the quality properties of the adaptation options. Such drift may imply that the quality of the system may deteriorate, eventually, no adaptation option may satisfy the initial set of adaptation goals, or adaptation options may emerge that allow enhancing the adaptation goals. In ML, such a shift corresponds to a novel class appearance, a type of concept drift in target data that common ML techniques have problems dealing with. To tackle this problem, we present a novel approach to self-adaptation that enhances learning-based self-adaptive systems with a lifelong ML layer. We refer to this approach as lifelong self-adaptation. The lifelong ML layer tracks the system and its environment, associates this knowledge with the current learning tasks, identifies new tasks based on differences, and updates the learning models of the self-adaptive system accordingly. A human stakeholder may be involved to support the learning process and adjust the learning and goal models. We present a general architecture for lifelong self-adaptation and apply it to the case of drift of adaptation spaces that affects the decision-making in self-adaptation. We validate the approach for a series of scenarios with a drift of adaptation spaces using the DeltaIoT exemplar.",,,,,,,,,,,,,,,,,,,,,,,,,,concept drift; lifelong self-adaptation; machine-learning; novel class appearance; Self-adaptation,,,1091
10.1145/3636534.3701546,0,0,0,0,0,213,"Branco da Silva, Paulo Ricardo; Henriques Sales de Lima, João Paulo; Costa Alves, Erika; Farfan, William Sanchez; Coutinho, Victor Aguiar; Paiva, Thomas William do Prado; Feferman, Daniel Lazkani; Neto, Francisco Hugo Costa",Evaluation of the Latency of Machine Learning Random Access DDoS Detection in Open RAN,Proceedings of the 30th Annual International Conference on Mobile Computing and Networking,979-8-4007-0489-5,,https://doi.org/10.1145/3636534.3701546,2024,ACM MobiCom '24,,,,Association for Computing Machinery,"New York, NY, USA","A testbed with open-source tools and commercial equipment is designed to investigate the timing of ML-based xApp operations in DDoS attack detection during the Random Access procedure. Results show high-precision and high-recall ML classifiers operating faster than the Medium Access Control (MAC) layer's Contention Resolution timer. For classifiers with over 97% precision and recall, XGBoost has the fastest execution time and the narrowest estimated run time probability density function.",,,,,,,,,,,,,,,,,,,,,,,,,,machine learning; RIC; DDoS; open RAN; signaling storm,,,670
10.1145/3636534.3701547,0,0,0,0,0,192,"Ghosh, Ushasi; Chiejina, Azuka Jude; Stephenson, Nathan Hughes; Shah, Vijay K; Shakkottai, Srinivas; Bharadia, Dinesh",DEMO: SPARC: Spatio-Temporal Adaptive Resource Control for Multi-site Spectrum Management in NextG Cellular Networks,Proceedings of the 30th Annual International Conference on Mobile Computing and Networking,979-8-4007-0489-5,,https://doi.org/10.1145/3636534.3701547,2024,ACM MobiCom '24,,,,Association for Computing Machinery,"New York, NY, USA","This work presents SPARC (Spatio-Temporal Adaptive Resource Control), a novel approach for multi-site spectrum management in NextG cellular networks. SPARC addresses the challenge of limited licensed spectrum in dynamic environments. We leverage the O-RAN architecture to develop a multi-timescale RAN Intelligent Controller (RIC) framework, featuring an xApp for near-real-time interference detection and localization, and a μApp for real-time intelligent resource allocation. By utilizing base stations as spectrum sensors, SPARC enables efficient and fine-grained dynamic resource allocation across multiple sites, enhancing signal-to-noise ratio (SNR) by up to 7dB, spectral efficiency by up to 15%, and overall system throughput by up to 20%.",,,,,,,,,,,,,,,,,,,,,,,,,,interference detection; traffic; RAN intelligent control,,,1015
10.1145/3638209.3638223,0,0,0,0,0,261,"Osaba, Eneko; Benguria, Gorka; Lobo, Jesus L.; Diaz-De-Arcaya, Josu; Alonso, Juncal; Etxaniz, Iñaki",Optimizing IaC Configurations: a Case Study Using Nature-inspired Computing,Proceedings of the 2023 6th International Conference on Computational Intelligence and Intelligent Systems,979-8-4007-0906-7,,https://doi.org/10.1145/3638209.3638223,2024,CIIS '23,,,,Association for Computing Machinery,"New York, NY, USA","In the last years, one of the fields of artificial intelligence that has been investigated the most is nature-inspired computing. The research done on this specific topic showcases the interest that sparks in researchers and practitioners, who put their focus on this paradigm because of the adaptability and ability of nature-inspired algorithms to reach high-quality outcomes on a wide range of problems. In fact, this kind of methods has been successfully applied to solve real-world problems in heterogeneous fields such as medicine, transportation, industry, or software engineering. Our main objective with this paper is to describe a tool based on nature-inspired computing for solving a specific software engineering problem. The problem faced consists of optimizing Infrastructure as Code deployment configurations. For this reason, the name of the system is IaC Optimizer Platform. A prototypical version of the IOP was described in previous works, in which the functionality of this platform was introduced. With this paper, we take a step forward by describing the final release of the IOP, highlighting its main contribution regarding the current state-of-the-art, and justifying the decisions made on its implementation. Also, we contextualize the IOP within the complete platform in which it is embedded, describing how a user can benefit from its use. To do that, we also present and solve a real-world use case.",,,,,,,,,,,,,,,,,,,,,,,,,,Combinatorial Optimization; Multi-objective Optimization; Nature-inspired Computing; PIACERE,0.0,EC3,3142
10.1145/3638837.3638883,0,0,0,0,0,154,"Huang, Yong Xuan; Wang, Kung Chun; Ke, Bi Shun",Accelerating 5G Service-Based Architecture with Ebpf,"Proceedings of the 2023 12th International Conference on Networks, Communication and Computing",979-8-4007-0926-5,,https://doi.org/10.1145/3638837.3638883,2024,ICNCC '23,,,,Association for Computing Machinery,"New York, NY, USA","Service-Based Architecture (SBA) serves as a foundational methodology for 5G core networks, as delineated by the 3rd Generation Partnership Project (3GPP). This modular approach segments the 5G Core Network into discrete, functionally autonomous Network Functions (NFs), deployed via either virtual machines or container technologies. The efficacy of the SBA execution platform significantly impacts the service capabilities of these NFs. In this study, we present an enhanced architecture that employs extended Berkeley Packet Filter (eBPF) and eXpress Data Path (XDP) technologies to fine-tune the network, security, service, and scheduling aspects within a Kubernetes framework. Additionally, eBPF’s monitoring and packet control features augment the service capabilities of 5G components, thereby improving encrypted data transmission. eBPF’s real-time monitoring aids the scheduler in making nuanced decisions, while an eBPF-driven load balancer ensures equitable service distribution. Comprehensive testing reveals that the enhanced architecture increases transmission efficiency by 17%, reduces average latency by 43%, and alleviates latency anomalies compared to conventional 5G NF platforms. It also enhances the inherent connectivity of 5G NFs by approximately 15%. For encryption-reliant applications, the architecture achieves a 17% performance gain in latency, transmission, and connectivity, while its scheduling and service balancing capabilities demonstrate a roughly 19% performance enhancement.",,,,,,,,,,,,,,,,,,,,,,,,,,5G Core Network; eBPF/XDP; Virtual Network Acceleration,,,337
10.1145/3638884.3638966,0,0,0,0,0,238,"Cui, Ling; Shi, Tianyi; Lu, Ruifang; Zhang, Tiankui",Autoscaling in Mobile Edge Computing Based on Multi-Agent Reinforcement Learning,Proceedings of the 2023 9th International Conference on Communication and Information Processing,979-8-4007-0890-9,,https://doi.org/10.1145/3638884.3638966,2024,ICCIP '23,,,,Association for Computing Machinery,"New York, NY, USA","Mobile edge computing (MEC) has emerged as a transformative paradigm by deploying computing and storage resources at the edge of access networks. This enables real-time processing, reduces latency, and improves user experiences, particularly with the advent of 5G networks. However, the limited resources of edge servers pose a challenge in ensuring service-level agreement (SLA) while optimizing resource utilization. Autoscaling, a dynamic resource adjustment mechanism, can play a pivotal role in meeting these demands. This paper explores the application of multi-agent reinforcement learning (MARL) to address autoscaling in MEC. The proposed algorithm models the horizontal autoscaling problem in MEC, defines state and action spaces, and introduces a global reward function. Additionally, time-series data prediction enhances scaling responsiveness. A testing platform is employed to validate the algorithm’s effectiveness. This work contributes a novel approach to autoscaling in MEC, addressing the complex interplay of autonomous scaling decisions, SLA compliance, and resource optimization within edge server clusters.",,,,,,,,,,,,,,,,,,,,,,,,,,autoscaling; mobile edge computing; reinforcement learning,0.0,EC3,1818
10.1145/3639476.3639759,0,0,0,0,0,155,"Wang, Zerui; Liu, Yan; Arumugam Thiruselvi, Abishek; Hamou-Lhadj, Abdelwahab",XAIport: A Service Framework for the Early Adoption of XAI in AI Model Development,Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results,979-8-4007-0500-7,,https://doi.org/10.1145/3639476.3639759,2024,ICSE-NIER'24,,,,Association for Computing Machinery,"New York, NY, USA","In this study, we propose the early adoption of Explainable AI (XAI) with a focus on three properties: Quality of explanation, the explanation summaries should be consistent across multiple XAI methods; Architectural Compatibility, for effective integration in XAI, the architecture styles of both the XAI methods and the models to be explained must be compatible with the framework; Configurable operations, XAI explanations are operable, akin to machine learning operations. Thus, an explanation for AI models should be reproducible and tractable to be trustworthy. We present XAIport, a framework of XAI microservices encapsulated into Open APIs to deliver early explanations as observation for learning model quality assurance. XAIport enables configurable XAI operations along with machine learning development. We quantify the operational costs of incorporating XAI with three cloud computer vision services on Microsoft Azure Cognitive Services, Google Cloud Vertex AI, and Amazon Rekognition. Our findings show comparable operational costs between XAI and traditional machine learning, with XAIport significantly improving both cloud AI model performance and explanation stability.",,,,,,,,,,,,,,,,,,,,,,,,,,MLOps; XAI; deployment strategy; operational cost analysis,,,608
10.1145/3640115.3640217,0,0,0,0,0,128,"Qiao, Junfeng; Zhou, Aihua; Peng, Lin; Wang, Yiqing; Shen, Xiaofeng; Pan, Sen; Yang, Pei",Research on the Integration and Fusion Technology of Distribution Network Data Resources Considering New Energy,Proceedings of the 6th International Conference on Information Technologies and Electrical Engineering,979-8-4007-0829-9,,https://doi.org/10.1145/3640115.3640217,2024,ICITEE '23,,,,Association for Computing Machinery,"New York, NY, USA","The essence of the construction and development of new power systems is to improve the functionality and performance of existing power systems, and the distribution network is the key to the functionality of the power system. Therefore, the upgrading and optimization of the distribution network is the key to the success of the modern energy revolution, and the development of the distribution network is also faced with management and technical challenges brought by new equipment and resources. This article proposes a distribution network data resource integration and fusion method that takes into account new energy, integrating the constantly emerging new power equipment and facilities as well as their corresponding data resources into existing power information management systems. Using the integrated distribution network data platform as the data base to support the deepening and development of distribution network business applications. Firstly, the data description characteristics of the new power equipment and facilities are sorted out, and the description rule system is extracted using the methods of power equivalence processing and equipment profiling. Due to the lack of standardized description rules for new energy equipment, it is necessary to reorganize and construct a corresponding description rule system. Then map and correspond its description rules with existing power information description rules, find its mapping method, and build a mapping rule library. Finally, based on the mapping rule library, the fusion of new energy data and existing distribution network data resources is achieved, and the accuracy and applicability of data resource fusion are verified by selecting real data from business application scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,Application validation; Data mapping rules; Data resource integration; Distribution network; New energy,0.0,EC3,2344
10.1145/3640310.3674100,0,0,0,0,0,244,"Pfeiffer, Jérôme; Fuchß, Dominik; Kühn, Thomas; Liebhart, Robin; Neumann, Dirk; Neimöck, Christer; Seiler, Christian; Koziolek, Anne; Wortmann, Andreas",Modeling Languages for Automotive Digital Twins: A Survey Among the German Automotive Industry,Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems,979-8-4007-0504-5,,https://doi.org/10.1145/3640310.3674100,2024,MODELS '24,,,,Association for Computing Machinery,"New York, NY, USA","The demand for digital twins and suitable modeling techniques in the automotive industry is increasing rapidly. Yet, there is no common understanding of digital twins in automotive, nor are there modeling techniques established to create automotive digital twins. Recent studies on digital twins focus on the analysis of the literature on digital twins for automotive or in general and, thus, neglect the industrial perspective of automotive practitioners. To mitigate this gap between scientific literature and the industrial perspective, we conducted a questionnaire survey among experts in the German automotive industry to identify i) the desired purposes for and capabilities of digital twins and ii) the modeling techniques related to engineering and operating digital twins across the phases of automotive development. To this end, we contacted 189 members of the Software-Defined Car research project and received 96 responses. The results show that digital twins are considered most useful in the usage and support phase of automotive development, representing vehicles as-operated. Moreover, simulation models, source code, and business process models are currently considered the most important models to be integrated into a digital twin alongside the associated, established tools.",,,,,,,,,,,,,,,,,,,,,,,,,,digital twins; survey; automotive; modeling languages,,,1282
10.1145/3640823,0,0,0,0,0,365,"Rac, Samuel; Brorsson, Mats",Cost-aware Service Placement and Scheduling in the Edge-Cloud Continuum,ACM Trans. Archit. Code Optim.,,1544-3566,https://doi.org/10.1145/3640823,2024-03,,,,,,,"The edge to data center computing continuum is the aggregation of computing resources located anywhere between the network edge (e.g., close to 5G antennas), and servers in traditional data centers. Kubernetes is the de facto standard for the orchestration of services in data center environments, where it is very efficient. It, however, fails to give the same performance when including edge resources. At the edge, resources are more limited, and networking conditions are changing over time.In this article, we present a methodology that lowers the costs of running applications in the edge-to-cloud computing continuum. This methodology can adapt to changing environments, e.g., moving end-users. We are also monitoring some Key Performance Indicators of the applications to ensure that cost optimizations do not negatively impact their Quality of Service. In addition, to ensure that performances are optimal even when users are moving, we introduce a background process that periodically checks if a better location is available for the service and, if so, moves the service. To demonstrate the performance of our scheduling approach, we evaluate it using a vehicle cooperative perception use case, a representative 5G application. With this use case, we can demonstrate that our scheduling approach can robustly lower the cost in different scenarios, while other approaches that are already available fail in either being adaptive to changing environments or will have poor cost-effectiveness in some scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,Cloud computing; resource allocation; scheduling; Kubernetes; edge computing; 5G; container orchestration,0.0,EC3,2049
10.1145/3641384.3641495,0,0,0,0,0,176,"Zheng, Kangzhen; Ni, Yanyan; Qi, Hongtao; Tang, Xu",Data Analysis and Decision Support Effect of Big Data Mining in Energy Internet Cyber-Physical Systems,Proceedings of the 3rd International Conference on Electronic Information Technology and Smart Agriculture,979-8-4007-1677-5,,https://doi.org/10.1145/3641384.3641495,2024,ICEITSA '23,,,,Association for Computing Machinery,"New York, NY, USA","The construction of energy Internet can realize the massive collection of load-side data, and the local utilization of load-side data through edge computing devices can effectively reduce the pressure of cloud computing and communication system, but the computing and solving ability of edge devices is worse than that of cloud, and too many computing tasks of edge devices may also affect the overall scheduling. Therefore, this paper combines big data mining technology to analyze the data analysis and decision support in the cyber-physical systems of energy Internet, and studies the optimal scheduling method of energy Internet in cloud edge environment around the multi-energy complementary process and accurate demand response of energy Internet. The research results show that the energy Internet information model based on information physical fusion takes into account the plug-and-play characteristics of distributed devices in the network, and at the same time reflects the multi-time scale characteristics of energy Internet, taking into account the autonomy of specific devices in the network and so on. From the experimental evaluation, it can be seen that big data mining has a good effect on data analysis and decision support in energy Internet cyber-physical systems.",,,,,,,,,,,,,,,,,,,,,,,,,,data analysis; Big Data Mining; Cyberphysics; Decision Support; Energy Internet,0.0,EC3,1835
10.1145/3641512.3690035,0,0,0,0,0,199,"Kapsalis, Panagiotis; Rimassa, Giovanni; Zeydan, Engin; Via, Selva; Risso, Fulvio; Chiasserini, Carla Fabiana; Vivo, Giulio",A Next Generation Architecture for Internet of Things in the Automotive Supply Chain for Electric Vehicles,"Proceedings of the Twenty-Fifth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing",979-8-4007-0521-2,,https://doi.org/10.1145/3641512.3690035,2024,MobiHoc '24,,,,Association for Computing Machinery,"New York, NY, USA","This paper presents a next-generation architecture that focuses on the advancement of edge computing and Internet of Things (IoT) technologies in the context of the automotive supply value chain for electric vehicles (EVs). First, we outline the general architecture design, the specific layers and their goals. Based on the principles of the proposed architecture, we also give a use case for improving the traceability, monitoring and efficiency of EV battery transportation using innovative approaches in federated data spaces, AI-powered inference and orchestration of a multi-objective computational continuum. The automotive supply chain use case is presented with potential Key Performance Indicators (KPIs) while emphasizing the potential impact on operational efficiency, cost reduction and sustainability. By addressing the current limitations in distributed intelligence, data governance, and cross-domain interoperability, we emphasize the importance of real-time data processing, dynamic field governance, and energy-efficient machine learning in the context of the electric vehicle supply chain. At the end of the paper, a discussion and comparative analysis highlights the advances over existing technologies and frameworks and identifies future directions to further improve innovations and applications in this area.",,,,,,,,,,,,,,,,,,,,,,,,,,distributed computing; cloud orchestration; decentralized AI; edge data processing; IoT orchestration,0.0,EC3,2311
10.1145/3642975.3678962,0,0,0,0,0,72,"Carlinet, Yannick; Perrot, Nancy; Valeyre, Laurent; Wary, Jean-Philippe; Bocianiak, Krzysztof; Niewolski, Wojciech; Podlasek, Aleksandra",Latency-Sensitive Service Chaining with Isolation Constraints,Proceedings of the 1st International Workshop on MetaOS for the Cloud-Edge-IoT Continuum,979-8-4007-0543-4,,https://doi.org/10.1145/3642975.3678962,2024,MECC '24,,,,Association for Computing Machinery,"New York, NY, USA","Multi-access Edge Computing (MEC) is one of the key enablers in 6G. By offering computing capabilities near the end users, it enables lower environmental impact, higher end-to-end latency, higher resiliency, among others. It is also a critical part in the Cloud-Edge-IoT continuum. However, the deployment of services in the edge poses some security challenges. This work aims at solving orchestration issues, such as how to deal with service deployment while meeting service-specific requirements, while at the same time meeting isolation requirements, in a multi-cluster environment. Our algorithm is estimated to be around 20% better than the baseline, while at the same time offering guarantees on the communication latency and isolation requirements, which the baseline cannot offer. We were able to build an orchestration engine, with an advanced algorithm to compute optimal placement of microservices, integrated in a state-of-art emulation platform, with industry best practice. This demonstrates the feasibility in practice and efficiency of our approach.",,,,,,,,,,,,,,,,,,,,,,,,,,Edge Computing; Latency-sensitive Service; Multi-Edge Computing; Security Isolation; Service Chaining,0.0,,1968
10.1145/3642975.3678965,0,0,0,0,0,1,"Galantino, Stefano; Pinto, Andrea; Esposito, Flavio; Manzalini, Antonio; Risso, Fulvio",Balancing Energy Efficiency and Infrastructure Knowledge in Cloud-to-Edge Task Distribution Systems,Proceedings of the 1st International Workshop on MetaOS for the Cloud-Edge-IoT Continuum,979-8-4007-0543-4,,https://doi.org/10.1145/3642975.3678965,2024,MECC '24,,,,Association for Computing Machinery,"New York, NY, USA","In the rapidly evolving landscape of distributed computing, maintaining energy efficiency in edge and data center infrastructures has become critical. While the problem has been faced with centralized approaches assuming knowledge of the available underlying infrastructure resources, this paper introduces a distributed task allocation framework emphasizing energy awareness without requiring infrastructure knowledge. The framework is designed to optimize energy consumption in heterogeneous computing environments, leveraging a distributed consensus algorithm that allows nodes to maximize individual or global goals. Each private custom utility function enables a node to carefully determine whether executing a task is efficient, thus ensuring flexibility in the task allocation process based on local preferences. While showcasing the energy efficiency of our framework, we also illustrate that it is not necessary to disclose the underlying infrastructure resources status, ensuring the preservation of potentially sensitive local resources information. Experimental results demonstrate the framework's ability to achieve optimal power consumption outcomes while maintaining privacy, offering a significant advancement over traditional centralized allocation policies and Kubernetes-like scheduling algorithms.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,EC3,1662
10.1145/3642978.3652833,0,0,0,0,0,210,"Mellone, Gennaro; De Vita, Ciro Giuseppe; Caramiello, Dario; Singh, David Exposito; Blas, Javier Garcia; Ciaramella, Angelo",Workflows' applications in computational environmental science: a survey,Proceedings of the 2nd Workshop on Workflows in Distributed Environments,979-8-4007-0546-5,,https://doi.org/10.1145/3642978.3652833,2024,WiDE '24,,,,Association for Computing Machinery,"New York, NY, USA","This survey paper explores the different applications of workflows in computational environmental science. Workflows are crucial in streamlining complex computational processes, enabling researchers to manage and analyze large-scale environmental data effectively. The paper reviews existing literature, methodologies, and tools associated with workflow applications in environmental science, highlighting their impact on research efficiency, reproducibility, and collaboration. By examining case studies and emerging trends, this survey aims to provide insights into the current landscape of workflow applications within the computational environmental science domain.",,,,,,,,,,,,,,,,,,,,,,,,,,survey; computational environmental science; workflows,,,358
10.1145/3643657.3643909,0,0,0,0,0,96,"Andrikopoulos, Vasilios; Di Nitto, Elisabetta",On the need to merge architectural and infrastructural considerations,Proceedings of the 1st International Workshop on New Trends in Software Architecture,979-8-4007-0560-1,,https://doi.org/10.1145/3643657.3643909,2024,SATrends '24,,,,Association for Computing Machinery,"New York, NY, USA","This work takes the position that attempting to keep a separation between the logical and deployment view of modern, cloud-native software systems only leads to a new type of technical debt that manifests during the operational side of the lifecycle. This is an issue that is fueled by the treatment of software qualities without considering where the systems under consideration are supposed to be deployed, and how long it takes until positive or negative treatments on these qualities manifest themselves. Sustainability is a particularly interesting case of a quality that requires logical and deployments views to be taken into account in a unified way.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; sustainability; software architecture; technical debt,,,657
10.1145/3643833.3656118,0,0,0,0,0,102,"Thimmaraju, Kashyap; Shaik, Altaf; Flück, Sunniva; Mora, Pere Joan Fullana; Werling, Christian; Seifert, Jean-Pierre",Security Testing The O-RAN Near-Real Time RIC &amp; A1 Interface,Proceedings of the 17th ACM Conference on Security and Privacy in Wireless and Mobile Networks,979-8-4007-0582-3,,https://doi.org/10.1145/3643833.3656118,2024,WiSec '24,,,,Association for Computing Machinery,"New York, NY, USA","Open-Radio Access Network (O-RAN) is the next evolutionary step in mobile network architecture and operations and the Near-Real Time RAN Intelligent Controller (Near-RT RIC) plays a central role in the O-RAN architecture as it interfaces between the orchestration layer and next generation eNodeBs. In this paper we highlight the architectural weakness of a centralized controller in O-RAN by first drawing parallels with the Software-Defined Networking (SDN) controller. We then present a two part security evaluation of two open-source Near-RT RICs (μONOS and OSC), focused on the newly introduced A1 interface of the Near-RT RIC. In the first part of our evaluation, we evaluate the supply-chain risks of μONOS and OSC using off-the-shelf open-source dependency analysis and configuration file analysis tools. In the second part, we present our run-time security testing of the A1 API implemented by μONOS and OSC using our custom O-RAN A1 Interface Testing Tool (OAITT). Our supply-chain risk analysis shows that both the open-source Near-RT RICs we evaluated have multiple dependency risks and weak or insecure configurations. We identified 211 and 285 known dependency vulnerabilities in μONOS and OSC respectively of which 82 and 190 dependencies were rated as high CVSS respectively. The A1 interface contributed to a majority of the dependency risks in both Near-RT RICs. From a security misconfiguration perspective, we identified issues concerning access control, lack of encryption and poor secret management. Our run-time testing of OSC and μONOS revealed the following. First, both Near-RT RICs lack TLS for the A1 interface. Second, malicious Non-Real Time RAN Intelligent Controller (Non-RT RIC)s or rApps that reside in the Non-RT RIC could tamper with policies installed in the Near-RT RIC which can impact the availability of the O-RAN. Third, the A1 protocol could be exploited by Non-RT RICs for covert communication via the Near-RT RIC. Fourth, the A1 implementation by μONOS was vulnerable to degradation of service attacks (10-60s response time for GET requests) and a denial of service attack, the latter has been ethically reported and a fix is underway.",,,,,,,,,,,,,,,,,,,,,,,,,,near-real time ric; o-ran; security testing; software security,,,1345
10.1145/3644713.3644786,0,0,0,0,0,235,"Pantin, Roman",Developing a Dynamic Decision-Support Framework for Higher Education Management Systems through Real-time Information Extraction,Proceedings of the 7th International Conference on Future Networks and Distributed Systems,979-8-4007-0903-6,,https://doi.org/10.1145/3644713.3644786,2024,ICFNDS '23,,,,Association for Computing Machinery,"New York, NY, USA","In the changing world of education, it is crucial for institutions to have the ability to make well informed decisions based on data. This paper presents an innovative Decision Support Framework (DDSF), for managing higher education systems. The DDSF utilizes real time information extraction to improve the decision-making process. It combines data analytics and user centric design to provide insights to education administrators. By gathering data from sources like records, financial reports and social media analytics the DDSF uses natural language processing (NLP) and machine learning (ML) algorithms to extract and interpret real time information. This information is then organized in a manner for analysis. The framework is adaptable allowing for the inclusion of emerging data streams ensuring relevance and usefulness. To evaluate the effectiveness of the DDSF we conducted a pilot study at a university. The results indicated improvements in meeting student needs optimizing resource allocation and enhancing operational efficiency. The framework also enables policy development by foreseeing challenges and identifying opportunities, within the education sector. The DDSF brings about a way of managing education by providing a strong platform for institutions to succeed in a competitive and constantly evolving environment. It emphasizes the significance of extracting real time information to guide planning and operational excellence. Future studies will concentrate on expanding the framework, for use, across institutions and incorporating analytics to improve foresight abilities.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,553
10.1145/3645092,0,0,0,0,0,320,"Malhotra, Ruchika; Bansal, Anjali; Kessentini, Marouane",A Systematic Literature Review on Maintenance of Software Containers,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3645092,2024-04,,,,,,,"Nowadays, cloud computing is gaining tremendous attention to deliver information via the internet. Virtualization plays a major role in cloud computing as it deploys multiple virtual machines on the same physical machine and thus results in improving resource utilization. Hypervisor-based virtualization and containerization are two commonly used approaches in operating system virtualization. In this article, we provide a systematic literature review on various phases in maintenance of containers including container image detection, container scheduling, container security measures, and performance evaluation of containers. We have selected 145 primary studies out of which 24% of studies are related to container performance evaluation, 42% of studies are related to container scheduling techniques, 22% of studies are related to container security measures, and 12% of studies are related to container image detection process. A few studies are related to container image detection process and evaluation of container security measures. Resource utilization is the most considered performance objective in almost all container scheduling techniques. We conclude that there is a need to introduce new tagging approaches, smell detection approaches, and also new approaches to detect and resolve threat issues in containers so that we can maintain the security of containers.",,,,,,,,,,,,,,,,,,,,,,,,,,kubernetes; orchestration; performance evaluation; Containers; Docker; scheduling techniques; vulnerability,0.0,EC3,2901
10.1145/3650110,0,0,0,0,0,350,"Pal, Asmita; Desai, Keerthana; Chatterjee, Rahul; San Miguel, Joshua",Camouflage: Utility-Aware Obfuscation for Accurate Simulation of Sensitive Program Traces,ACM Trans. Archit. Code Optim.,,1544-3566,https://doi.org/10.1145/3650110,2024-05,,,,,,,"Trace-based simulation is a widely used methodology for system design exploration. It relies on realistic traces that represent a range of behaviors necessary to be evaluated, containing a lot of information about the application, its inputs and the underlying system on which it was generated. Consequently, generating traces from real-world executions risks leakage of sensitive information. To prevent this, traces can be obfuscated before release. However, this can undermine their ideal utility, i.e., how realistically a program behavior was captured. To address this, we propose Camouflage, a novel obfuscation framework, designed with awareness of the necessary architectural properties required to preserve trace utility, while ensuring secrecy of the inputs used to generate the trace. Focusing on memory access traces, our extensive evaluation on various benchmarks shows that camouflaged traces preserve the performance measurements of the original execution, with an average τ correlation of 0.66. We model input secrecy as an input indistinguishability problem and show that the average security loss is 7.8%, which is better than traces generated from the state-of-the-art.",,,,,,,,,,,,,,,,,,,,,,,,,,Performance characterization; Privacy of traces; Synthetic trace generation,,,257
10.1145/3652620.3688266,1,1,1,1,0,23,"Aissat, Sara; Beaulieu, Jonathan; Bordeleau, Francis; Gascon-Samson, Julien; Poirier, Erik A.; Motamedi, Ali",JuNo-OPS: A DevOps Framework for the Engineering of Digital Twins for Built Assets,Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems,979-8-4007-0622-6,,https://doi.org/10.1145/3652620.3688266,2024,MODELS Companion '24,,,,Association for Computing Machinery,"New York, NY, USA","Digital Twins (DT) constitute complex software systems that need to be continuously modified/updated to meet evolving user requirements and priorities, and support continual improvement. Because they aim at monitoring and improving different system aspects, their development requires the collaboration of people from different domains of expertise, e.g. the development of a DT for built assets may involve the collaboration of experts in software engineering, thermal comfort, air quality, and energy consumption, etc. Consequently, DTs need to be engineered to enable the fast and secure integration and deployment of new code, the systematic and iterative evolution of their components, and the independent development of different system aspects by those experts.In this paper, we present JuNo-OPS, a DevOps framework for the engineering of DTs for built assets. The framework is being developed, tested and validated in the the context of a multi-function room at École de Technologie Supérieure (ETS). We focus on two main facets of the DT software: the microservices architecture; and the DevOps infrastructure used to support the development, continuous integration, continuous delivery and the automation thereof. We also discuss challenges and next steps related to the development and evolution of the framework.",,,,,,,,,,,,,,,,,,,,,,,,,,digital twins; microservices; DevOps; IoT; CI/CD pipelines,,,650
10.1145/3652892.3700754,0,0,0,0,0,206,"Sethuraman, Manasvini; Sarma, Anirudh; Ghaisas, Netra; Bauskar, Adwait; Dhekne, Ashutosh; Sivasubramaniam, Anand; Ramachandran, Kishore",BASS: A Resource Orchestrator to Account for Vagaries in Network Conditions in Community Wi-Fi Mesh,Proceedings of the 25th International Middleware Conference,979-8-4007-0623-3,,https://doi.org/10.1145/3652892.3700754,2024,Middleware '24,,,,Association for Computing Machinery,"New York, NY, USA","We investigate the issue of deploying applications on a set of loosely coupled compute devices, connected through a wireless mesh, typical in community networks. Wireless mesh networks experience significant temporal and spatial variations in link bandwidth. When application components, modeled as a directed acyclic graph, need to be scheduled on such a mesh with bandwidth constraints (and variations), the problem of mapping components to specific compute nodes becomes an instance of bin packing with constraints of CPU, memory, and bandwidth limits within the mesh. To make the scheduling tractable, we propose BASS (Bandwidth Aware Scheduling System), and develop heuristics for scheduling, based on the directed graph topology of the application components. We evaluate BASS on an emulated mesh using bandwidth traces collected from an actual wireless testbed - CityLab. Detailed evaluations show that contemporary orchestration frameworks can plug in BASS to provide better end-to-end performance for the applications deployed on the mesh while reducing resource utilization.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,EC3,2625
10.1145/3652892.3700757,0,0,0,0,0,157,"Zhao, Yuxuan; Weng, Weikang; van Nieuwpoort, Rob; Uta, Alexandru","In Serverless, OS Scheduler Choice Costs Money: A Hybrid Scheduling Approach for Cheaper FaaS",Proceedings of the 25th International Middleware Conference,979-8-4007-0623-3,,https://doi.org/10.1145/3652892.3700757,2024,Middleware '24,,,,Association for Computing Machinery,"New York, NY, USA","In Function-as-a-Service (FaaS) serverless, large applications are split into short-lived stateless functions. Deploying functions is mutually profitable: users need not be concerned with resource management, while providers can keep their servers at high utilization rates running thousands of functions concurrently on a single machine. It is exactly this high concurrency that comes at a cost. The standard Linux Completely Fair Scheduler (CFS) switches often between tasks, which leads to prolonged execution times. We present evidence that relying on the default Linux CFS scheduler increases serverless workloads cost by up to 10×.In this article, we raise awareness and make a case for rethinking the OS-level scheduling in Linux for serverless workloads composed of many short-lived processes. To make serverless more affordable we introduce a hybrid two-level scheduling approach that relies on FaaS characteristics. Short-running functions are executed in FIFO fashion without preemption, while longer-running functions are passed to CFS after a certain time period. We show that tailor-made OS scheduling is able to significantly reduce user-facing costs without adding any provider-facing overhead.",,,,,,,,,,,,,,,,,,,,,,,,,,FaaS; serverless computing; cost for serverless; CPU scheduling,0.0,EC3,2582
10.1145/3654522.3654594,0,0,0,0,0,201,"Roh, Seol; Jeong, Hong-Ju; Boo, Hacksung; Huh, Eui-Nam",An Efficient Serverless-VM Switching Mechanism for Cloud Cost Optimization,Proceedings of the 2024 9th International Conference on Intelligent Information Technology,979-8-4007-1671-3,,https://doi.org/10.1145/3654522.3654594,2024,ICIIT '24,,,,Association for Computing Machinery,"New York, NY, USA","This research introduces an efficient switching mechanism between serverless architecture and traditional Virtual Machines within a cloud environment. Serverless computing, especially Function-as-a-Service, is gaining prominence due to its scalability, flexibility, and cost-effectiveness. This study presents a methodology that leverages the strengths of both serverless architecture and VMs to minimize operational costs and optimize performance. In this paper, to combines the benefits of serverless computing and VMs to enhance the utilization of cloud resources for switching mechanism. The algorithm promotes efficient resource allocation based on various metrics, including traffic load, cost, and execution time. Additionally, we employ a PID controller for facilitating seamless transitions between serverless and VM environments. Through in-depth analysis using large-scale data processing scenarios, we demonstrate how adopting a serverless architecture can lead to cost savings and improved resource utilization.",,,,,,,,,,,,,,,,,,,,,,,,,,Cost effectiveness; Serverless; Switchable Event Manager,0.0,EC3,1776
10.1145/3656405,0,0,0,0,0,405,"Ball, Thomas; de Halleux, Peli; Devine, James; Hodges, Steve; Moskal, Michał",Jacdac: Service-Based Prototyping of Embedded Systems,Proc. ACM Program. Lang.,,,https://doi.org/10.1145/3656405,2024-06,,,,,,,"The traditional approach to programming embedded systems is monolithic: firmware on a microcontroller contains both application code and the drivers needed to communicate with sensors and actuators, using low-level protocols such as I2C, SPI, and RS232. In comparison, software development for the cloud has moved to a service-based development and operation paradigm: a service provides a discrete unit of functionality that can be accessed remotely by an application, or other service, but is independently managed and updated. We propose, design, implement, and evaluate a service-based approach to prototyping embedded systems called Jacdac. Jacdac defines a service specification language, designed especially for embedded systems, along with a host of specifications for a variety of sensors and actuators. With Jacdac, each sensor/actuator in a system is paired with a low-cost microcontroller that advertises the services that represent the functionality of the underlying hardware over an efficient and low-cost single-wire bus protocol. A separate microcontroller executes the user's application program, which is a client of the Jacdac services on the bus. Our evaluation shows that Jacdac supports a service-based abstraction for sensors/actuators at low cost and reasonable performance, with many benefits for prototyping: ease of use via the automated discovery of devices and their capabilities, substitution of same-service devices for each other, as well as high-level programming, monitoring, and debugging. We also report on the experience of bringing Jacdac to commercial availability via third-party manufacturers.",,,,,,,,,,,,,,,,,,,,,,,,,,embedded systems; microcontrollers; plug-and-play; services,,,492
10.1145/3656766.3656853,0,0,0,0,0,38,"Chen, Li; Wang, Hai; Hou, Fuyuan; Li, Biao; Zhang, Wangli",Research on the Security of Internet of Things Based on Microservices Techniques,"Proceedings of the 2023 3rd International Conference on Big Data, Artificial Intelligence and Risk Management",979-8-4007-1647-8,,https://doi.org/10.1145/3656766.3656853,2024,ICBAR '23,,,,Association for Computing Machinery,"New York, NY, USA","Internet of Things technology refers to connecting different types of physical devices, sensors, and other devices together through the Internet to achieve real-time data exchange, thereby achieving real-time monitoring and data analysis. With the continuous development of internet technology, the IoT technology is becoming an important component of future smart homes, smart cities, and Industry 4.0 fields. There are multiple communication protocols and data formats between IoT devices, and a large number of monolithic structures lead to issues such as insufficient scalability and flexibility, resulting in a large number of security vulnerabilities in IoT communication, threatening network security and data privacy. Microservices technology splits a single application into multiple small services that run in separate processes, using lightweight communication mechanisms between services. The interaction between microservices uses standard communication technology, which can enhance the scalability and security of IoT device applications. This article proposes a microservice based security access model that enables fine-grained access decisions, effectively reducing attacks related to IoT device connections and data security, and providing scalable, reconfigurable, and reusable security services. We analyzed the feasibility, reliability, and efficiency of the proposed scheme through theoretical analysis and experiments. The CPU and memory usage of microservices was tested, as well as microservices concurrency performance. The experimental results show that the proposed microservices can achieve corresponding functions and return device data. The microservice scheme designed in this article can meet normal access requests and security communication.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,EC3,1690
10.1145/3659677.3659734,0,0,0,0,0,32,"El Akhdar, Abir; Baidada, Chafik; Kartit, Ali",Adaptability of Microservices Architecture in IoT Systems : A Comprehensive Review,"Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security",979-8-4007-0929-6,,https://doi.org/10.1145/3659677.3659734,2024,NISS '24,,,,Association for Computing Machinery,"New York, NY, USA","Abstract. This paper investigates how microservices, a transformative paradigm in cloud and enterprise applications, can adapt to the challenges presented by the Internet of Things (IoT). Microservices architecture, identified by the disintegration of monolithic software into distributed and decoupled services, presents a solution that enhances scalability and maintainability. While traditionally top-down, microservices align with IoT's need for flexible, vendor-independent service combinations. Identifying challenges in traditional IoT architectures, this research addresses a gap in the lack of a comprehensive review on the integration of microservices. Objectives include analyzing characteristics, identifying benefits and challenges, exploring best practices, and evaluating adaptability in terms of flexibility, scalability, data management, security, privacy, performance, and efficiency. We aim, through this work, to add to the existing knowledge base on microservices and IoT systems and offer valuable perspectives for practitioners and researchers in the field.",,,,,,,,,,,,,,,,,,,,,,,,,,Microservices; Internet of Things; IoT; Software Architecture; MSA; Comprehensive Review,0.0,EC3,1654
10.1145/3659994.3660315,0,0,0,0,0,214,"Dazzi, Patrizio; Ferrucci, Luca; Danelutto, Marco; Tserpes, Konstantinos; Makris, Antonios; Theodoropoulos, Theodoros; Massa, Jacopo; Carlini, Emanuele; Mordacchini, Matteo",Urgent Edge Computing,Proceedings of the 4th Workshop on Flexible Resource and Application Management on the Edge,979-8-4007-0641-7,,https://doi.org/10.1145/3659994.3660315,2024,FRAME '24,,,,Association for Computing Machinery,"New York, NY, USA","This position paper introduces Urgent Edge Computing (UEC) as a paradigm shift addressing the evolving demands of time-sensitive applications in distributed edge environments, in time-critical scenarios. With a focus on ultra-low latency, availability, resource management, decentralization, self-organization, and robust security, UEC aims to facilitate operations in critical scenarios such as disaster response, environmental monitoring, and smart city management. This paper outlines and discusses the key requirements, challenges, and enablers along with a conceptual architecture. The paper also outlines the potential applications of Urgent Edge Computing.",,,,,,,,,,,,,,,,,,,,,,,,,,resource management; edge computing; decentralized computing; urgent computing,,,439
10.1145/3659995.3660037,0,0,0,0,0,135,"Ilager, Shashikant; Balouek, Daniel; Kaddour, Sidi Mohammed; Brandic, Ivona",Proteus: Towards Intent-driven Automated Resource Management Framework for Edge Sensor Nodes,Proceedings of the 14th Workshop on AI and Scientific Computing at Scale Using Flexible Computing Infrastructures,979-8-4007-0642-4,,https://doi.org/10.1145/3659995.3660037,2024,FlexScience'24,,,,Association for Computing Machinery,"New York, NY, USA","Edge computing provides critical resources for various latency-sensitive applications, including, safety-critical monitoring systems that process large volumes of data from sensors and IoT devices, employing machine learning pipelines for effective and reliable analysis. Such applications are deployed on specially designed Edge Sensor Nodes (ESNs) that possess various sensors and limited computing power and support multiple data analysis tasks. ESNs encounter unique operational challenges, including intermittent power supplies, limited connectivity, and dynamic application and resource requirements, which complicate runtime management. Conventional resource management platforms like Kubernetes and KubeEdge are unsuitable for the dynamic needs of ESNs due to their reliance on centralized control and expected stable conditions. To bridge this gap, our paper introduces a data-driven resource management framework tailored for the autonomous adaptation of ESNs to diverse application and infrastructure requirements. We propose an intent-based mechanism that aligns application requirements, such as end-to-end latency, with infrastructure goals like utilization levels. This mechanism translates high-level intents into actionable low-level configurations, balancing the competing demands of various applications and resources, thereby guiding us toward a more robust and efficient application management system. We have implemented a prototype system, evaluated it on an experimental testbed, and demonstrated that our approach performs better than static-only optimization approaches with minimal impact on application performance.",,,,,,,,,,,,,,,,,,,,,,,,,,edge computing; automated application management; environmental monitoring applications; intent-driven resource management; internet of things (IoT),0.0,EC3,1980
10.1145/3660319.3660330,0,0,0,0,0,153,"Poggiani, Leonardo; Puliafito, Carlo; Virdis, Antonio; Mingozzi, Enzo",Live Migration of Multi-Container Kubernetes Pods in Multi-Cluster Serverless Edge Systems,Proceedings of the 1st Workshop on Serverless at the Edge,979-8-4007-0647-9,,https://doi.org/10.1145/3660319.3660330,2024,SEATED '24,,,,Association for Computing Machinery,"New York, NY, USA","In the cloud-native landscape, serverless computing is establishing itself as a pillar for smart provisioning of modern microservice-based applications. Due to its resounding success, serverless is rapidly emerging as a powerful paradigm in edge computing as well, wherein computing resources are geographically distributed in proximity to users. In serverless edge systems, applications can be composed by two types of microservice instances. Remote-state instances are internally stateless and access a remote state, if needed. On the other hand, a local-state instance is dedicated to a user and locally retains state information. In this work, we focus on live migration of local-state instances across an edge system operated by Kubernetes, which is the de-facto standard for microservice automated orchestration. More specifically, we propose a solution able to migrate both single-container and multi-container instances between different Kubernetes clusters. Our approach leverages the Liqo open-source project to establish a peering relationship and transfer the instance state between the involved clusters. After presenting the design and proof-of-concept implementation of the proposed solution, we outline useful insights into how to deploy it in practice and describe the experiments carried out to validate our work. Our system represents a powerful Kubernetes extension that does not require any modification to its standard API.",,,,,,,,,,,,,,,,,,,,,,,,,,serverless; kubernetes; edge computing; function as a service; migration; multi-cluster,0.0,EC3,2052
10.1145/3660395.3660441,0,0,0,0,0,145,"Yuan, Miao; Wang, Wenshuo; Yang, Jian; Deng, Jinlong",Design of Simulation System for Typical Application Verification and Testing of Low-voltage Intelligent IoT Power Distribution Area Fusion Terminal,Proceedings of the 2023 3rd Guangdong-Hong Kong-Macao Greater Bay Area Artificial Intelligence and Big Data Forum,979-8-4007-1636-2,,https://doi.org/10.1145/3660395.3660441,2024,AIBDF '23,,,,Association for Computing Machinery,"New York, NY, USA","As the smallest unit for power supply and distribution management, the power distribution area directly faces the access of a large number of low-voltage users, with a huge scale. In particular, with the proposal of the goal of ""carbon peak, carbon neutrality"" and the gradual implementation of the construction of new power systems, a large number of distributed photovoltaic, electric vehicles, energy storage, and other new source loads are connected to the power distribution area in a large-scale manner. The demand for intelligent interaction between the distribution network and users is gradually emerging and growing. The construction of digitally intelligent Internet of Things power distribution area has become a consensus and an inevitable trend. In order to promote and implement the construction of digital intelligent IoT stations, it is necessary to carry out simulation construction of low-voltage intelligent IoT stations, simulate and build real application scenarios, in order to better carry out research and exploration verification of the application of key technologies such as intelligent IoT in distribution stations. Thus providing technical support and guidance for the implementation of the new power system construction at the low-voltage substation level. This article, based on project experience, focuses on the verification methods and environmental design ideas for three typical application functions of low-voltage topology identification, fault location analysis, and power quality analysis of the intelligent integration terminal in the power distribution area, in order to provide a reference for the simulation design of low-voltage intelligent IoT power distribution area.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,EC3,2128
10.1145/3663482,0,0,0,0,0,358,"Gilman, Ekaterina; Bugiotti, Francesca; Khalid, Ahmed; Mehmood, Hassan; Kostakos, Panos; Tuovinen, Lauri; Ylipulli, Johanna; Su, Xiang; Ferreira, Denzil",Addressing Data Challenges to Drive the Transformation of Smart Cities,ACM Trans. Intell. Syst. Technol.,,2157-6904,https://doi.org/10.1145/3663482,2024-11,,,,,,,"Cities serve as vital hubs of economic activity and knowledge generation and dissemination. As such, cities bear a significant responsibility to uphold environmental protection measures while promoting the welfare and living comfort of their residents. There are diverse views on the development of smart cities, from integrating Information and Communication Technologies into urban environments for better operational decisions to supporting sustainability, wealth, and comfort of people. However, for all these cases, data are the key ingredient and enabler for the vision and realization of smart cities. This article explores the challenges associated with smart city data. We start with gaining an understanding of the concept of a smart city, how to measure that the city is a smart one, and what architectures and platforms exist to develop one. Afterwards, we research the challenges associated with the data of the cities, including availability, heterogeneity, management, analysis, privacy, and security. Finally, we discuss ethical issues. This article aims to serve as a “one-stop shop” covering data-related issues of smart cities with references for diving deeper into particular topics of interest.",,,,,,,,,,,,,,,,,,,,,,,,,,machine learning; smart city; Big data; data analysis; urban computing,,,801
10.1145/3663741.3664789,0,0,0,0,1,24,"Ordonez, Carlos; Macyna, Wojciech; Bellatreche, Ladjel",Energy-Aware Analytics in the Cloud,Proceedings of the International Workshop on Big Data in Emergent Distributed Environments,979-8-4007-0679-0,,https://doi.org/10.1145/3663741.3664789,2024,BiDEDE '24,,,,Association for Computing Machinery,"New York, NY, USA","Big data is now mostly processed in the cloud and will keep growing, fed by databases and the Internet of Things (IoT: sensors, mobile devices, edge computing). On the other hand, AI is pushing computers and data analysis to limits we had not witnessed before. Analytics in the cloud is now a major fraction of energy consumption, among other less CPU-intensive tasks like web services. With this green computing motivation in mind, we present a survey of past research and a vision of big data analytics in the cloud. Energy consumption is difficult to minimize because it has conflicting correlated variables behind: high performance, money cost and pollution, We identify which software subsystems and hardware components have a higher impact on energy consumption, understanding how they can be tweaked or tuned to optimize energy consumption.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,EC3 but maybe interesting,100
10.1145/3664200,0,0,0,0,0,340,"Golpayegani, Fateneh; Chen, Nanxi; Afraz, Nima; Gyamfi, Eric; Malekjafarian, Abdollah; Schäfer, Dominik; Krupitzer, Christian",Adaptation in Edge Computing: A Review on Design Principles and Research Challenges,ACM Trans. Auton. Adapt. Syst.,,1556-4665,https://doi.org/10.1145/3664200,2024-09,,,,,,,"Edge computing places the computational services and resources closer to the user proximity, to reduce latency, and ensure the quality of service and experience. Low latency, context awareness and mobility support are the major contributors to edge-enabled smart systems. Such systems require handling new situations and change on the fly and ensuring the quality of service while only having access to constrained computation and communication resources and operating in mobile, dynamic and ever-changing environments. Hence, adaptation and self-organisation are crucial for such systems to maintain their performance, and operability while accommodating new changes in their environment.This article reviews the current literature in the field of adaptive edge computing systems. We use a widely accepted taxonomy, which describes the important aspects of adaptive behaviour implementation in computing systems. This taxonomy discusses aspects such as adaptation reasons, the various levels an adaptation strategy can be implemented, the time of reaction to a change, categories of adaptation technique and control of the adaptive behaviour. In this article, we discuss how these aspects are addressed in the literature and identify the open research challenges and future direction in adaptive edge computing systems.The results of our analysis show that most of the identified approaches target adaptation at the application level, and only a few focus on middleware, communication infrastructure and context. Adaptations that are required to address the changes in the context, changes caused by users or in the system itself are also less explored. Furthermore, most of the literature has opted for reactive adaptation, although proactive adaptation is essential to maintain the edge computing systems’ performance and interoperability by anticipating the required adaptations on the fly. Additionally, most approaches apply a centralised adaptation control, which does not perfectly fit the mostly decentralised/distributed edge computing settings.",,,,,,,,,,,,,,,,,,,,,,,,,,edge computing; Adaptation; edge-enabled; MAPE-loop,0.0,EC3,2307
10.1145/3664476.3670884,0,0,0,0,0,117,"Zuppelli, Marco; Guarascio, Massimo; Caviglione, Luca; Liguori, Angelica",No Country for Leaking Containers: Detecting Exfiltration of Secrets Through AI and Syscalls,"Proceedings of the 19th International Conference on Availability, Reliability and Security",979-8-4007-1718-5,,https://doi.org/10.1145/3664476.3670884,2024,ARES '24,,,,Association for Computing Machinery,"New York, NY, USA","Containers offer lightweight execution environments for implementing microservices or cloud-native applications. Owing to their ubiquitous diffusion jointly with the complex interplay of hardware, computing, and network resources, effectively enforcing container security is a difficult task. Specifically, runtime detection of threats poses many challenges since container images are often immutable, and many malware deploys obfuscation or elusive mechanisms. Therefore, in this work, we propose a deep-learning-based approach for identifying the presence of two containers colluding to covertly leak secret information. In more detail, we consider a threat actor trying to exfiltrate a 4,096-bit private TLS key via five different covert channels. To decide whether containers are colluding for leaking data, the deep learning model is fed with statistical indicators of the syscalls, which are built starting from simple counters. Results indicate the effectiveness of our approach, even if some adjustments are needed to reduce the number of false positives.",,,,,,,,,,,,,,,,,,,,,,,,,,Container Leakages; Deep Learning; Security,,,145
10.1145/3664476.3670942,0,0,0,0,0,162,"Delaitre, Sabine; Pulgar Gutiérrez, José Maria",Vulnerability detection tool in source code by building and leveraging semantic code graph.,"Proceedings of the 19th International Conference on Availability, Reliability and Security",979-8-4007-1718-5,,https://doi.org/10.1145/3664476.3670942,2024,ARES '24,,,,Association for Computing Machinery,"New York, NY, USA","The paper presents different vulnerability detection tools to ensure the security of software applications and container environments from a preventive and holistic approach. The solution aims to improve the quality and security of software by leveraging knowledge graph technology for a more accurate and comprehensive detection process of vulnerabilities. The ambition is to detect the vulnerabilities in the whole software supply chain and to support developers holding security as a key component over the Software Development life-cycle. We design reliable tools by building a semantic graph-based abstraction of the code from the compiler state, and we reach high accuracy by developing different static code analyzers optimizing the detection of software vulnerabilities in the source code and dependencies. In this paper, we will introduce the cybersecurity suite composed of different vulnerability detection tools to promote developer autonomy and security automation over the software supply chain. The main tool (DocSpot) detects vulnerabilities in the application source code and leverages knowledge graph technology. The second tool (DocDocker) scans for vulnerabilities in containers, and the third one (SirDocker) detects orchestration vulnerabilities, e.g. related to configuration, recommends secure best practices, and supports secure management of the containers and container images. The main contributions to the field of security automation are detailed and the first experiments and results of the tools are exposed. Finally, we describe the contributions to improving security in software and IoT applications.",,,,,,,,,,,,,,,,,,,,,,,,,,Application Security Testing; Automation; DevOps framework; Knowledge graph; Preventive secure systems; Vulnerability detection in Code,0.0,EC3,2749
10.1145/3664925,0,0,0,0,0,314,"Min, Dongmoon; Byun, Ilkwon; Lee, Gyu-Hyeon; Kim, Jangwoo",CoolDC: A Cost-Effective Immersion-Cooled Datacenter with Workload-Aware Temperature Scaling,ACM Trans. Archit. Code Optim.,,1544-3566,https://doi.org/10.1145/3664925,2024-09,,,,,,,"For datacenter architects, it is the most important goal to minimize the datacenter’s total cost of ownership for the target performance (i.e., TCO/performance). As the major component of a datacenter is a server farm, the most effective way of reducing TCO/performance is to improve the server’s performance and power efficiency. To achieve the goal, we claim that it is highly promising to reduce each server’s temperature to its most cost-effective point (or temperature scaling).In this article, we propose CoolDC, a novel and immediately applicable low-temperature cooling method to minimize the datacenter’s TCO. The key idea is to find and apply the most cost-effective sub-freezing temperature to target servers and workloads. For that purpose, we first apply the immersion cooling method to the entire servers to maintain a stable low temperature with little extra cooling and maintenance costs. Second, we define the TCO-optimal temperature for datacenter operation (e.g., 248K 273K (-25℃ 0℃)) by carefully estimating all the costs and benefits at low temperatures. Finally, we propose CoolDC, our immersion-cooling datacenter architecture to run every workload at its own TCO-optimal temperature. By incorporating our low-temperature workload-aware temperature scaling, CoolDC achieves 12.7% and 13.4% lower TCO/performance than the conventional air-cooled and immersion-cooled datacenters, respectively, without any modification to existing computers.",,,,,,,,,,,,,,,,,,,,,,,,,,Datacenter architecture; low-temperature computing; temperature scaling,,,400
10.1145/3665348.3665413,0,0,0,0,0,196,"Zhang, Xuyang; Ni, Li; Zhu, Hang; Shen, Jialu; Wang, Lei",Research on intelligent operation and maintenance system of distributed photovoltaic power station based on Internet of Things technology,Proceedings of the 2024 International Conference on Generative Artificial Intelligence and Information Security,979-8-4007-0956-2,,https://doi.org/10.1145/3665348.3665413,2024,GAIIS '24,,,,Association for Computing Machinery,"New York, NY, USA","With the rapid development of renewable energy, especially solar energy, distributed photovoltaic power plants have become a crucial component of energy transition. In order to improve the operational efficiency and reduce maintenance costs of photovoltaic power plants, this paper proposes an IoT-based intelligent operation and maintenance system for distributed photovoltaic power plants. The system integrates advanced sensor networks, cloud computing, big data analysis, and artificial intelligence technologies to achieve real-time monitoring, fault prediction, performance optimization, and automated maintenance decision-making for photovoltaic power plants. Through a series of experiments, this paper validates the effectiveness of the proposed system in improving the operational efficiency of photovoltaic power plants, ensuring stable system operation, and optimizing energy output. Furthermore, this research explores the application challenges and strategies of IoT technology in the operation and maintenance of photovoltaic power plants, providing new perspectives and methods for the intelligent management and maintenance of photovoltaic power plants.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,199
10.1145/3666015.3666018,0,0,0,0,0,44,"Morales, Jose; Antunes, Luiz; Earl, Patrick; Edman, Robert; Hamed, Jeffrey; Reynolds, Douglas; Maffey, Katherine R; Yankel, Joseph; Yasar, Hasan",Insights on Implementing a Metrics Baseline for Post-Deployment AI Container Monitoring,Proceedings of the 2024 International Conference on Software and Systems Processes,979-8-4007-0991-3,,https://doi.org/10.1145/3666015.3666018,2024,ICSSP '24,,,,Association for Computing Machinery,"New York, NY, USA","Post-deployment monitoring (PDM) occurs in the late stages of a DevSecOps (DSO) pipeline. Its role in DSO is critical in providing feedback loops on system performance leading to desirable changes achieving long-term system and application sustainment. Containers are the de-facto deployed artifacts in DSO for diverse forms of systems and applications including AI models. Long-term sustainment of containerized AI models requires appropriate metrics for the successful maintenance of optimal container and model computing performance and correct model inference. There is no agreed upon set of metrics that should always be present when monitoring a deployed containerized AI model. The current literature and practice can benefit from a standard baseline of metrics for long-term monitoring of containerized AI models focused on computing and inference. In this paper, we propose a candidate baseline of metrics for consideration as a standard across PDM for any containerized AI model. We present a proof-of-concept (PoC) that implements a baseline of metrics for the continuous monitoring of an operationally deployed containerized AI model. The baseline represents the minimal metrics required for any containerized model deployed and actively operating to ensure successful long-term monitoring and support of optimal operation and performance. The metrics focus on container operation, model operation, and model inference. This paper also details the raw data required for the metrics along with a PoC which demonstrates container engineering for their acquisition. The paper illustrates the baseline as a mix of dynamic metrics that are customized for each problem class (e.g., object detection, regression) and data modality together with static metrics that should be present for any containerized model. The paper further shows that a containerized AI model can be engineered to produce these metrics and describes the benefits of a standardized baseline of metrics to aid in the reduction of power consumption in the global digital enterprise.",,,,,,,,,,,,,,,,,,,,,,,,,,AI; Artificial Intelligence; Container; Continuous Deployment; Deployment; DevOps; DevSecOps.; Machine Learning; Metrics; ML; Performance,0.0,EC3,2797
10.1145/3666025.3699323,0,0,0,0,0,105,"Xu, Chi; Qian, Rongsheng; Fang, Hao; Ma, Xiaoqiang; Atlas, William I.; Liu, Jiangchuan; Spoljaric, Mark A.",SALINA: Towards Sustainable Live Sonar Analytics in Wild Ecosystems,Proceedings of the 22nd ACM Conference on Embedded Networked Sensor Systems,979-8-4007-0697-4,,https://doi.org/10.1145/3666025.3699323,2024,SenSys '24,,,,Association for Computing Machinery,"New York, NY, USA","Sonar radar captures visual representations of underwater objects and structures using sound wave reflections, making it essential for exploration, mapping, and continuous surveillance in wild ecosystems. Real-time analysis of sonar data is crucial for time-sensitive applications, including environmental anomaly detection and in-season fishery management, where rapid decision-making is needed. However, the lack of both relevant datasets andpre-trained DNN models, coupled with resource limitations in wild environments, hinders the effective deployment and continuous operation of live sonar analytics.We present SALINA, a sustainable live sonar analytics system designed to address these challenges. SALINA enables real-time processing of acoustic sonar data with spatial and temporal adaptations, and features energy-efficient operation through a robust energy management module. Deployed for six months at two inland rivers in British Columbia, Canada, SALINA provided continuous 24/7 underwater monitoring, supporting fishery stewardship and wildlife restoration efforts. Through extensive real-world testing, SALINA demonstrated an up to 9.5% improvement in average precision and a 10.1% increase in tracking metrics. The energy management module successfully handled extreme weather, preventing outages and reducing contingency costs. These results offer valuable insights for long-term deployment of acoustic data systems in the wild.",,,,,,,,,,,,,,,,,,,,,,,,,,edge computing; sustainability; edge-cloud collaboration; internet of things; live analytics; sensing; solar power; sonar radar,,,846
10.1145/3672461,0,0,0,0,0,313,"Ghammam, Anwar; Khalsi, Rania; Kessentini, Marouane; Hassan, Foyzul",Efficient Management of Containers for Software Defined Vehicles,ACM Trans. Softw. Eng. Methodol.,,1049-331X,https://doi.org/10.1145/3672461,2024-11,,,,,,,"Containerization technology, such as Docker, is gaining in popularity in newly established software-defined vehicle architectures (SDVA). However, executing those containers can quickly become computationally expensive in constrained environments, given the limited CPU, memory, and energy resources in the Electric Control Units (ECU) of SDVA. Consequently, the efficient management of these containers is crucial for enabling the on-demand usage of the applications in the vehicle based on the available resources while considering several constraints and priorities, including failure tolerance, security, safety, and comfort. In this article, we propose a dynamic software container management approach for constrained environments such as embedded devices/ECUs in SDVA within smart cars. To address the conflicting objectives and constraints within the vehicle, we design a novel search-based approach based on multi-objective optimization. This approach facilitates the allocation, movement, or suspension of containers between ECUs in the cluster. Collaborating with our industry partner, Ford Motor Company, we evaluate our approach using different real-world software-defined scenarios. These scenarios involve using heterogeneous clusters of ECU devices in vehicles based on real-world software containers and use-case studies from the automotive industry. The experimental results demonstrate that our scheduler outperforms existing scheduling algorithms, including the default Docker scheduler -Spread- commonly used in automotive applications. Our proposed scheduler exhibits superior performance in terms of energy and resource cost efficiency. Specifically, it achieves a 35% reduction in energy consumption in power-saving mode compared to the scheduler employed by Ford Motor Company. Additionally, our scheduler effectively distributes workload among the ECUs in the cluster, minimizing resource usage, and dynamically adjusts to the real-time requirements and constraints of the car environment. This work will serve as a fundamental building block in the automotive industry to efficiently manage software containers in smart vehicles, considering constraints and priorities in the real world.",,,,,,,,,,,,,,,,,,,,,,,,,,docker; Dynamic scheduling; management of software containers; many-objective optimization; software defined vehicles,0.0,EC3,2060
10.1145/3673277.3673327,0,0,0,0,0,152,"Wang, Rui; Zou, Xiaoying; Li, Yaxi; Li, Feng; Liu, Jiaoli; Wang, Rong",Research on Power Terminal Attack Detection Technology Based on ATT&amp;CK Multi-modal Perception,"Proceedings of the 2024 3rd International Conference on Cryptography, Network Security and Communication Technology",979-8-4007-1695-9,,https://doi.org/10.1145/3673277.3673327,2024,CNSCT '24,,,,Association for Computing Machinery,"New York, NY, USA","There are a large number of power terminals and complex environments, facing prominent attack risks, which are related to the safe operation of the entire power grid. This paper proposes a power terminal attack detection technology based on the ATT&amp;CK framework, which analyzes and detects attack behaviors through multi-modal perception. Collect attack-related data from various power terminals, combine it with the Bayesian framework to determine the attack technology, map the attack stages and realize the reconstruction of the attack path and the prediction of the attack target. Finally, the technology is applied through the data lake and microservice fusion architecture. This paper proposes a fusion method of attack data in electric power information network, solves the problem of correlation between the ATT&amp;CK framework and the attack process, and expands the attack behavior detection capability of multi-source data fusion.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,939
10.1145/3674734,1,1,1,0,1,309,"Antoniou, Georgia; Bartolini, Davide; Volos, Haris; Kleanthous, Marios; Wang, Zhe; Kalaitzidis, Kleovoulos; Rollet, Tom; Li, Ziwei; Mutlu, Onur; Sazeides, Yiannakis; Haj Yahya, Jawad",Agile C-states: A Core C-state Architecture for Latency Critical Applications Optimizing both Transition and Cold-Start Latency,ACM Trans. Archit. Code Optim.,,1544-3566,https://doi.org/10.1145/3674734,2024-11,,,,,,,"Latency-critical applications running in modern datacenters exhibit irregular request arrival patterns and are implemented using multiple services with strict latency requirements (30–250μs). These characteristics render existing energy-saving idle CPU sleep states ineffective due to the performance overhead caused by the state’s transition latency. Besides the state transition latency, another important contributor to the performance overhead of sleep states is the cold-start latency, or in other words, the time required to warm up the microarchitectural state (e.g., cache contents, branch predictor metadata) that is flushed or discarded when transitioning to a lower-power state. Both the transition latency and cold-start latency can be particularly detrimental to the performance of latency critical applications with short execution times. While prior work focuses on mitigating the effects of transition and cold-start latency by optimizing request scheduling, in this work we propose a redesign of the core C-state architecture for latency-critical applications. In particular, we introduce C6Awarm, a new Agile core C-state that drastically reduces the performance overhead caused by idle sleep state transition latency and cold-start latency while maintaining significant energy savings. C6Awarm achieves its goals by (1) implementing medium-grained power gating, (2) preserving the microarchitectural state of the core, and (3) keeping the clock generator and PLL active and locked. Our analysis for a set of microservices based on an Intel Skylake server shows that C6Awarm manages to reduce the energy consumption by up to 70% with limited performance degradation (at most 2%).",,,,,,,,,,,,,,,,,,,,,,,,,,microservices; datacenters; Power management; C-states; idle states,0.0,IC4,109
10.1145/3675094.3678482,0,0,0,0,0,198,"Stingl, Lukas; Knierim, Michael T.","OpenBCI Meets the Web: A Scalable, Customizable Platform for Open-Source EEG Data Collection",Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing,979-8-4007-1058-2,,https://doi.org/10.1145/3675094.3678482,2024,UbiComp '24,,,,Association for Computing Machinery,"New York, NY, USA","The traditional process of EEG-based BCI data collection is cum- bersome and expertise-dependent due to complex hardware in- strumentation, but also because data collection software typically comes as an on-premise solution with features tailored to trained experimenters. To overcome these limitations, we herein present a customizable platform for EEG data collection using OpenBCI hardware, that simplifies and accelerates the EEG setup process. The proposed system integrates various adaptations of the Open- BCI recording hardware, enabling easy customizability for versatile electrode configurations. The frontend, developed with Vue 3 and D3.js, provides real-time impedance checks and data visualization, while the backend, utilizing Apache, Node.js, and Flask, ensures efficient data transmission and storage. This platform addresses critical needs for cross-platform compatibility, customizability, effi- cient session management, and user-friendly interfaces, making it ideal for large-scale and field studies. The automated data manage- ment and real-time feedback enhance data quality and reliability, supporting various research applications.",,,,,,,,,,,,,,,,,,,,,,,,,,eeg; exg; headphones; open-ceegrids; openbci; wearable bci,,,208
10.1145/3677454.3677455,0,0,0,0,0,195,"Huang, Jin; Zeng, Zeng; Wu, Peng; Xu, Caiyun; Li, Xiuyi",Edge computing framework design for power intelligent IoT,"Proceedings of the 2024 International Conference on Advanced Robotics, Automation Engineering and Machine Learning",979-8-4007-1711-6,,https://doi.org/10.1145/3677454.3677455,2024,ARAEML '24,,,,Association for Computing Machinery,"New York, NY, USA","As the construction of the smart grid moves forward gradually, hundreds of millions of terminal devices will be linked to it. The central master station will be under tremendous pressure due to the enormous diverse daily incremental data gathering, computation, and transmission produced by these devices. With edge computing, a new computing paradigm that moves data storage and computation closer to the user, real-time and effective lightweight data processing is made possible, regional autonomy is realized, and communication and computation at the cloud master are less stressful. The edge computing framework may satisfy the requirements of the power grid in terms of precise sensing, unified IoT, edge intelligence, and open sharing since it is an open software platform that supports edge computing. In order to lay the groundwork for the development of edge intelligence in the power grid, we first analyze the demand for typical business scenarios related to power transmission, substation, and distribution. Next, we present the target positioning and design principles of the edge computing framework in the power grid. Finally, we design the edge computing framework's functional architecture based on meeting the technical requirements of the electric power Internet of things.",,,,,,,,,,,,,,,,,,,,,,,,,,Edge computing; Information interaction; Intelligent sensing; Power IoT,,,188
10.1145/3685651.3686698,0,0,0,0,0,130,"Zafeiropoulos, Anastasios; Filinis, Nikos; Spatharakis, Dimitrios; Dimolitsas, Ioannis; Fotopoulou, Eleni; Vassilakis, Constantinos; Papavassiliou, Symeon",Data Management and Exchange between a Meta-Orchestration Platform and Data Spaces,"Proceedings of the 4th Eclipse Security, AI, Architecture and Modelling Conference on Data Space",979-8-4007-0984-5,,https://doi.org/10.1145/3685651.3686698,2024,eSAAM '24,,,,Association for Computing Machinery,"New York, NY, USA","A transition is underway towards the development of orchestration systems for the distributed computing continuum, where microservices-based applications are deployed and managed over resources that span from the Internet of Things (IoT) to the edge to the cloud part of it. In parallel, distributed monitoring frameworks are deployed to manage the vast amount of data produced to assist orchestration actions, optimal deployment of Machine Learning (ML) workflows, and processing of data produced by the IoT infrastructure that are related to vertical industries. In the current work, we detail the data collection and management approach that we follow in the development of a novel meta-orchestration platform for processing and sharing such data. For the latter, the potential for exploitation of the produced data is analyzed along with the interlinkage of the meta-orchestration platform with emerging work in the domain of Data Spaces. Opportunities and limitations for such an interlinkage are discussed.",,,,,,,,,,,,,,,,,,,,,,,,,,Computing Continuum; Data Spaces; Distributed Application; Meta-Orchestration; Observability,0.0,EC3,2403
10.1145/3685694,0,0,0,0,0,322,"Karmakar, Prasenjit; Pradhan, Swadhin; Chakraborty, Sandip",Exploring Indoor Air Quality Dynamics in Developing Nations: A Perspective from India,ACM J. Comput. Sustain. Soc.,,,https://doi.org/10.1145/3685694,2024-09,,,,,,,"Indoor air pollution is a major issue in developing countries such as India and Bangladesh, exacerbated by factors such as traditional cooking methods, insufficient ventilation, and cramped living conditions, all of which elevate the risk of health issues such as lung infections and cardiovascular diseases. With the World Health Organization associating around 3.2 million annual deaths globally to household air pollution, the gravity of the problem is clear. Yet, extensive empirical studies exploring these unique patterns and indoor pollution’s extent are missing. To fill this gap, we carried out a 6-months long field study involving over 30 households, uncovering the complexity of indoor air pollution in developing countries, such as the longer lingering time of volatile organic compounds (VOCs) in the air or the significant influence of air circulation on the spatiotemporal distribution of pollutants. We introduced an innovative Internet of Things (IoT) air quality sensing platform, the Distributed Air QuaLiTy MONitor (DALTON), explicitly designed to meet the needs of these nations, considering factors such as cost, sensor type, accuracy, network connectivity, power, and usability. As a result of a multi-device deployment, the platform identifies pollution hot spots in low- and middle-income households in developing nations. It identifies best practices to minimize daily indoor pollution exposure. Our extensive qualitative survey estimates an overall system usability score of 2.04, indicating an efficient system for air quality monitoring.",,,,,,,,,,,,,,,,,,,,,,,,,,best practices; Indoor pollution; pollution dynamics,,,1089
10.1145/3685930,0,0,0,0,0,324,"Tian, Hao; Xu, Xiaolong; Wu, Hongyue; Zhao, Qingzhan; Dai, Jianguo; Khan, Maqbool",Cost-Efficient Deep Neural Network Placement in Edge Intelligence-Enabled Internet of Things,ACM Trans. Sen. Netw.,,1550-4859,https://doi.org/10.1145/3685930,2024-08,,,,,,,"Edge intelligence (EI) integrates edge computing and artificial intelligence empowering service providers to deploy deep neural networks (DNNs) on edge servers in proximity to users to provision intelligent applications (e.g., autonomous driving) for ubiquitous Internet of Things (IoT) in smart cities, which facilitates the quality of experience (QoE) of users and improves the processing and energy efficiency. However, considering DNN is typically computational-intensive and resource-hungry, conventional placement approaches ignore the influence of multi-dimensional resource requirements (processor, memory, etc.), which may degrade the real-time performance. Moreover, with the increasing scale of geo-distributed edge servers, centralized decision-making is still challenging to find the optimal strategies effectively. To overcome these shortcomings, in this paper we propose a game theoretic DNN placement approach in EI-enabled IoT. First, a DNN placement optimization problem is formulated to maximize system benefits, which is proven to be (mathcal Nmathcal P ) -hard and model the original problem as an exact potential game (EPG). Moreover, an EPG-based DNN model placement algorithm, named EPOL, is designed for edge servers to make sub-optimal strategies independently and theoretical analysis is possessed to guarantee the performance of EPOL. Finally, real-world dataset based experimental results corroborate the superiority and effectiveness of EPOL.",,,,,,,,,,,,,,,,,,,,,,,,,,Internet of Things; DNN placement; Edge intelligence; Game theory,0.0,EC3,2372
10.1145/3686161,0,0,0,0,0,383,"Ye, Peigen; Wang, Wenfeng; Mi, Bing; Chen, Kongyang",EdgeStreaming: Secure Computation Intelligence in Distributed Edge Networks for Streaming Analytics,ACM Trans. Multimedia Comput. Commun. Appl.,,1551-6857,https://doi.org/10.1145/3686161,2024-08,,,,,,,"In modern information systems, real-time streaming data are generated in various vertical application scenarios, such as industrial security cameras, household intelligent devices, mobile robots and among others. However, these low-end devices can hardly provide real-time and accurate data analysis functionalities due to their limited on-board performances. Traditional centralized server computing also suffers from its prolonged transmission latency, resulting in huge response time. To deal with this problem, this paper presents a novel distributed computation intelligent system with nearby edge devices, abbreviated as EdgeStreaming, to facilitate rapid and accurate analysis of streaming data. Firstly, we thoroughly explore the available edge devices surrounding the terminal to generate an internally interconnected edge network. This edge network real-time perceives and updates the internal resource status of each edge device, such as computational and storage resources. Dynamic allocation of external computational or storage demands can be made based on the current load of individual edge devices. Consequently, the streaming data perceived by external terminal devices can be transmitted in real-time to any edge gateway. The edge network employs a well-designed task scheduling strategy to partition and allocate streaming data processing demands to one or multiple edge devices. Additionally, it customizes computational requirements judiciously, for instance, by utilizing model compression to expedite computation speed. We deployed an edge network comprising multiple Raspberry Pis, NVIDIA Jetson Nano, and Jetson NVIDIA TX2 devices, successfully achieving real-time analysis and detection of video streaming data. We believe our work provides new technological support for the real-time processing of streaming data.",,,,,,,,,,,,,,,,,,,,,,,,,,Blockchain; Distributed Computing; Edge Computing; Streaming Data,0.0,EC3,2450
10.1145/3687472,0,0,0,0,0,384,"Barriga, Arturo; Barriga, José A.; Pérez-Toledano, Miguel A.; Clemente, Pedro J.",Model-Driven Development Towards Distributed Intelligent Systems,ACM Trans. Internet Technol.,,1533-5399,https://doi.org/10.1145/3687472,2024-11,,,,,,,"A Distributed Intelligent System (DIS) encompasses a set of intelligent subsystems and components that collaborate to perform tasks and solve problems. Given the advancements of paradigms such as the Internet of Things, along with the advancements of technologies such as Machine Learning and Digital Twins, DISs are on the rise. These systems are increasingly integrating components that perform intelligent functions, and these intelligent functions are increasingly heterogeneous and varied. Moreover, there is no standardized framework to help researchers and practitioners adequately address DISs. As a result, the complexity, interoperability issues, and development time and costs of these systems are growing. However, Model-Driven Development (MDD) can help to address these challenges by providing a Domain-Specific Language (DSL) for developing DISs. In this work, a DSL for the design, validation, generation, and deployment of DISs is proposed. Firstly, the proposed DSL captures in a metamodel the key and high-level abstract concepts of the distinct DISs documented in the literature. Then, it allows modeling of DISs conforming to this metamodel. Subsequently, the DSL enables formal validation of the modeled systems. Lastly, it allows the generation and deployment of all DISs into production. Therefore, the work undertaken in this communication provides a methodological, formal, and standardized approach to defining and developing DISs from a high level of abstraction. This work allows users to address DISs by facilitating agility, minimizing manual tasks, and reducing the number of defects introduced in their development. To illustrate the applicability of the proposed DSL, a real case study of an agricultural digital twin is presented.",,,,,,,,,,,,,,,,,,,,,,,,,,Internet of Things; model-driven development; digital twin; Distributed intelligent system; model-to- text transformation,,,258
10.1145/3689236.3696171,0,0,0,0,0,57,"Wang, Botao; Xu, Xu; Zhuo, Lin; Yuan, Xiangyu; Wang, Longzhi; Sun, Qixing; Fu, Wei; Teng, Kah Hou",A Method for Optimizing Elastic Scaling Resources of Private Cloud in Digital Grid,Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering,979-8-4007-1813-7,,https://doi.org/10.1145/3689236.3696171,2024,ICCSIE '24,,,,Association for Computing Machinery,"New York, NY, USA","Cloud computing improves server utilization and reduces the number of physical servers through virtualization technology, which directly reduces energy consumption and corresponding carbon emissions. The elastic scalability of cloud platforms allows resources to automatically expand or reduce according to actual needs, avoiding excessive resource allocation and waste. In order to further investigate the positive effects of cloud platform elastic scaling on resources in enterprise IT management. This paper summarizes the change rules of elastic scaling resources on cloud in large enterprises by sorting out the business operation characteristics and resource change rules, and classifies and analyzes the three demand scenarios of periodic changes, jitter changes, and stable changes of business resources. It proves through experimental methods that the containerized deployment and elastic scaling configuration methods of cloud platforms can effectively improve the operation efficiency of cloud resources. The application method has also been promoted and verified in many different regions across the country, further proving that it can effectively help enterprises improve resource efficiency and reduce operating costs, and provide a reference for large-scale application of elastic scaling configuration in large enterprises.",,,,,,,,,,,,,,,,,,,,,,,,,,Kubernetes; Cloud platform; Digital grid; Elastic scaling; Resource scheduling,0.0,EC3,2058
10.1145/3691338,0,0,0,0,0,334,"Zamanzadeh Darban, Zahra; Webb, Geoffrey I.; Pan, Shirui; Aggarwal, Charu; Salehi, Mahsa",Deep Learning for Time Series Anomaly Detection: A Survey,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3691338,2024-10,,,,,,,"Time series anomaly detection is important for a wide range of research fields and applications, including financial markets, economics, earth sciences, manufacturing, and healthcare. The presence of anomalies can indicate novel or unexpected events, such as production faults, system defects, and heart palpitations, and is therefore of particular interest. The large size and complexity of patterns in time series data have led researchers to develop specialised deep learning models for detecting anomalous patterns. This survey provides a structured and comprehensive overview of state-of-the-art deep learning for time series anomaly detection. It provides a taxonomy based on anomaly detection strategies and deep learning models. Aside from describing the basic anomaly detection techniques in each category, their advantages and limitations are also discussed. Furthermore, this study includes examples of deep anomaly detection in time series across various application domains in recent years. Finally, it summarises open issues in research and challenges faced while adopting deep anomaly detection models to time series data.",,,,,,,,,,,,,,,,,,,,,,,,,,Anomaly detection; deep learning; multivariate time series; outlier detection; time series; univariate time series,,,529
10.1145/3696348.3702013,0,0,0,0,0,54,"Fu, Silvery D.; Zhang, Hong; Teoh, Ryan; Priadka, Taras; Ratnasamy, Sylvia",Toward Data-Centric Service Composition,Proceedings of the 23rd ACM Workshop on Hot Topics in Networks,979-8-4007-1272-2,,https://doi.org/10.1145/3696348.3702013,2024,HotNets '24,,,,Association for Computing Machinery,"New York, NY, USA","Microservices are increasingly used in modern applications, leading to a growing need for effective service composition solutions. However, we argue that traditional API-centric composition mechanisms (e.g., RPC, REST, and Pub/Sub) hamper the modularity of microservices. These mechanisms introduce rigid code-level coupling, scatter composition logic, and hinder visibility into cross-service data exchanges. Ultimately, these limitations complicate the maintenance and evolution of microservice-based applications. In this paper, we propose a rethinking of service composition and present Knactor, a new data-centric composition framework to restore the modularity that microservices were intended to offer. Knactor decouples service composition from service development, allowing composition to be implemented as explicit data exchanges among multiple services. Our initial case study suggests that this approach not only simplifies service composition but also opens up opportunities for data-driven policies and optimizations.",,,,,,,,,,,,,,,,,,,,,,,,,,Microservices; Design Principles; Service Composition,0.0,EC3,2194
10.1145/3696405,0,0,0,0,0,339,"Ghosh, Ushasi; Chiejina, Azuka; Stephenson, Nathan; Shah, Vijay K; Shakkottai, Srinivas; Bharadia, Dinesh",SPARC: Spatio-Temporal Adaptive Resource Control for Multi-site Spectrum Management in NextG Cellular Networks,Proc. ACM Netw.,,,https://doi.org/10.1145/3696405,2024-11,,,,,,,"This work presents SPARC (Spatio-Temporal Adaptive Resource Control), a novel approach for multi-site spectrum management in NextG cellular networks. SPARC addresses the challenge of limited licensed spectrum in dynamic environments. We leverage the O-RAN architecture to develop a multi-timescale RAN Intelligent Controller (RIC) framework, featuring an xApp for near-real-time interference detection and localization, and a xApp for real-time intelligent resource allocation. By utilizing base stations as spectrum sensors, SPARC enables efficient and fine-grained dynamic resource allocation across multiple sites, enhancing signal-to-noise ratio (SNR) by up to 7dB, spectral efficiency by up to 15%, and overall system throughput by up to 20%. Comprehensive evaluations, including emulations and over-the-air experiments, demonstrate the significant performance gains achieved through SPARC, showcasing it as a promising solution for optimizing resource efficiency and network performance in NextG cellular networks.",,,,,,,,,,,,,,,,,,,,,,,,,,interference detection; ran intelligent control; traffic,,,761
10.1145/3698322.3698324,0,0,0,0,0,58,"Maranhão, João José; Guerra, Eduardo Martins",A Prompt Pattern Sequence Approach to Apply Generative AI in Assisting Software Architecture Decision-making,"Proceedings of the 29th European Conference on Pattern Languages of Programs, People, and Practices",979-8-4007-1683-6,,https://doi.org/10.1145/3698322.3698324,2024,EuroPLoP '24,,,,Association for Computing Machinery,"New York, NY, USA","This paper proposes an approach that employs generative AI, specifically GPT models, to enhance decision-making in software architecture through a sequence of prompt patterns. Five prompt patterns are introduced, each targeting software architects’ specific challenges when navigating complex design decisions. Through a structured and context-aware decision flow, we demonstrate how these patterns can mitigate risks, manage uncertainties, and optimize functional and non-functional requirements. The proposed approach is evaluated in two real-world scenarios and one fictional case, illustrating its practical application in optimizing operations and ensuring scalability, security, and performance. While AI demonstrates transformative potential in aiding architectural practices, we also highlight its limitations, emphasizing the importance of human oversight in validating technical assumptions and avoiding over-reliance on automated tools. This paper contributes to the ongoing dialogue on how AI can be integrated into software architecture to foster more efficient, informed, and context-sensitive decision-making processes for architects.",,,,,,,,,,,,,,,,,,,,,,,,,,Artificial Intelligence; AI Generative; Architecture Decision-Making; Design Patterns; Prompt Engineering; Prompt Patterns Sequence; Software Architecture,0.0,EC3,2207
10.1145/3698322.3698344,0,0,0,0,0,52,"Daniel, João; Wang, Xiaofeng; Guerra, Eduardo",Pattern Language for Leveraging Metadata to Reusability in APIs,"Proceedings of the 29th European Conference on Pattern Languages of Programs, People, and Practices",979-8-4007-1683-6,,https://doi.org/10.1145/3698322.3698344,2024,EuroPLoP '24,,,,Association for Computing Machinery,"New York, NY, USA","Nowadays, corporate systems are heterogeneous, with distributed pieces that collaborate to fulfill the requirements. Communication between autonomous components becomes a key aspect to maintain the quality of the architecture. As a consequence, remote APIs are in focus: the quality in their design might significantly impact the quality of the interacting components. We aim to assist practitioners with designing reusable APIs. We present 8 patterns: (i) “Flexibilize the Ingestion” that structures an API to provide seamless integration by accepting heterogeneous structures as input; (ii) “Enrich with Metadata” that adds a new layer containing metadata to the messages exchanged; (iii) “Agree on Meta Structure” that defines a common metadata structure among the communicating parts; (iv) “Plug the Processors In” that proposes decoupled but composable handlers for the data and metadata; then (v) “Embed Metadata”, (vi) “Configure Meta in Runtime” and (vii) “Configure Meta during Deploy” propose different settings for the placement and setting of metadata; and (viii) “Adapt the Ingestion” that includes an adapter dedicated to set the metadata for ingestion when it is impossible to change the original sender.",,,,,,,,,,,,,,,,,,,,,,,,,,API; Metadata; Reusability,,,205
10.1145/3698385.3699872,0,0,0,0,0,156,"Gao, Wenjing; Cao, Xingzhao; Zhou, Cheng; Wang, Jian; Song, Lianxin; Li, Jiawei",Exploratory Practice of SME Digital Transformation via Industrial Internet: A Case of Self-Developed APP,Proceedings of the First International Workshop on IoT Datasets for Multi-Modal Large Model,979-8-4007-1297-5,,https://doi.org/10.1145/3698385.3699872,2024,IOTMMIM '24,,,,Association for Computing Machinery,"New York, NY, USA","Small and medium-sized enterprises (SMEs) encounter various disadvantages in scale, policy, capital, technology, talent, and resources during the digital transformation journey. Consequently, issues such as ""No willingness to transform"", ""No courage to transform"", and ""No ability to transform"" pose significant challenges for them in implementing digital transformation. This paper takes a self-developed industrial Internet application as an example to deeply explore the path through which the industrial Internet empowers the digital transformation of SMEs. This application embraces a new digital transformation empowerment framework for SMEs that this paper proposes. This framework comprehensively presents the approaches to empower the digital transformation of SMEs from the aspects of empowerment strategy, service capability, collaboration scenarios, and technical architecture. Through cases in multiple comprehensive scenarios, this paper validates the effectiveness of this framework and the digital application developed based on it.",,,,,,,,,,,,,,,,,,,,,,,,,,Digital Transformation; Industrial Internet; Small and Medium Enterprises; SMEs,,,1289
10.1145/3699514,0,0,0,0,0,360,"Nickel, Matthias; Göhringer, Diana","A Survey on Architectures, Hardware Acceleration and Challenges for In-Network Computing",ACM Trans. Reconfigurable Technol. Syst.,,1936-7406,https://doi.org/10.1145/3699514,2024-12,,,,,,,"By moving data and computation away from the end user to more powerful servers in the cloud or to cloudlets at the edge, end user devices only need to compute locally for small amounts of data and when low latency is required. However, with the advent of 6G and Internet-of-Everything, the demand for more powerful networks continues to grow. The introduction of Software-Defined Networking and Network Function Virtualization has allowed us to rethink networks and use them for more than just routing data to servers. In addition, the use of more powerful network devices is bringing new life to the concept of active networks in the form of in-network computing. In-Network Computing provides the ability to move applications into the network and process data on programmable network devices as they are transmitted. In this work, we provide an overview of in-network computing and its enabling technologies. We take a look at the programmability and different hardware architectures for SmartNICs and switches, focusing primarily on accelerators such as FPGAs. We discuss the state of the art and challenges in this area, and look at CGRAs, a class of hardware accelerators that have not been widely discussed in this context.",,,,,,,,,,,,,,,,,,,,,,,,,,CGRA; FPGA; In-Network Computing; SoC; Software-Defined Networking,0.0,EC3,2286
10.1145/3699520,0,0,0,0,0,363,"Bellavista, Paolo; Bicocchi, Nicola; Fogli, Mattia; Giannelli, Carlo; Mamei, Marco; Picone, Marco",An Entanglement-Aware Middleware for Digital Twins,ACM Trans. Internet Things,,,https://doi.org/10.1145/3699520,2024-11,,,,,,,"The development of the Digital Twin (DT) approach is tilting research from initial approaches that aim at promoting early adoption to sophisticated attempts to develop, deploy, and maintain applications based on DTs. In this context, we propose a highly dynamic and distributed ecosystem where containerized DTs co-evolve with an orchestration middleware. DTs provide digitalized representations of the targeted physical systems, while the orchestration middleware monitors and re-configures the deployed DTs in light of application constraints, available resources, and the quality of cyber-physical entanglement. First, we lay out the reference scenario. Then, we discuss the limitations of current approaches and identify a set of requirements that shape both DTs and the orchestration middleware. Subsequently, we describe a blueprint architecture that meets those requirements. Finally, we report empirical evidence on both the feasibility and the effectiveness of a proof-of-concept implementation of the proposed ecosystem.",,,,,,,,,,,,,,,,,,,,,,,,,,digital twins; middleware; Cyber-physical systems; entanglement,,,267
10.1145/3699954,0,0,0,0,0,409,"Hannou, Fatma-Zohra; Lefrançois, Maxime; Jouvelot, Pierre; Charpenay, Victor; Zimmermann, Antoine",A Survey on IoT Programming Platforms: A Business-Domain Experts Perspective,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3699954,2024-12,,,,,,,"The vast growth and digitalization potential offered by the Internet of Things (IoT) is hindered by substantial barriers in accessibility, interoperability, and complexity, mainly affecting small organizations and non-technical entities. This survey article provides a detailed overview of the landscape of IoT programming platforms, focusing specifically on the development support they offer for varying end user profiles, ranging from developers with IoT expertise to business experts willing to take advantage of IoT solutions to automate their organization processes. The survey examines a range of IoT platforms, classified according to their programming approach between general-purpose programming solutions, model-driven programming, mashups, and end-user programming. Necessary IoT and programming backgrounds are described to empower non-technical readers with a comprehensive field summary. In addition, the article compares the features of the most representative platforms and provides decision insights and guidelines to support end users in selecting appropriate IoT platforms for their use cases. This work contributes to narrowing the knowledge gap between IoT specialists and end users, breaking accessibility barriers and further promoting the integration of IoT technologies in various domains.1",,,,,,,,,,,,,,,,,,,,,,,,,,Internet of Things; IoT platforms; IoT programming; smart agriculture; smart building,,,719
10.1145/3700144,0,0,0,0,0,404,"Åkesson, Alfred; Gehrmann, Christian; Hedin, Görel; Johnsson, Björn A.; Magnusson, Boris; Nordahl, Mattias; Ramezanian, Sara; Stankovski Wagner, Paul",A Trust Establishment and Key Management Architecture for Hospital-at-Home,ACM Trans. Comput. Healthcare,,,https://doi.org/10.1145/3700144,2025-01,,,,,,,"The landscape of healthcare is experiencing a digitalization shift, transferring many medical activities to the patients’ homes, a phenomenon commonly referred to as Hospital-at-Home. While Internet of Things (IoT) devices facilitate the building of such systems, there is a need for powerful middleware that encapsulates device-to-device communication and enables the construction of user-friendly, secure, and robust Hospital-at-Home systems. A key challenge for such middleware is to build a trustworthy and lightweight key management system allowing different devices in the system to exchange messages securely. In this article, we present a simple, easily manageable and scalable such architecture which, in addition, supports long-term data protection using post-quantum cryptographic primitives. Our proposed solution utilizes a Merkle tree to enable the IoT devices to establish trust between each other automatically, even in the absence of an Internet connection. We have implemented the architecture and present performance figures as well as a security analysis of our approach.",,,,,,,,,,,,,,,,,,,,,,,,,,Hospital-at-Home; IoT Middleware; Key Management; Post Quantum Cryptography; Trust Establishment,0.0,EC3,2926
10.1145/3700792,0,0,0,0,0,378,"Tang, Li; Hu, Haibo; Gabbouj, Moncef; Ye, Qingqing; Xiang, Yang; Li, Jin; Li, Lang",A Survey on Securing Image-Centric Edge Intelligence,ACM Trans. Multimedia Comput. Commun. Appl.,,1551-6857,https://doi.org/10.1145/3700792,2024-12,,,,,,,"Facing enormous data generated at the network edge, Edge Intelligence (EI) emerges as the fusion of Edge Computing and Artificial Intelligence, revolutionizing edge data processing and intelligent decision-making. Nonetheless, this emergent mode presents a complex array of security challenges, particularly prominent in image-centric applications due to the sheer volume of visual data and its direct connection to user privacy. These challenges include safeguarding model/image privacy and ensuring model integrity against various security threats, such as model poisoning. Essentially, those threats originate from data attacks, suggesting data protection as a promising solution. Although data protection measures are well-established in other domains, image-centric EI necessitates focused research. This survey examines the security issues inherent to image-centric EI and outlines the protection efforts, providing a comprehensive overview of the landscape. We begin by introducing EI, detailing its operational mechanics and associated security issues. We then explore the technologies facilitating security enhancement (e.g., differential privacy) and edge intelligence (e.g., compact networks and distributed learning frameworks). Next, we categorize security strategies by their application in data preparation, training, and inference, with a focus on image-based contexts. Despite these efforts on security, our investigation identifies research gaps. We also outline promising research directions to bridge these gaps, bolstering security frameworks in image-centric EI applications.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,938
10.1145/3700875,0,0,0,0,0,400,"Golec, Muhammed; Walia, Guneet Kaur; Kumar, Mohit; Cuadrado, Felix; Gill, Sukhpal Singh; Uhlig, Steve","Cold Start Latency in Serverless Computing: A Systematic Review, Taxonomy, and Future Directions",ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3700875,2024-11,,,,,,,"Recently, academics and the corporate sector have paid attention to serverless computing, which enables dynamic scalability and an economic model. In serverless computing, users only pay for the time they actually use resources, enabling zero scaling to optimise cost and resource utilisation. However, this approach also introduces the serverless cold start problem. Researchers have developed various solutions to address the cold start problem, yet it remains an unresolved research area. In this article, we propose a systematic literature review on cold start latency in serverless computing. Furthermore, we create a detailed taxonomy of approaches to cold start latency, which we use to investigate existing techniques for reducing the cold start time and frequency. We have classified the current studies on cold start latency into several categories such as caching and application-level optimisation-based solutions, as well as Artificial Intelligence/Machine Learning-based solutions. Moreover, we have analyzed the impact of cold start latency on quality of service, explored current cold start latency mitigation methods, datasets, and implementation platforms, and classified them into categories based on their common characteristics and features. Finally, we outline the open challenges and highlight the possible future directions.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; survey; Serverless computing; function as a service; cold start latency,0.0,EC3,2289
10.1145/3702191.3702193,0,0,0,0,0,185,"Gao, Jinming; Zhu, Pingfei; Xiong, Genxin; Shu, Yanfei",Research and Application of MSC Integrated Intelligent Platform for Software Development and Operation in Large-scale Software Engineering,Proceedings of the 2024 International Symposium on Integrated Circuit Design and Integrated Systems,979-8-4007-1822-9,,https://doi.org/10.1145/3702191.3702193,2025,ICDIS '24,,,,Association for Computing Machinery,"New York, NY, USA","With the deepening development of digital transformation, the power marketing business is showing diversified expansion and development trends, and the system architecture is shifting from chimney systems to platformization, ecologicalization, and centralization. To solve the problems of difficult collaboration, R&amp;D management, and unified technical architecture in large-scale software engineering organizations, this article proposes the design and construction of a large-scale enterprise digital project R&amp;D and production tool MSC based on a domestic cloud platform, which integrates business needs, product design, software development, and operation and maintenance management. MSC has shared R&amp;D capabilities and supports collaborative operations in multiple locations. MSC efficiently supports the full stack R&amp;D of various digital products, meets the real-time unified management requirements of a set of code and versions for the entire software network, and effectively solves the problem of same root and same origin from large-scale software engineering design to software development.",,,,,,,,,,,,,,,,,,,,,,,,,,msc; power system; quadrature scheduling; research and transportation integration,,,303
10.1145/3704436,0,0,0,0,0,312,"Soussi, Wissem; Gür, Gürkan; Stiller, Burkhard",Democratizing Container Live Migration for Enhanced Future Networks - A Survey,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3704436,2024-12,,,,,,,"Emerging cloud-centric networks span from edge clouds to large-scale datacenters with shared infrastructure among multiple tenants and applications with high availability, isolation, fault tolerance, security, and energy efficiency demands. Live migration (LiMi) plays an increasingly critical role in these environments by enabling seamless application mobility covering the edge-to-cloud continuum and maintaining these requirements. This paper presents a comprehensive survey of recent advancements that democratize LiMi, making it more applicable to a broader range of scenarios and network environments both for virtual machines (VMs) and containers, and analyzes LiMi’s technical underpinnings and optimization techniques. It also delves into the issue of connections handover, presenting a taxonomy to categorize methods of traffic redirection synthesized from the existing literature. Finally, it identifies technical challenges and paves the way for future research directions in this key technology.",,,,,,,,,,,,,,,,,,,,,,,,,,edge-to-cloud continuum; Moving Target Defense (MTD); Service live migration; traffic session handover,,,716
10.1145/3708497,0,0,0,0,0,357,"Bayram, Firas; Ahmed, Bestoun S.",Towards Trustworthy Machine Learning in Production: An Overview of the Robustness in MLOps Approach,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3708497,2025-01,,,,,,,"Artificial intelligence (AI), and especially its sub-field of Machine Learning (ML), are impacting the daily lives of everyone with their ubiquitous applications. In recent years, AI researchers and practitioners have introduced principles and guidelines to build systems that make reliable and trustworthy decisions. From a practical perspective, conventional ML systems process historical data to extract the features that are consequently used to train ML models that perform the desired task. However, in practice, a fundamental challenge arises when the system needs to be operationalized and deployed to evolve and operate in real-life environments continuously. To address this challenge, Machine Learning Operations (MLOps) have emerged as a potential recipe for standardizing ML solutions in deployment. Although MLOps demonstrated great success in streamlining ML processes, thoroughly defining the specifications of robust MLOps approaches remains of great interest to researchers and practitioners. In this paper, we provide a comprehensive overview of the trustworthiness property of MLOps systems. Specifically, we highlight technical practices to achieve robust MLOps systems. In addition, we survey the existing research approaches that address the robustness aspects of ML systems in production. We also review the tools and software available to build MLOps systems and summarize their support to handle the robustness aspects. Finally, we present the open challenges and propose possible future directions and opportunities within this emerging field. The aim of this paper is to provide researchers and practitioners working on practical AI applications with a comprehensive view to adopt robust ML solutions in production environments.",,,,,,,,,,,,,,,,,,,,,,,,,,machine learning; Artificial intelligence; DataOps; MLOps systems; model performance; ModelOps; robustness; Trustworthy AI,,,786
10.1145/3708996,0,0,0,0,0,354,"Anderson, Patrick; Aranas, Erika; Assaf, Youssef; Behrendt, Raphael; Black, Richard; Caballero, Marco; Cameron, Pashmina; Canakci, Burcu; Chatzieleftheriou, Andromachi; Clarke, Rebekah; Clegg, James; Cletheroe, Daniel; Cooper, Bridgette; De Carvalho, Thales; Deegan, Tim; Donnelly, Austin; Drevinskas, Rokas; Gaunt, Alexander; Gkantsidis, Christos; Gomez Diaz, Ariel; Haller, Istvan; Hong, Freddie; Ilieva, Teodora; Joshi, Shashidhar; Joyce, Russell; Kunkel, William; Lara, David; Legtchenko, Sergey; Liu, Fanglin; Magalhaes, Bruno; Marzoev, Alana; McNett, Marvin; Mohan, Jayashree; Myrah, Michael; Nguyen, Truong; Nowozin, Sebastian; Ogus, Aaron; Overweg, Hiske; Rowstron, Antony; Sah, Maneesh; Sakakura, Masaaki; Scholtz, Peter; Schreiner, Nina; Sella, Omer; Smith, Adam; Stefanovici, Ioan; Sweeney, David; Thomsen, Benn; Verkes, Govert; Wainman, Phil; Westcott, Jonathan; Weston, Luke; Whittaker, Charles; Wilke Berenguer, Pablo; Williams, Hugh; Winkler, Thomas; Winzeck, Stefan",Project Silica: Towards Sustainable Cloud Archival Storage in Glass,ACM Trans. Storage,,1553-3077,https://doi.org/10.1145/3708996,2025-02,,,,,,,"Sustainable and cost-effective long-term storage remains an unsolved problem. The most widely used storage technologies today are magnetic (hard disk drives and tape). They use media that degrades over time and has a limited lifetime, which leads to inefficient, wasteful, and costly solutions for long-lived data. This article presents Silica: the first cloud storage system for archival data underpinned by quartz glass, an extremely resilient media that allows data to be left in situ indefinitely. The hardware and software of Silica have been co-designed and co-optimized from the media up to the service level with sustainability as a primary objective. The design follows a cloud-first, data-driven methodology underpinned by principles derived from analyzing the archival workload of a large public cloud service. Silica can support a wide range of archival storage workloads and ushers in a new era of sustainable, cost-effective storage.",,,,,,,,,,,,,,,,,,,,,,,,,,data center; sustainability; archival; cloud storage; cold storage; disaggregation; glass; Storage,,,784
10.1145/3711609.3711613,0,0,0,0,0,33,"Truong, Thai Minh; Huynh, Khang Quy; Ngo, Thao Thi Thu; Luu, Ca; Le, Bang Minh",The Microservice Based Architecture for IoT Systems in Agriculture and Aquaculture,Proceedings of the 2024 14th International Conference on Information Communication and Management,979-8-4007-1747-5,,https://doi.org/10.1145/3711609.3711613,2025,ICICM '24,,,,Association for Computing Machinery,"New York, NY, USA","The rapid advancement of Internet of Things (IoT) technologies presents significant opportunities to enhance agriculture and aquaculture practices. This paper proposed a software architecture designed based on microservices to address the complex and evolving requirements of contemporary agriculture and aquaculture systems locations. These systems demand a versatile platform capable of integrating a wide range of functionalities, including environmental monitoring, aquaculture management, and comprehensive record-keeping for crops and aquatic life. The proposed IoT system employs a microservice architecture that ensures modularity, scalability, and flexibility. This approach facilitates the seamless integration of new services and features, easy adaptation to existing systems, and efficient reuse of established components. The microservice-based architecture has been successfully implemented in the TC2 project at Can Tho University under the Japan International Cooperation Agency (JICA) initiative. The project is in remote areas through the deployment of a network of sensor and actuator nodes, supported by a robust backend and user-friendly Front End applications. This implementation demonstrated notable improvements in operational efficiency and resource management. By leveraging edge computing for real-time data processing and integrating cloud-based analytics, the system achieved reduced response times and minimized data transmission costs. The modularity of the microservices architecture also enabled easy scalability and adaptability to change environmental conditions. The success of this implementation highlights the potential of microservices-based IoT solutions to drive innovation and enhance practices in agriculture and aquaculture, laying a strong foundation for future advancements in these fields.",,,,,,,,,,,,,,,,,,,,,,,,,,Internet of Things (IoT); Microservices Architecture; Agriculture and Aquaculture,0.0,EC3,1846
10.1145/3712002,0,0,0,0,0,356,"Murillo, Juan M.; Garcia-Alonso, Jose; Moguel, Enrique; Barzen, Johanna; Leymann, Frank; Ali, Shaukat; Yue, Tao; Arcaini, Paolo; Pérez-Castillo, Ricardo; García Rodríguez de Guzmán, Ignacio; Piattini, Mario; Ruiz-Cortés, Antonio; Brogi, Antonio; Zhao, Jianjun; Miranskyy, Andriy; Wimmer, Manuel",Quantum Software Engineering: Roadmap and Challenges Ahead,ACM Trans. Softw. Eng. Methodol.,,1049-331X,https://doi.org/10.1145/3712002,2025-01,,,,,,,"As quantum computers advance, the complexity of the software they can execute increases as well. To ensure this software is efficient, maintainable, reusable, and cost-effective —key qualities of any industry-grade software— mature software engineering practices must be applied throughout its design, development, and operation. However, the significant differences between classical and quantum software make it challenging to directly apply classical software engineering methods to quantum systems. This challenge has led to the emergence of Quantum Software Engineering as a distinct field within the broader software engineering landscape. In this work, a group of active researchers analyse in depth the current state of quantum software engineering research. From this analysis, the key areas of quantum software engineering are identified and explored in order to determine the most relevant open challenges that should be addressed in the next years. These challenges help identify necessary breakthroughs and future research directions for advancing Quantum Software Engineering.",,,,,,,,,,,,,,,,,,,,,,,,,,open challenges; QSE; Quantum Computing; Quantum Software Engineering,,,1158
10.1145/3712004,0,0,0,0,0,332,"Casadei, Roberto; Aguzzi, Gianluca; Audrito, Giorgio; Damiani, Ferruccio; Pianini, Danilo; Scarso, Giordano; Torta, Gianluca; Viroli, Mirko",Software Engineering for Collective Cyber-Physical Ecosystems,ACM Trans. Softw. Eng. Methodol.,,1049-331X,https://doi.org/10.1145/3712004,2025-01,,,,,,,"Today's distributed and pervasive computing addresses large-scale cyber-physical ecosystems, characterised by dense and large networks of devices capable of computation, communication and interaction with the environment and people. While most research focuses on treating these systems as “composites” (i.e., heterogeneous functional complexes), recent developments in fields such as self-organising systems and swarm robotics have opened up a complementary perspective: treating systems as “collectives” (i.e., uniform, collaborative, and self-organising groups of entities). This article explores the motivations, state of the art, and implications of this “collective computing paradigm” in software engineering. In particular, it discusses its peculiar challenges, implied by characteristics like distribution, situatedness, large scale, and cooperative nature. These challenges outline significant directions for future research in software engineering, touching on aspects such as macro-programming, collective intelligence, self-adaptive middleware, learning/synthesis of collective behaviour, human involvement, safety and security in collective cyber-physical ecosystems.",,,,,,,,,,,,,,,,,,,,,,,,,,edge-cloud continuum; collective adaptive systems; cyber-physical ecosystems; distributed artificial intelligence; macro-programming; multi-agent systems; swarm intelligence,,,1138
10.1145/3712016,0,0,0,0,0,343,"Syed, Naeem; Anwar, Adnan; Baig, Zubair; Zeadally, Sherali","Artificial Intelligence as a Service (AIaaS) for Cloud, Fog and the Edge: State-of-the-Art Practices",ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3712016,2025-01,,,,,,,"Artificial Intelligence (AI) fosters enormous business opportunities that build and utilize private AI models. Implementing AI models at scale and ensuring cost-effective production of AI-based technologies through entirely in-house capabilities is a challenge. The success of the Infrastructure as a Service (IaaS) and Software as a Service (SaaS) Cloud Computing models can be leveraged to facilitate a cost-effective and scalable AI service paradigm, namely, ‘AI as a Service.’ We summarize current state-of-the-art solutions for AI-as-a-Service (AIaaS), and we discuss its prospects for growth and opportunities to advance the concept. To this end, we perform a thorough review of recent research on AI and various deployment strategies for emerging domains considering both technical as well as survey articles. Next, we identify various characteristics and capabilities that need to be met before an AIaaS model can be successfully designed and deployed. Based on this we present a general framework of an AIaaS architecture that integrates the required aaS characteristics with the capabilities of AI. We also compare various approaches for offering AIaaS to end users. Finally, we illustrate several real-world use cases for AIaaS models, followed by a discussion of some of the challenges that must be addressed to enable AIaaS adoption.",,,,,,,,,,,,,,,,,,,,,,,,,,IoT; AIaaS; Artificial intelligence; Artificial intelligence as a Service; Cloud Computing; Edge,,,566
10.1145/3715001,0,0,0,0,0,375,"Jarkas, Omar; Ko, Ryan; Dong, Naipeng; Mahmud, Redowan","A Container Security Survey: Exploits, Attacks, and Defenses",ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3715001,2025-02,,,,,,,"Containerization significantly boosts cloud computing efficiency by reducing resource consumption, enhancing scalability, and simplifying orchestration. Yet, these same features introduce notable security vulnerabilities due to the shared Linux kernel and reduced isolation compared to traditional virtual machines (VMs). This architecture, while resource-efficient, increases susceptibility to software vulnerabilities, exposing containers to potential breaches; a single kernel vulnerability could compromise all containers on the same host. Existing academic research on container security is often theoretical and lacks empirical data on the nature of attacks, exploits, and vulnerabilities. Studies that do look at vulnerabilities often focus on specific types. This lack of detailed data and breadth hampers the development of effective mitigation strategies and restricts insights into the inherent weaknesses of containers. To address these gaps, our study introduces a novel taxonomy integrating academic knowledge with industry insights and real-world vulnerabilities, creating a comprehensive and actionable framework for container security. We analyzed over 200 container-related vulnerabilities, categorizing them into 47 exploit types across 11 distinct attack vectors. This taxonomy not only advances theoretical understanding but also facilitates the identification of vulnerabilities and the implementation of effective mitigation strategies in containerized environments. Our approach enhances the resilience of these environments by mapping vulnerabilities to their corresponding exploits and mitigation strategies, especially in complex, multi-tenant cloud settings. By providing actionable insights, our taxonomy helps practitioners enhance container security. Our findings have identified critical areas for further investigation, thereby laying a comprehensive foundation for future research and improving container security in cloud environments.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; confidential computing; Containerization security; hardware-based security; vulnerabilities,,,211
10.1145/3715700,1,1,1,1,1,308,"Yang, Yihong; Zhou, Zhangbing; Shu, Lei; Zhou, Feng; Gaaloul, Walid; Khan, Arif Ali",Web 3.0-Enabled Microservice Re-Scheduling for Heterogenous Resources Co-Optimization in Metaverse-Integrated Edge Networks,ACM Trans. Auton. Adapt. Syst.,,1556-4665,https://doi.org/10.1145/3715700,2025-01,,,,,,,"The Web 3.0 and metaverse can empower intelligent application of Connected Autonomous Vehicles (CAVs). The adoption of edge computing can contribute to the low latency interaction between CAVs and the metaverse. Microservices are widely deployed on edge networks and the cloud nowadays. User's requests from CAVs are typically fulfilled through the composition of microservices, which may be hosted by contiguous edge nodes. Requests may differ on their required resources at runtime. Consequently, when requests are continuously injected into edge networks, the usage of heterogenous resources, including CPU, memory, and network bandwidth, may not be the same, or differ significantly, on certain edge nodes. This happens especially when burst requests are injected into the network to be satisfied concurrently. Therefore, the usage of heterogenous resources provided by edge nodes should be co-optimized through re-scheduling microservices. To address this challenge, this paper proposes a Web 3.0-enabled Microservice Re-Scheduling approach (called MRS), which is a migration-based mechanism integrating a placement strategy. Specifically, we formulate the microservice re-scheduling task as a multi-objective and multi-constraint optimization problem, which can be solved through a penalty signal-integrated framework and an improved pointer network. Extensive experiments are conducted on two real-world datasets. Evaluation results show that our MRS performs better than the counterparts with improvements of at least 7.7%, 2.4% and 2.2% in terms of network throughput, latency and energy consumption.",,,,,,,,,,,,,,,,,,,,,,,,,,Connected Autonomous Vehicles (CAVs); Heterogenous Resources Co-Optimization; Metaverse; Microservice Re-Scheduling; Web 3.0,0.0,IC4,30
10.1145/3719003,0,0,0,0,0,304,"Liu, Guozhi; Lin, Weiwei; Zhang, Haotong; Lin, Jianpeng; Peng, Shaoliang; Li, Keqin",Public Datasets for Cloud Computing: A Comprehensive Survey,ACM Comput. Surv.,,0360-0300,https://doi.org/10.1145/3719003,2025-03,,,,,,,"Publicly available datasets are vital to researchers because they permit the testing of new algorithms under a variety of conditions and ensure the verifiability and reproducibility of scientific experiments. In cloud computing research, there is a particular dependence on obtaining load traces and network traces from real cloud computing clusters, which are used for designing energy efficiency prediction, workload analysis, and anomaly detection solutions. To address the current lack of a comprehensive overview and thorough analysis of cloud computing datasets and to gain insight into their current status and future trends, in this article, we provide a comprehensive survey of existing publicly cloud computing datasets. First, we utilize a systematic mapping approach to analyze 968 scientific papers from 6 scientific databases, resulting in the retrieval of 42 datasets related to cloud computing. Second, we categorize these datasets based on 11 characteristics to assist researchers in quickly finding datasets suitable for their specific needs. Third, we provide detailed descriptions of each dataset to assist researchers in gaining a clearer understanding of their characteristics. Fourth, we select 12 mainstream datasets and conduct a comprehensive analysis and comparison of their characteristics. Finally, we discuss the weaknesses of existing datasets, identify challenges, provide recommendations for long-term dataset maintenance and updates, and outline directions for the future creation of new cloud computing datasets. Related resources are available at .",,,,,,,,,,,,,,,,,,,,,,,,,,anomaly detection; Cloud computing; datasets; energy efficiency; resource allocation; security; task scheduling; workload,,,680
10.14778/3547305.3547313,0,0,0,0,0,419,"Ali, Ahsan; Pinciroli, Riccardo; Yan, Feng; Smirni, Evgenia",Optimizing inference serving on serverless platforms,Proc. VLDB Endow.,,2150-8097,https://doi.org/10.14778/3547305.3547313,2022-06,,,,,,,"Serverless computing is gaining popularity for machine learning (ML) serving workload due to its autonomous resource scaling, easy to use and pay-per-use cost model. Existing serverless platforms work well for image-based ML inference, where requests are homogeneous in service demands. That said, recent advances in natural language processing could not fully benefit from existing serverless platforms as their requests are intrinsically heterogeneous.Batching requests for processing can significantly increase ML serving efficiency while reducing monetary cost, thanks to the pay-per-use pricing model adopted by serverless platforms. Yet, batching heterogeneous ML requests leads to additional computation overhead as small requests need to be ""padded"" to the same size as large requests within the same batch. Reaching effective batching decisions (i.e., which requests should be batched together and why) is non-trivial: the padding overhead coupled with the serverless auto-scaling forms a complex optimization problem.To address this, we develop Multi-Buffer Serving (MBS), a framework that optimizes the batching of heterogeneous ML inference serving requests to minimize their monetary cost while meeting their service level objectives (SLOs). The core of MBS is a performance and cost estimator driven by analytical models supercharged by a Bayesian optimizer. MBS is prototyped and evaluated on AWS using bursty workloads. Experimental results show that MBS preserves SLOs while outperforming the state-of-the-art by up to 8 x in terms of cost savings while minimizing the padding overhead by up to 37 x with 3 x less number of serverless function invocations.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,EC3,2773
10.14778/3611540.3611565,0,0,0,0,0,413,"Li, Ji You; Zhang, Jiachi; Zhou, Wenchao; Liu, Yuhang; Zhang, Shuai; Xue, Zhuoming; Xu, Ding; Fan, Hua; Zhou, Fangyuan; Li, Feifei",Eigen: End-to-End Resource Optimization for Large-Scale Databases on the Cloud,Proc. VLDB Endow.,,2150-8097,https://doi.org/10.14778/3611540.3611565,2023-08,,,,,,,"Increasingly, cloud database vendors host large-scale geographically distributed clusters to provide cloud database services. When managing the clusters, we observe that it is challenging to simultaneously maximizing the resource allocation ratio and resource availability. This problem becomes more severe in modern cloud database clusters, where resource allocations occur more frequently and on a greater scale. To improve the resource allocation ratio without hurting resource availability, we introduce Eigen, a large-scale cloud-native cluster management system for large-scale databases on the cloud. Based on a resource flow model, we propose a hierarchical resource management system and three resource optimization algorithms that enable end-to-end resource optimization. Furthermore, we demonstrate the system optimization that promotes user experience by reducing scheduling latencies and improving scheduling throughput. Eigen has been launched in a large-scale public-cloud production environment for 30+ months and served more than 30+ regions (100+ available zones) globally. Based on the evaluation of real-world clusters and simulated experiments, Eigen can improve the allocation ratio by over 27% (from 60% to 87.0%) on average, while the ratio of delayed resource provisions is under 0.1%.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,EC3,2833
10.4108/eai.25-10-2016.2266649,0,0,0,0,0,166,"Casalicchio, Emiliano",Autonomic Orchestration of Containers: Problem Definition and Research Challenges,Proceedings of the 10th EAI International Conference on Performance Evaluation Methodologies and Tools on 10th EAI International Conference on Performance Evaluation Methodologies and Tools,978-1-63190-141-6,,https://doi.org/10.4108/eai.25-10-2016.2266649,2017,VALUETOOLS'16,,,,"ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)","Brussels, BEL","Today, a new technology is going to change the way cloud platforms are designed and managed. This technology is called container. A container is a software environment where to install an application or application component and all the library dependencies, the binaries, and a basic configuration needed to run the application. The container technology promises to solve many cloud application issues, for example the application portability problem and the virtual machine performance overhead problem. The cloud industry is adopting the container technology both for internal usage and as commercial offering. However, we are far away from the maturity stage and there are still manyresearch challenges to be solved. One of them is container orchestration, that make it possible to define how to select, deploy, monitor, and dynamically control the configuration ofmulti-container packaged applications in the cloud. This paper presents the problem of autonomic container orchestration,analyze the state of the art solutions and discuss open challenges.",,,,,,,,,,,,,,,,,,,,,,,,,,cloud computing; docker; autonomic computing; service orchestration; container,0.0,EC3,3036
