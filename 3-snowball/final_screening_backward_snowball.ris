TY  - JOUR
AU  - Lee, J.
AU  - Lee, E.
T2  - Concerto: Dynamic Processor Scaling for Distributed Data Systems with Replication
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152893965&partnerID=40&md5=17b07cf96ec5014b135659d14b643c16
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
AU  - Yu, G.
AU  - Chen, P.
AU  - Zheng, Z.
T2  - Microscaler: Cost-effective Scaling for Microservice Applications in the Cloud with an Online Learning Approach
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152899663&partnerID=40&md5=3f976483d64a3921d4744594dfd7fd2b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
AU  - Nunes, J.P.K.S.
AU  - Bianchi, T.
AU  - Iwazaki, A.Y.
AU  - Yumi Nakagawa, E.
T2  - State of the Art on Microservices Autoscaling: An Overview
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152945254&partnerID=40&md5=0c94550016431536c44d5c576456f868
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
AU  - Koschel, A.
AU  - Bertram, M.
AU  - Bischof, R.
AU  - Schulze, K.
AU  - Schaaf, M.
AU  - Astrova, I.
T2  - A Look at Service Meshes
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152899368&partnerID=40&md5=ae873a97b27551e01770900057775d86
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
AU  - GhatrehSamani, D.
AU  - Denninnart, C.
AU  - Bacik, J.
AU  - Salehi, M.A.
T2  - The Art of CPU-Pinning: Evaluating and Improving the Performance of Virtualization and Containerization Platforms
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152902834&partnerID=40&md5=7577bbb4e9265cdbdda74e6606c8f509
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
AU  - Li, W.
AU  - Lemieux, Y.
AU  - Zhao, Z.
AU  - Han, Y.
T2  - Service Mesh: Challenges, State of the Art, and Future Research Opportunities
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152918160&partnerID=40&md5=eaba2e6d3392b7a7ab39dcbc3a0aa180
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
T2  - A Service Mesh for Kubernetes
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152940191&partnerID=40&md5=ccc5fb89a90c1ecdc06fe036da60dadc
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
AU  - Devey, K.
AU  - Hunt, D.
AU  - MacNamara, C.
T2  - Power Management - Technology Overview
DA  - 2025///
PY  - 2025
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140956119&partnerID=40&md5=f29610258cb4972de6128d3f6cf36de6
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3</p>
ER  - 

TY  - JOUR
T2  - Intel Corporation Enhanced SpeedStep® Technology for the Intel ® Pentium ® M Processor White Paper
DA  - 2004///
PY  - 2004
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152933943&partnerID=40&md5=b794b477ace6efb945c869cd9b736be8
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
AU  - Schöne, R.
AU  - Ilsche, T.
AU  - Bielert, M.
AU  - Gocht, A.
AU  - Hackenberg, D.
T2  - Energy Efficiency Features of the Intel Skylake-SP Processor and Their Impact on Performance
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152934125&partnerID=40&md5=d246c91fdd6f5ddb8faeaadd01681118
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
T2  - Microservices-framework-benchmark
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152944158&partnerID=40&md5=139f58eeb8869be94b790b024d25554e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152937873&partnerID=40&md5=1240468af2a3a798a680668ead25291a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Models for availability and power consumption evaluation of a private cloud with VMM rejuvenation enabled by VM Live Migration
AU  - Torquato, M.
AU  - Umesh, I.M.
AU  - Maciel, P.
T2  - Journal of Supercomputing
AB  - Software aging affects the availability of Virtual Machine Monitor (VMM), one of the main components of virtualized environments. Software aging causes internal software degradation due to bugs activation and accumulation during its execution. Software rejuvenation implemented through Virtual Machine (VM) Live Migration may be applied to cope with software aging effects on cloud computing system. Another relevant problem is the power consumption of using software rejuvenation techniques in the virtualized environments. This paper presents availability models based on Stochastic Petri Nets to evaluate two VM Live Migration approaches. These approaches are based on the redundancy schemes: Warm-Standby and Cold-Standby. In the Cold-Standby migration, the migration target machine is started only before the VM Live Migration. In the Warm-Standby Migration, the migration target machine runs along with the source migration machine. Results show that VM Live Migration causes a significant improvement in system availability. Scenarios with a heavy workload present an annual downtime reduction of 164 h. The availability comparison between two approaches reveals that the Cold-Standby approach has a slightly better result due to the decrease in the total number of VM Live Migrations. The power consumption results show that Cold-Standby approach is more efficient in power consumption. In all the observed scenarios, the costs savings by using Cold-Standby approach exceed 40%. The highlights of this paper are: i) a comprehensive model for availability evaluation of cloud computing with VMM rejuvenation through VM migration scheduling; ii) sensitivity analysis to define proper rejuvenation scheduling to maximize system availability and iii) power consumption comparison of the two adopted migration approaches. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.
DA  - 2018///
PY  - 2018
DO  - 10.1007/s11227-018-2485-4
VL  - 74
IS  - 9
SP  - 4817
EP  - 4841
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050183501&doi=10.1007%2fs11227-018-2485-4&partnerID=40&md5=8bfcb412dc10cf373fbf828d3180d49a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 40</p>
KW  - Availability
KW  - Cloud computing
KW  - Comprehensive model
KW  - Electric power utilization
KW  - Endocrinology
KW  - Energy efficiency
KW  - Green computing
KW  - Live migrations
KW  - Machine components
KW  - Network security
KW  - Petri nets
KW  - Power consumption
KW  - Program debugging
KW  - Scheduling
KW  - Sensitivity analysis
KW  - Software aging
KW  - Software aging and rejuvenation
KW  - Software rejuvenation
KW  - Stochastic models
KW  - Stochastic Petri Nets
KW  - Stochastic systems
KW  - System availability
KW  - Virtual machine
KW  - Virtual machine monitors
KW  - Virtual reality
KW  - Virtualized environment
KW  - VM Live Migration
ER  - 

TY  - JOUR
T2  - MicroK8s the Lightweight Kubernetes
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180914125&partnerID=40&md5=096fabfceda5a670575e9d0602c952aa
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2</p>
ER  - 

TY  - CONF
TI  - DRACeo: A smart simulator to deploy energy saving methods in microservices based networks
AU  - Valera, H.H.A.
AU  - Dalmau, M.
AU  - Roose, P.
AU  - Larracoechea, J.
AU  - Herzog, C.
T2  - Proceedings of the Workshop on Enabling Technologies: Infrastructure for Collaborative Enterprises, WETICE
AB  - Nowadays, many researchers work to identify microservices-based application deployments and scheduling solutions to save energy without decreasing functional QoS. In this work, we present DRACeo: A simulator that allows facing this challenge in a simple and efficient way, enabling its users to focus uniquely on microservices deployment/scheduling algorithms and its hardware/software repercussions (load vs. energy consumption) without worrying about low-level network configurations or operating system issues. DRACeo is able to deploy and schedule (move, duplicate, start/stop) microservices and their dependencies on various devices with software and hardware heterogeneity (CPU, bandwidth, RAM, Battery, etc.), taking into account various scheduling heuristics algorithms: centralized vs non-centralized. To do this, DRACeo allows deploying custom network topologies based on client-server schemes or p2p distributions, where devices can (dis)appear, turn on/off obeying random circumstances or user strategies. Finally, the simulator performs relevant operations such as QoS definition, resource monitoring, calculation of energy saved and consumption tracking (at device and network level). We tested some ideas based on our previous work 'Kaligreen' to demonstrate the effectiveness of DRACeo. © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/WETICE49692.2020.00026
VL  - 2020-September
SP  - 94
EP  - 99
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100714002&doi=10.1109%2fWETICE49692.2020.00026&partnerID=40&md5=6dde8162ca929d1cfeb833956e56d257
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 10</p>
KW  - Application deployment
KW  - Computer hardware
KW  - consumption
KW  - CPU
KW  - energy
KW  - Energy conservation
KW  - Energy saving methods
KW  - Energy utilization
KW  - hard disk
KW  - Hardware/software
KW  - microservices
KW  - middleware
KW  - network
KW  - Network configuration
KW  - Network topology
KW  - prototype
KW  - Resource monitoring
KW  - Scheduling
KW  - Scheduling heuristics
KW  - simulator
KW  - Simulators
KW  - Software and hardwares
ER  - 

TY  - CONF
TI  - High availability storage server with kubernetes
AU  - Khatami, A.A.
AU  - Purwanto, Y.
AU  - Ruriawan, M.F.
T2  - 2020 International Conference on Information Technology Systems and Innovation, ICITSI 2020 - Proceedings
AB  - High availability server (HAS) is a concept in which the server resources continuously available through any interference of the virtual and physical server. HAS is widely used for various purposes such as online trading services, business and Big Data needs. To optimize storage, it needs a special way to minimize costs incurred and be able to optimize the existing server. This research has implemented a system of high availability server using Kubernetes and distributed storage. The system was capable of meeting the resource requests even though one instance was interrupted. And if an instance was down, the services stored in the cluster could still be accessed through other instances. Based on the reliability and availability testing, the system was capable of meeting the high availability criteria, by reaching an uptime rate of 100%.  © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ICITSI50517.2020.9264928
SP  - 74
EP  - 78
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099602167&doi=10.1109%2fICITSI50517.2020.9264928&partnerID=40&md5=70774a9615802659a5569457ca139864
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 6</p>
KW  - Availability
KW  - Big data
KW  - Container
KW  - Digital storage
KW  - Distributed storage
KW  - High availability
KW  - High availability server
KW  - Kubernetes
KW  - Online trading
KW  - Reliability and availability
KW  - Resource request
KW  - Server resources
KW  - Storage servers
ER  - 

TY  - CONF
TI  - Reliability Modeling and Analysis of Hospital Information System Based on Microservices
AU  - Liu, Z.
AU  - Yu, H.
AU  - Fan, G.
AU  - Chen, L.
T2  - Proceedings of the 2021 IEEE International Conference on Progress in Informatics and Computing, PIC 2021
AB  - In recent years, modern hospital has a large scale, complex relationship, and hospital information system (HIS) due to rapid development of computer networks. But there is still a big gap for reliable use of clinical information and management system, especially in terms of fault prevention. The microservice architecture has great advantages for development and delivery of complex system. This paper proposes a novel microservice reliability model (MSRM) for HIS based on the formalism of Predicate Petri net (PrT net). First, microservice reliability requirement design is given and PrT net is used to model the reliability of microservice, and the corresponding syntax and semantics are also presented. Then the redundancy and circuit breaker is designed by using PrT net, a composition strategy is proposed and the reliability of microservices is analyzed qualitatively and quantitatively. Based on the constructed MSRM, the correctness of PrT net modeling and effectiveness of the strategies have been proven theoretically. Finally, a public healthcare case is used to explain modeling process, and verify the effectiveness of proposed method. Experimental results show that the strategy for HIS microservice reliability is effective.  © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/PIC53636.2021.9687027
SP  - 313
EP  - 318
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125802205&doi=10.1109%2fPIC53636.2021.9687027&partnerID=40&md5=a20e3378ecbb0a3ec75761621ef1d3f2
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3</p>
KW  - Clinical information system
KW  - Clinical management systems
KW  - Complex networks
KW  - Complex relationships
KW  - Electric circuit breakers
KW  - Fault prevention
KW  - Hospital Information System
KW  - Hospital information systems
KW  - Hospitals
KW  - Information management
KW  - Information systems
KW  - Information use
KW  - Large-scales
KW  - Microservice architecture
KW  - Microservice Architecture
KW  - Modelling and analysis
KW  - Network architecture
KW  - Petri Net
KW  - Petri nets
KW  - Redundancy
KW  - Reliability analysis
KW  - Reliability modelling
KW  - Semantics
KW  - Software reliability
KW  - Software Reliability
KW  - Software-Reliability
ER  - 

TY  - JOUR
TI  - Combining Markov and Semi-Markov Modelling for Assessing Availability and Cybersecurity of Cloud and IoT Systems
AU  - Kharchenko, V.
AU  - Ponochovnyi, Y.
AU  - Ivanchenko, O.
AU  - Fesenko, H.
AU  - Illiashenko, O.
T2  - Cryptography
AB  - This paper suggests a strategy (C5) for assessing cloud and IoT system (CIS) dependability, availability, and cybersecurity based on the continuous collection, comparison, choice, and combination of Markov and semi-Markov models (MMs and SMMs). It proposes the systematic building of an adequate and accurate model to evaluate CISs considering (1) continuous evolution of the model(s) together with systems induced by changes in the CIS or physical and cyber environment parameters; (2) the necessity of collecting data on faults, failures, vulnerabilities, cyber-attacks, privacy violations, and patches to obtain actual data for assessment; (3) renewing the model set based on analysis of CIS operation; (4) the possibility of choice and utilizing “off-the-shelf” models with understandable techniques for their development to assure improved accuracy of assessment; (5) renewing the models during application of CIS by time, component or mixed combining, taking into consideration different operation and maintenance events. The results obtained were algorithms for data collection and analysis, choice, and combining appropriate MM and SMMs and their different types, such as multi-fragmental and multiphase models, considering changing failure rates, cyber-attack parameters, periodical maintenance, etc. To provide and verify the approach, several private and public clouds and IoT systems were researched and discussed in the context of C5 and proposed algorithms. © 2022 by the authors.
DA  - 2022///
PY  - 2022
DO  - 10.3390/cryptography6030044
VL  - 6
IS  - 3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138621888&doi=10.3390%2fcryptography6030044&partnerID=40&md5=fa1924ba1c16710cc2317e22bc285df0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 25</p>
KW  - availability
KW  - cloud
KW  - cybersecurity
KW  - IoT
KW  - Markov modelling
KW  - semi-Markov modelling
ER  - 

TY  - JOUR
TI  - Model-Driven Impact Quantification of Energy Resource Redundancy and Server Rejuvenation on the Dependability of Medical Sensor Networks in Smart Hospitals
AU  - Silva, F.A.
AU  - Brito, C.
AU  - Araújo, G.
AU  - Fé, I.
AU  - Tyan, M.
AU  - Lee, J.-W.
AU  - Nguyen, T.A.
AU  - Maciel, P.R.M.
T2  - Sensors
AB  - The spread of the Coronavirus (COVID-19) pandemic across countries all over the world urges governments to revolutionize the traditional medical hospitals/centers to provide sustainable and trustworthy medical services to patients under the pressure of the huge overload on the computing systems of wireless sensor networks (WSNs) for medical monitoring as well as treatment services of medical professionals. Uncertain malfunctions in any part of the medical computing infrastructure, from its power system in a remote area to the local computing systems at a smart hospital, can cause critical failures in medical monitoring services, which could lead to a fatal loss of human life in the worst case. Therefore, early design in the medical computing infrastructure’s power and computing systems needs to carefully consider the dependability characteristics, including the reliability and availability of the WSNs in smart hospitals under an uncertain outage of any part of the energy resources or failures of computing servers, especially due to software aging. In that regard, we propose reliability and availability models adopting stochastic Petri net (SPN) to quantify the impact of energy resources and server rejuvenation on the dependability of medical sensor networks. Three different availability models (A, B, and C) are developed in accordance with various operational configurations of a smart hospital’s computing infrastructure to assimilate the impact of energy resource redundancy and server rejuvenation techniques for high availability. Moreover, a comprehensive sensitivity analysis is performed to investigate the components that impose the greatest impact on the system availability. The analysis results indicate different impacts of the considered configurations on the WSN’s operational availability in smart hospitals, particularly 99.40%, 99.53%, and 99.64% for the configurations A, B, and C, respectively. This result highlights the difference of 21 h of downtime per year when comparing the worst with the best case. This study can help leverage the early design of smart hospitals considering its wireless medical sensor networks’ dependability in quality of service to cope with overloading medical services in world-wide virus pandemics. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.
DA  - 2022///
PY  - 2022
DO  - 10.3390/s22041595
VL  - 22
IS  - 4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125551176&doi=10.3390%2fs22041595&partnerID=40&md5=24cbf38a6bb22995fa90ef4b436f35af
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 16</p>
KW  - Availability
KW  - Computing infrastructures
KW  - Computing system
KW  - COVID-19
KW  - Early designs
KW  - Energy resources
KW  - hospital
KW  - Hospitals
KW  - human
KW  - Humans
KW  - Internet of thing
KW  - Internet of things
KW  - Internet of Things (IoT)
KW  - Medical sensor networks
KW  - Medical services
KW  - Patient treatment
KW  - Petri nets
KW  - Power
KW  - Quality of service
KW  - rejuvenation
KW  - Rejuvenation
KW  - Reliability and availability
KW  - reproducibility
KW  - Reproducibility of Results
KW  - SARS-CoV-2
KW  - Sensitivity analysis
KW  - Smart hospital
KW  - Software testing
KW  - Stochastic models
KW  - Stochastic Petri net
KW  - Stochastic Petri Nets
KW  - Stochastic systems
KW  - Viruses
KW  - Wireless sensor networks
ER  - 

TY  - JOUR
TI  - Understanding the energy consumption of information and communications equipment: A case study of schools in Taiwan
AU  - Wang, J.C.
T2  - Energy
AB  - Information and communications technology (ICT) equipment is often used to help conserve energy, but such equipment also requires energy to operate. This study investigated the energy consumption of ICT in 28 secondary schools (SSs) and 98 elementary schools (ESs) in Taiwan. The average annual energy consumption of ICT equipment is 28,162 kWh and 20,888 kWh per SS and ES, respectively; per unit floor area, this is estimated to be 60.5 and 27.5 Wh/m2/year, respectively. ICT accounted for 7.3%–7.7% of the total energy consumption of each school. Projection, computer, and network equipment together account for more than 90% of the total electricity consumed by ICT equipment. The total energy consumption of ICT equipment is positively correlated with school type and size. The study established three multiple regression models to investigate the total energy consumption of ICT equipment per school, per class, and per student, and the models all exhibited statistical significance. The energy consumption is highly correlated with usage time and power dissipation. Equipment procurement and energy consumption management play key roles in the energy conservation. This study may serve as references for school administrators evaluating energy conservation effectiveness. © 2022 Elsevier Ltd
DA  - 2022///
PY  - 2022
DO  - 10.1016/j.energy.2022.123701
VL  - 249
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127003046&doi=10.1016%2fj.energy.2022.123701&partnerID=40&md5=ace624b945b90e3c1069e17296292f0f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 16</p>
KW  - building
KW  - Case-studies
KW  - Communication equipments
KW  - electricity
KW  - Elementary schools
KW  - Energy audit
KW  - energy conservation
KW  - Energy conservation
KW  - Energy consumption
KW  - Energy utilization
KW  - Energy-consumption
KW  - ICT
KW  - Information and communication
KW  - Information and Communication Technologies
KW  - information and communication technology
KW  - multiple regression
KW  - Regression analysis
KW  - Schools
KW  - Secondary schools
KW  - spatiotemporal analysis
KW  - sustainable development
KW  - Sustainable development
KW  - Taiwan
KW  - Technology equipment
KW  - Total energy
ER  - 

TY  - JOUR
TI  - A Self-Adaptive Approach for Managing Applications and Harnessing Renewable Energy for Sustainable Cloud Computing
AU  - Xu, M.
AU  - Toosi, A.N.
AU  - Buyya, R.
T2  - IEEE Transactions on Sustainable Computing
AB  - Rapid adoption of Cloud computing for hosting services and its success is primarily attributed to its attractive features such as elasticity, availability and pay-as-you-go pricing model. However, the huge amount of energy consumed by cloud data centers makes it to be one of the fastest growing sources of carbon emissions. Approaches for improving the energy efficiency include enhancing the resource utilization to reduce resource wastage and applying the renewable energy as the energy supply. This work aims to reduce the carbon footprint of the data centers by reducing the usage of brown energy and maximizing the usage of renewable energy. Taking advantage of microservices and renewable energy, we propose a self-adaptive approach for the resource management of interactive workloads and batch workloads. To ensure the quality of service of workloads, a brownout-based algorithm for interactive workloads and a deferring algorithm for batch workloads are proposed. We have implemented the proposed approach in a prototype system and evaluated it with web services under real traces. The results illustrate our approach can reduce the brown energy usage by 21 percent and improve the renewable energy usage by 10 percent.  © 2020 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/TSUSC.2020.3014943
VL  - 6
IS  - 4
SP  - 544
EP  - 558
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089367582&doi=10.1109%2fTSUSC.2020.3014943&partnerID=40&md5=2f433e3f8ac589be591826822a62d110
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 52</p>
KW  - Brownout
KW  - Carbon emissions
KW  - Carbon footprint
KW  - Cloud computing
KW  - Cloud data centers
KW  - Energy efficiency
KW  - Energy supplies
KW  - Environmental impact
KW  - Microservices
KW  - Prototype system
KW  - QoS
KW  - Quality of service
KW  - Renewable energies
KW  - Renewable energy efficiency
KW  - Renewable energy resources
KW  - Resource management
KW  - Resource utilizations
KW  - Self adaptive approach
KW  - Web services
ER  - 

TY  - JOUR
TI  - A model-based approach for planning blockchain service provisioning
AU  - Melo, C.
AU  - Araujo, J.
AU  - Dantas, J.
AU  - Pereira, P.
AU  - Maciel, P.
T2  - Computing
AB  - Recently, the blockchain-as-a-service paradigm arose, and many works have evaluated the performance issues related to it. However, not as much has been done regarding Dependability attributes, which have ever been a crucial topic on service provisioning, let it be either public or private infrastructures. This paper presents the blockchain provisioning planning tool (BPPT), a framework to evaluate the availability, deployment, and maintenance costs of Hyperledger Fabric-based applications over private computational infrastructures. The BPPT uses continuous time markov chain (CTMC) and reliability block diagram (RBD) models as an evaluation method of Hyperledger Fabric’s environments and determines distributed applications’ deployment feasibility and endorsement policies related to the platform. We also present case studies that may help those interested in paradigm changes to decide whether they should migrate from old to new technology. Some of the obtained results pointed-out that the AND endorsement, which requires that all nodes sign the authenticity of a transaction, has the highest deployment and maintenance costs, as well as the lowest availability values due to operational requirements, already an OR endorsement, which needs that at least one available node signs the transaction, provides the best relationship between the evaluated metrics. The KooN endorsement (that requires that K out of N nodes signs a transaction authenticity) is a more general model that supports analyzing midterm configurations, besides the two extreme configurations, that is, to AND and OR arrangements. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature.
DA  - 2022///
PY  - 2022
DO  - 10.1007/s00607-021-00956-4
VL  - 104
IS  - 2
SP  - 315
EP  - 337
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106450322&doi=10.1007%2fs00607-021-00956-4&partnerID=40&md5=4b59752a6a9500e06bb22ca9ccf03e6a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 13</p>
KW  - Authentication
KW  - Availability
KW  - Blockchain
KW  - Computational infrastructure
KW  - Continuous time Markov chain
KW  - Continuous time systems
KW  - Costs
KW  - Distributed applications
KW  - Fabric
KW  - Framework
KW  - Infrastructure as a service (IaaS)
KW  - Maintenance
KW  - Markov chains
KW  - Model based approach
KW  - Operational requirements
KW  - Performance issues
KW  - Reliability block diagrams
KW  - Service provisioning
ER  - 

TY  - JOUR
TI  - A Genetic Algorithm-Based Energy-Efficient Container Placement Strategy in CaaS
AU  - Zhang, R.
AU  - Chen, Y.
AU  - Dong, B.
AU  - Tian, F.
AU  - Zheng, Q.
T2  - IEEE Access
AB  - Container placement (CP) is a nontrivial problem in Container as a Service (CaaS). Many works in the literature solve it by using linear server energy-consumption models. However, the solutions of using a linear model makes different CPs indistinguishable with regard to energy consumption in a homogeneous host environment that has a same amount of active hosts. As such, these solutions are energy inefficient. In this paper, we demonstrate that an energy-saving gain can be achieved by optimizing the placement of containers under a nonlinear energy consumption model. Specifically, we leverage a strategy based on genetic algorithm (GA) to search the optimal solution. Unfortunately, the conventional GA incurs performance degradation when the virtual machine (VM) resource utilization is high. In order to solve this problem, we propose an improved genetic algorithm called IGA for efficiently searching the optimal CP solution by introducing two different exchange mutation operations and constructing a function as the control parameter to selectively control the usage of the two operations. Extensive experiments are carried out under different settings, and their results show that our strategy is better than the existing CP strategies, i.e., spread and binpack, on energy efficiency target. In addition, the introduced IGA is experimentally proved to be more effective compared with the First Fit, Particle Swarm Optimization (PSO) algorithm and conventional GA. Moreover, the results validate that our proposed strategy can search new CP solutions with better fitness and alleviate the performance degradation caused by the conventional GA when the VM resource utilization is high.  © 2013 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/ACCESS.2019.2937553
VL  - 7
SP  - 121360
EP  - 121373
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090845423&doi=10.1109%2fACCESS.2019.2937553&partnerID=40&md5=25bbbc9a62d74b7a5a06cb7c854d05f0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 39</p>
KW  - CaaS
KW  - container placement
KW  - Containers
KW  - Control parameters
KW  - Energy consumption model
KW  - Energy efficiency
KW  - Energy utilization
KW  - exchange mutation operation
KW  - genetic algorithm
KW  - Genetic algorithms
KW  - Green computing
KW  - Mutation operations
KW  - Optimal solutions
KW  - Particle swarm optimization (PSO)
KW  - Particle swarm optimization algorithm
KW  - Performance degradation
KW  - Placement strategy
KW  - Resource utilizations
KW  - Virtual machine
ER  - 

TY  - JOUR
TI  - Performability evaluation of switch-over Moving Target Defence mechanisms in a Software Defined Networking using stochastic reward nets
AU  - Nguyen, T.A.
AU  - Kim, M.
AU  - Lee, J.
AU  - Min, D.
AU  - Lee, J.-W.
AU  - Kim, D.
T2  - Journal of Network and Computer Applications
AB  - It is essential to comprehend different aspects of performability of a system adopting Moving Target Defence (MTD) techniques. A number of previous works showed significant progress on security effectiveness evaluation for MTD mechanisms. While, a lesser amount of studies considered the impact of running MTD mechanisms on system dependability metrics, exposing a critical missing on the comprehension of pros and cons of MTD mechanisms in terms of security and dependability. In this paper, we present comprehensive modelling and analysis of time-based switch-over MTD strategies complying with IP shuffling techniques deployed in a Software Defined Network (SDN) using stochastic reward net (SRN). To investigate the impact of MTD strategies along with system availability on service performance metrics, we propose performability SRN models for various switch-over MTD strategies. The modelled behaviours of the switch-over MTD strategies are based on the integration of service management policies (drop/accept) with time-based switch-over policies (zero-time, fixed-time, and variable-time waiting policies) Critical performability metrics are comprehensively evaluated, including (i) system availability, downtime minutes, and Capacity-oriented Availability (COA) of a service, (ii) service throughput, (iii) response time of a job, (iv) average utilization of a server, (v) number of lost jobs, and (vi) operational cost (power consumption and business profit loss). The analysis results reveal sophisticated operational system behaviours and the impact of MTD strategies on system performability metrics. This study can help design and plan the development and adoption of MTD strategies in practice regarding the trade-offs between security and performability assurance. © 2021 Elsevier Ltd
DA  - 2022///
PY  - 2022
DO  - 10.1016/j.jnca.2021.103267
VL  - 199
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124069551&doi=10.1016%2fj.jnca.2021.103267&partnerID=40&md5=80ab24eaaf028fc2bb7da607bd915790
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 11</p>
KW  - Availability
KW  - Defence mechanisms
KW  - Defense strategy
KW  - Defense techniques
KW  - Economic and social effects
KW  - Moving Target Defence
KW  - Moving target defense
KW  - Network security
KW  - Performability
KW  - Performability evaluation
KW  - Software Defined Network
KW  - Software defined networking
KW  - Software-defined networkings
KW  - Software-defined networks
KW  - Stochastic reward net
KW  - Stochastic reward nets
KW  - Stochastic systems
KW  - Time based
ER  - 

TY  - CONF
TI  - SmartWatts: Self-Calibrating Software-Defined Power Meter for Containers
AU  - Fieni, G.
AU  - Rouvoy, R.
AU  - Seinturier, L.
T2  - Proceedings - 20th IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing, CCGRID 2020
AB  - Fine-grained power monitoring of software activities becomes unavoidable to maximize the power usage efficiency of data centers. In particular, achieving an optimal scheduling of containers requires the deployment of software-defined power meters to go beyond the granularity of hardware power monitoring sensors, such as Power Distribution Units (PDU) or Intel's Running Average Power Limit (RAPL), to deliver power estimations of activities at the granularity of software containers. However, the definition of the underlying power models that estimate the power consumption remains a long and fragile process that is tightly coupled to the host machine.To overcome these limitations, this paper introduces SmartWatts: a lightweight power monitoring system that adopts online calibration to automatically adjust the CPU and DRAM power models in order to maximize the accuracy of runtime power estimations of containers. Unlike state-of-the-art techniques, SmartWatts does not require any a priori training phase or hardware equipment to configure the power models and can therefore be deployed on a wide range of machines including the latest power optimizations, at no cost. © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/CCGrid49817.2020.00-45
SP  - 479
EP  - 488
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089066048&doi=10.1109%2fCCGrid49817.2020.00-45&partnerID=40&md5=deb42170a0a6e9d2bdabd86291a95a19
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 43</p>
KW  - Average power limit
KW  - Cluster computing
KW  - Containers
KW  - Energy
KW  - Fine-grained power
KW  - Multitasking
KW  - On-line calibration
KW  - Optical parametric oscillators
KW  - Optimal scheduling
KW  - Power distribution units
KW  - Power estimations
KW  - Power model
KW  - Power Optimization
KW  - State-of-the-art techniques
ER  - 

TY  - CONF
TI  - Towards orchestration in the cloud-fog continuum
AU  - Merino, X.
AU  - Otero, C.
AU  - Nieves-Acaron, D.
AU  - Luchterhand, B.
T2  - Conference Proceedings - IEEE SOUTHEASTCON
AB  - The growth of the Internet-of-Things has led to a rise in the need of computing power, storage, and network resources. As more data are being generated at the edge of the networks, the cloud model that enabled the affordable, on-demand, lease of these resources is ill-fitted to handle the volume and variety of data traveling to the core of the cloud and back. Some applications further showcase the limitations of the cloud by requiring strict low-latency communication and location awareness. Fog computing has been proposed as a solution to these issues that stem from the cloud's centralization. The fog is an emerging computing paradigm, conceived as an extension to the cloud, that aims to facilitate the creation of scalable infrastructures in the vicinity of the end-user. By decentralizing resources, it promises to optimize bandwidth consumption at the core and edge of the network while reducing latency between the service and the end-user. In this paper, we identify the requirements needed to orchestrate loads in the Cloud-Fog continuum and propose an architecture, built on available, open-source, components, that orchestrates loads with consideration to their geographical needs. We provide several levels of features (DNS-like service discovery, service mesh, health checks, encryption-as-a-service, among others) available to the operator and evaluate their quality-of-service implications, with respect to network latency and bandwidth, when compared to a simple deployment baseline. © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/SoutheastCon45413.2021.9401822
VL  - 2021-March
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105021688&doi=10.1109%2fSoutheastCon45413.2021.9401822&partnerID=40&md5=72d794e2096ac67db4e11b6c554282c8
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 9</p>
KW  - Architecture
KW  - Bandwidth
KW  - Cloud computing
KW  - Cloud-computing
KW  - Computer architecture
KW  - Computing power
KW  - Cryptography
KW  - Digital storage
KW  - End-users
KW  - Fog
KW  - Fog computing
KW  - Internet of things
KW  - IoT
KW  - Latency
KW  - Network architecture
KW  - Orchestration
KW  - Power networks
KW  - Power resources
KW  - Power storage
KW  - Quality of service
KW  - Storage resources
ER  - 

TY  - JOUR
TI  - Dependability and Security Quantification of an Internet of Medical Things Infrastructure Based on Cloud-Fog-Edge Continuum for Healthcare Monitoring Using Hierarchical Models
AU  - Nguyen, T.A.
AU  - Min, D.
AU  - Choi, E.
AU  - Lee, J.-W.
T2  - IEEE Internet of Things Journal
AB  - Rising aggressive virus pandemics urge to conduct studies on dependability and security of modern computing systems to secure autonomous and continuous operations of healthcare systems. In that regard, we propose to quantify dependability and security measures of an Internet-of-Medical Things (IoMT) infrastructure relied on an integrated physical architecture of cloud/fog/edge (CFE) computing paradigms in this article. We propose a reliability/availability quantification methodology for the IoMT infrastructure using a hierarchical model of three levels: 1) fault tree (FT) of overall IoMT infrastructure consisting of CFE member systems; 2) FT of subsystems within CFE member systems; and 3) continuous-time Markov chain (CTMC) models of components/devices in the subsystems. We incorporate a number of failure modes for the underlying subsystems, including Mandel-bug related failures and non-Mandel bugs related failure, as well as failures due to cyber-security attacks on software subsystems. Five case-studies of configuration alternation and four operational scenarios of the IoMT infrastructure are considered to comprehend the dependability characteristics of the IoMT physical infrastructure. The metrics of interest include reliability over time, steady state availability (SSA), sensitivity of SSA wrt. selected mean time to failure-equivalent (MTTFeq) and mean time to recovery-equivalent (MTTReq), and sensitivity of SSA wrt. frequencies of cyber-security attacks on software subsystems. The analysis results help comprehend operational behaviors and properties of a typical IoMT infrastructure. The findings of this study can improve the design and implementation of real-world IoMT infrastructures consisting of cloud, fog, and edge computing paradigms.  © 2014 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/JIOT.2021.3081420
VL  - 8
IS  - 21
SP  - 15704
EP  - 15748
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107208881&doi=10.1109%2fJIOT.2021.3081420&partnerID=40&md5=fd55804226f60d1f43322f8ac6c140e4
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 38</p>
KW  - Availability
KW  - cloud computing
KW  - Computer viruses
KW  - Computing paradigm
KW  - Continuous operation
KW  - cyber security attack
KW  - Design and implementations
KW  - e-health monitoring
KW  - edge computing
KW  - fog computing
KW  - Health care
KW  - Health-care system
KW  - Healthcare monitoring
KW  - hierarchical model
KW  - Hierarchical model
KW  - Hierarchical systems
KW  - Internet of Medical Things (IoMT)
KW  - Physical architecture
KW  - Program debugging
KW  - reliability
KW  - Security of data
KW  - Software subsystem
KW  - Viruses
ER  - 

TY  - CONF
TI  - Dependability and Sustainability Evaluation of Data Center Electrical Architectures
AU  - Camboim, K.
AU  - Ferreira, J.
AU  - Melo, C.
AU  - Araujo, J.
AU  - Alencar, F.
AU  - MacIel, P.
T2  - 15th Annual IEEE International Systems Conference, SysCon 2021 - Proceedings
AB  - Faced with the demand to maintain the high availability of data centers (DC), companies are being pressured to seek sustainable alternatives, given that these infrastructures consume a total of 1% of all electricity worldwide [1]. In a time of pandemic (COVID-19), when the digital economy has assumed an even greater share of representativeness, DCs and telecommunications companies need to meet the requisitions of 'everything-as-a-service'. Linked to this are the large amounts of carbon dioxide (CO2) emitted into the atmosphere due to the production and consumption of energy caused by these infrastructures. Given the above, this paper proposes models of energy flow and reliability block diagrams to quantity the environmental impact from different raw materials used to feed the data center loads and computes sustainability and dependability metrics for the entire DC's power infrastructure. According to the specifications for classifying the tiers, this study's hybrid modeling is performed to represent four different electrical architectures. From the model evaluations, we compare whether the availability achieved corresponds to the minimum availability suggested for each tier and show the emissions of CO2 in the atmosphere for each tier over a year. Besides, we apply a parametric sensitivity analysis technique to identify the most critical components for the modeled systems' availability.  © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/SysCon48628.2021.9447132
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111427864&doi=10.1109%2fSysCon48628.2021.9447132&partnerID=40&md5=37bbb7c8fbaf87657f14801ed5cf6cc1
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 5</p>
KW  - Carbon dioxide
KW  - Critical component
KW  - Data Center
KW  - Dependability
KW  - Electrical architecture
KW  - Energy Efficiency
KW  - Energy Flow Model
KW  - Environmental impact
KW  - High availability
KW  - Parametric sensitivity analysis
KW  - Power infrastructures
KW  - Production and consumption
KW  - Reliability block diagrams
KW  - Sensitivity analysis
KW  - Sensitivity Analysis
KW  - Sustainability
KW  - Sustainability evaluations
KW  - Sustainable development
KW  - Telecommunication industry
ER  - 

TY  - CONF
TI  - Availability and scalability optimized microservice discovery from enterprise systems
AU  - De Alwis, A.A.C.
AU  - Barros, A.
AU  - Fidge, C.
AU  - Polyvyanyy, A.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Microservices have been introduced to industry as a novel architectural design for software development in cloud-based applications. This development has increased interest in finding new methodologies to migrate existing enterprise systems into microservices to achieve desirable performance characteristics such as high scalability, high availability, high cohesion and low coupling. A key challenge in this context is discovering microserviceable components with promising characteristics from a complex monolithic code base while predicting their resulting characteristics. This paper presents a technique to support such re-engineering of an enterprise system based on the fundamental mechanisms for structuring its architecture, i.e., business objects managed by software functions and their interactions. The technique relies on queuing theory and business object relationship analysis. A prototype for microservice discovery and characteristic analysis was developed using the NSGA II software clustering and optimization technique and has been validated against two open-source enterprise systems, SugarCRM and ChurchCRM. Our experiments demonstrate that the proposed approach can recommend microservice design which improves scalability, availability and execution efficiency of the system while achieving high cohesion and low coupling in software modules. © Springer Nature Switzerland AG 2019.
DA  - 2019///
PY  - 2019
DO  - 10.1007/978-3-030-33246-4_31
VL  - 11877 LNCS
SP  - 496
EP  - 514
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077848871&doi=10.1007%2f978-3-030-33246-4_31&partnerID=40&md5=d216f80f69e6a052e795a7c25c3d7174
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 6</p>
KW  - Characteristic analysis
KW  - Cloud-based applications
KW  - Computation theory
KW  - Enterprise resource planning
KW  - Enterprise software
KW  - Microservice discovery
KW  - Multi agent systems
KW  - Open source enterprise systems
KW  - Open source software
KW  - Open systems
KW  - Optimization techniques
KW  - Performance characteristics
KW  - Queueing theory
KW  - Reengineering
KW  - Scalability
KW  - Semantics
KW  - Software design
KW  - Software prototyping
KW  - System optimization
KW  - System optimizations
KW  - System re-engineering
KW  - System reengineering
ER  - 

TY  - CONF
TI  - When Failure is (Not) an Option: Reliability Models for Microservices Architectures
AU  - Jagadeesan, L.J.
AU  - Mendiratta, V.B.
T2  - Proceedings - 2020 IEEE 31st International Symposium on Software Reliability Engineering Workshops, ISSREW 2020
AB  - Modern application development and deployment is rapidly evolving to microservices based architectures, in which thousands of microservices communicate with one another and can be independently scaled and updated. While these architectures enable flexibility of deployment and frequency of upgrades, the naive use of thousands of communicating and frequently updated microservices can significantly impact the reliability of applications. To address these challenges, service meshes are used to rapidly detect and respond to microservices failures without necessitating changes to the microservices themselves. However, there are inherent tradeoffs that service meshes must make with regards to how quickly they assume a microservice has failed and the subsequent impact on overall application reliability. We present in this paper a modeling framework for microservices and service mesh reliability that takes these tradeoffs into account. Index Terms-microservices, service mesh, sidecars, circuit breakers, reliability, availability, resilience, reliability models, probabilistic model checking, PRISM.  © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ISSREW51248.2020.00031
SP  - 19
EP  - 24
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099866678&doi=10.1109%2fISSREW51248.2020.00031&partnerID=40&md5=02036281c8d341d90b2350c3db5cf12e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 22</p>
KW  - Application deployment
KW  - Application development
KW  - Application reliabilities
KW  - Availability
KW  - Circuit-breakers
KW  - Commerce
KW  - Electric circuit breakers
KW  - Mesh generation
KW  - Microservice, service mesh, sidecar, circuit breaker, reliability, availability, resilience, reliability model, probabilistic model checking, PRISM
KW  - microservices, service mesh, sidecars, circuit breakers, reliability, availability, resilience, reliability models, probabilistic model checking, PRISM
KW  - Model checking
KW  - Modelling framework
KW  - Modern applications
KW  - Prisms
KW  - Probabilistic model checking
KW  - Probabilistic model-checking
KW  - Reliability
KW  - Reliability modelling
KW  - Timing circuits
ER  - 

TY  - JOUR
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180880814&partnerID=40&md5=00baa7e11f40f064630cf3275b10f435
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
T2  - KubeEdge Kubeedge Kubernetes Native Edge Computing Framework
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180828009&partnerID=40&md5=1c81b4be81af0c90297ebe9d6f3257e9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - Ant-man: Towards agile power management in the microservice era
AU  - Hou, X.
AU  - Li, C.
AU  - Liu, J.
AU  - Zhang, L.
AU  - Hu, Y.
AU  - Guo, M.
T2  - International Conference for High Performance Computing, Networking, Storage and Analysis, SC
AB  - The emerging trend of decomposing cloud applications into microservices has raised new questions about managing the performance/power trade-off of a datacenter at microsecondscale. We introduce ANT-Man, an Auto, Native and Transparent power Management framework that can exploit fine-grained microservice variability for system efficiency. To achieve this, ANT-Man abstracts away two major sources of latency overhead in traditional hierarchical power management frameworks. First, ANT-Man proposes an auto power budgeting scheme for reducing the power coordination latency at the datacenter level. It can proactively determine the power budget tailored to each individual microservice. Second, ANT-Man proposes a native and transparent power control scheme to overcome the power configuration latency for each microservice. It enables super-fast power budget enforcement with nanosecond-scale performance scaling. Extensive experiments on our prototyped system show that ANT-Man could slash power consumption by ; 7.8∼ 43.5\%; and in the meantime reduce the ;95th; tail latency by ; 9.7∼ 12.5\%; compared to existing techniques. © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/SC41405.2020.00082
VL  - 2020-November
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102333512&doi=10.1109%2fSC41405.2020.00082&partnerID=40&md5=a75251d6768fbf25060880aa4c61abda
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 36</p>
KW  - Budget control
KW  - Cloud applications
KW  - Economic and social effects
KW  - Emerging trends
KW  - Hierarchical power management
KW  - Management frameworks
KW  - microservice
KW  - Power budgeting
KW  - Power configuration
KW  - Power control
KW  - Power control schemes
KW  - power management
KW  - Power management
KW  - System efficiency
KW  - variability
ER  - 

TY  - CHAP
TI  - The mercury environment: A modeling tool for performance and dependability evaluation
AU  - Pinheiro, T.
AU  - Oliveira, D.
AU  - Matos, R.
AU  - Silva, B.
AU  - Pereira, P.
AU  - Melo, C.
AU  - Oliveira, F.
AU  - Tavares, E.
AU  - Dantas, J.
AU  - Maciel, P.
T2  - Intelligent Environments 2021: Workshop Proceedings of the 17th International Conference on Intelligent Environments
AB  - It is important to be able to judge the performance or dependability metrics of a system and often we do so by using abstract models even when the system is in the conceptual phase. Evaluating a system by performing measurements can have a high temporal and/or financial cost, which may not be feasible. Mathematical models can provide estimates about system behavior and we need tools supporting different types of formalisms in order to compute desired metrics. The Mercury tool enables a range of models to be created and evaluated for supporting performance and dependability evaluations, such as reliability block diagrams (RBDs), dynamic RBDs (DRBDs), fault trees (FTs), stochastic Petri nets (SPNs), continuous and discrete-time Markov chains (CTMCs and DTMCs), as well as energy flow models (EFMs). In this paper, we introduce recent enhancements to Mercury, namely new SPN simulators, support to prioritized timed transitions, sensitivity analysis evaluation, several improvements to the usability of the tool, and support to DTMC and FT formalisms. © 2021 The authors and IOS Press.
DA  - 2021///
PY  - 2021
VL  - 29
SP  - 16
EP  - 25
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114148193&doi=10.3233%2fAISE210075&partnerID=40&md5=8e1be42c2a2e44e835866092d5b93d8d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 17</p>
KW  - DTMC
KW  - Fault tree
KW  - Mercury
KW  - Models
KW  - Simulation
ER  - 

TY  - CONF
TI  - Dependable Fog Computing: A Systematic Literature Review
AU  - Bakhshi, Z.
AU  - Rodriguez-Navas, G.
AU  - Hansson, H.
T2  - Proceedings - 45th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2019
AB  - Fog computing has been recently introduced to bridge the gap between cloud resources and the network edge. Fog enables low latency and location awareness, which is considered instrumental for the realization of IoT, but also faces reliability and dependability issues due to node mobility and resource constraints. This paper focuses on the latter, and surveys the state of the art concerning dependability and fog computing, by means of a systematic literature review. Our findings show the growing interest in the topic but the relative immaturity of the technology, without any leading research group. Two problems have attracted special interest: Guaranteeing reliable data storage/collection in systems with unreliable and untrusted nodes, and guaranteeing efficient task allocation in the presence of varying computing load. Redundancy-based techniques, both static and dynamic, dominate the architectures of such systems. Reliability, availability and QoS are the most important dependability requirements for fog, whereas aspects such as safety and security, and their important interplay, have not been investigated in depth. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/SEAA.2019.00066
SP  - 395
EP  - 403
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075999858&doi=10.1109%2fSEAA.2019.00066&partnerID=40&md5=06d0a2fe943726d74d351081504b333e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 25</p>
KW  - Application programs
KW  - Dependability
KW  - Dependability means
KW  - Dependability Requirements
KW  - Dependability threats
KW  - Dependable Fog
KW  - Digital storage
KW  - Fault tolerance
KW  - Fog
KW  - Fog computing
KW  - Fog Computing
KW  - Systematic literature review
KW  - Systematic Literature Review
ER  - 

TY  - JOUR
TI  - Toward integrated Cloud–Fog networks for efficient IoT provisioning: Key challenges and solutions
AU  - Peng, L.
AU  - Dhaini, A.R.
AU  - Ho, P.-H.
T2  - Future Generation Computer Systems
AB  - Fog computing has been proposed as one of the promising technologies for the construction of a scalable network infrastructure in the user's vicinity, with the purpose of serving the tremendous amount of daily generated latency-sensitive Internet-of-Things (IoT) data. In provisioning the emerging IoT data in addition to the legacy Cloud services, the Cloud and Fog form a natural continuum of one another and the integration of these two key technologies would offer a promising infrastructure full with IoT resources for IoT data provisioning. In this article, we present iCloudFog, a reconfigurable architecture that enables an agile integration of Fog and Cloud networks. iCloudFog allows to construct different Fog types (i.e., wireless, wired, or hybrid) to fit the different characteristics of IoT devices and data, and Fog nodes. Due to its nature, iCloudFog presents several unique key research challenges that have not yet been addressed in existing literatures, such as network dimensioning and configuration, resource management/QoS, security/privacy, and positioning/localization. We discuss these challenges and suggest promising approaches to resolve them. Effective design and implementation of solutions based on the suggested approaches would allow iCloudFog to play a salient role towards the successful provisioning of future IoT applications and services. © 2018
DA  - 2018///
PY  - 2018
DO  - 10.1016/j.future.2018.05.015
VL  - 88
SP  - 606
EP  - 613
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048705614&doi=10.1016%2fj.future.2018.05.015&partnerID=40&md5=f5271da5e66941c979b89df122a3de50
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 59</p>
KW  - Agile integration
KW  - Cloud computing
KW  - Data provisioning
KW  - Edge computing
KW  - Effective designs
KW  - Fog
KW  - Fog computing
KW  - Internet of things
KW  - Internet of Things (IOT)
KW  - Network dimensioning
KW  - Reconfigurable architectures
KW  - Research challenges
KW  - Resource management
KW  - Scalable networks
ER  - 

TY  - JOUR
TI  - A comprehensive survey for scheduling techniques in cloud computing
AU  - Kumar, M.
AU  - Sharma, S.C.
AU  - Goel, A.
AU  - Singh, S.P.
T2  - Journal of Network and Computer Applications
AB  - Resource scheduling becomes the prominent issue in cloud computing due to rapid growth of on demand request and heterogeneous nature of cloud resources. Cloud provides dynamism, uncertainty and elasticity based services to users in pay-as-you-go fashion over the internet. In recent decade, increase in requests (diverse and complex applications) for cloud services is raising the workload in cloud environment. Inefficient scheduling techniques face the challenges of resources being over utilized and underutilized (imbalanced) which leads to either degradation in service performance (in case of over utilized) or wastage of cloud resources (in case of underutilized). The basic idea behind the scheduling is to distribute tasks (diverse and complex nature) among the cloud resources in such a manner that scheduling algorithm avoids the problem of imbalance. Scheduling algorithm should also optimize the key performance indicator parameters like response time, makespan time, reliability, availability, energy consumption, cost, resource utilization etc. To fulfill the above-mentioned objective, many state of art scheduling algorithms have been proposed based upon heuristic, meta-heuristic and hybrid, reported in the literature. This paper provides the systematic review as well as classification of proposed scheduling techniques along with their advantages and limitations. We hope that our systematic and comprehensive survey work as a stepping stone for new researchers in the field of cloud computing and will be helpful for further development of scheduling technique. © 2019
DA  - 2019///
PY  - 2019
DO  - 10.1016/j.jnca.2019.06.006
VL  - 143
SP  - 1
EP  - 33
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067581485&doi=10.1016%2fj.jnca.2019.06.006&partnerID=40&md5=3cdd56761e3ab596c9f85d070f7fad2f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 327</p>
KW  - Benchmarking
KW  - Cloud computing
KW  - Complex applications
KW  - Energy utilization
KW  - Heuristic
KW  - Heuristic algorithms
KW  - Key performance indicators
KW  - meta-Heuristic
KW  - Metaheuristic
KW  - Resource provisioning
KW  - Resource scheduling
KW  - Resource utilizations
KW  - Resource-scheduling
KW  - Scheduling
KW  - Scheduling algorithms
KW  - Scheduling techniques
KW  - Surveys
KW  - Virtual machine
KW  - Virtual machines
ER  - 

TY  - CONF
TI  - Rearchitecting Kubernetes for the Edge
AU  - Jeffery, A.
AU  - Howard, H.
AU  - Mortier, R.
T2  - EdgeSys 2021 - Proceedings of the 4th International Workshop on Edge Systems, Analytics and Networking, Part of EuroSys 2021
AB  - Recent years have seen Kubernetes emerge as a primary choice for container orchestration. Kubernetes largely targets the cloud environment but new use cases require performant, available and scalable orchestration at the edge. Kubernetes stores all cluster state in etcd, a strongly consistent key-value store. We find that at larger etcd cluster sizes, offering higher availability, write request latency significantly increases and throughput decreases similarly. Coupled with approximately 30% of Kubernetes requests being writes, this directly impacts the request latency and availability of Kubernetes, reducing its suitability for the edge. We revisit the requirement of strong consistency and propose an eventually consistent approach instead. This enables higher performance, availability and scalability whilst still supporting the broad needs of Kubernetes. This aims to make Kubernetes much more suitable for performance-critical, dynamically-scaled edge solutions.  © 2021 Owner/Author.
DA  - 2021///
PY  - 2021
DO  - 10.1145/3434770.3459730
SP  - 7
EP  - 12
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104662456&doi=10.1145%2f3434770.3459730&partnerID=40&md5=5affbaa5a658a55090c90324acaa80f5
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 47</p>
KW  - Cloud environments
KW  - Cluster sizes
KW  - Cluster state
KW  - CRDTs
KW  - edge
KW  - eventual consistency
KW  - Key-value stores
KW  - Kubernetes
KW  - orchestration
KW  - Strong consistency
ER  - 

TY  - JOUR
TI  - ICT sector, digitization and environmental sustainability: A systematic review of the literature from 2000 to 2022
AU  - Charfeddine, L.
AU  - Umlai, M.
T2  - Renewable and Sustainable Energy Reviews
AB  - In line with an intensified call for reducing greenhouse emissions and curbing the effects of climate change, scientists and experts have looked to information communication technology (ICT) and digitization as critical tools for the more efficient use and production of energy. Consequently, research on ICT/digitization and their impact on environmental sustainability has witnessed exponential growth in the last few decades. This study provides a systematic review of the relationship between ICT/digitization and environmental sustainability over the period from January 2000 to April 2022. It aimed to improve our understanding of the different theories and channels governing the ICT/digitization–environmental sustainability nexus, to provide an in-depth analysis and discussion of the trends and main empirical findings of the reviewed articles, and to highlight key avenues for future research. In total, 166 scientific articles examining 297 associations between ICT/digitization and environmental sustainability were selected for this review. The results revealed that most of the studies have used measures based on climate change and air pollution for environmental sustainability, and traditional ICT/digitization measures, e.g., mobile phone subscriptions and internet users. The results also showed that although most studies found evidence for ICT/digitization improving environmental sustainability, evidence for a negative association is concentrated in studies on the ’Group of’ countries. The results also revealed a scarcity of studies investigating nonlinear relationships between ICT/digitization and environmental sustainability. © 2023 The Author(s)
DA  - 2023///
PY  - 2023
DO  - 10.1016/j.rser.2023.113482
VL  - 184
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165004761&doi=10.1016%2fj.rser.2023.113482&partnerID=40&md5=b031435af62f65deca19305518513d70
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 113</p>
KW  - Climate change
KW  - Digitisation
KW  - Digitization
KW  - Empirical findings
KW  - Energy
KW  - Energy efficiency
KW  - Environmental sustainability
KW  - Exponential growth
KW  - Greenhouse emissions
KW  - ICT
KW  - In-depth analysis
KW  - Information communication technology
KW  - Sustainable development
KW  - Systematic review
KW  - Systematic Review
KW  - Technology sectors
ER  - 

TY  - JOUR
TI  - The fog cloud of things: A survey on concepts, architecture, standards, tools, and applications
AU  - Alli, A.A.
AU  - Alam, M.M.
T2  - Internet of Things (Netherlands)
AB  - The Fog computing paradigms are becoming popular means of utilizing resources optimally by the IoT devices, extending quality of service to the vicinity of the user, and achieve fast processing in the IoT-cloud ecosystems. Fog models allow fast processing of data, easy to reach storage, and reduce bulky network transition. The inefficiencies of the cloud inspire unnecessarily big data to be sent to the backhaul of the network, which incapacitates the cloud infrastructure. Fog computing addresses the limitation of the cloud systems by improving robustness, efficiency, and performance of cloud infrastructure. The need to process some of the big data produced at the peripheral of the network using keen techniques in the fog—cloud ecosystems is a key to new interesting architectures filed in the recent literature. These architectures provide new business opportunities that drive the Internet of things devices to function according to users’ demands. In this paper, we provide an extensive survey on Fog—Edge computing to give a foundation to solutions proposed in studies that involve IoT—Fog—Cloud ecosystems. This is done by providing insights of new research aspects filed, the state—of—art in fog computing architectures, standards, tools and applications. We project the future development trends and provide open issues in fog cloud of things. This will focus developers to develop applications that work well in a cloud-based controlled ecosystem across a range of network terminals. © 2020 Elsevier B.V.
DA  - 2020///
PY  - 2020
DO  - 10.1016/j.iot.2020.100177
VL  - 9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092163275&doi=10.1016%2fj.iot.2020.100177&partnerID=40&md5=c4fc4f8284cdc678e7dc414a3f88f461
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 140</p>
KW  - Big data analytics
KW  - Computational offloading
KW  - Content delivery network
KW  - Fogging
KW  - Internet of things
KW  - IoT—Fog—Cloud ecosystems
KW  - Simulation tools
KW  - Smart city applications
KW  - Smart farm applications
KW  - Web performance
ER  - 

TY  - JOUR
TI  - Kubernetes Scheduling: Taxonomy, Ongoing Issues and Challenges
AU  - Carrión, C.
T2  - ACM Computing Surveys
AB  - Continuous integration enables the development of microservices-based applications using container virtualization technology. Container orchestration systems such as Kubernetes, which has become the de facto standard, simplify the deployment of container-based applications. However, developing efficient and well-defined orchestration systems is a challenge. This article focuses specifically on the scheduler, a key orchestrator task that assigns physical resources to containers. Scheduling approaches are designed based on different Quality of Service (QoS) parameters to provide limited response time, efficient energy consumption, better resource utilization, and other things. This article aims to establish insight knowledge into Kubernetes scheduling, find the main gaps, and thus guide future research in the area. Therefore, we conduct a study of empirical research on Kubernetes scheduling techniques and present a new taxonomy for Kubernetes scheduling. The challenges, future direction, and research opportunities are also discussed.  © 2022 Association for Computing Machinery.
DA  - 2023///
PY  - 2023
DO  - 10.1145/3539606
VL  - 55
IS  - 7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134495149&doi=10.1145%2f3539606&partnerID=40&md5=c5f0f6ee1f5012fc705831648624a532
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 148</p>
KW  - containers
KW  - Containers
KW  - Continuous integrations
KW  - De facto standard
KW  - Energy utilization
KW  - Energy-consumption
KW  - Issues and challenges
KW  - Kubernetes
KW  - orchestration
KW  - Orchestration
KW  - Physical resources
KW  - Quality of service
KW  - Quality of Service parameters
KW  - scheduling
KW  - survey
KW  - Taxonomies
KW  - Time-efficient
KW  - Virtualization technologies
ER  - 

TY  - JOUR
TI  - Proactive Stateful Fault-Tolerant System for Kubernetes Containerized Services
AU  - Tran, M.-N.
AU  - Vu, X.T.
AU  - Kim, Y.
T2  - IEEE Access
AB  - Recently, the development of Kubernetes (K8s) containerization platform has enabled cloud-based, lightweight, highly scalable, and agile services in both general and telco use-cases. Ensuring high availability, reliable and continuous containerized services is a major requirement of service providers to provide fault-tolerance, transparent service experiences to end-users. To satisfy this requirement, fault prediction and proactive stateful service recovery features must be applied in cloud systems. Prior proactive failure recovery approaches mostly focused on either improving fault prediction performance based on different machine learning time series forecasting techniques or optimizing recovery service placement after fault prediction. However, a mechanism that enables stateful containerized service migration from the predicted faulty node to the healthy destination node has not been studied. Service migration in previous proactive works is only simulated or performed by virtual machine (VM) migration techniques. In this paper, we propose a proactive stateful fault-tolerant system for K8s containerized services that pipelines a Bidirectional Long Short-Term Memory (Bi-LSTM) fault prediction framework and a novel K8s stateful service migration mechanism for service recovery. Experimental results show how the Bi-LSTM model improved prediction performance against other time-series forecasting models used prior proactive works. We then combined the Bi-LSTM fault prediction framework with both the default K8s and our stateful migration mechanisms. The comparison between these two proactive systems proves our system efficiency in terms of reducing Quality of Service (QoS) violation percentage and service recovery time.  © 2013 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ACCESS.2022.3209257
VL  - 10
SP  - 102181
EP  - 102194
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139440476&doi=10.1109%2fACCESS.2022.3209257&partnerID=40&md5=c2db74083042f447eb46fb512b14385f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 18</p>
KW  - Containerization
KW  - Containers
KW  - Fault prediction
KW  - Fault tolerance
KW  - Fault- tolerant systems
KW  - Fault-tolerant
KW  - Forecasting
KW  - Kubernetes
KW  - Learning systems
KW  - Long short-term memory
KW  - Prediction algorithms
KW  - Predictive models
KW  - Proactive fault
KW  - proactive fault-tolerant
KW  - Proactive fault-tolerant
KW  - Quality of service
KW  - Quality-of-service
KW  - Recovery
KW  - Time series
ER  - 

TY  - JOUR
TI  - Microservice-Based Architecture for an Energy Management System
AU  - Lyu, Z.
AU  - Wei, H.
AU  - Bai, X.
AU  - Lian, C.
T2  - IEEE Systems Journal
AB  - This article proposes a microservice-based architecture for an energy management system (MS-EMS) to address the fragility, poor flexibility, and hardware dependence of EMSs. Compared with the service-oriented architecture (SOA), the proposed architecture can significantly improve the load performance and scalability of an EMS through fine-grained decomposition of the system and decentralized data management. Container and cluster technologies are used to manage the microservices. A $k$-fault $(k\geq 2)$-tolerant model is proposed to improve the reliability of the MS-EMS. The model employs containerized microservices as essential components to achieve a parallel connection of the essential components using the horizontal-scale technology of the containers. On the other hand, a MILP model-based algorithm for managing computing resources is also suggested. By minimizing the number of worker nodes of the cluster, where the MS-EMS is deployed, we can improve the utilization of the computing resources and avoid unnecessary costs. The result of the performance analysis showed that the reliability of the MS-EMS is $99.99965625{\%}$, which is two orders and one order of magnitude higher than those of the existing EMSs and the SOA-based EMS (S-EMS), respectively. Moreover, the cost of the MS-EMS is also lower than those of the existing EMSs and S-EMSs. The proposed architecture is implemented in a real-power system and has shown favorable operation performances, indicating a promising prospect for future applications.  © 2007-2012 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/JSYST.2020.2981095
VL  - 14
IS  - 4
SP  - 5061
EP  - 5072
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097045354&doi=10.1109%2fJSYST.2020.2981095&partnerID=40&md5=2221cf4b263ef6a0faf73588773ca706
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 28</p>
KW  - Cluster computing
KW  - Cluster technology
KW  - Computer architecture
KW  - Container
KW  - Containers
KW  - Electric connectors
KW  - Energy management
KW  - energy management system (EMS)
KW  - Energy management systems
KW  - Future applications
KW  - Information management
KW  - Information services
KW  - Integer programming
KW  - microservice
KW  - Minimizing the number of
KW  - Operation performance
KW  - Parallel connections
KW  - Performance analysis
KW  - Proposed architectures
KW  - Real power systems
KW  - reliability
KW  - Reliability analysis
KW  - resource management
KW  - Service oriented architecture (SOA)
ER  - 

TY  - JOUR
TI  - An Approach to Modeling and Analyzing Reliability for Microservice-Oriented Cloud Applications
AU  - Liu, Z.
AU  - Fan, G.
AU  - Yu, H.
AU  - Chen, L.
T2  - Wireless Communications and Mobile Computing
AB  - Microservice architecture is a cloud-native architectural style, which has attracted extensive attention from the scientific research and industry communities to benefit independent development and deployment. However, due to the complexity of cloud-based platforms, the design of fault-tolerant strategies for microservice-oriented cloud applications becomes challenging. In order to improve the quality of service, it is essential to focus on the microservice with more criticality and maximize the reliability of the entire cloud application. This paper studies the modeling and analysis of service reliability in the cloud environment. Firstly, a formal description language is defined to model microservice, user request, and container accurately. Secondly, the reliability analysis is conducted to measure a critical microservice's fluctuation and vibration attributes within a period, and the related properties of the constructed model are analyzed. Thirdly, a fault-tolerant strategy with redundancy operation has been proposed to optimize cloud application reliability. Finally, the effectiveness of the method is verified by experiments. The simulation results show that the algorithm obtains the maximum benefits and has high performance through several experiments. © 2021 Zheng Liu et al.
DA  - 2021///
PY  - 2021
DO  - 10.1155/2021/5750646
VL  - 2021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114641496&doi=10.1155%2f2021%2f5750646&partnerID=40&md5=df54c466196e8f328d00ace86756fe8d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 9</p>
KW  - Architectural style
KW  - Cloud based platforms
KW  - Cloud environments
KW  - Fault tolerance
KW  - Fault-tolerant strategy
KW  - Formal description language
KW  - Industrial research
KW  - Model and analysis
KW  - Quality of service
KW  - Redundancy
KW  - Reliability analysis
KW  - Scientific researches
KW  - Service reliability
KW  - Vibration analysis
ER  - 

TY  - JOUR
TI  - KCSS: Kubernetes container scheduling strategy
AU  - Menouer, T.
T2  - Journal of Supercomputing
AB  - The Kubernetes framework is a well-known open-source container-orchestration system widely used in industrial and academic fields. In this paper, we introduce a new Kubernetes Container Scheduling Strategy called KCSS. The goal of the KCSS is to optimize the scheduling of several containers submitted online by users to improve the performance concerning the user need in terms of makespan and the cloud provider need in terms of power consumption. In the literature, several container scheduling strategies are proposed. Each one uses a dedicated approach to select one node from the node set supplied by the cloud infrastructure. Then, this node welcomes the newly submitted container. The majority of the proposed container scheduling strategies select for each newly submitted container a node based on a single criteria, such as the number of running containers or the amount of available resources in each node. However, the scheduling based on a single criterion downgrades the performance because the scheduler has a limited vision of the state of the cloud infrastructure and the user need. The contribution of our KCSS is to introduce a multi-criteria selection of the node. This approach provides the scheduler with a global vision about the state of the cloud and the user’s need. The idea is to select from each newly submitted container the best node, with a good compromise between hybrid criteria related to the cloud infrastructure and the user need. In the KCSS, we consider six key criteria: (1) the CPUs utilization rate in each node; (2) the memory utilization rate in each node; (3) the disk utilization rate in each node; (4) the power consumption of each node; (5) the number of running containers in each node; and (6) the time of transmitting the image selected by the user for its container. In our context, the KCSS is based on a multi-criteria decision analysis algorithm to aggregate all criteria in a single rank. Then, select the node which has the highest rank to execute the new submitted container. The multi-criteria algorithm used by KCSS is the Technique for the Order of Prioritisation by Similarity to Ideal Solution (TOPSIS) algorithm. The KCSS is implemented in Go language inside the Kubernetes framework with minimal changes to be used easily with the next Kubernetes versions. The experimental results show that the KCSS improves the performance under different scenarios compared to other container scheduling strategies. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.
DA  - 2021///
PY  - 2021
DO  - 10.1007/s11227-020-03427-3
VL  - 77
IS  - 5
SP  - 4267
EP  - 4293
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091507109&doi=10.1007%2fs11227-020-03427-3&partnerID=40&md5=018c2cbd20c3c39c7937e54bab2e926f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 103</p>
KW  - Cloud computing
KW  - Cloud infrastructures
KW  - Cloud providers
KW  - Container scheduling
KW  - Container technology
KW  - Containers
KW  - Disk utilization
KW  - Electric power utilization
KW  - Green computing
KW  - Ideal solutions
KW  - Memory utilization
KW  - Multi-criteria decision analysis
KW  - Multi-criteria scheduling
KW  - On-line scheduling
KW  - Open systems
KW  - Power consumption in scheduling
KW  - Program processors
KW  - Scheduling
KW  - Scheduling strategy
KW  - Utilization rates
ER  - 

TY  - JOUR
TI  - A latency-aware and energy-efficient computation offloading in mobile fog computing: a hidden Markov model-based approach
AU  - Jazayeri, F.
AU  - Shahidinejad, A.
AU  - Ghobaei-Arani, M.
T2  - Journal of Supercomputing
AB  - In recent years, Fog Computing (FC) is known as a good infrastructure for the Internet of Things (IoT). Using this architecture for the mobile applications in the IoT is named the Mobile Fog Computing (MFC). If we assume that an application includes some modules, thus, these modules can be sent to the Fog or Cloud layer because of the resource limitation or increased runtime at the mobile. This increases the efficiency of the whole system. As data is entered sequentially, and the input is given to the modules, the number of executable modules increases. So, this research is conducted to find the best place in order to run the modules that can be on the mobile, Fog, or Cloud. According to the proposed method, when the modules arrive at gateway, then, a Hidden Markov model Auto-scaling Offloading (HMAO) finds the best destination to execute the module to create a compromise between the energy consumption and execution time of the modules. The evaluation results obtained regarding the parameters of the energy consumption, execution cost, delay, and network resource usage shows that the proposed method on average is better than the local execution, First-Fit (FF), and Q-learning based method. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.
DA  - 2021///
PY  - 2021
DO  - 10.1007/s11227-020-03476-8
VL  - 77
IS  - 5
SP  - 4887
EP  - 4916
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094629653&doi=10.1007%2fs11227-020-03476-8&partnerID=40&md5=b520d2cbc86993ccecc191c09761cc55
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 71</p>
KW  - Energy efficiency
KW  - Energy efficient
KW  - Energy utilization
KW  - Evaluation results
KW  - Execution costs
KW  - Fog
KW  - Fog computing
KW  - Gateways (computer networks)
KW  - Hidden Markov model
KW  - Hidden Markov models
KW  - Internet of thing (IOT)
KW  - Internet of things
KW  - Latency-aware
KW  - Mobile applications
KW  - Mobile fog computing
KW  - Network resource
KW  - Offloading
KW  - Reinforcement learning
KW  - Resource limitations
ER  - 

TY  - CONF
TI  - Highly-Distributed Systems Based on Micro-Services and their Construction Paradigms
AU  - Luntovskyy, A.
AU  - Shubyn, B.
T2  - Proceedings - 15th International Conference on Advanced Trends in Radioelectronics, Telecommunications and Computer Engineering, TCSET 2020
AB  - A definition for the HDS, as well as the demarcation to conventional distributed systems, were given. Typical architectures for HDS were discussed which affect increasing of QoS and of so-called QoE (Quality of Experience). The distinguishing features for HDS are clearly formulated. The advanced SWT (Software Technologies) approaches lead to use of young flexible service-oriented architectures like Micro-Services, which provide higher performance and small latencies, as well as better scalability, energy-efficiency and autarky.One possible option in the frame of HDS regarding security, privacy, authentication and compulsoriness of workflow steps, modules and service execution for such apps Blockchain and Smart Contracting are. The theoretical issues are proven via the represented examples and case studies. © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/TCSET49122.2020.235378
SP  - 7
EP  - 14
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086306642&doi=10.1109%2fTCSET49122.2020.235378&partnerID=40&md5=c71f3b2ed287cad4bd9b4425fa7ab366
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 6</p>
KW  - Agile Process Models
KW  - Blockchain
KW  - Case-studies
KW  - Conway's Law
KW  - DevOps
KW  - Distributed database systems
KW  - Distributed systems
KW  - Energy efficiency
KW  - Flexible service
KW  - Highly-Distributed Systems
KW  - Information services
KW  - Micro services
KW  - Micro-Services
KW  - Network security
KW  - Quality of Experience
KW  - Quality of experience (QoE)
KW  - Quality of service
KW  - Scrum
KW  - Service execution
KW  - Service oriented architecture (SOA)
KW  - Service-Oriented Architectures
KW  - Software technology
ER  - 

TY  - JOUR
TI  - QoS-aware service provisioning in fog computing
AU  - Murtaza, F.
AU  - Akhunzada, A.
AU  - Islam, S.U.
AU  - Boudjadar, J.
AU  - Buyya, R.
T2  - Journal of Network and Computer Applications
AB  - Fog computing has emerged as a complementary solution to address the issues faced in cloud computing. While fog computing allows us to better handle time/delay-sensitive Internet of Everything (IoE) applications (e.g. smart grids and adversarial environment), there are a number of operational challenges. For example, the resource-constrained nature of fog-nodes and heterogeneity of IoE jobs complicate efforts to schedule tasks efficiently. Thus, to better streamline time/delay-sensitive varied IoE requests, the authors contributes by introducing a smart layer between IoE devices and fog nodes to incorporate an intelligent and adaptive learning based task scheduling technique. Specifically, our approach analyzes the various service type of IoE requests and presents an optimal strategy to allocate the most suitable available fog resource accordingly. We rigorously evaluate the performance of the proposed approach using simulation, as well as its correctness using formal verification. The evaluation findings are promising, both in terms of energy consumption and Quality of Service (QoS). © 2020
DA  - 2020///
PY  - 2020
DO  - 10.1016/j.jnca.2020.102674
VL  - 165
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084937424&doi=10.1016%2fj.jnca.2020.102674&partnerID=40&md5=eb3d7970ee7080e7d501c4f766f6c106
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 54</p>
KW  - Adaptive learning
KW  - Adversarial environments
KW  - Cloud computing
KW  - Energy utilization
KW  - Fog
KW  - Fog computing
KW  - Internet of everything
KW  - LRFC
KW  - Operational challenges
KW  - Optimal strategies
KW  - Quality control
KW  - Quality of experience
KW  - Quality of service
KW  - Service provisioning
KW  - Smart grid
KW  - Smart layers
KW  - Task-scheduling
ER  - 

TY  - JOUR
TI  - Reliability modelling and optimization for microservice-based cloud application using multi-agent system
AU  - Liu, Z.
AU  - Yu, H.
AU  - Fan, G.
AU  - Chen, L.
T2  - IET Communications
AB  - In the process of the continuous development of the Internet of Things, cloud computing has been applied in many fields, how to guarantee the quality of service, such as low latency, high bandwidth, high reliability etc., has become a challenging problem. This paper proposes a method to model and optimize reliability for microservice-based cloud applications using multi-agent system (MAS), thus maximizing the reliability of cloud computing and dynamically scheduling microservices to minimize the delay within the budget. Firstly, a dynamic microservice scheduling scheme is proposed to provide efficient computing services by using MAS. A hierarchical cloud computing model is formed by predicated Petri net (PrT net) and the properties of constructed model are analysed. Secondly, agents have been utilized to describe the essential characteristics of microservice scheduling process in the cloud applications. The partial critical path (PCP) aims to maximize the reliability of cloud applications under the limitation of budget and meet the user-defined deadline. Finally, the proposed PCPRO algorithm has been applied to cloud environment, which is suitable for different scientific workflows in the cloud computing environment. The effectiveness of this method is verified by simulation, the experiment results show the effectiveness of the proposed method. © 2022 The Authors. IET Communications published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.
DA  - 2022///
PY  - 2022
DO  - 10.1049/cmu2.12371
VL  - 16
IS  - 10
SP  - 1182
EP  - 1199
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126098914&doi=10.1049%2fcmu2.12371&partnerID=40&md5=9cb89b082c15ae31fc8cc04ba64c8d4b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 14</p>
KW  - Budget control
KW  - Cloud applications
KW  - Cloud computing
KW  - Cloud-computing
KW  - Continuous development
KW  - High bandwidth
KW  - High reliability
KW  - Low latency
KW  - Modeling and optimization
KW  - Multi agent systems
KW  - Petri nets
KW  - Quality of service
KW  - Quality-of-service
KW  - Reliability
KW  - Reliability modelling
KW  - Reliability optimization
KW  - Scheduling
ER  - 

TY  - JOUR
TI  - Performance Modeling of Microservice Platforms
AU  - Khazaei, H.
AU  - Mahmoudi, N.
AU  - Barna, C.
AU  - Litoiu, M.
T2  - IEEE Transactions on Cloud Computing
AB  - Microservice architecture has transformed the way developers are building and deploying applications in the nowadays cloud computing centers. This new approach provides increased scalability, flexibility, manageability, and performance while reducing the complexity of the whole software development life cycle. The increase in cloud resource utilization also benefits microservice providers. Various microservice platforms have emerged to facilitate the DevOps of containerized services by enabling continuous integration and delivery. Microservice platforms deploy application containers on virtual or physical machines provided by public/private cloud infrastructures in a seamless manner. In this article, we study and evaluate the provisioning performance of microservice platforms by incorporating the details of all layers (i.e., both micro and macro layers) in the modeling process. To this end, we first build a microservice platform on top of Amazon EC2 cloud and then leverage it to develop a comprehensive performance model to perform what-if analysis and capacity planning for microservice platforms at scale. In other words, the proposed performance model provides a systematic approach to measure the elasticity of the microservice platform by analyzing the provisioning performance at both the microservice platform and the back-end macroservice infrastructures.  © 2013 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/TCC.2020.3029092
VL  - 10
IS  - 4
SP  - 2848
EP  - 2862
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145020201&doi=10.1109%2fTCC.2020.3029092&partnerID=40&md5=c18980f71961664bdc764335b2a1de87
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 14</p>
KW  - Cloud infrastructure and stochastic process
KW  - cloud infrastructure and stochastic processes
KW  - Cloud infrastructures
KW  - Cloud-computing
KW  - Computing center
KW  - containers
KW  - Containers
KW  - Life cycle
KW  - Microservice platform
KW  - microservice platforms
KW  - New approaches
KW  - Performance
KW  - Performance modeling
KW  - Performance Modeling
KW  - Random processes
KW  - Resources utilizations
KW  - Software design
KW  - Software development life-cycle
KW  - Stochastic systems
ER  - 

TY  - CONF
TI  - TeaStore: A micro-service reference application for benchmarking, modeling and resource management research
AU  - Von Kistowski, J.
AU  - Eismann, S.
AU  - Schmitt, N.
AU  - Bauer, A.
AU  - Grohmann, J.
AU  - Kounev, S.
T2  - Proceedings - 26th IEEE International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems, MASCOTS 2018
AB  - Modern distributed applications offer complex performance behavior and many degrees of freedom regarding deployment and configuration. Researchers employ various methods of analysis, modeling, and management that leverage these degrees of freedom to predict or improve non-functional properties of the software under consideration. In order to demonstrate and evaluate their applicability in the real world, methods resulting from such research areas require test and reference applications that offer a range of different behaviors, as well as the necessary degrees of freedom. Existing production software is often inaccessible for researchers or closed off to instrumentation. Existing testing and benchmarking frameworks, on the other hand, are either designed for specific testing scenarios, or they do not offer the necessary degrees of freedom. Further, most test applications are difficult to deploy and run, or are outdated. In this paper, we introduce the TeaStore, a state-of-The-Art micro-service-based test and reference application. TeaStore offers services with different performance characteristics and many degrees of freedom regarding deployment and configuration to be used as a benchmarking framework for researchers. The TeaStore allows evaluating performance modeling and resource management techniques; it also offers instrumented variants to enable extensive run-Time analysis. We demonstrate TeaStore's use in three contexts: performance modeling, cloud resource management, and energy efficiency analysis. Our experiments show that TeaStore can be used for evaluating novel approaches in these contexts and also motivates further research in the areas of performance modeling and resource management. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/MASCOTS.2018.00030
SP  - 223
EP  - 236
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058299875&doi=10.1109%2fMASCOTS.2018.00030&partnerID=40&md5=b9ee5510d42b5ac1ead035c7c172885c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 124</p>
KW  - Auto Scaler
KW  - Benchmarking
KW  - Cloud
KW  - Clouds
KW  - Container
KW  - Containers
KW  - Degrees of freedom (mechanics)
KW  - Deployment and configuration
KW  - Energy efficiency
KW  - Energy Efficiency
KW  - Energy efficiency analysis
KW  - Green computing
KW  - Microservice
KW  - Models
KW  - Natural resources management
KW  - Performance
KW  - Performance characteristics
KW  - Power
KW  - Resource allocation
KW  - Resource management techniques
ER  - 

TY  - CONF
TI  - Dependability Evaluation of a Blockchain-as-a-Service Environment
AU  - Melo, C.
AU  - Dantas, J.
AU  - Oliveira, D.
AU  - Fe, I.
AU  - Matos, R.
AU  - Dantas, R.
AU  - MacIel, R.
AU  - MacIel, P.
T2  - Proceedings - IEEE Symposium on Computers and Communications
AB  - The blockchain shared ledger emerged as an alternative to the bureaucratic banking system that may take days to confirm a payment or a transfer between clients. The blockchain concept evolved and became viable for various applications beyond the domain of financial transactions. Blockchains become a way to reach better relationships through contract validation, documents transfer, and personal and business data security. Recently, the blockchain-as-a-service has debuted on Microsoft Data Centers, and now many share an infrastructure that can change and improve their security routines. This paper evaluates the feasibility of a blockchain-as-a-service infrastructure and helps those who plan to deploy or sell blockchains. A modeling methodology based on Dynamical Reliability Block Diagrams (DRBD) is adopted to evaluate two dependability attributes: system's reliability and availability. The proposed infrastructure contains the minimum requirements to deploy the Hyperledger Cello, a platform to create and manage blockchains. The availability results pointed out a system downtime of 121 hours per year and reliability issues that must be addressed when building blockchain-as-a-service infrastructures. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/ISCC.2018.8538752
VL  - 2018-June
SP  - 909
EP  - 914
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059220101&doi=10.1109%2fISCC.2018.8538752&partnerID=40&md5=23ed71d0d35ea06bd6506c1d4e9f7e50
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 18</p>
KW  - Blockchain
KW  - Contract validations
KW  - Dependability evaluation
KW  - Financial transactions
KW  - Minimum requirements
KW  - Modeling methodology
KW  - Reliability
KW  - Reliability and availability
KW  - Reliability block diagrams
KW  - Service infrastructure
ER  - 

TY  - JOUR
TI  - Data centers’ services restoration based on the decision-making of distributed agents
AU  - Lima, P.A.
AU  - Neto, A.S.B.
AU  - Maciel, P.
T2  - Telecommunication Systems
AB  - The increasing number of companies that are migrating their IT infrastructure to cloud environments has been motivated many studies on distributed backup strategies to improve the availability of these companies’ systems. In this scenario, it is essential to study mechanisms to evaluate the network conditions to minimize the transmission time to improve the availability of the system. The goal of this study is to build models to evaluate the availability of services running in cloud data center infrastructure, emphasizing the impact of the variation of throughput on the data redundancy, and consequently, on the availability of the service. Based on it, this research purposes some smart models which can be deployed in each data center of a distributed arrange of data centers and help the system administrator to choose the best data center to restore the services of a faulty one. To analyze the impact of the network throughput over the service’s availability, we gathered the MTTF and MTTR metrics of data center’s components and services, generated a reliability block diagram to get the MTTF of the system as a whole, and developed a formalism to model the network component. Based on the results, we built an SPN model to represent the system and get the availability of it in many network conditions. After that, we analyze the availability of the system to discuss the impact of the network conditions over the system’s availability. After building the models and get the system’s availability in many network conditions, we can perceive the enormous impact of the network conditions over the system’s availability through a plot that exhibits the annual downtime along of a year. Using the models developed to study the system availability, we developed smart agents capable of predicting the transfer time of a bulk of data and, with it, choose the data center with the best network conditions to restore the services of a faulty one. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.
DA  - 2020///
PY  - 2020
DO  - 10.1007/s11235-020-00660-2
VL  - 74
IS  - 3
SP  - 367
EP  - 378
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082754227&doi=10.1007%2fs11235-020-00660-2&partnerID=40&md5=50196d18b0a5c87fa94cb043b2e9f1fe
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 7</p>
KW  - Agent
KW  - Agents
KW  - Availability
KW  - Data center
KW  - Data centers
KW  - Decision making
KW  - Distributed agents
KW  - IT infrastructures
KW  - Learning systems
KW  - Machine learning
KW  - Monitoring
KW  - Network throughput
KW  - Reliability block diagrams
KW  - Restoration
KW  - System administrators
KW  - System availability
KW  - VM migration
KW  - Vm migrations
ER  - 

TY  - JOUR
TI  - Dependability impact in the smart solar power systems: An analysis of smart buildings
AU  - Araujo, E.
AU  - Pereira, P.
AU  - Dantas, J.
AU  - Maciel, P.
T2  - Energies
AB  - The Internet has been going through significant transformations and changing the world around us. We can also see the Internet to be used in many areas, for innumerable purposes, and, currently, it is even used by objects. This evolution leads to the Internet of Things (IoT) paradigm. This new concept can be defined as a system composed of storage resources, sensor devices, con-trollers, applications, and network infrastructure, in order to provide specific services to its users. Since IoT comprises heterogeneous components, the creation of these systems, the communication, and maintenance of their components became a complex task. In this paper, we present a dependability model to evaluate an IoT system. Amid different systems, we chose to assess availability in a smart building. The proposed models allow us to calculate estimations of other measures besides steady-state availability, such as reliability. Thus, it was possible to notice that there was no considerable gain of availability in the system when applying grid-tie solar power or off-grid solar power. The grid-tie solar power system is cheaper than the off-grid solar power system, even though it produces more energy. However, in our research, we were able to observe that the off-grid solar power system recovers the applied financial investment in smaller interval of time. © 2020 by the authors. Li-censee MDPI, Basel, Switzerland.
DA  - 2021///
PY  - 2021
DO  - 10.3390/en14010124
VL  - 14
IS  - 1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106171493&doi=10.3390%2fen14010124&partnerID=40&md5=de0b49ee88e7621807e8e139e691aeff
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 9</p>
KW  - Complex task
KW  - Dependability model
KW  - Dependability modeling
KW  - Electric power transmission networks
KW  - Heterogeneous component
KW  - Intelligent buildings
KW  - Internet of things
KW  - Investments
KW  - Network infrastructure
KW  - Off-grid solar
KW  - Power grids
KW  - Sensor device
KW  - Smart building
KW  - Smart power grids
KW  - Solar energy
KW  - Solar power
KW  - Solar Power Systems
KW  - Steady-state availability
KW  - Storage resources
ER  - 

TY  - JOUR
TI  - A Kubernetes controller for managing the availability of elastic microservice based stateful applications
AU  - Vayghan, L.A.
AU  - Saied, M.A.
AU  - Toeroe, M.
AU  - Khendek, F.
T2  - Journal of Systems and Software
AB  - The architectural style of microservices has been gaining popularity in recent years. In this architectural style, small and loosely coupled modules are deployed and scaled independently to compose cloud-native applications. Carrier-grade service providers are migrating their legacy applications to a microservice based architecture running on Kubernetes which is an open source platform for orchestrating containerized microservice based applications. However, in this migration, service availability remains a concern. Service availability is measured as the percentage of time the service is provisioned. High Availability (HA) is achieved when the service is available at least 99.999% of the time. In this paper, we identify possible architectures for deploying stateful microservice based applications with Kubernetes and evaluate Kubernetes from the perspective of availability it provides for its managed applications. The results of our experiments show that the repair actions of Kubernetes cannot satisfy HA requirements, and in some cases cannot guarantee service recovery. Therefore, we propose an HA State Controller which integrates with Kubernetes and allows for application state replication and automatic service redirection to the healthy microservice instances by enabling service recovery in addition to the repair actions of Kubernetes. Based on experiments we evaluate our solution and compare the different architectures from the perspective of availability and scaling overhead. The results of our investigations show that our solution can improve the recovery time of stateful microservice based applications by 50%. © 2021 Elsevier Inc.
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.jss.2021.110924
VL  - 175
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100910850&doi=10.1016%2fj.jss.2021.110924&partnerID=40&md5=b01d6d2e3e057f7b774c14580fc176f6
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 46</p>
KW  - Architectural style
KW  - Architecture
KW  - Availability
KW  - Carrier grade services
KW  - Containers
KW  - Elasticity
KW  - Failure
KW  - High availability
KW  - Kubernetes
KW  - Legacy applications
KW  - Microservices
KW  - Open source platforms
KW  - Recovery
KW  - Service availability
KW  - Service recoveries
KW  - State replications
ER  - 

TY  - JOUR
TI  - Testing microservice architectures for operational reliability
AU  - Pietrantuono, R.
AU  - Russo, S.
AU  - Guerriero, A.
T2  - Software Testing Verification and Reliability
AB  - Microservice architectures (MSA) is an emerging software architectural paradigm for service-oriented applications, well-suited for dynamic contexts requiring loosely coupled independent services, frequent software releases and decentralized governance. A key problem in the engineering of MSA applications is the estimate of their reliability, which is difficult to perform prior to release due frequent releases/service upgrades, dynamic service interactions, and changes in the way customers use the applications. This paper presents an in vivo testing method, named EMART, to faithfully assess the reliability of an MSA application in operation. EMART is based on an adaptive sampling strategy, leveraging monitoring data about microservices usage and failure/success of user demands. We present results of evaluation of estimation accuracy, confidence and efficiency, through a set of controlled experiments with publicly available subjects. © 2019 John Wiley & Sons, Ltd. Copyright © 2019 John Wiley & Sons, Ltd.
DA  - 2020///
PY  - 2020
DO  - 10.1002/stvr.1725
VL  - 30
IS  - 2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076794331&doi=10.1002%2fstvr.1725&partnerID=40&md5=6e46f8f124833347e7c779a0f125cdaa
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 19</p>
KW  - Application programs
KW  - Dynamic contexts
KW  - in vivo testing
KW  - In vivo testing
KW  - In-vivo
KW  - Loosely coupled
KW  - Memory architecture
KW  - microservice architecture
KW  - Microservice architecture
KW  - Operational reliability
KW  - Service oriented architecture (SOA)
KW  - Service-oriented applications
KW  - Software architectural
KW  - software reliability
KW  - Software reliability
KW  - Software testing
KW  - Software-Reliability
KW  - Testing
KW  - Vivo testing
ER  - 

TY  - JOUR
T2  - K3s: Lightweight Kubernetes
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084831492&partnerID=40&md5=b56aade0f7f680e0fe756089fce4b226
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 29</p>
ER  - 

TY  - JOUR
TI  - Assessing ICT global emissions footprint: Trends to 2040 & recommendations
AU  - Belkhir, L.
AU  - Elmeligi, A.
T2  - Journal of Cleaner Production
AB  - In light of the concerted efforts to reduce global greenhouse gas emissions (GHGE) per the so-called Paris Agreement, the Information and Communication Industry (ICT) has received little attention as a significant contributor to GHGE and if anything is often highly praised for enabling efficiencies that help reduce other industry sectors footprint. In this paper, we aim at assessing the global carbon footprint of the overall ICT industry, including the contribution from the main consumer devices, the data centers and communication networks, and compare it with the to the total worldwide GHGE. We conduct a detailed and rigorous analysis of the ICT global carbon footprint, including both the production and the operational energy of ICT devices, as well as the operational energy for the supporting ICT infrastructure. We then compare this contribution to the global 2016-level GHGE. We have found that, if unchecked, ICT GHGE relative contribution could grow from roughly 1–1.6% in 2007 to exceed 14% of the 2016-level worldwide GHGE by 2040, accounting for more than half of the current relative contribution of the whole transportation sector. Our study also highlights the contribution of smart phones and shows that by 2020, the footprint of smart phones alone would surpass the individual contribution of desktops, laptops and displays. Finally, we offer some actionable recommendations on how to mitigate and curb the ICT explosive GHGE footprint, through a combination of renewable energy use, tax policies, managerial actions and alternative business models. © 2018 Elsevier Ltd
DA  - 2018///
PY  - 2018
DO  - 10.1016/j.jclepro.2017.12.239
VL  - 177
SP  - 448
EP  - 463
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041215618&doi=10.1016%2fj.jclepro.2017.12.239&partnerID=40&md5=c91dee0ffd32a303af9af58776c4c37e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 694</p>
KW  - Carbon footprint
KW  - Energy utilization
KW  - Gas emissions
KW  - Greenhouse gases
KW  - ICT infrastructures
KW  - Information and communication
KW  - Managerial actions
KW  - Operational energy
KW  - Relative contribution
KW  - Renewable energy use
KW  - Rigorous analysis
KW  - Smartphones
KW  - Telephone sets
KW  - Transportation sector
ER  - 

TY  - JOUR
TI  - Availability analysis of design configurations to compose virtual performance-optimized data center systems in next-generation cloud data centers
AU  - Rosendo, D.
AU  - Gomes, D.
AU  - Leoni Santos, G.
AU  - Silva, L.
AU  - Moreira, A.
AU  - Kelner, J.
AU  - Sadok, D.
AU  - Gonçalves, G.
AU  - Mehta, A.
AU  - Wildeman, M.
AU  - Takako Endo, P.
T2  - Software - Practice and Experience
AB  - Next-generation cloud data centers are based on software-defined data center infrastructures that promote flexibility, automation, optimization, and scalability. The Redfish standard and the Intel Rack Scale Design technology enable software-defined infrastructure and disaggregate bare-metal compute, storage, and networking resources into virtual pools to dynamically compose resources and create virtual performance-optimized data centers (vPODs) tailored to workload-specific demands. This article proposes four chassis design configurations based on Distributed Management Task Force's Redfish industry standard applied to compose vPOD systems, namely, a fully shared design, partially shared homogeneous design, partially shared heterogeneous design, and not shared design; their main difference is based on the used hardware disaggregation level. Furthermore, we propose models that combine reliability block diagram and stochastic Petri net modeling approaches to represent the complexity of the relationship between the pool of disaggregated hardware resources and their power and cooling sources in a vPOD. These four proposed design configurations were analyzed and compared in terms of availability and component's sensitivity indexes by scaling their configurations considering different data center infrastructure. From the obtained results, we can state that, in general, when one increases the hardware disaggregation, availability is improved. However, after a given point, the availability level of the fully shared, partially shared homogeneous, and partially shared heterogeneous configurations remain almost equal, while the not shared configuration is still able to improve its availability. © 2020 John Wiley & Sons, Ltd.
DA  - 2020///
PY  - 2020
DO  - 10.1002/spe.2833
VL  - 50
IS  - 6
SP  - 805
EP  - 826
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084033362&doi=10.1002%2fspe.2833&partnerID=40&md5=a4d25b77122456edcc283b62f8c17e14
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 6</p>
KW  - availability
KW  - Availability
KW  - Cloud data centers
KW  - Composable
KW  - Composable system
KW  - composable systems
KW  - Data center systems
KW  - Datacenter
KW  - Design configurations
KW  - next-generation cloud data center
KW  - Next-generation cloud data center
KW  - Petri nets
KW  - Random access storage
KW  - Redfish standard
KW  - stochastic models
KW  - Stochastic models
KW  - Stochastic systems
KW  - Stochastic-modeling
KW  - Virtual performance
ER  - 

TY  - JOUR
TI  - Cocoa: Dynamic Container-Based Group Buying Strategies for Cloud Computing
AU  - Yi, X.
AU  - Liu, F.
AU  - Niu, D.
AU  - Jin, H.
AU  - Lui, J.C.S.
T2  - ACM Transactions on Modeling and Performance Evaluation of Computing Systems
AB  - Although the Infrastructure-as-a-Service (IaaS) cloud offers diverse instance types to users, a significant portion of cloud users, especially those with small and short demands, cannot find an instance type that exactly fits their needs or fully utilize purchased instance-hours. In the meantime, cloud service providers are also faced with the challenge to consolidate small, short jobs, which exhibit strong dynamics, to effectively improve resource utilization. To handle such inefficiencies and improve cloud resource utilization, we propose Cocoa (COmputing in COntAiners), a novel group buying mechanism that organizes jobs with complementary resource demands into groups and allocates them to group buying deals predefined by cloud providers. Each group buying deal offers a resource pool for all the jobs in the deal, which can be implemented as either a virtual machine or a physical server. By running each user job on a virtualized container, our mechanism allows flexible resource sharing among different users in the same group buying deal, while improving resource utilization for cloud providers. To organize jobs with varied resource demands and durations into groups, we model the initial static group organization as a variable-sized vector bin packing problem, and the subsequent dynamic group organization problem as an online multidimensional knapsack problem. Through extensive simulations driven by a large amount of real usage traces from a Google cluster, we evaluate the potential cost reduction achieved by Cocoa. We show that through the effective combination and interaction of the proposed static and dynamic group organization strategies, Cocoa greatly outperforms the existing cloud workload consolidation mechanism, substantiating the feasibility of group buying in cloud computing. © 2017 ACM.
DA  - 2017///
PY  - 2017
DO  - 10.1145/3022876
VL  - 2
IS  - 2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043693914&doi=10.1145%2f3022876&partnerID=40&md5=69808f781d2dc7172257db8695f570c8
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 16</p>
KW  - Bin packing problem
KW  - Cloud service providers
KW  - Cocoa
KW  - Combinatorial optimization
KW  - container
KW  - Containers
KW  - Cost reduction
KW  - cost saving
KW  - Dynamics
KW  - Extensive simulations
KW  - Flexible resources
KW  - Group buying
KW  - Infrastructure as a service (IaaS)
KW  - Multidimensional knapsack problems
KW  - Resource demands
KW  - Resource utilizations
KW  - Workload consolidation
ER  - 

TY  - JOUR
TI  - A cloud-agnostic queuing system to support the implementation of deadline-based application execution policies
AU  - Kiss, T.
AU  - DesLauriers, J.
AU  - Gesmier, G.
AU  - Terstyanszky, G.
AU  - Pierantoni, G.
AU  - Oun, O.A.
AU  - Taylor, S.J.E.
AU  - Anagnostou, A.
AU  - Kovacs, J.
T2  - Future Generation Computer Systems
AB  - There are many scientific and commercial applications that require the execution of a large number of independent jobs resulting in significant overall execution time. Therefore, such applications typically require distributed computing infrastructures and science gateways to run efficiently and to be easily accessible for end-users. Optimising the execution of such applications in a cloud computing environment by keeping resource utilisation at minimum but still completing the experiment by a set deadline has paramount importance. As container-based technologies are becoming more widespread, support for job-queuing and auto-scaling in such environments is becoming important. Current container management technologies, such as Docker Swarm or Kubernetes, while provide auto-scaling based on resource consumption, do not support job queuing and deadline-based execution policies directly. This paper presents JQueuer, a cloud-agnostic queuing system that supports the scheduling of a large number of jobs in containerised cloud environments. The paper also demonstrates how JQueuer, when integrated with a cloud application-level orchestrator and auto-scaling framework, called MiCADO, can be used to implement deadline-based execution policies. This novel technical solution provides an important step towards the cost-optimisation of batch processing and job submission applications. In order to test and prove the effectiveness of the solution, the paper presents experimental results when executing an agent-based simulation application using the open source REPAST simulation framework. © 2019 The Authors
DA  - 2019///
PY  - 2019
DO  - 10.1016/j.future.2019.05.062
VL  - 101
SP  - 99
EP  - 111
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067198184&doi=10.1016%2fj.future.2019.05.062&partnerID=40&md5=19f2026013d0f9631667f7bbf2eac555
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 16</p>
KW  - Agent based simulation
KW  - Agent-based simulation
KW  - Application execution
KW  - Batch data processing
KW  - Cloud computing
KW  - Cloud computing environments
KW  - Commercial applications
KW  - Computing infrastructures
KW  - Container technologies
KW  - Containers
KW  - Deadline-based auto-scaling
KW  - JQueuer
KW  - MiCADO
KW  - Queueing networks
KW  - Queueing theory
ER  - 

TY  - CONF
TI  - An Open-Source Benchmark Suite for Microservices and Their Hardware-Software Implications for Cloud & Edge Systems
AU  - Gan, Y.
AU  - Zhang, Y.
AU  - Cheng, D.
AU  - Shetty, A.
AU  - Rathi, P.
AU  - Katarki, N.
AU  - Bruno, A.
AU  - Hu, J.
AU  - Ritchken, B.
AU  - Jackson, B.
AU  - Hu, K.
AU  - Pancholi, M.
AU  - He, Y.
AU  - Clancy, B.
AU  - Colen, C.
AU  - Wen, F.
AU  - Leung, C.
AU  - Wang, S.
AU  - Zaruvinsky, L.
AU  - Espinosa, M.
AU  - Lin, R.
AU  - Liu, Z.
AU  - Padilla, J.
AU  - Delimitrou, C.
T2  - International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS
AB  - Cloud services have recently started undergoing a major shift from monolithic applications, to graphs of hundreds of loosely-coupled microservices. Microservices fundamentally change a lot of assumptions current cloud systems are designed with, and present both opportunities and challenges when optimizing for quality of service (QoS) and utilization. In this paper we explore the implications microservices have across the cloud system stack. We first present Death- StarBench, a novel, open-source benchmark suite built with microservices that is representative of large end-to-end services, modular and extensible. DeathStarBench includes a social network, a media service, an e-commerce site, a banking system, and IoT applications for coordination control of UAV swarms. We then use DeathStarBench to study the architectural characteristics of microservices, their implications in networking and operating systems, their challenges with respect to cluster management, and their trade-offs in terms of application design and programming frameworks. Finally, we explore the tail at scale effects of microservices in real deployments with hundreds of users, and highlight the increased pressure they put on performance predictability. © 2019 Association for Computing Machinery.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3297858.3304013
SP  - 3
EP  - 18
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064688619&doi=10.1145%2f3297858.3304013&partnerID=40&md5=b9f662fba2a35d83eb40d32009cf7676
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 538</p>
KW  - acceleration
KW  - Benchmark suites
KW  - cloud computing
KW  - Cloud systems
KW  - Cloud-computing
KW  - Cluster computing
KW  - cluster management
KW  - Cluster management
KW  - Datacenter
KW  - datacenters
KW  - Economic and social effects
KW  - Field programmable gate arrays (FPGA)
KW  - fpga
KW  - Fpgum
KW  - Microservice
KW  - microservices
KW  - Open source software
KW  - Open systems
KW  - Open-source
KW  - QoS
KW  - Quality of service
KW  - Quality-of-service
KW  - serverless
KW  - Serverless
ER  - 

TY  - JOUR
TI  - A view of cloud computing
AU  - Armbrust, M.
AU  - Fox, A.
AU  - Griffith, R.
AU  - Joseph, A.D.
AU  - Katz, R.
AU  - Konwinski, A.
AU  - Lee, G.
AU  - Patterson, D.
AU  - Rabkin, A.
AU  - Stoica, I.
AU  - Zaharia, M.
T2  - Communications of the ACM
AB  - CLOUD COMPUTING, the long-held dream of computing as a utility, has the potential to transform a large part of the IT industry, making software even more attractive as a service and shaping the way IT hardware is designed and purchased. Developers with innovative ideas for new Internet services no longer require the large capital outlays in hardware to deploy their service or the human expense to operate it. They need not be concerned about overprovisioning for a service whose popularity does not meet their predictions, thus wasting costly resources, or underprovisioning for one that becomes wildly popular, thus missing potential customers and revenue. Moreover, companies with large batch-oriented tasks can get results as quickly as their programs can scale, since using 1,000 servers for one hour costs no more than using one server for 1,000. © 2010 ACM.
DA  - 2010///
PY  - 2010
DO  - 10.1145/1721654.1721672
VL  - 53
IS  - 4
SP  - 50
EP  - 58
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950347409&doi=10.1145%2f1721654.1721672&partnerID=40&md5=6164f58679f057d5164d1f8f31d3f125
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 7570</p>
KW  - Capital outlay
KW  - Cloud computing
KW  - Innovative ideas
KW  - Internet
KW  - Internet services
KW  - IT industry
KW  - Large parts
KW  - Potential customers
KW  - Servers
ER  - 

TY  - JOUR
TI  - Communication-Aware Container Placement and Reassignment in Large-Scale Internet Data Centers
AU  - Lv, L.
AU  - Zhang, Y.
AU  - Li, Y.
AU  - Xu, K.
AU  - Wang, D.
AU  - Wang, W.
AU  - Li, M.
AU  - Cao, X.
AU  - Liang, Q.
T2  - IEEE Journal on Selected Areas in Communications
AB  - Containerization has been used in many applications for isolation purposes due to its lightweight, scalable, and highly portable properties. However, to apply containerization in large-scale Internet data centers faces a big challenge. Services in data centers are always instantiated as a group of containers, which often generate heavy communication workloads and therefore resulting in inefficient communications and downgraded service performance. Although assigning the containers of the same service to the same server can reduce the communication overhead, this may cause heavily imbalanced resource utilization since containers of the same service are usually intensive to the same resource. To reduce communication cost as well as balance the resource utilization in large-scale data centers, we further explore the container distribution issues in a real industrial environment and find that such conflict lies in two phases-container placement and container reassignment. The objective of this paper is to address the container distribution problem in these two phases. For the container placement problem, we propose an efficient communication aware worst fit decreasing algorithm to place a set of new containers into data centers. For the container reassignment problem, we propose a two-stage algorithm called SweepSearch to optimize a given initial distribution of containers by migrating containers among servers. We implement the proposed algorithms in Baidu's data centers and conduct extensive evaluations. Compared with the state-of-The-Art strategies, the evaluation results show that our algorithms perform better up to 70% and increase the overall service throughput up to 90% simultaneously. © 1983-2012 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/JSAC.2019.2895473
VL  - 37
IS  - 3
SP  - 540
EP  - 555
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061303318&doi=10.1109%2fJSAC.2019.2895473&partnerID=40&md5=f1a368136c670b0d0e1dafb1d960a554
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 86</p>
KW  - Container communication
KW  - container placement
KW  - container reassignment
KW  - Containers
KW  - Cost reduction
KW  - Data centers
KW  - Distributed computer systems
KW  - Information management
KW  - Internet
KW  - Large scale data
KW  - large-scale data centers
KW  - Load balance
KW  - multi-resource load balance
KW  - Packaging
KW  - Resource management
KW  - Servers
KW  - Throughput
KW  - Time factors
ER  - 

TY  - JOUR
TI  - Interconnected cloud computing environments: Challenges, taxonomy, and survey
AU  - Toosi, A.N.
AU  - Calheiros, R.N.
AU  - Buyya, R.
T2  - ACM Computing Surveys
AB  - A brief review of the Internet history reveals the fact that the Internet evolved after the formation of primarily independent networks. Similarly, interconnected clouds, also called Inter-cloud, can be viewed as a natural evolution of cloud computing. Recent studies show the benefits in utilizing multiple clouds and present attempts for the realization of an Inter-cloud or federated cloud environment. However, cloud vendors have not taken into account cloud interoperability issues, and each cloud comes with its own solution and interfaces for services. This survey initially discusses all the relevant aspects motivating cloud interoperability. Furthermore, it categorizes and identifies possible cloud interoperability scenarios and architectures. The spectrum of challenges and obstacles that the Inter-cloud realization is faced with are covered, a taxonomy of them is provided, and fitting enablers that tackle each challenge are identified. All these aspects require a comprehensive review of the state of the art, including ongoing projects and studies in the area. We conclude by discussing future directions and trends toward the holistic approach in this regard. © 2014 ACM.
DA  - 2014///
PY  - 2014
DO  - 10.1145/2593512
VL  - 47
IS  - 1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905817532&doi=10.1145%2f2593512&partnerID=40&md5=697eb8026fa5a768ebc718a86bee7597
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 377</p>
KW  - Cloud computing
KW  - Cloud computing environments
KW  - Cloud environments
KW  - Cloud federation
KW  - Cloud federations
KW  - Cloud-computing
KW  - Cross-cloud
KW  - Cross-clouds
KW  - Federated clouds
KW  - Inter clouds
KW  - Inter-cloud
KW  - Interoperability
KW  - Multi-cloud
KW  - Multi-clouds
KW  - Natural evolution
KW  - Surveys
KW  - Taxonomies
KW  - Utility computing
ER  - 

TY  - JOUR
TI  - MiCADO—Microservice-based Cloud Application-level Dynamic Orchestrator
AU  - Kiss, T.
AU  - Kacsuk, P.
AU  - Kovacs, J.
AU  - Rakoczi, B.
AU  - Hajnal, A.
AU  - Farkas, A.
AU  - Gesmier, G.
AU  - Terstyanszky, G.
T2  - Future Generation Computer Systems
AB  - Various scientific and commercial applications require automated scalability and orchestration on cloud computing resources. However, extending applications with such automated scalability on an individual basis is not feasible. This paper investigates how such automated orchestration can be added to cloud applications without major reengineering of the application code. We suggest a generic architecture for an application level cloud orchestration framework, called MiCADO that supports various application scenarios on multiple heterogeneous federated clouds. Besides the generic architecture description, the paper also presents the first MiCADO reference implementation, and explains how the scalability of the Data Avenue service that is applied for data transfer in WS-PGRADE/gUSE based science gateways, can be improved. Performance evaluation of the implemented scalability based on up and downscaling experiments is presented. © 2017 Elsevier B.V.
DA  - 2019///
PY  - 2019
DO  - 10.1016/j.future.2017.09.050
VL  - 94
SP  - 937
EP  - 946
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030791545&doi=10.1016%2fj.future.2017.09.050&partnerID=40&md5=9c76c61725df8d55d77b845b82b28205
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 69</p>
KW  - Application scenario
KW  - Automated scalability
KW  - Automation
KW  - Cloud applications
KW  - Cloud orchestration
KW  - Commercial applications
KW  - Data Avenue
KW  - Data transfer
KW  - Generic architecture
KW  - Performance evaluations
KW  - Reference implementation
KW  - Scalability
KW  - Science gateway
ER  - 

TY  - CONF
TI  - Market-oriented cloud computing: Vision, hype, and reality for delivering IT services as computing utilities
AU  - Buyya, R.
AU  - Yeo, C.S.
AU  - Venugopal, S.
T2  - Proceedings - 10th IEEE International Conference on High Performance Computing and Communications, HPCC 2008
AB  - This keynote paper: presents a 21st century vision of computing; identifies various computing paradigms promising to deliver the vision of computing utilities; defines Cloud computing and provides the architecture for creating market-oriented Clouds by leveraging technologies such as VMs; provides thoughts on market-based resource management strategies that encompass both customer-driven service management and computational risk management to sustain SLA-oriented resource allocation; presents some representative Cloud platforms especially those developed in industries along with our current work towards realising market-oriented resource allocation of Clouds by leveraging the 3 rd generation Aneka enterprise Grid technology; reveals our early thoughts on interconnecting Clouds for dynamically creating an atmospheric computing environment along with pointers to future community research; and concludes with the need for convergence of competing IT paradigms for delivering our 21st century vision. © 2008 IEEE.
DA  - 2008///
PY  - 2008
DO  - 10.1109/HPCC.2008.172
SP  - 5
EP  - 13
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349150824&doi=10.1109%2fHPCC.2008.172&partnerID=40&md5=6c72d9bf489047e57ce934a25be09075
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1462</p>
KW  - Clouds
KW  - Computational risk managements
KW  - Computer systems
KW  - Computing environments
KW  - Computing paradigms
KW  - Grid computing
KW  - GRID technologies
KW  - High performance liquid chromatography
KW  - Information management
KW  - It services
KW  - Management
KW  - Marketing
KW  - Planning
KW  - Resource allocation
KW  - Resource Management strategies
KW  - Risk analysis
KW  - Risk management
KW  - Service managements
ER  - 

TY  - JOUR
TI  - A Genetic Algorithm-Based Energy-Efficient Container Placement Strategy in CaaS
AU  - Zhang, R.
AU  - Chen, Y.
AU  - Dong, B.
AU  - Tian, F.
AU  - Zheng, Q.
T2  - IEEE Access
AB  - Container placement (CP) is a nontrivial problem in Container as a Service (CaaS). Many works in the literature solve it by using linear server energy-consumption models. However, the solutions of using a linear model makes different CPs indistinguishable with regard to energy consumption in a homogeneous host environment that has a same amount of active hosts. As such, these solutions are energy inefficient. In this paper, we demonstrate that an energy-saving gain can be achieved by optimizing the placement of containers under a nonlinear energy consumption model. Specifically, we leverage a strategy based on genetic algorithm (GA) to search the optimal solution. Unfortunately, the conventional GA incurs performance degradation when the virtual machine (VM) resource utilization is high. In order to solve this problem, we propose an improved genetic algorithm called IGA for efficiently searching the optimal CP solution by introducing two different exchange mutation operations and constructing a function as the control parameter to selectively control the usage of the two operations. Extensive experiments are carried out under different settings, and their results show that our strategy is better than the existing CP strategies, i.e., spread and binpack, on energy efficiency target. In addition, the introduced IGA is experimentally proved to be more effective compared with the First Fit, Particle Swarm Optimization (PSO) algorithm and conventional GA. Moreover, the results validate that our proposed strategy can search new CP solutions with better fitness and alleviate the performance degradation caused by the conventional GA when the VM resource utilization is high.  © 2013 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/ACCESS.2019.2937553
VL  - 7
SP  - 121360
EP  - 121373
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090845423&doi=10.1109%2fACCESS.2019.2937553&partnerID=40&md5=25bbbc9a62d74b7a5a06cb7c854d05f0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 39</p>
KW  - CaaS
KW  - container placement
KW  - Containers
KW  - Control parameters
KW  - Energy consumption model
KW  - Energy efficiency
KW  - Energy utilization
KW  - exchange mutation operation
KW  - genetic algorithm
KW  - Genetic algorithms
KW  - Green computing
KW  - Mutation operations
KW  - Optimal solutions
KW  - Particle swarm optimization (PSO)
KW  - Particle swarm optimization algorithm
KW  - Performance degradation
KW  - Placement strategy
KW  - Resource utilizations
KW  - Virtual machine
ER  - 

TY  - JOUR
TI  - Optimization of Business Process Execution in Services Architecture: A Systematic Literature Review
AU  - Gorski, T.
AU  - Wozniak, A.P.
T2  - IEEE Access
AB  - Web services have become a standard way to provide functions of information systems. The number of web services grows rapidly with the increasing popularity of microservices architecture. In consequence, many business processes are executed entirely through web services. Therefore, optimizing the performance of business process execution may bring many benefits. There are many optimization methods in this area. Our systematic literature review aims to introduce available methods to researchers interested in the optimization of business process execution. We queried four databases: ACM, IEEE Xplore, Science Direct, and Springer. Out of 12150 initially found papers, we have selected 128 for the review. We have grouped methods presented in those papers into three stages of business process optimization: Resource Allocation, Service Composition, and Service Scheduling. Service Composition attracts the largest group of researchers with a vast majority of 119 articles in it. Moreover, the most popular are genetic algorithms. In general, researchers mainly propose heuristic methods that optimize business processes during run-time. We see the potential for further exploration at both Resource Allocation and Service Scheduling stages.  © 2013 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/ACCESS.2021.3102668
VL  - 9
SP  - 111833
EP  - 111852
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112200345&doi=10.1109%2fACCESS.2021.3102668&partnerID=40&md5=4508370dfbbae73b1b4330dcc8a52caf
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 18</p>
KW  - business process
KW  - Business Process
KW  - Business process execution
KW  - Business process optimization
KW  - Genetic algorithms
KW  - Heuristic methods
KW  - micro-services
KW  - optimization
KW  - Optimization method
KW  - Quality of service
KW  - reliability
KW  - Resource allocation
KW  - Scheduling
KW  - Service compositions
KW  - Service-oriented architecture
KW  - Service-scheduling
KW  - Services Architectures
KW  - Systematic literature review
KW  - Web services
KW  - Websites
ER  - 

TY  - JOUR
TI  - Structure-aware online virtual machine consolidation for datacenter energy improvement in cloud computing
AU  - Esfandiarpoor, S.
AU  - Pahlavan, A.
AU  - Goudarzi, M.
T2  - Computers and Electrical Engineering
AB  - The necessity and significance of improving the energy efficiency of cloud implementations have increased due to the rapid growth and proliferation of cloud computing services around the world. Virtual machines (VMs) comprise the backend of most, if not all, cloud computing services. Several VMs are often consolidated on a physical machine to efficiently utilize its resources. In this paper, we take into account the cooling and network structure of the datacenter host ing the physical machines when consolidating the VMs so that fewer racks and routers are employed, without compromising the service-level agreements; consequently, idle routing and cooling equipment can be turned off in order to reduce the energy consumption. Our experimental results on four benchmarks show that the proposed techniques improve energy consumption of servers, network equipment, and cooling systems by 2.5%, 18.8%, and 28.2% respectively compared to state of the art, resulting in a total of 14.7% energy improvement on average in the entire datacenter. © 2014 Elsevier Ltd. All rights reserved.
DA  - 2015///
PY  - 2015
DO  - 10.1016/j.compeleceng.2014.09.005
VL  - 42
SP  - 74
EP  - 89
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961340594&doi=10.1016%2fj.compeleceng.2014.09.005&partnerID=40&md5=8d4fdd17fab9ba30c33ea94b18c70752
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 84</p>
KW  - Cloud computing
KW  - Cloud computing services
KW  - Computer simulation
KW  - Consolidation
KW  - Cooling
KW  - Cooling equipment
KW  - Cooling systems
KW  - Datacenter
KW  - Energy efficiency
KW  - Energy utilization
KW  - Java programming language
KW  - Network equipment
KW  - Network structures
KW  - Service Level Agreements
KW  - Service-level agreements
KW  - Virtual machine
KW  - Virtual machine consolidations
KW  - Virtual machines
ER  - 

TY  - JOUR
TI  - Concurrent container scheduling on heterogeneous clusters with multi-resource constraints
AU  - Hu, Y.
AU  - Zhou, H.
AU  - de Laat, C.
AU  - Zhao, Z.
T2  - Future Generation Computer Systems
AB  - By effectively virtualizing operating systems and encapsulating necessary runtime contexts of software components and services, container technologies can significantly improve portability and efficiency for distributed application deployment. It flexibly extends virtual machine based cloud (Infrastructure-as-a-Service) as a much lighter virtual environment (container cluster) for agile application management. However, existing container management systems are not capable of handling concurrent requests efficiently, particularly for the underlying clusters with heterogeneous machines and the requested containers with multi-resource demands. In this paper, we propose an Enhanced Container Scheduler (ECSched) for efficiently scheduling concurrent container requests on heterogeneous clusters with multi-resource constraints. We formulate the container scheduling problem as a minimum cost flow problem (MCFP), and represent the container requirements using a specific graph data structure (flow network). ECSched affords flexibility in constructing the flow network based on a batch of concurrent requests, and performs the MCFP algorithm to schedule the concurrent requests in an online manner. We evaluate ECSched in different testbed clusters, and measure the scheduling overhead with large-scale simulations. The experimental results show that ECSched outperforms state-of-the-art container schedulers in container performance and resource efficiency, and only introduces a small and acceptable scheduling overhead in large-scale clusters. © 2019
DA  - 2020///
PY  - 2020
DO  - 10.1016/j.future.2019.08.025
VL  - 102
SP  - 562
EP  - 573
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072185441&doi=10.1016%2fj.future.2019.08.025&partnerID=40&md5=d478ea29948a8e62e10945cbf4ffaae5
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 67</p>
KW  - Application management
KW  - Application programs
KW  - Computer software portability
KW  - Concurrent scheduling
KW  - Container
KW  - Container management
KW  - Containers
KW  - Cost accounting
KW  - Distributed applications
KW  - Efficiency
KW  - Flow graphs
KW  - Flow measurement
KW  - Heterogeneous cluster
KW  - Heterogeneous clusters
KW  - Infrastructure as a service (IaaS)
KW  - Large scale simulations
KW  - Minimum cost flow problem
KW  - Multi-resource constraints
KW  - Resource efficiencies
KW  - Scheduling
KW  - Scheduling algorithms
KW  - Virtual reality
ER  - 

TY  - JOUR
TI  - Dynamic energy-aware scheduling for parallel task-based application in cloud computing
AU  - Juarez, F.
AU  - Ejarque, J.
AU  - Badia, R.M.
T2  - Future Generation Computer Systems
AB  - Green Computing is a recent trend in computer science, which tries to reduce the energy consumption and carbon footprint produced by computers on distributed platforms such as clusters, grids, and clouds. Traditional scheduling solutions attempt to minimize processing times without taking into account the energetic cost. One of the methods for reducing energy consumption is providing scheduling policies in order to allocate tasks on specific resources that impact over the processing times and energy consumption. In this paper, we propose a real-time dynamic scheduling system to execute efficiently task-based applications on distributed computing platforms in order to minimize the energy consumption. Scheduling tasks on multiprocessors is a well known NP-hard problem and optimal solution of these problems is not feasible, we present a polynomial-time algorithm that combines a set of heuristic rules and a resource allocation technique in order to get good solutions on an affordable time scale. The proposed algorithm minimizes a multi-objective function which combines the energy-consumption and execution time according to the energy-performance importance factor provided by the resource provider or user, also taking into account sequence-dependent setup times between tasks, setup times and down times for virtual machines (VM) and energy profiles for different architectures. A prototype implementation of the scheduler has been tested with different kinds of DAG generated at random as well as on real task-based COMPSs applications. We have tested the system with different size instances and importance factors, and we have evaluated which combination provides a better solution and energy savings. Moreover, we have also evaluated the introduced overhead by measuring the time for getting the scheduling solutions for a different number of tasks, kinds of DAG, and resources, concluding that our method is suitable for run-time scheduling. © 2016 Elsevier B.V.
DA  - 2018///
PY  - 2018
DO  - 10.1016/j.future.2016.06.029
VL  - 78
SP  - 257
EP  - 271
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003666406&doi=10.1016%2fj.future.2016.06.029&partnerID=40&md5=a674981d2df0280fb716d55d5ac0ec41
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 188</p>
KW  - Carbon
KW  - Carbon footprint
KW  - Cloud computing
KW  - Computational complexity
KW  - Computer resource management
KW  - Distributed computer systems
KW  - Distributed computing
KW  - Distributed computing platform
KW  - Energy conservation
KW  - Energy policy
KW  - Energy utilization
KW  - Energy-aware scheduling
KW  - Green computing
KW  - Multi-heuristic resource allocation
KW  - Optimization
KW  - Polynomial approximation
KW  - Polynomial-time algorithms
KW  - Polynomials
KW  - Power management
KW  - Reducing energy consumption
KW  - Resource allocation
KW  - Resource allocation techniques
KW  - Scheduling
KW  - Sequence-dependent setup time
KW  - Task-based
KW  - Task-based applications
ER  - 

TY  - JOUR
TI  - Profile-based power-aware workflow scheduling framework for energy-efficient data centers
AU  - Qureshi, B.
T2  - Future Generation Computer Systems
AB  - In the age of big data, software-as-a-service (SaaS) clouds provide heterogeneous and multitenant utilization of underlying virtual environments in data centers. Real-time and parallel deployment of applications with data-intensive workloads of various sizes pose challenges in optimal resource scheduling, power utilization, task completion time, network latency, and so on, causing degradation in the quality of service and affecting the user experience. In this paper, we investigate the role of application profiles in addressing the tradeoff between performance and energy efficiency of small- to medium-scale data centers. A power-aware framework for efficient placement of application workloads in the data center is proposed. The framework considers various application workflow constraints, such as CPU, memory, network I/O, and power consumption requirements to develop realistic profiles of application workloads. A system model for the efficient workflow assignment in the data center using a novel scheduler algorithm is presented. The performance of the proposed scheduler is validated through simulation studies. We compare the proposed scheduler with two scheduling algorithms: robust time cost (RTC) and heterogeneous earliest finish time (HEFT). Results show that the proposed scheduler is 19% and 38% more energy efficient than RTC and HEFT, respectively for medium–large sized workloads. © 2018 Elsevier B.V.
DA  - 2019///
PY  - 2019
DO  - 10.1016/j.future.2018.11.010
VL  - 94
SP  - 453
EP  - 467
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058448825&doi=10.1016%2fj.future.2018.11.010&partnerID=40&md5=dde9da3a5ed903e8ba5fc78a376476be
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 46</p>
KW  - Application-based profiles
KW  - Big data
KW  - Cloud computing
KW  - Data center
KW  - Data centers
KW  - Data-intensive workloads
KW  - Electric power utilization
KW  - Energy efficiency
KW  - Green computing
KW  - Hadoop
KW  - Network latencies
KW  - Power management
KW  - Quality of service
KW  - Resource-scheduling
KW  - Scheduling
KW  - Scheduling algorithms
KW  - Simulation studies
KW  - Software as a service (SaaS)
KW  - Task completion time
KW  - Virtual reality
KW  - Workflow scheduling
ER  - 

TY  - JOUR
TI  - Energy-efficient load balancing algorithm for workflow scheduling in cloud data centers using queuing and thresholds
AU  - Malik, N.
AU  - Sardaraz, M.
AU  - Tahir, M.
AU  - Shah, B.
AU  - Ali, G.
AU  - Moreira, F.
T2  - Applied Sciences (Switzerland)
AB  - Cloud computing is a rapidly growing technology that has been implemented in various fields in recent years, such as business, research, industry, and computing. Cloud computing provides different services over the internet, thus eliminating the need for personalized hardware and other resources. Cloud computing environments face some challenges in terms of resource utilization, energy efficiency, heterogeneous resources, etc. Tasks scheduling and virtual machines (VMs) are used as consolidation techniques in order to tackle these issues. Tasks scheduling has been extensively studied in the literature. The problem has been studied with different parameters and objectives. In this article, we address the problem of energy consumption and efficient resource utilization in virtualized cloud data centers. The proposed algorithm is based on task classification and thresholds for efficient scheduling and better resource utilization. In the first phase, workflow tasks are pre-processed to avoid bottlenecks by placing tasks with more dependencies and long execution times in separate queues. In the next step, tasks are classified based on the intensities of the required resources. Finally, Particle Swarm Optimization (PSO) is used to select the best schedules. Experiments were performed to validate the proposed technique. Comparative results obtained on benchmark datasets are presented. The results show the effectiveness of the proposed algorithm over that of the other algorithms to which it was compared in terms of energy consumption, makespan, and load balancing. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
DA  - 2021///
PY  - 2021
DO  - 10.3390/app11135849
VL  - 11
IS  - 13
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109127901&doi=10.3390%2fapp11135849&partnerID=40&md5=5624f4ac5b1050a253be087f8092f18b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 45</p>
KW  - Cloud computing
KW  - Energy consumption
KW  - Load balancing
KW  - Makespan
KW  - PSO
KW  - Task scheduling
ER  - 

TY  - JOUR
TI  - Energy-Efficient virtual machine selection based on resource ranking and utilization factor approach in cloud computing for IoT
AU  - Mekala, M.S.
AU  - Viswanathan, P.
T2  - Computers and Electrical Engineering
AB  - IoT leads to abrupt variations producing an immense number of data streams for storage, which is a considerable task in the heterogeneous cloud computing environment. Extant techniques consider task deadlines for virtual machine (VM) allocation and migration. This creates a resource famine leading to haphazard and numerous VM migrations, high energy consumption and unbalanced resource utilization. To solve this issue, an energy-efficient resource ranking and utilization factor-based virtual machine selection (ERVS) approach is proposed. ERVS encompasses the resource requirement rate for task classification, comprehensive resource balance ranking, processing element cost and the resource utilization square model for migration. It evaluates overloaded and underloaded hosts and types of VM by predicting CPU utilization rate and energy consumption. Based on this, tasks are sorted and VMs are optimally assigned, which enhances the resource utilization rate, reducing the number of live VM migrations. The experiments evaluate the ability of the proposed approach to diminish energy consumption without violation of service level agreements. © 2018 Elsevier Ltd
DA  - 2019///
PY  - 2019
DO  - 10.1016/j.compeleceng.2018.11.021
VL  - 73
SP  - 227
EP  - 244
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057774182&doi=10.1016%2fj.compeleceng.2018.11.021&partnerID=40&md5=0ab7ebaaa39b38c97a6c989622e8e443
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 62</p>
KW  - Cloud computing
KW  - CRB Ranking
KW  - Digital storage
KW  - Energy efficiency
KW  - Energy utilization
KW  - Food supply
KW  - Internet of things
KW  - Network security
KW  - Processing element cost
KW  - Processing elements
KW  - Resource requirement rate
KW  - Resource requirements
KW  - Task categorization
KW  - Virtual machine
KW  - VM Migration
KW  - Vm migrations
ER  - 

TY  - CONF
TI  - Horizontal and vertical scaling of container-based applications using reinforcement learning
AU  - Rossi, F.
AU  - Nardelli, M.
AU  - Cardellini, V.
T2  - IEEE International Conference on Cloud Computing, CLOUD
AB  - Software containers are changing the way distributed applications are executed and managed on cloud computing resources. Interestingly, containers offer the possibility of handling workload fluctuations by exploiting both horizontal and vertical elasticity 'on the fly'. However, most of the existing control policies consider horizontal and vertical scaling as two disjointed control knobs. In this paper, we propose Reinforcement Learning (RL) solutions for controlling the horizontal and vertical elasticity of container-based applications with the goal to increase the flexibility to cope with varying workloads. Although RL represents an interesting approach, it may suffer from a possible long learning phase, especially when nothing about the system is known a-priori. To speed up the learning process and identify better adaptation policies, we propose RL solutions that exploit different degrees of knowledge about the system dynamics (i.e., Q-learning, Dyna-Q, and Model-based). We integrate the proposed policies in Elastic Docker Swarm, our extension that introduces self-adaptation capabilities in the container orchestration tool Docker Swarm. We demonstrate the effectiveness and flexibility of model-based RL policies through simulations and prototype-based experiments. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/CLOUD.2019.00061
VL  - 2019-July
SP  - 329
EP  - 338
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072308220&doi=10.1109%2fCLOUD.2019.00061&partnerID=40&md5=a79b3944fa91c9bd5f10915a4892e490
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 141</p>
KW  - Adaptation policies
KW  - Application programs
KW  - Cloud computing
KW  - Container
KW  - Containers
KW  - Distributed applications
KW  - Docker
KW  - Elasticity
KW  - Learning process
KW  - Machine learning
KW  - Model-based OPC
KW  - Reinforcement learning
KW  - Reinforcement Learning
KW  - Self adaptation
KW  - System Dynamics
KW  - Vertical scaling
ER  - 

TY  - JOUR
TI  - BrownoutCon: A software system based on brownout and containers for energy-efficient cloud computing
AU  - Xu, M.
AU  - Buyya, R.
T2  - Journal of Systems and Software
AB  - VM consolidation and Dynamic Voltage Frequency Scaling approaches have been proved to be efficient to reduce energy consumption in cloud data centers. However, the existing approaches cannot function efficiently when the whole data center is overloaded. An approach called brownout has been proposed to solve the limitation, which dynamically deactivates or activates optional microservices or containers. In this paper, we propose a brownout-based software system for container-based clouds to handle overloads and reduce power consumption. We present its design and implementation based on Docker Swarm containers. The proposed system is integrated with existing Docker Swarm without the modification of their configurations. To demonstrate the potential of BrownoutCon software in offering energy-efficient services in brownout situation, we implemented several policies to manage containers and conducted experiments on French Grid’5000 cloud infrastructure. The results show the currently implemented policies in our software system can save about 10%–40% energy than the existing baselines while ensuring quality of services. © 2019 Elsevier Inc.
DA  - 2019///
PY  - 2019
DO  - 10.1016/j.jss.2019.05.031
VL  - 155
SP  - 91
EP  - 103
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065894559&doi=10.1016%2fj.jss.2019.05.031&partnerID=40&md5=c35adecba4fa8eadc6b68d173fb3f2ef
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 32</p>
KW  - Brownout
KW  - Cloud data centers
KW  - Cloud infrastructures
KW  - Computer software
KW  - Containers
KW  - Design and implementations
KW  - Distributed computer systems
KW  - Dynamic frequency scaling
KW  - Dynamic voltage frequency scaling
KW  - Energy efficiency
KW  - Energy utilization
KW  - Green computing
KW  - Microservices
KW  - Quality of service
KW  - Reduce energy consumption
KW  - Software systems
KW  - Voltage scaling
ER  - 

TY  - JOUR
TI  - Evaluating service-oriented and microservice architecture patterns to deploy ehealth applications in cloud computing environment
AU  - Calderón-Gómez, H.
AU  - Mendoza-Pittí, L.
AU  - Vargas-Lombardo, M.
AU  - Gómez-Pulido, J.M.
AU  - Rodríguez-Puyol, D.
AU  - Sención, G.
AU  - Polo-Luque, M.-L.
T2  - Applied Sciences (Switzerland)
AB  - This article proposes a new framework for a Cloud-based eHealth platform concept focused on Cloud computing environments, since current and emerging approaches using digital clinical history increasingly demonstrate their potential in maintaining the quality of the benefits in medical care services, especially in computer-assisted clinical diagnosis within the field of infectious diseases and due to the worsening of chronic pathologies. Our objective is to evaluate and contrast the performance of the architectural patterns most commonly used for developing eHealth applications (i.e., service-oriented architecture (SOA) and microservices architecture (MSA)), using as reference the quantitative values obtained from the various performance tests and their ability to adapt to the required software attribute (i.e., versatile high-performance). Therefore, it was necessary to modify our platform to fit two architectural variants. As a follow-up to this activity, corresponding tests were performed that showed that the MSA variant functions better in terms of performance and response time compared to the SOA variant; however, it consumed significantly more bandwidth than SOA, and scalability in SOA is generally not possible or requires significant effort to be achieved. We conclude that the implementation of SOA and MSA depends on the nature and needs of organizations (e.g., performance or interoperability). © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
DA  - 2021///
PY  - 2021
DO  - 10.3390/app11104350
VL  - 11
IS  - 10
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106660998&doi=10.3390%2fapp11104350&partnerID=40&md5=5fb71f9aed43e333ebe2945833fcec73
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 19</p>
KW  - Cloud computing
KW  - EHealth
KW  - Elderly people
KW  - Infectious diseases
KW  - MSA
KW  - SOA
KW  - Telemonitoring
KW  - Versatile high-performance
ER  - 

TY  - JOUR
TI  - Geo-distributed efficient deployment of containers with Kubernetes
AU  - Rossi, F.
AU  - Cardellini, V.
AU  - Lo Presti, F.
AU  - Nardelli, M.
T2  - Computer Communications
AB  - Software containers are changing the way applications are designed and executed. Moreover, in the last few years, we see the increasing adoption of container orchestration tools, such as Kubernetes, to simplify the management of multi-container applications. Kubernetes includes simple deployment policies that spread containers on computing resources located in the cluster and automatically scale them out or in based on some cluster-level metrics. As such, Kubernetes is not well-suited for deploying containers in a geo-distributed computing environment and dealing with the dynamism of application workload and computing resources. To tackle the problem, in this paper we present ge-kube (Geo-distributed and Elastic deployment of containers in Kubernetes), an orchestration tool that relies on Kubernetes and extends it with self-adaptation and network-aware placement capabilities. Ge-kube introduces flexible and decentralized control loops that can be easily equipped with different deployment policies. Specifically, we propose a two-step control loop, in which a model-based reinforcement learning approach dynamically controls the number of replicas of individual containers on the basis of the application response time, and a network-aware placement policy allocates containers on geo-distributed computing resources. To address the placement issue, we propose an optimization problem formulation and a network-aware heuristic, which explicitly take into account the non-negligible network delays among computing resources so to satisfy Quality of Service requirements of latency-sensitive applications. Using a surrogate CPU-intensive application and a real application (i.e., Redis), we conducted an extensive set of experiments, which show the benefits arising from the combination of elasticity and placement policies, as well as the advantages of using network-aware placement solutions. © 2020 Elsevier B.V.
DA  - 2020///
PY  - 2020
DO  - 10.1016/j.comcom.2020.04.061
VL  - 159
SP  - 161
EP  - 174
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084943561&doi=10.1016%2fj.comcom.2020.04.061&partnerID=40&md5=92df7a3da86927a4a6bae90fbdbb6076
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 113</p>
KW  - Application programs
KW  - Cluster computing
KW  - Computing resource
KW  - Containers
KW  - Decentralized control
KW  - Decentralized control loops
KW  - Distributed computing environment
KW  - Distributed computing resources
KW  - Elastic deployment
KW  - Elasticity
KW  - Geographically distributed resources
KW  - Kubernetes
KW  - Model-based reinforcement learning
KW  - Optimization problems
KW  - Placement
KW  - Quality of service
KW  - Reinforcement learning
KW  - Self-management
KW  - Sensitive application
ER  - 

TY  - JOUR
TI  - Novel resource allocation algorithms to performance and energy efficiency in cloud computing
AU  - Horri, A.
AU  - Mozafari, M.S.
AU  - Dastghaibyfard, G.
T2  - Journal of Supercomputing
AB  - The rapid growth in demand for computational power has led to a shift to the cloud computing model established by large-scale virtualized data centers. Such data centers consume enormous amounts of electrical energy. Cloud providers must ensure that their service delivery is flexible to meet various consumer requirements. However, to support green computing, cloud providers also need to minimize the cloud infrastructure energy consumption while conducting the service delivery. In this paper, for cloud environments, a novel QoS-aware VMs consolidation approach is proposed that adopts a method based on resource utilization history of virtual machines. Proposed algorithms have been implemented and evaluated using CloudSim simulator. Simulation results show improvement in QoS metrics and energy consumption as well as demonstrate that there is a trade-off between energy consumption and quality of service in the cloud environment. © 2014, Springer Science+Business Media New York.
DA  - 2014///
PY  - 2014
DO  - 10.1007/s11227-014-1224-8
VL  - 69
IS  - 3
SP  - 1445
EP  - 1461
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919877932&doi=10.1007%2fs11227-014-1224-8&partnerID=40&md5=93d310b40534cbb7a80b57193481804a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 111</p>
KW  - Cloud computing
KW  - Cloud environments
KW  - Cloud infrastructures
KW  - Computational power
KW  - Dynamic consolidation
KW  - Economic and social effects
KW  - Energy efficiency
KW  - Energy utilization
KW  - Energy-efficient resource management
KW  - Green computing
KW  - Quality of service
KW  - Resource allocation
KW  - Resource allocation algorithms
KW  - Resource management
KW  - Resource utilizations
KW  - Virtualized data centers
ER  - 

TY  - JOUR
TI  - Profiling SLAs for Cloud System Infrastructures and User Interactions
AU  - Cambronero, M.E.
AU  - Bernal, A.
AU  - Valero, V.
AU  - Cañizares, P.C.
AU  - Núñez, A.
T2  - PeerJ Computer Science
AB  - Cloud computing has emerged as a cutting-edge technology which is widely used by both private and public institutions, since it eliminates the capital expense of buying, maintaining, and setting up both hardware and software. Clients pay for the services they use, under the so-called Service Level Agreements (SLAs), which are the contracts that establish the terms and costs of the services. In this paper, we propose the CloudCost UML profile, which allows the modeling of cloud architectures and the users’ behavior when they interact with the cloud to request resources. We then investigate how to increase the profits of cloud infrastructures by using price schemes. For this purpose, we distinguish between two types of users in the SLAs: regular and high-priority users. Regular users do not require a continuous service, so they can wait to be attended to. In contrast, high-priority users require a constant and immediate service, so they pay a greater price for their services. In addition, a computer-aided design tool, called MSCC (Modeling SLAs Cost Cloud), has been implemented to support the CloudCost profile, which enables the creation of specific cloud scenarios, as well as their edition and validation. Finally, we present a complete case study to illustrate the applicability of the CloudCost profile, thus making it possible to draw conclusions about how to increase the profits of the cloud infrastructures studied by adjusting the different cloud parameters and the resource configuration. Copyright 2021 Cambronero et al.
DA  - 2021///
PY  - 2021
DO  - 10.7717/PEERJ-CS.513
VL  - 7
SP  - 1
EP  - 37
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107721190&doi=10.7717%2fPEERJ-CS.513&partnerID=40&md5=ab1fb2f0cf1b804eeddd22b46f6e74f6
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 5</p>
KW  - Cloud
KW  - Cloud architectures
KW  - Cloud infrastructures
KW  - Computer aided design
KW  - Computer aided design tools
KW  - Continuous services
KW  - Cutting edge technology
KW  - Design and simulation tools
KW  - Hardware and software
KW  - Model development
KW  - Platform as a Service (PaaS)
KW  - Profit improvement
KW  - Profitability
KW  - Resource configurations
KW  - Service level agreement (SLAs)
KW  - SLAs
ER  - 

TY  - JOUR
TI  - A placement architecture for a container as a service (CaaS) in a cloud environment
AU  - Hussein, M.K.
AU  - Mousa, M.H.
AU  - Alqarni, M.A.
T2  - Journal of Cloud Computing
AB  - Unlike a traditional virtual machine (VM), a container is an emerging lightweight virtualization technology that operates at the operating system level to encapsulate a task and its library dependencies for execution. The Container as a Service (CaaS) strategy is gaining in popularity and is likely to become a prominent type of cloud service model. Placing container instances on virtual machine instances is a classical scheduling problem. Previous research has focused separately on either virtual machine placement on physical machines (PMs) or container, or only tasks without containerization, placement on virtual machines. However, this approach leads to underutilized or overutilized PMs as well as underutilized or overutilized VMs. Thus, there is a growing research interest in developing a container placement algorithm that considers the utilization of both instantiated VMs and used PMs simultaneously. The goal of this study is to improve resource utilization, in terms of number of CPU cores and memory size for both VMs and PMs, and to minimize the number of instantiated VMs and active PMs in a cloud environment. The proposed placement architecture employs scheduling heuristics, namely, Best Fit (BF) and Max Fit (MF), based on a fitness function that simultaneously evaluates the remaining resource waste of both PMs and VMs. In addition, another meta-heuristic placement algorithm is proposed that uses Ant Colony Optimization based on Best Fit (ACO-BF) with the proposed fitness function. Experimental results show that the proposed ACO-BF placement algorithm outperforms the BF and MF heuristics and maintains significant improvement of the resource utilization of both VMs and PMs. © 2019, The Author(s).
DA  - 2019///
PY  - 2019
DO  - 10.1186/s13677-019-0131-1
VL  - 8
IS  - 1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066495867&doi=10.1186%2fs13677-019-0131-1&partnerID=40&md5=619dcedbedfe2f6ee01328ad9ee79dd5
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 100</p>
KW  - Ant colonies
KW  - Ant colony
KW  - Ant colony optimization
KW  - Artificial intelligence
KW  - Best fit
KW  - Cloud computing
KW  - Cloud container
KW  - Container placement
KW  - Containers
KW  - Digital libraries
KW  - Max fit
KW  - Network security
KW  - Placement algorithm
KW  - Scheduling
KW  - Two-tier placement algorithm
KW  - Virtual machine
KW  - Virtual machine placement
KW  - Virtual machine placements
ER  - 

TY  - CONF
TI  - A NSGA-II-based approach for service resource allocation in Cloud
AU  - Tan, B.
AU  - Ma, H.
AU  - Mei, Y.
T2  - 2017 IEEE Congress on Evolutionary Computation, CEC 2017 - Proceedings
AB  - Web service and Cloud computing have significantly reformed the software industry. The need for web service allocation in the cloud environment is increasing dramatically. In order to reduce the cost for service providers as well as improve the utilization of cloud resource for cloud providers, this paper formulates the web service resource allocation in cloud environment problem as a two-level multi-objective bin packing problem. It proposes a NSGA-II-based algorithm with specifically designed genetic operators. We are compared with two varieties of the algorithm. The results show that the proposed algorithm can provide reasonably good results with low violation rate. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/CEC.2017.7969618
SP  - 2574
EP  - 2581
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027868076&doi=10.1109%2fCEC.2017.7969618&partnerID=40&md5=558f17ce73278ad06c2bc0bdf8d7fa19
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 23</p>
ER  - 

TY  - JOUR
TI  - Dynamic energy-aware cloudlet-based mobile cloud computing model for green computing
AU  - Gai, K.
AU  - Qiu, M.
AU  - Zhao, H.
AU  - Tao, L.
AU  - Zong, Z.
T2  - Journal of Network and Computer Applications
AB  - Employing mobile cloud computing (MCC) to enable mobile users to acquire benefits of cloud computing by an environmental friendly method is an efficient strategy for meeting current industrial demands. However, the restrictions of wireless bandwidth and device capacity have brought various obstacles, such as extra energy waste and latency delay, when deploying MCC. Addressing this issue, we propose a dynamic energy-aware cloudlet-based mobile cloud computing model (DECM) focusing on solving the additional energy consumptions during the wireless communications by leveraging dynamic cloudlets (DCL)-based model. In this paper, we examine our model by a simulation of practical scenario and provide solid results for the evaluations. The main contributions of this paper are twofold. First, this paper is the first exploration in solving energy waste problems within the dynamic networking environment. Second, the proposed model provides future research with a guideline and theoretical supports. © 2015 Elsevier Ltd. All rights reserved.
DA  - 2016///
PY  - 2016
DO  - 10.1016/j.jnca.2015.05.016
VL  - 59
SP  - 46
EP  - 54
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949627105&doi=10.1016%2fj.jnca.2015.05.016&partnerID=40&md5=5abb99d76ead892a44038e02e5a26544
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 477</p>
KW  - Cloud computing
KW  - Cloudlets
KW  - Dynamic models
KW  - Dynamic program
KW  - Dynamic programs
KW  - Efficient strategy
KW  - Energy aware
KW  - Energy-aware
KW  - Environmental friendly methods
KW  - Green computing
KW  - Mobile cloud computing
KW  - Power management
KW  - Power management (telecommunication)
KW  - Wireless bandwidth
KW  - Wireless communications
KW  - Wireless telecommunication systems
ER  - 

TY  - CONF
TI  - Novel genetic algorithm with dual chromosome representation for resource allocation in container-based clouds
AU  - Tan, B.
AU  - Ma, H.
AU  - Mei, Y.
T2  - IEEE International Conference on Cloud Computing, CLOUD
AB  - Containerization does not only support fast development and deployment of web applications but also provides the potential to improve the energy efficiency in cloud data centers. In container-based clouds, containers are allocated to virtual machines (VMs) and VMs are allocated to physical machines (PMs). This new architecture requires consolidation algorithms to select heterogeneous VMs to host containers and consolidate VMs to PMs simultaneously. Existing server consolidation techniques in VM-based clouds can hardly be applied because of the two-level architecture of the container-based clouds. This paper proposes a novel genetic algorithm (GA) with dual chromosome representation to solve the problem. The experiments show that the proposed GA achieves significantly higher energy efficiency than the compared state-of-the-art algorithms on a wide range of test problems. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/CLOUD.2019.00078
VL  - 2019-July
SP  - 452
EP  - 456
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072335660&doi=10.1109%2fCLOUD.2019.00078&partnerID=40&md5=e2b59ea608fa876acf93eb4087754319
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 33</p>
KW  - Chromosomes
KW  - Cloud computing
KW  - Cloud data centers
KW  - Cloud resource allocation
KW  - Container
KW  - Containers
KW  - Energy efficiency
KW  - Evolutionary algorithms
KW  - Evolutionary computation
KW  - Genetic algorithm
KW  - Genetic algorithms
KW  - Green computing
KW  - Novel genetic algorithm
KW  - Resource allocation
KW  - Server consolidation
KW  - State-of-the-art algorithms
KW  - Test problem
KW  - WEB application
ER  - 

TY  - JOUR
TI  - An energy, performance efficient resource consolidation scheme for heterogeneous cloud datacenters
AU  - Khan, A.A.
AU  - Zakarya, M.
AU  - Khan, R.
AU  - Rahman, I.U.
AU  - Khan, M.
AU  - Khan, A.U.R.
T2  - Journal of Network and Computer Applications
AB  - Datacenters are the principal electricity consumers for cloud computing that provide an IT backbone for today's business and economy. Numerous studies suggest that most of the servers, in the US datacenters, are idle or less-utilised, making it possible to save energy by using resource consolidation techniques. However, consolidation involves migrations of virtual machines, containers and/or applications, depending on the underlying virtualisation method; that can be expensive in terms of energy consumption and performance loss. In this paper, we: (a) propose a consolidation algorithm which favours the most effective migration among VMs, containers and applications; and (b) investigate how migration decisions should be made to save energy without any negative impact on the service performance. We demonstrate through a number of experiments, using the real workload traces for 800 hosts, approximately 1516 VMs, and more than million containers, how different approaches to migration, will impact on datacenter's energy consumption and performance. We suggest, using reasonable assumptions for datacenter set-up, that there is a trade-off involved between migrating containers and virtual machines. It is more performance efficient to migrate virtual machines; however, migrating containers could be more energy efficient than virtual machines. Moreover, migrating containerised applications, that run inside virtual machines, could lead to energy and performance efficient consolidation technique in large-scale datacenters. Our evaluation suggests that migrating applications could be ~5.5% more energy efficient and ~11.9% more performance efficient than VMs migration. Further, energy and performance efficient consolidation is ~14.6% energy and ~7.9% performance efficient than application migration. Finally, we generalise our results using several repeatable experiments over various workloads, resources and datacenter set-ups. © 2019 Elsevier Ltd
DA  - 2020///
PY  - 2020
DO  - 10.1016/j.jnca.2019.102497
VL  - 150
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075576818&doi=10.1016%2fj.jnca.2019.102497&partnerID=40&md5=14be49026985907961b015fa6fc7e510
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 75</p>
KW  - Application migrations
KW  - Cloud computing
KW  - Clouds
KW  - Consolidation techniques
KW  - Containers
KW  - Economic and social effects
KW  - Electricity consumers
KW  - Energy efficiency
KW  - Energy utilization
KW  - Green computing
KW  - Migrations
KW  - Network security
KW  - Performance
KW  - Performance loss
KW  - Resource consolidation
KW  - Service performance
KW  - Virtual machine
ER  - 

TY  - CONF
TI  - Experimental analysis of task-based energy consumption in cloud computing systems
AU  - Chen, F.
AU  - Grundy, J.
AU  - Yang, Y.
AU  - Schneider, J.-G.
AU  - He, Q.
T2  - ICPE 2013 - Proceedings of the 2013 ACM/SPEC International Conference on Performance Engineering
AB  - Cloud computing delivers IT solutions as a utility to users. One consequence of this model is that large cloud data centres consume large amounts of energy and produce significant carbon footprints. A common objective of cloud providers is to develop resource provisioning and management solutions that minimise energy consumption while guaranteeing Service Level Agreements (SLAs). In order to achieve this objective, a thorough understanding of energy consumption patterns in complex cloud systems is imperative. We have developed an energy consumption model for cloud computing systems. To operationalise this model, we have conducted extensive experiments to profile the energy consumption in cloud computing systems based on three types of tasks: computation- intensive, data-intensive and communication-intensive tasks. We collected fine-grained energy consumption and performance data with varying system configurations and workloads. Our experimental results show the correlation coefficients of energy consumption, system configuration and workload, as well as system performance in cloud systems. These results can be used for designing energy consumption monitors, and static or dynamic system-level energy consumption optimisation strategies for green cloud computing systems. © 2013 ACM.
DA  - 2013///
PY  - 2013
DO  - 10.1145/2479871.2479911
SP  - 295
EP  - 306
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878194291&doi=10.1145%2f2479871.2479911&partnerID=40&md5=50fc4bb7a4bd5e428794fe725c6d236d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 47</p>
KW  - Carbon footprint
KW  - cloud computing
KW  - Cloud computing
KW  - Computer systems
KW  - Correlation coefficient
KW  - energy consumption
KW  - Energy consumption model
KW  - Energy consumption monitors
KW  - energy efficiency
KW  - Energy efficiency
KW  - Energy utilization
KW  - Experimental analysis
KW  - green cloud
KW  - Green Clouds
KW  - performance analysis
KW  - Performance analysis
KW  - Service level agreement (SLAs)
KW  - System configurations
ER  - 

TY  - CONF
TI  - A Multi-Criteria Strategy for Redesigning Legacy Features as Microservices: An Industrial Case Study
AU  - Assunção, W.K.G.
AU  - Colanzi, T.E.
AU  - Carvalho, L.
AU  - Pereira, J.A.
AU  - Garcia, A.
AU  - De Lima, M.J.
AU  - Lucena, C.
T2  - Proceedings - 2021 IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2021
AB  - Microservices are small and autonomous services that communicate through lightweight protocols. Companies have often been adopting microservices to incrementally redesign legacy systems as part of a modernization process. Microservices promote better reuse and customization of existing features while increasing business capabilities, if appropriate design decisions are made. There are some partially-automated approaches supporting the re-design of legacy features into microservices. However, they fail in covering two key aspects: (i) provide an architectural design of the features being redesigned, and (ii) simultaneously support relevant criteria, e.g., feature modularization and decrease of network communication overhead. Also, these two aspects tend to be poorly discussed along industrial case studies. To fulfill these gaps, we propose a redesign strategy to support the re-engineering of features legacy code as microservices. This strategy covers key possibly-conflicting criteria on microservice-based architectures. We employ search-based optimization to deal with such conflicting criteria. The output of the strategy is a set of redesign candidates of legacy features as microservices. We reflect upon the benefits and drawbacks of the proposed strategy through an industrial case study. In particular, we perform an in-depth analysis of the resulting microservice candidates, and a discussion about their potential for customization and reuse. The reflections/discussions are also supported by observations of developers involved in the process. © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/SANER50967.2021.00042
SP  - 377
EP  - 387
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106562484&doi=10.1109%2fSANER50967.2021.00042&partnerID=40&md5=f4878c936ea66a41ca9b8f11ade5728e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 34</p>
KW  - Appropriate designs
KW  - Automated approach
KW  - In-depth analysis
KW  - Industrial case study
KW  - legacy systems
KW  - Legacy systems
KW  - Lightweight protocols
KW  - microservice architecture
KW  - Modular construction
KW  - Modularizations
KW  - Network communication overhead
KW  - Reengineering
KW  - Search based optimizations
KW  - search-based software engineering
KW  - software evolution
ER  - 

TY  - CONF
TI  - Container-VM-PM Architecture: A Novel Architecture for Docker Container Placement
AU  - Zhang, R.
AU  - Zhong, A.-M.
AU  - Dong, B.
AU  - Tian, F.
AU  - Li, R.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Docker is a mature containerization technique used to perform operating system level virtualization. One open issue in the cloud environment is how to properly choose a virtual machine (VM) to initialize its instance, i.e., container, which is similar to the conventional problem of VM placement towards physical machines (PMs). Current studies mainly focus on container placement and VM placement independently, but rarely take into consideration of the two placements’ systematic collaboration. However, we view it as a main reason for scattered distribution of containers in a data center, which finally results in worse physical resource utilization. In this paper, we propose a definition named “Container-VM-PM” architecture and propose a novel container placement strategy by simultaneously taking into account the three involved entities. Furthermore, we model a fitness function for the selection of VM and PM. Simulation experiments show that our method is superior to the existing strategy with regarding to the physical resource utilization. © 2018, Springer International Publishing AG, part of Springer Nature.
DA  - 2018///
PY  - 2018
DO  - 10.1007/978-3-319-94295-7_9
VL  - 10967 LNCS
SP  - 128
EP  - 140
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049337921&doi=10.1007%2f978-3-319-94295-7_9&partnerID=40&md5=8448a5e06864d8939a95f882211f5b79
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 34</p>
KW  - Architecture
KW  - Client server computer systems
KW  - Cloud computing
KW  - Cloud environments
KW  - Containers
KW  - Docker container
KW  - Docker containers
KW  - Fitness functions
KW  - Network security
KW  - Novel architecture
KW  - Physical resources
KW  - Placement strategy
KW  - Resource fragment
KW  - Three-tier architecture
KW  - Virtual machine
ER  - 

TY  - JOUR
TI  - A Resource Utilization Prediction Model for Cloud Data Centers Using Evolutionary Algorithms and Machine Learning Techniques
AU  - Malik, S.
AU  - Tahir, M.
AU  - Sardaraz, M.
AU  - Alourani, A.
T2  - Applied Sciences (Switzerland)
AB  - Cloud computing has revolutionized the modes of computing. With huge success and diverse benefits, the paradigm faces several challenges as well. Power consumption, dynamic resource scaling, and over-and under-provisioning issues are challenges for the cloud computing paradigm. The research has been carried out in cloud computing for resource utilization prediction to overcome over-and under-provisioning issues. Over-provisioning of resources consumes more energy and leads to high costs. However, under-provisioning induces Service Level Agreement (SLA) violation and Quality of Service (QoS) degradation. Most of the existing mechanisms focus on single resource utilization prediction, such as memory, CPU, storage, network, or servers allocated to cloud applications but overlook the correlation among resources. This research focuses on multi-resource utilization prediction using Functional Link Neural Network (FLNN) with hybrid Genetic Algorithm (GA) and Particle Swarm Optimization (PSO). The proposed technique is evaluated on Google cluster traces data. Experimental results show that the proposed model yields better accuracy as compared to traditional techniques. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.
DA  - 2022///
PY  - 2022
DO  - 10.3390/app12042160
VL  - 12
IS  - 4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124946319&doi=10.3390%2fapp12042160&partnerID=40&md5=9e461e050680597e2e8bdfdf8cb1d739
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 56</p>
KW  - Cloud computing
KW  - Forecasting
KW  - GA
KW  - Neural networks
KW  - PSO
KW  - Resource utilization
ER  - 

TY  - JOUR
T2  - Microservices Divvy up Tasks to Improve Cloud Apps | Cornell Chronicle
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132281015&partnerID=40&md5=000085d6ccb910c48f449fedf1a27c9c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - JCallGraph: Tracing microservices in very large scale container cloud platforms
AU  - Liu, H.
AU  - Zhang, J.
AU  - Shan, H.
AU  - Li, M.
AU  - Chen, Y.
AU  - He, X.
AU  - Li, X.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Microservice architecture splits giant and complex enterprise applications into fine-grained microservices, promoting agile development, integration, delivery and deployment. However, monitoring tens of thousands of microservices is extremely challenging, and debugging problems among massive microservices is like looking for a needle in a haystack. We present JCallGraph, a tracing and analytics tool to capture and visualize the microservice invocation relationship of tens of thousands of microservices with millions of containers at JD.com. JCallGraph achieves three main goals for distributed tracing and debugging: online microservices invocation construction within milliseconds, minimal overhead without any significant performance impact on real-production applications, and application-agnostic with zero-intrusion to application. Our evaluation shows that JCallGraph can accurately capture the real-time invocation relationship at massive scale and help developers to efficiently understand interactions among microservices, pinpoint root-cause of problems. © Springer Nature Switzerland AG 2019.
DA  - 2019///
PY  - 2019
DO  - 10.1007/978-3-030-23502-4_20
VL  - 11513 LNCS
SP  - 287
EP  - 302
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068217062&doi=10.1007%2f978-3-030-23502-4_20&partnerID=40&md5=78356a6c290932cd264c50d472a6d612
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 18</p>
KW  - Agile development
KW  - Analytics tools
KW  - Cloud computing
KW  - Cloud platforms
KW  - Containers
KW  - Distributed tracing
KW  - Distributed tracing system
KW  - Enterprise applications
KW  - Invocation graph
KW  - Microservice invocation graph
KW  - Performance analysis
KW  - Performance analysis and measurement
KW  - Performance impact
KW  - Program debugging
ER  - 

TY  - JOUR
TI  - ContainerCloudSim: An environment for modeling and simulation of containers in cloud data centers
AU  - Piraghaj, S.F.
AU  - Dastjerdi, A.V.
AU  - Calheiros, R.N.
AU  - Buyya, R.
T2  - Software - Practice and Experience
AB  - Containers are increasingly gaining popularity and becoming one of the major deployment models in cloud environments. To evaluate the performance of scheduling and allocation policies in containerized cloud data centers, there is a need for evaluation environments that support scalable and repeatable experiments. Simulation techniques provide repeatable and controllable environments, and hence, they serve as a powerful tool for such purpose. This paper introduces ContainerCloudSim, which provides support for modeling and simulation of containerized cloud computing environments. We developed a simulation architecture for containerized clouds and implemented it as an extension of CloudSim. We described a number of use cases to demonstrate how one can plug in and compare their container scheduling and provisioning policies in terms of energy efficiency and SLA compliance. Our system is highly scalable as it supports simulation of large number of containers, given that there are more containers than virtual machines in a data center. Copyright © 2016 John Wiley & Sons, Ltd. Copyright © 2016 John Wiley & Sons, Ltd.
DA  - 2017///
PY  - 2017
DO  - 10.1002/spe.2422
VL  - 47
IS  - 4
SP  - 505
EP  - 521
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977508166&doi=10.1002%2fspe.2422&partnerID=40&md5=c5f51b9c0d8e162bb0c22f052c69f901
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 153</p>
KW  - Allocation policies
KW  - cloud computing
KW  - Cloud computing
KW  - Cloud computing environments
KW  - container as a service (CaaS)
KW  - Container scheduling
KW  - containerized clouds
KW  - Containers
KW  - Energy efficiency
KW  - Model and simulation
KW  - Provisioning policies
KW  - Scheduling
KW  - simulation
KW  - Simulation
KW  - Simulation architecture
KW  - Simulation technique
ER  - 

TY  - JOUR
TI  - Two levels autonomic resource management in virtualized IaaS
AU  - Tchana, A.
AU  - Son Tran, G.
AU  - Broto, L.
AU  - Depalma, N.
AU  - Hagimont, D.
T2  - Future Generation Computer Systems
AB  - Virtualized cloud infrastructures are very popular as they allow resource mutualization and therefore cost reduction. For cloud providers, minimizing the number of used resources is one of the main services that such environments must ensure. Cloud customers are also concerned with the minimization of used resources in the cloud since they want to reduce their invoice. Thus, resource management in the cloud should be considered by the cloud provider at the virtualization level and by the cloud customers at the application level. Many research works investigate resource management strategies in these two levels. Most of them study virtual machine consolidation (according to the virtualized infrastructure utilization rate) at the virtualized level and dynamic application sizing (according to its workload) at the application level. However, these strategies are studied separately. In this article, we show that virtual machine consolidation and dynamic application sizing are complementary. We show the efficiency of the combination of these two strategies, in reducing resource usage and keeping an application's Quality of Service. Our demonstration is done by comparing the evaluation of three resource management strategies (implemented at the virtualization level only, at the application level only, or complementary at both levels) in a private cloud infrastructure, hosting typical JEE web applications (evaluated with the RUBiS benchmark). © 2013 Elsevier B.V. All rights reserved.
DA  - 2013///
PY  - 2013
DO  - 10.1016/j.future.2013.02.002
VL  - 29
IS  - 6
SP  - 1319
EP  - 1332
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875197085&doi=10.1016%2fj.future.2013.02.002&partnerID=40&md5=5b5c3ac6977d47303553edcbbd7b538f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 20</p>
KW  - Application level
KW  - Autonomic administration
KW  - Benchmarking
KW  - Cloud computing
KW  - Cloud infrastructures
KW  - Dynamic applications
KW  - Minimizing the number of
KW  - Natural resources management
KW  - Quality of service
KW  - Resource allocation
KW  - Resource management
KW  - Utilization rates
KW  - Virtual machine consolidations
KW  - Virtual reality
KW  - WEB application
ER  - 

TY  - JOUR
TI  - Supporting Programmable Autoscaling Rules for Containers and Virtual Machines on Clouds
AU  - Kovács, J.
T2  - Journal of Grid Computing
AB  - With the increasing utilization of cloud computing and container technologies, orchestration is becoming an important area on both cloud and container levels. Beyond resource allocation, deployment and configuration, scaling is a key functionality in orchestration in terms of policy, description and flexibility. This paper presents an approach where the aim is to provide a high degree of flexibility in terms of available monitoring metrics and in terms of the definition of elasticity rules to implement practically any possible business logic for a given application. The aim is to provide a general interface for supporting programmable scaling policies utilizing monitoring metrics originating from infrastructure, application or any external components. The paper introduces a component, called Policy Keeper performing the auto-scaling based on user-defined rules, details how this component is operating in the auto-scaling framework, called MiCADO and demonstrates a deadline-based scaling use case. © 2019, The Author(s).
DA  - 2019///
PY  - 2019
DO  - 10.1007/s10723-019-09488-w
VL  - 17
IS  - 4
SP  - 813
EP  - 829
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072128121&doi=10.1007%2fs10723-019-09488-w&partnerID=40&md5=c046b5407e3d615944790d1cff07dabc
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 23</p>
KW  - Autoscaling
KW  - Business logic
KW  - Cloud
KW  - Cloud computing
KW  - Clouds
KW  - Computation theory
KW  - Container
KW  - Containers
KW  - Degree of flexibility
KW  - Deployment and configuration
KW  - Distributed monitoring
KW  - Docker
KW  - External components
KW  - Monitoring metrics
KW  - Network security
KW  - Virtual machine
ER  - 

TY  - CONF
TI  - Improvement of container scheduling for Docker using Ant Colony Optimization
AU  - Kaewkasi, C.
AU  - Chuenmuneewong, K.
T2  - 2017 9th International Conference on Knowledge and Smart Technology: Crunching Information of Everything, KST 2017
AB  - Docker, a software container implementation, has emerged not only as an operating-system level virtualization but also an application delivery platform for today Internet. However, the scheduling algorithm shipped with SwarmKit, the orchestration engine behind Docker, is suboptimal when resources are nonuniform. The use of meta-heuristic, like Ant Colony Optimization (ACO), is feasible to improve the scheduler's optimality. This paper presents a study of ACO to implement a new scheduler for Docker. The main contribution of this paper is an ACO-based algorithm, which distributes application containers over Docker hosts. It is to balance the resource usages and finally l eads to the better performance of applications. The experimental results showed that workloads placed by ACO performed better than those of the greedy algorithm by approximately 15% on the same host configuration. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/KST.2017.7886112
SP  - 254
EP  - 259
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017513531&doi=10.1109%2fKST.2017.7886112&partnerID=40&md5=c25c6c7e8c1f8110164504b541ccb644
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 102</p>
KW  - Aco-based algorithms
KW  - Ant colony optimization
KW  - Ant Colony Optimization (ACO)
KW  - Application delivery
KW  - Application programs
KW  - Artificial intelligence
KW  - Container scheduling
KW  - Containers
KW  - Greedy algorithms
KW  - Metaheuristic
KW  - Optimization
KW  - Resource usage
KW  - Scheduling
KW  - Scheduling algorithms
KW  - System levels
ER  - 

TY  - CONF
TI  - Low-cost and fast failure recovery using In-VM containers in clouds
AU  - Morikawa, T.
AU  - Kourai, K.
T2  - Proceedings - IEEE 17th International Conference on Dependable, Autonomic and Secure Computing, IEEE 17th International Conference on Pervasive Intelligence and Computing, IEEE 5th International Conference on Cloud and Big Data Computing, 4th Cyber Science and Technology Congress, DASC-PiCom-CBDCom-CyberSciTech 2019
AB  - Recently, various services are provided using virtual machines (VMs) in clouds. Therefore, it is necessary to prepare for system failures of VMs, hosts running VMs, and even data centers, e.g., using active/standby clustering. However, a trade-off exists between the maintenance cost for additional VMs and the recovery time in traditional techniques. For example, hot standby can rapidly fail over to the secondary system on a system failure, but the secondary system has to always run the same number of VMs as the primary system. In contrast, cold standby does not need to run VMs until a system failure, but it has to boot VMs on failure recovery. In this paper, we propose VCRecovery, which is the system for achieving both low-cost and fast failure recovery. VCRecovery consolidates services using containers inside VMs (in-VM containers) in the secondary system. For hot standby, it can reduce the maintenance cost by using only a smaller number of VMs in the secondary system. For cold standby, it can reduce the recovery time by quickly booting in-VM containers. If a VM is overloaded after the recovery, VCRecovery can migrate several in-VM containers to other VMs. To synchronize storage between VMs in the primary system and in-VM containers in the secondary system, it efficiently performs minimum file-based synchronization based on software packages. We have implemented VCRecovery using LXD and Zabbix and examined the performance. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/DASC/PiCom/CBDCom/CyberSciTech.2019.00112
SP  - 572
EP  - 579
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075150152&doi=10.1109%2fDASC%2fPiCom%2fCBDCom%2fCyberSciTech.2019.00112&partnerID=40&md5=719e7783c3442eaf54392734ba796321
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3</p>
KW  - Active/standby
KW  - Availability
KW  - Big data
KW  - Containers
KW  - Costs
KW  - Digital storage
KW  - Economic and social effects
KW  - Failure detection
KW  - Failure recovery
KW  - Fast failure recovery
KW  - Maintenance
KW  - Maintenance cost
KW  - Network security
KW  - Primary systems
KW  - Recovery
KW  - Secondary system
KW  - Systems engineering
KW  - Traditional techniques
KW  - Virtual machine
KW  - Virtual machines
ER  - 

TY  - CONF
TI  - Design Pattern Based Distribution of Microservices in Cloud Computing Environment
AU  - Saboor, A.
AU  - Mahmood, A.K.
AU  - Hassan, M.F.
AU  - Shah, S.N.M.
AU  - Hassan, F.
AU  - Siddiqui, M.A.
T2  - Proceedings - International Conference on Computer and Information Sciences: Sustaining Tomorrow with Digital Innovation, ICCOINS 2021
AB  - Cloud computing is a paradigm that has already evolved. Cloud computing moved widely to microservices from monoliths. The modular cloud application has gained attention for Microservices. Intensive network communication is required to call the interdependent microservices operating inside the cloud nodes. This research focuses on container-based microservices pre-distribution techniques and proposes two distribution strategies i.e. design pattern distribution and random distribution. The microservices are arbitrarily distributed to the available data centers in the random allocation method. While the microservices are clustered in the pattern distribution based on behavioral design patterns, which identify common contact patterns between entities. A custom-built modeling environment has been used to evaluate the proposed method. The findings revealed that the pre-distribution of microservices in accordance with the application architecture trend led to substantial less response time for the calls made to services hosted at geographically dispersed data centers. © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/ICCOINS49721.2021.9497188
SP  - 396
EP  - 400
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112426648&doi=10.1109%2fICCOINS49721.2021.9497188&partnerID=40&md5=616aca5df05d88fa2b5a06b1446cf9a2
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 6</p>
KW  - Application architecture
KW  - Artificial intelligence
KW  - Cloud applications
KW  - Cloud computing
KW  - Cloud Computing
KW  - Cloud computing environments
KW  - Cloud Containers
KW  - Computer science
KW  - Computers
KW  - Distribution strategies
KW  - Microservices
KW  - Modeling environments
KW  - Network communications
KW  - Optimization
KW  - Random allocation
KW  - Random distribution
KW  - Software engineering
KW  - Virtual Machine
ER  - 

TY  - JOUR
TI  - Planning vs. Dynamic Control: Resource Allocation in Corporate Clouds
AU  - Wolke, A.
AU  - Bichler, M.
AU  - Setzer, T.
T2  - IEEE Transactions on Cloud Computing
AB  - Nowadays corporate data centers leverage virtualization technology to cut operational and management costs. Virtualization allows splitting and assigning physical servers to virtual machines (VM) that run particular business applications. This has led to a new stream in the capacity planning literature dealing with the problem of assigning VMs with volatile demands to physical servers in a static way such that energy costs are minimized. Live migration technology allows for dynamic resource allocation, where a controller responds to overload or underload on a server during runtime and reallocates VMs in order to maximize energy efficiency. Dynamic resource allocation is often seen as the most efficient means to allocate hardware resources in a data center. Unfortunately, there is hardly any experimental evidence for this claim. In this paper, we provide the results of an extensive experimental analysis of both capacity management approaches on a data center infrastructure. We show that with typical workloads of transactional business applications dynamic resource allocation does not increase energy efficiency over the static allocation of VMs to servers and can even come at a cost, because migrations lead to overheads and service disruptions. © 2013 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/TCC.2014.2360399
VL  - 4
IS  - 3
SP  - 322
EP  - 335
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986571155&doi=10.1109%2fTCC.2014.2360399&partnerID=40&md5=defc85bcc612d4727bbb1df287687d8f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 34</p>
KW  - Business applications
KW  - Capacity management
KW  - Capacity planning
KW  - Costs
KW  - Dynamic resource allocations
KW  - Dynamics
KW  - Energy efficiency
KW  - Experimental analysis
KW  - Experimental evidence
KW  - IT service management
KW  - resource allocation
KW  - Resource allocation
KW  - Virtual reality
KW  - Virtualization technologies
ER  - 

TY  - CONF
TI  - Energy aware consolidation for cloud computing
AU  - Srikantaiah, S.
AU  - Kansal, A.
AU  - Zhao, F.
T2  - Workshop on Power Aware Computing and Systems, HotPower 2008
AB  - Consolidation of applications in cloud computing environments presents a significant opportunity for energy optimization. As a first step toward enabling energy efficient consolidation, we study the inter-relationships between energy consumption, resource utilization, and performance of consolidated workloads. The study reveals the energy performance trade-offs for consolidation and shows that optimal operating points exist. We model the consolidation problem as a modified bin packing problem and illustrate it with an example. Finally, we outline the challenges in finding effective solutions to the consolidation problem. © Workshop on Power Aware Computing and Systems, HotPower 2008. All rights reserved.
DA  - 2008///
PY  - 2008
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021450123&partnerID=40&md5=8516c07058681fe98cfd3b25d0bd6240
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 560</p>
KW  - Bin packing problem
KW  - Cloud computing
KW  - Cloud computing environments
KW  - Economic and social effects
KW  - Effective solution
KW  - Energy efficiency
KW  - Energy optimization
KW  - Energy performance
KW  - Energy utilization
KW  - Inter-relationships
KW  - Optimal operating point
KW  - Optimization
KW  - Power management
KW  - Resource utilizations
ER  - 

TY  - CONF
TI  - MiCADO - Towards a microservice-based cloud application-level dynamic orchestrator
AU  - Visti, H.
AU  - Kiss, T.
AU  - Terstyanszky, G.
AU  - Gesmier, G.
AU  - Winter, S.
T2  - CEUR Workshop Proceedings
AB  - In order to satisfy end-user requirements, many scientific and commercial applications require access to dynamically adjustable infrastructure resources. Cloud computing has the potential to provide these dynamic capabilities. However, utilising these capabilities from application code is not trivial and requires application developers to understand low-level technical details of clouds. This paper investigates how a generic framework can be developed that supports the dynamic orchestration of cloud applications both at deployment and at run-time. The advantages and challenges of designing such framework based on microservices is analysed, and a generic framework, called MiCADO - (Microservices-based Cloud Application-level Dynamic Orchestrator) is proposed. A first prototype implementation of MiCADO to support data intensive commercial web applications is also presented.
DA  - 2016///
PY  - 2016
VL  - 1871
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025151047&partnerID=40&md5=6ecc4923cc7d043e3aacfa7df11e8950
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
KW  - Application developers
KW  - Application level
KW  - Application-level orchestration
KW  - Cloud applications
KW  - Commercial applications
KW  - Container technologies
KW  - Dynamic capabilities
KW  - End user requirements
KW  - Infrastructure resources
KW  - Microservices-based architectures
KW  - Prototype implementations
ER  - 

TY  - JOUR
TI  - Key Characteristics of a Container Orchestration Platform to Enable a Modern Application
AU  - Khan, A.
T2  - IEEE Cloud Computing
AB  - As compute evolves from bare metal to virtualized environments to containers towards serverless, the efficiency gains have enabled a wide variety of use cases. Organizations have used containers to run long running services, batch processing at scale, control planes, Internet of Things, and Artificial Intelligence workloads. Further, methodologies for software as a service, such as twelve-factor app, emphasize a clean contract with the underlying operating system and maximum portability between execution environments.1 In this paper, we address a set of capabilities required of a container orchestration platform to embody the design principles as illustrated by twelve factor app design. This paper also provides a non-exhaustive and prescriptive guide to identifying and implementing key mechanisms required in a container orchestration platform. We will cover capabilities such as cluster state management and scheduling, high availability and fault tolerance, security, networking, service discovery, continuous deployment, monitoring, and governance. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/MCC.2017.4250933
VL  - 4
IS  - 5
SP  - 42
EP  - 48
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038634613&doi=10.1109%2fMCC.2017.4250933&partnerID=40&md5=2dcb5fc2b388db00d5bc60ece6aedf0c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 58</p>
KW  - application load balancing
KW  - Application programs
KW  - Batch data processing
KW  - cluster management
KW  - Cluster management
KW  - code review
KW  - Code review
KW  - Computer software portability
KW  - container orchestration
KW  - container security
KW  - containers
KW  - Containers
KW  - continuous delivery and deployment
KW  - cyclomatic complexity
KW  - Cyclomatic complexity
KW  - Docker
KW  - dynamic port mapping
KW  - Fault tolerance
KW  - fault tolerant
KW  - Fault tolerant computer systems
KW  - Fault-tolerant
KW  - high availability
KW  - High availability
KW  - microservices
KW  - monitoring
KW  - Monitoring
KW  - Port mapping
KW  - scheduling
KW  - Scheduling
KW  - service discovery
KW  - Service discovery
KW  - Software as a service (SaaS)
KW  - tracing
KW  - twelve-factor
KW  - Virtual reality
ER  - 

TY  - JOUR
TI  - Enabling rank-based distribution of microservices among containers for green cloud computing environment
AU  - Saboor, A.
AU  - Mahmood, A.K.
AU  - Omar, A.H.
AU  - Hassan, M.F.
AU  - Shah, S.N.M.
AU  - Ahmadian, A.
T2  - Peer-to-Peer Networking and Applications
AB  - Microservices architecture is a functional software design methodology that promises the redefinition of the architectural style that aims to create a single application as a suite of tiny, loosely coupled services or components, each performing its own tasks and interacting with each other. The cloud services widely shifted from monoliths to microservices and gained the popularity for use in scalable cloud application. The usage of microservices involved intensive network communication to call number of interdependent microservices running inside the cloud nodes. It provides flexibility in the delivery of service but also increases energy usage and poor service efficiency which results in increased carbon emissions. To solve these issues, the prevailing technologies were designed for single unit monolithic cloud applications, and not tailored for the chain oriented service delivery. This study addresses the dynamic provisioning of containers and respective microservices in cloud computing environment by building rank-based profiles and using those profiles for allocation of web application’s microservices along with containers to the cloud data centers. The MicroRanker service is proposed to rank all of the participating microservices and distribute them across different nodes even before the execution of the cloud services. Further, the MicroRanker service is utilized to dynamically update the container placement due to continuous DevOps actions. The proposed solution was tested using custom built simulation environment. The achieved results showed that the distribution of containers along with respective microservices in accordance with MicroRanker service resulted in less energy consumption (i.e. between 81.6 kWh-87.7 kWh compared to 88.9 kWh-95.7 kWh) and significantly lowered the emission of carbon (i.e. between 5.92 kg-33.31 kg compared to 17.2 kg-47.35 kg) due to higher utilization of renewable energy. The use of rank-based microservices distribution also decreased response time (i.e. between 29 ms-142 ms compared to 106 ms-217 ms) due to the availability of the container along with microservice within the same data center region. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
DA  - 2022///
PY  - 2022
DO  - 10.1007/s12083-021-01218-y
VL  - 15
IS  - 1
SP  - 77
EP  - 91
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112841408&doi=10.1007%2fs12083-021-01218-y&partnerID=40&md5=ee57caac17e75a71c6e3bdd06f31114e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 20</p>
KW  - Application programs
KW  - Architectural style
KW  - Carbon
KW  - Cloud computing
KW  - Cloud computing environments
KW  - Cloud data centers
KW  - Containers
KW  - Dynamic provisioning
KW  - Energy utilization
KW  - Green computing
KW  - High performance computing
KW  - Microservices
KW  - Network communications
KW  - Optimization
KW  - Ranking
KW  - Renewable energies
KW  - Simulation environment
KW  - Software design
KW  - Software design methodologies
KW  - Web services
ER  - 

TY  - JOUR
TI  - An Efficient Online Placement Scheme for Cloud Container Clusters
AU  - Zhou, R.
AU  - Li, Z.
AU  - Wu, C.
T2  - IEEE Journal on Selected Areas in Communications
AB  - Containers represent an agile alternative to virtual machines (VMs), for providing cloud computing services. Containers are more flexible and lightweight, and can be easily instrumented. Enterprise users often create clusters of inter-connected containers to provision complex services. Compared to traditional cloud services, key challenges in container cluster (CC) provisioning lie in the optimal placement of containers while considering inter-container traffic in a CC. The challenge further escalates, when CCs are provisioned in an online fashion. We propose an online algorithm to address the above challenges, aiming to maximize the aggregate value of all served clusters. We first study a one-shot CC placement problem. Leveraging techniques of exhaustive sampling and ST rounding, we design an efficient one-shot algorithm to determine the placement scheme of a given CC. We then propose a primal-dual online placement scheme that employs the one-shot algorithm as a building block to make decisions upon the arrival of each CC request. Through both theoretical analysis and trace-driven simulations, we verify that the online placement algorithm is computationally efficient and achieves a good competitive ratio. © 1983-2012 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/JSAC.2019.2906745
VL  - 37
IS  - 5
SP  - 1046
EP  - 1058
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064688806&doi=10.1109%2fJSAC.2019.2906745&partnerID=40&md5=74d4c34ac5f4f6c647eff4c6890c8ac5
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 28</p>
KW  - Cloud computing services
KW  - Cloud container clusters
KW  - Clustering algorithms
KW  - compact exponential optimization
KW  - Computationally efficient
KW  - Container traffic
KW  - Containers
KW  - On-line algorithms
KW  - online algorithms
KW  - Optimal placements
KW  - Placement algorithm
KW  - Placement problems
KW  - Trace driven simulation
ER  - 

TY  - CONF
TI  - Automated analysis of performance and energy consumption for cloud applications
AU  - Chen, F.
AU  - Grundy, J.
AU  - Schneider, J.-G.
AU  - Yang, Y.
AU  - He, Q.
T2  - ICPE 2014 - Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering
AB  - In cloud environments, IT solutions are delivered to users via shared infrastructure. One consequence of this model is that large cloud data centres consume large amounts of energy and produce significant carbon footprints. A key objective of cloud providers is thus to develop resource provisioning and management solutions at minimum energy consumption while still guaranteeing Service Level Agreements (SLAs). However, a thorough understanding of both system performance and energy consumption patterns in complex cloud systems is imperative to achieve a balance of energy efficiency and acceptable performance. In this paper, we present StressCloud, a performance and energy consumption analysis tool for cloud systems. StressCloud can automatically generate load tests and profile system performance and energy consumption data. Using StressCloud, we have conducted extensive experiments to profile and analyse system performance and energy consumption with different types and mixes of runtime tasks. We collected finegrained energy consumption and performance data with different resource allocation strategies, system configurations and workloads. The experimental results show the correlation coefficients of energy consumption, system resource allocation strategies and workload, as well as the performance of the cloud applications. Our results can be used to guide the design and deployment of cloud applications to balance energy and performance requirements. Copyright is held by the owner/author(s). Publication rights licensed to ACM.
DA  - 2014///
PY  - 2014
DO  - 10.1145/2568088.2568093
SP  - 39
EP  - 50
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899684460&doi=10.1145%2f2568088.2568093&partnerID=40&md5=622fb7a332b7288418657e57354f2225
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 29</p>
KW  - Automation
KW  - Carbon footprint
KW  - Cloud computing
KW  - Energy consumption
KW  - Energy consumption analysis
KW  - Energy consumption datum
KW  - Energy efficiency
KW  - Energy utilization
KW  - Green cloud
KW  - Green Clouds
KW  - Minimum energy consumption
KW  - Performance analysis
KW  - Performance requirements
KW  - Resource allocation
KW  - Resource allocation strategies
KW  - Service level agreement (SLAs)
ER  - 

TY  - CONF
TI  - Voyager: Complete Container State Migration
AU  - Nadgowda, S.
AU  - Suneja, S.
AU  - Bila, N.
AU  - Isci, C.
T2  - Proceedings - International Conference on Distributed Computing Systems
AB  - Due to the small memory footprint and fast startup times offerred by container virtualization, made ever more popular by the Docker platform, containers are seeing rapid adoption as a foundational capability to build PaaS and SaaS clouds. For such container clouds, which are fundamentally different from VM clouds, various cloud management services need to be revisited. In this paper, we present our Voyager-just-in-time live container migration service, designed in accordance with the Open Container Initiative (OCI) principles. Voyager is a novel filesystem-agnostic and vendor-agnostic migration service that provides consistent full-system migration. Voyager combines CRIU-based memory migration together with the data federation capabilities of union mounts to minimize migration downtime. With a union view of data between the source and target hosts, Voyager containers can resume operation instantly on the target host, while performing disk state transfer lazily in the background. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/ICDCS.2017.91
SP  - 2137
EP  - 2142
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027285087&doi=10.1109%2fICDCS.2017.91&partnerID=40&md5=ce48676f7441bf8002bc8cf0827f12ea
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 108</p>
KW  - Cloud
KW  - Cloud managements
KW  - Clouds
KW  - Container Migration
KW  - Containers
KW  - Data federation
KW  - Distributed computer systems
KW  - Fast start-up
KW  - Just in time
KW  - Lazy replication
KW  - Lazy Replication
KW  - Small memory footprint
KW  - State migrations
KW  - System migration
ER  - 

TY  - CONF
TI  - Bubble-flux: Precise online QoS management for increased utilization in warehouse scale computers
AU  - Yang, H.
AU  - Breslow, A.
AU  - Mars, J.
AU  - Tang, L.
T2  - Proceedings - International Symposium on Computer Architecture
AB  - Ensuring the quality of service (QoS) for latency-sensitive applications while allowing co-locations of multiple applications on servers is critical for improving server utilization and reducing cost in modern warehouse-scale computers (WSCs). Recent work relies on static profiling to precisely predict the QoS degradation that results from performance interference among co-running applications to increase the number of "safe" co-locations. However, these static profiling techniques have several critical limitations: 1) a priori knowledge of all workloads is required for profiling, 2) it is difficult for the prediction to capture or adapt to phase or load changes of applications, and 3) the prediction technique is limited to only two co-running applications. To address all of these limitations, we present Bubble-Flux, an integrated dynamic interference measurement and online QoS management mechanism to provide accurate QoS control and maximize server utilization. Bubble-Flux uses a Dynamic Bubble to probe servers in real time to measure the instantaneous pressure on the shared hardware resources and precisely predict how the QoS of a latency-sensitive job will be affected by potential co-runners. Once "safe" batch jobs are selected and mapped to a server, Bubble-Flux uses an Online Flux Engine to continuously monitor the QoS of the latency-sensitive application and control the execution of batch jobs to adapt to dynamic input, phase, and load changes to deliver satisfactory QoS. Batch applications remain in a state of flux throughout execution. Our results show that the utilization improvement achieved by Bubble-Flux is up to 2.2x better than the prior static approach. Copyright 2013 ACM.
DA  - 2013///
PY  - 2013
DO  - 10.1145/2485922.2485974
SP  - 607
EP  - 618
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881190996&doi=10.1145%2f2485922.2485974&partnerID=40&md5=bf2c0ba94cd80ac95309a58a0f29e770
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 325</p>
KW  - Computer architecture
KW  - Forecasting
KW  - Hardware resources
KW  - Instantaneous pressures
KW  - Interference measurements
KW  - Multiple applications
KW  - Prediction techniques
KW  - Priori knowledge
KW  - Quality of service
KW  - Static approach
KW  - Utilization improvement
KW  - Warehouses
ER  - 

TY  - JOUR
TI  - Joint optimization of service request routing and instance placement in the microservice system
AU  - Yu, Y.
AU  - Yang, J.
AU  - Guo, C.
AU  - Zheng, H.
AU  - He, J.
T2  - Journal of Network and Computer Applications
AB  - Microservice architecture is a promising architectural style. It decomposes monolithic software into a set of loosely coupled containerized microservices and associates them into multiple microservice chains to serve service requests. The new architecture creates flexibility for service provisioning but also introduces increased energy consumption and low service performance. Efficient resource allocation is critical. Unfortunately, existing solutions are designed at a coarse level for virtual machine (VM)-based clouds and not optimized for such chain-oriented service provisioning. In this paper, we study the resource allocation optimization problem for service request routing and microservice instance placement, so as to jointly reduce both resource usage and chains’ end-to-end response time for saving energy and guaranteeing Quality of Service (QoS). We design detailed workload models for microservices and chains and formulate the optimization problem as a bi-criteria optimization problem. To address it, a three-stage scheme is proposed to search and optimize the trade-off decisions, route service requests into instances and deploy instances to servers in a balanced manner. Through numerical evaluations, we show that while assuring the same QoS, our scheme performs significantly better than and faster than benchmarking algorithms on reducing energy consumption and balancing load. © 2019
DA  - 2019///
PY  - 2019
DO  - 10.1016/j.jnca.2019.102441
VL  - 147
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072519087&doi=10.1016%2fj.jnca.2019.102441&partnerID=40&md5=12d28f55304bc761583ba7edea990eb5
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 47</p>
KW  - Balancing
KW  - Benchmarking algorithm
KW  - Bi-criteria optimization
KW  - Bicriteria optimization
KW  - Economic and social effects
KW  - Efficient resource allocation
KW  - End-to-end response time
KW  - Energy consumption
KW  - Energy utilization
KW  - Load balance
KW  - Microservice
KW  - Microservice chain
KW  - Optimization
KW  - QoS
KW  - Quality of service
KW  - Reducing energy consumption
KW  - Resource allocation
KW  - Resource allocation optimization
ER  - 

TY  - JOUR
TI  - Jamaisvu: Robust Scheduling with Auto-Estimated Job Runtimes
AU  - Tumanov, A.
AU  - Jiang, A.
AU  - Park, J.W.
AU  - Kozuch, M.A.
AU  - Ganger, G.R.
T2  - JamaisVu: Robust Scheduling with Auto-Estimated Job Runtimes
DA  - 2016///
PY  - 2016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050887060&partnerID=40&md5=2fa5b49d2ba113886b70582cdf0e6b3d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 21</p>
ER  - 

TY  - JOUR
TI  - Profile-Guided Three-Phase Virtual Resource Management for Energy Efficiency of Data Centers
AU  - Ding, Z.
AU  - Tian, Y.-C.
AU  - Tang, M.
AU  - Li, Y.
AU  - Wang, Y.-G.
AU  - Zhou, C.
T2  - IEEE Transactions on Industrial Electronics
AB  - Energy efficiency is a critical issue in the management of data centers, which form the backbone of cloud computing. Virtual resource management has a significant impact on improving the energy efficiency of data centers. Despite the progress in this area, virtual resource management has been considered mainly at two separate levels: application assignment and virtual machine placement. It has not been well-investigated in a unified framework for both levels, limiting further improvement in the energy efficiency of data centers. To address this issue, this paper proposes the virtual resource management problem for energy efficiency as a constrained optimization problem. Then, this paper simplifies the problem through profile-guided task classification and problem decomposition for complexity reduction and improved energy efficiency. After that, a three-phase framework and algorithms are presented for profiling and profile updating, task classification and application assignment, and successive virtual machine placement. Experimental studies show energy savings of 8-12% by the three-phase framework compared to the existing technique. © 1982-2012 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/TIE.2019.2902786
VL  - 67
IS  - 3
SP  - 2460
EP  - 2468
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074686925&doi=10.1109%2fTIE.2019.2902786&partnerID=40&md5=100e3d1868bf115799f4e4a1b0b17050
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 22</p>
KW  - Cloud computing
KW  - Constrained optimi-zation problems
KW  - Constrained optimization
KW  - Data center
KW  - Data centers
KW  - energy efficiency
KW  - Energy efficiency
KW  - Framework and algorithms
KW  - Green computing
KW  - Information management
KW  - Natural resources management
KW  - Network security
KW  - Problem decomposition
KW  - profile
KW  - Resource allocation
KW  - task classification
KW  - Task classification
KW  - Virtual addresses
KW  - Virtual machine
KW  - Virtual machine placements
KW  - virtual resource management
KW  - Virtual resource management
ER  - 

TY  - JOUR
TI  - Emerging Trends, Techniques and Open Issues of Containerization: A Review
AU  - Watada, J.
AU  - Roy, A.
AU  - Kadikar, R.
AU  - Pham, H.
AU  - Xu, B.
T2  - IEEE Access
AB  - Containerization is revolutionizing the way that many industries operate, provisioning major impact to modern computing technologies because it is extra lightweight, highly portable, energy, resource and storage efficient, cost-effective, performance efficient, and extremely quick during boot up. These often facilitate efficient load balancing, low-level system maintenance, server consolidation (for efficient energy and resource utilization) and replication of instances over geographical locations for better fault tolerance to escalate application reliability. However, some recent literature have addressed various challenges (such as complex networking, persistent~storage facilities, cross~data centers and multicloud supports, security issues, and lack of available, capable container management APIs, etc.) regarding successful container adoption in industries, which might have resulted in a seemingly meager increase in industrial deployments of containerization over the past few years despite bestowing efficient lightweight virtualization. Moreover, a comprehensive overview of containerizations along with their popularity dynamics has still not been found in contemporary literature, which further extends knowledge gap between developers and available technologies. Hence, current study touches upon different technicalities involved in containerization with potential problems and possible solutions along with various important industrial applications to manifest its existing supports and technical hardships. Finally, we have conducted a comprehensive experimental study to compare the performance of VMs, containers and unikernels in terms of CPU utilization, memory footprints, network bandwidth, execution time and technological maturity using standard benchmarks and observed containers to deliver satisfactory performance in almost all aspects, however, are still not free from issues regarding isolation security, performance stability, lack of available efficient tools for cross-platform support and persistent storage. Unikernels deliver good performance with VM-like isolation but still need to achieve desired technical maturity (in terms of microprocessor stability, process containment, persistent storage, etc.). VMs, on the other hand, are found to provide stable performance throughout, though bigger memory footprints and slower spin up/down remain their biggest weaknesses. © 2013 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/ACCESS.2019.2945930
VL  - 7
SP  - 152443
EP  - 152472
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078452782&doi=10.1109%2fACCESS.2019.2945930&partnerID=40&md5=8cee3372ef3ca737a11b1b4b3bd4438d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 106</p>
KW  - Application reliabilities
KW  - Benchmarking
KW  - Computing technology
KW  - containerization
KW  - Containers
KW  - Cost effectiveness
KW  - Digital storage
KW  - Fault tolerance
KW  - Geographical locations
KW  - Industrial deployment
KW  - Information management
KW  - isolation and security
KW  - management and orchestration
KW  - Packaging
KW  - Performance stability
KW  - Resource utilizations
KW  - Virtual reality
KW  - Virtualization
ER  - 

TY  - JOUR
TI  - The NIST definition of cloud computing
AU  - Mell, P.
AU  - Grance, T.
T2  - The NIST Definition of Cloud Computing
DA  - 2009///
PY  - 2009
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954051808&partnerID=40&md5=f6091a4b8da174c1360c9fe07e10e513
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 8443</p>
ER  - 

TY  - JOUR
AU  - Hemmat, R.A.
AU  - Hafid, A.
T2  - SLA Violation Prediction in Cloud Computing: A Machine Learning Perspective
DA  - 2016///
PY  - 2016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029500612&partnerID=40&md5=84717489e78e87853ff40db2f8f7a964
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 22</p>
ER  - 

TY  - JOUR
TI  - An Efficient Multivariate Autoscaling Framework Using Bi-LSTM for Cloud Computing
AU  - Dang-Quang, N.-M.
AU  - Yoo, M.
T2  - Applied Sciences (Switzerland)
AB  - With the rapid development of 5G technology, the need for a flexible and scalable realtime system for data processing has become increasingly important. By predicting future resource workloads, cloud service providers can automatically provision and deprovision user resources for the system beforehand, to meet service level agreements. However, workload demands fluctuate continuously over time, which makes their prediction difficult. Hence, several studies have proposed a technique called time series forecasting to accurately predict the resource workload. However, most of these studies focused solely on univariate time series forecasting; in other words, they only analyzed the measurement of a single feature. This study proposes an efficient multivariate autoscaling framework using bidirectional long short-term memory (Bi-LSTM) for cloud computing. The system framework was designed based on the monitor–analyze–plan–execute loop. The results obtained from our experiments on different actual workload datasets indicated that the proposed multivariate Bi-LSTM exhibited a root-mean-squared error (RMSE) prediction error 1.84-times smaller than that of the univariate one. Furthermore, it reduced the RMSE prediction error by 6.7% and 5.4% when compared with the multivariate LSTM and convolutional neural network-long shortterm memory (CNN-LSTM) models, respectively. Finally, in terms of resource provisioning, the multivariate Bi-LSTM autoscaler was 47.2% and 14.7% more efficient than the multivariate LSTM and CNN-LSTM autoscalers, respectively. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.
DA  - 2022///
PY  - 2022
DO  - 10.3390/app12073523
VL  - 12
IS  - 7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128185483&doi=10.3390%2fapp12073523&partnerID=40&md5=27d52741a0a8dc2accedaed973beaa9e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 24</p>
KW  - autoscaling
KW  - cloud computing
KW  - multivariate variables
KW  - resource estimation
KW  - time series forecasting
ER  - 

TY  - JOUR
TI  - Reliability modelling and optimization for microservice-based cloud application using multi-agent system
AU  - Liu, Z.
AU  - Yu, H.
AU  - Fan, G.
AU  - Chen, L.
T2  - IET Communications
AB  - In the process of the continuous development of the Internet of Things, cloud computing has been applied in many fields, how to guarantee the quality of service, such as low latency, high bandwidth, high reliability etc., has become a challenging problem. This paper proposes a method to model and optimize reliability for microservice-based cloud applications using multi-agent system (MAS), thus maximizing the reliability of cloud computing and dynamically scheduling microservices to minimize the delay within the budget. Firstly, a dynamic microservice scheduling scheme is proposed to provide efficient computing services by using MAS. A hierarchical cloud computing model is formed by predicated Petri net (PrT net) and the properties of constructed model are analysed. Secondly, agents have been utilized to describe the essential characteristics of microservice scheduling process in the cloud applications. The partial critical path (PCP) aims to maximize the reliability of cloud applications under the limitation of budget and meet the user-defined deadline. Finally, the proposed PCPRO algorithm has been applied to cloud environment, which is suitable for different scientific workflows in the cloud computing environment. The effectiveness of this method is verified by simulation, the experiment results show the effectiveness of the proposed method. © 2022 The Authors. IET Communications published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.
DA  - 2022///
PY  - 2022
DO  - 10.1049/cmu2.12371
VL  - 16
IS  - 10
SP  - 1182
EP  - 1199
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126098914&doi=10.1049%2fcmu2.12371&partnerID=40&md5=9cb89b082c15ae31fc8cc04ba64c8d4b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 14</p>
KW  - Budget control
KW  - Cloud applications
KW  - Cloud computing
KW  - Cloud-computing
KW  - Continuous development
KW  - High bandwidth
KW  - High reliability
KW  - Low latency
KW  - Modeling and optimization
KW  - Multi agent systems
KW  - Petri nets
KW  - Quality of service
KW  - Quality-of-service
KW  - Reliability
KW  - Reliability modelling
KW  - Reliability optimization
KW  - Scheduling
ER  - 

TY  - JOUR
T2  - SaaS vs. PaaS vs. IaaS: What’s The Difference & How To Choose—BMC Software|Blogs
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132272208&partnerID=40&md5=f71cb3f5fe8111b9056b65038283cfb4
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Energy-efficient application assignment in profile-based data center management through a Repairing Genetic Algorithm
AU  - Vasudevan, M.
AU  - Tian, Y.-C.
AU  - Tang, M.
AU  - Kozan, E.
AU  - Zhang, X.
T2  - Applied Soft Computing Journal
AB  - The massive deployment of data center services and cloud computing comes with exorbitant energy costs and excessive carbon footprint. This demands green initiatives and energy-efficient strategies for greener data centers. Assignment of an application to different virtual machines has a significant impact on both energy consumption and resource utilization in virtual resource management of a data centre. However, energy efficiency and resource utilization are conflicting in general. Thus, it is imperative to develop a scalable application assignment strategy that maintains a trade-off between energy efficiency and resource utilization. To address this problem, this paper formulates application assignment to virtual machines as a profile-driven optimization problem under constraints. Then, a Repairing Genetic Algorithm (RGA) is presented to solve the large-scale optimization problem. It enhances penalty-based genetic algorithm by incorporating the Longest Cloudlet Fastest Processor (LCFP), from which an initial population is generated, and an infeasible-solution repairing procedure (ISRP). The application assignment with RGA is integrated into a three-layer energy management framework for data centres. Experiments are conducted to demonstrate the effectiveness of the presented approach, e.g., 23% less energy consumption and 43% more resource utilization in comparison with the steady-state Genetic Algorithm (GA) under investigated scenarios. © 2018 Elsevier B.V.
DA  - 2018///
PY  - 2018
DO  - 10.1016/j.asoc.2018.03.016
VL  - 67
SP  - 399
EP  - 408
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044168150&doi=10.1016%2fj.asoc.2018.03.016&partnerID=40&md5=0c3c4578b839a55ffeb6f26a238862d0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 24</p>
KW  - Application assignment
KW  - Carbon
KW  - Carbon footprint
KW  - Cloud computing
KW  - Data center
KW  - Data center management
KW  - Data centers
KW  - Economic and social effects
KW  - Energy efficiency
KW  - Energy efficient strategies
KW  - Energy management
KW  - Energy utilization
KW  - Genetic algorithm
KW  - Genetic algorithms
KW  - Green computing
KW  - Information management
KW  - Large-scale optimization
KW  - Network security
KW  - Optimization
KW  - Profile-driven optimizations
KW  - Repair
KW  - Resource scheduling
KW  - Resource-scheduling
KW  - Scheduling algorithms
KW  - Steady-state genetic algorithms
KW  - Virtual addresses
KW  - Virtual machine
KW  - Virtual resource management
ER  - 

TY  - JOUR
TI  - Towards green cloud computing: Demand allocation and pricing policies for cloud service brokerage
AU  - Qiu, C.
AU  - Shen, H.
AU  - Chen, L.
T2  - IEEE Transactions on Big Data
AB  - Functioning as an intermediary between tenants and cloud providers, cloud service brokerages (CSBs) can bring about great benefits to the cloud market. As energy costs of cloud computing have been increasing rapidly, there is a need for cloud providers to optimize energy efficiency while maintain high service level performance to tenants, not only for their own benefit but also for social welfares. Thus, for green cloud companies, two questions have arisen: 1) under what pricing policies from the cloud providers to the CSB, a profit-driven CSB is willing to minimize the total energy cost while satisfy tenant demands and 2) how should a CSB distribute tenants demands to achieve this objective? To address question 1), we find a pricing policy for cloud providers such that maximizing CSBs profit is equivalent to minimizing cloud providers energy cost. To address question 2), we first devise a greedy solution, and then propose an approximation algorithm and a decomposition-based solution with a constant approximation ratio. Both simulation and real-world Amazon EC2 experimental results demonstrate the effectiveness of our pricing policy to incentivize CSBs to save energy and the superior performance of our algorithms in energy efficiency and resource utilization over the previous algorithms. © 2015 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/TBDATA.2018.2823330
VL  - 5
IS  - 2
SP  - 238
EP  - 251
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140807374&doi=10.1109%2fTBDATA.2018.2823330&partnerID=40&md5=b400573b46ab15d79e7d0c02672746ba
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 17</p>
KW  - Approximation algorithms
KW  - Cloud computing
KW  - Cloud providers
KW  - cloud service brokerage
KW  - Cloud service brokerage
KW  - Cloud services
KW  - Cloud-computing
KW  - Computing demands
KW  - Costs
KW  - demand allocation
KW  - Demand allocation
KW  - Distributed database systems
KW  - Energy cost
KW  - Energy efficiency
KW  - Green Clouds
KW  - Green computing
KW  - Performance
KW  - pricing policy
KW  - Pricing policy
KW  - Profitability
ER  - 

TY  - CONF
TI  - Stratus: Cost-aware container scheduling in the public cloud
AU  - Chung, A.
AU  - Park, J.W.
AU  - Ganger, G.R.
T2  - SoCC 2018 - Proceedings of the 2018 ACM Symposium on Cloud Computing
AB  - Stratus is a new cluster scheduler specialized for orchestrating batch job execution on virtual clusters, dynamically allocated collections of virtual machine instances on public IaaS platforms. Unlike schedulers for conventional clusters, Stratus focuses primarily on dollar cost considerations, since public clouds provide effectively unlimited, highly heterogeneous resources allocated on demand. But, since resources are charged-for while allocated, Stratus aggressively packs tasks onto machines, guided by job runtime estimates, trying to make allocated resources be either mostly full (highly utilized) or empty (so they can be released to save money). Simulation experiments based on cluster workload traces from Google and TwoSigma show that Stratus reduces cost by 17–44% compared to state-of-the-art approaches to virtual cluster scheduling. © 2018 Association for Computing Machinery.
DA  - 2018///
PY  - 2018
DO  - 10.1145/3267809.3267819
SP  - 121
EP  - 134
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058982961&doi=10.1145%2f3267809.3267819&partnerID=40&md5=92b2e003340b423f8524c57fc0a3dace
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 70</p>
KW  - Cloud computing
KW  - Cluster computing
KW  - Cluster scheduling
KW  - Container scheduling
KW  - Cost-aware
KW  - Heterogeneous resources
KW  - Public clouds
KW  - Runtime estimates
KW  - Scheduling
KW  - State-of-the-art approach
KW  - Transient server
KW  - Virtual clusters
ER  - 

TY  - JOUR
T2  - KubeEdge: Kubernetes Native Edge Computing Framework.
DA  - 2021///
PY  - 2021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119094775&partnerID=40&md5=cdb349b672f62bdfb7b9d2659dace76d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2</p>
ER  - 

TY  - CONF
TI  - A Computation Offloading Strategy in Satellite Terrestrial Networks with Double Edge Computing
AU  - Wang, Y.
AU  - Zhang, J.
AU  - Zhang, X.
AU  - Wang, P.
AU  - Liu, L.
T2  - 2018 IEEE International Conference on Communication Systems, ICCS 2018
AB  - Mobile edge computing (MEC) can efficiently minimize computational latency, reduce response time, and improve quality-of-service (QoS) by offloading tasks in the access network. Although lots of edge computation offloading schemes have been proposed in terrestrial networks, the hybrid satellite terrestrial communication, as an emerging trend for the next generation communication, has not taken edge computing into consideration. In this paper, a novel satellite terrestrial network with double edge computing is introduced to reap the benefits of providing computing service for remote areas. A strategy is designed to solve the problem of efficiently scheduling the edge servers distributed in the satellite terrestrial networks to provide more powerful edge computing services. For the purpose of allocating satellite edge computing resource efficiently in the strategy, a double edge computation offloading algorithm is proposed to optimize energy consumption and reduce latency by assigning tasks to edge servers with minimal cost. Numerical results verify that the proposed algorithm can reduce the average latency and the energy consumption by approximately 45% and 49% respectively. The study on the number selection of utilized satellite edge servers provides an insight for following studies of edge servers scheduling in satellite terrestrial networks. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/ICCS.2018.8689224
SP  - 450
EP  - 455
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065047384&doi=10.1109%2fICCS.2018.8689224&partnerID=40&md5=24513282b98a1e188b72d85e7731281b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 123</p>
KW  - computation offloading
KW  - Computation offloading
KW  - Computational efficiency
KW  - Computing resource
KW  - Computing services
KW  - double edge computing
KW  - Edge computing
KW  - Energy utilization
KW  - Green computing
KW  - Hybrid satellites
KW  - Numerical results
KW  - Quality of service
KW  - satellite terrestrial networks
KW  - Satellite-terrestrial network
KW  - Satellites
KW  - Scheduling
KW  - Terrestrial communication
KW  - Terrestrial networks
ER  - 

TY  - JOUR
TI  - Dyme: Dynamic Microservice Scheduling in Edge Computing Enabled IoT
AU  - Samanta, A.
AU  - Tang, J.
T2  - IEEE Internet of Things Journal
AB  - In recent years, the rapid development of mobile edge computing (MEC) provides an efficient execution platform at the edge for Internet-of-Things (IoT) applications. Nevertheless, the MEC also provides optimal resources to different microservices, however, underlying network conditions and infrastructures inherently affect the execution process in MEC. Therefore, in the presence of varying network conditions, it is necessary to optimally execute the available task of end users while maximizing the energy efficiency in edge platform and we also need to provide fair Quality-of-Service (QoS). On the other hand, it is necessary to schedule the microservices dynamically to minimize the total network delay and network price. Thus, in this article, unlike most of the existing works, we propose a dynamic microservice scheduling scheme for MEC. We design the microservice scheduling framework mathematically and also discuss the computational complexity of the scheduling algorithm. Extensive simulation results show that the microservice scheduling framework significantly improves the performance metrics in terms of total network delay, average price, satisfaction level, energy consumption rate (ECR), failure rate, and network throughput over other existing baselines. © 2014 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/JIOT.2020.2981958
VL  - 7
IS  - 7
SP  - 6164
EP  - 6174
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089309950&doi=10.1109%2fJIOT.2020.2981958&partnerID=40&md5=38b71fcbb6b26d203c0c225cde1c059e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 131</p>
KW  - Dynamic microservice scheduling
KW  - edge computing
KW  - Edge computing
KW  - Energy consumption rates
KW  - Energy efficiency
KW  - Energy utilization
KW  - Execution platforms
KW  - Extensive simulations
KW  - Failure analysis
KW  - Internet of things
KW  - Internet of Things (IoT)
KW  - Internet of Things (IOT)
KW  - microservice
KW  - Network throughput
KW  - Performance metrics
KW  - Quality of service
KW  - Quality-of-Service (QoS)
KW  - Scheduling
KW  - Scheduling frameworks
KW  - Underlying networks
ER  - 

TY  - CONF
TI  - A Comparative Research on Open Source Edge Computing Systems
AU  - Liang, J.
AU  - Liu, F.
AU  - Li, S.
AU  - Cai, Z.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - With the development of edge computing, open source communities have put forward several edge computing systems. This paper discussed about edge computing and its current situation, then presented typical open source edge computing systems such as EdgeX Foundry, Azure IoT Edge, CORD, Apache Edgent and Akraino Edge Stack, and gave a comparison about them on their characteristics. A comparison study on their characteristics were given to help users to understand these open source edge computing systems and make choices. © 2019, Springer Nature Switzerland AG.
DA  - 2019///
PY  - 2019
DO  - 10.1007/978-3-030-24265-7_14
VL  - 11633 LNCS
SP  - 157
EP  - 170
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070225902&doi=10.1007%2f978-3-030-24265-7_14&partnerID=40&md5=871cff564de3a48b4b9c9f2f527b62d4
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 9</p>
KW  - Artificial intelligence
KW  - Comparative research
KW  - Comparison study
KW  - Computing system
KW  - Current situation
KW  - Edge computing
KW  - Edge computing systems
KW  - Open source
KW  - Open source communities
KW  - Open sources
KW  - Open systems
ER  - 

TY  - CONF
TI  - CaaS: A Management Framework of Edge Container as a Service for Business Workload
AU  - Cao, L.
AU  - Merican, A.
AU  - Tootaghaj, D.Z.
AU  - Ahmed, F.
AU  - Sharma, P.
AU  - Saxena, V.
T2  - EdgeSys 2021 - Proceedings of the 4th International Workshop on Edge Systems, Analytics and Networking, Part of EuroSys 2021
AB  - Enterprises are containerizing their business applications and extending those applications from cloud to edge to achieve better flexibility, agility, and performance for their business workload. Unlike data centers, edge sites including infrastructure and orchestration systems are often heterogeneous and highly customized depending on the resource availability, business requirements of the use case, and technical requirements of the application. However, in many business use cases, the lack of IT professionals with proper domain expertise makes it very challenging to create, manage, and support heterogeneous containerized edge sites at a large scale. In this work, we present the eCaaS framework that provides automated lifecycle management of containerized edge sites and applications. With eCaaS, users can create customized edge sites with only high-level business intents which are analyzed and translated to deployment templates with low-level specifications. The edge site deployment templates are then automatically executed to build, deploy, and configure the containerized edge sites and applications. To support more customization options in the future, eCaaS decouples user intents, deployment rules, and deployment specifications and formulates deployment template generation as an SMT problem to achieve better scalability and extensibility. For creating an edge site with five nodes, eCaaS takes less than one second to generate the deployment template and less than ten minutes to complete the entire deployment.  © 2021 ACM.
DA  - 2021///
PY  - 2021
DO  - 10.1145/3434770.3459741
SP  - 73
EP  - 78
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104683347&doi=10.1145%2f3434770.3459741&partnerID=40&md5=c611321688b3f8a02de87b8dd36cb37d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 5</p>
KW  - Business applications
KW  - Business requirement
KW  - Containers
KW  - Domain expertise
KW  - Life cycle
KW  - Life-cycle management
KW  - Management frameworks
KW  - Resource availability
KW  - Specifications
KW  - Technical requirement
KW  - Template generation
ER  - 

TY  - JOUR
T2  - K3s: Lightweight Kubernetes
DA  - 2021///
PY  - 2021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107067452&partnerID=40&md5=e81cca55863540110c756df6d8b91e2a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 5</p>
ER  - 

TY  - JOUR
TI  - System integration of terrestrial mobile communication and satellite communication-The trends, challenges and key technologies in B5G and 6G
AU  - Chen, S.
AU  - Sun, S.
AU  - Kang, S.
T2  - China Communications
AB  - Mobile communication standards have been developed into a new era of B5G and 6G. In recent years, low earth orbit (LEO) satellites and space Internet have become hot topics. The integrated satellite and terrestrial systems have been widely discussed by industries and academics, and even are expected to be applied in those huge constellations in construction. This paper points out the trends of two stages towards system integration of the terrestrial mobile communication and the satellite communications: to be compatible with 5G, and to be integrated within 6G. Based on analysis of the challenges of both stages, key technologies are thereafter analyzed in detail, covering both air interface currently discussed in 3GPP for B5G and also novel network architecture and related transmission technologies toward future 6G. © 2013 China Institute of Communications.
DA  - 2020///
PY  - 2020
DO  - 10.23919/JCC.2020.12.011
VL  - 17
IS  - 12
SP  - 156
EP  - 171
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099147415&doi=10.23919%2fJCC.2020.12.011&partnerID=40&md5=5e12dd7a62a1f2eda6afe0b870f6d72f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 322</p>
KW  - 6G
KW  - B5G
KW  - satellite communication
KW  - space internet
KW  - system integration
KW  - terrestrial mobile communication
ER  - 

TY  - CONF
TI  - Collaborative Transmission in Hybrid Satellite-Terrestrial Networks: Design and Implementation
AU  - Jia, Y.
AU  - Zhang, J.
AU  - Wang, P.
AU  - Liu, L.
AU  - Zhang, X.
AU  - Wang, W.
T2  - IEEE Wireless Communications and Networking Conference, WCNC
AB  - With the rapid development of 5G technology, there is an increasing demand of high-definition (HD) video service, so that efficient content transmission is expected to guaranteed the quality of experience of users. However, traditional terrestrial networks can hardly support this kind of service due to the limited coverage and capability especially in scene of remote area or peak hours of hotspots. Under the support of High Throughput Satellite, satellite can serve as a supplement in hybrid satellite-terrestrial networks (HSTN) to provide various of services. Specifically, aggregation in packet level for collaborative transmission between satellite and terrestrial networks is in direction of development which should be reconsidered. In this paper, a classic SDN-aware HSTN architecture is adopted to capture content information of the system and make strategy dynamically for efficient distribution of content in finer scale. Key technologies, including tag method, path selection strategy and reordering scheme, are proposed to achieve collaborative transmission of HD videos. Finally, complete implementation of a prototype, a Hardware In the Loop (HIL) platform with the SITL module of OPNET, is built to illustrate the feasibility and effectiveness of the proposed solutions. Numerical results show that collaborative transmission in HSTN can effectively realize link aggregation, which has great significance for complex conditions in future networks. © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/WCNC45663.2020.9120560
VL  - 2020-May
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087274899&doi=10.1109%2fWCNC45663.2020.9120560&partnerID=40&md5=71f5fd33a760795c7f04cf1598db8674
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 10</p>
KW  - 5G mobile communication systems
KW  - collaborative transmission
KW  - Content information
KW  - Content transmission
KW  - Design and implementations
KW  - Digital television
KW  - Hardware In the Loop
KW  - Hardware in the loops
KW  - High-definition videos
KW  - hybrid satellite-terrestrial networks
KW  - OPNET.
KW  - Path selection strategies
KW  - Quality of experience (QoE)
KW  - Quality of service
KW  - Satellites
KW  - software defined networks
KW  - Terrestrial networks
KW  - User experience
ER  - 

TY  - JOUR
TI  - A Survey on Mobile Edge Networks: Convergence of Computing, Caching and Communications
AU  - Wang, S.
AU  - Zhang, X.
AU  - Zhang, Y.
AU  - Wang, L.
AU  - Yang, J.
AU  - Wang, W.
T2  - IEEE Access
AB  - As the explosive growth of smart devices and the advent of many new applications, traffic volume has been growing exponentially. The traditional centralized network architecture cannot accommodate such user demands due to heavy burden on the backhaul links and long latency. Therefore, new architectures, which bring network functions and contents to the network edge, are proposed, i.e., mobile edge computing and caching. Mobile edge networks provide cloud computing and caching capabilities at the edge of cellular networks. In this survey, we make an exhaustive review on the state-of-the-art research efforts on mobile edge networks. We first give an overview of mobile edge networks, including definition, architecture, and advantages. Next, a comprehensive survey of issues on computing, caching, and communication techniques at the network edge is presented. The applications and use cases of mobile edge networks are discussed. Subsequently, the key enablers of mobile edge networks, such as cloud technology, SDN/NFV, and smart devices are discussed. Finally, open research challenges and future directions are presented as well. © 2013 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/ACCESS.2017.2685434
VL  - 5
SP  - 6757
EP  - 6779
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026285613&doi=10.1109%2fACCESS.2017.2685434&partnerID=40&md5=15df1f231649db466b92db53c693a85d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 869</p>
KW  - Cloud technologies
KW  - Communication techniques
KW  - computational offloading
KW  - content delivery
KW  - Content delivery
KW  - D2D
KW  - Distributed computer systems
KW  - Edge caching
KW  - Edge computing
KW  - mobile edge caching
KW  - Mobile edge computing
KW  - Network architecture
KW  - Network functions
KW  - NFV
KW  - Research challenges
KW  - SDN
KW  - Surveys
ER  - 

TY  - JOUR
TI  - Physical Layer Security for Multiuser Satellite Communication Systems with Threshold-Based Scheduling Scheme
AU  - Guo, K.
AU  - An, K.
AU  - Zhang, B.
AU  - Huang, Y.
AU  - Tang, X.
AU  - Zheng, G.
AU  - Tsiftsis, T.A.
T2  - IEEE Transactions on Vehicular Technology
AB  - Satellite communication (SatCom) has attracted much attention due to its inherent characteristics. Security issues have gained severe concerns in SatCom since it is susceptible to be illegally eavesdropped by malicious ground stations within large-scale wireless coverage. In this paper, we investigate the physical layer security of a multiuser SatCom system in the presence of multiple eavesdroppers. Particularly, we propose a threshold-based scheduling scheme, where the geographically clustered eavesdroppers with both the colluded and collaborated eavesdropping scenarios are assumed. Specifically, closed-form expression for the secrecy outage probability (SOP) is derived for the passive eavesdropping scenario when the channel state information (CSI) of the eavesdroppers is unavailable. Moreover, we obtain a closed-form expression for the average secrecy capacity (ASC) of the considered system under the proposed user scheduling scheme. In order to get further insights of the proposed scheduling scheme at high signal-to-noise ratios (SNRs), the asymptotic analysis for the SOP and ASC is also demonstrated. Moreover, the reduced percentage with respect to number of user examination is also given, which validates the simplicity and efficiency of our proposed scheme compared to the traditional approaches. Numerical results deduce that with the proposed scheme, a comparable system performance with regard to the maximal selection (MS) scheme can be achieved. © 1967-2012 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/TVT.2020.2979496
VL  - 69
IS  - 5
SP  - 5129
EP  - 5141
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085063068&doi=10.1109%2fTVT.2020.2979496&partnerID=40&md5=4b5185b04585a3ef4a02343a2ac11def
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 92</p>
KW  - Asymptotic analysis
KW  - average secrecy capacity (ASC)
KW  - Channel state information
KW  - Closed-form expression
KW  - High signal-to-noise ratio
KW  - Inherent characteristics
KW  - Physical layer
KW  - Physical layer security
KW  - Satellite communication (SatCom)
KW  - Satellite communication systems
KW  - Satellite communications
KW  - Scheduling
KW  - Scheduling schemes
KW  - Secrecy outage probabilities
KW  - secrecy outage probability (SOP)
KW  - Security systems
KW  - Signal to noise ratio
KW  - threshold-based scheduling
KW  - Traditional approaches
ER  - 

TY  - JOUR
TI  - Double-edge intelligent integrated satellite terrestrial networks
AU  - Zhang, J.
AU  - Zhang, X.
AU  - Wang, P.
AU  - Liu, L.
AU  - Wang, Y.
T2  - China Communications
AB  - The efficient integration of satellite and terrestrial networks has become an important component for 6G wireless architectures to provide highly reliable and secure connectivity over a wide geographical area. As the satellite and cellular networks are developed separately these years, the integrated network should synergize the communication, storage, computation capabilities of both sides towards an intelligent system more than mere consideration of coexistence. This has motivated us to develop double-edge intelligent integrated satellite and terrestrial networks (DILIGENT). Leveraging the boost development of multi-access edge computing (MEC) technology and artificial intelligence (AI), the framework is entitled with the systematic learning and adaptive network management of satellite and cellular networks. In this article, we provide a brief review of the state-of-art contributions from the perspective of academic research and standardization. Then we present the overall design of the proposed DILIGENT architecture, where the advantages are discussed and summarized. Strategies of task offloading, content caching and distribution are presented. Numerical results show that the proposed network architecture outperforms the existing integrated networks.  © 2013 China Institute of Communications.
DA  - 2020///
PY  - 2020
DO  - 10.23919/JCC.2020.09.011
VL  - 17
IS  - 9
SP  - 128
EP  - 146
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094903592&doi=10.23919%2fJCC.2020.09.011&partnerID=40&md5=7743d87d67b2e6206aa466ac32511a45
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 48</p>
KW  - 6G Networks
KW  - content caching and distribution
KW  - edge intelligence
KW  - integrated satellite and terrestrial networks
KW  - non-terrestrial networks
KW  - task offloading
ER  - 

TY  - CONF
TI  - Optimizing network performance of computing pipelines in distributed environments
AU  - Wu, Q.
AU  - Gu, Y.
AU  - Zhu, M.
AU  - Rao, N.S.V.
T2  - IPDPS Miami 2008 - Proceedings of the 22nd IEEE International Parallel and Distributed Processing Symposium, Program and CD-ROM
AB  - Supporting high performance computing pipelines over wide-area networks is critical to enabling large-scale distributed scientific applications that require fast responses for interactive operations or smooth flows for data streaming. We construct analytical cost models for computing modules, network nodes, and communication links to estimate the computing times on nodes and the data transport times over connections. Based on these time estimates, we present the Efficient Linear Pipeline Configuration method based on dynamic programming that partitions the pipeline modules into groups and strategically maps them onto a set of selected computing nodes in a network to achieve minimum end-to-end delay or maximum frame rate. We implemented this method and evaluated its effectiveness with experiments on a large set of simulated application pipelines and computing networks. The experimental results show that the proposed method outperforms the Streamline and Greedy algorithms. These results, together with polynomial computational complexity, make our method a potential scalable solution for large practical deployments. ©2008 IEEE.
DA  - 2008///
PY  - 2008
DO  - 10.1109/IPDPS.2008.4536465
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-51049114925&doi=10.1109%2fIPDPS.2008.4536465&partnerID=40&md5=77a71149440384046a16a3dd2ae0bf14
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 20</p>
KW  - Chlorine compounds
KW  - Computational complexity
KW  - Computer networks
KW  - Data reduction
KW  - Distributed parameter networks
KW  - High performance computing
KW  - Interactive operations
KW  - Java programming language
KW  - Mathematical programming
KW  - Parallel and distributed processing
KW  - Pipelines
KW  - Scientific applications
KW  - Systems engineering
KW  - Wide area networks
ER  - 

TY  - JOUR
TI  - Resource Allocation for Cognitive Satellite-HAP-Terrestrial Networks With Non-Orthogonal Multiple Access
AU  - Liu, R.
AU  - Guo, K.
AU  - An, K.
AU  - Huang, Y.
AU  - Zhou, F.
AU  - Zhu, S.
T2  - IEEE Transactions on Vehicular Technology
AB  - Cognitive radio and non-orthogonal multiple access are promising to improve the spectrum efficiency for the satellite-high altitude platform (HAP)-terrestrial networks. To achieve the optimal system performance and guarantee the quality of service, we formulate a constrained mixed integer nonlinear programming problem to maximize the sum rate of secondary network for the considered NOMA-enabled cognitive satellite-HAP-terrestrial network, while considering the constraints of the quality of service of primary users, the maximum power of HAP, and the number of secondary users in a NOMA group. To tackle the non-convex optimization problem, we decouple it into subchannel assignment and power assignment subproblems. Then, a greedy heuristic algorithm is proposed to assign subchannels to secondary users. Moreover, to solve the power allocation subproblem, we further design a power allocation algorithm by utilizing successive convex approximation, dual decomposition, and subgradient methods. On this basis, an iterative joint resource allocation algorithm is designed. Numerical simulations and comparisons with the orthogonal multiple access scheme are provided to verify the effectiveness of the proposed scheme. © 1967-2012 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/TVT.2023.3252642
VL  - 72
IS  - 7
SP  - 9659
EP  - 9663
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151533759&doi=10.1109%2fTVT.2023.3252642&partnerID=40&md5=48720bc968bcb6213cb7cf2365407fe9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 36</p>
KW  - Approximation algorithms
KW  - Array processing
KW  - Array signal processing
KW  - cognitive radio
KW  - Cognitive radio
KW  - Convex optimization
KW  - Heuristic algorithms
KW  - Heuristics algorithm
KW  - Integer programming
KW  - Interference
KW  - Iterative methods
KW  - Multiple access
KW  - NOMA
KW  - Non-orthogonal
KW  - non-orthogonal multiple access
KW  - Non-orthogonal multiple access
KW  - Nonlinear programming
KW  - Quality of service
KW  - Quality-of-service
KW  - Radio interference
KW  - resource allocation
KW  - Resource allocation
KW  - Resource management
KW  - Resources allocation
KW  - Satellite communication systems
KW  - Satellite-HAP-terrestrial networks
KW  - Satellite-high altitude platform-terrestrial network
KW  - Satellites
KW  - Spectrum efficiency
KW  - Terrestrial networks
ER  - 

TY  - JOUR
TI  - Performance modeling and workflow scheduling of microservice-based applications in clouds
AU  - Bao, L.
AU  - Wu, C.
AU  - Bu, X.
AU  - Ren, N.
AU  - Shen, M.
T2  - IEEE Transactions on Parallel and Distributed Systems
AB  - Microservice has been increasingly recognized as a promising architectural style for constructing large-scale cloud-based applications within and across organizational boundaries. This microservice-based architecture greatly increases application scalability, but meanwhile incurs an expensive performance overhead, which calls for a careful design of performance modeling and task scheduling. However, these problems have thus far remained largely unexplored. In this paper, we develop a performance modeling and prediction method for independent microservices, design a three-layer performance model for microservice-based applications, formulate a Microservice-based Application Workflow Scheduling problem for minimum end-to-end delay under a user-specified Budget Constraint (MAWS-BC), and propose a heuristic microservice scheduling algorithm. The performance modeling and prediction method are validated and justified by experimental results generated through a well-known microservice benchmark on disparate computing nodes, and the performance superiority of the proposed scheduling solution is illustrated by extensive simulation results in comparison with existing algorithms. © 1990-2012 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/TPDS.2019.2901467
VL  - 30
IS  - 9
SP  - 2101
EP  - 2116
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070656668&doi=10.1109%2fTPDS.2019.2901467&partnerID=40&md5=d75e7f1cff1ad9a14d73bf84e309b633
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 119</p>
KW  - Application scalability
KW  - Benchmarking
KW  - Budget control
KW  - cloud computing
KW  - Cloud computing
KW  - Cloud-based applications
KW  - Extensive simulations
KW  - Forecasting
KW  - Heuristic methods
KW  - Microservice
KW  - Multitasking
KW  - Organizational boundaries
KW  - performance modeling and prediction
KW  - Performance modeling and prediction
KW  - Scheduling
KW  - Scheduling algorithms
KW  - task scheduling
KW  - Task-scheduling
KW  - Workflow scheduling
ER  - 

TY  - JOUR
TI  - Satellite-Terrestrial Integrated Edge Computing Networks: Architecture, Challenges, and Open Issues
AU  - Xie, R.
AU  - Tang, Q.
AU  - Wang, Q.
AU  - Liu, X.
AU  - Yu, F.R.
AU  - Huang, T.
T2  - IEEE Network
AB  - STN has been considered a novel network architecture to accommodate a variety of services and applications in future networks. Being a promising paradigm, MEC has been regarded as a key technology-enabler to offer further service innovation and business agility in STN. However, most of the existing research in MEC enabled STN regards a satellite network as a relay network, and the feasibility of tasks processing directly on the satellites is largely ignored. Moreover, the problem of multi-layer edge computing architecture design and heterogeneous edge computing resource co-scheduling, have not been fully considered. Therefore, different from previous works, in this article, we propose a novel architecture named STECN, in which computing resources exist in multi-layer heterogeneous edge computing clusters. The detailed functional components of the proposed STECN are discussed, and we present the promising technical challenges, including meeting QoE requirements, cooperative computation offloading, multi-node task scheduling, mobility management and fault/failure recovery. Finally, some potential research issues for future research are highlighted. © 1986-2012 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/MNET.011.1900369
VL  - 34
IS  - 3
SP  - 224
EP  - 231
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082557931&doi=10.1109%2fMNET.011.1900369&partnerID=40&md5=5b74c6ef5dca3c190835e7a8488a52c4
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 266</p>
KW  - Computer architecture
KW  - Computing architecture
KW  - Computing resource
KW  - Cooperative computation
KW  - Edge computing
KW  - Functional components
KW  - Mobility management
KW  - Network architecture
KW  - Potential researches
KW  - Satellites
KW  - Scheduling
KW  - Services and applications
KW  - Technical challenges
ER  - 

TY  - CONF
TI  - Supporting distributed application workflows in heterogeneous computing environments
AU  - Wu, Q.
AU  - Gu, Y.
T2  - Proceedings of the International Conference on Parallel and Distributed Systems - ICPADS
AB  - Next-generation computation-intensive applications in various fields of science and engineering feature large-scale computing workflows with complex structures that are often modeled as directed acyclic graphs. Supporting such task graphs and optimizing their end-to-end network performances in heterogeneous computing environments are critical to the success of these distributed applications that require fast response. We construct analytical models for computing modules, network nodes, and communication links to estimate data processing and transport overhead, and formulate the task graph mapping with node reuse and resource sharing for minimum end-to-end delay as an NP-complete optimization problem. We propose a heuristic approach to this problem that recursively computes and maps the critical path to the network using a dynamic programming-based procedure. The performance superiority of the proposed approach is justified by an extensive set of experiments on simulated data sets in comparison with existing methods. © 2008 IEEE.
DA  - 2008///
PY  - 2008
DO  - 10.1109/ICPADS.2008.40
SP  - 3
EP  - 10
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-60649109658&doi=10.1109%2fICPADS.2008.40&partnerID=40&md5=62379e43a6133fbdb84b0076a589fa9f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 51</p>
KW  - Analytical models
KW  - Applications
KW  - Communication links
KW  - Complex structures
KW  - Critical paths
KW  - Data processing
KW  - Data storage equipment
KW  - Directed acyclic graphs
KW  - Distributed applications
KW  - Existing methods
KW  - Fast response
KW  - Graph mapping
KW  - Heterogeneous computing
KW  - Heuristic algorithm
KW  - Heuristic algorithms
KW  - Heuristic approaches
KW  - Heuristic methods
KW  - Large-scale computing
KW  - Minimum end-to-end delay
KW  - Network nodes
KW  - NP-complete
KW  - Optimization problems
KW  - Portals
KW  - Pptimization problem
KW  - Resource-sharing
KW  - Science and engineerings
KW  - Simulated data sets
KW  - Systems engineering
KW  - Task graphs
KW  - Wireless local area networks (WLAN)
KW  - Work-flows
ER  - 

TY  - CONF
TI  - Rearchitecting Kubernetes for the Edge
AU  - Jeffery, A.
AU  - Howard, H.
AU  - Mortier, R.
T2  - EdgeSys 2021 - Proceedings of the 4th International Workshop on Edge Systems, Analytics and Networking, Part of EuroSys 2021
AB  - Recent years have seen Kubernetes emerge as a primary choice for container orchestration. Kubernetes largely targets the cloud environment but new use cases require performant, available and scalable orchestration at the edge. Kubernetes stores all cluster state in etcd, a strongly consistent key-value store. We find that at larger etcd cluster sizes, offering higher availability, write request latency significantly increases and throughput decreases similarly. Coupled with approximately 30% of Kubernetes requests being writes, this directly impacts the request latency and availability of Kubernetes, reducing its suitability for the edge. We revisit the requirement of strong consistency and propose an eventually consistent approach instead. This enables higher performance, availability and scalability whilst still supporting the broad needs of Kubernetes. This aims to make Kubernetes much more suitable for performance-critical, dynamically-scaled edge solutions.  © 2021 Owner/Author.
DA  - 2021///
PY  - 2021
DO  - 10.1145/3434770.3459730
SP  - 7
EP  - 12
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104662456&doi=10.1145%2f3434770.3459730&partnerID=40&md5=5affbaa5a658a55090c90324acaa80f5
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 47</p>
KW  - Cloud environments
KW  - Cluster sizes
KW  - Cluster state
KW  - CRDTs
KW  - edge
KW  - eventual consistency
KW  - Key-value stores
KW  - Kubernetes
KW  - orchestration
KW  - Strong consistency
ER  - 

TY  - JOUR
TI  - SatEC: A 5G satellite edge computing framework based on microservice architecture
AU  - Yan, L.
AU  - Cao, S.
AU  - Gong, Y.
AU  - Han, H.
AU  - Wei, J.
AU  - Zhao, Y.
AU  - Yang, S.
T2  - Sensors (Switzerland)
AB  - As outlined in the 3Gpp Release 16, 5G satellite access is important for 5G network development in the future. A terrestrial-satellite network integrated with 5G has the characteristics of low delay, high bandwidth, and ubiquitous coverage. A few researchers have proposed integrated schemes for such a network; however, these schemes do not consider the possibility of achieving optimization of the delay characteristic by changing the computing mode of the 5G satellite network. We propose a 5G satellite edge computing framework (5GsatEC), which aims to reduce delay and expand network coverage. This framework consists of embedded hardware platforms and edge computing microservices in satellites. To increase the flexibility of the framework in complex scenarios, we unify the resource management of the central processing unit (CPU), graphics processing unit (GPU), and field-programmable gate array (FPGA); we divide the services into three types: system services, basic services, and user services. In order to verify the performance of the framework, we carried out a series of experiments. The results show that 5GsatEC has a broader coverage than the ground 5G network. The results also show that 5GsatEC has lower delay, a lower packet loss rate, and lower bandwidth consumption than the 5G satellite network. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.
DA  - 2019///
PY  - 2019
DO  - 10.3390/s19040831
VL  - 19
IS  - 4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061862436&doi=10.3390%2fs19040831&partnerID=40&md5=26a618b1c3da3d416270a02563a65f3d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 68</p>
KW  - 5G mobile communication systems
KW  - Bandwidth
KW  - Bandwidth consumption
KW  - Computer graphics
KW  - Computer graphics equipment
KW  - Computing frameworks
KW  - Data handling
KW  - Delay characteristics
KW  - Edge computing
KW  - Embedded hardware
KW  - Field programmable gate arrays (FPGA)
KW  - Graphics processing unit
KW  - Integrated Terrestrial-Satellite Networks
KW  - Microservices
KW  - On-board data processing
KW  - Program processors
KW  - Queueing networks
KW  - Resource management
KW  - Satellite network
KW  - Satellites
ER  - 

TY  - JOUR
TI  - A Novel Blockchain Based Secured and QoS Aware IoT Vehicular Network in Edge Cloud Computing
AU  - Ahmed, A.
AU  - Abdullah, S.
AU  - Iftikhar, S.
AU  - Ahmad, I.
AU  - Ajmal, S.
AU  - Hussain, Q.
T2  - IEEE Access
AB  - A software-defined vehicular network is made up of an IoT (Internet of Things) based vehicular ad-hoc network and a software-defined network. For better communication in IoT based vehicle networks, researchers are now working on the VANET (Vehicular Ad-hoc Network) to increase the overall system performance. To maximize the VANET ad-hoc network's information application performance and reliability, edge computing has gained the attention of researchers. In current research, cloud computing is used for message related task execution, which increases the response time. We propose a Software-defined Fault Tolerance and QoS-Aware (Quality of Service) IoT-Based Vehicular Networks Using Edge Computing Secured by Blockchain to reduce overall communication delay, message failure fault tolerance, and secure service provisioning for VANET ad-hoc networks in this article. We proposed heuristic algorithms to solve the above mentioned problems of response delay, message failure, fault tolerance, and security provided by the Blockchain. The proposed model gets vehicle messages through SDN (Software defined network) nodes, which are placed on nearby edge servers, and the edge servers are validated by the blockchain to provide secure services to vehicles. The SDN controller, which exists on an edge server, which is placed on the road side to overcome communication delays, receives different messages from the vehicles and divides these messages in to two different categories. The message division is performed by the edge server by judging the time line, size, and emergency situation. SDN controller organized these messages and forwarded them to their destination. After the message is delivered to its destination, a fault tolerance mechanism checks their acknowledgements. If the message delivery fails, the fault tolerance algorithm will resend the failure message. The proposed model is implemented using a custom simulator and compared with the latest VANET based QoS and fault tolerance models. The result shows the performance of the proposed model, which decreased the overall message communication delay by 55% of the normal and emergency messages by using the edge server SDN controller. Furthermore, the proposed model reduces the execution time, security risk, and message failure ratio by using the edge server, cloud server and blockchain infrastructure. © 2013 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ACCESS.2022.3192111
VL  - 10
SP  - 77707
EP  - 77722
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135240594&doi=10.1109%2fACCESS.2022.3192111&partnerID=40&md5=5d86ea1b2cf855a0f2a088b17cde327d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 45</p>
KW  - Ad-hoc networks
KW  - Block-chain
KW  - Blockchain
KW  - cloud
KW  - Cloud-computing
KW  - Controllers
KW  - Delay
KW  - edge computing
KW  - Edge computing
KW  - Fault tolerance
KW  - fault-tolerance
KW  - Heuristic algorithms
KW  - Internet of thing system
KW  - Internet of things
KW  - IoT systems
KW  - Network security
KW  - quality of service
KW  - Quality of service
KW  - Quality-of-service
KW  - response time
KW  - Response time
KW  - security
KW  - Security
KW  - Vehicle to vehicle communications
KW  - Vehicles
KW  - Vehicular ad hoc networks
KW  - Vehicular ad-hoc network
KW  - Vehicular Adhoc Networks (VANETs)
ER  - 

TY  - JOUR
TI  - Open Issues in Scheduling Microservices in the Cloud
AU  - Fazio, M.
AU  - Celesti, A.
AU  - Ranjan, R.
AU  - Liu, C.
AU  - Chen, L.
AU  - Villari, M.
T2  - IEEE Cloud Computing
AB  - The adoption of container-based microservices architectures is revolutionizing application design. By adopting a microservices architecture, developers can engineer applications that are composed of multiple lightweight, self-contained, and portable runtime components deployed across a large number of geodistributed servers. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/MCC.2016.112
VL  - 3
IS  - 5
SP  - 81
EP  - 88
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996569901&doi=10.1109%2fMCC.2016.112&partnerID=40&md5=d52198bb1cc22e10b6f9e5e3d6df7c95
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 139</p>
KW  - Application architecture
KW  - Application design
KW  - Application programs
KW  - Cloud computing
KW  - Cloud computing architecture
KW  - Cloud systems
KW  - Containers
KW  - Distributed applications
KW  - Federated clouds
KW  - Independent components
KW  - Internet of things
KW  - microservices
KW  - Runtimes
KW  - Service deployment
KW  - Software developer
ER  - 

TY  - JOUR
TI  - An Energy-Efficient Data Aggregation Mechanism for IoT Secured by Blockchain
AU  - Ahmed, A.
AU  - Abdullah, S.
AU  - Bukhsh, M.
AU  - Ahmad, I.
AU  - Mushtaq, Z.
T2  - IEEE Access
AB  - The Internet of Things (IoT) is getting important and interconnected technologies of the world, consisting of sensor devices. The internet is smoothly changing from an internet of people towards an Internet of Things, which permits various objects to connect to another wirelessly. The energy consumption of the IoT routing protocol can affect the network life span. In addition, the high volume of data produced by IoT will result in transmission collision, security issues, and energy dissipation due to increased data redundancy because tiny sensors are usually hard to recharge after they are deployed. Generally, to save energy, data aggregation reduces data redundancy at each node by turning some nodes into sleep mode and others into wake mode. Therefore, it is important to group the nodes with high data similarity using the fuzzy matrix. Then, the data received from the member nodes at the Cluster Head (CH) are analyzed using a fuzzy similarity matrix for clustering. In the next step, after clustering, some nodes are chosen from all groups as redundant nodes. The sleep scheduling mechanism is then applied to reduce data redundancy, network traffic jamming, and transmission costs. We have proposed an Energy-Efficient Data Aggregation Mechanism (EEDAM) secured by blockchain, which uses a data aggregation mechanism at the cluster level to save energy. As edge computing is used to provide on-demand trusted services to IoT with minimum delay, blockchain is integrated inside a cloud server, so the edge is validated by the blockchain to provide secure services to IoT. Finally, we performed simulations to calculate the performance of the proposed mechanism and compared it with the conventional energy-efficient algorithms. The simulation results show that the proposed structural design can successfully reduce the amount of data, provide proper security to the IoT, and extend the wireless sensor network (WSN).  © 2013 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ACCESS.2022.3146295
VL  - 10
SP  - 11404
EP  - 11419
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123737399&doi=10.1109%2fACCESS.2022.3146295&partnerID=40&md5=e4a8f008ac1f1b88bb2c81785dc11f1c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 104</p>
KW  - Aggregation mechanism
KW  - availability
KW  - Availability
KW  - Block-chain
KW  - blockchain
KW  - Blockchain
KW  - Cloud security
KW  - Cloud-computing
KW  - Computer architecture
KW  - Cost reduction
KW  - Data aggregation
KW  - Data reduction
KW  - Edge computing
KW  - Energy dissipation
KW  - energy efficiency
KW  - Energy efficiency
KW  - Energy utilization
KW  - Green computing
KW  - Internet of thing system
KW  - Internet of things
KW  - IoT systems
KW  - Matrix algebra
KW  - Network architecture
KW  - Network security
KW  - Power management (telecommunication)
KW  - Redundancy
KW  - security
KW  - Security
KW  - Sensor nodes
KW  - Structural design
KW  - wireless sensor networks
ER  - 

TY  - JOUR
TI  - Satellite Mobile Edge Computing: Improving QoS of High-Speed Satellite-Terrestrial Networks Using Edge Computing Techniques
AU  - Zhang, Z.
AU  - Zhang, W.
AU  - Tseng, F.-H.
T2  - IEEE Network
AB  - The high-speed satellite-terrestrial network (STN) is an indispensable alternative in future mobile communication systems. In this article, we first introduce the architecture and application scenarios of STNs, and then investigate possible ways to implement mobile edge computing (MEC) technique for QoS improvement in STNs. We propose satellite MEC (SMEC), in which a user equipment without a proximal MEC server can also enjoy MEC services via satellite links. We propose a dynamic network virtualization technique to integrate the network resources, and furtherly design a cooperative computation offloading (CCO) model to achieve parallel computation in STNs. Task scheduling models in SMEC are discussed in detail, and an elemental simulation is conducted to evaluate the performance of the proposed CCO model in SMEC. © 1986-2012 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/MNET.2018.1800172
VL  - 33
IS  - 1
SP  - 70
EP  - 76
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060086385&doi=10.1109%2fMNET.2018.1800172&partnerID=40&md5=078a78017daf2673969efb35a6b0e797
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 352</p>
KW  - Application scenario
KW  - Computing techniques
KW  - Cooperative computation
KW  - Edge computing
KW  - Mobile Edge Computing
KW  - Mobile telecommunication systems
KW  - Network resource
KW  - Parallel Computation
KW  - Radio broadcasting
KW  - Satellite communication systems
KW  - Satellite links
KW  - Satellite-terrestrial network
KW  - Satellites
KW  - User equipments
ER  - 

TY  - JOUR
T2  - K8s: An Open-Source System for Automating Deployment, Scaling, and Management of Containerized Application.
DA  - 2021///
PY  - 2021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177196335&partnerID=40&md5=5fbd26fbdb996aa781c87e4bd11683e6
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - Microservice Scheduling for Satellite-Terrestrial Hybrid Network with Edge Computing
AU  - Huang, Y.
AU  - Zhang, X.
T2  - 2022 IEEE/CIC International Conference on Communications in China, ICCC Workshops 2022
AB  - Satellite-Terrestrial Hybrid Network (STHN) has become a focus for future communication architecture to accommodate more services and applications. STHN combined with Edge Computing (EC) could provide lower delay, faster transfer speed, and better information transmission. Meanwhile, microservice-based architecture greatly increases system flexibility, which fits the STHN. In this paper, to verify the efficiency of STHN with EC system, we develop a platform based on an open-source EC architecture called KubeEdge. The platform not only helps terrestrial to deploy and manage microservice on satellites but also increases the flexibility of satellite service provision. Besides, in this platform, we design a microservice scheduling algorithm called Optimal Microservice Scheduling with Adaptive Link Changes (OMS-ALC) to minimize communication delay. The experimental results show that OMS-ALC outperforms other solutions in end-to-end delay performance.  © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ICCCWorkshops55477.2022.9896704
SP  - 24
EP  - 29
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141205148&doi=10.1109%2fICCCWorkshops55477.2022.9896704&partnerID=40&md5=79779d3dbc3692df1eb90ce6051f23b0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 5</p>
KW  - Communication architectures
KW  - Computer architecture
KW  - Computing system
KW  - Edge computing
KW  - Fast transfer
KW  - Hybrid network
KW  - Information transmission
KW  - Low delay
KW  - Network architecture
KW  - Open systems
KW  - Satellites
KW  - Scheduling
KW  - Scheduling algorithms
KW  - Services and applications
KW  - System flexibility
KW  - Transfer speed
ER  - 

TY  - JOUR
TI  - Ericsson mobility report
T2  - Ericsson Mobility Report
DA  - 2022///
PY  - 2022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147910500&partnerID=40&md5=8a0b639c4a1a81d48586a1f2f18faa1c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 212</p>
ER  - 

TY  - CONF
TI  - Tiansuan Constellation: An Open Research Platform
AU  - Wang, S.
AU  - Li, Q.
AU  - Xu, M.
AU  - Ma, X.
AU  - Zhou, A.
AU  - Sun, Q.
T2  - Proceedings - IEEE International Conference on Edge Computing
AB  - Satellite network is the first step towards interstellar voyages. It can provide global Internet connectivity everywhere on the earth, where most areas cannot access the Internet by the terrestrial infrastructure due to the geographic accessibility and high deployment cost. The space industry experiences a rise in large low-earth-orbit satellite constellations to achieve universal connectivity. The research community is also urgent to do some leading research to bridge the connectivity divide. Researchers now conduct their work by simulation, which is far from enough. However, experiments on real satellites are hindered by the exceptionally high bar of space technology, such as deployment cost and unknown risks. To solve the above challenges, we are eager to contribute to the universal connectivity and build an open research platform, Tiansuan constellation, to support experiments on real satellite networks. We discuss the potential research topics that would benefit from Tiansuan. We provide two case studies that have already been deployed in two experimental satellites of Tiansuan. © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/EDGE53862.2021.00022
VL  - 2021-September
SP  - 94
EP  - 101
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136239745&doi=10.1109%2fEDGE53862.2021.00022&partnerID=40&md5=df3275bc7b78ea0b858d24438ab285fe
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 44</p>
KW  - 6g
KW  - 6G
KW  - Communication satellites
KW  - Deployment costs
KW  - Earth (planet)
KW  - Edge computing
KW  - Geographics
KW  - Global internet connectivity
KW  - Orbits
KW  - Research platforms
KW  - Satellite edge computing
KW  - Satellite Edge Computing
KW  - Satellite internet
KW  - Satellite Internet
KW  - Satellite network
KW  - Space industry
KW  - Testbed
ER  - 

TY  - CONF
TI  - SatEdgeSim: A Toolkit for Modeling and Simulation of Performance Evaluation in Satellite Edge Computing Environments
AU  - Wei, J.
AU  - Cao, S.
AU  - Pan, S.
AU  - Han, J.
AU  - Yan, L.
AU  - Zhang, L.
T2  - 2020 12th International Conference on Communication Software and Networks, ICCSN 2020
AB  - Along with the progress of satellite technology, satellites are becoming more and more powerful, making emerging technologies such as mist computing and edge computing considered in the future development of satellite networks. Satellite networks will use mist/edge computing to provide low-latency, highly reliable information transmission capabilities. However, despite much research in the area of satellite edge computing, there is still a lack of simulation tools with satellite edge computing environments. Here, we propose a satellite edge computing simulator-SatEdgeSim, which is an extension of PureEdgeSim and utilizes the effectiveness of it in simulating cloud, mist and edge computing environments to evaluate the satellite edge computing environment. Finally, we use SatEdgeSim to simulate the impact of different task deployment strategies in satellite edge computing scenarios. Simulation results show that SatEdgeSim is effective in satellite edge computing environment. Compared with the traditional task deployment strategy, the proposed deployment strategy designed in this paper can reduce delay and energy consumption, and improve the task success rate. © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ICCSN49894.2020.9139057
SP  - 307
EP  - 313
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088888490&doi=10.1109%2fICCSN49894.2020.9139057&partnerID=40&md5=6af8230a42250dca31d9dd5fae24aa05
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 24</p>
KW  - Computing environments
KW  - Deployment strategy
KW  - Edge computing
KW  - Edge Computing
KW  - Emerging technologies
KW  - Energy utilization
KW  - Information transmission
KW  - Internet of Things
KW  - Low latency
KW  - Mist Computing
KW  - Model and simulation
KW  - Modeling and Simulation
KW  - Satellite Edge Computing
KW  - Satellite network
KW  - Satellite technology
KW  - Satellites
ER  - 

TY  - CONF
TI  - Containers and clusters for edge cloud architectures-A technology review
AU  - Pahl, C.
AU  - Lee, B.
T2  - Proceedings - 2015 International Conference on Future Internet of Things and Cloud, FiCloud 2015 and 2015 International Conference on Open and Big Data, OBD 2015
AB  - Cloud technology is moving towards more distribution across multi-clouds and the inclusion of various devices, as evident through IoT and network integration in the context of edge cloud and fog computing. Generally, lightweight virtualisation solutions are beneficial for this architectural setting with smaller, but still virtualised devices to host application and platform services, and the logistics required to manage this. Containerisation is currently discussed as a lightweight virtualisation solution. In addition to having benefits over traditional virtual machines in the cloud in terms of size and flexibility, containers are specifically relevant for platform concerns typically dealt with Platform-as-a-Service (PaaS) clouds such as application packaging and orchestration. For the edge cloud environment, application and service orchestration can help to manage and orchestrate applications through containers as an application packaging mechanism. We review edge cloud requirements and discuss the suitability container and cluster technology of that arise from having to facilitate applications through distributed multi-cloud platforms build from a range of networked nodes ranging from data centres to small devices, which we refer to here as edge cloud. © 2015 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/FiCloud.2015.35
SP  - 379
EP  - 386
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959064257&doi=10.1109%2fFiCloud.2015.35&partnerID=40&md5=62b8b344c5929aa444c08ae98c6abf59
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 221</p>
KW  - Big data
KW  - Cloud computing
KW  - Cloud Computing
KW  - Cluster
KW  - Cluster computing
KW  - Container
KW  - Containers
KW  - Distributed computer systems
KW  - Edge Cloud
KW  - Edge clouds
KW  - Internet
KW  - Internet of things
KW  - Multi-cloud
KW  - Multi-clouds
KW  - Orchestration
KW  - PaaS
KW  - Platform as a Service (PaaS)
KW  - Topology
KW  - Virtual reality
KW  - Virtualisation
ER  - 

TY  - CONF
TI  - Extend cloud to edge with KubeEdge
AU  - Xiong, Y.
AU  - Sun, Y.
AU  - Xing, L.
AU  - Huang, Y.
T2  - Proceedings - 2018 3rd ACM/IEEE Symposium on Edge Computing, SEC 2018
AB  - In this paper, we introduce an infrastructure in edge computing environment, KubeEdge, to extend cloud capabilities to the edge. In the new form of cloud architecture, Cloud consists of computing resources both at centralized data centers and at distributed edges. KubeEdge infrastructure connects and coordinates two computing environments for applications leveraging both computing resources to achieve better performance and user experience. Technically, KubeEdge provides the network protocol infrastructure and the same runtime environment on the edge as in the cloud, which allows the seamless communication of applications with components running on edge nodes as well as cloud servers. It also allows the existing cloud services and cloud development model to be adopted at edge. Based on Kubernetes [1], KubeEdge architecture includes a network protocol stack called KubeBus, a distributed metadata store and synchronization service, and a lightweight agent (EdgeCore) for the edge. KubeBus is designed to have its own implementation of OSI network protocol layers, which connects servers at edge and VMs in the cloud as one virtual network. KubeBus provides a unified multitenant communication infrastructure with fault tolerance and high availability. The distributed metadata store and sync service is designed to support the offline scenario when edge nodes are not connected to the cloud. EdgeController component in KubeEdge architecture is a controller plugin for Kubernetes [1] to manage remote edge nodes and cloud VMs as one logical cluster, which enables KubeEdge to schedule, deploy and manage container applications across edge and cloud with the same API. © 2018 IEEE
DA  - 2018///
PY  - 2018
DO  - 10.1109/SEC.2018.00048
SP  - 373
EP  - 377
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060224033&doi=10.1109%2fSEC.2018.00048&partnerID=40&md5=54f1ee667ca65b525e64f5b63ed890ee
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 188</p>
KW  - Cloud computing
KW  - Cluster computing
KW  - Communication infrastructure
KW  - Computer architecture
KW  - Computing environments
KW  - Data synchronization
KW  - Distributed systems
KW  - Edge computing
KW  - Fault tolerance
KW  - Fault tolerance and high availabilities
KW  - Fault tolerant computer systems
KW  - Information services
KW  - Internet protocols
KW  - Metadata
KW  - Network architecture
KW  - Network layers
KW  - Network protocol
KW  - Network protocol stack
KW  - Network protocols
KW  - Runtime environments
KW  - Seamless communication
ER  - 

TY  - JOUR
TI  - Toward Sustainable Serverless Computing
AU  - Patros, P.
AU  - Spillner, J.
AU  - Papadopoulos, A.V.
AU  - Varghese, B.
AU  - Rana, O.
AU  - Dustdar, S.
T2  - IEEE Internet Computing
AB  - Although serverless computing generally involves executing short-lived functions, the increasing migration to this computing paradigm requires careful consideration of energy and power requirements. serverless computing is also viewed as an economically-driven computational approach, often influenced by the cost of computation, as users are charged for per-subsecond use of computational resources rather than the coarse-grained charging that is common with virtual machines and containers. To ensure that the startup times of serverless functions do not discourage their use, resource providers need to keep these functions hot, often by passing in synthetic data. We describe the real power consumption characteristics of serverless, based on execution traces reported in the literature, and describe potential strategies (some adopted from existing VM and container-based approaches) that can be used to reduce the energy overheads of serverless execution. Our analysis is, purposefully, biased toward the use of machine learning workloads because: (1) workloads are increasingly being used widely across different applications; (2) functions that implement machine learning algorithms can range in complexity from long-running (deep learning) versus short-running (inference only), enabling us to consider serverless across a variety of possible execution behaviors. The general findings are easily translatable to other domains.  © 1997-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/MIC.2021.3093105
VL  - 25
IS  - 6
SP  - 42
EP  - 50
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121710845&doi=10.1109%2fMIC.2021.3093105&partnerID=40&md5=f28e5566d277c6654ea70fa0605f15d5
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 44</p>
KW  - Coarse-grained
KW  - Computational approach
KW  - Computational resources
KW  - Computing paradigm
KW  - Containers
KW  - Deep learning
KW  - Energy requirements
KW  - Inference engines
KW  - Learning algorithms
KW  - Power requirement
KW  - Real power
KW  - Resource providers
KW  - Startup time
KW  - Synthetic data
ER  - 

TY  - JOUR
TI  - Enabling rank-based distribution of microservices among containers for green cloud computing environment
AU  - Saboor, A.
AU  - Mahmood, A.K.
AU  - Omar, A.H.
AU  - Hassan, M.F.
AU  - Shah, S.N.M.
AU  - Ahmadian, A.
T2  - Peer-to-Peer Networking and Applications
AB  - Microservices architecture is a functional software design methodology that promises the redefinition of the architectural style that aims to create a single application as a suite of tiny, loosely coupled services or components, each performing its own tasks and interacting with each other. The cloud services widely shifted from monoliths to microservices and gained the popularity for use in scalable cloud application. The usage of microservices involved intensive network communication to call number of interdependent microservices running inside the cloud nodes. It provides flexibility in the delivery of service but also increases energy usage and poor service efficiency which results in increased carbon emissions. To solve these issues, the prevailing technologies were designed for single unit monolithic cloud applications, and not tailored for the chain oriented service delivery. This study addresses the dynamic provisioning of containers and respective microservices in cloud computing environment by building rank-based profiles and using those profiles for allocation of web application’s microservices along with containers to the cloud data centers. The MicroRanker service is proposed to rank all of the participating microservices and distribute them across different nodes even before the execution of the cloud services. Further, the MicroRanker service is utilized to dynamically update the container placement due to continuous DevOps actions. The proposed solution was tested using custom built simulation environment. The achieved results showed that the distribution of containers along with respective microservices in accordance with MicroRanker service resulted in less energy consumption (i.e. between 81.6 kWh-87.7 kWh compared to 88.9 kWh-95.7 kWh) and significantly lowered the emission of carbon (i.e. between 5.92 kg-33.31 kg compared to 17.2 kg-47.35 kg) due to higher utilization of renewable energy. The use of rank-based microservices distribution also decreased response time (i.e. between 29 ms-142 ms compared to 106 ms-217 ms) due to the availability of the container along with microservice within the same data center region. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
DA  - 2022///
PY  - 2022
DO  - 10.1007/s12083-021-01218-y
VL  - 15
IS  - 1
SP  - 77
EP  - 91
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112841408&doi=10.1007%2fs12083-021-01218-y&partnerID=40&md5=ee57caac17e75a71c6e3bdd06f31114e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 20</p>
KW  - Application programs
KW  - Architectural style
KW  - Carbon
KW  - Cloud computing
KW  - Cloud computing environments
KW  - Cloud data centers
KW  - Containers
KW  - Dynamic provisioning
KW  - Energy utilization
KW  - Green computing
KW  - High performance computing
KW  - Microservices
KW  - Network communications
KW  - Optimization
KW  - Ranking
KW  - Renewable energies
KW  - Simulation environment
KW  - Software design
KW  - Software design methodologies
KW  - Web services
ER  - 

TY  - JOUR
TI  - A Microservice and Serverless Architecture for Secure IoT System
AU  - Ouyang, R.
AU  - Wang, J.
AU  - Xu, H.
AU  - Chen, S.
AU  - Xiong, X.
AU  - Tolba, A.
AU  - Zhang, X.
T2  - Sensors
AB  - In cross-border transactions, the transmission and processing of logistics information directly affect the trading experience and efficiency. The use of Internet of Things (IoT) technology can make this process more intelligent, efficient, and secure. However, most traditional IoT logistics systems are provided by a single logistics company. These independent systems need to withstand high computing loads and network bandwidth when processing large-scale data. Additionally, due to the complex network environment of cross-border transactions, the platform’s information security and system security are difficult to guarantee. To address these challenges, this paper designs and implements an intelligent cross-border logistics system platform that combines serverless architecture and microservice technology. This system can uniformly distribute the services of all logistics companies and divide microservices based on actual business needs. It also studies and designs corresponding Application Programming Interface (API) gateways to solve the interface exposure problem of microservices, thereby ensuring the system’s security. Furthermore, asymmetric encryption technology is used in the serverless architecture to ensure the security of cross-border logistics data. The experiments show that this research solution validates the advantages of combining serverless architecture and microservices, which can significantly reduce the operating costs and system complexity of the platform in cross-border logistics scenarios. It allows for resource expansion and billing based on application program requirements at runtime. The platform can effectively improve the security of cross-border logistics service processes and meet cross-border transaction needs in terms of data security, throughput, and latency. © 2023 by the authors.
DA  - 2023///
PY  - 2023
DO  - 10.3390/s23104868
VL  - 23
IS  - 10
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160394089&doi=10.3390%2fs23104868&partnerID=40&md5=0a7d2eb590561230fad53713c8587eff
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 13</p>
KW  - Application programming interfaces (API)
KW  - Application programs
KW  - article
KW  - bandwidth
KW  - Complex networks
KW  - Computer architecture
KW  - Cross-border
KW  - Cryptography
KW  - edge computing
KW  - Edge computing
KW  - encryption
KW  - Independent systems
KW  - information security
KW  - Internet of thing security
KW  - internet of things
KW  - Internet of things
KW  - Internet of things technologies
KW  - IoT security
KW  - Logistics company
KW  - Logistics information
KW  - Logistics system
KW  - microservice
KW  - Microservice
KW  - Network architecture
KW  - Network security
KW  - Operating costs
KW  - serverless architecture
KW  - Serverless architecture
ER  - 

TY  - JOUR
TI  - Enriching computing simulators by generating realistic serverless traces
AU  - Sallo, D.H.
AU  - Kecskemeti, G.
T2  - Journal of Cloud Computing
AB  - Serverless computing is stepping forward to provide a cloud environment that mainly focuses on managing infrastructure, resources and configurations on the behalf of a user. Research in this field can’t rely on commercial providers such as AWS and Azure, as their inflexibility and cost often limits the required levels of reproducibility and scalability. Therefore, simulators have been opted as an alternative solution by the research community. They offer a reduced-cost and easy-setup environment. To get respectable precision, simulators use real traces collected and offered by commercial providers. These traces represent comprehensive information of executed tasks that reflect user behaviour. Due to serverless computing’s recency, typical workload traces employed by IaaS simulators are not well adoptable to the new computing model. In this paper, we propose an approach for generating realistic serverless traces. We enhance our previous generator approach that was based on the Azure Functions dataset. Our new, genetic algorithm based approach improves the statistical properties of the generated traces. We also enabled arbitrary scaling of the workload, while maintaining real users’ behaviour. These advances further support reproducibility in the serverless research community. We validated the results of our generator approach using the coefficient of determination (R2), which shows that our generated workload closely matches the original dataset’s characteristics in terms of execution time, memory utilisation as well as user participation percentage. To demonstrate the benefits of the reusability of the generated traces, we applied them with a diverse set of simulators and shown that they offer reproducible results independently of the simulator used. © 2023, The Author(s).
DA  - 2023///
PY  - 2023
DO  - 10.1186/s13677-023-00397-8
VL  - 12
IS  - 1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150208452&doi=10.1186%2fs13677-023-00397-8&partnerID=40&md5=68c93581a3135014027d6b9fa9d8b39f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3</p>
KW  - Alternative solutions
KW  - Behavioral research
KW  - Cloud environments
KW  - Faas
KW  - FaaS
KW  - Genetic Algorithm
KW  - Genetic algorithms
KW  - Infrastructure resources
KW  - Reduced cost
KW  - Reproducibilities
KW  - Research communities
KW  - Reusability
KW  - Serverless trace
KW  - Serverless workload
KW  - User behaviors
ER  - 

TY  - JOUR
TI  - Performance Modeling of Metric-Based Serverless Computing Platforms
AU  - Mahmoudi, N.
AU  - Khazaei, H.
T2  - IEEE Transactions on Cloud Computing
AB  - Analytical performance models are very effective in ensuring the quality of service and cost of service deployment remain desirable under different conditions and workloads. While various analytical performance models have been proposed for previous paradigms in cloud computing, serverless computing lacks such models that can provide developers with performance guarantees. Besides, most serverless computing platforms still require developers’ input to specify the configuration for their deployment that could affect both the performance and cost of their deployment, without providing them with any direct and immediate feedback. In previous studies, we built such performance models for steady-state and transient analysis of scale-per-request serverless computing platforms (e.g., AWS Lambda, Azure Functions, Google Cloud Functions) that could give developers immediate feedback about the quality of service and cost of their deployments. In this work, we aim to develop analytical performance models for latest trend in serverless computing platforms that use concurrency value and the rate of requests per second for autoscaling decisions. Examples of such serverless computing platforms are Knative and Google Cloud Run (a managed Knative service by Google). The proposed performance model can help developers and providers predict the performance and cost of deployments with different configurations which could help them tune the configuration toward the best outcome. We validate the applicability and accuracy of the proposed performance model by extensive real-world experimentation on Knative and show that our performance model is able to accurately predict the steady-state characteristics of a given workload with minimal amount of data collection. © 2022 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/TCC.2022.3169619
VL  - 11
IS  - 2
SP  - 1899
EP  - 1910
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129646106&doi=10.1109%2fTCC.2022.3169619&partnerID=40&md5=03968c7aea5a520e6e7a7b101751e886
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 18</p>
KW  - Analytical models
KW  - Autoscaling
KW  - Computational modelling
KW  - Concurrent computing
KW  - Cost benefit analysis
KW  - Google cloud run
KW  - Google+
KW  - knative
KW  - Knative
KW  - metric-based autoscaling
KW  - Metric-based autoscaling
KW  - Optimisations
KW  - optimization
KW  - Performance Modeling
KW  - performance modelling
KW  - Quality control
KW  - Quality of service
KW  - Random processes
KW  - serverless computing
KW  - Serverless computing
KW  - Stochastic models
KW  - stochastic processes
KW  - Stochastic systems
KW  - Transient analysis
ER  - 

TY  - JOUR
TI  - Energy efficient utilization of resources in cloud computing systems
AU  - Lee, Y.C.
AU  - Zomaya, A.Y.
T2  - Journal of Supercomputing
AB  - The energy consumption of under-utilized resources, particularly in a cloud environment, accounts for a substantial amount of the actual energy use. Inherently, a resource allocation strategy that takes into account resource utilization would lead to a better energy efficiency; this, in clouds, extends further with virtualization technologies in that tasks can be easily consolidated. Task consolidation is an effective method to increase resource utilization and in turn reduces energy consumption. Recent studies identified that server energy consumption scales linearly with (processor) resource utilization. This encouraging fact further highlights the significant contribution of task consolidation to the reduction in energy consumption. However, task consolidation can also lead to the freeing up of resources that can sit idling yet still drawing power. There have been some notable efforts to reduce idle power draw, typically by putting computer resources into some form of sleep/power-saving mode. In this paper, we present two energy-conscious task consolidation heuristics, which aim to maximize resource utilization and explicitly take into account both active and idle energy consumption. Our heuristics assign each task to the resource on which the energy consumption for executing the task is explicitly or implicitly minimized without the performance degradation of that task. Based on our experimental results, our heuristics demonstrate their promising energy-saving capability. © Springer Science+Business Media, LLC 2010.
DA  - 2012///
PY  - 2012
DO  - 10.1007/s11227-010-0421-3
VL  - 60
IS  - 2
SP  - 268
EP  - 280
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027919535&doi=10.1007%2fs11227-010-0421-3&partnerID=40&md5=7eb98f2d838791fa4fb7b3edbcb6cd4c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 551</p>
KW  - Cloud computing
KW  - Distributed computer systems
KW  - Energy aware computing
KW  - Energy conservation
KW  - Energy efficiency
KW  - Energy utilization
KW  - Energy-aware computing
KW  - Green computing
KW  - Load balancing
KW  - Performance degradation
KW  - Power management
KW  - Reduction in energy consumption
KW  - Resource allocation
KW  - Resource allocation strategies
KW  - Resource utilizations
KW  - Scheduling
KW  - Task consolidation
KW  - Utilization of resources
KW  - Virtualization technologies
ER  - 

TY  - JOUR
AU  - Taruna, S.
AU  - Singh, P.
AU  - Joshi, S.
T2  - Green Computing In Developed And Developing Countries
DA  - 2024///
PY  - 2024
VL  - 14
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207485808&partnerID=40&md5=21a49eee3a5a381287159982ab226b0b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Effects of biomass energy consumption on environmental quality: The role of education and technology in Asia-Pacific Economic Cooperation countries
AU  - Zafar, M.W.
AU  - Sinha, A.
AU  - Ahmed, Z.
AU  - Qin, Q.
AU  - Zaidi, S.A.H.
T2  - Renewable and Sustainable Energy Reviews
AB  - Rising concern regarding traditional non-renewable energy consumption has led policymakers to explore the potential of economical renewable energy sources. In this regard, biomass energy has received considerable attention because previous studies have found mixed results regarding the effect of biomass energy on environmental quality. Together with modern technology, biomass energy may significantly influence environmental quality. This study investigates the impact of biomass energy consumption, education, and technological innovation on environmental quality by controlling for the role of economic growth and financial development in the function of environmental quality. Second-generation econometric methods were used to solve the issues of heterogeneity and cross-sectional dependence in the study variables. The Westerlund and Edgerton (2008) cointegration technique confirmed the existence of a long-run equilibrium among the variables in the presence of structural breaks. The panel quantile regression results indicate that biomass energy use and technological innovation reduce environmental quality. Similarly, economic growth increases carbon emissions in the environment. Education and financial development contribute to reduce carbon emissions. © 2021 Elsevier Ltd
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.rser.2021.110868
VL  - 142
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101769756&doi=10.1016%2fj.rser.2021.110868&partnerID=40&md5=58c4f04ec1dac46f2d3b980922623b18
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 233</p>
KW  - Bio-mass energy
KW  - Biomass
KW  - Biomass energy consumption
KW  - Carbon
KW  - Carbon emissions
KW  - Economic analysis
KW  - Economic and social effects
KW  - Economic Co-operation
KW  - Economic growths
KW  - Education
KW  - Energy utilization
KW  - Energy-consumption
KW  - Engineering education
KW  - Environmental quality
KW  - Environmental technology
KW  - Financial development
KW  - Non-renewable energy
KW  - Technological innovation
ER  - 

TY  - JOUR
TI  - Sustainable Campus Policy Strategy in Estimating CO2 Emissions at the Universitas Negeri Semarang, Indonesia
AU  - Sunardiyo, S.
AU  - Purwanto, P.
AU  - Hermawan, H.
T2  - Nature Environment and Pollution Technology
AB  - In the fight against global warming, various options for reducing CO2 emissions are being implemented on campus. Furthermore, the management of campus sustainability at the Universitas Negeri Semarang (UNNES), Central Java, Indonesia, should be supported by accurate forecasts of electrical energy consumption. Therefore, this research aims to develop a predictive model to forecast the consumption of electrical energy in reducing CO2 emissions and to determine the factors triggering the increase. The prediction model is developed using Back Propagation Neural Network Artificial (BP-ANN) architecture. Furthermore, the data on the occupancy of lecturers and education staff as well as on students was obtained from the University's staffing and student affairs bureau. Climatic data such as temperature, humidity, wind speed, the duration of irradiation, and the average intensity of solar radiation were obtained per month from the Meteorology, Climatology, and Geophysics Agency of Semarang, Central Java for the 2013-2019 period as input data. The results of the empirical analysis showed an increase in electrical energy consumption from 2020 to 2025. In March, the consumption decreased but increased from April to June and decreased in July. It then increased until November and December, and it decreased every year. The results of CO2 emissions calculated by considering the emission factors from Indonesia's RUPTLPLN in 2020-2025 showed an increase in electrical energy consumption and the ecological consequences affecting the campus area. Furthermore, the main factors causing the high consumption of electrical energy are the occupancy rate, lecturers, students, and campus employees, as well as local climate influences such as temperature, humidity, wind speed, duration of solar radiation, and intensity of solar radiation. Therefore, developing guidelines to reduce power consumption on campus should be a priority. © 2023 Technoscience Publications. All rights reserved.
DA  - 2023///
PY  - 2023
DO  - 10.46488/NEPT.2023.V22I01.044
VL  - 22
IS  - 1
SP  - 463
EP  - 470
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152522220&doi=10.46488%2fNEPT.2023.V22I01.044&partnerID=40&md5=2349c487d342067d2f98f335f2086871
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
KW  - BP-ANN
KW  - Campus management policies
KW  - carbon emission
KW  - Central Java
KW  - CO2 emissions
KW  - electrical power
KW  - Electricity consumption
KW  - environmental factor
KW  - forecasting method
KW  - Forecasts
KW  - Indonesia
KW  - policy strategy
KW  - Semarang
KW  - spatiotemporal analysis
KW  - sustainability
ER  - 

TY  - JOUR
TI  - Microservices: Migration of a Mission Critical System
AU  - Mazzara, M.
AU  - Dragoni, N.
AU  - Bucchiarone, A.
AU  - Giaretta, A.
AU  - Larsen, S.T.
AU  - Dustdar, S.
T2  - IEEE Transactions on Services Computing
AB  - An increasing interest is growing around the idea of microservices and the promise of improving scalability when compared to monolithic systems. Several companies are evaluating pros and cons of a complex migration. In particular, financial institutions are positioned in a difficult situation due to the economic climate and the appearance of agile competitors that can navigate in a more flexible legal framework and started their business since day one with more agile architectures and without being bounded to outdated technological standard. In this paper, we present a real world case study in order to demonstrate how scalability is positively affected by re-implementing a monolithic architecture (MA) into a microservices architecture (MSA). The case study is based on the FX Core system, a mission critical system of Danske Bank, the largest bank in Denmark and one of the leading financial institutions in Northern Europe. The technical problem that has been addressed and solved in this paper is the identification of a repeatable migration process that can be used to convert a real world Monolithic architecture into a Microservices architecture in the specific setting of financial domain, typically characterized by legacy systems and batch-based processing on heterogeneous data sources.  © 2008-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/TSC.2018.2889087
VL  - 14
IS  - 5
SP  - 1464
EP  - 1477
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059027049&doi=10.1109%2fTSC.2018.2889087&partnerID=40&md5=5c8e72ef29ee1e2faf74cecae680ec14
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 56</p>
KW  - Agile architectures
KW  - Agile manufacturing systems
KW  - Automation
KW  - Batch data processing
KW  - Computer architecture
KW  - Data handling
KW  - Finance
KW  - Financial institution
KW  - Heterogeneous data sources
KW  - Information services
KW  - Legacy systems
KW  - microservices
KW  - Microservices
KW  - Mission critical systems
KW  - Monolithic architecture
KW  - scalability
KW  - Scalability
KW  - Service computing
KW  - Service oriented architecture (SOA)
KW  - software architecture
KW  - Software architecture
KW  - Technological standards
KW  - Tools
ER  - 

TY  - JOUR
TI  - The Serverless Computing Survey: A Technical Primer for Design Architecture
AU  - Li, Z.
AU  - Guo, L.
AU  - Cheng, J.
AU  - Chen, Q.
AU  - He, B.
AU  - Guo, M.
T2  - ACM Computing Surveys
AB  - The development of cloud infrastructures inspires the emergence of cloud-native computing. As the most promising architecture for deploying microservices, serverless computing has recently attracted more and more attention in both industry and academia. Due to its inherent scalability and flexibility, serverless computing becomes attractive and more pervasive for ever-growing Internet services. Despite the momentum in the cloud-native community, the existing challenges and compromises still wait for more advanced research and solutions to further explore the potential of the serverless computing model. As a contribution to this knowledge, this article surveys and elaborates the research domains in the serverless context by decoupling the architecture into four stack layers: Virtualization, Encapsule, System Orchestration, and System Coordination. Inspired by the security model, we highlight the key implications and limitations of these works in each layer, and make suggestions for potential challenges to the field of future serverless computing.  © 2022 Association for Computing Machinery.
DA  - 2022///
PY  - 2022
DO  - 10.1145/3508360
VL  - 54
IS  - 10
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149425293&doi=10.1145%2f3508360&partnerID=40&md5=fa85ba8ddf2629f7804768d2b9738f39
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 131</p>
KW  - Advanced researches
KW  - Architecture
KW  - architecture design
KW  - Architecture designs
KW  - Cloud infrastructures
KW  - Computer architecture
KW  - Computing surveys
KW  - Design architecture
KW  - Faas
KW  - FaaS
KW  - Internet-services
KW  - Lambda paradigm
KW  - Lambda's
KW  - Serverless computing
ER  - 

TY  - JOUR
TI  - Improving microservices extraction using evolutionary search
AU  - Sellami, K.
AU  - Ouni, A.
AU  - Saied, M.A.
AU  - Bouktif, S.
AU  - Mkaouer, M.W.
T2  - Information and Software Technology
AB  - Context: Microservices constitute a modern style of building software applications as collections of small, cohesive, and loosely coupled services, i.e., modules, that are developed, deployed, and scaled independently. Objective: The migration from legacy systems towards the microservice-based architecture is not a trivial task. It is still manual, time-consuming, error-prone and subsequently costly. The most critical and challenging issue is the cost-effective identification of microservices boundaries that ensure adequate granularity and cohesiveness. Method: To address this problem, we introduce in this paper a novel approach, named MSExtractor, that formulates microservices identification as a multi-objective optimization problem. The proposed solution aims at decomposing a legacy application into a set of cohesive, loosely-coupled and coarse-grained services. We employ the Indicator-Based Evolutionary Algorithm (IBEA) to drive a search process towards optimal microservices identification while considering structural and semantic dependencies in the source code. Results: We conduct an empirical evaluation on a benchmark of seven software systems to assess the efficiency of our approach. Results show that MSExtractor is able to carry out an effective identification of relevant microservice candidates and outperforms three other existing approaches. Conclusion: In this paper, we show that MSExtractor is able to extract cohesive and loosely coupled services with higher performance than three other considered methods. However, we advocate that while automated microservices identification approaches are very helpful, the role of the human experts remains crucial to validate and calibrate the extracted microservices. © 2022
DA  - 2022///
PY  - 2022
DO  - 10.1016/j.infsof.2022.106996
VL  - 151
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134792906&doi=10.1016%2fj.infsof.2022.106996&partnerID=40&md5=fc5c1a3e4564f414f946f9e2c52f23b0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 24</p>
KW  - Application programs
KW  - Building softwares
KW  - Cost effectiveness
KW  - Error prones
KW  - Evolutionary algorithms
KW  - Evolutionary search
KW  - Legacy decomposition
KW  - Legacy systems
KW  - Loosely coupled
KW  - Microservice
KW  - Microservice architecture
KW  - Microservices
KW  - Microservices architecture
KW  - Multiobjective optimization
KW  - Optimal systems
KW  - Search-based
KW  - Search-based software engineering
KW  - Semantics
KW  - Software applications
ER  - 

TY  - JOUR
TI  - Review on E-waste management and its impact on the environment and society
AU  - Jain, M.
AU  - Kumar, D.
AU  - Chaudhary, J.
AU  - Kumar, S.
AU  - Sharma, S.
AU  - Singh Verma, A.
T2  - Waste Management Bulletin
AB  - Electronic trash, often known as E-waste, is a type of garbage generated by electronic in the industrial world, trash is one of the most difficult and rapidly expanding issues. E-waste is made up of old or end-of-life electronic appliances such as computers, laptops, televisions, generators, DVDs, mobile phones, freezers, and other items that are typically discarded by their original owners due to their short lifespan. It contains a number of hazardous constituents that have a negative impact on the environment and, more importantly, human health if not properly managed. Because it includes harmful chemical elements, E-waste proves to be a significant difficulty. Since it is believed that E-waste is a future of communications but due to the short life span of various appliances, they are being trashed and pollutes the environment. Many groups and governments from various nations have implemented a variety of ways to address the problem and threat to the environment and human health. Hence, this review presents a compendium of various sources of E-waste, environmental hazards, its composition and characterization, E-waste scenarios in India and global world. For the sake of the future, techniques of handling and processing, as well as E-waste recycling, should be used. This paper mainly outlines the issue of E-waste also covering the improvement and plan to tackle the issue. © 2023 The Authors
DA  - 2023///
PY  - 2023
DO  - 10.1016/j.wmb.2023.06.004
VL  - 1
IS  - 3
SP  - 34
EP  - 44
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185097073&doi=10.1016%2fj.wmb.2023.06.004&partnerID=40&md5=6e826c3f4097eed306c540a091a3cc38
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 91</p>
KW  - E-waste management
KW  - E-waste recycling
KW  - Environmental pollution
KW  - Human health
ER  - 

TY  - CONF
TI  - Green Cloud Computing- Next Step Towards Eco-friendly Work Stations
AU  - Badhoutiya, A.
T2  - 6th International Conference on Electronics, Communication and Aerospace Technology, ICECA 2022 - Proceedings
AB  - Cloud computing is a solution to various computational problems by offering different services through the internet making use of various resources. IT industry always face energy consumption and carbon emission problems. Datacenters play crucial role as a support in cloud computing. That's why the urge to open more and more datacenters is increasing day by day. As a result of the higher power usage at data centers, this boosts the economic and environmental costs. Furthermore, the increased release of CO2 and other gases has resulted in an increase in carbon footprint and, as a result, the greenhouse effect. These are the forces that propel green computing technology forward. A brief introduction of cloud computing, related relevant researches and various approaches to use green computing is provided in this paper. Measurement, modelling and prediction of consumed energy by different organizations is necessary to maintain the environmental balance by reducing carbon emission as well as to make sure to avail the presently available energy resources for our upcoming future generations. The elaboration of several research issues in implementing green computing as well as their solutions is discussed in this paper.  © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ICECA55336.2022.10009629
SP  - 809
EP  - 813
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147521755&doi=10.1109%2fICECA55336.2022.10009629&partnerID=40&md5=b2af86bcc17b05374a161ebdfc77dd14
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
KW  - Carbon emissions
KW  - Carbon footprint
KW  - Cloud computing
KW  - cloud data centers
KW  - Cloud data centers
KW  - Cloud-computing
KW  - Computational problem
KW  - Computing power
KW  - Datacenter
KW  - Eco-friendly
KW  - Electric power utilization
KW  - electronic equipment
KW  - Energy resources
KW  - google search
KW  - Google search
KW  - Google+
KW  - Green Clouds
KW  - Green computing
KW  - Greenhouse effect
KW  - Oscillators (electronic)
KW  - power consumption
KW  - Work station
ER  - 

TY  - CONF
TI  - Four decades of the green computing study: A bibliometric overview
AU  - Purnomo, A.
AU  - Anam, F.
AU  - Afia, N.
AU  - Septianto, A.
AU  - Mufliq, A.
T2  - Proceedings of 2021 International Conference on Information Management and Technology, ICIMTech 2021
AB  - Study on green computing continues to develop but is limited to one country and/or one field. From a bibliometric overview, this study aims to visually study mapping and research trends in the field of green computing on an international scale. This study used bibliometric techniques with secondary data from Scopus. Analyze and visualize data using the VOSViewer program and the analyze search results function on Scopus. This study analyzed 2, 596 scientific documents published from 1979 to 2020. According to the research, the Chinese Academy of Sciences, China, and Rajkumar Buyya from the University of Melbourne had the most active organization and individual scientists in green computing research. China and IEEE Access were the most countries and disseminated outlets of green computing. There were five category maps of collaborative researchers from around the world. Based on the identification of a collection of knowledge created from forty-one years of publication, this research proposes a grouping of green computing study themes: Computer science, Environmental management, Mobile computing, Energy, and Sustainability, abbreviated as CEMES study themes.  © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/ICIMTech53080.2021.9535069
SP  - 795
EP  - 800
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116101481&doi=10.1109%2fICIMTech53080.2021.9535069&partnerID=40&md5=7325076e3b6add0c4b2528395e14d932
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 4</p>
KW  - Bibliometric
KW  - Bibliometric techniques
KW  - Computing research
KW  - Environmental management
KW  - Green computing
KW  - International scale
KW  - Mapping
KW  - Research mapping
KW  - Research trends
KW  - Scientific documents
KW  - Secondary datum
KW  - Study theme
KW  - Study themes
KW  - Sustainable development
KW  - University of Melbourne
ER  - 

TY  - JOUR
TI  - Harnessing green IT: Principles and practices
AU  - Murugesan, S.
T2  - IT Professional
AB  - The basic principles and practices that need to be adopted for a greener Information Technology (IT) regarding environmental impacts from IT and the regulations that need to be complied are discussed. Green IT includes various focus areas and activities such as design of environmental sustainability, energy-efficient computing, power management, and server visualization. Adoption of green IT practices offer financial and other benefits in terms of lesser power consumption. A holistic approach including green use, green disposal, green design, and green manufacturing address the problems faced while adopting green IT practices. The environmentally sound practices while achieving green IT include reduction of energy consumption by PCs, enabling power management features, using screensavers, and greening data centers.
DA  - 2008///
PY  - 2008
DO  - 10.1109/MITP.2008.10
VL  - 10
IS  - 1
SP  - 24
EP  - 33
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-38949154490&doi=10.1109%2fMITP.2008.10&partnerID=40&md5=0335ea79e81859dbd9ca0a58523e6066
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 833</p>
KW  - Energy utilization
KW  - Environmental sustainability
KW  - Greening data centers
KW  - Information management
KW  - Information technology
KW  - Power management
KW  - Problem solving
KW  - Servers
KW  - Visualization
ER  - 

TY  - JOUR
TI  - GREEN INFORMATION AND COMMUNICATION TECHNOLOGY AT HIGHER EDUCATION ORGANIZATION: SOLUTION FOR SUSTENANCE OF ICT IN FUTURE
AU  - Adhikari, A.
T2  - Int. J. Humanit. Eng. Sci. Manag
DA  - 2023///
PY  - 2023
VL  - 4
IS  - 1
SP  - 48
EP  - 56
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207507159&partnerID=40&md5=3e69df54247b5fffb5735e6808bcfa35
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2</p>
ER  - 

TY  - CONF
TI  - An Empirical Study of Resource-Stressing Faults in Edge-Computing Applications
AU  - Pourreza, M.
AU  - Narasimhan, P.
T2  - EdgeSys 2023 - Proceedings of the 6th International Workshop on Edge Systems, Analytics and Networking, Part of EuroSys 2023
AB  - Our growing reliance on edge-computing applications makes it crucial to improve the reliability of edge-computing systems. With multiple classes of edge-computing applications, different types of faults, and different kinds of resources needed by the applications, it remains unclear which resource exhaustion has the most disruptive impact on the latency experienced by the edge applications. Without this information, it is challenging to determine which faults to prioritize when implementing fault tolerance for edge computing. To address this challenge, we conduct an empirical study on a representative edge computing environment using well-known edge-computing benchmark applications (DeFog and ComB) and injecting resource-stressing faults (via stress-ng and hping). Our study reveals that memory overloads, CPU cache thrashing, frequent context switching, and page faults are the biggest disruptors of latency for edge-based applications.  © 2023 Owner/Author(s).
DA  - 2023///
PY  - 2023
DO  - 10.1145/3578354.3592873
SP  - 54
EP  - 59
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159356560&doi=10.1145%2f3578354.3592873&partnerID=40&md5=431c73981c4bffea11cf7ef1177985c4
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3</p>
KW  - benchmark
KW  - Benchmark
KW  - Benchmark applications
KW  - Benchmarking
KW  - Computing applications
KW  - Computing environments
KW  - Computing system
KW  - Context switching
KW  - edge computing
KW  - Edge computing
KW  - Empirical studies
KW  - fault injection
KW  - Fault injection
KW  - fault tolerance
KW  - Fault tolerance
KW  - Multiple class
ER  - 

TY  - JOUR
TI  - Unsupervised real-time anomaly detection for streaming data
AU  - Ahmad, S.
AU  - Lavin, A.
AU  - Purdy, S.
AU  - Agha, Z.
T2  - Neurocomputing
AB  - We are seeing an enormous increase in the availability of streaming, time-series data. Largely driven by the rise of connected real-time data sources, this data presents technical challenges and opportunities. One fundamental capability for streaming analytics is to model each stream in an unsupervised fashion and detect unusual, anomalous behaviors in real-time. Early anomaly detection is valuable, yet it can be difficult to execute reliably in practice. Application constraints require systems to process data in real-time, not batches. Streaming data inherently exhibits concept drift, favoring algorithms that learn continuously. Furthermore, the massive number of independent streams in practice requires that anomaly detectors be fully automated. In this paper we propose a novel anomaly detection algorithm that meets these constraints. The technique is based on an online sequence memory algorithm called Hierarchical Temporal Memory (HTM). We also present results using the Numenta Anomaly Benchmark (NAB), a benchmark containing real-world data streams with labeled anomalies. The benchmark, the first of its kind, provides a controlled open-source environment for testing anomaly detection algorithms on streaming data. We present results and analysis for a wide range of algorithms on this benchmark, and discuss future challenges for the emerging field of streaming analytics. © 2017 The Author(s)
DA  - 2017///
PY  - 2017
DO  - 10.1016/j.neucom.2017.04.070
VL  - 262
SP  - 134
EP  - 147
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020719859&doi=10.1016%2fj.neucom.2017.04.070&partnerID=40&md5=9e99f987e8e50f3937ebf8b39a79cb6b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 775</p>
KW  - Anomaly detection
KW  - Anomaly-detection algorithms
KW  - Article
KW  - Benchmark dataset
KW  - Benchmark datasets
KW  - benchmarking
KW  - classification algorithm
KW  - Concept drift
KW  - Concept drifts
KW  - controlled study
KW  - data analysis
KW  - Data streams
KW  - Hierarchical Temporal Memory
KW  - Hierarchical temporal memory (htm)
KW  - learning algorithm
KW  - mathematical computing
KW  - online system
KW  - priority journal
KW  - Real time systems
KW  - Real-time anomaly detections
KW  - Signal detection
KW  - Streaming data
KW  - Technical challenges
KW  - Temporal memory
KW  - time series analysis
KW  - Unsupervised learning
KW  - unsupervised machine learning
ER  - 

TY  - CONF
TI  - Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications
AU  - Xu, H.
AU  - Chen, W.
AU  - Zhao, N.
AU  - Li, Z.
AU  - Bu, J.
AU  - Li, Z.
AU  - Liu, Y.
AU  - Zhao, Y.
AU  - Pei, D.
AU  - Feng, Y.
AU  - Chen, J.
AU  - Wang, Z.
AU  - Qiao, H.
T2  - The Web Conference 2018 - Proceedings of the World Wide Web Conference, WWW 2018
AB  - To ensure undisrupted business, large Internet companies need to closely monitor various KPIs (e.g., Page Views, number of online users, and number of orders) of its Web applications, to accurately detect anomalies and trigger timely troubleshooting/mitigation. However, anomaly detection for these seasonal KPIs with various patterns and data quality has been a great challenge, especially without labels. In this paper, we proposed Donut, an unsupervised anomaly detection algorithm based on VAE. Thanks to a few of our key techniques, Donut greatly outperforms a state-of-arts supervised ensemble approach and a baseline VAE approach, and its best F-scores range from 0.75 to 0.9 for the studied KPIs from a top global Internet company. We come up with a novel KDE interpretation of reconstruction for Donut, making it the first VAE-based anomaly detection algorithm with solid theoretical explanation. © 2018 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC BY 4.0 License.
DA  - 2018///
PY  - 2018
DO  - 10.1145/3178876.3185996
SP  - 187
EP  - 196
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085158868&doi=10.1145%2f3178876.3185996&partnerID=40&md5=6fbda4532daa158dda4772ddf2a9fd25
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 805</p>
KW  - Anomaly detection
KW  - Anomaly-detection algorithms
KW  - Arts computing
KW  - Auto encoders
KW  - Data quality
KW  - Ensemble approaches
KW  - Global Internet
KW  - Online users
KW  - Seasonal KPI
KW  - Signal detection
KW  - Signal encoding
KW  - Unsupervised anomaly detection
KW  - Variational auto-encoder
KW  - WEB application
KW  - World Wide Web
ER  - 

TY  - CONF
TI  - MicroMILTS: Fault Location for Microservices Based Mutual Information and LSTM Autoencoder
AU  - Yang, L.
AU  - Li, J.
AU  - Shi, K.
AU  - Yang, S.
AU  - Yang, Q.
AU  - Sun, J.
T2  - APNOMS 2022 - 23rd Asia-Pacific Network Operations and Management Symposium: Data-Driven Intelligent Management in the Era of beyond 5G
AB  - Driven by the development of cloud computing and artificial intelligence, architecture has dramatically improved in terms of flexibility and scalability in software development. Therefore, it is increasingly being used to build large-scale applications for agile development. However, along with the technology heterogeneity, the dynamics of running instances, and the complexity of service dependencies, fault localization is extraordinarily difficult. In this paper, we present MicroMILTS, a microservice fault location method based on mutual information and an LSTM Autoencoder. MicroMILTS first uses BIRCH for anomaly detection based on the analysis of the performance metrics data correlated to microservice anomalies. Once anomalies are detected, a service dependency property graph is constructed based on the real-time microservice invocation relationships and the reconstructed deviations of performance metrics with the LSTM Autoencoder. Next, MicroMILTS dynamically updates the weight of each node in the service dependency property graph. Then, a PageRank-based random walk is applied for further ranking root causes. Finally, a Sock-shop microservice system is built on the Huawei Cloud to evaluate the performance of MicroMILTS. The experiment shows that MicroMILTS achieves a good root cause location result, with 90.4 % in precision and 91.6% in mean average precision, outperforming state-of-the-art methods. © 2022 IEICE.
DA  - 2022///
PY  - 2022
DO  - 10.23919/APNOMS56106.2022.9919941
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142092014&doi=10.23919%2fAPNOMS56106.2022.9919941&partnerID=40&md5=f656f179b9ae17b27e203c312a65a85a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 4</p>
KW  - Anomaly detection
KW  - Auto encoders
KW  - Cloud-computing
KW  - fault location
KW  - Large-scale applications
KW  - Learning systems
KW  - Location
KW  - Long short-term memory
KW  - LSTM autoencoder
KW  - LSTM Autoencoder
KW  - Microservice
KW  - microservices
KW  - Mutual informations
KW  - Performance metrices
KW  - performance metrics
KW  - Property
KW  - Root cause
KW  - Service dependency
KW  - Software design
ER  - 

TY  - CONF
TI  - DeepTraLog: Trace-Log Combined Microservice Anomaly Detection through Graph-based Deep Learning
AU  - Zhang, C.
AU  - Peng, X.
AU  - Sha, C.
AU  - Zhang, K.
AU  - Fu, Z.
AU  - Wu, X.
AU  - Lin, Q.
AU  - Zhang, D.
T2  - Proceedings - International Conference on Software Engineering
AB  - A microservice system in industry is usually a large-scale dis-tributed system consisting of dozens to thousands of services run-ning in different machines. An anomaly of the system often can be reflected in traces and logs, which record inter-service interactions and intra-service behaviors respectively. Existing trace anomaly detection approaches treat a trace as a sequence of service invocations. They ignore the complex structure of a trace brought by its invocation hierarchy and parallel/asynchronous invocations. On the other hand, existing log anomaly detection approaches treat a log as a sequence of events and cannot handle microservice logs that are distributed in a large number of services with complex interactions. In this paper, we propose DeepTraLog, a deep learning based microservice anomaly detection approach. DeepTraLog uses a unified graph representation to describe the complex structure of a trace together with log events embedded in the structure. Based on the graph representation, DeepTraLog trains a GGNNs based deep SVDD model by combing traces and logs and detects anom-alies in new traces and the corresponding logs. Evaluation on a microservice benchmark shows that DeepTraLog achieves a high precision (0.93) and recall (0.97), outperforming state-of-the-art trace/log anomaly detection approaches with an average increase of 0.37 in F1-score. It also validates the efficiency of DeepTraLog, the contribution of the unified graph representation, and the impact of the configurations of some key parameters. © 2022 ACM.
DA  - 2022///
PY  - 2022
DO  - 10.1145/3510003.3510180
VL  - 2022-May
SP  - 623
EP  - 634
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133555577&doi=10.1145%2f3510003.3510180&partnerID=40&md5=c15de94f433782e8cc6a26ecf087108f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 119</p>
KW  - Anomaly detection
KW  - Anomaly Detection
KW  - Complex networks
KW  - Complexes structure
KW  - Deep learning
KW  - Deep Learning
KW  - Deep neural networks
KW  - Detection approach
KW  - Graph Neural Network
KW  - Graph neural networks
KW  - Graph representation
KW  - Graph-based
KW  - Graphic methods
KW  - Log analysis
KW  - Log Analysis
KW  - Microservice
KW  - Tracing
KW  - World Wide Web
ER  - 

TY  - JOUR
TI  - Adaptive anomaly detection in performance metric streams
AU  - Ibidunmoye, O.
AU  - Rezaie, A.-R.
AU  - Elmroth, E.
T2  - IEEE Transactions on Network and Service Management
AB  - Continuous detection of performance anomalies such as service degradations has become critical in cloud and Internet services due to impact on quality of service and end-user experience. However, the volume and fast changing behavior of metric streams have rendered it a challenging task. Many diagnosis frameworks often rely on thresholding with stationarity or normality assumption, or on complex models requiring extensive offline training. Such techniques are known to be prone to spurious false-alarms in online settings as metric streams undergo rapid contextual changes from known baselines. Hence, we propose two unsupervised incremental techniques following a two-step strategy. First, we estimate an underlying temporal property of the stream via adaptive learning and, then we apply statistically robust control charts to recognize deviations. We evaluated our techniques by replaying over 40 time-series streams from the Yahoo Webscope S5 datasets as well as four other traces of real Web service QoS and ISP traffic measurements. Our methods achieve high detection accuracy and few false-alarms, and better performance in general compared to an open-source package for time-series anomaly detection. © 2004-2012 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/TNSM.2017.2750906
VL  - 15
IS  - 1
SP  - 217
EP  - 231
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043695414&doi=10.1109%2fTNSM.2017.2750906&partnerID=40&md5=ff1b87715b87270275d5c02e7db83fbc
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 51</p>
KW  - anomaly detection
KW  - Anomaly detection
KW  - computer network management
KW  - Computer networks
KW  - Continuous detections
KW  - Errors
KW  - Incremental techniques
KW  - Open source package
KW  - Performance anomaly
KW  - Performance metrices
KW  - Performance monitoring
KW  - Performance monitoring and measurement
KW  - Quality control
KW  - quality of service
KW  - Quality of service
KW  - Robust control
KW  - Telecommunication services
KW  - time series analysis
KW  - Time series analysis
KW  - Traffic measurements
KW  - unsupervised learning
KW  - Unsupervised learning
KW  - Web services
ER  - 

TY  - CONF
TI  - AIOps-Driven Enhancement of Log Anomaly Detection in Unsupervised Scenarios
AU  - Dave, D.
AU  - Sawhney, G.
AU  - Khut, D.
AU  - Nawale, S.
AU  - Aggrawal, P.
AU  - Bhavathankar, P.
T2  - Proceedings of the 8th International Conference on Big Data, Knowledge and Control Systems Engineering, BdKCSE 2023
AB  - Artificial intelligence operations (AIOps) play a pivotal role in identifying, mitigating, and analyzing anomalous system behaviors and alerts. However, the research landscape in this field remains limited, leaving significant gaps unexplored. This study introduces a novel hybrid framework through an innovative algorithm that incorporates an unsupervised strategy. This strategy integrates Principal Component Analysis (PCA) and Artificial Neural Networks (ANNs) and uses a custom loss function to substantially enhance the effectiveness of log anomaly detection. The proposed approach encompasses the utilization of both simulated and real-world datasets, including logs from SockShop and Hadoop Distributed File System (HDFS). The experimental results are highly promising, demonstrating significant reductions in pseudo-positives. Moreover, this strategy offers notable advantages, such as the ability to process logs in their raw, unprocessed form, and the potential for further enhancements. The successful implementation of this approach showcases a remarkable reduction in anomalous logs, thus un-equivocally establishing the efficacy of the proposed methodology. Ultimately, this study makes a substantial contribution to the advancement of log anomaly detection within AIOps platforms, addressing the critical need for effective and efficient log analysis in modern and complex systems.  © 2023 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/BdKCSE59280.2023.10339699
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182274725&doi=10.1109%2fBdKCSE59280.2023.10339699&partnerID=40&md5=de6ed58ee60d4ab0249400fdd6b60879
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2</p>
KW  - % reductions
KW  - AIOps
KW  - Anomaly detection
KW  - anomaly log detection
KW  - Anomaly log detection
KW  - Artificial intelligence operation
KW  - File organization
KW  - Intelligence operations
KW  - Log data
KW  - Log data analyse
KW  - log data analysis
KW  - Neural networks
KW  - Principal component analysis
KW  - Pseudo positive
KW  - pseudo positives
KW  - recurring anomalies
KW  - Recurring anomaly
KW  - System behaviors
ER  - 

TY  - JOUR
TI  - Anomaly Detection and Failure Root Cause Analysis in (Micro) Service-Based Cloud Applications: A Survey
AU  - Soldani, J.
AU  - Brogi, A.
T2  - ACM Computing Surveys
AB  - The proliferation of services and service interactions within microservices and cloud-native applications, makes it harder to detect failures and to identify their possible root causes, which is, on the other hand crucial to promptly recover and fix applications. Various techniques have been proposed to promptly detect failures based on their symptoms, viz., observing anomalous behaviour in one or more application services, as well as to analyse logs or monitored performance of such services to determine the possible root causes for observed anomalies. The objective of this survey is to provide a structured overview and qualitative analysis of currently available techniques for anomaly detection and root cause analysis in modern multi-service applications. Some open challenges and research directions stemming out from the analysis are also discussed. © 2022 Association for Computing Machinery.
DA  - 2022///
PY  - 2022
DO  - 10.1145/3501297
VL  - 55
IS  - 3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143306533&doi=10.1145%2f3501297&partnerID=40&md5=88ae0f33e8f50b1363c68cff414fc265
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 149</p>
KW  - anomaly detection
KW  - Anomaly detection
KW  - Failure (mechanical)
KW  - failure detection
KW  - Failure detection
KW  - Failure root cause analysis
KW  - Micro services
KW  - Microservice
KW  - Microservices
KW  - Multi-service application
KW  - multi-service applications
KW  - Multi-services
KW  - Root cause
KW  - root cause analysis
KW  - Root cause analysis
KW  - Services applications
ER  - 

TY  - CONF
TI  - Web service classification using support vector machine
AU  - Wang, H.
AU  - Shi, Y.
AU  - Zhou, X.
AU  - Zhou, Q.
AU  - Shao, S.
AU  - Bouguettaya, A.
T2  - Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI
AB  - Classification is a widely used mechanism for facilitatingWeb service discovery. Existing methods for automaticWeb service classification only consider the case where the category set is small. When the category set is big, the conventional classification methods usually require a large sample collection, which is hardly available in real world settings. This paper presents a novel method to conduct service classification with a medium or big category set. It uses the descriptive information of categories in a large-scale taxonomy as sample data, so as to disengage from the dependence on sample service documents. A new feature selection method is introduced to enable efficient classification using this new type of sample data. We demonstrate the effectiveness of our classification method through extensive experiments. © 2010 IEEE.
DA  - 2010///
PY  - 2010
DO  - 10.1109/ICTAI.2010.9
VL  - 1
SP  - 3
EP  - 6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751533734&doi=10.1109%2fICTAI.2010.9&partnerID=40&md5=79edf28a8230c1092727b8cff603f212
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 71</p>
KW  - Artificial intelligence
KW  - Classification methods
KW  - Conventional classification methods
KW  - Descriptive information
KW  - Existing method
KW  - Feature extraction
KW  - Feature selection methods
KW  - Novel methods
KW  - Real world setting
KW  - Sample collection
KW  - Sample data
KW  - Service discovery
KW  - Web services
ER  - 

TY  - CONF
TI  - Failure Root Cause Analysis for Microservices, Explained
AU  - Soldani, J.
AU  - Forti, S.
AU  - Brogi, A.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Determining the root causes of observed failures is a main issue in microservice-based applications. Unfortunately, available root cause analysis techniques do not focus on explaining how root failures actually caused the observed failure. On the other hand, the availability of such explanations would greatly help to pick adequate countermeasures, e.g., by introducing circuit breakers or bulkheads. We hence present a declarative root cause analysis technique, which can determine the cascading failures that possibly caused an observed failure, identifying also (or starting from) a root cause. We also introduce a prototype implementation of our technique, and we use it to assess our technique by means of controlled experiments. © 2022, IFIP International Federation for Information Processing.
DA  - 2022///
PY  - 2022
DO  - 10.1007/978-3-031-16092-9_6
VL  - 13272 LNCS
SP  - 74
EP  - 91
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137990399&doi=10.1007%2f978-3-031-16092-9_6&partnerID=40&md5=6516f18d1faf8c8cc8c81e8d8a891b94
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 6</p>
KW  - Analysis techniques
KW  - Cascading failures
KW  - Circuit-breakers
KW  - Controlled experiment
KW  - Electric circuit breakers
KW  - Failure (mechanical)
KW  - Failure root cause analysis
KW  - Prototype implementations
KW  - Root cause
KW  - Root cause analysis
ER  - 

TY  - JOUR
TI  - The real climate and transformative impact of ICT: A critique of estimates, trends, and regulations
AU  - Freitag, C.
AU  - Berners-Lee, M.
AU  - Widdicks, K.
AU  - Knowles, B.
AU  - Blair, G.S.
AU  - Friday, A.
T2  - Patterns
AB  - In this paper, we critique ICT's current and projected climate impacts. Peer-reviewed studies estimate ICT's current share of global greenhouse gas (GHG) emissions at 1.8%–2.8% of global GHG emissions; adjusting for truncation of supply chain pathways, we find that this share could actually be between 2.1% and 3.9%. For ICT's future emissions, we explore assumptions underlying analysts' projections to understand the reasons for their variability. All analysts agree that ICT emissions will not reduce without major concerted efforts involving broad political and industrial action. We provide three reasons to believe ICT emissions are going to increase barring intervention and find that not all carbon pledges in the ICT sector are ambitious enough to meet climate targets. We explore the underdevelopment of policy mechanisms for enforcing sector-wide compliance, and contend that, without a global carbon constraint, a new regulatory framework is required to keep the ICT sector's footprint aligned with the Paris Agreement. © 2021 The Authors
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.patter.2021.100340
VL  - 2
IS  - 9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122638594&doi=10.1016%2fj.patter.2021.100340&partnerID=40&md5=d7af9a116bbc52d9530b7e2158984995
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 341</p>
KW  - 'current
KW  - AI
KW  - big data
KW  - Block-chain
KW  - blockchain
KW  - Blockchain
KW  - carbon footprint
KW  - Carbon footprint
KW  - Climate impacts
KW  - Current share
KW  - data science
KW  - Gas emissions
KW  - Greenhouse gas emissions
KW  - Greenhouse gases
KW  - ICT
KW  - Industrial emissions
KW  - Internet of things
KW  - IoT
KW  - policy
KW  - Political actions
KW  - Regulation
KW  - regulations
KW  - Regulatory compliance
KW  - Supply chains
KW  - Trend
KW  - trends
ER  - 

TY  - CONF
TI  - A review of energy measurement approaches
AU  - Noureddine, A.
AU  - Rouvoy, R.
AU  - Seinturier, L.
T2  - Operating Systems Review (ACM)
AB  - Reducing the energy footprint of digital devices and software is a task challenging the research in Green IT. Researches have proposed approaches for energy management, ranging from reducing usage of software and hardware, compilators optimization, to server consolidation and software migration. However, optimizing the energy consumption requires knowledge of that said consumption. In particular, measuring the energy consumption of hardware and software is an important requirement for efficient energy strategies. In this review, we outline the different categories of approaches in energy measurements, and provide insights into example of each category. We draw recommendations from our review on requirements on how to efficiently measure energy consumption of devices and software. © 2012 IEEE Computer Society Washington.
DA  - 2013///
PY  - 2013
DO  - 10.1145/2553070.2553077
VL  - 47
SP  - 42
EP  - 49
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901316580&doi=10.1145%2f2553070.2553077&partnerID=40&md5=e7ad9ded84795cab94da689f7cb9ca65
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 77</p>
KW  - Computer hardware
KW  - Digital devices
KW  - Electric power measurement
KW  - Energy management
KW  - Energy Management
KW  - Energy Measurement
KW  - Energy metrics
KW  - Energy Metrics
KW  - Energy strategy
KW  - Energy utilization
KW  - Green IT
KW  - Hardware
KW  - Hardware and software
KW  - Instruments
KW  - Middleware
KW  - Server consolidation
KW  - Software and hardwares
KW  - Software migration
ER  - 

TY  - CONF
TI  - Latent error prediction and fault localization for microservice applications by learning from system trace logs
AU  - Zhou, X.
AU  - Peng, X.
AU  - Xie, T.
AU  - Sun, J.
AU  - Ji, C.
AU  - Liu, D.
AU  - Xiang, Q.
AU  - He, C.
T2  - ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering
AB  - In the production environment, a large part of microservice failures are related to the complex and dynamic interactions and runtime environments, such as those related to multiple instances, environmental configurations, and asynchronous interactions of microservices. Due to the complexity and dynamism of these failures, it is often hard to reproduce and diagnose them in testing environments. It is desirable yet still challenging that these failures can be detected and the faults can be located at runtime of the production environment to allow developers to resolve them efficiently. To address this challenge, in this paper, we propose MEPFL, an approach of latent error prediction and fault localization for microservice applications by learning from system trace logs. Based on a set of features defined on the system trace logs, MEPFL trains prediction models at both the trace level and the microservice level using the system trace logs collected from automatic executions of the target application and its faulty versions produced by fault injection. The prediction models thus can be used in the production environment to predict latent errors, faulty microservices, and fault types for trace instances captured at runtime. We implement MEPFL based on the infrastructure systems of container orchestrator and service mesh, and conduct a series of experimental studies with two opensource microservice applications (one of them being the largest open-source microservice application to our best knowledge). The results indicate that MEPFL can achieve high accuracy in intraapplication prediction of latent errors, faulty microservices, and fault types, and outperforms a state-of-the-art approach of failure diagnosis for distributed systems. The results also show that MEPFL can effectively predict latent errors caused by real-world fault cases. © 2019 ACM.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3338906.3338961
SP  - 683
EP  - 694
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071904016&doi=10.1145%2f3338906.3338961&partnerID=40&md5=3c6b30fca792ecf2002e705b167b6395
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 223</p>
KW  - Asynchronous interaction
KW  - Computer debugging
KW  - Debugging
KW  - Error prediction
KW  - Errors
KW  - Fault localization
KW  - Forecasting
KW  - Infrastructure systems
KW  - Learning systems
KW  - Machine learning
KW  - Microservices
KW  - Open source software
KW  - Open systems
KW  - Production environments
KW  - Safety engineering
KW  - State-of-the-art approach
KW  - Tracing
ER  - 

TY  - CONF
TI  - ϵ-Diagnosis: Unsupervised and real-time diagnosis of small-window long-tail latency in large-scale microservice platforms
AU  - Shan, H.
AU  - Zhang, Y.
AU  - Chen, Y.
AU  - Xiao, X.
AU  - Liu, H.
AU  - He, X.
AU  - Li, M.
AU  - Ding, W.
T2  - The Web Conference 2019 - Proceedings of the World Wide Web Conference, WWW 2019
AB  - Microservice architectures and container technologies are broadly adopted by giant internet companies to support their web services, which typically have a strict service-level objective (SLO), tail latency, rather than average latency. However, diagnosing SLO violations, e.g., long tail latency problem, is non-trivial for large-scale web applications in shared microservice platforms due to million-level operational data and complex operational environments. We identify a new type of tail latency problem for web services, small-window long-tail latency (SWLT), which is typically aggregated during a small statistical window (e.g., 1-minute or 1-second). We observe SWLT usually occurs in a small number of containers in microservice clusters and sharply shifts among different containers at different time points. To diagnose root-causes of SWLT, we propose an unsupervised and low-cost diagnosis algorithm-ϵ-Diagnosis, using two-sample test algorithm and ϵ-statistics for measuring similarity of time series to identify root-cause metrics from millions of metrics. We implement and deploy a real-time diagnosis system in our real-production microservice platforms. The evaluation using real web application datasets demonstrates that ϵ-Diagnosis can identify all the actual root-causes at runtime and significantly reduce the candidate problem space, outperforming other time-series distance based root-cause analysis algorithms. © 2019 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY 4.0 License.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3308558.3313653
SP  - 3215
EP  - 3222
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066912897&doi=10.1145%2f3308558.3313653&partnerID=40&md5=9dfebc5856fd8aca09b9594b4d34b753
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 49</p>
KW  - Containers
KW  - Diagnosis algorithms
KW  - Measuring similarities
KW  - Operational environments
KW  - Real time systems
KW  - Real-time diagnosis
KW  - Root cause analysis
KW  - Root-cause analysis
KW  - Service level objective
KW  - Statistical window
KW  - Tail latency
KW  - Time series
KW  - Time series analysis
KW  - Time series similarity
KW  - Web services
KW  - Websites
ER  - 

TY  - JOUR
AU  - Wohlin, C.
AU  - Runeson, P.
AU  - Host, M.
AU  - Ohlsson, M.C.
AU  - Regnell, B.
AU  - Wesslen, A.
T2  - Experimentation in Software Engineering: An Introduction
DA  - 2000///
PY  - 2000
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003639957&partnerID=40&md5=7341cbbc34c4d05ef0cc0917e28416e4
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2391</p>
ER  - 

TY  - CONF
TI  - Towards Independent Run-time Cloud Monitoring
AU  - Klaver, L.
AU  - van der Knaap, T.
AU  - van der Geest, J.
AU  - Harmsma, E.
AU  - van der Waaij, B.
AU  - Pileggi, P.
T2  - ICPE 2021 - Companion of the ACM/SPEC International Conference on Performance Engineering
AB  - Cloud computing services are integral to the digital transformation. They deliver greater connectivity, tremendous savings, and lower total cost of ownership. Despite such benefits and benchmarking advances, costs are still quite unpredictable, performance is unclear, security is inconsistent, and there is minimal control over aspects like data and service locality. Estimating performance of cloud environments is very hard for cloud consumers. They would like to make informed decisions about which provider better suits their needs using specialized evaluation mechanisms. Providers have their own tools reporting specific metrics, but they are potentially biased and often incomparable across providers. Current benchmarking tools allow comparison but consumers need more flexibility to evaluate environments under actual operating conditions for specialized applications. Ours is early stage work and a step towards a monitoring solution that enables independent evaluation of clouds for very specific application needs. In this paper, we present our initial architecture of the Cloud Monitor that aims to integrate existing and new benchmarks in a flexible and extensible way. By way of a simplistic demonstrator, we illustrate the concept. We report some preliminary monitoring results after a brief time of monitoring and are able to observe unexpected anomalies. The results suggest an independent monitoring solution is a powerful enabler of next generation cloud computing, not only for the consumer but potentially the whole ecosystem. © 2021 Association for Computing Machinery.
DA  - 2021///
PY  - 2021
DO  - 10.1145/3447545.3451180
SP  - 21
EP  - 26
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104973631&doi=10.1145%2f3447545.3451180&partnerID=40&md5=1e96abf0390d8e5d948ec4231c3f440a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 9</p>
KW  - Benchmarking
KW  - Benchmarking tools
KW  - Cloud computing
KW  - Cloud computing services
KW  - Cloud environments
KW  - Digital transformation
KW  - Informed decision
KW  - Monitoring results
KW  - Operating condition
KW  - Total cost of ownership
ER  - 

TY  - CONF
TI  - RAPL in action: Experiences in using RAPL for power measurements
AU  - Khan, K.N.
AU  - Hirki, M.
AU  - Niemi, T.
AU  - Nurminen, J.K.
AU  - Ou, Z.
T2  - ACM Transactions on Modeling and Performance Evaluation of Computing Systems
AB  - To improve energy efficiency and comply with the power budgets, it is important to be able to measure the power consumption of cloud computing servers. Intel's Running Average Power Limit (RAPL) interface is a powerful tool for this purpose. RAPL provides power limiting features and accurate energy readings for CPUs and DRAM, which are easily accessible through different interfaces on large distributed computing systems. Since its introduction, RAPL has been used extensively in power measurement and modeling. However, the advantages and disadvantages of RAPL have not been well investigated yet. To fill this gap, we conduct a series of experiments to disclose the underlying strengths and weaknesses of the RAPL interface by using both customized microbenchmarks and three well-known application level benchmarks: Stream, Stress-ng, and ParFullCMS. Moreover, to make the analysis as realistic as possible, we leverage two production-level power measurement datasets from the Taito, a supercomputing cluster of the Finnish Center of Scientific Computing and also replicate our experiments on Amazon EC2. Our results illustrate different aspects of RAPL and document the findings through comprehensive analysis. Our observations reveal that RAPL readings are highly correlated with plug power, promisingly accurate enough, and have negligible performance overhead. Experimental results suggest RAPL can be a very useful tool to measure and monitor the energy consumption of servers without deploying any complex power meters. We also show that there are still some open issues, such as driver support, non-atomicity of register updates, and unpredictable timings that might weaken the usability of RAPL in certain scenarios. For such scenarios, we pinpoint solutions and workarounds. © 2018 ACM.
DA  - 2018///
PY  - 2018
DO  - 10.1145/3177754
VL  - 3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057252724&doi=10.1145%2f3177754&partnerID=40&md5=49dadace1ce44c47e3440ed831b60ad1
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 226</p>
KW  - Automobile drivers
KW  - Average power limit
KW  - Benchmarking
KW  - Budget control
KW  - Cluster computing
KW  - Comprehensive analysis
KW  - Distributed computing systems
KW  - Dram power
KW  - Energy efficiency
KW  - Energy utilization
KW  - Green computing
KW  - Highly-correlated
KW  - Power model
KW  - Power modeling
KW  - Program processors
KW  - RAPL
KW  - Rapl accuracy
KW  - Rapl validation
ER  - 

TY  - CONF
TI  - AdapPF: Self-Adaptive Scrape Interval for Monitoring in Geo-Distributed Cluster Federations
AU  - Huang, C.-K.
AU  - Pierre, G.
T2  - Proceedings - IEEE Symposium on Computers and Communications
AB  - Monitoring plays a vital role in geo-distributed cluster federation environments to accurately schedule applications across geographically dispersed computing resources. However, using a fixed frequency for collecting monitoring data from clusters may waste network bandwidth and is not necessary for ensuring accurate scheduling. In this paper, we propose Adaptive Prometheus Federation (AdapPF), an extension of the widely-used open-source monitoring tool, Prometheus, and its feature, Prometheus Federation. AdapPF aims to dynamically adjust the collection frequency of monitoring data for each cluster in geo-distributed cluster federations. Based on actual deployment in the geo-distributed Grid'5000 testbed, our evaluations demonstrate that AdapPF can achieve comparable results to Prometheus Federation with 5-seconds scrape interval while reducing cross-cluster network traffic by 36%. © 2023 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/ISCC58397.2023.10218080
VL  - 2023-July
SP  - 417
EP  - 423
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171980853&doi=10.1109%2fISCC58397.2023.10218080&partnerID=40&md5=e27b361fba831ec8629b45981f8f3a13
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 4</p>
KW  - Cluster computing
KW  - Cluster federation
KW  - Computing resource
KW  - Data acquisition
KW  - Distributed clusters
KW  - Fixed frequency
KW  - Fog computing
KW  - Geo-distributed cluster federation
KW  - Geo-distributed cluster federations
KW  - Monitoring
KW  - Monitoring tools
KW  - Network bandwidth
KW  - Open-source
KW  - Prometheus
KW  - Self-adaptive
KW  - Self-Adaptive
ER  - 

TY  - JOUR
TI  - KNN model-based approach in classification
AU  - Guo, G.
AU  - Wang, H.
AU  - Bell, D.
AU  - Bi, Y.
AU  - Greer, K.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - The k-Nearest-Neighbours (kNN) is a simple but effective method for classification. The major drawbacks with respect to kNN are (1) its low efficiency - being a lazy learning method prohibits it in many applications such as dynamic web mining for a large repository, and (2) its dependency on the selection of a "good value" for k. In this paper, we propose a novel kNN type method for classification that is aimed at overcoming these shortcomings. Our method constructs a kNN model for the data, which replaces the data to serve as the basis of classification. The value of k is automatically determined, is varied for different data, and is optimal in terms of classification accuracy. The construction of the model reduces the dependency on k and makes classification faster. Experiments were carried out on some public datasets collected from the UCI machine learning repository in order to test our method. The experimental results show that the kNN based model compares well with C5.0 and kNN in terms of classification accuracy, but is more efficient than the standard kNN. © Springer-Verlag Berlin Heidelberg 2003.
DA  - 2003///
PY  - 2003
DO  - 10.1007/978-3-540-39964-3_62
VL  - 2888
SP  - 986
EP  - 996
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0242712881&doi=10.1007%2f978-3-540-39964-3_62&partnerID=40&md5=dad6f64de7f0f167d788e0c382867023
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1678</p>
KW  - Artificial intelligence
KW  - Classification accuracy
KW  - Distributed database systems
KW  - K nearest neighbours (k-NN)
KW  - Lazy learning
KW  - Learning systems
KW  - Model based approach
KW  - Multi agent systems
KW  - Nearest neighbor search
KW  - Semantic Web
KW  - Semantics
KW  - Type methods
KW  - UCI machine learning repository
KW  - Web Mining
ER  - 

TY  - CONF
TI  - Sage: Practical and scalable ML-driven performance debugging in microservices
AU  - Gan, Y.
AU  - Liang, M.
AU  - Dev, S.
AU  - Lo, D.
AU  - Delimitrou, C.
T2  - International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS
AB  - Cloud applications are increasingly shifting from large monolithic services to complex graphs of loosely-coupled microservices. Despite the advantages of modularity and elasticity microservices offer, they also complicate cluster management and performance debugging, as dependencies between tiers introduce backpressure and cascading QoS violations. Prior work on performance debugging for cloud services either relies on empirical techniques, or uses supervised learning to diagnose the root causes of performance issues, which requires significant application instrumentation, and is difficult to deploy in practice. We present Sage, a machine learning-driven root cause analysis system for interactive cloud microservices that focuses on practicality and scalability. Sage leverages unsupervised ML models to circumvent the overhead of trace labeling, captures the impact of dependencies between microservices to determine the root cause of unpredictable performance online, and applies corrective actions to recover a cloud service's QoS. In experiments on both dedicated local clusters and large clusters on Google Compute Engine we show that Sage consistently achieves over 93% accuracy in correctly identifying the root cause of QoS violations, and improves performance predictability. © 2021 ACM.
DA  - 2021///
PY  - 2021
DO  - 10.1145/3445814.3446700
SP  - 135
EP  - 151
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104802944&doi=10.1145%2f3445814.3446700&partnerID=40&md5=ac1119c03a8d48d24c437606287afbc7
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 148</p>
KW  - Auto encoders
KW  - Bayesia n networks
KW  - Bayesian network
KW  - Bayesian networks
KW  - Cloud applications
KW  - cloud computing
KW  - Cloud services
KW  - Cloud-computing
KW  - counterfactual
KW  - Counterfactuals
KW  - Distributed database systems
KW  - Learning systems
KW  - Machine learning
KW  - Microservice
KW  - microservices
KW  - performance debugging
KW  - Performance debugging
KW  - Program debugging
KW  - QoS
KW  - Root cause
KW  - variational autoencoder
KW  - Variational autoencoder
KW  - Web services
ER  - 

TY  - JOUR
TI  - IntMA: Dynamic Interaction-aware resource allocation for containerized microservices in cloud environments
AU  - Joseph, C.T.
AU  - Chandrasekaran, K.
T2  - Journal of Systems Architecture
AB  - The Information Technology sector has undergone tremendous changes arising due to the emergence and prevalence of Cloud Computing. Microservice Architectures have also been attracting attention from several industries and researchers. Due to the suitability of microservices for the Cloud environments, an increasing number of Cloud applications are now provided as microservices. However, this transition to microservices brings a wide range of infrastructural orchestration challenges. Though several research works have discussed the engineering of microservice-based applications, there is an inevitable need for research on handling the operational phases of the microservice components. Microservice application deployment in containerized datacenters must be optimized to enhance the overall system performance. In this research work, the deployment of microservice application modules on the Cloud infrastructure is first modelled as a Binary Quadratic Programming Problem. In order to reduce the adverse impact of communication latencies on the response time, the interaction pattern between the microservice components is modelled as an undirected doubly weighted complete Interaction Graph. A novel, robust heuristic approach IntMA is also proposed for deploying the microservices in an interaction-aware manner with the aid of the interaction information obtained from the Interaction Graph. The proposed allocation policies are implemented in Kubernetes. The effectiveness of the proposed approach is evaluated on the Google Cloud Platform, using different microservice reference applications. Experimental results indicate that the proposed approach improves the response time and throughput of the microservice-based systems. © 2020 Elsevier B.V.
DA  - 2020///
PY  - 2020
DO  - 10.1016/j.sysarc.2020.101785
VL  - 111
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084330280&doi=10.1016%2fj.sysarc.2020.101785&partnerID=40&md5=e8e088ca640e149ab34a5a81754acb56
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 62</p>
KW  - Allocation policies
KW  - Application deployment
KW  - Binary quadratic programming problems
KW  - Cloud computing
KW  - Cloud infrastructures
KW  - Communication latency
KW  - Container orchestration
KW  - Containers
KW  - Engineering research
KW  - Heuristic methods
KW  - Information technology sector
KW  - Interaction information
KW  - Interaction pattern
KW  - Microservice placement
KW  - Performance optimization
KW  - Quadratic programming
KW  - Response time (computer systems)
KW  - Service oriented computing
ER  - 

TY  - CONF
TI  - Root-Cause Metric Location for Microservice Systems via Log Anomaly Detection
AU  - Wang, L.
AU  - Zhao, N.
AU  - Chen, J.
AU  - Li, P.
AU  - Zhang, W.
AU  - Sui, K.
T2  - Proceedings - 2020 IEEE 13th International Conference on Web Services, ICWS 2020
AB  - Microservice systems are typically fragile and failures are inevitable in them due to their complexity and large scale. However, it is challenging to localize the root-cause metric due to its complicated dependencies and the huge number of various metrics. Existing methods are based on either correlation between metrics or correlation between metrics and failures. All of them ignore the key data source in microservice, i.e., logs. In this paper, we propose a novel root-cause metric localization approach by incorporating log anomaly detection. Our approach is based on a key observation, the value of root-cause metric should be changed along with the change of the log anomaly score of the system caused by the failure. Specifically, our approach includes two components, collecting anomaly scores by log anomaly detection algorithm and identifying root-cause metric by robust correlation analysis with data augmentation. Experiments on an open-source benchmark microservice system have demonstrated our approach can identify root-cause metrics more accurately than existing methods and only require a short localization time. Therefore, our approach can assist engineers to save much effort in diagnosing and mitigating failures as soon as possible.  © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ICWS49710.2020.00026
SP  - 142
EP  - 150
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097182831&doi=10.1109%2fICWS49710.2020.00026&partnerID=40&md5=300256216fd6bc7b17219c74e3472fbc
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 60</p>
KW  - Anomaly detection
KW  - Anomaly-detection algorithms
KW  - Correlation analysis
KW  - Data augmentation
KW  - Data-source
KW  - Open sources
KW  - Open systems
KW  - Root cause
KW  - Two-component
KW  - Web services
KW  - Websites
ER  - 

TY  - CONF
TI  - Anomaly detection and diagnosis for container-based microservices with performance monitoring
AU  - Du, Q.
AU  - Xie, T.
AU  - He, Y.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - With emerging container technologies, such as Docker, microservices-based applications can be developed and deployed in cloud environment much agiler. The dependability of these microservices becomes a major concern of application providers. Anomalous behaviors which may lead to unexpected failures can be detected with anomaly detection techniques. In this paper, an anomaly detection system (ADS) is designed to detect and diagnose the anomalies in microservices by monitoring and analyzing real-time performance data of them. The proposed ADS consists of a monitoring module that collects the performance data of containers, a data processing module based on machine learning models and a fault injection module integrated for training these models. The fault injection module is also used to assess the anomaly detection and diagnosis performance of our ADS. Clearwater, an open source virtual IP Multimedia Subsystem, is used for the validation of our ADS and experimental results show that the proposed ADS works well. © Springer Nature Switzerland AG 2018.
DA  - 2018///
PY  - 2018
DO  - 10.1007/978-3-030-05063-4_42
VL  - 11337 LNCS
SP  - 560
EP  - 572
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058619119&doi=10.1007%2f978-3-030-05063-4_42&partnerID=40&md5=9c09c0bbfda8e0cc3516ad3638ec227a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 52</p>
KW  - Anomaly detection
KW  - Anomaly detection systems
KW  - Application providers
KW  - Artificial intelligence
KW  - Chemical detection
KW  - Containers
KW  - Data handling
KW  - Deceleration
KW  - Fault detection
KW  - IP multimedia subsystems
KW  - Learning systems
KW  - Machine learning
KW  - Microservices
KW  - Parallel architectures
KW  - Performance monitoring
KW  - Real time performance
KW  - Software testing
KW  - Unexpected Failures
ER  - 

TY  - CONF
TI  - Seer: Leveraging Big Data to Navigate the Complexity of Performance Debugging in Cloud Microservices
AU  - Gan, Y.
AU  - Zhang, Y.
AU  - Hu, K.
AU  - Cheng, D.
AU  - He, Y.
AU  - Pancholi, M.
AU  - Delimitrou, C.
T2  - International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS
AB  - Performance unpredictability is a major roadblock towards cloud adoption, and has performance, cost, and revenue ramifications. Predictable performance is even more critical as cloud services transition from monolithic designs to microservices. Detecting QoS violations after they occur in systems with microservices results in long recovery times, as hotspots propagate and amplify across dependent services. We present Seer, an online cloud performance debugging system that leverages deep learning and the massive amount of tracing data cloud systems collect to learn spatial and temporal patterns that translate to QoS violations. Seer combines lightweight distributed RPC-level tracing, with detailed low-level hardware monitoring to signal an upcoming QoS violation, and diagnose the source of unpredictable performance. Once an imminent QoS violation is detected, Seer notifies the cluster manager to take action to avoid performance degradation altogether.We evaluate Seer both in local clusters, and in large-scale deployments of end-to-end applications built with microservices with hundreds of users. We show that Seer correctly anticipates QoS violations 91% of the time, and avoids the QoS violation to begin with in 84% of cases. Finally, we show that Seer can identify applicationlevel design bugs, and provide insights on how to better architect microservices to achieve predictable performance. © 2019 Association for Computing Machinery.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3297858.3304004
SP  - 19
EP  - 33
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064659746&doi=10.1145%2f3297858.3304004&partnerID=40&md5=67a5e54db5ebf1ecee4bb2bc5f5c7790
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 252</p>
KW  - Big data
KW  - Cloud adoptions
KW  - cloud computing
KW  - Cloud computing
KW  - Cloud-computing
KW  - data mining
KW  - Data mining
KW  - datacenter
KW  - Datacenter
KW  - deep learning
KW  - Deep learning
KW  - Information management
KW  - Microservice
KW  - microservices
KW  - monitoring
KW  - Online systems
KW  - Performance
KW  - Performance costs
KW  - performance debugging
KW  - Performance debugging
KW  - Program debugging
KW  - QoS
KW  - Quality of service
KW  - resource management
KW  - Resource management
KW  - tracing
KW  - Tracing
ER  - 

TY  - CONF
TI  - A framework for measuring business processes based on GQM
AU  - Aversano, L.
AU  - Bodhuin, T.
AU  - Canfora, G.
AU  - Tortorella, M.
T2  - Proceedings of the Hawaii International Conference on System Sciences
AB  - The evolution of business processes and supporting software systems requires their analysis and assessment from both quantitative and qualitative points of view. The analysis and evaluation activities need the support of methodological and technological tools, customizable to the innovation requirements of the chosen processes and supporting software systems. This paper proposes a measurement framework based on the Goal-Question-Metric (GQM) paradigm. It is generally applicable to any business process and supporting software system after its instantiation. The collaborative software environment WebEv, Web for the Evaluation, is proposed for facilitating the collection and elaboration of the required measures. Finally, the paper describes the application of the measurement framework in a real context.
DA  - 2004///
PY  - 2004
DO  - 10.1109/hicss.2004.1265061
VL  - 37
SP  - 163
EP  - 172
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-12344288282&doi=10.1109%2fhicss.2004.1265061&partnerID=40&md5=e19b39c197b73a054c7e5dfa47a2492b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 25</p>
KW  - Business process reengineering (BPR)
KW  - Characterization
KW  - Computer software
KW  - Data acquisition
KW  - Electronic commerce
KW  - Electronic mail
KW  - Goal-Question-Metric (GQM)
KW  - Knowledge acquisition
KW  - Learning systems
KW  - Legacy systems
KW  - Online systems
KW  - Organizational goals
KW  - Risk analysis
KW  - Risk assessment
KW  - Societies and institutions
KW  - World Wide Web
ER  - 

TY  - CONF
TI  - Self-supervised anomaly detection from distributed traces
AU  - Bogatinovski, J.
AU  - Nedelkoski, S.
AU  - Cardoso, J.
AU  - Kao, O.
T2  - Proceedings - 2020 IEEE/ACM 13th International Conference on Utility and Cloud Computing, UCC 2020
AB  - Artificial Intelligence for IT Operations (AIOps) combines big data and machine learning to replace a broad range of IT Operations tasks including reliability and performance monitoring of services. By exploiting observability data, AIOps enable detection of faults and issues of services. The focus of this work is on detecting anomalies based on distributed tracing records that contain detailed information of the services of the distributed system. Timely and accurately detecting trace anomalies is very challenging due to the large number of underlying microservices and the complex call relationships between them. We addresses the problem anomaly detection from distributed traces with a novel self-supervised method and a new learning task formulation. The method is able to have high performance even in large traces and capture complex interactions between the services. The evaluation shows that the approach achieves high accuracy and solid performance in the experimental testbed. © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/UCC48980.2020.00054
SP  - 342
EP  - 347
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099576837&doi=10.1109%2fUCC48980.2020.00054&partnerID=40&md5=183c99268d9949e744adc2aab1a7489a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 35</p>
KW  - Anomaly detection
KW  - Artificial intelligence
KW  - Cloud computing
KW  - Detecting trace
KW  - Distributed database systems
KW  - Distributed systems
KW  - Distributed traces
KW  - Distributed tracing
KW  - Experimental testbed
KW  - High-accuracy
KW  - Learning systems
KW  - Learning tasks
KW  - Performance monitoring
KW  - Self-supervised learning
KW  - Supervised methods
ER  - 

TY  - CONF
TI  - MicroRCA: Root Cause Localization of Performance Issues in Microservices
AU  - Wu, L.
AU  - Tordsson, J.
AU  - Elmroth, E.
AU  - Kao, O.
T2  - Proceedings of IEEE/IFIP Network Operations and Management Symposium 2020: Management in the Age of Softwarization and Artificial Intelligence, NOMS 2020
AB  - Software architecture is undergoing a transition from monolithic architectures to microservices to achieve resilience, agility and scalability in software development. However, with microservices it is difficult to diagnose performance issues due to technology heterogeneity, large number of microservices, and frequent updates to both software features and infrastructure. This paper presents MicroRCA, a system to locate root causes of performance issues in microservices. MicroRCA infers root causes in real time by correlating application performance symptoms with corresponding system resource utilization, with-out any application instrumentation. The root cause localization is based on an attributed graph that model anomaly propagation across services and machines. Our experimental evaluation where common anomalies are injected to a microservice benchmark running in a Kubernetes cluster shows that MicroRCA locates root causes well, with 89% precision and 97% mean average precision, outperforming several state-of-the-art methods. © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/NOMS47738.2020.9110353
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086754646&doi=10.1109%2fNOMS47738.2020.9110353&partnerID=40&md5=1e3e7daea9f1f548cb845401f4281bb5
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 190</p>
KW  - Anomaly propagation
KW  - Application performance
KW  - Experimental evaluation
KW  - microservices
KW  - Monolithic architecture
KW  - Network architecture
KW  - performance degradation
KW  - Performance issues
KW  - Petroleum reservoir evaluation
KW  - root cause analysis
KW  - Software design
KW  - Software features
KW  - State-of-the-art methods
KW  - System resource utilization
ER  - 

TY  - JOUR
TI  - Anomaly detection: A survey
AU  - Chandola, V.
AU  - Banerjee, A.
AU  - Kumar, V.
T2  - ACM Computing Surveys
AB  - Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and more succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with. © 2009 ACM.
DA  - 2009///
PY  - 2009
DO  - 10.1145/1541880.1541882
VL  - 41
IS  - 3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-68049121093&doi=10.1145%2f1541880.1541882&partnerID=40&md5=5f32c8c06af2667c0db2c0c61fdf75b1
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 9509</p>
KW  - Anomalous behavior
KW  - Anomaly detection
KW  - Application domains
KW  - Computational complexity
KW  - Outlier detection
KW  - Real applications
KW  - Research
KW  - Research areas
KW  - Surveys
ER  - 

TY  - CONF
TI  - Protected Coordination of Service Mesh for Container-Based 3-Tier Service Traffic
AU  - Kang, M.
AU  - Shin, J.-S.
AU  - Kim, J.
T2  - International Conference on Information Networking
AB  - With the rapid expansion of cloud-native computing paradigm, a growing number of applications are adopting container-based microservices architecture (MSA). The resulting container-based MSA functions are required to be tightly stitched together and therefore the stitching interconnections need persistent monitor/control coordination. Recent service mesh approaches are trying to solve this maintenance problem in a transparent manner. However, with the popular Istio-based service mesh realization, the monitor/control traffic for coordination is exposed to application tenants, which could be a potential security concern. Typically, containers are limited with a single network interface and thus it is not possible to isolate control / monitor traffic from application data traffic. Thus, in this paper, we propose a protected coordination scheme for service mesh as a work-Around solution. For this protected coordination of service mesh, the monitor/control traffic for coordination is separated and encrypted while data traffic is treated in the same way with the existing service mesh approach. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/ICOIN.2019.8718120
VL  - 2019-January
SP  - 427
EP  - 429
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066732778&doi=10.1109%2fICOIN.2019.8718120&partnerID=40&md5=c6df1567721aa2700367adc4222f2482
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 16</p>
KW  - and protected coordination of service mesh
KW  - Application data
KW  - Computer architecture
KW  - Computing paradigm
KW  - container-based cloud-native computing
KW  - Containers
KW  - Coordination scheme
KW  - Maintenance Problem
KW  - Mesh generation
KW  - Microservices architecture
KW  - Network architecture
KW  - Rapid expansion
KW  - service mesh
KW  - Service traffic
ER  - 

TY  - CONF
TI  - Causal Inference Techniques for Microservice Performance Diagnosis: Evaluation and Guiding Recommendations
AU  - Wu, L.
AU  - Tordsson, J.
AU  - Elmroth, E.
AU  - Kao, O.
T2  - Proceedings - 2021 IEEE International Conference on Autonomic Computing and Self-Organizing Systems, ACSOS 2021
AB  - Causal inference (CI) is one of the popular performance diagnosis methods, which infers the anomaly propagation from the observed data for locating the root causes. Although some specific CI methods have been employed in the literature, the overall performance of this class of methods on microservice performance diagnosis is not well understood. To this end, we select six representative CI methods from three categories and evaluate their performance against the challenges of microservice operations, including the large-scale observable data, heterogeneous anomaly symptoms, and a wide range of root causes. Our experimental results show that 1) CI techniques must be integrated with anomaly detection or anomaly scores to differentiate the causality in normal and abnormal data; 2) CI techniques are more robust to false positives in anomaly detection than knowledge-based non-CI method; 3) To get the fine-grained root causes, an effective way with CI techniques is to identify the faulty service first and infer the detailed explanation of the service abnormality. Overall, this work broadens the understanding of how CI methods perform on microservice performance diagnosis and provides recommendations for an efficient application of CI methods.  © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/ACSOS52086.2021.00029
SP  - 21
EP  - 30
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124790415&doi=10.1109%2fACSOS52086.2021.00029&partnerID=40&md5=786db1ae8ed4e81abc085f716a1b2de5
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 23</p>
KW  - Anomaly detection
KW  - Causal inference
KW  - Causal inferences
KW  - Experimental evaluation
KW  - Inference methods
KW  - Inference techniques
KW  - Knowledge based systems
KW  - Microservice
KW  - Microservices
KW  - Performance
KW  - Performance diagnosis
KW  - Root cause
KW  - Self-healing
KW  - Self-healing materials
ER  - 

TY  - CONF
TI  - CloudRanger: Root cause identification for cloud native systems
AU  - Wang, P.
AU  - Xu, J.
AU  - Ma, M.
AU  - Lin, W.
AU  - Pan, D.
AU  - Wang, Y.
AU  - Chen, P.
T2  - Proceedings - 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, CCGRID 2018
AB  - As more and more systems are migrating to cloud environment, the cloud native system becomes a trend. This paper presents the challenges and implications when diagnosing root causes for cloud native systems by analyzing some real incidents occurred in IBM Bluemix (a large commercial cloud). To tackle these challenges, we propose CloudRanger, a novel system dedicated for cloud native systems. To make our system more general, we propose a dynamic causal relationship analysis approach to construct impact graphs amongst applications without given the topology. A heuristic investigation algorithm based on second-order random walk is proposed to identify the culprit services which are responsible for cloud incidents. Experimental results in both simulation environment and IBM Bluemix platform show that CloudRanger outperforms some state-of-the-art approaches with a 10% improvement in accuracy. It offers a fast identification of culprit services when an anomaly occurs. Moreover, this system can be deployed rapidly and easily in multiple kinds of cloud native systems without any predefined knowledge. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/CCGRID.2018.00076
SP  - 492
EP  - 502
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050958994&doi=10.1109%2fCCGRID.2018.00076&partnerID=40&md5=162906183623bed7fd3d3dc29714a4be
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 150</p>
KW  - Anomaly detection
KW  - Causality
KW  - Cloud native systems
KW  - Cloud natives
KW  - Cluster computing
KW  - Grid computing
KW  - Micro service architecture
KW  - Micro services
KW  - Root cause analysis
KW  - Topology
ER  - 

TY  - JOUR
TI  - An Anomaly Detection Algorithm for Microservice Architecture Based on Robust Principal Component Analysis
AU  - Jin, M.
AU  - Lv, A.
AU  - Zhu, Y.
AU  - Wen, Z.
AU  - Zhong, Y.
AU  - Zhao, Z.
AU  - Wu, J.
AU  - Li, H.
AU  - He, H.
AU  - Chen, F.
T2  - IEEE Access
AB  - Microservice architecture (MSA) is a new software architecture, which divides a large single application and service into dozens of supporting microservices. With the increasingly popularity of MSA, the security issues of MSA get a lot of attention. In this paper, we propose an algorithm for mining causality and the root cause. Our algorithm consists of two parts: invocation chain anomaly analysis based on robust principal component analysis (RPCA) and a single indicator anomaly detection algorithm. The single indicator anomaly detection algorithm is composed of Isolation Forest (IF) algorithm, One-Class Support Vector Machine (SVM) algorithm, Local Outlier Factor (LOF) algorithm, and 3σ principle. For general and network time-consuming anomaly in the process of the MSA, we formulate different anomaly time-consuming detection strategies. We select a batch of sample data and three batches of test data of the 2020 International AIOps Challenge to debug our algorithm. According to the scoring criteria of the competition organizers, our algorithm has an average score of 0.8304 (The full score is 1) in the four batches of data. Our proposed algorithm has higher accuracy than some traditional machine learning algorithms in anomaly detection. © 2013 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ACCESS.2020.3044610
VL  - 8
SP  - 226397
EP  - 226408
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130828992&doi=10.1109%2fACCESS.2020.3044610&partnerID=40&md5=b9d7d008b39768cc71925147bfe1666c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 42</p>
KW  - anomaly detection
KW  - Microservice architecture
KW  - root cause analysis
ER  - 

TY  - CONF
TI  - An experimental comparison of software-based power meters: Focus on CPU and GPU
AU  - Jay, M.
AU  - Ostapenco, V.
AU  - Lefevre, L.
AU  - Trystram, D.
AU  - Orgerie, A.-C.
AU  - Fichel, B.
T2  - Proceedings - 23rd IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing, CCGrid 2023
AB  - The global energy demand for digital activities is constantly growing. Computing nodes and cloud services are at the heart of these activities. Understanding their energy consumption is an important step towards reducing it. On one hand, physical power meters are very accurate in measuring energy but they are expensive, difficult to deploy on a large scale, and are not able to provide measurements at the service level. On the other hand, power models and vendor-specific internal interfaces are already available or can be implemented on existing systems. Plenty of tools, called software-based power meters, have been developed around the concepts of power models and internal interfaces, in order to report the power consumption at levels ranging from the whole computing node to applications and services. However, we have found that it can be difficult to choose the right tool for a specific need. In this work, we qualitatively and experimentally compare several software-based power meters able to deal with CPU or GPU-based infrastructures. For this purpose, we evaluate them against high-precision physical power meters while executing various intensive workloads. We extend this empirical study to highlight the strengths and limitations of each software-based power meter.  © 2023 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/CCGrid57682.2023.00020
SP  - 106
EP  - 118
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163867329&doi=10.1109%2fCCGrid57682.2023.00020&partnerID=40&md5=4a8d9932fa1d8e9882d85ec10ae66b7d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 56</p>
KW  - Application programs
KW  - Computing nodes
KW  - Computing power
KW  - Electric power measurement
KW  - Energy measurement
KW  - Energy utilization
KW  - Experimental comparison
KW  - Global energy demand
KW  - Graphics processing unit
KW  - Green computing
KW  - Internal interfaces
KW  - Physical power
KW  - Power
KW  - Power measurement
KW  - Power modeling
KW  - Powermeter
KW  - Software evaluation
ER  - 

TY  - CONF
TI  - TADL: Fault Localization with Transformer-based Anomaly Detection for Dynamic Microservice Systems
AU  - Li, Y.
AU  - Lu, Y.
AU  - Wang, J.
AU  - Qi, Q.
AU  - Wang, J.
AU  - Wang, Y.
AU  - Liao, J.
T2  - Proceedings - 2023 IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2023
AB  - Due to the complexity of microservice architecture, it is difficult to accomplish efficient microservice anomaly detection and localization tasks and achieve the target of high system reliability. For rapid failure recovery and user satisfaction, it is significant to detect and locate anomalies fast and accurately in microservice systems. In this paper, we propose an anomaly detection and localization model based on Transformer, named TADL (Transformer-based Anomaly Detector and Locator), which models the temporal features and dynamically captures container relationships using Transformer with sandwich structure. TADL uses readily available container performance metrics, making it easy to implement in already-running container clusters. Evaluations are conducted on a sock-shop dataset collected from a real microservice system and a publicly available dataset SMD. Empirical studies on the above two datasets demonstrate that TADL can outperform baseline methods in the performance of anomaly detection, the latency of anomaly detection, and the effect of anomalous container localization, which indicates that TADL is useful in maintaining complex and dynamic microservice systems in the real world. © 2023 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/SANER56733.2023.00078
SP  - 718
EP  - 722
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160528611&doi=10.1109%2fSANER56733.2023.00078&partnerID=40&md5=a693ac80cd929572fde44eb9c253c95e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 10</p>
KW  - anomaly detection
KW  - Anomaly detection
KW  - Anomaly detector
KW  - Anomaly localizations
KW  - Containers
KW  - Detection and localization
KW  - Failure recovery
KW  - Fault detection
KW  - fault localization
KW  - Fault localization
KW  - Microservice system
KW  - microservice systems
KW  - System reliability
KW  - transformer
KW  - Transformer
KW  - Users' satisfactions
ER  - 

TY  - JOUR
TI  - Machine Learning for Anomaly Detection: A Systematic Review
AU  - Nassif, A.B.
AU  - Talib, M.A.
AU  - Nasir, Q.
AU  - Dakalbab, F.M.
T2  - IEEE Access
AB  - Anomaly detection has been used for decades to identify and extract anomalous components from data. Many techniques have been used to detect anomalies. One of the increasingly significant techniques is Machine Learning (ML), which plays an important role in this area. In this research paper, we conduct a Systematic Literature Review (SLR) which analyzes ML models that detect anomalies in their application. Our review analyzes the models from four perspectives; the applications of anomaly detection, ML techniques, performance metrics for ML models, and the classification of anomaly detection. In our review, we have identified 290 research articles, written from 2000-2020, that discuss ML techniques for anomaly detection. After analyzing the selected research articles, we present 43 different applications of anomaly detection found in the selected research articles. Moreover, we identify 29 distinct ML models used in the identification of anomalies. Finally, we present 22 different datasets that are applied in experiments on anomaly detection, as well as many other general datasets. In addition, we observe that unsupervised anomaly detection has been adopted by researchers more than other classification anomaly detection systems. Detection of anomalies using ML models is a promising area of research, and there are a lot of ML models that have been implemented by researchers. Therefore, we provide researchers with recommendations and guidelines based on this review.  © 2013 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/ACCESS.2021.3083060
VL  - 9
SP  - 78658
EP  - 78700
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113269963&doi=10.1109%2fACCESS.2021.3083060&partnerID=40&md5=88521f3ba2e821eaa1752edce8c07c43
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 484</p>
KW  - Anomalous component
KW  - Anomaly detection
KW  - Anomaly detection systems
KW  - machine learning
KW  - Machine learning
KW  - Performance metrics
KW  - Research papers
KW  - security and privacy protection
KW  - Systematic literature review (SLR)
KW  - Systematic Review
KW  - Unsupervised anomaly detection
ER  - 

TY  - JOUR
TI  - BAE: Anomaly Detection Algorithm Based on Clustering and Autoencoder
AU  - Wang, D.
AU  - Nie, M.
AU  - Chen, D.
T2  - Mathematics
AB  - In this paper, we propose an outlier-detection algorithm for detecting network traffic anomalies based on a clustering algorithm and an autoencoder model. The BIRCH clustering algorithm is employed as the pre-algorithm of the autoencoder to pre-classify datasets with complex data distribution characteristics, while the autoencoder model is used to detect outliers based on a threshold. The proposed BIRCH-Autoencoder (BAE) algorithm has been tested on four network security datasets, KDDCUP99, UNSW-NB15, CICIDS2017, and NSL-KDD, and compared with representative algorithms. The BAE algorithm achieved average F-scores of 96.160, 81.132, and 91.424 on the KDDCUP99, UNSW-NB15, and CICIDS2017 datasets, respectively. These experimental results demonstrate that the proposed approach can effectively and accurately detect anomalous data. © 2023 by the authors.
DA  - 2023///
PY  - 2023
DO  - 10.3390/math11153398
VL  - 11
IS  - 15
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167571991&doi=10.3390%2fmath11153398&partnerID=40&md5=9f9a1bf2f4ba1229ba04a5d9b184975d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 17</p>
KW  - anomaly detection
KW  - Autoencoder
KW  - BIRCH
KW  - pre-classification
ER  - 

TY  - JOUR
TI  - Monitoring tools for DevOps and microservices: A systematic grey literature review
AU  - Giamattei, L.
AU  - Guerriero, A.
AU  - Pietrantuono, R.
AU  - Russo, S.
AU  - Malavolta, I.
AU  - Islam, T.
AU  - Dînga, M.
AU  - Koziolek, A.
AU  - Singh, S.
AU  - Armbruster, M.
AU  - Gutierrez-Martinez, J.M.
AU  - Caro-Alvaro, S.
AU  - Rodriguez, D.
AU  - Weber, S.
AU  - Henss, J.
AU  - Vogelin, E.F.
AU  - Panojo, F.S.
T2  - Journal of Systems and Software
AB  - Microservice-based systems are usually developed according to agile practices like DevOps, which enables rapid and frequent releases to promptly react and adapt to changes. Monitoring is a key enabler for these systems, as they allow to continuously get feedback from the field and support timely and tailored decisions for a quality-driven evolution. In the realm of monitoring tools available for microservices in the DevOps-driven development practice, each with different features, assumptions, and performance, selecting a suitable tool is an as much difficult as impactful task. This article presents the results of a systematic study of the grey literature we performed to identify, classify and analyze the available monitoring tools for DevOps and microservices. We selected and examined a list of 71 monitoring tools, drawing a map of their characteristics, limitations, assumptions, and open challenges, meant to be useful to both researchers and practitioners working in this area. Results are publicly available and replicable. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2023 The Author(s)
DA  - 2024///
PY  - 2024
DO  - 10.1016/j.jss.2023.111906
VL  - 208
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182148345&doi=10.1016%2fj.jss.2023.111906&partnerID=40&md5=95d4ebf97c7fd28d403838eacaf30ac2
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 24</p>
KW  - Agile practices
KW  - Development practices
KW  - DevOps
KW  - Grey literature
KW  - Literature reviews
KW  - Microservice
KW  - Monitoring
KW  - Monitoring tools
KW  - MSA
KW  - Open science
KW  - Performance
KW  - Systematic study
KW  - Tools
ER  - 

TY  - JOUR
AU  - Newman, S.
T2  - Building Microservices
DA  - 2015///
PY  - 2015
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950338538&partnerID=40&md5=aec25db8f81564a4ab82f370c5e620cc
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1629</p>
ER  - 

TY  - CONF
TI  - On the energy optimization for precedence constrained applications using local search algorithms
AU  - Pecero, J.E.
AU  - Huacuja, H.J.F.
AU  - Bouvry, P.
AU  - Pineda, A.A.S.
AU  - Locés, M.C.L.
AU  - Barbosa, J.J.G.
T2  - Proceedings of the 2012 International Conference on High Performance Computing and Simulation, HPCS 2012
AB  - We investigate the problem of scheduling precedence constrained applications on a distributed heterogeneous computing system with the aim of minimizing schedule length and reducing energy consumption. We present a scheduling algorithm based on the best-effort idea that promotes local search algorithms and dynamic voltage scaling to reduce energy consumption. The final goal is to maintain a given performance while minimizing energy use. The proposed approach first uses a list-based scheduling algorithm to find near-optimal solutions for schedule length, then local search algorithms with dynamic voltage scaling are applied to reduce energy consumption. However the algorithm it's not allowed to deteriorate the schedule length computed by the best-effort algorithm. We discuss simulation results obtained with sets of real-world applications that emphasize the interest of the approach. © 2012 IEEE.
DA  - 2012///
PY  - 2012
DO  - 10.1109/HPCSim.2012.6266902
SP  - 133
EP  - 139
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866976184&doi=10.1109%2fHPCSim.2012.6266902&partnerID=40&md5=3266477650faca0fb2f3573b9bb0de38
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 9</p>
KW  - Best-effort
KW  - Constrained optimization
KW  - Distributed heterogeneous computing
KW  - Distributed systems
KW  - Dynamic voltage scaling
KW  - Energy conservation
KW  - Energy optimization
KW  - Energy utilization
KW  - greenIT
KW  - heterogenous distributed systems
KW  - Learning algorithms
KW  - Local search algorithm
KW  - Minimizing energy
KW  - Near-optimal solutions
KW  - optimization
KW  - Optimization
KW  - performance of systems
KW  - Performance of systems
KW  - Real-world application
KW  - Reducing energy consumption
KW  - Schedule length
KW  - scheduling
KW  - Scheduling
KW  - Voltage stabilizing circuits
ER  - 

TY  - CONF
TI  - The straw that broke the camel's back: Safe cloud overbooking with application brownout
AU  - Tomas, L.
AU  - Klein, C.
AU  - Tordsson, J.
AU  - Hernandez-Rodriguez, F.
T2  - Proceedings - 2014 International Conference on Cloud and Autonomic Computing, ICCAC 2014
AB  - Resource overbooking is an admission control technique to increase utilization in cloud environments. However, due to uncertainty about future application workloads, overbooking may result in overload situations and deteriorated performance. We mitigate this using brownout, a feedback approach to application performance steering, that ensures graceful degradation during load spikes and thus avoids overload. Additionally, brownout management information is included into the overbooking system, enabling the development of improved reactive methods to overload situations. Our combined brownout-overbooking approach is evaluated based on real-life interactive workloads and non-interactive batch applications. The results show that our approach achieves an improvement of resource utilization of 11 to 37 percentage points, while keeping response times lower than the set target of 1 second, with negligible application degradation. © 2014 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/ICCAC.2014.10
SP  - 151
EP  - 160
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923972497&doi=10.1109%2fICCAC.2014.10&partnerID=40&md5=8b2ea2c3f695f04eef39fc7e39e61500
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 24</p>
KW  - Application performance
KW  - Cloud environments
KW  - Computer programming
KW  - Computer science
KW  - Future applications
KW  - Graceful degradation
KW  - Information management
KW  - Management information
KW  - Over-booking approaches
KW  - Resource overbooking
KW  - Resource utilizations
ER  - 

TY  - CONF
TI  - Brownout: Building more robust cloud applications
AU  - Klein, C.
AU  - Maggio, M.
AU  - Arzén, K.-E.
AU  - Hernández-Rodriguez, F.
T2  - Proceedings - International Conference on Software Engineering
AB  - Self-adaptation is a first class concern for cloud applications, which should be able to withstand diverse runtime changes. Variations are simultaneously happening both at the cloud infrastructure level - for example hardware failures - and at the user workload level - flash crowds. However, robustly withstanding extreme variability, requires costly hardware over-provisioning. In this paper, we introduce a self-adaptation programming paradigm called brownout. Using this paradigm, applications can be designed to robustly withstand unpredictable runtime variations, without over-provisioning. The paradigm is based on optional code that can be dynamically deactivated through decisions based on control theory. We modified two popular web application prototypes - RUBiS and RUBBoS - with less than 170 lines of code, to make them brownout-compliant. Experiments show that brownout self-adaptation dramatically improves the ability to withstand flash-crowds and hardware failures. © 2014 ACM.
DA  - 2014///
PY  - 2014
DO  - 10.1145/2568225.2568227
SP  - 700
EP  - 711
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994065778&doi=10.1145%2f2568225.2568227&partnerID=40&md5=65399bd44b01bdd11e43ebcda0ddb8ab
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 120</p>
KW  - Adaptive software
KW  - Adaptive Software
KW  - Brownout
KW  - Cloud
KW  - Cloud applications
KW  - Cloud infrastructures
KW  - Clouds
KW  - Control theory
KW  - Control Theory
KW  - Hardware
KW  - Hardware failures
KW  - Over provisioning
KW  - Programming paradigms
KW  - Reconfigurable hardware
KW  - Run-time variations
KW  - Software engineering
ER  - 

TY  - JOUR
TI  - Probabilistic Consolidation of Virtual Machines in Self-Organizing Cloud Data Centers
AU  - Mastroianni, C.
AU  - Meo, M.
AU  - Papuzzo, G.
T2  - IEEE Transactions on Cloud Computing
AB  - Power efficiency is one of the main issues that will drive the design of data centers, especially of those devoted to provide Cloud computing services. In virtualized data centers, consolidation of Virtual Machines (VMs) on the minimum number of physical servers has been recognized as a very efficient approach, as this allows unloaded servers to be switched off or used to accommodate more load, which is clearly a cheaper alternative to buy more resources. The consolidation problem must be solved on multiple dimensions, since in modern data centers CPU is not the only critical resource: depending on the characteristics of the workload other resources, for example, RAM and bandwidth, can become the bottleneck. The problem is so complex that centralized and deterministic solutions are practically useless in large data centers with hundreds or thousands of servers. This paper presents ecoCloud a self-organizing and adaptive approach for the consolidation of VMs on two resources, namely CPU and RAM. Decisions on the assignment and migration of VMs are driven by probabilistic processes and are based exclusively on local information, which makes the approach very simple to implement. Both a fluid-like mathematical model and experiments on a real data center show that the approach rapidly consolidates the workload, and CPU-bound and RAM-bound VMs are balanced, so that both resources are exploited efficiently. © 2013 IEEE.
DA  - 2013///
PY  - 2013
DO  - 10.1109/TCC.2013.17
VL  - 1
IS  - 2
SP  - 215
EP  - 228
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969916663&doi=10.1109%2fTCC.2013.17&partnerID=40&md5=ca388967b1d387943d840d8ba2d0e6ec
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 156</p>
KW  - Cloud computing
KW  - Cloud computing services
KW  - Cloud data centers
KW  - Critical resources
KW  - data center
KW  - Data centers
KW  - Digital storage
KW  - Energy conservation
KW  - energy saving
KW  - Multiple dimensions
KW  - Network security
KW  - Probabilistic consolidations
KW  - Probabilistic process
KW  - Virtual machine
KW  - Virtualized data centers
KW  - VM consolidation
ER  - 

TY  - JOUR
TI  - Energy-efficient operation of multicore processors by DVFS, task migration, and active cooling
AU  - Hanumaiah, V.
AU  - Vrudhula, S.
T2  - IEEE Transactions on Computers
AB  - Energy efficiency has taken center stage in all aspects of computing, regardless of whether it is performed on a portable battery-powered device, a desktop PC, on servers in a data center, or on a supercomputer. It is expressed as performance-per-watt (PPW), which is equal to the number of instructions that are executed per Joule of energy. The shift to multicore processors, with tens or hundreds of cores on a single die requires that the operation of the cores be dynamically controlled to maximize the processor's overall energy efficiency. This paper presents a unified formulation and an efficient solution for this problem. The solution considers dynamic frequency and voltage scaling, thread migration, and active cooling as the means to control the cores. The solution method is efficient for a real-time implementation. The formulation includes accurate power and thermal models, temperature constraints, and accounts for the dependence of leakage power and circuit delay on temperature. The PPW metric is extended to $(P^{\alpha }PW)$ (performance$(^\alpha) $-per-watt), which allows examining the tradeoffs between optimizing for performance versus optimizing for energy by varying $(\alpha)$. Simulation experiments assuming a four-core processor demonstrate that the derived control strategy can achieve 3.2× greater energy efficiency (i.e., executes more than three times the number of instructions per Joule) over the performance-optimal solution. The formulation and the efficiency of the solution method also allows for fast design space exploration. Specifically, it is shown how simply increasing the number of cores in a processor can significantly diminish its energy efficiency, and that there is an optimal number of cores that maximize the PPW. This number depends on the ratio of how much the power of an individual core is reduced by scaling, i.e., as the number of cores are increased. Finally, the proposed method is implemented on a quad-core Intel Sandy Bridge processor, and verified by running benchmarks. The experiments suggest that the proposed method results in an improvement of 37 percent over the current state-of-the-art energy-efficient schemes. © 2014 IEEE.
DA  - 2014///
PY  - 2014
DO  - 10.1109/TC.2012.213
VL  - 63
IS  - 2
SP  - 349
EP  - 360
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892391479&doi=10.1109%2fTC.2012.213&partnerID=40&md5=da0387e1b3aafbe6ffadf81a1306808d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 105</p>
KW  - active cooling
KW  - Active cooling
KW  - closed-loop control
KW  - Closed-loop control
KW  - Cooling
KW  - dynamic voltage and frequency scaling
KW  - Dynamic voltage and frequency scaling
KW  - energy efficiency
KW  - Energy efficiency
KW  - Experiments
KW  - Leakage currents
KW  - Leakage power
KW  - leakage power dependence on temperature
KW  - Multi core
KW  - Multicore
KW  - optimization
KW  - Optimization
KW  - performance/Watt
KW  - Real time control
KW  - Supercomputers
KW  - task migration
KW  - Task migration
KW  - Temperature control
KW  - thermal management
ER  - 

TY  - JOUR
TI  - Revenue Maximization with Optimal Capacity Control in Infrastructure as a Service Cloud Markets
AU  - Toosi, A.N.
AU  - Vanmechelen, K.
AU  - Ramamohanarao, K.
AU  - Buyya, R.
T2  - IEEE Transactions on Cloud Computing
AB  - Infrastructure-as-a-Service cloud providers offer diverse purchasing options and pricing plans, namely on-demand, reservation, and spot market plans. This allows them to efficiently target a variety of customer groups with distinct preferences and to generate more revenue accordingly. An important consequence of this diversification however, is that it introduces a non-trivial optimization problem related to the allocation of the provider's available data center capacity to each pricing plan. The complexity of the problem follows from the different levels of revenue generated per unit of capacity sold, and the different commitments consumers and providers make when resources are allocated under a given plan. In this work, we address a novel problem of maximizing revenue through an optimization of capacity allocation to each pricing plan by means of admission control for reservation contracts, in a setting where aforementioned plans are jointly offered to customers. We devise both an optimal algorithm based on a stochastic dynamic programming formulation and two heuristics that trade-off optimality and computational complexity. Our evaluation, which relies on an adaptation of a large-scale real-world workload trace of Google, shows that our algorithms can significantly increase revenue compared to an allocation without capacity control given that sufficient resource contention is present in the system. In addition, we show that our heuristics effectively allow for online decision making and quantify the revenue loss caused by the assumptions made to render the optimization problem tractable. © 2013 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/TCC.2014.2382119
VL  - 3
IS  - 3
SP  - 261
EP  - 274
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941000043&doi=10.1109%2fTCC.2014.2382119&partnerID=40&md5=acd8b114b2216c5cf24465dc742d5a09
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 51</p>
KW  - Capacity allocation
KW  - Costs
KW  - Decision making
KW  - Dynamic programming
KW  - Economic and social effects
KW  - Economics
KW  - Infrastructure as a service (IaaS)
KW  - On-line decision makings
KW  - Optimal algorithm
KW  - Optimal capacity
KW  - Optimization problems
KW  - Resource contention
KW  - Revenue maximization
KW  - Sales
KW  - Stochastic dynamic programming
KW  - Stochastic systems
ER  - 

TY  - CONF
TI  - Control-theoretical load-balancing for cloud applications with brownout
AU  - Durango, J.
AU  - Dellkrantz, M.
AU  - Maggio, M.
AU  - Klein, C.
AU  - Papadopoulos, A.V.
AU  - Hernandez-Rodriguez, F.
AU  - Elmroth, E.
AU  - Arzen, K.-E.
T2  - Proceedings of the IEEE Conference on Decision and Control
AB  - Cloud applications are often subject to unexpected events like flash crowds and hardware failures. Without a predictable behaviour, users may abandon an unresponsive application. This problem has been partially solved on two separate fronts: first, by adding a self-adaptive feature called brownout inside cloud applications to bound response times by modulating user experience, and, second, by introducing replicas - copies of the applications having the same function-alities - for redundancy and adding a load-balancer to direct incoming traffic. © 2014 IEEE.
DA  - 2014///
PY  - 2014
DO  - 10.1109/CDC.2014.7040221
VL  - 2015-February
SP  - 5320
EP  - 5327
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931846547&doi=10.1109%2fCDC.2014.7040221&partnerID=40&md5=9bb0f0ace051d9fcae0a26db67ce43ea
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 21</p>
KW  - Cloud applications
KW  - Control engineering
KW  - Flash crowd
KW  - Hardware failures
KW  - Incoming traffic
KW  - Load balancer
KW  - Self-Adaptive
KW  - Unexpected events
KW  - User experience
ER  - 

TY  - JOUR
TI  - Power-aware provisioning of virtual machines for real-time Cloud services
AU  - Kim, K.H.
AU  - Beloglazov, A.
AU  - Buyya, R.
T2  - Concurrency and Computation: Practice and Experience
AB  - Reducing power consumption has been an essential requirement for Cloud resource providers not only to decrease operating costs, but also to improve the system reliability. As Cloud computing becomes emergent for the Anything as a Service (XaaS) paradigm, modern real-time services also become available through Cloud computing. In this work, we investigate power-aware provisioning of virtual machines for real-time services. Our approach is (i) to model a real-time service as a real-time virtual machine request; and (ii) to provision virtual machines in Cloud data centers using dynamic voltage frequency scaling schemes. We propose several schemes to reduce power consumption by hard real-time services and power-aware profitable provisioning of soft real-time services. © 2011 John Wiley & Sons, Ltd.
DA  - 2011///
PY  - 2011
DO  - 10.1002/cpe.1712
VL  - 23
IS  - 13
SP  - 1491
EP  - 1505
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051798127&doi=10.1002%2fcpe.1712&partnerID=40&md5=99b529f15e78a203deecd0580efda5df
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 141</p>
KW  - cloud computing
KW  - Cloud computing
KW  - Cloud data centers
KW  - Dynamic frequency scaling
KW  - Dynamic voltage frequency scaling
KW  - Electric power utilization
KW  - Energy efficiency
KW  - Energy efficient computing
KW  - energy-efficient computing
KW  - Green computing
KW  - green data centers
KW  - Green data centers
KW  - Hard real-time
KW  - Information services
KW  - Network security
KW  - Operating costs
KW  - Real time service
KW  - Real time systems
KW  - real-time services
KW  - Resource providers
KW  - System reliability
KW  - Virtual machine
KW  - Voltage scaling
KW  - Web services
ER  - 

TY  - JOUR
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034027192&partnerID=40&md5=360bff5b61c56cc45665f535d2fc3b94
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Managing overloaded hosts for dynamic consolidation of virtual machines in cloud data centers under quality of service constraints
AU  - Beloglazov, A.
AU  - Buyya, R.
T2  - IEEE Transactions on Parallel and Distributed Systems
AB  - Dynamic consolidation of virtual machines (VMs) is an effective way to improve the utilization of resources and energy efficiency in cloud data centers. Determining when it is best to reallocate VMs from an overloaded host is an aspect of dynamic VM consolidation that directly influences the resource utilization and quality of service (QoS) delivered by the system. The influence on the QoS is explained by the fact that server overloads cause resource shortages and performance degradation of applications. Current solutions to the problem of host overload detection are generally heuristic based, or rely on statistical analysis of historical data. The limitations of these approaches are that they lead to suboptimal results and do not allow explicit specification of a QoS goal. We propose a novel approach that for any known stationary workload and a given state configuration optimally solves the problem of host overload detection by maximizing the mean intermigration time under the specified QoS goal based on a Markov chain model. We heuristically adapt the algorithm to handle unknown nonstationary workloads using the Multisize Sliding Window workload estimation technique. Through simulations with workload traces from more than a thousand PlanetLab VMs, we show that our approach outperforms the best benchmark algorithm and provides approximately 88 percent of the performance of the optimal offline algorithm. © 1990-2012 IEEE.
DA  - 2013///
PY  - 2013
DO  - 10.1109/TPDS.2012.240
VL  - 24
IS  - 7
SP  - 1366
EP  - 1379
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878476684&doi=10.1109%2fTPDS.2012.240&partnerID=40&md5=01be4a531f0874fe562725f0dc176ae7
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 447</p>
KW  - Algorithms
KW  - Benchmarking
KW  - cloud computing
KW  - Cloud computing
KW  - Computer simulation
KW  - Distributed systems
KW  - dynamic consolidation
KW  - Dynamic consolidation
KW  - energy efficiency
KW  - Energy efficiency
KW  - host overload detection
KW  - Overload detection
KW  - Performance degradation
KW  - Quality of service
KW  - Quality of Service constraints
KW  - Resource utilizations
KW  - Utilization of resources
KW  - virtualization
KW  - Virtualizations
ER  - 

TY  - JOUR
TI  - Global Warming: Data Centres to Consume Three Times as Much Energy in Next Decade, Experts Warn. Independent
AU  - Bawden, T.
T2  - Global Warming: Data Centres to Consume Three Times As Much Energy in Next Decade, Experts Warn
DA  - 2016///
PY  - 2016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027445487&partnerID=40&md5=123e9283abd4476f899e2d3f932614ad
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 41</p>
ER  - 

TY  - JOUR
TI  - CloudSim: A toolkit for modeling and simulation of cloud computing environments and evaluation of resource provisioning algorithms
AU  - Calheiros, R.N.
AU  - Ranjan, R.
AU  - Beloglazov, A.
AU  - De Rose, C.A.F.
AU  - Buyya, R.
T2  - Software - Practice and Experience
AB  - Cloud computing is a recent advancement wherein IT infrastructure and applications are provided as 'services' to end-users under a usage-based payment model. It can leverage virtualized services even on the fly based on requirements (workload patterns and QoS) varying with time. The application services hosted under Cloud computing model have complex provisioning, composition, configuration, and deployment requirements. Evaluating the performance of Cloud provisioning policies, application workload models, and resources performance models in a repeatable manner under varying system and user configurations and requirements is difficult to achieve. To overcome this challenge, we propose CloudSim: an extensible simulation toolkit that enables modeling and simulation of Cloud computing systems and application provisioning environments. The CloudSim toolkit supports both system and behavior modeling of Cloud system components such as data centers, virtual machines (VMs) and resource provisioning policies. It implements generic application provisioning techniques that can be extended with ease and limited effort. Currently, it supports modeling and simulation of Cloud computing environments consisting of both single and inter-networked clouds (federation of clouds). Moreover, it exposes custom interfaces for implementing policies and provisioning techniques for allocation of VMs under inter-networked Cloud computing scenarios. Several researchers from organizations, such as HP Labs in U.S.A., are using CloudSim in their investigation on Cloud resource provisioning and energy-efficient management of data center resources. The usefulness of CloudSim is demonstrated by a case study involving dynamic provisioning of application services in the hybrid federated clouds environment. The result of this case study proves that the federated Cloud computing model significantly improves the application QoS requirements under fluctuating resource and service demand patterns. Copyright © 2010 John Wiley & Sons, Ltd.
DA  - 2011///
PY  - 2011
DO  - 10.1002/spe.995
VL  - 41
IS  - 1
SP  - 23
EP  - 50
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650777991&doi=10.1002%2fspe.995&partnerID=40&md5=e5acc8ba81ccb3bce707222348c8d53b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 4377</p>
KW  - application scheduling
KW  - Application scheduling
KW  - Cloud computing
KW  - Clouds
KW  - Computer simulation
KW  - Distributed computer systems
KW  - Mathematical models
KW  - modelling and simulation
KW  - Modelling and simulations
KW  - Natural resources management
KW  - performance evaluation
KW  - Performance evaluation
KW  - Quality of service
KW  - Research
KW  - Resource allocation
KW  - resource management
KW  - Resource management
KW  - Satellite communication systems
ER  - 

TY  - JOUR
TI  - Energy Efficient Scheduling of Cloud Application Components with Brownout
AU  - Xu, M.
AU  - Dastjerdi, A.V.
AU  - Buyya, R.
T2  - IEEE Transactions on Sustainable Computing
AB  - It is common for cloud data centers meeting unexpected loads like request bursts, which may lead to overloaded situation and performance degradation. Dynamic Voltage Frequency Scaling and VM consolidation have been proved effective to manage overloads. However, they cannot function when the whole data center is overloaded. Brownout provides a promising direction to avoid overloads through configuring applications to temporarily degrade user experience. Additionally, brownout can also be applied to reduce data center energy consumption. As a complementary option for Dynamic Voltage Frequency Scaling and VM consolidation, our combined brownout approach reduces energy consumption through selectively and dynamically deactivating application optional components, which can also be applied to self-contained microservices. The results show that our approach can save more than 20 percent energy consumption and there are trade-offs between energy saving and discount offered to users. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/TSUSC.2017.2661339
VL  - 1
IS  - 2
SP  - 40
EP  - 43
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081713746&doi=10.1109%2fTSUSC.2017.2661339&partnerID=40&md5=e00446747da4f43b0f291cc8f5ee0154
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 31</p>
KW  - application component
KW  - Application components
KW  - brownout
KW  - Cloud data centers
KW  - Dynamic frequency scaling
KW  - Economic and social effects
KW  - Energy efficiency
KW  - energy efficient
KW  - Energy efficient
KW  - Energy utilization
KW  - Green computing
KW  - microservices
KW  - Voltage scaling
ER  - 

TY  - JOUR
TI  - VM consolidation: A real case based on OpenStack Cloud
AU  - Corradi, A.
AU  - Fanelli, M.
AU  - Foschini, L.
T2  - Future Generation Computer Systems
AB  - In recent years, Cloud computing has been emerging as the next big revolution in both computer networks and Web provisioning. Because of raised expectations, several vendors, such as Amazon and IBM, started designing, developing, and deploying Cloud solutions to optimize the usage of their own data centers, and some open-source solutions are also underway, such as Eucalyptus and OpenStack. Cloud architectures exploit virtualization techniques to provision multiple Virtual Machines (VMs) on the same physical host, so as to efficiently use available resources, for instance, to consolidate VMs in the minimal number of physical servers to reduce the runtime power consumption. VM consolidation has to carefully consider the aggregated resource consumption of co-located VMs, in order to avoid performance reductions and Service Level Agreement (SLA) violations. While various works have already treated the VM consolidation problem from a theoretical perspective, this paper focuses on it from a more practical viewpoint, with specific attention on the consolidation aspects related to power, CPU, and networking resource sharing. Moreover, the paper proposes a Cloud management platform to optimize VM consolidation along three main dimensions, namely power consumption, host resources, and networking. Reported experimental results point out that interferences between co-located VMs have to be carefully considered to avoid placement solutions that, although being feasible from a more theoretical viewpoint, cannot ensure VM provisioning with SLA guarantees.
DA  - 2014///
PY  - 2014
DO  - 10.1016/j.future.2012.05.012
VL  - 32
IS  - 1
SP  - 118
EP  - 127
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891606241&doi=10.1016%2fj.future.2012.05.012&partnerID=40&md5=e3fd3a653ead49786de3f27b61d334e1
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 202</p>
KW  - Cloud architectures
KW  - Cloud computing
KW  - Cloud managements
KW  - Open-source solutions
KW  - OpenStack
KW  - Optimization
KW  - Resource consumption
KW  - Resource sharing
KW  - Service Level Agreements
KW  - Virtualization Techniques
KW  - VM consolidation
ER  - 

TY  - JOUR
TI  - Data center efficiency assessment-scaling up energy efficiency across the data center industry: Evaluating key drivers and barriers
AU  - Whitney, J.
AU  - Delforge, P.
T2  - Data Center Efficiency Assessment-Scaling Up Energy Efficiency Across the Data Center Industry: Evaluating Key Drivers and Barriers
DA  - 2014///
PY  - 2014
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957375958&partnerID=40&md5=74d0323715ac988c9cdcb79ae0c9b082
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 182</p>
ER  - 

TY  - JOUR
TI  - CoMon: A mostly-scalable monitoring system for PlanetLab
AU  - Park, K.
AU  - Pai, V.S.
T2  - Operating Systems Review (ACM)
AB  - CoMon is an evolving, mostly-scalable monitoring system for PlanetLab that has the goal of presenting environment-tailored information for both the administrators and users of the PlanetLab global testbed. In addition to passively reporting metrics provided by the operating system, CoMon also actively gathers a number of metrics useful for developers of networked systems. Using CoMon, PlanetLab administrators and users can easily spot problematic machines, where the problem may arise from the machine itself, local configuration/environment problems, or the workload running on the machine. Furthermore, users can easily observe many properties of all of the experiments running across multiple PlanetLab nodes, facilitating not only their own experiment monitoring and debugging, but also helping scale the task of finding PlanetLab problems. In this paper we describe CoMon's design and operation, including what kinds of data are gathered, the scale of the processing involved, and the approaches we have taken to keep CoMon running. Our goal is not only to illustrate the kinds of problems faced in this environment, but also to invite others to participate, either by experimenting with the data generated by CoMon, or by building on the CoMon system itself.
DA  - 2006///
PY  - 2006
DO  - 10.1145/1113361.1113374
VL  - 40
IS  - 1
SP  - 65
EP  - 74
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845223204&doi=10.1145%2f1113361.1113374&partnerID=40&md5=109f1b187c53974942bb2a417154a8e0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 509</p>
KW  - CoMon systems
KW  - Computer networks
KW  - Computer operating systems
KW  - Data acquisition
KW  - Information services
KW  - Man machine systems
KW  - Networked systems
KW  - PlanetLab nodes
KW  - Problem solving
KW  - Program debugging
KW  - Testbeds
KW  - User interfaces
ER  - 

TY  - CONF
TI  - Utilization-based VM consolidation scheme for power efficiency in cloud data centers
AU  - Chen, Q.
AU  - Chen, J.
AU  - Zheng, B.
AU  - Cui, J.
AU  - Qian, Y.
T2  - 2015 IEEE International Conference on Communication Workshop, ICCW 2015
AB  - Cloud computing offers utility-oriented services to users, which is supported by large-scale data center. Although virtualized data centers provide high performance computing service, they also consume enormous amount of power. To solve the problem, dynamic consolidation of Virtual Machines (VMs) is considered as an efficient way to reduce power consumption and guarantee Quality of Service (QoS). Live migration is applied into the dynamic consolidation, which allows VMs to be migrated to other hosts and aims to minimize the number of hosts in data centers. However, the migration overhead is essential to be taken into account and massive migrations will lead to performance degradation and extra power consumption. In this paper, we propose a utilization-based migration algorithm (UMA) to migrate VMs to stable hosts, which efficiently reduces migration time and power consumption. Experiment results show that our UMA can reduce about 77.5%-82.4% migrations and save up to 39.3% -42.2% power consumption compared with the MinPower policy. © 2015 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/ICCW.2015.7247462
SP  - 1928
EP  - 1933
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947776140&doi=10.1109%2fICCW.2015.7247462&partnerID=40&md5=c79891a7cdb7bf5e860a097a90b342c3
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 15</p>
KW  - Cloud data centers
KW  - Dynamic consolidation
KW  - Electric power utilization
KW  - Energy efficiency
KW  - High performance computing
KW  - Large scale data
KW  - Migration algorithms
KW  - Performance degradation
KW  - Power efficiency
KW  - Quality of service
KW  - Virtual machine
KW  - Virtualized data centers
ER  - 

TY  - JOUR
TI  - Energy-aware resource allocation heuristics for efficient management of data centers for Cloud computing
AU  - Beloglazov, A.
AU  - Abawajy, J.
AU  - Buyya, R.
T2  - Future Generation Computer Systems
AB  - Cloud computing offers utility-oriented IT services to users worldwide. Based on a pay-as-you-go model, it enables hosting of pervasive applications from consumer, scientific, and business domains. However, data centers hosting Cloud applications consume huge amounts of electrical energy, contributing to high operational costs and carbon footprints to the environment. Therefore, we need Green Cloud computing solutions that can not only minimize operational costs but also reduce the environmental impact. In this paper, we define an architectural framework and principles for energy-efficient Cloud computing. Based on this architecture, we present our vision, open research challenges, and resource provisioning and allocation algorithms for energy-efficient management of Cloud computing environments. The proposed energy-aware allocation heuristics provision data center resources to client applications in a way that improves energy efficiency of the data center, while delivering the negotiated Quality of Service (QoS). In particular, in this paper we conduct a survey of research in energy-efficient computing and propose: (a) architectural principles for energy-efficient management of Clouds; (b) energy-efficient resource allocation policies and scheduling algorithms considering QoS expectations and power usage characteristics of the devices; and (c) a number of open research challenges, addressing which can bring substantial benefits to both resource providers and consumers. We have validated our approach by conducting a performance evaluation study using the CloudSim toolkit. The results demonstrate that Cloud computing model has immense potential as it offers significant cost savings and demonstrates high potential for the improvement of energy efficiency under dynamic workload scenarios. © 2011 Elsevier B.V. All rights reserved.
DA  - 2012///
PY  - 2012
DO  - 10.1016/j.future.2011.04.017
VL  - 28
IS  - 5
SP  - 755
EP  - 768
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857370722&doi=10.1016%2fj.future.2011.04.017&partnerID=40&md5=404d8a6f96e7208ce49f7fb3ffc699cd
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2402</p>
KW  - Allocation algorithm
KW  - Architectural frameworks
KW  - Architectural principles
KW  - Business domain
KW  - Carbon footprint
KW  - Client applications
KW  - Cloud computing
KW  - Computer systems
KW  - Computing environments
KW  - Computing solutions
KW  - Cost saving
KW  - Data centers
KW  - Dynamic consolidation
KW  - Electrical energy
KW  - Energy aware
KW  - Energy efficiency
KW  - Energy efficient
KW  - Energy-efficient resource allocation
KW  - Environmental impact
KW  - Green IT
KW  - High potential
KW  - IT services
KW  - Operational costs
KW  - Pay-as-you-go
KW  - Performance evaluation
KW  - Pervasive applications
KW  - Power usage
KW  - Quality of service
KW  - Research
KW  - Research challenges
KW  - Resource allocation
KW  - Resource management
KW  - Resource providers
KW  - Resource provisioning
KW  - Virtualization
KW  - Virtualizations
ER  - 

TY  - CONF
TI  - Joint power optimization of data center network and servers with correlation analysis
AU  - Zheng, K.
AU  - Wang, X.
AU  - Li, L.
AU  - Wang, X.
T2  - Proceedings - IEEE INFOCOM
AB  - Data center power optimization has recently received a great deal of research attention. For example, server consolidation has been demonstrated as one of the most effective energy saving methodologies. Likewise, traffic consolidation has also been recently proposed to save energy for data center networks (DCNs). However, current research on data center power optimization focuses on servers and DCN separately. As a result, the optimization results are often inferior, because server consolidation without considering the DCN may cause traffic congestion and thus degraded network performance. On the other hand, server consolidation may change the DCN topology, allowing new opportunities for energy savings. In this paper, we propose PowerNetS, a power optimization strategy that leverages workload correlation analysis to jointly minimize the total power consumption of servers and the DCN. The design of PowerNetS is based on the key observations that the workloads of different servers and DCN traffic flows do not peak at exactly the same time. Thus, more energy savings can be achieved if the workload correlations are considered in server and traffic consolidations. In addition, PowerNetS considers the DCN topology during server consolidation, which leads to less inter-server traffic and thus more energy savings and shorter network delays. We implement PowerNetS on a hardware testbed composed of 10 virtual switches configured with a production 48-port OpenFlow switch and 6 servers. Our empirical results with Wikipedia, Yahoo!, and IBM traces demonstrate that PowerNetS can save up to 51.6% of energy for a data center. PowerNetS also outperforms two state-of-the-art baselines by 44.3% and 15.8% on energy savings, respectively. Our simulation results with 72 switches and 122 servers also show the superior energy efficiency of PowerNetS over the baselines. © 2014 IEEE.
DA  - 2014///
PY  - 2014
DO  - 10.1109/INFOCOM.2014.6848207
SP  - 2598
EP  - 2606
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904410981&doi=10.1109%2fINFOCOM.2014.6848207&partnerID=40&md5=ec3b4daf09fd8b2c667563df1cd786e4
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 118</p>
KW  - Correlation analysis
KW  - Correlation methods
KW  - Data center networks
KW  - Data center networks (DCNs)
KW  - Energy efficiency
KW  - Hardware testbeds
KW  - Power Optimization
KW  - Server consolidation
KW  - Topology
KW  - Total power consumption
KW  - Traffic congestion
KW  - Workload correlations
ER  - 

TY  - JOUR
TI  - Dynamic programming and Lagrange multipliers
AU  - Bellman, R.
T2  - Proceedings of the National Academy of Sciences of the United States of America
DA  - 1956///
PY  - 1956
VL  - 42
IS  - 10
SP  - 767
EP  - 769
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-47149096461&partnerID=40&md5=123363222e36f8285ea8129a523035a5
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 333</p>
ER  - 

TY  - CONF
TI  - Dynamic virtual machine management via approximate Markov decision process
AU  - Han, Z.
AU  - Tan, H.
AU  - Chen, G.
AU  - Wang, R.
AU  - Chen, Y.
AU  - Lau, F.C.M.
T2  - Proceedings - IEEE INFOCOM
AB  - Efficient virtual machine (VM) management can dramatically reduce energy consumption in data centers. Existing VM management algorithms fall into two categories based on whether the VMs' resource demands are assumed to be static or dynamic. The former category fails to maximize the resource utilization as they cannot adapt to the dynamic nature of VMs' resource demands. Most approaches in the latter category are heuristical and lack theoretical performance guarantees. In this work, we formulate dynamic VM management as a large-scale Markov Decision Process (MDP) problem and derive an optimal solution. Our analysis of real-world data traces supports our choice of the modeling approach. However, solving the large-scale MDP problem suffers from the curse of dimensionality. Therefore, we further exploit the special structure of the problem and propose an approximate MDP-based dynamic VM management method, called MadVM. We prove the convergence of MadVM and analyze the bound of its approximation error. Moreover, MadVM can be implemented in a distributed system, which should suit the needs of real data centers. Extensive simulations based on two real-world workload traces show that MadVM achieves significant performance gains over two existing baseline approaches in power consumption, resource shortage and the number of VM migrations. Specifically, the more intensely the resource demands fluctuate, the more MadVM outperforms. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/INFOCOM.2016.7524384
VL  - 2016-July
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983261312&doi=10.1109%2fINFOCOM.2016.7524384&partnerID=40&md5=ad72f6d581877dfd92d061df60b3b28e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 72</p>
KW  - Approximation errors
KW  - Curse of dimensionality
KW  - Decision making
KW  - Energy utilization
KW  - Extensive simulations
KW  - Markov Decision Processes
KW  - Markov processes
KW  - Reduce energy consumption
KW  - Resource utilizations
KW  - Telecommunication networks
KW  - Theoretical performance
KW  - Virtual machine management
ER  - 

TY  - CONF
TI  - Towards an Approach of Risk Analysis in Access Control
AU  - Jayabalan, M.
T2  - Proceedings - International Conference on Developments in eSystems Engineering, DeSE
AB  - Information security provides a set of mechanisms to be implemented in the organisation to protect the disclosure of data to the unauthorised person. Access control is the primary security component that allows the user to authorise the consumption of resources and data based on the predefined permissions. However, the access rules are static in nature, which does not adapt to the dynamic environment includes but not limited to healthcare, cloud computing, IoT, National Security and Intelligence Arena and multi-centric system. There is a need for an additional countermeasure in access decision that can adapt to those working conditions to assess the threats and to ensure privacy and security are maintained. Risk analysis is an act of measuring the threats to the system through various means such as, analysing the user behaviour, evaluating the user trust, and security policies. It is a modular component that can be integrated into the existing access control to predict the risk. This study presents the different techniques and approaches applied for risk analysis in access control. Based on the insights gained, this paper formulates the taxonomy of risk analysis and properties that will allow researchers to focus on areas that need to be improved and new features that could be beneficial to stakeholders.  © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/DeSE51703.2020.9450772
VL  - 2020-December
SP  - 287
EP  - 292
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112559026&doi=10.1109%2fDeSE51703.2020.9450772&partnerID=40&md5=1722a16f1a6214f59f2398b74ca4898f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 5</p>
KW  - Access control
KW  - Access decision
KW  - Access rules
KW  - Behavioral research
KW  - Dynamic environments
KW  - Information security
KW  - Modular components
KW  - National security
KW  - Privacy
KW  - Privacy and security
KW  - Privacy by design
KW  - Risk analysis
KW  - Risk assessment
KW  - security
KW  - Security components
KW  - Security policy
KW  - User behaviour
ER  - 

TY  - JOUR
TI  - Trust-Based Optimized Reporting for Detection and Prevention of Black Hole Attacks in Low-Power and Lossy Green IoT Networks
AU  - Khan, M.A.
AU  - Rais, R.N.B.
AU  - Khalid, O.
AU  - Ahmad, S.
T2  - Sensors
AB  - The Internet of Things (IoT) is empowering various sectors and aspects of daily life. Green IoT systems typically involve Low-Power and Lossy Networks (LLNs) with resource-constrained nodes. Lightweight routing protocols, such as the Routing Protocol for Low-Power and Lossy Networks (RPL), are increasingly being applied for efficient communication in LLNs. However, RPL is susceptible to various attacks, such as the black hole attack, which compromises network security. The existing black hole attack detection methods in Green IoT rely on static thresholds and unreliable metrics to compute trust scores. This results in increasing false positive rates, especially in resource-constrained IoT environments. To overcome these limitations, we propose a delta-threshold-based trust model called the Optimized Reporting Module (ORM) to mitigate black hole attacks in Green IoT systems. The proposed scheme comprises both direct trust and indirect trust and utilizes a forgetting curve. Direct trust is derived from performance metrics, including honesty, dishonesty, energy, and unselfishness. Indirect trust requires the use of similarity. The forgetting curve provides a mechanism to consider the most significant and recent feedback from direct and indirect trust. To assess the efficacy of the proposed scheme, we compare it with the well-known trust-based attack detection scheme. Simulation results demonstrate that the proposed scheme has a higher detection rate and low false positive alarms compared to the existing scheme, confirming the applicability of the proposed scheme in green IoT systems. © 2024 by the authors.
DA  - 2024///
PY  - 2024
DO  - 10.3390/s24061775
VL  - 24
IS  - 6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189028251&doi=10.3390%2fs24061775&partnerID=40&md5=37dbbd03feff9d8474a45707bf7fd402
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 4</p>
KW  - article
KW  - benchmarking
KW  - black hole
KW  - black hole attack
KW  - Black hole attack
KW  - diagnosis
KW  - Direct trusts
KW  - Gravitation
KW  - Green internet of thing
KW  - green internet of things
KW  - Green internets
KW  - honesty
KW  - human
KW  - Indirect trusts
KW  - internet of things
KW  - Internet of things
KW  - Lossy networks
KW  - Low power electronics
KW  - Low Power Networks
KW  - Low-power and lossy network
KW  - low-power and lossy networks (LLNs)
KW  - network security
KW  - Network security
KW  - prevention
KW  - Routing protocols
KW  - RPL
KW  - simulation
KW  - Stars
KW  - trust-based
KW  - Trust-based
ER  - 

TY  - JOUR
TI  - A Machine Learning approach for anomaly detection on the Internet of Things based on Locality-Sensitive Hashing
AU  - Hernandez-Jaimes, M.L.
AU  - Martinez-Cruz, A.
AU  - Ramírez-Gutiérrez, K.A.
T2  - Integration
AB  - The increasing connectivity of devices on the Internet of Things (IoT) has created a favorable field for attacks. Consequently, current anomaly-based intrusion detection systems (AIDS) integrate artificial intelligence algorithms, such as machine learning (ML) and deep learning (DL), to manage high data volumes, recognize complex patterns, and detect unknown anomalies. However, the effectiveness of these methods is contingent upon the quality and meaningfulness of the extracted features from IoT-based communications. Also, with the growth of the IoT, feature extraction and selection are becoming increasingly difficult due to data heterogeneity, the generation of massive amounts of information, and the lack of feature standardization. Moreover, current proposals rely on complex feature extraction and selection techniques. As a result, this study introduces a novel approach for ML modeling, including decision trees and random forests, to detect anomalies in IoT. This study aims to overcome feature extraction and selection process dependency by integrating fingerprinting techniques based on locality-sensitive hashing (LSH) to represent network packet information in a suitable format for ML modeling and detecting harmful sequential network packets. The anomaly detection performance was assessed using two benchmark IoT datasets, ToN-IoT and MQTT-IoT, which contain cyberattacks threatening IoT networks. The proposal outperforms other methods regarding accuracy, precision, and FPR with values of 99.82%, 99.93%, and 0.13%, respectively. © 2024 Elsevier B.V.
DA  - 2024///
PY  - 2024
DO  - 10.1016/j.vlsi.2024.102159
VL  - 96
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184140607&doi=10.1016%2fj.vlsi.2024.102159&partnerID=40&md5=3cdf9838bd1d08fa0e7aa80a18743f75
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 13</p>
KW  - 'current
KW  - Anomaly based intrusion detection systems
KW  - Anomaly detection
KW  - Artificial intelligence
KW  - Benchmarking
KW  - Complex networks
KW  - Decision trees
KW  - Deep learning
KW  - Extraction
KW  - Feature extraction
KW  - Feature extraction and selection
KW  - Information management
KW  - Internet of things
KW  - Internet of Things
KW  - Intrusion detection
KW  - Intrusion-Detection
KW  - Learning systems
KW  - Locality sensitive hashing
KW  - Locality-sensitive hashing
KW  - Machine learning
KW  - Machine learning approaches
KW  - Machine learning models
KW  - Machine-learning
KW  - Network packets
ER  - 

TY  - JOUR
TI  - Extending SysML to Integrate Cost Analysis into Model-Based Systems Engineering
AU  - Kotronis, C.
AU  - Nikolaidou, M.
AU  - Tsadimas, A.
AU  - Michalakelis, C.
AU  - Anagnostopoulos, D.
T2  - IEEE Transactions on Engineering Management
AB  - Model-based systems design (MBSD), a current trend adopted by INCOSE, employs the systems modeling language (SysML), a standard introduced by OMG and INCOSE. Though there are numerous works on integrating performance exploitation in SysML, cost is not sufficiently explored as a driving design parameter. By integrating cost analysis in a popular modeling language like SysML, the proposed approach may be applied to any system designed using standardized languages and tools. In this work, we integrate cost analysis within SysML models at a generic level to explore design alternatives under specific cost and performance restrictions and perform tradeoff analysis. The proposed SysML extensions provide: 1) cost-related entities to encode cost aspects, such as capital and operating expenses, and 2) functions that enable the automatic computation of costs; these extensions are contained in a custom SysML cost profile. The feasibility and benefits of the approach are explored in two distinct real-world case studies with different purpose and characteristics. 1) Configuring a remote elderly monitoring system, taking into consideration patients' budgetary and operational concerns regarding the equipment installed in their homes. In this case, patients had the opportunity to evaluate and prioritize their concerns prior to using the system. 2) Exploring the improvement of the passengers' comfort as a level of service indicator in the Athens Metro railway system, taking into account operational cost constraints. In this case, the operator obtained forecasts of performance and cost of the metro system operation in order to choose between different operational policies.  © 1988-2012 IEEE.
DA  - 2024///
PY  - 2024
DO  - 10.1109/TEM.2022.3200148
VL  - 71
SP  - 2865
EP  - 2880
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137553281&doi=10.1109%2fTEM.2022.3200148&partnerID=40&md5=34ce187b42416a10c1fd5ca2dc59a3bc
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 6</p>
KW  - Budget control
KW  - Capital expenditure
KW  - Capital expenditures
KW  - Capital expenditures (CapEx)
KW  - cost analysis
KW  - Cost analysis
KW  - Cost benefit analysis
KW  - Cost engineering
KW  - Cost-benefits analysis
KW  - Elderly monitoring
KW  - Model-based system engineering
KW  - model-based system engineering (MBSD)
KW  - Model-based system engineerings
KW  - Modeling
KW  - Monitoring
KW  - Object oriented modelling
KW  - Operating expense
KW  - operating expenses (OpEx)
KW  - Railroads
KW  - Railway transportation
KW  - railway transportation system
KW  - Railway transportation system
KW  - remote elderly monitoring
KW  - Remote elderly monitoring
KW  - System analysis and design
KW  - System modeling language
KW  - System models
KW  - Systems analysis
KW  - Systems engineering
KW  - systems modeling language (SysML)
KW  - Transportation system
KW  - Unified Modeling Language
ER  - 

TY  - CONF
TI  - Defense systems and IoT: Security issues in an era of distributed command and control
AU  - Palmer, D.
AU  - Fazzari, S.
AU  - Wartenberg, S.
T2  - Proceedings of the ACM Great Lakes Symposium on VLSI, GLSVLSI
AB  - Welcome to the 26th edition of the Great Lakes Symposium on VLSI (GLSVLSI) 2016, held at Boston University Department of Electrical and Computer Engineering in Boston, MA, USA. GLSVLSI is a premier venue for the dissemination of manuscripts of the highest quality in all areas related to VLSI, devices, and system-level design. The location of this year's GLSVLSI is Boston, which continues GLSVLSI's tradition of organizing meetings near noted bodies of water. While not a "great lake", the Charles River flows across the Greater Boston area, separating the city of Boston from Cambridge. The basin of the Charles River is located in Hopkinton, Massachusetts, and serves as the starting point of the Boston Marathon. While the marathoners run 26 miles to the city, the Charles River twists and turns for 80 miles before ending its journey in Boston Harbor, where it meets the Atlantic Ocean. The Greater Boston area, consisting of Boston and its surrounding neighborhoods, is the 10th largest metropolitan area of the US. Boston is one of the oldest cities in the US, and was the first city to open a public school and a transportation system in the country. Boston is truly an intellectual, technological, and political hub. It is home to around 50 universities and colleges, making it one of the most prominent centers of higher education and medicine internationally. Boston is experiencing substantial industrial growth, particularly in the fields of computer and electrical engineering. Each year, millions of tourists visit Boston to explore the city's historic landmarks, to include Faneuil Hall, Fenway Park (home of the Boston Red Sox), Boston Common, and Harvard Square. Boston University, located along the Charles River, is in the heart of the city with easy access to all major tourist attractions, parks, concert venues, and museums. Boston is a great location for this highquality symposium on VLSI, and we believe that you will enjoy the beautiful city as well as the symposium. This year's special theme for GLSVLSI is Hardware and System Design for Security and Privacy. To support this theme, we have organized several keynote talks by recognized experts and dedicated three special sessions to this topic. On Wednesday we will open the symposium with Marc Witteman, the Chief Executive Officer of Riscure in the Netherlands, who will speak about building secure chips. Kevin Fu, professor at University of Michigan and the Chief Scientist of Virta Labs, will talk about medical device security on Thursday afternoon. On Friday, Ingrid Verbauwhede, professor at University of Leuven in Belgium and at UCLA, will deliver another keynote talk on low power embedded encryption. Special sessions on the hardware security theme cover Internet-of-Things security (Wednesday), emerging technologies and security (Thursday), and a panel on future of hardware security research with speakers from industry, government agencies, and academia (Friday). In addition to including a substantial amount of focus on the hardware security theme, we made sure to provide perspectives on other cutting edge topics to enrich and diversify the symposium program. Yusuf Leblebici, professor at the Swiss Federal Institute of Technology, will talk about multi-sensor vision system design on Thursday. Also on Thursday, we have an exciting special session discussing synthetic biology research, explicitly targeting the VLSI and EDA communities. As for the technical content, GLSVLSI 2016 had outstanding statistics: 197 papers were submitted, by authors from 25 different countries, of which 50 papers were accepted as full papers for oral presentation (with a 25% acceptance rate). Including poster papers, a total of 71 papers will be included in the symposium and published in the proceedings. Of the authors of these papers in the program, 53% are from the Americas, 28% from Asia, and 19% from Europe. The final technical program consists of 21 long presentations and 29 short presentations in 12 oral sessions, and 21 posters in two poster sessions. Special sessions add another 9 exciting papers to the proceedings of the symposium. GLSVLSI 2016 starts on Wednesday, May 18th 2016, with an exciting line up of speakers on a broad range of issues including hardware security, encryption, new methods in CAD and network-on-chips (NoCs), low-power systems and emerging memory designs, and stochastic computing applications. On the second day, in addition to continuing the discussion on CAD and VLSI design challenges, we have technical sessions focused on error resilience and robustness, emerging technologies and post-CMOS VLSI. On the third day, further discussions are dedicated to the innovative solutions for emerging technologies, low-power systems, and VLSI design. The technical program of GLSVLSI 2016 contains two parallel tracks to allow the inclusion of more papers and extensive discussions during the three days of the symposium. Overall, there are 12 regular sessions, 4 special sessions, and 2 poster sessions in the technical program. Four outstanding papers have been selected as candidates for the best paper award and will be presented in different technical sessions (as marked in the program). The best paper award will be announced at the symposium banquet on Thursday evening. The social events of GLSVLSI 2016 include two special events this year. The first event is a reception cocktail on Wednesday evening at the historic Boston University Castle. Then, on Thursday, May 19th, in a special version of the famous "Boston Duck Tour", attendees will get a chance to see various Boston monuments and neighborhoods, and enjoy a short trip on the river. The 1-hour tour will be followed by the symposium banquet that will be held at the Boston University Trustees Ballroom. © 2016 ACM.
DA  - 2016///
PY  - 2016
DO  - 10.1145/2902961.2903038
VL  - 18-20-May-2016
SP  - 175
EP  - 179
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974716011&doi=10.1145%2f2902961.2903038&partnerID=40&md5=a9bdaa21d8e06a7453dbc52e139eda79
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 5</p>
KW  - Biomedical equipment
KW  - Chief executive officer
KW  - Command and control systems
KW  - Computer aided design
KW  - Computer hardware
KW  - Computer vision
KW  - Cryptography
KW  - Design
KW  - Distributed computer systems
KW  - Education
KW  - Electrical and computer engineering
KW  - Emerging technologies
KW  - Hardware
KW  - Hardware security
KW  - Integrated circuit design
KW  - Internet of things
KW  - Low power electronics
KW  - Multi-sensor vision system
KW  - Network security
KW  - Network-on-chip
KW  - Paper
KW  - Reconfigurable hardware
KW  - Rivers
KW  - Stochastic systems
KW  - Surrounding neighborhood
KW  - Swiss Federal Institute of Technology
KW  - Systems analysis
KW  - Universities and colleges
KW  - University of Michigan
ER  - 

TY  - JOUR
TI  - Machine Learning Solutions for the Security of Wireless Sensor Networks: A Review
AU  - Ghadi, Y.Y.
AU  - Mazhar, T.
AU  - Shloul, T.A.
AU  - Shahzad, T.
AU  - Salaria, U.A.
AU  - Ahmed, A.
AU  - Hamam, H.
T2  - IEEE Access
AB  - Energy efficiency and safety are two essential factors that play a significant role in operating a wireless sensor network. However, it is claimed that these two factors are naturally conflicting. The level of electrical consumption required by a security system is directly proportional to its degree of complexity. Wireless sensor networks require additional security measures above the capabilities of conventional network security protocols, such as encryption and key management. The potential application of machine learning techniques to address network security concerns is frequently discussed. These devices will have complete artificial intelligence capabilities, enabling them to understand their environment and respond. During the training phase, machine-learning systems may face challenges due to the large amount of data required and the complex nature of the training procedure. The main objective of the article is to know about different machine learning algorithms that are used to solve the security issues of wireless sensor networks. This study also focuses on the use of wireless sensor networks in different fields. Furthermore, this study also focuses on different Machine learning algorithms that are used to secure wireless sensor networks. Moreover, this study also addresses issues of adapting machine learning algorithms to accommodate the sensors' functionalities in the network configuration. Furthermore, this article also focuses on open issues in this field that must be solved.  © 2013 IEEE.
DA  - 2024///
PY  - 2024
DO  - 10.1109/ACCESS.2024.3355312
VL  - 12
SP  - 12699
EP  - 12719
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182939547&doi=10.1109%2fACCESS.2024.3355312&partnerID=40&md5=cd43d440f86cacba7d01d4a102310101
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 64</p>
KW  - Accident prevention
KW  - Artificial intelligence
KW  - Complex networks
KW  - Cryptography
KW  - Electrical consumption
KW  - Energy efficiency
KW  - Internet of things
KW  - IoT
KW  - Learning algorithms
KW  - Learning systems
KW  - LoWPAN
KW  - machine learning
KW  - Machine learning algorithms
KW  - Machine-learning
KW  - Network protocols
KW  - Network security
KW  - Security
KW  - Security of wireless sensor networks
KW  - Wireless sensor networks
KW  - WSN security
KW  - WSNs securities
KW  - WSNs security
ER  - 

TY  - JOUR
TI  - Distributed adaptive neural network control applied to a formation tracking of a group of low-cost underwater drones in hazardous environments
AU  - Pham, H.A.
AU  - Soriano, T.
AU  - Ngo, V.H.
AU  - Gies, V.
T2  - Applied Sciences (Switzerland)
AB  - This paper addresses a formation tracking problem of multiple low-cost underwater drones by implementing distributed adaptive neural network control (DANNC). It is based on a leader-follower architecture to operate in hazardous environments. First, unknown parameters of underwater vehicle dynamics, which are important requirements for real-world applications, are approximated by a neural network using a radial basis function. More specifically, those parameters are only calculated by local information, which can be obtained by an on-board camera without using an external positioning system. Secondly, a potential function is employed to ensure there is no collision between the underwater drones. We then propose a desired configuration of a group of unmanned underwater vehicles (UUVs) as a time-variant function so that they can quickly change their shape between them to facilitate the crossing in a narrow area. Finally, three UUVs, based on a robot operating system (ROS) platform, are used to emphasize the realistic low-cost aspect of underwater drones. The proposed approach is validated by evaluating in different experimental scenarios. © 2020 by the authors.
DA  - 2020///
PY  - 2020
DO  - 10.3390/app10051732
VL  - 10
IS  - 5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081980126&doi=10.3390%2fapp10051732&partnerID=40&md5=32da075ecf072039b645612a6485f503
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 11</p>
KW  - Collision and obstacle avoidance
KW  - Distributed adaptive neural network control
KW  - Gazebo
KW  - Low-cost underwater robotics
KW  - Robot operating system (ROS)
ER  - 

TY  - JOUR
TI  - LoRaWAN sensor: energy analysis and modeling
AU  - Ghaderi, M.R.
AU  - Amiri, N.
T2  - Wireless Networks
AB  - Nowadays, with the increasing growth of the Internet of Things (IoT), where reliable sensors are needed to operate for extended periods, the issue of energy consumption efficiency has become crucial. To address this, it is suggested to utilize low-power network (LPN) technology for IoT sensor networks. Additionally, a detailed analysis of sensor node performance and a comprehensive understanding of energy consumption sources in the sensor are necessary to tackle the energy management challenge. Therefore, it is highly valuable to have a model that can analyze the performance of the sensor node in various operation modes. In this article, we analyze the impact of various parameters on sensor node performance and present a comprehensive model for the sensor node energy consumption in the network based on long-range/long-range wide-area network (LoRa/LoRaWAN) technologies. This model enables the analysis of network performance and the estimation of energy consumption in different modes of the sensor node. The model can be practically utilized in the optimal design of sensor nodes in IoT networks based on LoRa/LoRaWAN technology, with a focus on increasing sensor lifetime. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.
DA  - 2024///
PY  - 2024
DO  - 10.1007/s11276-023-03542-y
VL  - 30
IS  - 2
SP  - 1013
EP  - 1036
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175201730&doi=10.1007%2fs11276-023-03542-y&partnerID=40&md5=242af800826237ea1c64ee246746cf90
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 13</p>
KW  - Analysis and models
KW  - Energy analysis
KW  - Energy efficiency
KW  - Energy management
KW  - Energy model
KW  - Energy utilization
KW  - Energy-consumption
KW  - Internet of things
KW  - IoT
KW  - Lora
KW  - LoRa
KW  - LoRaWAN
KW  - Low power electronics
KW  - Network technologies
KW  - Network-based
KW  - Node performance
KW  - Sensor
KW  - Sensor nodes
KW  - Wide area networks
KW  - Wide-area networks
ER  - 

TY  - JOUR
TI  - lIDS-SIoEL: intrusion detection framework for IoT-based smart environments security using ensemble learning
AU  - Hazman, C.
AU  - Guezzaz, A.
AU  - Benkirane, S.
AU  - Azrour, M.
T2  - Cluster Computing
AB  - Smart cities are being enabled all around the world by Internet of Things (IoT) applications. A smart city idea necessitates the integration of information and communication technologies and devices throughout a network in order to provide improved services to consumers. Because of their increasing amount and mobility, they are increasingly appealing to attackers. Therefore, several solutions, including as encryptions, authentication, availability, and data integrity, have been combined to protect IoT. Intrusion detection systems (IDSs) are a powerful security tool that may be improved by incorporating machine learning (ML) and deep learning (DP) techniques. This paper presents a novel intrusion detection framework for IoT-based smart environments with Ensemble Learning called IDS-SIoEL. Typically, the framework proposed an optimal anomaly detection model that uses AdaBoost, and combining different feature selection techniques Boruta, mutual information and correlation furthermore. The proposed model was evaluated on IoT-23, BoT-IoT, and Edge-IIoT datasets using the GPU. When compared to existing IDS, our approach provides good rating performance features of ACC, recall, and precision, with around 99.9% on record detection and calculation time of 33.68 s for learning and 0.02156 s for detection. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
DA  - 2023///
PY  - 2023
DO  - 10.1007/s10586-022-03810-0
VL  - 26
IS  - 6
SP  - 4069
EP  - 4083
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142454304&doi=10.1007%2fs10586-022-03810-0&partnerID=40&md5=5fbddb9fb9db2235565f9b89750ee6d6
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 82</p>
KW  - Adaptive boosting
KW  - Anomaly detection
KW  - BoT-internet of thing
KW  - BoT-IoT
KW  - Deep learning
KW  - Detection framework
KW  - Edge-IIoT
KW  - Ensemble learning
KW  - Feature extraction
KW  - Information and Communication Technologies
KW  - Internet of thing-23
KW  - Internet of things
KW  - Intrusion detection
KW  - Intrusion Detection Systems
KW  - Intrusion-Detection
KW  - IoT
KW  - IoT-23
KW  - Learning systems
KW  - Machine-learning
KW  - ML
KW  - Network security
KW  - Smart city
KW  - Smart environment
KW  - Smart environments
ER  - 

TY  - JOUR
TI  - REALISE-IoT: RISC-V-Based Efficient and Lightweight Public-Key System for IoT Applications
AU  - Mao, G.
AU  - Liu, Y.
AU  - Dai, W.
AU  - Li, G.
AU  - Zhang, Z.
AU  - Lam, A.H.F.
AU  - Cheung, R.C.C.
T2  - IEEE Internet of Things Journal
AB  - LoRa is a promising choice for deploying an IoT network due to its lightweight feature and the extensive support by LoRa Alliance. However, as a fundamental part of LoRa, the typical LoRaWAN protocol confronts severe security challenges because it insecurely utilizes AES-128 to support the low-cost feature. In this article, we propose a systematic solution that is compatible with LoRaWAN for IoT applications. We extend the standard LoRaWAN protocol with public-key infrastructures. Public-key features like key exchange and authentication are supported by lightweight hardware implementations of SHA-2, ECDH, EdDSA, and TRNG. A lightweight RISC-V processor with a security coprocessor is implemented and verified using FPGA technology. The security protocol and the prototype hardware system are validated and evaluated on practical applications from our industrial partner. The prototyped development board consumes a static power of 0.116 W and a dynamic power of 0.206 W. The proposed system can achieve a $5.6\times $ - $144.7\times $ speed up and reduce memory usage by $2.4\times $ - $12.3\times $ for security computations.  © 2014 IEEE.
DA  - 2024///
PY  - 2024
DO  - 10.1109/JIOT.2023.3296135
VL  - 11
IS  - 2
SP  - 3044
EP  - 3055
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165422366&doi=10.1109%2fJIOT.2023.3296135&partnerID=40&md5=a1a5b68bb84d4d7f2f995ee4162f387d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 8</p>
KW  - Computer hardware
KW  - Hardware
KW  - Hardware security
KW  - Internet of things
KW  - Internet of Things (IoT)
KW  - Light-weight cryptography
KW  - lightweight cryptography
KW  - Lora network
KW  - LoRa network
KW  - LoRaWAN
KW  - Low-costs
KW  - Network protocols
KW  - Network security
KW  - Public key cryptography
KW  - Public key infrastructure
KW  - Public key systems
KW  - public-key cryptography
KW  - RISC-V
KW  - Security
KW  - Security challenges
KW  - Software prototyping
ER  - 

TY  - CONF
TI  - Security Requirements Analysis for the IoT
AU  - Oh, S.-R.
AU  - Kim, Y.-G.
T2  - 2017 International Conference on Platform Technology and Service, PlatCon 2017 - Proceedings
AB  - Due to the rapid growth of network infrastructure and sensor, the age of the IoT (internet of things) that can be implemented into the smart car, smart home, smart building, and smart city is coming. IoT is a very useful ecosystem that provides various services (e.g., amazon echo); however, at the same time, risk can be huge too. Collecting information to help people could lead serious information leakage, and if IoT is combined with critical control system (e.g., train control system), security attack would cause loss of lives. Furthermore, research on IoT security requirements is insufficient now. Therefore, this paper focuses on IoT security, and its requirements. First, we propose basic security requirements of IoT by analyzing three basic characteristics (i.e., heterogeneity, resource constraint, dynamic environment). Then, we suggest six key elements of IoT (i.e., IoT network, cloud, user, attacker, service, platform) and analyze their security issues for overall security requirements. In addition, we evaluate several IoT security requirement researches. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/PlatCon.2017.7883727
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018188263&doi=10.1109%2fPlatCon.2017.7883727&partnerID=40&md5=04fbc9d69f37da28dfd372b842596484
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 77</p>
KW  - Automation
KW  - Basic characteristics
KW  - Control systems
KW  - Critical control systems
KW  - Cryptography
KW  - Dynamic environments
KW  - Information leakage
KW  - Intelligent buildings
KW  - Internet of things
KW  - Internet of Things
KW  - Network infrastructure
KW  - Network security
KW  - Resource Constraint
KW  - Security requirements
KW  - Security Requirements
KW  - Smart city
KW  - Train control systems
ER  - 

TY  - JOUR
TI  - A proposed healthcare architecture using cloud computing in WSN environment with a case study
AU  - Thalor, M.
AU  - Gharat, Y.
T2  - Int. J. Integr. Sci. Technol.
DA  - 2024///
PY  - 2024
VL  - 2
IS  - 1
SP  - 37
EP  - 44
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209186722&partnerID=40&md5=846a970606f52fb6f05ce6a45e1c72ac
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Enabling Lightweight Device Authentication in Message Queuing Telemetry Transport Protocol
AU  - Narasimha Swamy, S.
AU  - Anna, D.M.
AU  - Vijayalakshmi, M.N.
AU  - Kota, S.R.
T2  - IEEE Internet of Things Journal
AB  - Recent advancements in Internet of Things (IoT) have led to emergence of fascinating breakthroughs in diverse applications. Nowadays, the use cases of smart home systems are augmenting as they provide functionalities like real-time monitoring and high degree of remote control. Message queuing telemetry transport (MQTT) protocol is one of the most widely used messaging protocols in IoT-based applications including smart homes. This protocol lacks required security features owing to which, the intruders can launch variety of attacks easily. Stirred by this, we proposed a lightweight device authentication scheme for MQTT protocol. In this work, publisher/subscriber, and broker use lightweight cryptographic operations to enable device authentication. Also, this mechanism utilizes the lightweight cryptographic keys, such as one-time key (mathbf {mathrm {OT}}_{mathbf {mathrm {Key}}}) and Tokens ({T}_{i}) to complete registration and authentication process, respectively. Compared to other protocols, our approach reduces both communication and computation costs while maintaining the security demands. We put a prototype into practice to assess the performance of the proposed authentication mechanism. Further, we perform the formal analysis of the proposed authentication mechanism using automated validation of Internet security protocols and application protocol analyzer tool. The proposed security mechanism is resistant to various attacks, such as replay attack, device impersonate attack, malicious node attack, etc., and it enables the security features like device authentication and device anonymity in smart homes. © 2014 IEEE.
DA  - 2024///
PY  - 2024
DO  - 10.1109/JIOT.2024.3349394
VL  - 11
IS  - 9
SP  - 15792
EP  - 15807
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182377899&doi=10.1109%2fJIOT.2024.3349394&partnerID=40&md5=913a5583e27655ab9a18464227f97b78
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 13</p>
KW  - Authentication
KW  - Automation
KW  - Bandpass filters
KW  - Cryptography
KW  - Cuckoo filter
KW  - Cuckoo filters
KW  - device authentication
KW  - Device authentications
KW  - edge computing
KW  - Edge computing
KW  - Industrial internet of thing
KW  - Intelligent buildings
KW  - Internet of things
KW  - Internet of Things (IoT)
KW  - IoT
KW  - message queuing telemetry transport (MQTT)
KW  - MQTT
KW  - Network security
KW  - Real time systems
KW  - Remote control
KW  - security
KW  - Security
KW  - Security systems
KW  - smart homes
KW  - Smart homes
ER  - 

TY  - JOUR
TI  - LoRaWAN-Based IoT System Implementation for Long-Range Outdoor Air Quality Monitoring
AU  - Jabbar, W.A.
AU  - Subramaniam, T.
AU  - Ong, A.E.
AU  - Shu'Ib, M.I.
AU  - Wu, W.
AU  - de Oliveira, M.A.
T2  - Internet of Things (Netherlands)
AB  - This study proposes a smart long-range (LoRa) sensing node to timely collect the air quality information and update it on the cloud. The developed long-range wide area network (LoRaWAN)-based Internet of Things (IoT) air quality monitoring system (AQMS), hereafter called LoRaWAN-IoT-AQMS, was deployed in an outdoor environment to validate its reliability and effectiveness. The system is composed of multiple sensors (NO2, SO2, CO2, CO, PM2.5, temperature, and humidity), Arduino microcontroller, LoRa shield, LoRaWAN gateway, and The Thing Network (TTN) IoT platform. The LoRaWAN-IoT-AQMS is a standalone system powered continuously by a rechargeable battery with a photovoltaic solar panel via a solar charger shield for sustainable operation. Our system simultaneously gathers the considered air quality information by using the smart sensing unit. Then, the system transmits the information through the gateway to the TTN platform, which is integrated with the ThingSpeak IoT server. This action updates the collected data and displays these data on a developed Web-based dashboard and a Graphical User Interface (GUI) that uses the Virtuino mobile application. Thus, the displayed information can be easily accessed by users via their smartphones. The results obtained by the developed LoRaWAN-IoT-AQMS are validated by comparing them with experimental results based on the high-technology Aeroqual air quality monitoring devices. Our system can reliably monitor various air quality indicators and efficiently transmit the information in real time over the Internet. © 2022
DA  - 2022///
PY  - 2022
DO  - 10.1016/j.iot.2022.100540
VL  - 19
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129939051&doi=10.1016%2fj.iot.2022.100540&partnerID=40&md5=366f1114414f8d20dea2c956c9bc7be1
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 103</p>
KW  - Air quality monitoring
KW  - Iot lora lorawan
KW  - TTN ThingSpeak Virtuino
ER  - 

TY  - CONF
TI  - IoTFlow: Inferring IoT Device Behavior at Scale through Static Mobile Companion App Analysis
AU  - Schmidt, D.
AU  - Tagliaro, C.
AU  - Borgolte, K.
AU  - Lindorfer, M.
T2  - CCS 2023 - Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security
AB  - The number of “smart” devices, that is, devices making up the Internet of Things (IoT), is steadily growing. They suffer from vulnerabilities just as other software and hardware. Automated analysis techniques can detect and address weaknesses before attackers can misuse them. Applying existing techniques or developing new approaches that are sufficiently general is challenging though. Contrary to other platforms, the IoT ecosystem features various software and hardware architectures. We introduce IoTFlow, a new static analysis approach for IoT devices that leverages their mobile companion apps to address the diversity and scalability challenges. IoTFlow combines Value Set Analysis (VSA) with more general data-flow analysis to automatically reconstruct and derive how companion apps communicate with IoT devices and remote cloud-based backends, what data they receive or send, and with whom they share it. To foster future work and reproducibility, our IoTFlow implementation is open source. We analyze 9,889 manually verified companion apps with IoTFlow to understand and characterize the current state of security and privacy in the IoT ecosystem, which also demonstrates the utility of IoTFlow. We compare how these IoT apps differ from 947 popular general-purpose apps in their local network communication, the protocols they use, and who they communicate with. Moreover, we investigate how the results of IoTFlow compare to dynamic analysis, with manual and automated interaction, of 13 IoT devices when paired and used with their companion apps. Overall, utilizing IoTFlow, we discover various IoT security and privacy issues, such as abandoned domains, hard-coded credentials, expired certificates, and sensitive personal information being shared. © 2023 Copyright held by the owner/author(s).
DA  - 2021///
PY  - 2021
DO  - 10.1145/3576915.3623211
SP  - 681
EP  - 695
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179840428&doi=10.1145%2f3576915.3623211&partnerID=40&md5=c0df5839671bd44f1ee55a5913117d99
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 11</p>
KW  - Analysis techniques
KW  - Automated analysis
KW  - Companion app
KW  - companion apps
KW  - Data flow analysis
KW  - Ecosystems
KW  - Hardware architecture
KW  - Internet of thing
KW  - Internet of thing privacy
KW  - Internet of thing security
KW  - Internet of things
KW  - Internet of Things (IoT)
KW  - IoT privacy
KW  - IoT security
KW  - network analysis
KW  - Network security
KW  - New approaches
KW  - Open source software
KW  - Sensitive data
KW  - Smart devices
KW  - Software and hardwares
KW  - static analysis
KW  - Static analysis
ER  - 

TY  - JOUR
TI  - LoRaWAN Behaviour Analysis through Dataset Traffic Investigation
AU  - Spadaccino, P.
AU  - Crinó, F.G.
AU  - Cuomo, F.
T2  - Sensors
AB  - The large development of Internet of Things technologies is increasing the use of smart-devices to solve and support several real-life issues. In many cases, the aim is to move toward systems that, even if significant demands are not required in terms of quantity of exchanged data, they should be very reliable in terms of battery life and signal coverage. Networks that have these characteristics are the Low Power WAN (LPWAN). One of the most interesting LPWAN is LoRaWAN. LoRaWAN is a network with four principal components: end-devices, gateways, network servers, and application servers. It uses a LoRa physical layer to exchange messages between end-devices and gateways that forward these messages, through classic TCP/IP protocol, to the network server. In this work, we analyse LoRa and LoRaWAN by looking at its transmission characteristics and network behaviour, respectively, explaining the role of its components and showing the message exchange. This analysis is performed through the exploration of a dataset taken from the literature collecting real LoRaWAN packets. The goal of the work is twofold: (1) to investigate, under different perspectives, how a LoRaWAN works and (2) to provide software tools that can be used in several other LoraWAN datasets to measure the network behaviour. We carry out six different analyses to look at the most important features of LoRaWAN. For each analysis we present the adopted measurement strategy as well as the obtained results in the specific use case. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.
DA  - 2022///
PY  - 2022
DO  - 10.3390/s22072470
VL  - 22
IS  - 7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126743315&doi=10.3390%2fs22072470&partnerID=40&md5=055edd64e974fa99a91d55501f5df5fb
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 21</p>
KW  - Behavior analysis
KW  - Electric Power Supplies
KW  - End-devices
KW  - Gateways (computer networks)
KW  - Internet of things technologies
KW  - LoRA
KW  - LoRaWAN
KW  - Low Power
KW  - Low power electronics
KW  - Low power WAN
KW  - LPWAN
KW  - Network behaviors
KW  - Network layers
KW  - Network server
KW  - Performance analysis
KW  - Performances analysis
KW  - power supply
KW  - software
KW  - Software
ER  - 

TY  - JOUR
TI  - A novel intelligent approach for man-in-the-middle attacks detection over internet of things environments based on message queuing telemetry transport
AU  - Michelena, Á.
AU  - Aveleira-Mata, J.
AU  - Jove, E.
AU  - Bayón-Gutiérrez, M.
AU  - Novais, P.
AU  - Romero, O.F.
AU  - Calvo-Rolle, J.L.
AU  - Aláiz-Moretón, H.
T2  - Expert Systems
AB  - One of the most common attacks is man-in-the-middle (MitM) which, due to its complex behaviour, is difficult to detect by traditional cyber-attack detection systems. MitM attacks on internet of things systems take advantage of special features of the protocols and cause system disruptions, making them invisible to legitimate elements. In this work, an intrusion detection system (IDS), where intelligent models can be deployed, is the approach to detect this type of attack considering network alterations. Therefore, this paper presents a novel method to develop the intelligent model used by the IDS, being this method based on a hybrid process. The first stage of the process implements a feature extraction method, while the second one applies different supervised classification techniques, both over a message queuing telemetry transport (MQTT) dataset compiled by authors in previous works. The contribution shows excellent performance for any compared classification methods. Likewise, the best results are obtained using the method with the highest computational cost. Thanks to this, a functional IDS will be able to prevent MQTT attacks. © 2023 The Authors. Expert Systems published by John Wiley & Sons Ltd.
DA  - 2024///
PY  - 2024
DO  - 10.1111/exsy.13263
VL  - 41
IS  - 2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149345425&doi=10.1111%2fexsy.13263&partnerID=40&md5=cd711e3c7af302116f0021d935dca1b9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 23</p>
KW  - artificial neural networks
KW  - Attack detection
KW  - Classification (of information)
KW  - Computer crime
KW  - Cyber security
KW  - cybersecurity
KW  - Cybersecurity
KW  - decision trees
KW  - Decision trees
KW  - Intelligent models
KW  - Internet of things
KW  - Intrusion detection
KW  - intrusion detection system
KW  - Intrusion Detection Systems
KW  - K-near-neighbor
KW  - K-nearest-neighbours
KW  - Man in the middle
KW  - man-in-the-middle
KW  - message queuing telemetry transport
KW  - Message queuing telemetry transport
KW  - Nearest neighbor search
KW  - Nearest-neighbour
KW  - Network security
KW  - Neural networks
KW  - principal component analysis
KW  - Principal component analysis
KW  - Principal-component analysis
KW  - random forest
KW  - Random forests
ER  - 

TY  - JOUR
TI  - Geographic information system
AU  - Chang, K.-T.
T2  - Geographic Information System
DA  - 2017///
PY  - 2017
SP  - 1
EP  - 9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030028858&partnerID=40&md5=a3a85241c51a0994d7a81cd5c09d61fa
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 169</p>
ER  - 

TY  - JOUR
TI  - Slotted ALOHA on LoRaWAN-design, analysis, and deployment
AU  - Polonelli, T.
AU  - Brunelli, D.
AU  - Marzocchi, A.
AU  - Benini, L.
T2  - Sensors (Switzerland)
AB  - LoRaWAN is one of the most promising standards for long-range sensing applications. However, the high number of end devices expected in at-scale deployment, combined with the absence of an effective synchronization scheme, challenge the scalability of this standard. In this paper, we present an approach to increase network throughput through a Slotted-ALOHA overlay on LoRaWAN networks. To increase the single channel capacity, we propose to regulate the communication of LoRaWAN networks using a Slotted-ALOHA variant on the top of the Pure-ALOHA approach used by the standard; thus, no modification in pre-existing libraries is necessary. Our method is based on an innovative synchronization service that is suitable for low-cost wireless sensor nodes. We modelled the LoRaWAN channel with extensive measurement on hardware platforms, and we quantified the impact of tuning parameters on physical and medium access control layers, as well as the packet collision rate. Results show that Slotted-ALOHA supported by our synchronization service significantly improves the performance of traditional LoRaWAN networks regarding packet loss rate and network throughput. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.
DA  - 2019///
PY  - 2019
DO  - 10.3390/s19040838
VL  - 19
IS  - 4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061858181&doi=10.3390%2fs19040838&partnerID=40&md5=aa88ce4e8e482f3864bdf5559278497f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 161</p>
KW  - ALOHA
KW  - Information services
KW  - Internet of things
KW  - Internet of Things
KW  - Long range radio
KW  - LoRa
KW  - LoRaWAN
KW  - Low power electronics
KW  - Low-energy wireless protocols
KW  - Medium access control
KW  - Packet networks
KW  - Sensor nodes
KW  - Slotted Aloha
KW  - Slotted ALOHA
KW  - Synchronization
KW  - Wireless protocol
KW  - Wireless sensor networks
ER  - 

TY  - CONF
TI  - Anomaly Detection in IoT Networks Based on Intelligent Security Event Correlation
AU  - Kotenko, I.
AU  - Levshun, D.
T2  - 2024 16th International Conference on COMmunication Systems and NETworkS, COMSNETS 2024
AB  - Modern Internet of Things networks combine many devices and sensors that transmit and process large amounts of data. Security tools identify security events that contain information about detected system or network states. In turn, high-performance data anomaly detection methods are required to ensure stability and reliability of work processes. Information about the correlation of identified security events can be used to detect and explain deviations from normal states. This study proposes an anomaly detection approach based on the causal correlation of security events using machine learning. The proposed approach does not require prior knowledge of event scenarios. Using cluster analysis and a convolutional recurrent neural network, we construct a security state correlation graph corresponding to the normal behavior of the system. Cluster analysis determines the similarity of events to each other. A convolutional LSTM, analyzes the spatio-temporal relationship of events. Using the identified event correlation thresholds, we look for anomalies in real time. Experimental results on an Internet of Things sensor dataset show that the proposed method is efficient in anomaly detection tasks.  © 2024 IEEE.
DA  - 2024///
PY  - 2024
DO  - 10.1109/COMSNETS59351.2024.10426939
SP  - 816
EP  - 824
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186714164&doi=10.1109%2fCOMSNETS59351.2024.10426939&partnerID=40&md5=749d50da3bc3aa8bb0b08a2e883ed550
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3</p>
KW  - anomaly detection
KW  - Anomaly detection
KW  - Cluster analysis
KW  - Convolution
KW  - Convolutional neural networks
KW  - Cyber security
KW  - cybersecurity
KW  - Cybersecurity
KW  - event correlation
KW  - Event correlation
KW  - Intelligent security
KW  - Internet of things
KW  - Large amounts of data
KW  - Long short-term memory
KW  - machine learning
KW  - Machine-learning
KW  - Network security
KW  - Network-based
KW  - Security events
KW  - Security tools
KW  - System state
ER  - 

TY  - JOUR
TI  - Machine Learning Based Classification of IoT Traffic
AU  - Velichkovska, B.
AU  - Cholakoska, A.
AU  - Atanasovski, V.
T2  - Radioengineering
AB  - With the rapid expansion and widespread adoption of the Internet of Things (IoT), maintaining secure connections among active devices can be challenging. Since IoT devices are limited in power and storage, they cannot performcomplex tasks, which makes them vulnerable to different types of attacks. Given the volume of data generated daily, detecting anomalous behavior can be demanding. However, machine learning (ML) algorithms have proven successful in extracting complex patterns from big data, which has led to active applications in IoT. In this paper, we perform a comprehensive analysis, including 4 ML algorithms and 3 neural networks (NNs), and propose a pipeline which analyzes the influence data reduction (loss) has on the performance of these algorithms. We use random undersampling as a data reduction technique, which simulates reduced network traffic data. The pipeline investigates several degrees of data loss. The results show that models trained on the original data distribution obtain accuracy that verges on 100%. XGBoost performs best from the classic ML algorithms. From the deep learning models, the 2-layered NN provides excellent results and has sufficient depth for practical application. On the other hand, when the models are trained on the undersampled data, there is a decrease in performance, most notably in the case of NNs. The most prominent change is seen in the 4-layered NN, where the model trained on the original dataset detects attacks with a success of 93.53%, whereas the model trained on the maximally reduced data has a success of only 39.39%. © 2023, Radioengineering. All Rights Reserved.
DA  - 2023///
PY  - 2023
DO  - 10.13164/re.2023.0256
VL  - 32
IS  - 2
SP  - 256
EP  - 263
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160644768&doi=10.13164%2fre.2023.0256&partnerID=40&md5=94cb84a2106208008cfc8c5a82fa000b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 6</p>
KW  - deep learning
KW  - Internet of Things (IoT)
KW  - intrusion detection
KW  - Machine learning
KW  - traffic modelling
ER  - 

TY  - JOUR
TI  - QKDNetSim+: Improvement of the quantum network simulator for NS-3
AU  - Soler, D.
AU  - Cillero, I.
AU  - Dafonte, C.
AU  - Fernández-Veiga, M.
AU  - Vilas, A.F.
AU  - Nóvoa, F.J.
T2  - SoftwareX
AB  - The first Quantum Key Distribution (QKD) networks are currently being deployed, but the implementation cost is still prohibitive for most researchers. As such, there is a need for realistic QKD network simulators. The QKDNetSim module for the network simulator NS-3 focuses on the representation of packets and the management of key material in a QKD network at the application layer. Although QKDNetSim's representation of a QKD network is insightful, some its components lack the depth that would allow the simulator to faithfully represent the behaviour of a real quantum network. In this work, we analyse QKDNetSim's architecture to identify its limitations, and we present an enhanced version of QKDNetSim in which some of its components have been modified to provide a more realistic simulation environment. © 2024 The Author(s)
DA  - 2024///
PY  - 2024
DO  - 10.1016/j.softx.2024.101685
VL  - 26
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188569139&doi=10.1016%2fj.softx.2024.101685&partnerID=40&md5=21ea72e8c89919c1bb700302b664bb94
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 4</p>
KW  - Application layers
KW  - Implementation cost
KW  - Key materials
KW  - Network layers
KW  - Network simulation
KW  - Network simulators
KW  - NS-3
KW  - QKD
KW  - Quantum communication
KW  - Quantum communications
KW  - Quantum cryptography
KW  - Quantum network
KW  - Realistic simulation
KW  - Simulation environment
ER  - 

TY  - JOUR
TI  - Native speakerism as a source of agency-related critical incidents: Implications for non-native English teachers’ professional identity construction
AU  - Ershadi, F.
AU  - Nazari, M.
AU  - Chegenie, M.S.
T2  - System
AB  - Despite the developments on the contributions of native speakerism (NS) for the professionalism of non-native English-speaking teachers (NNESTs), little is known about how NS influences NNESTs' agency and identity construction. The present study draws on an ecological-poststructural lens and explores such a contribution through critical incidents that happened to 15 Iranian NNESTs. Data were collected from a questionnaire, narrative frames, and semi-structured interviews. The analysis of the data revealed three major themes: (1) native speakerism as a source of NNEST marginalization, (2) the role of school policymakers in NS-induced inequality, and (3) native speakerism as a source of pedagogy of doubt. The findings showed that NS not only serves as a discourse that transcends geographical borders to shape NNESTs' agency and identity, but institutional participants also add to the negative effects of NS on NNESTs' professional practice. The study concludes with implications for institutional policymakers and teacher educators to revisit their understanding of NS and the ripple effects that NS bears for NNESTs’ agency and identity construction. © 2023
DA  - 2024///
PY  - 2024
DO  - 10.1016/j.system.2023.103182
VL  - 120
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178549523&doi=10.1016%2fj.system.2023.103182&partnerID=40&md5=539081115f841d305f56f8703b1c6c4d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 14</p>
KW  - Critical incidents
KW  - Language teacher agency
KW  - Language teacher identity
KW  - Native speakerism
KW  - NESTs
KW  - NNESTs
ER  - 

TY  - JOUR
TI  - AixLib: an open-source Modelica library for compound building energy systems from component to district level with automated quality management
AU  - Maier, L.
AU  - Jansen, D.
AU  - Wüllhorst, F.
AU  - Kremer, M.
AU  - Kümpel, A.
AU  - Blacha, T.
AU  - Müller, D.
T2  - Journal of Building Performance Simulation
AB  - Open-source modelling libraries facilitate the standardization and harmonization of model development. In the context of building energy systems, Modelica is a suitable modelling language as it is equation-based and object-oriented. As an outcome of the IBPSA project 1 cooperation, four open-source modelling libraries have been successfully deployed which all share the core library Modelica IBPSA. One of them is the AixLib modelling library. AixLib supports different modelling depths ranging from component to district level and covers all relevant domains in the context of building energy systems. To ensure high-quality simulations, continuous integration has been successfully added to automatically compare simulation results with existing validation data. This paper presents AixLib's key features, scope, and associated tools. We present three use cases that highlight the broad application range of AixLib models. Furthermore, an overview of relevant research and industry projects is provided. Finally, we give an outlook on future development goals. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.
DA  - 2024///
PY  - 2024
DO  - 10.1080/19401493.2023.2250521
VL  - 17
IS  - 2
SP  - 196
EP  - 219
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169156047&doi=10.1080%2f19401493.2023.2250521&partnerID=40&md5=f6fc2f92cffa1c62ba562454251b71e3
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 24</p>
KW  - building energy system
KW  - Building energy systems
KW  - Buildings
KW  - continuous integration
KW  - Continuous integrations
KW  - Equation based
KW  - Harmonisation
KW  - HVAC
KW  - Industrial research
KW  - Libraries
KW  - Model development
KW  - Model library
KW  - Modelica
KW  - Modeling languages
KW  - modelling library
KW  - Object oriented
KW  - Open systems
KW  - Open-source
KW  - Open-source model
KW  - Quality management
ER  - 

TY  - JOUR
TI  - Implementation of Nonlinear Adaptive U-Model Control Synthesis Using a Robot Operating System for an Unmanned Underwater Vehicle
AU  - Hussain, N.A.A.
AU  - Ali, S.S.A.
AU  - Ridao, P.
AU  - Cieslak, P.
AU  - Al-Saggaf, U.M.
T2  - IEEE Access
AB  - This paper presents the development of unmanned marine robotic control modelling and control synthesis using a coupled multivariable underactuated nonlinear adaptive U-model approach. The proposed controller was developed using thru an open source robot operating system (ROS) platform. The new adaptive coupled U-model based internal model control (IMC) node was successfully developed and tested. The proposed controller demonstrated the simplicity of the control synthesis process and the implementation of the mathematical algorithm in real-time. The controller was compared with the proven existing GIRONA 500 UUV for real-time performance. The ROS environment provides fast and reliable controller design and development compared to conventional software architecture. Simulation and real-time experiment were conducted using ROS via the GIRONA 500 UUV platform and compared with a PID mission controller. A new ROS node of nonlinear adaptive U-model based IMC was developed using ROS. The results showed good control signal convergence and tracking performance between the plant or system model with the proposed method.  © 2013 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ACCESS.2020.3037122
VL  - 8
SP  - 205685
EP  - 205695
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097383471&doi=10.1109%2fACCESS.2020.3037122&partnerID=40&md5=04a558ce36cf433d508747a0f3909770
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 10</p>
KW  - Adaptive control
KW  - Adaptive control systems
KW  - Control system synthesis
KW  - Controllers
KW  - Internal model control
KW  - Mathematical algorithms
KW  - Model predictive control
KW  - Modelling and controls
KW  - nonlinear
KW  - Open source software
KW  - Open systems
KW  - Real time performance
KW  - Real-time experiment
KW  - Robot operating system
KW  - Robot operating systems (ROS)
KW  - Robots
KW  - ROS
KW  - Tracking performance
KW  - underactuated
KW  - Unmanned underwater vehicles
KW  - UUV
ER  - 

TY  - JOUR
TI  - Analysis and Performance Comparison of IoT Message Transfer Protocols Applying in Real Photovoltaic System
AU  - Tran, K.T.M.
AU  - Pham, A.X.
AU  - Nguyen, N.P.
AU  - Dang, P.T.
T2  - International Journal of Networked and Distributed Computing
AB  - The adoption of reliable and real-time communication technology is an absolute necessity for the advancement of Internet of Things (IoT) applications. Messaging protocols such as MQTT, AMQP, and HTTP are frequently used for communication with resource-constrained IoT devices. However, choosing a suitable and effective messaging protocol presents a daunting challenge for organizations, as it depends on the specific characteristics and messaging requirements of the IoT system. Therefore, it is crucial to have a comprehensive understanding of three established messaging protocols, such as the Hypertext Transfer Protocol (HTTP), the Message Queuing Telemetry Transport (MQTT), and the Advanced Message Queuing Protocol (AMQP), to appropriately apply them in practical projects. In this paper, information technology solutions are provided for a chain of solar farms to improve harvest productivity, facilitate warning notifications, and enable remote control. Subsequently, a detailed comparative analysis is performed, considering various interconnected criteria, to gain valuable insight into the strengths and limitations of these protocols. The results show that MQTT and AMQP play a role in enhancing overall efficiency and speed within the framework of our suggested photovoltaic system. © The Author(s) 2024.
DA  - 2024///
PY  - 2024
DO  - 10.1007/s44227-024-00021-4
VL  - 12
IS  - 1
SP  - 131
EP  - 143
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187241415&doi=10.1007%2fs44227-024-00021-4&partnerID=40&md5=8289764daec2eb3afded98c71b982feb
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 9</p>
KW  - Advanced Message Queuing Protocol
KW  - Hypertext Transfer Protocol
KW  - IoT Message Transfer Protocols
KW  - IoT smart agriculture
KW  - Message Queuing Telemetry Transport
ER  - 

TY  - JOUR
TI  - Using Model-Based Systems Engineering to Avoid Unnecessary Technology Resulting from Dynamic Requirements
AU  - Mittal, V.
AU  - Gillespie, S.
T2  - IEEE Transactions on Engineering Management
AB  - As systems develop and evolve, operational requirements, system architecture, and technological capabilities change. These changes can result in design changes, making once necessary components unnecessary. Engineering managers are often hesitant to remove these unnecessary components though the driving requirements no longer apply. This issue, 'technology for the sake of technology,' occurs frequently when a system is the product of a long development time line, such as those implemented by the defense community. Model-based systems engineering (MBSE) enables engineering managers to identify unnecessary technology, and accurately and quickly update the design and assess risk associated with removing unnecessary technology. A proposed methodology uses MBSE products to identify and mitigate 'technology for the sake of technology'. A case study is presented of this process based on the power system for a robotic exoskeleton under development where the power draw and mission requirements changed significantly over the development life cycle. When the design team followed traditional systems engineering processes, they included many unnecessary components in the final design; however, an MBSE approach identified numerous components that were no longer necessary, resulting in a simpler and more robust design. This approach can be used to improve design quality and cost for varied systems.  © 1988-2012 IEEE.
DA  - 2024///
PY  - 2024
DO  - 10.1109/TEM.2022.3181268
VL  - 71
SP  - 2660
EP  - 2671
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134242081&doi=10.1109%2fTEM.2022.3181268&partnerID=40&md5=fa10a876fe3e82dcc34dafaae2384691
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3</p>
KW  - Complexity theory
KW  - Cost engineering
KW  - Defense engineering
KW  - Exoskeleton (Robotics)
KW  - Life cycle
KW  - Managers
KW  - Model-based system engineering
KW  - Model-based system engineerings
KW  - model-based systems engineering (MBSE)
KW  - Modeling languages
KW  - Network security
KW  - Risk assessment
KW  - Security
KW  - Software
KW  - Stakeholder
KW  - System modeling language
KW  - System models
KW  - Systems engineering
KW  - technology evaluation
KW  - Technology evaluation
KW  - technology forecasting
KW  - Technology forecasting
ER  - 

TY  - JOUR
TI  - Toward Intelligent Monitoring in IoT: AI Applications for Real-Time Analysis and Prediction
AU  - Villegas-Ch, W.
AU  - Garcia-Ortiz, J.
AU  - Sanchez-Viteri, S.
T2  - IEEE Access
AB  - In the contemporary era, the intersection of the Internet of Things and artificial intelligence revolutionizes how industries monitor and optimize their operations. In this work, we present a system that combines real-time monitoring provided by Internet of Things devices with predictive analytics based on artificial intelligence. This system detects anomalies in real-time and anticipates possible failures, allowing proactive interventions to maximize efficiency and minimize operating costs. Our findings reveal a significant improvement in the early detection of abnormal trends, as the system consistently identifies potential problems long before they become critical failures. Our evaluation employed data sets collected from controlled and industrial production environments, with more than 1 million records, including critical parameters such as temperature, humidity, and pressure. The results highlight a significant improvement in the early detection of abnormal trends, with a temperature detection accuracy of 98.7%, exceeding reference values and demonstrating the system's effectiveness in preventing critical failures. The analysis also revealed previously unrecognized operational patterns, offering opportunities for industrial process optimization. This work highlights the effective integration of the Internet of Things and artificial intelligence to improve industrial monitoring, highlighting the tangible benefits of such integration, such as the adaptability and continuous learning of the system, ensuring its long-term effectiveness. © 2013 IEEE.
DA  - 2024///
PY  - 2024
DO  - 10.1109/ACCESS.2024.3376707
VL  - 12
SP  - 40368
EP  - 40386
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187987118&doi=10.1109%2fACCESS.2024.3376707&partnerID=40&md5=4f43ada4170ea3e3ee50930a60e2d041
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 32</p>
KW  - Adaptive feedback
KW  - Artificial intelligence
KW  - Critical failures
KW  - Feedback
KW  - Humidity control
KW  - intelligent adaptation
KW  - Intelligent adaptation
KW  - Intelligent monitoring
KW  - Interactive computer systems
KW  - Internet of things
KW  - IoT-based optimization
KW  - Operating costs
KW  - Optimisations
KW  - Optimization
KW  - Predictive analytics
KW  - Proposal
KW  - Real - Time system
KW  - Real time systems
KW  - Real- time
KW  - real-time precision
KW  - Real-time precision
ER  - 

TY  - JOUR
TI  - Protocol-Based Deep Intrusion Detection for DoS and DDoS Attacks Using UNSW-NB15 and Bot-IoT Data-Sets
AU  - Zeeshan, M.
AU  - Riaz, Q.
AU  - Bilal, M.A.
AU  - Shahzad, M.K.
AU  - Jabeen, H.
AU  - Haider, S.A.
AU  - Rahim, A.
T2  - IEEE Access
AB  - Since its inception, the Internet of Things (IoT) has witnessed mushroom growth as a breakthrough technology. In a nutshell, IoT is the integration of devices and data such that processes are automated and centralized to a certain extent. IoT is revolutionizing the way business is done and is transforming society as a whole. As this technology advances further, the need to exploit detection and weakness awareness increases to prevent unauthorized access to critical resources and business functions, thereby rendering the system unavailable. Denial of Service (DoS) and Distributed DoS attacks are all too common. In this paper, we propose a Protocol Based Deep Intrusion Detection (PB-DID) architecture, in which we created a data-set of packets from IoT traffic by comparing features from the UNSWNB15 and Bot-IoT data-sets based on flow and Transmission Control Protocol (TCP). We classify non-anomalous, DoS, and DDoS traffic uniquely by taking care of the problems like imbalanced and over-fitting. We have achieved a classification accuracy of 96.3& by using deep learning (DL) technique. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
DA  - 2022///
PY  - 2022
DO  - 10.1109/ACCESS.2021.3137201
VL  - 10
SP  - 2269
EP  - 2283
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122069667&doi=10.1109%2fACCESS.2021.3137201&partnerID=40&md5=6258f31ba9b429aec7a1acce29008d15
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 152</p>
KW  - Computer crime
KW  - Computer hacking
KW  - DDoS detection
KW  - Deep learning
KW  - Deep learning for intrusion detection
KW  - Denial of Service
KW  - Denial of service detection
KW  - Denial-of-service attack
KW  - Internet of things
KW  - Intrusion detection
KW  - Intrusion detection in internet of thing
KW  - Intrusion-Detection
KW  - Network architecture
KW  - Network security
KW  - Personal computing
KW  - Security
KW  - Support vector machines
KW  - Support vectors machine
KW  - Training
KW  - Transmission control protocol
ER  - 

TY  - CONF
TI  - AI-enabled IoT Applications: Towards a Transparent Governance Framework
AU  - Fares, N.Y.
AU  - Nedeljkovic, D.
AU  - Jammal, M.
T2  - 2023 IEEE Global Conference on Artificial Intelligence and Internet of Things, GCAIoT 2023
AB  - Internet of Things (IoT) and Artificial Intelligence (AI) systems have become prevalent across various industries, steering to diverse and far-reaching outcomes, and their convergence has garnered significant attention in the tech world. Studies and reviews are instrumental in supplying industries with the nuanced understanding of the multifaceted developments of this joint domain. This paper undertakes a critical examination of existing perspectives and governance policies, adopting a contextual approach, and addressing not only the potential but also the limitations of these governance policies. In the complex landscape of AI-infused IoT systems, transparency and interpretability are pivotal qualities for informed decision-making and effective governance. In AI governance, transparency allows for scrutiny and accountability, while interpretability facilitates trust and confidence in AI-driven decisions. Therefore, we also evaluate and advocate for the use of two very popular eXplainable AI (XAI) techniques-SHAP and LIME-in explaining the predictive results of AI models. Subsequently, this paper underscores the imperative of not only maximizing the advantages and services derived from the incorporation of IoT and AI but also diligently minimizing possible risks and challenges.  © 2023 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/GCAIoT61060.2023.10385106
SP  - 109
EP  - 114
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184666640&doi=10.1109%2fGCAIoT61060.2023.10385106&partnerID=40&md5=9ba550cc6ea3f858cd0713d6ef72865f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 7</p>
KW  - AI
KW  - Artificial intelligence systems
KW  - Decision making
KW  - Decisions makings
KW  - Ethical regulation
KW  - Ethical Regulations
KW  - Ethical technology
KW  - Explainable AI
KW  - Explainable artificial intelligence
KW  - Governance
KW  - Informed decision
KW  - Intelligence models
KW  - Internet of things
KW  - Interpretability
KW  - IoT
KW  - Lime
KW  - Smart Cities
KW  - Smart city
KW  - Transparency
ER  - 

TY  - JOUR
TI  - IoT-Enabled Pest Identification and Classification with New Meta-Heuristic-Based Deep Learning Framework
AU  - Kathole, A.B.
AU  - Vhatkar, K.N.
AU  - Patil, S.D.
T2  - Cybernetics and Systems
AB  - The pest and insect affected crop is an important concern to cause damage to the agricultural sector. While identifying the pest in the crop, the camera placement is not supported in an inconsistent manner to capture the pest images. Hence, certain Internet of Things (IoT) devices are used to catch the pest images with its corresponding agriculture based sensor, yet it also faces some limitations to provide the accurate results. In order to alleviate the problem, an IoT-assisted pest identification and classification method is proposed. Initially, the IoT sensors are used to collect the required images. Subsequently, the input images are used to perform the object detection phase that is accomplished by the Yolov3, where the pest is detected significantly. Further, the detected images are fed into the model of “Convolutional Neural Network (CNN),” in which the deep features are fetched and finally given as input to the classifier model of “Convolution Neural Long Short-Term Memory (CNLSTM),” in turn some hyper parameters are optimally tuned by “Adaptive Honey Badger Algorithm (AHBA).” Hence, the experimental results prove that the recommended method achieves the better performance in terms of diverse metrics. © 2022 Taylor & Francis Group, LLC.
DA  - 2024///
PY  - 2024
DO  - 10.1080/01969722.2022.2122001
VL  - 55
IS  - 2
SP  - 380
EP  - 408
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139129575&doi=10.1080%2f01969722.2022.2122001&partnerID=40&md5=f310e17e409691477564f622323f5c31
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 23</p>
KW  - Adaptive honey badger algorithm
KW  - Agricultural sector
KW  - agriculture
KW  - Agriculture systems
KW  - Brain
KW  - Convolution
KW  - convolution neural long short-term memory
KW  - Convolution neural long short-term memory
KW  - Convolutional neural networks
KW  - Crops
KW  - Food products
KW  - internet of things
KW  - Internet of things
KW  - Learning frameworks
KW  - Long short-term memory
KW  - Metaheuristic
KW  - Object detection
KW  - pest identification and classification
KW  - Pest identification and classification
KW  - Pests images
KW  - smart agriculture system
KW  - Smart agriculture system
KW  - Smart agricultures
ER  - 

TY  - JOUR
TI  - A Hierarchical Deep Learning-Based Intrusion Detection Architecture for Clustered Internet of Things
AU  - Elsayed, R.
AU  - Hamada, R.
AU  - Hammoudeh, M.
AU  - Abdalla, M.
AU  - Elsaid, S.A.
T2  - Journal of Sensor and Actuator Networks
AB  - The Internet of Things (IoT) system’s ever-expanding attack surface calls for a new intrusion detection system (IDS). These systems may include thousands of wireless devices that need to be protected from cyberattacks. Recent research efforts used machine learning to analyze and identify various attacks and abnormal behavior on IoT systems. Most of these techniques are characterized by low accuracy and they do not scale to today’s IoT-enabled smart cities applications. This article proposes a secure automatic two-levels intrusion detection system (SATIDS) which utilizes the minimum redundancy maximum relevance (MRMR) feature selection technique and an enhanced version of long short-term memory (LSTM) based on an artificial recurrent neural network (RNN) to enhance the IDS performance. SATIDS aims at detecting traffic anomalies with greater accuracy while also reducing the time it takes to perform this task. The proposed algorithm was trained and evaluated using two of the most recent datasets based on realistic data: ToN-IoT and InSDN datasets. The performance analysis of the proposed system proves that it can differentiate between attacks and normal traffic, identify the attack category, and finally define the type of sub-attack with high accuracy. Comparing the performance of the proposed system with the existing IDSs reveals that it outperforms its best rivals from the literature in detecting many types of attacks. It improves accuracy, detection rates, F1-score, and precision. Using 500 hidden and two LSTM layers achieves accuracy of 97.5%, precision of 98.4%, detection rate of 97.9%, and F1-score of 98.05% on ToN-IoT dataset, and precision of 99%, detection rate of 99.6%, and F1-score of 99.3% on InSDN dataset. Finally, SATIDS was applied to an IoT network which utilizes the energy harvesting real-time routing protocol (EHRT). EHRT optimizes the low-energy adaptive clustering hierarchy (LEACH) routing technique using a modified artificial fish swarm algorithm. The integration between the optimized LEACH and the proposed IDS enhances the network lifetime, energy consumption, and security. © 2022 by the authors.
DA  - 2023///
PY  - 2023
DO  - 10.3390/jsan12010003
VL  - 12
IS  - 1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148581471&doi=10.3390%2fjsan12010003&partnerID=40&md5=1c89ac6cd0f22f31adc9ef8e5c29bd4b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 39</p>
KW  - artificial fish swarm algorithm (AFSA)
KW  - deep learning (DL)
KW  - energy harvesting real-time routing protocol (EHRT)
KW  - InSDN dataset
KW  - internet of things (IoT)
KW  - intrusion detection system (IDS)
KW  - long short-term memory (LSTM)
KW  - low-energy adaptive clustering hierarchy (LEACH)
KW  - ToN-IoT dataset
ER  - 

TY  - JOUR
TI  - SAD-IoT: Security Analysis of DDoS Attacks in IoT Networks
AU  - Kumar, P.
AU  - Bagga, H.
AU  - Netam, B.S.
AU  - Uduthalapally, V.
T2  - Wireless Personal Communications
AB  - Internet of Things is one of the most versatile technologies in existence today. It has taken over our day to day activities and thus has many applications that are designed to make life easier and simpler. Partly because IoT is new, it is replete with insecurities and vulnerabilities. Due to the lack of fundamental security controls, and the integration of real-world objects with the Internet, IoT devices are facile targets for cyber-criminals and other aggressors. This means that these vulnerabilities can be exploited for hacking, adding to Botnets, and then used to launch DoS and DDoS against organizations. To provide security from DoS and DDoS attacks, various solutions have been proposed. In this paper, Machine Learning, as well as Deep Learning algorithms, have been employed to analyze the DoS and DDoS attacks. The Bot-IoT dataset of the Centre of UNSW Canberra Cyber was used for training purposes. ARGUS software was used to generate the features from the pcap files of UNSW. A testbed was setup using 20 devices and generated dataset. From the result, the best accuracy of attack classification is 99.5% and 99.9% for Deep Learning and Machine Learning algorithms respectively. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
DA  - 2022///
PY  - 2022
DO  - 10.1007/s11277-021-08890-6
VL  - 122
IS  - 1
SP  - 87
EP  - 108
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113386313&doi=10.1007%2fs11277-021-08890-6&partnerID=40&md5=73856522d14209936c3a84befb26646c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 42</p>
KW  - Attack classifications
KW  - Attack detection
KW  - Cyber criminals
KW  - DDoS Attack
KW  - DDoS attacks
KW  - Deep learning
KW  - Deep Learning
KW  - Denial-of-service attack
KW  - Internet of things
KW  - Internet of Things (IoT)
KW  - IOT networks
KW  - Keras
KW  - Learning algorithms
KW  - Learning systems
KW  - Machine Learning
KW  - Network security
KW  - Personal computing
KW  - Real-world objects
KW  - Security analysis
KW  - Security controls
KW  - Training purpose
ER  - 

TY  - JOUR
TI  - Lorawansim: A flexible simulator for lorawan networks
AU  - Marini, R.
AU  - Mikhaylov, K.
AU  - Pasolini, G.
AU  - Buratti, C.
T2  - Sensors (Switzerland)
AB  - Among the low power wide area network communication protocols for large scale Internet of Things, LoRaWAN is considered one of the most promising, owing to its flexibility and energy-saving capabilities. For these reasons, during recent years, the scientific community has invested efforts into assessing the fundamental performance limits and understanding the trade-offs between the parameters and performance of LoRaWAN communication for different application scenarios. However, this task cannot be effectively accomplished utilizing only analytical methods, and precise network simulators are needed. To that end, this paper presents LoRaWANSim, a LoRaWAN simulator implemented in MATLAB, developed to characterize the behavior of LoRaWAN networks, accounting for physical, medium access control and network aspects. In particular, since many simulators described in the literature are deployed for specific research purposes, they are usually oversimplified and hold a number of assumptions affecting the accuracy of their results. In contrast, our simulator has been developed for the sake of completeness and it is oriented towards an accurate representation of the LoRaWAN at the different layers. After a detailed description of the simulator, we report a validation of the simulator itself and we then conclude by presenting some results of its use revealing notable and non-intuitive trade-offs present in LoRaWAN. Assuming the acceptance of the paper, the simulator will be made available via open access to the research community. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
DA  - 2021///
PY  - 2021
DO  - 10.3390/s21030695
VL  - 21
IS  - 3
SP  - 1
EP  - 19
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099686108&doi=10.3390%2fs21030695&partnerID=40&md5=328f8edd64e3ddb7bcd452373bd26244
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 57</p>
KW  - Analysis
KW  - Application scenario
KW  - Commerce
KW  - Economic and social effects
KW  - Energy conservation
KW  - Flexible simulators
KW  - Fundamental performance limits
KW  - Internet of things
KW  - Large scale Internet
KW  - LoRa
KW  - LoRaWAN
KW  - Low power electronics
KW  - LPWAN
KW  - MATLAB
KW  - Medium access control
KW  - Model
KW  - Network simulator
KW  - Network simulators
KW  - Performance
KW  - Research communities
KW  - Scientific community
KW  - Simulation
KW  - Simulators
KW  - Wide area network communication
KW  - Wide area networks
ER  - 

TY  - JOUR
TI  - Process Algebraic Approach for Probabilistic Verification of Safety and Security Requirements of Smart IoT (Internet of Things) Systems in Digital Twin
AU  - Song, J.
AU  - Lee, S.
AU  - Karagiannis, D.
AU  - Lee, M.
T2  - Sensors
AB  - Process algebra can be considered one of the most practical formal methods for modeling Smart IoT Systems in Digital Twin, since each IoT device in the systems can be considered as a process. Further, some of the algebras are applied to predict the behavior of the systems. For example, PALOMA (Process Algebra for Located Markovian Agents) and PACSR (Probabilistic Algebra of Communicating Shared Resources) process algebras are designed to predict the behavior of IoT Systems with probability on choice operations. However, there is a lack of analytical methods in the algebras to predict the nondeterministic behavior of the systems. Further, there is no control mechanism to handle undesirable nondeterministic behavior of the systems. In order to overcome these limitations, this paper proposes a new process algebra, called dTP-Calculus, which can be used (1) to specify the nondeterministic behavior of the systems with static probability, (2) verify the safety and security requirements of the nondeterministic behavior with probability requirements, and (3) control undesirable nondeterministic behavior with dynamic probability. To demonstrate the feasibility and practicality of the approach, the SAVE (Specification, Analysis, Verification, Evaluation) tool has been developed on the ADOxx Meta-Modeling Platform and applied to a SEMS (Smart Emergency Medical Service) example. In addition, a miniature digital twin system for the SEMS example was constructed and applied to the SAVE tool as a proof of concept for Digital Twin. It shows that the approach with dTP-Calculus on the tool can be very efficient and effective for Smart IoT Systems in Digital Twin. © 2024 by the authors.
DA  - 2024///
PY  - 2024
DO  - 10.3390/s24030767
VL  - 24
IS  - 3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184721934&doi=10.3390%2fs24030767&partnerID=40&md5=0b65e14254f9d2551d3af9b531797b33
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 5</p>
KW  - ADOxx
KW  - Algebra
KW  - Analysis verification
KW  - analytic method
KW  - article
KW  - Cryptography
KW  - digital twin
KW  - dTP-Calculus
KW  - DTP-calculus
KW  - emergency health service
KW  - Emergency services
KW  - female
KW  - Forecasting
KW  - formal method
KW  - Formal methods
KW  - internet of things
KW  - Internet of things
KW  - mathematics
KW  - Nondeterministic behavior
KW  - probability
KW  - process algebra
KW  - Process algebras
KW  - proof of concept
KW  - safety
KW  - Safety and securities
KW  - Safety requirements
KW  - SAVE
KW  - Security requirements
KW  - Smart internet of thing
KW  - smart IoT
KW  - Specification, analyse, verification, evaluation
KW  - stone formation
ER  - 

TY  - JOUR
TI  - Security issues of containerization of microservices
AU  - Ziyatbekova, G.Z.
AU  - Aralbayev, S.U.
AU  - Kisala, P.P.
T2  - KazUTB
DA  - 2023///
PY  - 2023
VL  - 4
IS  - 21
SP  - 1
EP  - 7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209185143&partnerID=40&md5=366014530dfcca3a276d4d9e2bd5c0ca
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Autonomous Vehicles: The Cybersecurity Vulnerabilities and Countermeasures for Big Data Communication
AU  - Algarni, A.
AU  - Thayananthan, V.
T2  - Symmetry
AB  - The possible applications of communication based on big data have steadily increased in several industries, such as the autonomous vehicle industry, with a corresponding increase in security challenges, including cybersecurity vulnerabilities (CVs). The cybersecurity-related symmetry of big data communication systems used in autonomous vehicles may raise more vulnerabilities in the data communication process between these vehicles and IoT devices. The data involved in the CVs may be encrypted using an asymmetric and symmetric algorithm. Autonomous vehicles with proactive cybersecurity solutions, power-based cyberattacks, and dynamic countermeasures are the modern issues/developments with emerging technology and evolving attacks. Research on big data has been primarily focused on mitigating CVs and minimizing big data breaches using appropriate countermeasures known as security solutions. In the future, CVs in data communication between autonomous vehicles (DCAV), the weaknesses of autonomous vehicular networks (AVN), and cyber threats to network functions form the primary security issues in big data communication, AVN, and DCAV. Therefore, efficient countermeasure models and security algorithms are required to minimize CVs and data breaches. As a technique, policies and rules of CVs with proxy and demilitarized zone (DMZ) servers were combined to enhance the efficiency of the countermeasure. In this study, we propose an information security approach that depends on the increasing energy levels of attacks and CVs by identifying the energy levels of each attack. To show the results of the performance of our proposed countermeasure, CV and energy consumption are compared with different attacks. Thus, the countermeasures can secure big data communication and DCAV using security algorithms related to cybersecurity and effectively prevent CVs and big data breaches during data communication. © 2022 by the authors.
DA  - 2022///
PY  - 2022
DO  - 10.3390/sym14122494
VL  - 14
IS  - 12
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144844081&doi=10.3390%2fsym14122494&partnerID=40&md5=0b036f761e47e0f743af617148645521
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 19</p>
KW  - autonomous vehicles
KW  - cybersecurity vulnerabilities
KW  - security solutions
KW  - vehicular communications
ER  - 

TY  - JOUR
TI  - Performance assessment and comparison of lightweight D2D-IoT communication protocols over resource constraint environment
AU  - Mishra, M.
AU  - Reddy, S.R.N.
T2  - Multimedia Tools and Applications
AB  - The Internet of Things (IoT) based smart strategies are often resource constrained with respect to energy, computation and memory. Outdated communication protocols are inappropriate for IoT ecosystem because of large overhead, lack of Quality of Service (QoS) and increased complexity. As billions of devices are required to be deployed over diverse applications, the IoT communication system acts as a prominent aspect and so the selection of suitable IoT communication protocol is highly essential. Also, there is a greater need of analysing the protocol behaviour under diverse network conditions. Hence to select a suitable protocol by addressing the limitations, this research paper mainly focuses on comparing lightweight application layer protocols, including Message Queuing Telemetry Transport (MQTT), Constrained Application Protocol (CoAP) and MQTT for sensor Network (MQTTSN). Evaluating the performance of protocol libraries in real environment is highly significant because it helps to discover potential interoperability and compatibility challenges. Also, it can reveal the protocol’s ability in handling scalability and its support in dealing a number of devices efficiently. A testbed named “ProtoLab” has been created for evaluating the performances of CoAP, MQTT and MQTTSN protocols under variable network condition. Using the testbed, the client and server can exchange the data packets under the variable network condition created with the help of network emulator. The data packets can be received and exported using the wireshark application to create a dataset for analysis. Different parameters like round trip time, duplication, round trip reliability, server response time, reliability towards the client to server and client overhead are analysed by configuring loss, corruption, reordering and network delay in the network emulator using wide area network emulator (WANEM) to evaluate the performance of IoT communication protocols. Variable network conditions are considered and analysed using real-time ProtoLab testbed by varying the parameters. The results and observations analyzed through this research can support IoT application developers in making informed decisions while selecting communication protocols for different applications. On analysing the parameters under diverse network conditions, the MQTTSN protocol performs comparatively better in terms of resource efficient delivery in constrained environment. Meanwhile, the MQTT protocol is analysed to be better when concerned with reliability. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.
DA  - 2024///
PY  - 2024
DO  - 10.1007/s11042-024-18132-z
VL  - 83
IS  - 26
SP  - 67569
EP  - 67598
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183125753&doi=10.1007%2fs11042-024-18132-z&partnerID=40&md5=c4e18db4d2cdfa9b5c8ad835bdef5f54
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 6</p>
KW  - Application protocols
KW  - Communications protocols
KW  - Constrained application protocol resource constraint
KW  - Internet of things
KW  - Internet protocols
KW  - Message queuing telemetry transport
KW  - Message Queuing Telemetry Transport
KW  - Network condition
KW  - Network emulators
KW  - Performance
KW  - Protocol stack
KW  - Quality of service
KW  - Quality of Service
KW  - Quality-of-service
KW  - Reliability
KW  - Resource Constraint
KW  - Sensor networks
KW  - Telemetering equipment
KW  - Testbeds
KW  - Wide area networks
ER  - 

TY  - JOUR
TI  - Ontology driven testing strategies for IoT applications
AU  - Naqvi, M.R.
AU  - Iqbal, M.W.
AU  - Ashraf, M.U.
AU  - Ahmad, S.
AU  - Soliman, A.T.
AU  - Khurram, S.
AU  - Shafiq, M.
AU  - Choi, J.-G.
T2  - Computers, Materials and Continua
AB  - Internet-of-Things (IoT) has attained a major share in embedded software development. The new era of specialized intelligent systems requires adaptation of customized software engineering approaches. Currently, software engineering has merged the development phases with the technologies provided by industrial automation. The improvements are still required in testing phase for the software developed to IoT solutions. This research aims to assist in developing the testing strategies for IoT applications, therein ontology has been adopted as a knowledge representation technique to different software engineering processes. The proposed ontological model renders 101 methodology by using Protégé. After completion, the ontology was evaluated in three-dimensional view by the domain experts of software testing, IoT and ontology engineering. Satisfied results of the research are showed in interest of the specialists regarding proposed ontology development and suggestions for improvements. The Proposed reasoning-based ontological model for development of testing strategies in IoT application contributes to increase the general understanding of tests in addition to assisting for the development of testing strategies for different IoT devices. © 2022 Tech Science Press. All rights reserved.
DA  - 2022///
PY  - 2022
DO  - 10.32604/cmc.2022.019188
VL  - 70
IS  - 3
SP  - 5855
EP  - 5869
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117021689&doi=10.32604%2fcmc.2022.019188&partnerID=40&md5=fdda5a9bcae6305ad586e8700f61aff5
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 20</p>
KW  - Application programs
KW  - Automation
KW  - Customized software
KW  - Development phasis
KW  - Embedded software development
KW  - Industrial automation
KW  - Intelligent systems
KW  - Internet of things
KW  - Knowledge representation
KW  - Knowledge-representation
KW  - Ontological modeling
KW  - Ontology
KW  - Ontology's
KW  - Planning
KW  - Software design
KW  - Software testing
KW  - Software testings
KW  - Testing phase
KW  - Testing strategies
ER  - 

TY  - JOUR
TI  - Message Queuing Telemetry Transport (MQTT) Security: A Cryptographic Smart Card Approach
AU  - Sanjuan, E.B.
AU  - Cardiel, I.A.
AU  - Cerrada, J.A.
AU  - Cerrada, C.
T2  - IEEE Access
AB  - The Message Queuing Telemetry Transport (MQTT) protocol is one of the most extended protocols on the Internet of Things (IoT). However, this protocol does not implement a strong security scheme by default, which does not allow a secure authentication mechanism between participants in the communication. Furthermore, we cannot trust the confidentiality and integrity of data. Lightweight IoT devices send more and more sensible data in areas of Smart Building, Smart City, Smart House, Smart Car, Connected Car, Health Care, Smart Retail, Industrial IoT (IIoT), etc. This makes the security challenges in the protocols used in the IoT particularly important. The standard of MQTT protocol strongly recommends implement it over Transport Layer Security (TLS) instead of plain TCP. Nonetheless, this option is not possible in most lightweight devices that make up the IoT ecosystem. Quite often, the constrained resources of IoT devices prevent the use of secure asymmetric cryptography algorithms implemented by themselves. In this article, we propose making a security schema in MQTT protocol using Cryptographic Smart Cards, for both challenges, the authentication schema and the trusted data confidentiality and data integrity. We carry out this security schema without modifying the standard protocol messages. And finally, we present a time results experiment using an example implementation model with JavaCard library. © 2013 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ACCESS.2020.3003998
VL  - 8
SP  - 115051
EP  - 115062
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087834582&doi=10.1109%2fACCESS.2020.3003998&partnerID=40&md5=d349786d7ad7e3cd153b3d7a3868e484
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 54</p>
KW  - Asymmetric cryptography
KW  - Authentication
KW  - Constrained resources
KW  - Cryptography
KW  - Data confidentiality
KW  - Hardware security
KW  - Implementation models
KW  - Industrial internet of things (IIoT)
KW  - Internet of thing (IOT)
KW  - Internet of Things (IoT)
KW  - javacard
KW  - message queuing telemetry transport (MQTT)
KW  - mutual authentication
KW  - Network security
KW  - Secure authentications
KW  - Security challenges
KW  - smart card
KW  - Smart cards
KW  - Telemetering equipment
KW  - Transport layer security
ER  - 

TY  - JOUR
TI  - An anomaly mitigation framework for iot using fog computing
AU  - Lawal, M.A.
AU  - Shaikh, R.A.
AU  - Hassan, S.R.
T2  - Electronics (Switzerland)
AB  - The advancement in IoT has prompted its application in areas such as smart homes, smart cities, etc., and this has aided its exponential growth. However, alongside this development, IoT networks are experiencing a rise in security challenges such as botnet attacks, which often appear as network anomalies. Similarly, providing security solutions has been challenging due to the low resources that characterize the devices in IoT networks. To overcome these challenges, the fog computing paradigm has provided an enabling environment that offers additional resources for deploying security solutions such as anomaly mitigation schemes. In this paper, we propose a hybrid anomaly mitigation framework for IoT using fog computing to ensure faster and accurate anomaly detection. The framework employs signature-and anomaly-based detection methodologies for its two modules, respectively. The signature-based module utilizes a database of attack sources (blacklisted IP addresses) to ensure faster detection when attacks are executed from the blacklisted IP address, while the anomaly-based module uses an extreme gradient boosting algorithm for accurate classification of network traffic flow into normal or abnormal. We evaluated the performance of both modules using an IoT-based dataset in terms response time for the signature-based module and accuracy in binary and multiclass classification for the anomaly-based module. The results show that the signature-based module achieves a fast attack detection of at least six times faster than the anomaly-based module in each number of instances evaluated. The anomaly-based module using the XGBoost classifier detects attacks with an accuracy of 99% and at least 97% for average recall, average precision, and average F1 score for binary and multiclass classification. Additionally, it recorded 0.05 in terms of false-positive rates. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.
DA  - 2020///
PY  - 2020
DO  - 10.3390/electronics9101565
VL  - 9
IS  - 10
SP  - 1
EP  - 24
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091643039&doi=10.3390%2felectronics9101565&partnerID=40&md5=f341fcd436c906761deb00e660cbd271
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 57</p>
KW  - Anomaly mitigation
KW  - Classification algorithms
KW  - Fog computing
KW  - Internet of things (IoT)
KW  - Intrusion Detection System (IDS)
ER  - 

TY  - JOUR
TI  - A novel approach to feature collection for anomaly detection in Kubernetes environment and agent for metrics collection from Kubernetes nodes
AU  - Darwesh, G.
AU  - Hammoud, J.
AU  - Vorobeva, A.A.
T2  - Scientific and Technical Journal of Information Technologies, Mechanics and Optics
AB  - Kubernetes is a widely adopted open-source platform for managing containerized workloads and deploying applications in a microservices architecture. Despite its popularity, Kubernetes has faced numerous security challenges; deployments using Kubernetes are vulnerable to security risks. The current solutions for detecting anomalous behavior within a Kubernetes cluster lack real-time detection capabilities allowing hackers to exploit vulnerabilities and cause damage to production assets. This study aims to address these security concerns by proposing a new approach and novel agent to feature collection for anomaly detection in Kubernetes environment. It is proposed to use metrics (related to disk usage, CPU and network) collected by node exporter (Prometeus) directly from Kubernetes nodes. The simulation was conducted in a real-world production Kubernetes environment hosted on the Microsoft Azure, with results indicating the agent success in collecting 24 security metrics in a short amount of time. These metrics can be used to create a labeled time-series dataset of anomalies produced by microservices, enabling real-time detection of attacks based on the behavior of compromised nodes within the Kubernetes cluster. The proposed approach and developed agent for monitoring can be used to generate datasets for training anomaly detection models in the Kubernetes environment, based on artificial intelligence technologies, in real-time mode. The obtained results will be useful for researchers and specialists in the field of Kubernetes cybersecurity. © 2023 Mechanics and Optics. All rights reserved.
DA  - 2023///
PY  - 2023
DO  - 10.17586/2226-1494-2023-23-3-538-546
VL  - 23
IS  - 3
SP  - 538
EP  - 546
ST  - Новый способ сбора данных для обнаружения аномального поведения в среде Kubernetes и агент для сбора метрик с узлов
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166756074&doi=10.17586%2f2226-1494-2023-23-3-538-546&partnerID=40&md5=6b89b4dde71458c7febc883a475b9662
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3</p>
KW  - anomalies detection
KW  - attack detection
KW  - Kubernetes
KW  - Kubernetes monitoring
KW  - security
ER  - 

TY  - JOUR
TI  - Utilizing technologies of fog computing in educational IoT systems: privacy, security, and agility perspective
AU  - Adel, A.
T2  - Journal of Big Data
AB  - Fog computing architecture is referred to the architecture that is distributed over the geographical area. This architectural arrangement mainly focuses on physical and logical network elements, and software for the purpose of implementing proper network. Fog computing architecture allows the users to have a flexible communication and also ensures that the storage services are maintained efficiently for the purpose of managing the data. However, it has been observed that in the field of education fog computing architecture has gained huge importance due to its real time application feature. The main objective of the survey is to develop a systematic literature review for the technology of fog computing in the education IoT system. The survey will also focus on evaluating the essential factors that has a crucial role in the fields of education as well as investigating the limitation and findings associated with the fog computing technologies in educational systems from the perspective of privacy, security, and agility. © 2020, The Author(s).
DA  - 2020///
PY  - 2020
DO  - 10.1186/s40537-020-00372-z
VL  - 7
IS  - 1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095934542&doi=10.1186%2fs40537-020-00372-z&partnerID=40&md5=8587c12e3e14f0d2b6ed514adbb3e655
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 36</p>
KW  - Computer architecture
KW  - Computing architecture
KW  - Digital storage
KW  - Edge computing
KW  - Education system
KW  - Education systems
KW  - Flexible communication
KW  - Fog
KW  - Fog computing
KW  - Geographical area
KW  - Internet of things
KW  - IoT
KW  - Logical network
KW  - Network architecture
KW  - Network element
KW  - Physical network
KW  - Virtualization
KW  - Virtualizations
ER  - 

TY  - JOUR
TI  - Potential usage of fly ash and nano silica in high-strength concrete: Laboratory experiment and application in rigid pavement
AU  - Tran, H.-B.
AU  - Phan, V.T.-A.
T2  - Case Studies in Construction Materials
AB  - High-strength concrete (HSC) using supplementary cementitious material of 30% fly ash (FA) and various nano silica (NS) contents were investigated in this study. FA and NS are waste materials collected from local thermal plants and rice husk ash in Southern Vietnam, respectively. Different concentrations of NS at 0%, 0.5%, 1.0%, and 1.5% were incorporated into the base mixture in place of cement and FA. The water-to-binder ratio of 0.32 was constant for all the mixtures. Results indicated that the combination binder of cement, FA, and NS satisfied the setting time as pure cement binder. The slump test value was in the range of 30–40 mm by adjusting the superplasticzer. Mechanical properties of HSC were studied at various curing ages of 3, 7, 28, and 56 days, including compressive strength, flexural strength, elastic modulus, and abrasion resistance. Results indicated that 1% NS can be considered as optimum content for preparing an HSC mixture, which showed the best mechanical properties. Furthermore, a good correlation between compressive strength and flexural strength was obtained from the results. Finally, rigid pavement calculation revealed that the addition of 1% NS resulted in a reduction in the thickness of the concrete slab by 30 mm (around 10.7%) compared to non-NS concrete. These findings suggest that incorporating nano-silica into concrete can lead to thinner concrete slabs with improved performance characteristics. © 2024 The Authors
DA  - 2024///
PY  - 2024
DO  - 10.1016/j.cscm.2024.e02856
VL  - 20
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182258179&doi=10.1016%2fj.cscm.2024.e02856&partnerID=40&md5=1791ade1e036b0dcb38d142bbbf74a43
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 23</p>
KW  - Bending strength
KW  - Cements
KW  - Compressive strength
KW  - Concrete mixtures
KW  - Concrete slabs
KW  - Fly ash
KW  - High performance concrete
KW  - High strength concretes
KW  - High-strength concrete
KW  - Laboratory experiments
KW  - Mechanical properties
KW  - Nano silica
KW  - Nano Silica
KW  - Pavements
KW  - Rice-husk ash
KW  - Rigid pavement
KW  - Rigid pavements
KW  - Setting
KW  - Silica
KW  - Silica content
KW  - Supplementary cementitious material
KW  - Thermal plants
KW  - Viet Nam
KW  - Water-to-binder ratio
ER  - 

TY  - CONF
TI  - Erms: Efficient Resource Management for Shared Microservices with SLA Guarantees
AU  - Luo, S.
AU  - Xu, H.
AU  - Ye, K.
AU  - Xu, G.
AU  - Zhang, L.
AU  - He, J.
AU  - Yang, G.
AU  - Xu, C.
T2  - International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS
AB  - A common approach to improving resource utilization in data centers is to adaptively provision resources based on the actual workload. One fundamental challenge of doing this in microservice management frameworks, however, is that different components of a service can exhibit significant differences in their impact on end-to-end performance. To make resource management more challenging, a single microservice can be shared by multiple online services that have diverse workload patterns and SLA requirements. We present an efficient resource management system, namely Erms, for guaranteeing SLAs in shared microservice environments. Erms profiles microservice latency as a piece-wise linear function of the workload, resource usage, and interference. Based on this profiling, Erms builds resource scaling models to optimally determine latency targets for microservices with complex dependencies. Erms also designs new scheduling policies at shared microservices to further enhance resource efficiency. Experiments across microservice benchmarks as well as trace-driven simulations demonstrate that Erms can reduce SLA violation probability by 5× and more importantly, lead to a reduction in resource usage by 1.6×, compared to state-of-the-art approaches.  © 2022 ACM.
DA  - 2023///
PY  - 2023
DO  - 10.1145/3567955.3567964
VL  - 1
SP  - 62
EP  - 77
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145590582&doi=10.1145%2f3567955.3567964&partnerID=40&md5=766ccd04af408fccedd66abdced59467
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 41</p>
KW  - Datacenter
KW  - End-to-end performance
KW  - Management frameworks
KW  - Natural resources management
KW  - On-line service
KW  - Resource allocation
KW  - Resource management
KW  - Resource Management
KW  - Resource usage
KW  - Resources based
KW  - Resources utilizations
KW  - Shared microservice
KW  - Shared Microservices
KW  - SLA guarantee
KW  - SLA Guarantees
ER  - 

TY  - JOUR
TI  - Ant Colony Algorithm for Multi-Objective Optimization of Container-Based Microservice Scheduling in Cloud
AU  - Lin, M.
AU  - Xi, J.
AU  - Bai, W.
AU  - Wu, J.
T2  - IEEE Access
AB  - In cloud architectures, the microservice model divides an application into a set of loosely coupled and collaborative fine-grained services. As a lightweight virtualization technology, the container supports the encapsulation and deployment of microservice applications. Despite a large number of solutions and implementations, there remain open issues that have not been completely addressed in the deployment and management of the microservice containers. An effective method for container resource scheduling not only satisfies the service requirements of users but also reduces the running overhead and ensures the performance of the cluster. In this paper, a multi-objective optimization model for the container-based microservice scheduling is established, and an ant colony algorithm is proposed to solve the scheduling problem. Our algorithm considers not only the utilization of computing and storage resources of the physical nodes but also the number of microservice requests and the failure rate of the physical nodes. Our algorithm uses the quality evaluation function of the feasible solutions to ensure the validity of pheromone updating and combines multi-objective heuristic information to improve the selection probability of the optimal path. By comparing with other related algorithms, the experimental results show that the proposed optimization algorithm achieves better results in the optimization of cluster service reliability, cluster load balancing, and network transmission overhead. © 2013 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/ACCESS.2019.2924414
VL  - 7
SP  - 83088
EP  - 83100
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068797342&doi=10.1109%2fACCESS.2019.2924414&partnerID=40&md5=c1956fbc2c5eb82d3bc88c87d86cf3c6
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 122</p>
KW  - Ant colony algorithm
KW  - Ant colony algorithms
KW  - Ant colony optimization
KW  - Cloud computing
KW  - Clustering algorithms
KW  - Container scheduling
KW  - Containers
KW  - Failure analysis
KW  - Genetic algorithms
KW  - Heuristic information
KW  - Microservices
KW  - Multi-objective optimization
KW  - Multi-objective optimization models
KW  - Multiobjective optimization
KW  - Optimization algorithms
KW  - Scheduling
KW  - Selection probabilities
KW  - Virtualization technologies
ER  - 

TY  - CONF
TI  - Computation offloading for mobile edge computing: A deep learning approach
AU  - Yu, S.
AU  - Wang, X.
AU  - Langar, R.
T2  - IEEE International Symposium on Personal, Indoor and Mobile Radio Communications, PIMRC
AB  - Computation offloading has already shown itself to be successful for enabling resource-intensive applications on mobile devices. Moreover, in view of mobile edge computing (MEC) system, mobile devices can offload compute-intensive tasks to a nearby cloudlet, so as to save the energy and enhance the processing speed. However, due to the varying network conditions and limited computation resources of cloudlets, the offloading actions taken by a mobile user may not achieve the lowest cost. In this paper, we develop a dynamic offloading framework for mobile users, considering the local overhead in the mobile terminal side, as well as the limited communication and computation resources in the network side. We formulate the offloading decision problem as a multi-label classification problem and develop the Deep Supervised Learning (DSL) method to minimize the computation and offloading overhead. Simulation results show that our proposal can reduce system cost up to 49.24%, 23.87%, 15.69%, and 11.18% compared to the "no offloading" scheme, "random offloading" scheme, "total offloading" scheme and "multi-label linear classifier-based offloading" scheme, respectively. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/PIMRC.2017.8292514
VL  - 2017-October
SP  - 1
EP  - 6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045273064&doi=10.1109%2fPIMRC.2017.8292514&partnerID=40&md5=3378e12c00da12a403ad544698cbe98f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 160</p>
KW  - Classification (of information)
KW  - computation offloading
KW  - Computation offloading
KW  - Computation resources
KW  - Compute-intensive tasks
KW  - Computing system
KW  - Deep learning
KW  - Energy
KW  - Learning approach
KW  - Low-costs
KW  - Mobile edge computing
KW  - Mobile telecommunication systems
KW  - Mobile users
KW  - Network condition
KW  - Processing speed
ER  - 

TY  - JOUR
TI  - GA-Par: Dependable Microservice Orchestration Framework for Geo-Distributed Clouds
AU  - Wen, Z.
AU  - Lin, T.
AU  - Yang, R.
AU  - Ji, S.
AU  - Ranjan, R.
AU  - Romanovsky, A.
AU  - Lin, C.
AU  - Xu, J.
T2  - IEEE Transactions on Parallel and Distributed Systems
AB  - Recent advances in composing Cloud applications have been driven by deployments of inter-networking heterogeneous microservices across multiple Cloud datacenters. System dependability has been of the upmost importance and criticality to both service vendors and customers. Security, a measurable attribute, is increasingly regarded as the representative example of dependability. Literally, with the increment of microservice types and dynamicity, applications are exposed to aggravated internal security threats and externally environmental uncertainties. Existing work mainly focuses on the QoS-aware composition of native VM-based Cloud application components, while ignoring uncertainties and security risks among interactive and interdependent container-based microservices. Still, orchestrating a set of microservices across datacenters under those constraints remains computationally intractable. This paper describes a new dependable microservice orchestration framework GA-Par to effectively select and deploy microservices whilst reducing the discrepancy between user security requirements and actual service provision. We adopt a hybrid (both whitebox and blackbox based) approach to measure the satisfaction of security requirement and the environmental impact of network QoS on system dependability. Due to the exponential grow of solution space, we develop a parallel Genetic Algorithm framework based on Spark to accelerate the operations for calculating the optimal or near-optimal solution. Large-scale real world datasets are utilized to validate models and orchestration approach. Experiments show that our solution outperforms the greedy-based security aware method with 42.34 percent improvement. GA-Par is roughly 4x faster than a Hadoop-based genetic algorithm solver and the effectiveness can be constantly guaranteed under different application scales. © 1990-2012 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/TPDS.2019.2929389
VL  - 31
IS  - 1
SP  - 129
EP  - 143
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077089009&doi=10.1109%2fTPDS.2019.2929389&partnerID=40&md5=79383bc854066cbcb1850be77d2887db
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 40</p>
KW  - dependability
KW  - Environmental impact
KW  - Environmental uncertainty
KW  - Genetic algorithms
KW  - Large dataset
KW  - microservice
KW  - Near-optimal solutions
KW  - Parallel genetic algorithms
KW  - Quality of service
KW  - Security requirements
KW  - Service orchestration
KW  - System dependability
ER  - 

TY  - JOUR
TI  - An open IoT framework based on microservices architecture
AU  - Sun, L.
AU  - Li, Y.
AU  - Memon, R.A.
T2  - China Communications
AB  - With the continuous development and evolvement of Internet of Things (IoT), monolithic application becomes much larger in scale and even more complex in structure. This leads to poor scalability, extensibility and maintainability. In response to those challenges, microservice architecture has been introduced in the field of IoT application, due to its flexibility, lightweight and loose coupling. However, the existing IoT framework of microservice mainly focus on a specific domain, therefore, this greatly limits its application. In this paper, we propose a general microservice system framework for the IoT application, which is a better scalable, extendable and maintainable architecture. We introduce its system design and related microservices, and emphasize on core service and device communication from service layer to physical layer. It has better capacity to support interoperability and accommodate heterogeneous objects. In addition, this framework can easily achieve more application integration such as automation, intelligence, Geo service and Big Data. © 2013 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/CC.2017.7868163
VL  - 14
IS  - 2
SP  - 154
EP  - 162
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015161312&doi=10.1109%2fCC.2017.7868163&partnerID=40&md5=1e8bc5d8fbd474a9854e3989393ad074
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 119</p>
KW  - Internet of things
KW  - IoT
KW  - micro service
KW  - software architecture
ER  - 

TY  - JOUR
TI  - iFogSim2: An extended iFogSim simulator for mobility, clustering, and microservice management in edge and fog computing environments
AU  - Mahmud, R.
AU  - Pallewatta, S.
AU  - Goudarzi, M.
AU  - Buyya, R.
T2  - Journal of Systems and Software
AB  - Internet of Things (IoT) has already proven to be the building block for next-generation Cyber–Physical Systems (CPSs). The considerable amount of data generated by the IoT devices needs latency-sensitive processing, which is not feasible by deploying the respective applications in remote Cloud datacentres. Edge/Fog computing, a promising extension of Cloud at the IoT-proximate network, can meet such requirements for smart CPSs. However, the structural and operational differences of Edge/Fog infrastructure resist employing Cloud-based service regulations directly to these environments. As a result, many research works have been recently conducted, focusing on efficient application and resource management in Edge/Fog computing environments. Scalable Edge/Fog infrastructure is a must to validate these policies, which is also challenging to accommodate in the real-world due to high cost and implementation time. Considering simulation as a key to this constraint, various software have been developed that can imitate the physical behavior of Edge/Fog computing environments. Nevertheless, the existing simulators often fail to support advanced service management features because of their monolithic architecture, lack of actual dataset, and limited scope for a periodic update. To overcome these issues, we have developed modular simulation models for service migration, dynamic distributed cluster formation, and microservice orchestration for Edge/Fog computing based on real datasets and extended the basic components of iFogSim, a widely used Edge/Fog computing simulator for their ease of adoption as iFogSim2. The performance of iFogSim2 and its built-in service management policies are evaluated using three use case scenarios and compared with the contemporary simulators and benchmark policies under different settings. Results indicate that our simulator consumes less memory and minimizes simulation time by an average of 28% when compared to other simulators. © 2022 Elsevier Inc.
DA  - 2022///
PY  - 2022
DO  - 10.1016/j.jss.2022.111351
VL  - 190
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130094579&doi=10.1016%2fj.jss.2022.111351&partnerID=40&md5=b650072584d3ec15cdfca92a7638dd74
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 167</p>
KW  - Benchmarking
KW  - Building blockes
KW  - Cluster computing
KW  - Clustering
KW  - Clusterings
KW  - Computer software
KW  - Computing environments
KW  - Cybe-physical systems
KW  - Cyber-physical systems
KW  - Datacenter
KW  - Edge/fog computing
KW  - Edge/Fog computing
KW  - Environmental management
KW  - Fog computing
KW  - Internet of things
KW  - Internet of Things
KW  - Microservice
KW  - Microservices
KW  - Mobility
KW  - Simulation
KW  - Simulators
ER  - 

TY  - CONF
TI  - Asynchronous methods for deep reinforcement learning
AU  - Mnih, V.
AU  - Badia, A.P.
AU  - Mirza, L.
AU  - Graves, A.
AU  - Harley, T.
AU  - Lillicrap, T.P.
AU  - Silver, D.
AU  - Kavukcuoglu, K.
T2  - 33rd International Conference on Machine Learning, ICML 2016
AB  - Wc propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input. © 2016 by the author(s).
DA  - 2016///
PY  - 2016
VL  - 4
SP  - 2850
EP  - 2869
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999036937&partnerID=40&md5=e49f5cfd9d8c4eec69ab63009c3d8037
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2016</p>
KW  - Artificial intelligence
KW  - Asynchronous methods
KW  - Deep neural networks
KW  - Gradient descent
KW  - Learning algorithms
KW  - Learning systems
KW  - Lightweight frameworks
KW  - Multi-core cpus
KW  - Neural network controllers
KW  - Reinforcement learning
KW  - Stabilizing effects
KW  - State of the art
ER  - 

TY  - CONF
TI  - Reinforcement Learning for Task Placement in Collaborative Cloud- Edge Computing
AU  - Zhou, P.
AU  - Wu, G.
AU  - Alzahrani, B.
AU  - Barnawi, A.
AU  - Alhindi, A.
AU  - Chen, M.
T2  - Proceedings - IEEE Global Communications Conference, GLOBECOM
AB  - With the advantage of being close to the network, edge cloud-enabled computing mode brings flexibility to task scheduling. However, with the heterogeneity of computing resources between cloud and edge cloud, and the complexity of computing and communication processes between multi-edge cloud, challenges have been brought to the deployment and computing of tasks in cloud-edge collaborative environments. In order to solve this challenge, firstly a deep reinforcement learning controller based cloud-edge collaborative computing framework has been proposed. Then a system QoS model has been estab-lished considering both the user benefits and the service provider benefits. By using deep Q-network, a deep reinforcement learning based collaborative task placement algorithm has been proposed for dynamically optimizing the target system utility. Finally, the experimental results show that the proposed method has a good learning ability for the computing cost of cloud and edge cloud as well as the communication cost between multi-edge cloud. In addition, compared with Q-table learning, random computing and cloud computing, a 10% improvement of system utility has been achieved with the proposed method. © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/GLOBECOM46510.2021.9685049
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184631292&doi=10.1109%2fGLOBECOM46510.2021.9685049&partnerID=40&md5=de6fe62523754f051477463d8b713173
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 12</p>
KW  - Collaborative cloud-edge computing
KW  - Collaborative Cloud-Edge Computing
KW  - Crowd Management
KW  - Crowd managements
KW  - Deep learning
KW  - Edge clouds
KW  - Edge computing
KW  - Learning systems
KW  - Multi-edge cloud
KW  - Multi-Edge Cloud
KW  - Network edges
KW  - Reinforcement learning
KW  - Reinforcement Learning
KW  - Reinforcement learnings
KW  - Resource Allocation
KW  - Resources allocation
KW  - Task placement
KW  - Task Placement
KW  - Tasks scheduling
ER  - 

TY  - CHAP
TI  - Deep Reinforcement Learning: An Overview
AU  - Mousavi, S.S.
AU  - Schukat, M.
AU  - Howley, E.
T2  - Lecture Notes in Networks and Systems
AB  - In recent years, a specific machine learning method called deep learning has gained huge attraction, as it has obtained astonishing results in broad applications such as pattern recognition, speech recognition, computer vision, and natural language processing. Recent research has also been shown that deep learning techniques can be combined with reinforcement learning methods to learn useful representations for the problems with high dimensional raw data input. This article reviews the recent advances in deep reinforcement learning with focus on the most used deep architectures such as autoencoders, convolutional neural networks and recurrent neural networks which have successfully been come together with the reinforcement learning framework. © Springer International Publishing AG 2018.
DA  - 2018///
PY  - 2018
VL  - 16
SP  - 426
EP  - 440
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054286523&doi=10.1007%2f978-3-319-56991-8_32&partnerID=40&md5=72571879e22527b6864886d1fecbe3c9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 230</p>
KW  - Deep leaning
KW  - MDPs
KW  - Neural networks
KW  - Observable MDPs
KW  - Reinforcement learning
ER  - 

TY  - JOUR
TI  - Edge Intelligence: Paving the Last Mile of Artificial Intelligence With Edge Computing
AU  - Zhou, Z.
AU  - Chen, X.
AU  - Li, E.
AU  - Zeng, L.
AU  - Luo, K.
AU  - Zhang, J.
T2  - Proceedings of the IEEE
AB  - | With the breakthroughs in deep learning, the recent years have witnessed a booming of artificial intelligence (AI) applications and services, spanning from personal assistant to recommendation systems to video/audio surveillance. More recently, with the proliferation of mobile computing and Internet of Things (IoT), billions of mobile and IoT devices are connected to the Internet, generating zillions bytes of data at the network edge. Driving by this trend, there is an urgent need to push the AI frontiers to the network edge so as to fully unleash the potential of the edge big data. To meet this demand, edge computing, an emerging paradigm that pushes computing tasks and services from the network core to the network edge, has been widely recognized as a promising solution. The resulted new interdiscipline, edge AI or edge intelligence (EI), is beginning to receive a tremendous amount of interest. However, research on EI is still in its infancy stage, and a dedicated venue for exchanging the recent advances of EI is highly desired by both the computer system and AI communities. To this end, we conduct a comprehensive survey of the recent research efforts on EI. Specifically, we first review the background and motivation for AI running at the network edge. We then provide an overview of the overarching architectures, frameworks, and emerging key technologies for deep learning model toward training/inference at the network edge. Finally, we discuss future research opportunities on EI. We believe that this survey will elicit escalating attentions, stimulate fruitful discussions, and inspire further research ideas on EI. © 2019 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.
DA  - 2019///
PY  - 2019
DO  - 10.1109/JPROC.2019.2918951
VL  - 107
IS  - 8
SP  - 1738
EP  - 1762
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067598102&doi=10.1109%2fJPROC.2019.2918951&partnerID=40&md5=ad91a6e8db4dce26e5c696ede789d627
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1757</p>
KW  - Artificial intelligence
KW  - Big data
KW  - Computational modelling
KW  - Computer architecture
KW  - deep learning
KW  - Deep learning
KW  - edge computing
KW  - Edge computing
KW  - edge intelligence
KW  - Edge intelligence
KW  - Edge intelligence.
KW  - Internet of things
KW  - Job analysis
KW  - Last mile
KW  - Network edges
KW  - Security systems
KW  - Surveys
KW  - Task analysis
ER  - 

TY  - CONF
TI  - GMA: Graph Multi-agent Microservice Autoscaling Algorithm in Edge-Cloud Environment
AU  - Tong, G.
AU  - Meng, C.
AU  - Song, S.
AU  - Pan, M.
AU  - Yu, Y.
T2  - Proceedings - 2023 IEEE International Conference on Web Services, ICWS 2023
AB  - The emerging edge-cloud computing paradigm, comprising cloud centers and multiple distributed edge servers, extends the computing capability from the cloud center to a range of servers. Although the microservice autoscaling problem has been intensively studied in the context of cloud computing, existing algorithms in most cases cannot be effectively migrated to the edge-cloud environment because servers are geographically distributed and heterogeneous, and information is not synchronized between servers. Existing works, however, mainly focus on centralized strategies with time-consuming synchronization methods, i.e. strategies shared by all servers, without comprehensively considering the heterogeneity and distribution of the environment. Soft information synchronization, autonomy and collaboration is proposed to tackle the aforementioned issues, and refer to it as SAC paradigm. According to the SAC paradigm, each server with inferred information of other servers can collaborate with others by a dedicated autoscaling strategy, that is, server collaboration. The microservice autoscaling problem is then transformed into the Graph-based Jointly Microservice Autoscaling (GJMA) problem based on spectral graph theory. GJMA problem aims to minimize average waiting time of microservice-based application while reducing service-level agreement(SLA) violation rate and fluctuations in the autoscaling process, taking into account resource heterogeneity. Graph-based Multi-agent Algorithm(GMA), an implementation of SAC paradigm based on graph convolutional networks and multi-agent reinforcement learning, is implemented to solve GJMA problem. Experimental results show that the proposed algorithm for the edge-cloud environment is always efficient to find a better autoscaling strategy compared to the implemented comparison algorithms. © 2023 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/ICWS60048.2023.00058
SP  - 393
EP  - 404
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173838267&doi=10.1109%2fICWS60048.2023.00058&partnerID=40&md5=4a7180e1b5dfdd272ac4abd1d90905b0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 6</p>
KW  - Autoscaling
KW  - Cloud computing
KW  - Cloud environments
KW  - Computation theory
KW  - Convolution
KW  - Convolutional networks
KW  - Edge clouds
KW  - Edge-cloud environment
KW  - Graph convolutional network
KW  - Graph convolutional networks
KW  - Graph theory
KW  - Graphic methods
KW  - Learning algorithms
KW  - Microservice autoscaling
KW  - Multi agent
KW  - Multi agent systems
KW  - Multi-agent reinforecment learning
KW  - Reinforcement learning
KW  - Server collaboration
KW  - Software agents
KW  - Synchronization
ER  - 

TY  - CONF
TI  - Microservices-based IoT application placement within heterogeneous and resource constrained fog computing environments
AU  - Pallewatta, S.
AU  - Kostakos, V.
AU  - Buyya, R.
T2  - UCC 2019 - Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing
AB  - Fog computing paradigm has created innovation opportunities within Internet of Things (IoT) domain by extending cloud services to the edge of the network. Due to the distributed, heterogeneous and resource constrained nature of the Fog computing nodes, Fog applications need to be developed as a collection of interdependent, lightweight modules. Since this concept aligns with the goals of microservices architecture, efficient placement of microservices-based IoT applications within Fog environments has the potential to fully leverage capabilities of Fog devices. In this paper, we propose a decentralized microservices-based IoT application placement policy for heterogeneous and resource constrained Fog environments. The proposed policy utilizes the independently deployable and scalable nature of microservices to place them as close as possible to the data source to minimize latency and network usage. Moreover, it aims to handle service discovery and load balancing related challenges of the microservices architecture. We implement and evaluate our policy using iFogSim simulated Fog environment. Results of the simulations show around 85% improvement in latency and network usage for the proposed microservice placement policy when compared with Cloud-only placement approach and around 40% improvement over an alternative Fog application placement method known as Edge-ward placement policy. Moreover, the decentralized placement approach proposed in this paper demonstrates significant reduction in microservice placement delay over centralized placement. © 2019 Association for Computing Machinery.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3344341.3368800
SP  - 71
EP  - 81
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078454708&doi=10.1145%2f3344341.3368800&partnerID=40&md5=b99c5eccd2b311a7d66f0b4f73c80c63
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 101</p>
KW  - Application deployment
KW  - Application placement
KW  - Application placements
KW  - Computing environments
KW  - Computing paradigm
KW  - Fog
KW  - Fog computing
KW  - Internet of things
KW  - Internet of things (IoT)
KW  - Internet of Things (IOT)
KW  - IOT applications
KW  - Microservices architecture
KW  - Network architecture
KW  - Service discovery
KW  - Simulated fog environments
ER  - 

TY  - JOUR
TI  - Online Deployment Algorithms for Microservice Systems With Complex Dependencies
AU  - He, X.
AU  - Tu, Z.
AU  - Wagner, M.
AU  - Xu, X.
AU  - Wang, Z.
T2  - IEEE Transactions on Cloud Computing
AB  - Cloud and edge computing have been widely adopted in many application scenarios. With the increasing demand of fast iteration and complexity of business logic, it is challenging to achieve rapid development and continuous delivery in such highly distributed cloud and edge computing environment. At present, the microservice-based architecture has been the dominant deployment style, and a microservice system has to evolve agilely to offer stable Quality of Service (QoS) in the situation where user requirement changes frequently. A lot of research have been conducted to optimally re-deploy microservices to adapt to changing requirements. Nevertheless, complex dependencies between microservices and the existence of multiple instances of one single microservice in a microservice system together have not been fully considered in existing work. This article defines SPPMS, the Service Placement Problem in Microservice Systems that feature complex dependencies and multiple instances, as a Fractional Polynomial Problem (FPP). Considering the high computation complexity of FPP, it is then transformed into a Quadratic Sum-of-Ratios Fractional Problem (QSRFP) which is further solved by the our proposed greedy-based algorithms. Experiments demonstrate that our models and algorithms outperform existing approaches in both qualities of the generated solutions and computation speed. 2022 IEEE. © 2023 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.
DA  - 2023///
PY  - 2023
DO  - 10.1109/TCC.2022.3161684
VL  - 11
IS  - 2
SP  - 1746
EP  - 1763
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127076615&doi=10.1109%2fTCC.2022.3161684&partnerID=40&md5=8dc6248dded8fe69867d7169681c8286
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 43</p>
KW  - Cloud computing
KW  - Cloud-computing
KW  - Computer architecture
KW  - Edge computing
KW  - Iterative methods
KW  - Job analysis
KW  - Microservice architecture
KW  - Microservice system
KW  - microservice systems
KW  - multiple instance coexistence
KW  - Multiple instance coexistence
KW  - Multiple instances
KW  - Online systems
KW  - Production facility
KW  - Quality of service
KW  - Quality-of-service
KW  - service dependencies
KW  - Service dependency
KW  - service placement
KW  - Service placements
KW  - Task analysis
KW  - Time factors
ER  - 

TY  - CONF
TI  - Performance engineering for microservices: Research challenges & directions
AU  - Heinrich, R.
AU  - Van Hoorn, A.
AU  - Knoche, H.
AU  - Li, F.
AU  - Lwakatare, L.E.
AU  - Pahl, C.
AU  - Schulte, S.
AU  - Wettinger, J.
T2  - ICPE 2017 - Companion of the 2017 ACM/SPEC International Conference on Performance Engineering
AB  - Microservices complement approaches like DevOps and continuous delivery in terms of software architecture. Along with this architectural style, several important deployment technologies, such as container-based virtualization and container orchestration solutions, have emerged. These technologies allow to efficiently exploit cloud platforms, providing a high degree of scalability, availability, and portability for microservices. Despite the obvious importance of a sufficient level of performance, there is still a lack of performance engineering approaches explicitly taking into account the particularities of microservices. In this paper, we argue why new solutions to performance engineering for microservices are needed. Furthermore, we identify open issues and outline possible re- search directions with regard to performance-Aware testing, monitoring, and modeling of microservices. © 2017 ACM.
DA  - 2017///
PY  - 2017
DO  - 10.1145/3053600.3053653
SP  - 223
EP  - 226
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019457941&doi=10.1145%2f3053600.3053653&partnerID=40&md5=6cb0eb2434a77000e1386723f784863d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 98</p>
KW  - Architectural style
KW  - Cloud platforms
KW  - Containers
KW  - Degree of scalability
KW  - Engineering
KW  - Industrial engineering
KW  - New solutions
KW  - Performance engineering
KW  - Research challenges
KW  - Search direction
ER  - 

TY  - JOUR
TI  - Dynamic Microservice Deployment and Offloading for Things-Edge-Cloud Computing
AU  - Tian, X.
AU  - Meng, H.
AU  - Shen, Y.
AU  - Zhang, J.
AU  - Chen, Y.
AU  - Li, Y.
T2  - IEEE Internet of Things Journal
AB  - The growing edge cloud computing paradigm allows flexible handling of latency-sensitive and computation-intensive applications operating on user devices as the Internet of Things and 5G technologies gain in popularity. Microservices based on container technology are regarded as a potential architecture when applied to edge computing because of their lightweight and layered image properties. However, many current studies on the combination of the two simply treat microservices as a replacement for traditional virtual machine architecture without fully utilizing its advantages. In addition to discussing the impact of image loading strategy on neighboring time slots, this article also focuses on the advantages of microservices layered image sharing. Our research in this article studies the microservice deployment and task offloading of a mobility-aware things-edge-cloud system, and a deep reinforcement learning-based algorithm is proposed in this work to make decisions that optimize the system's long-term throughput and delay utility.  © 2014 IEEE.
DA  - 2024///
PY  - 2024
DO  - 10.1109/JIOT.2024.3370170
VL  - 11
IS  - 11
SP  - 19537
EP  - 19548
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187013269&doi=10.1109%2fJIOT.2024.3370170&partnerID=40&md5=9a2be96345626229ed141489f8e6142a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 10</p>
KW  - 5G mobile communication systems
KW  - Cloud-computing
KW  - Computation offloading
KW  - Computer architecture
KW  - Containers
KW  - Deep learning
KW  - Deep reinforcement learning
KW  - E-learning
KW  - Edge clouds
KW  - Edge detection
KW  - Heuristic algorithms
KW  - Heuristics algorithm
KW  - Image edge detection
KW  - Job analysis
KW  - Learning algorithms
KW  - Markov decision process (MDP)
KW  - Markov Decision Processes
KW  - Markov processes
KW  - Microservice
KW  - Microservice architecture
KW  - microservice online offloading
KW  - Online offloading
KW  - Reinforcement learning
KW  - Reinforcement learnings
KW  - Task analysis
KW  - Thing-edge-cloud computing
KW  - things-edge-cloud (TEC) computing
ER  - 

TY  - JOUR
TI  - Migration Modeling and Learning Algorithms for Containers in Fog Computing
AU  - Tang, Z.
AU  - Zhou, X.
AU  - Zhang, F.
AU  - Jia, W.
AU  - Zhao, W.
T2  - IEEE Transactions on Services Computing
AB  - Fog Computing (FC) is a flexible architecture to support distributed domain-specific applications with cloud-like quality of service. However, current FC still lacks the mobility support mechanism when facing many mobile users with diversified application quality requirements. Such mobility support mechanism can be critical such as in the industrial internet where human, products, and devices are moveable. To fill in such gaps, in this paper we propose novel container migration algorithms and architecture to support mobility tasks with various application requirements. Our algorithms are realized from three aspects: 1) We consider mobile application tasks can be hosted in a container of a corresponding fog node that can be migrated, taking the communication delay and computational power consumption into consideration; 2) We further model such container migration strategy as multiple dimensional Markov Decision Process (MDP) spaces. To effectively reduce the large MDP spaces, efficient deep reinforcement learning algorithms are devised to achieve fast decision-making and 3) We implement the model and algorithms as a container migration prototype system and test its feasibility and performance. Extensive experiments show that our strategy outperforms the existing baseline approaches 2.9, 48.5 and 58.4 percent on average in terms of delay, power consumption, and migration cost, respectively. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/TSC.2018.2827070
VL  - 12
IS  - 5
SP  - 712
EP  - 725
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045628753&doi=10.1109%2fTSC.2018.2827070&partnerID=40&md5=4d8c2542a77e034c13ebffee101ee3cd
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 182</p>
KW  - Behavioral research
KW  - Cloud computing
KW  - container migration
KW  - Containers
KW  - Decision making
KW  - Deep learning
KW  - deep reinforcement learning
KW  - delay
KW  - Delays
KW  - Electric power utilization
KW  - Fog
KW  - Fog computing
KW  - Green computing
KW  - Job analysis
KW  - Learning algorithms
KW  - Machine learning
KW  - Markov processes
KW  - Mobile applications
KW  - Mobile computing
KW  - power consumption
KW  - Power demands
KW  - Quality of service
KW  - Reinforcement learning
KW  - Resource management
KW  - Task analysis
KW  - user mobility
KW  - User mobility
ER  - 

TY  - JOUR
TI  - Microservice Deployment in Edge Computing Based on Deep Q Learning
AU  - Lv, W.
AU  - Wang, Q.
AU  - Yang, P.
AU  - Ding, Y.
AU  - Yi, B.
AU  - Wang, Z.
AU  - Lin, C.
T2  - IEEE Transactions on Parallel and Distributed Systems
AB  - The microservice deployment strategy is promising in reducing the overall service response time in the microservice-oriented edge computing platform. However, existing works ignore the effect of different interaction frequencies among microservices and the decrease in service execution performance caused by the increased node loads. In this article, we first model the invocation relationships among microservices as an undirected and weighted interaction graph to characterize the communication overhead. Then, we propose a multi-objective microservice deployment problem (MMDP) in edge computing. MMDP aims to minimize the communication overhead while achieving load balance between edge nodes. Without the requirement for domain experts, we propose Reward Sharing Deep Q Learning (RSDQL), a learning-based algorithm, to solve MMDP and obtain the optimal deployment strategy. In addition, to improve the scalability of the services, we propose an Elastic Scaling algorithm (ES) based on heuristics to deal with the dynamic pressure of requests. Finally, we conduct a series of experiments in Kubernetes to evaluate the performance of our approach. Experimental results indicate that, compared with interaction-aware strategy and Kubernetes default strategy, RSDQL has shorter response times, more balanced resource loads, and makes services scale elastically according to the request pressure. © 1990-2012 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/TPDS.2022.3150311
VL  - 33
IS  - 11
SP  - 2968
EP  - 2978
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124746898&doi=10.1109%2fTPDS.2022.3150311&partnerID=40&md5=ed190a294b1f4bb5c1840c962fd259f4
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 87</p>
KW  - Deep learning
KW  - deep Q learning
KW  - Deep Q learning
KW  - Edge computing
KW  - elastic scaling
KW  - Elastic scaling
KW  - interaction awareness
KW  - Interaction awareness
KW  - Learning algorithms
KW  - load balancing
KW  - Load-Balancing
KW  - Microservice
KW  - Multi objective
KW  - multi-objective model
KW  - Multiobjective modeling
KW  - Q-learning
KW  - Reinforcement learning
KW  - Scalings
ER  - 

TY  - CONF
TI  - Microservices and their design trade-offs: A self-adaptive roadmap
AU  - Hassan, S.
AU  - Bahsoon, R.
T2  - Proceedings - 2016 IEEE International Conference on Services Computing, SCC 2016
AB  - Migrating to microservices (microservitization) enablesoptimising the autonomy, replaceability, decentralisedgovernance and traceability of software architectures. Despitethe hype for microservitization, the state of the art still lacksconsensus on the definition of microservices, their propertiesand their modelling techniques. This paper summarises viewsof microservices from informal literature to reflect on the foundationalcontext of this paradigm shift. A strong foundationalcontext can advance our understanding of microservitizationand help guide software architects in addressing its designproblems. One such design problem is finalising the optimallevel of granularity of a microservice architecture. Relateddesign trade-offs include: balancing the size and numberof microservices in an architecture and balancing the nonfunctionalrequirement satisfaction levels of the individualmicroservices as well as their satisfaction for the overall system. We propose how self-adaptivity can assist in addressing thesedesign trade-offs and discuss some of the challenges such a selfadaptivesolution. We use a hypothetical online movie streamingsystem to motivate these design trade-offs. A solution roadmapis presented in terms of the phases of a feedback control loop. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/SCC.2016.113
SP  - 813
EP  - 818
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989952704&doi=10.1109%2fSCC.2016.113&partnerID=40&md5=9f19cdb66a2b79f1c63821348e8afa34
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 93</p>
KW  - Commerce
KW  - Decision making
KW  - Decision-making
KW  - Economic and social effects
KW  - Feedback control
KW  - Granularity
KW  - Microservices
KW  - Non-functional requirements
KW  - Self-adaptativity
KW  - Software architecture
KW  - Trade off
KW  - Trade-offs
ER  - 

TY  - JOUR
TI  - IoT Microservice Deployment in Edge-Cloud Hybrid Environment Using Reinforcement Learning
AU  - Chen, L.
AU  - Xu, Y.
AU  - Lu, Z.
AU  - Wu, J.
AU  - Gai, K.
AU  - Hung, P.C.K.
AU  - Qiu, M.
T2  - IEEE Internet of Things Journal
AB  - The edge-cloud hybrid environment requires complex deployment strategies to enable the smart Internet-of-Things (IoT) system. However, current service deployment strategies use simple, generalized heuristics and ignore the heterogeneous characteristics in the edge-cloud hybrid environment. In this article, we devise a method to find a microservice-based service deployment strategy that can reduce the average waiting time of IoT devices in the hybrid environment. For this purpose, we first propose a microservice-based deployment problem (MSDP) based on the heterogeneous and dynamic characteristics in the edge-cloud hybrid environment, including heterogeneity of edge server capacities, dynamic geographical information of IoT devices, and changing device preference for applications and complex application structures. We then propose a multiple buffer deep deterministic policy gradient (MBDDPG) to provide more preferable service deployment solutions. Our algorithm leverages reinforcement learning and neural network to learn a deployment strategy without any human instruction. Therefore, the service provider can make full use of limited resources to improve the Quality of Service (QoS). Finally, we implement MBDDPG based on real-world data sets and some synthetic data, and we also implement another two algorithms, genetic algorithm and random algorithm, as a contrast. The experimental results demonstrate that MBDDPG is able to learn a preferable strategy which, in terms of average waiting time, outperforms genetic algorithm and the random algorithm by 32% and 44%, respectively. © 2014 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/JIOT.2020.3014970
VL  - 8
IS  - 16
SP  - 12610
EP  - 12622
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099587770&doi=10.1109%2fJIOT.2020.3014970&partnerID=40&md5=ebfdb53382bb060f17a2fc9bcf63a784
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 83</p>
KW  - Complex networks
KW  - Deployment strategy
KW  - Deterministics
KW  - Edge clouds
KW  - Edge-cloud hybrid environment
KW  - Genetic algorithms
KW  - Heterogeneous characteristic
KW  - Internet of things
KW  - microservice deployment
KW  - Microservice deployment
KW  - Policy gradient
KW  - Quality of service
KW  - reinforcement learning
KW  - Reinforcement learning
KW  - Reinforcement learnings
KW  - Service deployment
KW  - Smart internet-of-thing  system
KW  - smart Internet-of-Things (IoT) system
ER  - 

TY  - JOUR
TI  - Intelligent Edge Computing: Security and Privacy Challenges
AU  - Mukherjee, M.
AU  - Matam, R.
AU  - Mavromoustakis, C.X.
AU  - Jiang, H.
AU  - Mastorakis, G.
AU  - Guo, M.
T2  - IEEE Communications Magazine
AB  - Edge computing has already shown its potential benefits to support the delay-sensitive and computation-intensive service provisioning in the Internet-of-Things-based environments. Simultaneously, artificial intelligence is expected to enhance the cognizance and intelligence of edge computing, resulting in a new paradigm, intelligent edge computing. However, as is evident, several new security and privacy-related issues will arise due to the distinct characteristics and working principles of intelligent edge computing. In this article, we focus on malicious attacks targeting the intelligent engines of edge computing. These attacks counteract the gains from intelligent systems and also compromise the edge computing system. We provide the main factors that are responsible for the security and privacy-related challenges in intelligent edge computing. Finally, the potential research directions in privacy and security-related issues are outlined for intelligent edge computing.  © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/MCOM.001.2000297
VL  - 58
IS  - 9
SP  - 26
EP  - 31
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092524517&doi=10.1109%2fMCOM.001.2000297&partnerID=40&md5=20fa71e66ac59a48ad8680efb759b933
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 46</p>
KW  - Computation intensives
KW  - Edge computing
KW  - Intelligent computing
KW  - Intelligent engines
KW  - Intelligent systems
KW  - Malicious attack
KW  - Network security
KW  - Potential benefits
KW  - Potential researches
KW  - Privacy and security
KW  - Security and privacy
KW  - Service provisioning
ER  - 

TY  - JOUR
TI  - Foundations and Evolution of Modern Computing Paradigms: Cloud, IoT, Edge, and Fog
AU  - De Donno, M.
AU  - Tange, K.
AU  - Dragoni, N.
T2  - IEEE Access
AB  - In the last few years, Internet of Things, Cloud computing, Edge computing, and Fog computing have gained a lot of attention in both industry and academia. However, a clear and neat definition of these computing paradigms and their correlation is hard to find in the literature. This makes it difficult for researchers new to this area to get a concrete picture of these paradigms. This work tackles this deficiency, representing a helpful resource for those who will start next. First, we show the evolution of modern computing paradigms and related research interest. Then, we address each paradigm, neatly delineating its key points and its relation with the others. Thereafter, we extensively address Fog computing, remarking its outstanding role as the glue between IoT, Cloud, and Edge computing. In the end, we briefly present open challenges and future research directions for IoT, Cloud, Edge, and Fog computing. © 2013 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/ACCESS.2019.2947652
VL  - 7
SP  - 150936
EP  - 150948
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078341344&doi=10.1109%2fACCESS.2019.2947652&partnerID=40&md5=f0ed9688504d4eeb4a589c1f86754d67
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 317</p>
KW  - cloud computing
KW  - Cloud computing
KW  - Computing paradigm
KW  - edge computing
KW  - Edge computing
KW  - Fog
KW  - Fog computing
KW  - Future research directions
KW  - Internet of things
KW  - Internet of Things
KW  - Keypoints
KW  - mobile cloud computing
KW  - Mobile cloud computing
KW  - mobile edge computing
KW  - Research interests
ER  - 

TY  - JOUR
TI  - Open Issues in Scheduling Microservices in the Cloud
AU  - Fazio, M.
AU  - Celesti, A.
AU  - Ranjan, R.
AU  - Liu, C.
AU  - Chen, L.
AU  - Villari, M.
T2  - IEEE Cloud Computing
AB  - The adoption of container-based microservices architectures is revolutionizing application design. By adopting a microservices architecture, developers can engineer applications that are composed of multiple lightweight, self-contained, and portable runtime components deployed across a large number of geodistributed servers. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/MCC.2016.112
VL  - 3
IS  - 5
SP  - 81
EP  - 88
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996569901&doi=10.1109%2fMCC.2016.112&partnerID=40&md5=d52198bb1cc22e10b6f9e5e3d6df7c95
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 139</p>
KW  - Application architecture
KW  - Application design
KW  - Application programs
KW  - Cloud computing
KW  - Cloud computing architecture
KW  - Cloud systems
KW  - Containers
KW  - Distributed applications
KW  - Federated clouds
KW  - Independent components
KW  - Internet of things
KW  - microservices
KW  - Runtimes
KW  - Service deployment
KW  - Software developer
ER  - 

TY  - JOUR
TI  - Multi-objective and parallel particle swarm optimization algorithm for container-based microservice scheduling
AU  - Chen, X.
AU  - Xiao, S.
T2  - Sensors
AB  - An application based on a microservice architecture with a set of independent, fine-grained modular services is desirable, due to its low management cost, simple deployment, and high portability. This type of container technology has been widely used in cloud computing. Several methods have been applied to container-based microservice scheduling, but they come with significant disadvantages, such as high network transmission overhead, ineffective load balancing, and low service reliability. In order to overcome these disadvantages, in this study, we present a multi-objective optimization problem for container-based microservice scheduling. Our approach is based on the particle swarm optimization algorithm, combined parallel computing, and Pareto-optimal theory. The particle swarm optimization algorithm has fast convergence speed, fewer parameters, and many other advantages. First, we detail the various resources of the physical nodes, cluster, local load balancing, failure rate, and other aspects. Then, we discuss our improvement with respect to the relevant parameters. Second, we create a multi-objective optimization model and use a multi-objective optimization parallel particle swarm optimization algorithm for container-based microservice scheduling (MOPPSO-CMS). This algorithm is based on user needs and can effectively balance the performance of the cluster. After comparative experiments, we found that the algorithm can achieve good results, in terms of load balancing, network transmission overhead, and optimization speed. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
DA  - 2021///
PY  - 2021
DO  - 10.3390/s21186212
VL  - 21
IS  - 18
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114941810&doi=10.3390%2fs21186212&partnerID=40&md5=28c2691de5e2093cd82ccd481182b736
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 17</p>
KW  - Cloud computing
KW  - Clustering algorithms
KW  - Comparative experiments
KW  - Computation theory
KW  - Container-based microservice scheduling
KW  - Containers
KW  - Failure analysis
KW  - Fast convergence speed
KW  - Multi-objective optimization
KW  - Multi-objective optimization models
KW  - Multi-objective optimization problem
KW  - Multiobjective optimization
KW  - Network transmission
KW  - Parallel particle swarm optimization
KW  - Pareto principle
KW  - Particle swarm optimization (PSO)
KW  - Particle swarm optimization algorithm
KW  - Scheduling
KW  - Service reliability
ER  - 

TY  - JOUR
TI  - Throughput-Aware Partitioning and Placement of Applications in Fog Computing
AU  - Faticanti, F.
AU  - De Pellegrini, F.
AU  - Siracusa, D.
AU  - Santoro, D.
AU  - Cretti, S.
T2  - IEEE Transactions on Network and Service Management
AB  - Fog computing promises to extend cloud computing to match emerging demands for low latency, location-awareness and dynamic computation. It thus brings data processing close to the edge of the network by leveraging on devices with different computational characteristics. However, the heterogeneity, the geographical distribution, and the data-intensive profiles of IoT deployments render the placement of fog applications a fundamental problem to guarantee target performance figures. This is a core challenge for fog computing providers to offer fog infrastructure as a service, while satisfying the requirements of this new class of microservices-based applications. In this article we root our analysis on the throughput requirements of the applications while exploiting offloading towards different regions. The resulting resource allocation problem is developed for a fog-native application architecture based on containerised microservice modules. An algorithmic solution is designed to optimise the placement of applications modules either in cloud or in fog. Finally, the overall solution consists of two cascaded algorithms. The first one performs a throughput-oriented partitioning of fog application modules. The second one rules the orchestration of applications over a region-based infrastructure. Extensive numerical experiments validate the performance of the overall scheme and confirm that it outperforms state-of-the-art solutions adapted to our context.  © 2004-2012 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/TNSM.2020.3023011
VL  - 17
IS  - 4
SP  - 2436
EP  - 2450
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090929963&doi=10.1109%2fTNSM.2020.3023011&partnerID=40&md5=878b902c11074aed1ebbc176d3fa548a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 46</p>
KW  - Algorithmic solutions
KW  - Application architecture
KW  - Application module
KW  - applications partitioning
KW  - Data handling
KW  - Dynamic computations
KW  - Fog
KW  - Fog computing
KW  - Geographical distribution
KW  - Infrastructure as a service (IaaS)
KW  - IoT
KW  - Location awareness
KW  - microservices
KW  - Numerical experiments
KW  - resource allocation
KW  - Resource allocation problem
KW  - State of the art
ER  - 

TY  - JOUR
TI  - EdgeSimPy: Python-based modeling and simulation of edge computing resource management policies
AU  - Souza, P.S.
AU  - Ferreto, T.
AU  - Calheiros, R.N.
T2  - Future Generation Computer Systems
AB  - The increasing popularity of applications with tight latency requirements has motivated research on Edge Computing, which positions computing resources near data sources at the Internet's edge. Despite the emergence of simulation tools that make prototype validation less complex, time-consuming, and expensive, researchers and practitioners still face significant challenges when developing resource management strategies for the edge, as existing simulators fall short in providing a fine-grained model of edge applications provisioning. To overcome this challenge, we propose EdgeSimPy, a simulation framework written in Python for modeling and evaluating resource management policies in Edge Computing environments. EdgeSimPy features a modular architecture that incorporates several functional abstractions for edge servers, network devices, and applications with built-in models for user mobility, application composition, and power consumption that allow the simulation of various scenarios. Furthermore, we propose a novel conceptual model that accurately represents the entire lifecycle of edge applications and ensures seamless integration with real application traces. In addition to submitting EdgeSimPy to an in-depth verification that checks the simulator implementation, we discuss case studies that show EdgeSimPy in action in different large-scale scenarios. © 2023 Elsevier B.V.
DA  - 2023///
PY  - 2023
DO  - 10.1016/j.future.2023.06.013
VL  - 148
SP  - 446
EP  - 459
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163891959&doi=10.1016%2fj.future.2023.06.013&partnerID=40&md5=695770f8524f07a75dc3debfcf4f85ba
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 34</p>
KW  - Based modelling
KW  - Computer software
KW  - Computing power
KW  - Computing resource
KW  - Computing resource management
KW  - Containers
KW  - Data-source
KW  - Edge computing
KW  - Life cycle
KW  - Model and simulation
KW  - Modeling
KW  - Natural resources management
KW  - Python
KW  - Resource allocation
KW  - Resource management
KW  - Resource management policy
KW  - Simulation
ER  - 

TY  - JOUR
TI  - Machine learning methods for reliable resource provisioning in edge-cloud computing: A survey
AU  - Le Duc, T.
AU  - Leiva, R.G.
AU  - Casari, P.
AU  - Östberg, P.-O.
T2  - ACM Computing Surveys
AB  - Large-scale software systems are currently designed as distributed entities and deployed in cloud data centers. To overcome the limitations inherent to this type of deployment, applications are increasingly being supplemented with components instantiated closer to the edges of networks—a paradigm known as edge computing. The problem of how to efficiently orchestrate combined edge-cloud applications is, however, incompletely understood, and a wide range of techniques for resource and application management are currently in use. This article investigates the problem of reliable resource provisioning in joint edge-cloud environments, and surveys technologies, mechanisms, and methods that can be used to improve the reliability of distributed applications in diverse and heterogeneous network environments. Due to the complexity of the problem, special emphasis is placed on solutions to the characterization, management, and control of complex distributed applications using machine learning approaches. The survey is structured around a decomposition of the reliable resource provisioning problem into three categories of techniques: workload characterization and prediction, component placement and system consolidation, and application elasticity and remediation. Survey results are presented along with a problem-oriented discussion of the state-of-the-art. A summary of identified challenges and an outline of future research directions are presented to conclude the article. © 2019 Association for Computing Machinery.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3341145
VL  - 52
IS  - 5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072380854&doi=10.1145%2f3341145&partnerID=40&md5=47076a686bc04cca9a6254ea60f18b51
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 189</p>
KW  - Autoscaling
KW  - Cloud computing
KW  - Cloud-computing
KW  - Complex networks
KW  - Consolidation
KW  - Distributed applications
KW  - Distributed systems
KW  - Edge clouds
KW  - Edge computing
KW  - Heterogeneous networks
KW  - Machine learning
KW  - Machine learning methods
KW  - Machine-learning
KW  - Optimisations
KW  - Optimization
KW  - Placement
KW  - Reliability
KW  - Remediation
KW  - Surveys
ER  - 

TY  - JOUR
TI  - Proximal policy optimization algorithms
AU  - Schulman, J.
AU  - Wolski, F.
AU  - Dhariwal, P.
AU  - Radford, A.
AU  - Klimov, O.
T2  - Proximal Policy Optimization Algorithms
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041194636&partnerID=40&md5=83b11cddc6f5a1263e16098bcdf57974
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 15076</p>
ER  - 

TY  - JOUR
TI  - Multi-objective optimization of container-based microservice scheduling in edge computing
AU  - Fan, G.
AU  - Chen, L.
AU  - Yu, H.
AU  - Qi, W.
T2  - Computer Science and Information Systems
AB  - Edge computing provides physical resources closer to end users, becoming a good complement to cloud computing. With the rapid development of container technology and microservice architecture, container orchestration has become a hot issue. However, the container-based microservice scheduling problem in edge computing is still urgent to be solved. In this paper, we first formulate the container-based microservice scheduling as a multi-objective optimization problem, aiming to optimize network latency among microservices, reliability of microservice applications and load balancing of the cluster. We further propose a latency, reliability and load balancing aware scheduling (LRLBAS) algorithm to determine the container-based microservice deployment in edge computing. Our proposed algorithm is based on particle swarm optimization (PSO). In addition, we give a handling strategy to separate the fitness function from constraints, so that each particle has two fitness values. In the proposed algorithm, a new particle comparison criterion is introduced and a certain proportion of infeasible particles are reserved adaptively. Extensive simulation experiments are conducted to demonstrate the effectiveness and efficiency of the proposed algorithm compared with other related algorithms. © 2020, ComSIS Consortium. All rights reserved.
DA  - 2020///
PY  - 2020
DO  - 10.2298/CSIS200229041F
VL  - 18
IS  - 1
SP  - 23
EP  - 42
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100578451&doi=10.2298%2fCSIS200229041F&partnerID=40&md5=469c3de7ec132fe2f19f9688e9f1ec9a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 38</p>
KW  - Container orchestration
KW  - Edge computing
KW  - Microservice
KW  - Multi-objective optimization
KW  - Particle swarm optimization
ER  - 

TY  - JOUR
TI  - Dynamic Service Placement in Multi-Access Edge Computing: A Systematic Literature Review
AU  - Tabatabaee Malazi, H.
AU  - Chaudhry, S.R.
AU  - Kazmi, A.
AU  - Palade, A.
AU  - Cabrera, C.
AU  - White, G.
AU  - Clarke, S.
T2  - IEEE Access
AB  - The advent of new cloud-based applications such as mixed reality, online gaming, autonomous driving, and healthcare has introduced infrastructure management challenges to the underlying service network. Multi-access edge computing (MEC) extends the cloud computing paradigm and leverages servers near end-users at the network edge to provide a cloud-like environment. The optimum placement of services on edge servers plays a crucial role in the performance of such service-based applications. Dynamic service placement problem addresses the adaptive configuration of application services at edge servers to facilitate end-users and those devices that need to offload computation tasks. While reported approaches in the literature shed light on this problem from a particular perspective, a panoramic study of this problem reveals the research gaps in the big picture. This paper introduces the dynamic service placement problem and outline its relations with other problems such as task scheduling, resource management, and caching at the edge. We also present a systematic literature review of existing dynamic service placement methods for MEC environments from networking, middleware, applications, and evaluation perspectives. In the first step, we review different MEC architectures and their enabling technologies from a networking point of view. We also introduce different cache deployment solutions in network architectures and discuss their design considerations. The second step investigates dynamic service placement methods from a middleware viewpoint. We review different service packaging technologies and discuss their trade-offs. We also survey the methods and identify eight research directions that researchers follow. Our study categorises the research objectives into six main classes, proposing a taxonomy of design objectives for the dynamic service placement problem. We also investigate the reported methods and devise a solutions taxonomy comprising six criteria. In the third step, we concentrate on the application layer and introduce the applications that can take advantage of dynamic service placement. The fourth step investigates evaluation environments used to validate the solutions, including simulators and testbeds. We introduce real-world datasets such as edge server locations, mobility traces, and service requests used to evaluate the methods. We compile a list of open issues and challenges categorised by various viewpoints in the last step.  © 2013 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ACCESS.2022.3160738
VL  - 10
SP  - 32639
EP  - 32688
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127032753&doi=10.1109%2fACCESS.2022.3160738&partnerID=40&md5=bc63487fa7bb2b72a15e15b6f7b233ca
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 72</p>
KW  - Cloud-computing
KW  - computational offloading
KW  - Computational offloading
KW  - Decentralised
KW  - decentralised cloud
KW  - Decentralized cloud
KW  - Economic and social effects
KW  - Edge computing
KW  - Job analysis
KW  - MEC server
KW  - Memory architecture
KW  - Mixed reality
KW  - Mobile edge computing
KW  - Multi-access edge computing server
KW  - Multiaccess
KW  - Natural resources management
KW  - Network architecture
KW  - Resource allocation
KW  - resource management
KW  - Resource management
KW  - Scheduling
KW  - service caching
KW  - Service caching
KW  - service deployment
KW  - Service deployment
KW  - service offloading
KW  - Service offloading
KW  - service orchestration
KW  - Service orchestration
KW  - Task analysis
KW  - Taxonomies
KW  - Vehicle's dynamics
KW  - Wireless fidelities
ER  - 

TY  - JOUR
TI  - Microservices scheduling model over heterogeneous cloud-edge environments as support for IoT applications
AU  - Filip, I.-D.
AU  - Pop, F.
AU  - Serbanescu, C.
AU  - Choi, C.
T2  - IEEE Internet of Things Journal
AB  - Motivated by the high-interest in increasing the utilization of nongeneral purpose devices in reaching computational objectives with a reduced cost, we propose a new model for scheduling microservices over heterogeneous cloud-edge environments. Our model uses a particular mathematical formulation for describing an architecture that includes heterogeneous machines that can handle different microservices. Since any new model asks for an early risk-analysis of the solution, we improved the CloudSim simulation framework to be suitable for an experiment that includes that kind of systems. In this paper, we discuss two examples of real-life utilizations of our proposed scheduling architecture. For an objective appreciation of the first example, we also include some experimental results based on the developed simulation tool. As a result of our interpretation of the experimental results we find out that some very simple scheduling algorithms may outperform some others in given situations that are frequently present in cloud-edge environments when we are using a microservice-oriented approach. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/JIOT.2018.2792940
VL  - 5
IS  - 4
SP  - 2672
EP  - 2681
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041178328&doi=10.1109%2fJIOT.2018.2792940&partnerID=40&md5=a37ccd9b469584b0b2b9b54c8557d0a0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 86</p>
KW  - Cloud computing
KW  - Computational model
KW  - Computer architecture
KW  - Edge computing
KW  - Energy efficiency
KW  - Green computing
KW  - Heterogeneous systems
KW  - Internet of things
KW  - IOT applications
KW  - Mathematical formulation
KW  - Mathematical models
KW  - Microservice scheduling
KW  - Processor scheduling
KW  - Risk analysis
KW  - Risk assessment
KW  - Scheduling
KW  - Scheduling algorithms
KW  - Scheduling architecture
KW  - Scheduling models
KW  - Simulation framework
ER  - 

TY  - CHAP
TI  - Microservices: Yesterday, today, and tomorrow
AU  - Dragoni, N.
AU  - Giallorenzo, S.
AU  - Lafuente, A.L.
AU  - Mazzara, M.
AU  - Montesi, F.
AU  - Mustafin, R.
AU  - Safina, L.
T2  - Present and Ulterior Software Engineering
AB  - Microservices is an architectural style inspired by service-oriented computing that has recently started gaining popularity. Before presenting the current state of the art in the field, this chapter reviews the history of software architecture, the reasons that led to the diffusion of objects and services first, and microservices later. Finally, open problems and future challenges are introduced. This survey primarily addresses newcomers to the discipline, while offering an academic viewpoint on the topic. In addition, we investigate some practical issues and point out a few potential solutions. © Springer International Publishing AG 2017.
DA  - 2017///
PY  - 2017
SP  - 195
EP  - 216
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054938535&doi=10.1007%2f978-3-319-67425-4_12&partnerID=40&md5=a052a4a4bb9e8ef78561c5d7aefbe45c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 899</p>
ER  - 

TY  - JOUR
TI  - MicroFog: A framework for scalable placement of microservices-based IoT applications in federated Fog environments
AU  - Pallewatta, S.
AU  - Kostakos, V.
AU  - Buyya, R.
T2  - Journal of Systems and Software
AB  - MicroService Architecture (MSA) is gaining rapid popularity for developing large-scale IoT applications for deployment within distributed and resource-constrained Fog computing environments. As a cloud-native application architecture, the true power of microservices comes from their loosely coupled, independently deployable and scalable nature, enabling distributed placement and dynamic composition across federated Fog and Cloud clusters. Thus, it is necessary to develop novel placement algorithms that utilise these microservice characteristics to improve the performance of the applications. However, existing Fog computing frameworks lack support for integrating such placement policies due to their shortcomings in multiple areas, including MSA application placement and deployment across multi-fog multi-cloud environments, dynamic microservice composition across multiple distributed clusters, scalability of the framework to operate within federated environments, support for deploying heterogeneous microservice applications, etc. To this end, we design and implement MicroFog, a Fog computing framework compatible with cloud-native technologies such as Docker, Kubernetes and Istio. MicroFog provides an extensible and configurable control engine that executes placement algorithms and deploys applications across federated Fog environments. Furthermore, MicroFog provides a sufficient abstraction over container orchestration and dynamic microservice composition, thus enabling users to easily incorporate new placement policies and evaluate their performance. The capabilities of the MicroFog framework, such as the scalability and flexibility of the design and deployment architecture of MicroFog and its ability to ensure the deployment and composition of microservices across distributed fog–cloud environments, are validated using multiple use cases. Experiments also demonstrate MicroFog's ability to integrate and evaluate novel placement policies and load-balancing techniques. To this end, we integrate multiple microservice placement policies to demonstrate MicroFog's ability to support horizontally scaled placement, service discovery and load balancing of microservices across federated environments, thus reducing the application service response time up to 54%. © 2023 The Authors
DA  - 2024///
PY  - 2024
DO  - 10.1016/j.jss.2023.111910
VL  - 209
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179470424&doi=10.1016%2fj.jss.2023.111910&partnerID=40&md5=21b1ceef6e8bb21ec1a6750b388908d6
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 17</p>
KW  - Application architecture
KW  - Application placement
KW  - Application placements
KW  - Balancing
KW  - Cloud environments
KW  - Computing environments
KW  - Computing frameworks
KW  - Fog
KW  - Fog computing
KW  - Internet of things
KW  - Large-scales
KW  - Memory architecture
KW  - Microservice
KW  - Microservice composition
KW  - Microservices
KW  - Performance
KW  - Placement algorithm
KW  - Scalability
ER  - 

TY  - JOUR
TI  - Delay-Aware Optimization of Fine-Grained Microservice Deployment and Routing in Edge via Reinforcement Learning
AU  - Peng, K.
AU  - He, J.
AU  - Guo, J.
AU  - Liu, Y.
AU  - He, J.
AU  - Liu, W.
AU  - Hu, M.
T2  - IEEE Transactions on Network Science and Engineering
AB  - Microservices have exerted a profound impact on the development of internet applications. Meanwhile, the growing number of mobile terminal user requests has made the communication between microservices extremely complex, significantly impacting the quality of user service experience in mobile edge computing. Therefore, the joint optimization of microservice deployment and request routing is necessary to alleviate server pressure and enhance overall performance of large-scaled MEC applications. However, most existing work studies the microservice deployment and request routing as two isolated problems and neglects the dependencies between microservices. This paper focuses on the data dependency relationship of request and multi-instance processing problem, and then formulate the joint problem of microservice deployment and request routing as an integer nonlinear program and queuing optimization model under complex constraints. To address this problem, we propose a fine-grained reinforcement learning-based algorithm named Reward Memory Shaping Deep Deterministic Policy Gradient (RMS DDPG). Furthermore, we introduce the Long Short-Term Memory (LSTM) block into the actor network and critical network to make actions memorable. Finally, our experiments demonstrate that our algorithm is more superior in terms of delay target, load balancing and algorithm robustness compared with four baseline algorithms.  © 2024 IEEE.
DA  - 2024///
PY  - 2024
DO  - 10.1109/TNSE.2024.3436616
VL  - 11
IS  - 6
SP  - 6024
EP  - 6037
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200823725&doi=10.1109%2fTNSE.2024.3436616&partnerID=40&md5=93c6b92ee94a899ec9859293195b965c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 18</p>
KW  - Brain
KW  - Complex networks
KW  - Delay
KW  - Fine grained
KW  - Heuristic algorithms
KW  - Heuristics algorithm
KW  - Integer programming
KW  - long short-term memory
KW  - Long short-term memory
KW  - Memory architecture
KW  - Microservice architecture
KW  - Microservice deployment
KW  - Mobile edge computing
KW  - Nonlinear programming
KW  - Optimisations
KW  - reinforcement learning
KW  - Reinforcement learning
KW  - Reinforcement learnings
KW  - request routing
KW  - Request routing
KW  - Routing algorithms
KW  - Routings
ER  - 

TY  - JOUR
TI  - DVFO: Learning-Based DVFS for Energy-Efficient Edge-Cloud Collaborative Inference
AU  - Zhang, Z.
AU  - Zhao, Y.
AU  - Li, H.
AU  - Lin, C.
AU  - Liu, J.
T2  - IEEE Transactions on Mobile Computing
AB  - Due to limited resources on edge and different characteristics of deep neural network (DNN) models, it is a big challenge to optimize DNN inference performance in terms of energy consumption and end-to-end latency. In addition to dynamic voltage frequency scaling (DVFS) technique, edge-cloud architecture provides a collaborative approach for efficient DNN inference. However, current edge-cloud collaborative inference methods have not optimized various compute resources on edge devices. Thus, we propose DVFO, a novel DVFS-enabled edge-cloud collaborative inference framework, which co-optimizes DVFS and offloading parameters via deep reinforcement learning (DRL). Specifically, DVFO automatically co-optimizes 1) the CPU, GPU and memory frequencies of edge devices, and 2) the offloaded feature map. In addition, it leverages a thinking-while-moving concurrent mechanism to accelerate the DRL learning process, and a spatial-channel attention mechanism to identify the less important DNN feature map for efficient offloading. This approach improves inference performance for different DNN models under various edge-cloud network conditions. Extensive evaluations using two datasets and six widely-deployed DNN models on five heterogeneous edge devices show that DVFO significantly reduces the energy consumption by 33% on average, compared to state-of-the-art schemes. Moreover, DVFO achieves up to 28.6%sim∼59.1% end-to-end latency reduction, while maintaining accuracy within 1% loss on average. © 2002-2012 IEEE.
DA  - 2024///
PY  - 2024
DO  - 10.1109/TMC.2024.3357218
VL  - 23
IS  - 10
SP  - 9042
EP  - 9059
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183648571&doi=10.1109%2fTMC.2024.3357218&partnerID=40&md5=2e9ac1bf6456b1c28f02c12a71b93629
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 22</p>
KW  - Collaboration
KW  - collaborative inference
KW  - Collaborative inference
KW  - Computation offloading
KW  - Computational modelling
KW  - Computer graphics
KW  - Deep neural networks
KW  - deep reinforcement learning
KW  - Deep reinforcement learning
KW  - DVFS technology
KW  - Dynamic frequency scaling
KW  - Dynamic voltage
KW  - Dynamic voltage frequency scaling technology
KW  - Edge computing
KW  - Energy efficiency
KW  - Energy utilization
KW  - Energy-consumption
KW  - Frequency-scaling
KW  - Graphics processing unit
KW  - Green computing
KW  - Performances evaluation
KW  - Program processors
KW  - Reinforcement learning
KW  - Reinforcement learnings
KW  - Scaling technology
KW  - Voltage frequency
KW  - Voltage scaling
ER  - 

TY  - JOUR
TI  - Multiagent Deep Reinforcement Learning for Cost- and Delay-Sensitive Virtual Network Function Placement and Routing
AU  - Wang, S.
AU  - Yuen, C.
AU  - Ni, W.
AU  - Guan, Y.L.
AU  - Lv, T.
T2  - IEEE Transactions on Communications
AB  - This paper proposes an effective and novel multi-agent deep reinforcement learning (MADRL)-based method for solving the joint virtual network function (VNF) placement and routing (P&R), where multiple service requests with differentiated demands are delivered at the same time. The differentiated demands of the service requests are reflected by their delay- and cost-sensitive factors. We first construct a VNF P&R problem to jointly minimize a weighted sum of service delay and resource consumption cost, which is NP-complete. Then, the joint VNF P&R problem is decoupled into two iterative subtasks: placement subtask and routing subtask. Each subtask consists of multiple concurrent parallel sequential decision processes. By invoking the deep deterministic policy gradient method and multi-agent technique, an MADRL-P&R framework is designed to perform the two subtasks. The new joint reward and internal rewards mechanism is proposed to match the goals and constraints of the placement and routing subtasks. We also propose the parameter migration-based model-retraining method to deal with changing network topologies. Corroborated by experiments, the proposed MADRL-P&R framework is superior to its alternatives in terms of service cost and delay, and offers higher flexibility for personalized service demands. The parameter migration-based model-retraining method can efficiently accelerate convergence under moderate network topology changes. © 1972-2012 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/TCOMM.2022.3187146
VL  - 70
IS  - 8
SP  - 5208
EP  - 5224
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133579989&doi=10.1109%2fTCOMM.2022.3187146&partnerID=40&md5=38e35eb2e9748640ce89a4a53678c3d7
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 29</p>
KW  - Cost functions
KW  - Deep learning
KW  - Delay
KW  - E-learning
KW  - Gradient methods
KW  - Minimisation
KW  - Multi agent
KW  - Multi agent systems
KW  - multi-agent deep reinforcement learning
KW  - Multi-agent deep reinforcement learning
KW  - Network function virtualization
KW  - Network functions
KW  - Network routing
KW  - Network topology
KW  - Optimisations
KW  - Optimization
KW  - Placement and routing
KW  - Quality of service
KW  - Reinforcement learning
KW  - Reinforcement learnings
KW  - Routings
KW  - Transfer functions
KW  - Virtual network function
KW  - virtual network functions
KW  - Virtual networks
KW  - Virtual reality
ER  - 

TY  - JOUR
TI  - On the Effective Parallelization and Near-Optimal Deployment of Service Function Chains
AU  - Luo, J.
AU  - Li, J.
AU  - Jiao, L.
AU  - Cai, J.
T2  - IEEE Transactions on Parallel and Distributed Systems
AB  - Network operators compose Service Function Chains (SFCs) by tying different network functions (e.g., packet inspection, flow shaping, network address translation) together and process traffic flows in the order the network functions are chained. Leveraging the technique of Network Function Virtualization (NFV), each network function can be 'virtualized' and decoupled from its dedicated hardware, and therefore can be deployed flexibly for better performance at any appropriate location of the underlying network infrastructure. However, an SFC often incurs high latency as traffic goes through the virtual network functions one after another. In this article, we first design an algorithm that leverages virtual network function dependency to convert an original SFC into a parallelized SFC (p-SFC). Then, to deploy multiple p-SFCs over the network for serving a large number of users, we model the deployment problem as an Integer Linear Program and propose a heuristic, ParaSFC, based on the Viterbi dynamic programming algorithm to estimate each p-SFC's occupation of the bottleneck resources and adjust the processing order of the p-SFCs in order to approximate the optimal solution. Finally, we conduct extensive trace-driven evaluations and exhibit that, compared to the Greedy method and the state-of-the-art CoordVNF method, ParaSFC reduces the average service latency of all the deployed p-SFCs by about 15 percent through parallelization while accommodating more SFC deployment requests over resource-limited networks.  © 1990-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/TPDS.2020.3043768
VL  - 32
IS  - 5
SP  - 1238
EP  - 1255
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098782371&doi=10.1109%2fTPDS.2020.3043768&partnerID=40&md5=0230d22db125cebfb02db9741e8969f1
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 52</p>
KW  - deployment
KW  - Deployment
KW  - Dynamic programming
KW  - Heuristic algorithms
KW  - Integer programming
KW  - Near-optimal
KW  - Network function virtualization
KW  - Network functions
KW  - Network operator
KW  - Optimal deployment
KW  - parallelization
KW  - Parallelizations
KW  - quality of service
KW  - Quality of service
KW  - Quality-of-service
KW  - service function chain
KW  - Service function chain
KW  - Service functions
KW  - Transfer functions
KW  - Virtual networks
KW  - Virtual reality
KW  - Viterbi algorithm
ER  - 

TY  - CONF
TI  - A load balancing inspired optimization framework for exascale multicore systems: A complex networks approach
AU  - Xiao, Y.
AU  - Xue, Y.
AU  - Nazarian, S.
AU  - Bogdan, P.
T2  - IEEE/ACM International Conference on Computer-Aided Design, Digest of Technical Papers, ICCAD
AB  - Many-core multi-threaded performance is plagued by on-chip communication nonidealities, limited memory bandwidth, and critical sections. Inspired by complex network theory of social communities, we propose a novel methodology to model the dynamic execution of an application and partition the application into an optimal number of clusters for parallel execution. We first adopt an LLVM IR compiler analysis of a specific application and construct a dynamic application dependency graph encoding its computational and memory operations. Next, based on this graph, we propose an optimization model to find the optimal clusters such that (1) the intra-cluster edges are maximized, (2) the execution times of the clusters are nearly equalized, for load balancing, and (3) the cluster size does not exceed the core count. Our novel approach confines data movement to be mainly inside a cluster for power reduction and congestion prevention. Finally, we propose an algorithm to sort the graph of connected clusters topologically and map the clusters onto NoC. Experimental results on a 32-core NoC demonstrate a maximum speedup of 131.82% when compared to thread-based execution. Furthermore, the scalability of our framework makes it a promising software design automation platform. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/ICCAD.2017.8203781
VL  - 2017-November
SP  - 217
EP  - 224
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043503412&doi=10.1109%2fICCAD.2017.8203781&partnerID=40&md5=19a24624d0052b995a4680de4748a9da
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 52</p>
KW  - Complex networks
KW  - Computation theory
KW  - Computer aided design
KW  - Design automations
KW  - Dynamic applications
KW  - Multi-core systems
KW  - Network-on-chip
KW  - On chip communication
KW  - Optimization
KW  - Optimization framework
KW  - Optimization modeling
KW  - Parallel executions
KW  - Social communities
KW  - Software design
KW  - Topology
ER  - 

TY  - JOUR
TI  - A Fluid Scheduling Algorithm for DAG Tasks With Constrained or Arbitrary Deadlines
AU  - Guan, F.
AU  - Peng, L.
AU  - Qiao, J.
T2  - IEEE Transactions on Computers
AB  - A number of scheduling algorithms have been proposed for real-time parallel tasks modeled as Directed Acyclic Graphs (DAGs). Many of them focus on scheduling DAG tasks with implicit deadlines. Fewer studies have considered DAG tasks with constrained deadlines or arbitrary deadlines. In this study, we propose a scheduling strategy based on fluid scheduling theory and we target DAG tasks with constrained or arbitrary deadlines. We prove that the proposed algorithm has a capacity augmentation bound of $\frac{1}{2}(1+\beta +\sqrt{(1+\beta)^2-\frac{4}{m}})$12(1+β+(1+β)2-4m) when scheduling multiple DAG tasks with constrained deadlines, in which $m$m is the number of processors and $\beta$β is the maximum ratio of task period to deadline. This value is lower than the current best result $\beta +2\sqrt{(\beta +1-\frac{1}{m})(1-\frac{1}{m})}$β+2(β+1-1m)(1-1m). We also prove that a capacity augmentation bound of $\frac{1}{2}(1+\sqrt{2}+\sqrt{(1+\sqrt{2})^2-\frac{4\sqrt{2}}{m}})$12(1+2+(1+2)2-42m) is guaranteed by our algorithm in the case of scheduling multiple DAG tasks with deadlines greater than periods. To the best of our knowledge, this is the first capacity augmentation bound that has been proven for scheduling multiple DAG tasks with deadlines greater than periods. Our experiments show that our algorithm outperforms the state of the art scheduling algorithms in the percentage of schedulable task sets.  © 1968-2012 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/TC.2021.3111512
VL  - 71
IS  - 8
SP  - 1860
EP  - 1873
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114739677&doi=10.1109%2fTC.2021.3111512&partnerID=40&md5=d061d3074485eaf1c5d2889f79a9f947
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 8</p>
KW  - Arbitrary deadlines
KW  - Directed acyclic graphs
KW  - Directed graphs
KW  - Graph algorithms
KW  - Maximum ratio
KW  - multi-processor system
KW  - parallel task
KW  - Parallel task
KW  - Real time
KW  - Real-time scheduling
KW  - Scheduling
KW  - Scheduling strategies
KW  - Scheduling theory
KW  - State of the art
ER  - 

TY  - JOUR
TI  - PowerPack: Energy profiling and analysis of high-performance systems and applications
AU  - Ge, R.
AU  - Feng, X.
AU  - Song, S.
AU  - Chang, H.-C.
AU  - Li, D.
AU  - Cameron, K.W.
T2  - IEEE Transactions on Parallel and Distributed Systems
AB  - Energy efficiency is a major concern in modern high-performance computing system design. In the past few years, there has been mounting evidence that power usage limits system scale and computing density, and thus, ultimately system performance. However, despite the impact of power and energy on the computer systems community, few studies provide insight to where and how power is consumed on high-performance systems and applications. In previous work, we designed a framework called PowerPack that was the first tool to isolate the power consumption of devices including disks, memory, NICs, and processors in a high-performance cluster and correlate these measurements to application functions. In this work, we extend our framework to support systems with multicore, multiprocessor-based nodes, and then provide in-depth analyses of the energy consumption of parallel applications on clusters of these systems. These analyses include the impacts of chip multiprocessing on power and energy efficiency, and its interaction with application executions. In addition, we use PowerPack to study the power dynamics and energy efficiencies of dynamic voltage and frequency scaling (DVFS) techniques on clusters. Our experiments reveal conclusively how intelligent DVFS scheduling can enhance system energy efficiency while maintaining performance. © 2010 IEEE.
DA  - 2010///
PY  - 2010
DO  - 10.1109/TPDS.2009.76
VL  - 21
IS  - 5
SP  - 658
EP  - 671
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950629423&doi=10.1109%2fTPDS.2009.76&partnerID=40&md5=d1073f75a6d11d21af117f3d2c0cde9d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 324</p>
KW  - Application execution
KW  - Application functions
KW  - Chip multiprocessing
KW  - CMP-based cluster
KW  - Computer systems
KW  - Distributed system
KW  - Distributed systems
KW  - Dynamic voltage and frequency scaling
KW  - Electric power measurement
KW  - Energy consumption
KW  - Energy efficiency
KW  - Energy management
KW  - Energy profiling
KW  - High performance computing systems
KW  - High performance systems
KW  - High-performance clusters
KW  - In-depth analysis
KW  - Measurements
KW  - Multi core
KW  - Nanotechnology
KW  - Parallel application
KW  - Power Consumption
KW  - Power dynamics
KW  - Power management
KW  - Power managements
KW  - Power measurement
KW  - Power usage
KW  - Support systems
KW  - System energy
KW  - System tools
ER  - 

TY  - JOUR
TI  - DRS: Auto-Scaling for Real-Time Stream Analytics
AU  - Fu, T.Z.J.
AU  - Ding, J.
AU  - Ma, R.T.B.
AU  - Winslett, M.
AU  - Yang, Y.
AU  - Zhang, Z.
T2  - IEEE/ACM Transactions on Networking
AB  - In a stream data analytics system, input data arrive continuously and trigger the processing and updating of analytics results. We focus on applications with real-time constraints, in which, any data unit must be completely processed within a given time duration. To handle fast data, it is common to place the stream data analytics system on top of a cloud infrastructure. Because stream properties, such as arrival rates can fluctuate unpredictably, cloud resources must be dynamically provisioned and scheduled accordingly to ensure real-time responses. It is essential, for existing systems or future developments, to possess the ability of scaling resources dynamically according to the instantaneous workload, in order to avoid wasting resources or failing in delivering the correct analytics results on time. Motivated by this, we propose DRS, a dynamic resource scaling framework for cloud-based stream data analytics systems. DRS overcomes three fundamental challenges: 1 how to model the relationship between the provisioned resources and the application performance, 2 where to best place resources, and 3 how to measure the system load with minimal overhead. In particular, DRS includes an accurate performance model based on the theory of Jackson open queueing networks and is capable of handling arbitrary operator topologies, possibly with loops, splits, and joins. Extensive experiments with real data show that DRS is capable of detecting sub-optimal resource allocation and making quick and effective resource adjustment. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/TNET.2017.2741969
VL  - 25
IS  - 6
SP  - 3338
EP  - 3352
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029168732&doi=10.1109%2fTNET.2017.2741969&partnerID=40&md5=387f66002bc59eb98878e9b444d156c8
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 64</p>
KW  - Cloud computing
KW  - Computation theory
KW  - Data reduction
KW  - Distributed computer systems
KW  - Dynamic scheduling
KW  - Electronic mail
KW  - Feature extraction
KW  - Interactive computer systems
KW  - Optimal scheduling
KW  - Program processors
KW  - queueing network model
KW  - Queueing network model
KW  - Queueing networks
KW  - Queueing theory
KW  - Real time systems
KW  - resource auto-scaling
KW  - Scheduling
KW  - Stream data
KW  - stream data analytics
KW  - Termsa-Cloud computing
KW  - Topology
ER  - 

TY  - JOUR
TI  - A distributed graph-theoretic framework for automatic parallelization in multi-core systems
AU  - Ma, G.
AU  - Xiao, Y.
AU  - Willke, T.
AU  - Ahmed, N.
AU  - Nazarian, S.
AU  - Bogdan, P.
T2  - Proc. Mach. Learn. Syst.
DA  - 2021///
PY  - 2021
VL  - 3
SP  - 550
EP  - 568
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130032940&partnerID=40&md5=bedf4e54b873e424d70a9e1124f727d8
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 12</p>
ER  - 

TY  - JOUR
TI  - What is big data? Bringing big data to the enterprise
T2  - What Is Big Data? Bringing Big Data to the Enterprise
DA  - 2013///
PY  - 2013
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898602221&partnerID=40&md5=19231e4d37d6599c5ed33bebbc7200c8
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 62</p>
ER  - 

TY  - JOUR
TI  - DAG-Fluid: A Real-Time Scheduling Algorithm for DAGs
AU  - Guan, F.
AU  - Qiao, J.
AU  - Han, Y.
T2  - IEEE Transactions on Computers
AB  - Various scheduling algorithms have been proposed for real-time parallel tasks modeled as a Directed Acyclic Graph (DAG). The capacity augmentation bound is a quantitative metric widely used in this field to compare the algorithms. Among the existing algorithms, the lowest capacity augmentation bound for DAG tasks with implicit deadlines is 2, which has been achieved by federated scheduling. To improve the schedulability and lower the capacity augmentation bound, this paper proposes DAG-Fluid, an algorithm based on fluid scheduling. We prove that DAG-Fluid has a capacity augmentation bound of $2-\frac{1}{m+1}$2-1m+1, in which $m$m is the number of processors in the system. Experiments show that DAG-Fluid performs better than the state of the art scheduling algorithms.  © 1968-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/TC.2020.2990282
VL  - 70
IS  - 3
SP  - 471
EP  - 482
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101446740&doi=10.1109%2fTC.2020.2990282&partnerID=40&md5=2857fd5fc0f496cb0b6f078555d6a2f2
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 17</p>
KW  - Directed acyclic graph (DAG)
KW  - Directed graphs
KW  - Graph algorithms
KW  - multi-processor system
KW  - parallel task
KW  - Parallel task
KW  - Quantitative metric
KW  - Real time
KW  - Real time systems
KW  - Real-time scheduling
KW  - Real-time scheduling algorithms
KW  - Schedulability
KW  - Scheduling
KW  - State of the art
ER  - 

TY  - JOUR
TI  - On the energy cost of robustness for green virtual network function placement in 5G virtualized infrastructures
AU  - Marotta, A.
AU  - D'Andreagiovanni, F.
AU  - Kassler, A.
AU  - Zola, E.
T2  - Computer Networks
AB  - Next generation 5G networks will rely on virtualized Data Centers (vDC) to host virtualized network functions on commodity servers. Such Network Function Virtualization (NFV) will lead to significant savings in terms of infrastructure cost and reduced management complexity. However, green strategies for networking and computing inside data centers, such as server consolidation or energy aware routing, should not negatively impact the quality and service level agreements expected from network operators. In this paper, we study how robust strategies that place virtual network functions (VNF) inside vDC impact the energy savings and the protection level against resource demand uncertainty. We propose novel optimization models that allow the minimization of the energy of the computing and network infrastructure which is hosting a set of service chains that implement the VNFs. The model explicitly provides for robustness to unknown or imprecisely formulated resource demand variations, powers down unused routers, switch ports and servers, and calculates the energy optimal VNF placement and network embedding also considering latency constraints on the service chains. We propose both exact and heuristic methods. Our experiments were carried out using the virtualized Evolved Packet Core (vEPC), which allows us to quantitatively assess the trade-off between energy cost, robustness and the protection level of the solutions against demand uncertainty. Our heuristic is able to converge to a good solution in a very short time, in comparison to the exact solver, which is not able to output better results in a longer run as demonstrated by our numerical evaluation. We also study the degree of robustness of a solution for a given protection level and the cost of additional energy needed because of the usage of more computing and network elements. © 2017 Elsevier B.V.
DA  - 2017///
PY  - 2017
DO  - 10.1016/j.comnet.2017.04.045
VL  - 125
SP  - 64
EP  - 75
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019009444&doi=10.1016%2fj.comnet.2017.04.045&partnerID=40&md5=8d4da6071f8a9805794af4cb7b95b6b9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 67</p>
KW  - 5G
KW  - 5G mobile communication systems
KW  - Binary linear programming
KW  - Chains
KW  - Costs
KW  - Degree of robustness
KW  - Economic and social effects
KW  - Energy conservation
KW  - Energy-aware routing
KW  - EPC
KW  - Green computing
KW  - Heuristic methods
KW  - Linear programming
KW  - Management complexity
KW  - Network function virtualization
KW  - Network function virtualization (NFV)
KW  - Network infrastructure
KW  - Optimization
KW  - Power management
KW  - Power management (telecommunication)
KW  - Robust optimization
KW  - Service Level Agreements
KW  - Transfer functions
KW  - Uncertainty analysis
KW  - Virtual reality
KW  - Virtualization
KW  - Virtualized data centers
ER  - 

TY  - JOUR
TI  - Intelligent Service Deployment Policy for Next-Generation Industrial Edge Networks
AU  - Hazra, A.
AU  - Adhikari, M.
AU  - Amgoth, T.
AU  - Srirama, S.N.
T2  - IEEE Transactions on Network Science and Engineering
AB  - Edge computing has appeared as a promising technology for realizing industrial computation data at the edge of the network. The fundamental challenge in edge-enabled industrial networks is how to deploy the service requests while utilizing the available edge resources efficiently. In this paper, we aim to design an intelligent service deployment strategy for simultaneously handling both Industrial Internet of Things (IIoT) generated dynamic service requests and edge resources in the next-generation industrial networks. Initially, we present the objective function as the mixed-integer nonlinear programming problem for optimizing the weighted energy-delay in the edge environment. To accomplish this objective, we model a heuristic-based task execution strategy and exploit the advantage of Deep Reinforcement Learning (DRL) to make accurate decisions in industrial networks. The proposed DRL-based strategy can learn well to control the industrial networks from its own experience and guarantees to handle as many service requests as possible using the set of available resource constraint edge servers. Experimental analysis reveals that the proposed strategy is robust to network changes and achieves better performance than existing algorithms in terms of energy consumption up to 13%, delay minimization by 23%, and other Quality of Service (QoS) parameters.  © 2013 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/TNSE.2021.3122178
VL  - 9
IS  - 5
SP  - 3057
EP  - 3066
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118557915&doi=10.1109%2fTNSE.2021.3122178&partnerID=40&md5=00a1e344f0b7b81124625443e28e4c9d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 34</p>
KW  - Computational modelling
KW  - Deep learning
KW  - deep reinforcement learning
KW  - Deep reinforcement learning
KW  - Delay
KW  - Economic and social effects
KW  - edge computing
KW  - Edge computing
KW  - energy efficiency
KW  - Energy efficiency
KW  - Energy utilization
KW  - Energy-consumption
KW  - Green computing
KW  - Industrial internet of thing
KW  - industrial networks
KW  - Industrial networks
KW  - Integer programming
KW  - Internet of things
KW  - Job analysis
KW  - Nonlinear programming
KW  - Quality control
KW  - Quality of service
KW  - Quality-of-service
KW  - Reinforcement learning
KW  - Reinforcement learnings
KW  - Service deployment
KW  - Task analysis
ER  - 

TY  - JOUR
TI  - DRSIR: A Deep Reinforcement Learning Approach for Routing in Software-Defined Networking
AU  - Casas-Velasco, D.M.
AU  - Rendon, O.M.C.
AU  - Da Fonseca, N.L.S.
T2  - IEEE Transactions on Network and Service Management
AB  - Traditional routing protocols employ limited information to make routing decisions, which leads to slow adaptation to traffic variability and restricted support to the quality of service requirements of applications. To address these shortcomings, in previous work, we proposed RSIR, a routing solution based on Reinforcement Learning (RL) in Software-Defined Networking (SDN). However, RL-based solutions usually suffer an increase in time during the learning process when dealing with large action and state spaces. This paper introduces a different routing approach, called Deep Reinforcement Learning and Software-Defined Networking Intelligent Routing (DRSIR). DRSIR defines a routing algorithm based on Deep RL (DRL) in SDN that overcomes the limitations of RL-based solutions. DRSIR considers path-state metrics to produce proactive, efficient, and intelligent routing that adapts to dynamic traffic changes. DRSIR was evaluated by emulation using real and synthetic traffic matrices. The results show that this solution outperforms the routing algorithms based on Dijkstra's algorithm and RSIR in relation to stretch, packet loss, and delay. Moreover, the results obtained demonstrate that DRSIR provides a practical and feasible solution for routing in SDN.  © 2004-2012 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/TNSM.2021.3132491
VL  - 19
IS  - 4
SP  - 4807
EP  - 4820
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120854771&doi=10.1109%2fTNSM.2021.3132491&partnerID=40&md5=99374df5ada0208d2bec3614dfd64d5b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 68</p>
KW  - Application programs
KW  - Computational modelling
KW  - Computer architecture
KW  - Deep learning
KW  - Deep reinforcement learning
KW  - Intelligent routing
KW  - Network architecture
KW  - Network routing
KW  - Network topology
KW  - Neural networks
KW  - Quality of service
KW  - Reinforcement learning
KW  - Reinforcement learnings
KW  - routing
KW  - Routings
KW  - software defined networking
KW  - Software defined networking
KW  - Software-defined networking.
KW  - Software-defined networkings
KW  - Topology
ER  - 

TY  - JOUR
TI  - Adaptive Placement and Routing for Service Function Chains with Service Deadlines
AU  - Wang, Y.
AU  - Huang, C.-K.
AU  - Shen, S.-H.
AU  - Chiu, G.-M.
T2  - IEEE Transactions on Network and Service Management
AB  - Network Function Virtualization (NFV) pushes the hardware-based network functions to generic servers as software and brings a highly flexible for deployment. The availability of Virtual Machines (VMs) enables the dynamic placement of Virtual Network Functions (VNFs) on demand, and it can reduce a large number of manual configuration processes that increase deployment efficiency. However, some services require more than one VNF to process. Therefore, the network flows need to traverse a set of sequential network functions called Service Function Chain (SFC). How to efficiently route traffic along service function chain and place VNFs in a network under operational constraints is a crucial issue. In this paper, we must overcome two challenges: (1) determining a flow path that traverses suitable network functions in the required order to meet the requirement of services, and (2) considering network loading and other dynamic characteristics when traffic is routed through existing VNFs. Thus, we present methods to solve the routing and placement problems for the service function chain. Our solutions transform the network representation to a virtual layered graph that considers NFV processing latency and allows conventional shortest path algorithms to solve the problem. We are not only pursuing high success rates to serve more flows but also taking into account the execution time of the algorithms.  © 2004-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/TNSM.2021.3086977
VL  - 18
IS  - 3
SP  - 3021
EP  - 3036
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111010229&doi=10.1109%2fTNSM.2021.3086977&partnerID=40&md5=49330c4614bc2296270223e3633e8f9c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 33</p>
KW  - Configuration process
KW  - Dynamic characteristics
KW  - Graph algorithms
KW  - Graph theory
KW  - Network function virtualization
KW  - network function virtualization (NFV)
KW  - Network representation
KW  - Operational constraints
KW  - Placement and routing
KW  - Placement problems
KW  - routing
KW  - scheduling
KW  - service function chain (SFC)
KW  - Service functions
KW  - Shortest path algorithms
KW  - Software-defined networking (SDN)
KW  - Transfer functions
ER  - 

TY  - JOUR
TI  - On the Joint Design of Microservice Deployment and Routing in Cloud Data Centers
AU  - Xu, B.
AU  - Guo, J.
AU  - Ma, F.
AU  - Hu, M.
AU  - Liu, W.
AU  - Peng, K.
T2  - Journal of Grid Computing
AB  - In recent years, internet enterprises have transitioned from traditional monolithic service to microservice architecture to better meet evolving business requirements. However, it also brings great challenges to the resource management of service providers. Existing research has not fully considered the request characteristics of internet application scenarios. Some studies apply traditional task scheduling models and strategies to microservice scheduling scenarios, while others optimize microservice deployment and request routing separately. In this paper, we propose a microservice instance deployment algorithm based on genetic and local search, and a request routing algorithm based on probabilistic forwarding. The service graph with complex dependencies is decomposed into multiple service chains, and the open Jackson queuing network is applied to analyze the performance of the microservice system. Data evaluation results demonstrate that our scheme significantly outperforms the benchmark strategy. Our algorithm has reduced the average response latency by 37%-67% and enhanced request success rate by 8%-115% compared to other baseline algorithms. © The Author(s), under exclusive licence to Springer Nature B.V. 2024.
DA  - 2024///
PY  - 2024
DO  - 10.1007/s10723-024-09759-1
VL  - 22
IS  - 2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188655111&doi=10.1007%2fs10723-024-09759-1&partnerID=40&md5=4f644c5b031b06eb5798982d5f32a085
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 20</p>
KW  - Business requirement
KW  - Cloud data centers
KW  - Joint designs
KW  - Microservice
KW  - Monolithics
KW  - Request routing
KW  - Resource management
KW  - Routings
KW  - Service dependencies
KW  - Service dependency
KW  - Service placement
KW  - Service placements
ER  - 

TY  - JOUR
TI  - A Survey on Mobile Edge Computing: The Communication Perspective
AU  - Mao, Y.
AU  - You, C.
AU  - Zhang, J.
AU  - Huang, K.
AU  - Letaief, K.B.
T2  - IEEE Communications Surveys and Tutorials
AB  - Driven by the visions of Internet of Things and 5G communications, recent years have seen a paradigm shift in mobile computing, from the centralized mobile cloud computing toward mobile edge computing (MEC). The main feature of MEC is to push mobile computing, network control and storage to the network edges (e.g., base stations and access points) so as to enable computation-intensive and latency-critical applications at the resource-limited mobile devices. MEC promises dramatic reduction in latency and mobile energy consumption, tackling the key challenges for materializing 5G vision. The promised gains of MEC have motivated extensive efforts in both academia and industry on developing the technology. A main thrust of MEC research is to seamlessly merge the two disciplines of wireless communications and mobile computing, resulting in a wide-range of new designs ranging from techniques for computation offloading to network architectures. This paper provides a comprehensive survey of the state-of-the-art MEC research with a focus on joint radio-and-computational resource management. We also discuss a set of issues, challenges, and future research directions for MEC research, including MEC system deployment, cache-enabled MEC, mobility management for MEC, green MEC, as well as privacy-aware MEC. Advancements in these directions will facilitate the transformation of MEC from theory to practice. Finally, we introduce recent standardization efforts on MEC as well as some typical MEC application scenarios. © 1998-2012 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/COMST.2017.2745201
VL  - 19
IS  - 4
SP  - 2322
EP  - 2358
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028734877&doi=10.1109%2fCOMST.2017.2745201&partnerID=40&md5=6d1262992dcf95ea5c521e168f340065
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 4660</p>
KW  - 5G mobile communication systems
KW  - Cloud computing
KW  - computation offloading
KW  - Computation offloading
KW  - Computation theory
KW  - Edge computing
KW  - Energy utilization
KW  - fog computing
KW  - green computing
KW  - Green computing
KW  - mobile cloud computing
KW  - Mobile cloud computing
KW  - Mobile communications
KW  - Mobile computing
KW  - Mobile edge computing
KW  - Mobile telecommunication systems
KW  - Natural resources management
KW  - Network architecture
KW  - Resource allocation
KW  - resource management
KW  - Resource management
KW  - Surveys
KW  - Wireless communications
KW  - Wireless telecommunication systems
ER  - 

TY  - JOUR
TI  - Delay and Reliability-Constrained VNF Placement on Mobile and Volatile 5G Infrastructure
AU  - Nemeth, B.
AU  - Molner, N.
AU  - Martin-Perez, J.
AU  - Bernardos, C.J.
AU  - De La Oliva, A.
AU  - Sonkoly, B.
T2  - IEEE Transactions on Mobile Computing
AB  - Ongoing research and industrial exploitation of SDN and NFV technologies promise higher flexibility on network automation and infrastructure optimization. Choosing the location of Virtual Network Functions is a central problem in the automation and optimization of the software-defined, virtualization-based next generation of networks such as 5G and beyond. Network services provided for autonomous vehicles, factory automation, e-health and cloud robotics often require strict delay bounds and reliability constraints influenced by the location of its composing Virtual Network Functions. Robots, vehicles and other end-devices provide significant capabilities such as actuators, sensors and local computation which are essential for some services. Moreover, these devices are continuously on the move and might lose network connection or run out of battery, which further challenge service delivery in this dynamic environment. This work tackles the mobility, and battery restrictions; as well as the temporal aspects and conflicting traits of reliable, low latency service deployment over a volatile network, where mobile compute nodes act as an extension of the cloud and edge computing infrastructure. The problem is formulated as a cost-minimizing Virtual Network Function placement optimization and an efficient heuristic is proposed. The algorithms are extensively evaluated from various aspects by simulation on detailed real-world scenarios. © 2002-2012 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/TMC.2021.3055426
VL  - 21
IS  - 9
SP  - 3150
EP  - 3162
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100456492&doi=10.1109%2fTMC.2021.3055426&partnerID=40&md5=ee15f63ddb408daf7356f39912386746
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 27</p>
KW  - 5G
KW  - 5G mobile communication systems
KW  - cloud
KW  - Computing infrastructures
KW  - Dynamic environments
KW  - edge
KW  - Factory automation
KW  - Industrial research
KW  - Infrastructure as a service (IaaS)
KW  - Network automations
KW  - Network function virtualization
KW  - Next generation of network
KW  - optimization
KW  - Optimization
KW  - Placement optimization
KW  - Real-world scenario
KW  - Reliability constraints
KW  - robots
KW  - Secondary batteries
KW  - Service deployment
KW  - Transfer functions
KW  - URLLC
KW  - VNF placement
ER  - 

TY  - JOUR
TI  - Collaborative Deployment and Routing of Industrial Microservices in Smart Factories
AU  - Hu, M.
AU  - Guo, Z.
AU  - Wen, H.
AU  - Wang, Z.
AU  - Xu, B.
AU  - Xu, J.
AU  - Peng, K.
T2  - IEEE Transactions on Industrial Informatics
AB  - In large smart factories, massive microservices compose complicated modular IT systems, providing various service functions. However, large microservices-based IT systems incur sophisticated communications and invocations among the massive microservices, which calls for efficient orchestration techniques to meet the high requirements in smart factories. Also, complex data interdependencies among microservices tightly couple deployment with routing, further intensifying the difficulties in orchestration. Such challenges demand delicate joint optimization of service deployment and request routing, which however, are neglected by previous work. In this case, this article investigates the collaborative optimization of microservice deployment and routing in smart factories. First, we construct a communication queuing network model to analyze service performance under dynamic load. Second, two heuristics are proposed to provide differentiated deployment and routing schemes for various demands. Finally, rigorous experiments validate that our approach significantly enhances network efficiency across various production scenarios in smart factories.  © 2005-2012 IEEE.
DA  - 2024///
PY  - 2024
DO  - 10.1109/TII.2024.3424347
VL  - 20
IS  - 11
SP  - 12758
EP  - 12770
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202445680&doi=10.1109%2fTII.2024.3424347&partnerID=40&md5=a923af3067b1dafbf4d6b476432bdb1c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 18</p>
KW  - Complex data
KW  - Edge computing
KW  - IT system
KW  - Joint optimization
KW  - Modulars
KW  - Queueing networks
KW  - Queuing network
KW  - queuing networks
KW  - request routing
KW  - Request routing
KW  - Routings
KW  - service deployment
KW  - Service deployment
KW  - Service functions
KW  - smart factory
ER  - 

TY  - JOUR
T2  - Alibab cluster data
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216655226&partnerID=40&md5=d8996e093749339f4bff73a32384598e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2</p>
ER  - 

TY  - JOUR
TI  - Microservice Deployment in Edge Computing Based on Deep Q Learning
AU  - Lv, W.
AU  - Wang, Q.
AU  - Yang, P.
AU  - Ding, Y.
AU  - Yi, B.
AU  - Wang, Z.
AU  - Lin, C.
T2  - IEEE Transactions on Parallel and Distributed Systems
AB  - The microservice deployment strategy is promising in reducing the overall service response time in the microservice-oriented edge computing platform. However, existing works ignore the effect of different interaction frequencies among microservices and the decrease in service execution performance caused by the increased node loads. In this article, we first model the invocation relationships among microservices as an undirected and weighted interaction graph to characterize the communication overhead. Then, we propose a multi-objective microservice deployment problem (MMDP) in edge computing. MMDP aims to minimize the communication overhead while achieving load balance between edge nodes. Without the requirement for domain experts, we propose Reward Sharing Deep Q Learning (RSDQL), a learning-based algorithm, to solve MMDP and obtain the optimal deployment strategy. In addition, to improve the scalability of the services, we propose an Elastic Scaling algorithm (ES) based on heuristics to deal with the dynamic pressure of requests. Finally, we conduct a series of experiments in Kubernetes to evaluate the performance of our approach. Experimental results indicate that, compared with interaction-aware strategy and Kubernetes default strategy, RSDQL has shorter response times, more balanced resource loads, and makes services scale elastically according to the request pressure. © 1990-2012 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/TPDS.2022.3150311
VL  - 33
IS  - 11
SP  - 2968
EP  - 2978
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124746898&doi=10.1109%2fTPDS.2022.3150311&partnerID=40&md5=ed190a294b1f4bb5c1840c962fd259f4
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 87</p>
KW  - Deep learning
KW  - deep Q learning
KW  - Deep Q learning
KW  - Edge computing
KW  - elastic scaling
KW  - Elastic scaling
KW  - interaction awareness
KW  - Interaction awareness
KW  - Learning algorithms
KW  - load balancing
KW  - Load-Balancing
KW  - Microservice
KW  - Multi objective
KW  - multi-objective model
KW  - Multiobjective modeling
KW  - Q-learning
KW  - Reinforcement learning
KW  - Scalings
ER  - 

TY  - JOUR
TI  - DAPR
T2  - Microsoft
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002381577&partnerID=40&md5=0ab337a6605031f498d47b70ef5bf467
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Joint Optimization of Service Deployment and Request Routing for Microservices in Mobile Edge Computing
AU  - Peng, K.
AU  - Wang, L.
AU  - He, J.
AU  - Cai, C.
AU  - Hu, M.
T2  - IEEE Transactions on Services Computing
AB  - Microservices as an emerging architecture are creating new opportunities to enable superior network services in Mobile Edge Computing (MEC). In the presence of huge amounts of user requests, the massive communications among microservices have become notoriously complicated. Due to the intricate data dependencies of the microservices, the overall performance of large-scale MEC applications simultaneously depends on both service deployment and request routing. However, most existing work ignores the interdependencies of microservices and studies the deployment and routing as two isolated problems. In this case, this article investigates the joint optimization of service deployment and request routing in edge computing. We first formulate a delay minimization problem via mixed integer linear programming and queuing analysis, and then provide a hardness proof on the problem. In addition, this article presents a 2-approximation algorithm, followed with rigorous mathematical proofs to demonstrate the approximation ratio. The proposed two-phase algorithm consists of rounding based service deployment and adaptive-scaling-based request routing policies, which employ fine grained joint optimization to minimize service response delay. Finally, we illustrate the near-optimal performance of the proposed algorithm via comprehensive experiments.  © 2008-2012 IEEE.
DA  - 2024///
PY  - 2024
DO  - 10.1109/TSC.2024.3349408
VL  - 17
IS  - 3
SP  - 1016
EP  - 1028
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181562777&doi=10.1109%2fTSC.2024.3349408&partnerID=40&md5=1480f033711980d5b0d0ad57dc8d3105
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 43</p>
KW  - approximation
KW  - Approximation
KW  - Approximation algorithms
KW  - Computer architecture
KW  - Delay
KW  - Heuristic algorithms
KW  - Heuristics algorithm
KW  - Integer programming
KW  - Microservice architecture
KW  - mobile computing
KW  - Mobile computing
KW  - Mobile edge computing
KW  - Mobile-computing
KW  - Network architecture
KW  - Queueing analysis
KW  - request routing
KW  - Request routing
KW  - Routing algorithms
KW  - Routings
KW  - Service computing
KW  - service deployment
KW  - Service deployment
KW  - Services computing
ER  - 

TY  - JOUR
TI  - Multiobjective Optimization for Computation Offloading in Fog Computing
AU  - Liu, L.
AU  - Chang, Z.
AU  - Guo, X.
AU  - Mao, S.
AU  - Ristaniemi, T.
T2  - IEEE Internet of Things Journal
AB  - Fog computing system is an emergent architecture for providing computing, storage, control, and networking capabilities for realizing Internet of Things. In the fog computing system, the mobile devices (MDs) can offload its data or computational expensive tasks to the fog node within its proximity, instead of distant cloud. Although offloading can reduce energy consumption at the MDs, it may also incur a larger execution delay including transmission time between the MDs and the fog/cloud servers, and waiting and execution time at the servers. Therefore, how to balance the energy consumption and delay performance is of research importance. Moreover, based on the energy consumption and delay, how to design a cost model for the MDs to enjoy the fog and cloud services is also important. In this paper, we utilize queuing theory to bring a thorough study on the energy consumption, execution delay, and payment cost of offloading processes in a fog computing system. Specifically, three queuing models are applied, respectively, to the MD, fog, and cloud centers, and the data rate and power consumption of the wireless link are explicitly considered. Based on the theoretical analysis, a multiobjective optimization problem is formulated with a joint objective to minimize the energy consumption, execution delay, and payment cost by finding the optimal offloading probability and transmit power for each MD. Extensive simulation studies are conducted to demonstrate the effectiveness of the proposed scheme and the superior performance over several existed schemes are observed. © 2014 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/JIOT.2017.2780236
VL  - 5
IS  - 1
SP  - 283
EP  - 294
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038366106&doi=10.1109%2fJIOT.2017.2780236&partnerID=40&md5=605cb33fa35e39deb25888c36a3361d1
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 483</p>
KW  - Cloud computing
KW  - Computation theory
KW  - Computational model
KW  - cost
KW  - Cost benefit analysis
KW  - Costs
KW  - Delays
KW  - Digital storage
KW  - Distributed computer systems
KW  - Edge computing
KW  - energy consumption
KW  - Energy utilization
KW  - execution delay
KW  - Fog
KW  - fog computing
KW  - Green computing
KW  - Internet of things
KW  - Mobile cloud computing
KW  - Mobile communications
KW  - Mobile telecommunication systems
KW  - Multiobjective optimization
KW  - offloading probability
KW  - Optimization
KW  - power allocation
KW  - Power allocations
KW  - Queueing theory
ER  - 

TY  - JOUR
TI  - Reliability-Aware Service Function Chaining with Function Decomposition and Multipath Routing
AU  - Qu, L.
AU  - Assi, C.
AU  - Khabbaz, M.J.
AU  - Ye, Y.
T2  - IEEE Transactions on Network and Service Management
AB  - Network Function Virtualization (NFV) converts network functions executed by costly middleboxes into instances of Virtual Network Functions (VNFs) hosted by industry-standard Physical Machines (PMs). This has proven to be quite an efficient approach when it comes to enabling automated network operations and the elastic provisioning of resources to support heterogeneous services. Today's revolutionary services impose a remarkably elevated reliability together with ultra-low latency requirements. Therefore, in addition to having highly reliable VNFs, these VNFs have to be optimally placed in such a way to rapidly route traffic among them with the least utilization of bandwidth. Hence, the proper selection of PMs to meet the above-mentioned reliability and delay requirements becomes a remarkably challenging problem. None of the existing publications addressing such a problem concurrently adopts VNF decomposition to enhance the flexibility of the VNFs' placement and a hybrid routing scheme to achieve an optimal trade-off between the above-mentioned objectives. In this paper, a VNF-decomposition-based backup strategy is proposed together with a delay-aware hybrid multipath routing scheme for enhancing the reliability of NFV-enabled network services while jointly reducing delays these services experience. The problem is formulated as a Mixed Integer Linear Program (MILP) whose resolution yields an optimal VNF placement and traffic routing policy. Next, the delay-aware hybrid shortest path-based heuristic algorithm is proposed to work around the MILP's complexity. Thorough numerical analysis and simulations are conducted to validate the proposed algorithm and evaluate its performance. Results show that the proposed algorithm outperforms its existing counterparts by 7.53% in terms of computing resource consumption. © 2019 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/TNSM.2019.2961153
VL  - 17
IS  - 2
SP  - 835
EP  - 848
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086637787&doi=10.1109%2fTNSM.2019.2961153&partnerID=40&md5=f60f858a9e20e96f9c81077a837ac08c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 60</p>
KW  - Analysis and simulation
KW  - delay
KW  - Economic and social effects
KW  - Function decomposition
KW  - Heterogeneous services
KW  - Heuristic algorithms
KW  - Industry standards
KW  - Integer programming
KW  - Mixed integer linear program
KW  - Multi path routing
KW  - Multi-path routing schemes
KW  - multipath routing
KW  - Network function virtualization
KW  - network functions decomposition
KW  - Network operations
KW  - Network routing
KW  - optimization
KW  - Reliability
KW  - Routing protocols
KW  - Transfer functions
ER  - 

TY  - JOUR
TI  - Holu: Power-Aware and Delay-Constrained VNF Placement and Chaining
AU  - Varasteh, A.
AU  - Madiwalar, B.
AU  - Van Bemten, A.
AU  - Kellerer, W.
AU  - Mas-Machuca, C.
T2  - IEEE Transactions on Network and Service Management
AB  - Service function chains (SFCs) are an ordered set of virtual network functions (VNFs) which can realize a specific network service. Enabled by virtualization technologies, these VNFs are hosted on physical machines (PMs), and interconnected by network switches. In today networks, these resources are usually under-utilized and/or over-provisioned, resulting in power-inefficient deployments. To improve power-efficiency, SFCs should be deployed utilizing the minimum number of PMs and network equipment, which are not concomitant. Considering the existing PM and switch power consumption models and their resource constraints, we formulate the power-aware and delay-constrained joint VNF placement and routing (PD-VPR) problem as an Integer Linear Program (ILP). Due to the NP-completeness of the problem, we propose Holu, a fast heuristic framework that efficiently solves the PD-VPR problem in an online manner. Specifically, Holu decomposes the PD-VPR into two sub-problems and solve them sequentially: i) a VNF placement problem that consists of mapping the VNFs to PMs using a centrality-based ranking method, and ii) a routing problem that efficiently splits the delay budget between consecutive VNFs of the SFC, and finds a Delay-Constrained Least-Cost (DCLC) shortest-path through the selected PMs (hosting VNFs) using the Lagrange Relaxation based Aggregated Cost (LARAC) algorithm. Our simulation results indicate that Holu outperforms the state-of-the-art algorithms in terms of total power consumption and acceptance rate by 24.7% and 31%, respectively.  © 2004-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/TNSM.2021.3055693
VL  - 18
IS  - 2
SP  - 1524
EP  - 1539
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100498450&doi=10.1109%2fTNSM.2021.3055693&partnerID=40&md5=40d10ec633bf722773b2c1cd333c1b17
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 82</p>
KW  - Budget control
KW  - Delay constrained least cost
KW  - Electric power utilization
KW  - energy efficiency
KW  - Integer linear programs
KW  - Integer programming
KW  - Lagrange relaxation
KW  - Placement and routing
KW  - power efficiency
KW  - Power management (telecommunication)
KW  - Power optimization
KW  - Resource Constraint
KW  - service function chaining
KW  - State-of-the-art algorithms
KW  - Total power consumption
KW  - Virtualization technologies
KW  - VNF placement
ER  - 

TY  - JOUR
TI  - The Design and Architecture of Microservices
AU  - Sill, A.
T2  - IEEE Cloud Computing
AB  - Microservices are sweeping through cloud design architectures, at once embodying new trends and making use of previous paradigms. This column explores the basis for these trends in both modern and historical standards, and sets out a direction for the future of microservices development. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/MCC.2016.111
VL  - 3
IS  - 5
SP  - 76
EP  - 80
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997402736&doi=10.1109%2fMCC.2016.111&partnerID=40&md5=f4a9d927dbbe0c63632bfb31c6c8a4d1
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 114</p>
KW  - architecture
KW  - Architecture
KW  - automation
KW  - Automation
KW  - cloud
KW  - Clouds
KW  - Computer architecture
KW  - containers
KW  - Containers
KW  - data
KW  - design
KW  - Design
KW  - Design architecture
KW  - Microservices
KW  - Network architecture
KW  - networks
KW  - Networks (circuits)
KW  - standards
KW  - Standards
ER  - 

TY  - CONF
TI  - Joint server and network energy saving in data centers for latency-sensitive applications
AU  - Zhou, L.
AU  - Chou, C.-H.
AU  - Bhuyan, L.N.
AU  - Ramakrishnan, K.K.
AU  - Wong, D.
T2  - Proceedings - 2018 IEEE 32nd International Parallel and Distributed Processing Symposium, IPDPS 2018
AB  - Achieving energy proportionality in data centers supporting latency-sensitive applications is challenging because of the strict Service Level Agreements. Previous works individually focus on making the server energy proportional or reducing the data center network's power consumption for latency-Tolerant applications. In this paper, we propose EPRONS to minimize the overall data center's power consumption with latency-sensitive applications by trading-off network slack in favor of providing additional slack for computations. We utilize the linear programming model to consolidate latency-sensitive search queries and latency-Tolerant background flows to a minimal subnet of the topology by turning off unused switches and links without violating the application deadlines. Servers take advantage of the additional 'network-provided' slack to allow slowing down request processing. For servers, we design a novel power saving technique using Dynamic Voltage and Frequency Scaling (DVFS) based on the average tail latency of a request. If needed, we turn on a minimal number of additional network links and switches to reduce network latency while still maximizing entire data center's power saving. Experimental results show that our scheme saves up to 31.25% of a data center's total power budget. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/IPDPS.2018.00079
SP  - 700
EP  - 709
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052245147&doi=10.1109%2fIPDPS.2018.00079&partnerID=40&md5=7fc7ec89939d770e61f46cf47eafbebd
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 17</p>
KW  - Budget control
KW  - Data Center
KW  - Data centers
KW  - DVFS
KW  - Dynamic frequency scaling
KW  - Dynamic voltage and frequency scaling
KW  - Electric power utilization
KW  - Energy conservation
KW  - Energy Proportional
KW  - Energy proportionalities
KW  - Green computing
KW  - Latency-Sensitive
KW  - Linear programming
KW  - Linear programming models
KW  - Service Level Agreements
KW  - Topology
KW  - Traffic Consolidation
KW  - Voltage scaling
ER  - 

TY  - CONF
TI  - A microservice store for efficient edge offloading
AU  - Gedeon, J.
AU  - Wagner, M.
AU  - Heuschkel, J.
AU  - Wang, L.
AU  - Muhlhauser, M.
T2  - Proceedings - IEEE Global Communications Conference, GLOBECOM
AB  - Current edge computing frameworks require tight coupling between mobile clients and surrogates, i.e., the offloaded code has been preconfigured with its required execution environment. In many cases, this includes prior transfers of code blocks or execution environments from mobile devices to the offloading infrastructure. This approach incurs additional latency and is detrimental for the energy consumption of the mobile devices. In this paper, we propose the concept of a microservice store. Using the microservice abstraction common in software development and following the serverless paradigm, we envision a repository through which said services are made accessible to developers and can be re-used across applications. We implement a proof-of-concept edge computing system based on a microservice repository and demonstrate its benefits with real-world applications on mobile devices. Our results show that we were able to reduce latencies by up to 14x and save up to 94% of battery life. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/GLOBECOM38437.2019.9014114
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078267072&doi=10.1109%2fGLOBECOM38437.2019.9014114&partnerID=40&md5=8142f21ef34e01d2675b526cc53336b5
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 6</p>
KW  - 'current
KW  - Application programs
KW  - Codes (symbols)
KW  - computation offloading
KW  - Computation offloading
KW  - Computing frameworks
KW  - Cyber foraging
KW  - Edge computing
KW  - Energy utilization
KW  - Execution environments
KW  - Fog computing
KW  - Microservice
KW  - Microservices
KW  - Mobile client
KW  - Serverless
KW  - Software design
KW  - Tight coupling
ER  - 

TY  - JOUR
TI  - Energy-efficient task scheduling for cpu-intensive streaming jobs on hadoop
AU  - Jin, P.
AU  - Hao, X.
AU  - Wang, X.
AU  - Yue, L.
T2  - IEEE Transactions on Parallel and Distributed Systems
AB  - Hadoop, especially Hadoop 2.0, has been a dominant framework for real-Time big data processing. However, Hadoop is not optimized for energy efficiency. Aiming to solve this problem, in this paper, we propose a new framework to improve the energy efficiency of Hadoop 2.0. We focus on the resource manager in Hadoop 2.0, namely YARN, and propose energy-efficient task scheduling mechanisms on YARN. Particularly, we focus on CPU-intensive streaming jobs and classify streaming jobs into two types, namely batch streaming jobs (i.e., a set of jobs are submitted simultaneously) and online streaming jobs (i.e., jobs are continuously submitted one by one). We devise different energy-efficient task scheduling algorithms for each kind of streaming jobs. Specially, we first propose to abstractly model performance and energy consumption by considering the characteristics of tasks as well as the computational resources in YARN. Based on this model, we study the energy efficiency of streaming tasks which consist of the performance model and energy consumption model of task. We propose two key principles for improving energy efficiency: 1) CPU usage aware task allocation, partitions tasks to NMs based on the task characteristic in term of CPU usage; and 2) resource efficient task allocation, reduce idle resource. Then, we propose a D-based binning algorithm for the batch task scheduling and K-based binning algorithm for the online task scheduling that can adapt to continuously arriving tasks. We conduct extensive experiments on a real Hadoop 2.0 cluster and use two kinds of workloads to evaluate the performance and energy efficiency of our proposal. Compared with Storm (the streaming data processing tool in Hadoop 2.0) and other approaches including TAPA and DVFS-MR, our proposal is more energy efficient. The batch task scheduling algorithm reduces up to 10 percent of energy consumption and keeps comparable performance. In addition, the online task scheduling algorithm reduces up to 7 percent over the existing algorithms. © 1990-2012 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/TPDS.2018.2881176
VL  - 30
IS  - 6
SP  - 1298
EP  - 1311
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056563947&doi=10.1109%2fTPDS.2018.2881176&partnerID=40&md5=8261e99b441c86172b6945e2365de31a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 28</p>
KW  - Big data
KW  - Computational resources
KW  - Data handling
KW  - Energy efficiency
KW  - Energy utilization
KW  - Green computing
KW  - Hadoop
KW  - Job analysis
KW  - Model performance
KW  - Multitasking
KW  - Processor scheduling
KW  - Resource managers
KW  - Resource-efficient
KW  - scheduling algorithms
KW  - Scheduling algorithms
KW  - Servers
KW  - Task analysis
KW  - Task-scheduling algorithms
KW  - Wool
KW  - Yarn
KW  - YARN
ER  - 

TY  - JOUR
TI  - Collaborative, distributed, scalable and low-cost platform based on microservices, containers, mobile devices and cloud services to solve compute-intensive tasks
AU  - Petrocelli, D.
AU  - De Giusti, A.
AU  - Naiouf, M.
T2  - Proc. Eur. Conf. Parallel Process.
DA  - 2021///
PY  - 2021
SP  - 545
EP  - 548
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002370244&partnerID=40&md5=1c5d75ca542955c2f0cc3ccc0c0a0402
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - On the Linearity of Performance and Energy at Virtual Machine Consolidation: The CiS2 Index for CPU Workload in Server Saturation
AU  - Bermejo, B.
AU  - Juiz, C.
AU  - Guerrero, C.
T2  - Proceedings - 20th International Conference on High Performance Computing and Communications, 16th International Conference on Smart City and 4th International Conference on Data Science and Systems, HPCC/SmartCity/DSS 2018
AB  - HPC-based datacenters consume significant energy, despite of a multitude of efforts in reducing it. Virtual Machine Consolidation is an effective technique to minimize the number of physical servers in order to reduce energy consumption, and, consequently, data center costs. However, consolidating virtual machines also reduces performance of servers. Thus, an efficient number of virtual machines consolidated in a physical server is one key issue to optimize the tradeoff between performance and energy efficiency in datacenters. In this paper, we propose a new index, namely CiS2 (Consolidation index for Server CPU in Saturation), to quantify the relation of performance degradation and energy efficiency of virtual machine consolidation under parallel workload execution, that is, the tradeoff of performance and energy in consolidation. As a result, CiS2 provides an approach to help performance engineers to decide about the suitability of a number of consolidated virtual machines. CiS2 could be modified and extended to any other non-functional features for servers. © 2018 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/HPCC/SmartCity/DSS.2018.00154
SP  - 928
EP  - 933
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062433188&doi=10.1109%2fHPCC%2fSmartCity%2fDSS.2018.00154&partnerID=40&md5=14f92fbad14e438cfffe87d51844b465
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 6</p>
KW  - Consolidation
KW  - Data centers
KW  - Data communication systems
KW  - Energy and performance tradeoff
KW  - Energy efficiency
KW  - Energy utilization
KW  - Green computing
KW  - Key Issues
KW  - Network security
KW  - Non-functional features
KW  - Parallel workloads
KW  - Performance degradation
KW  - Reduce energy consumption
KW  - Smart city
KW  - Virtual machine
KW  - Virtual machine consolidations
KW  - Virtualization
KW  - VM performance degradation
ER  - 

TY  - JOUR
TI  - Exploring efficient ML-based scheduler for microservices in heterogenous clusters
AU  - Mahapatra, R.
AU  - Ahn, B.H.
AU  - Wang, S.-T.
AU  - Xu, H.
AU  - Esmaeilzadeh, H.
T2  - Proc. Mach. Learn. Comput. Archit. Syst.
DA  - 2022///
PY  - 2022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190293145&partnerID=40&md5=90d0084ef4f9c08a4d997d9044afe803
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2</p>
ER  - 

TY  - JOUR
TI  - Endogenous Trusted DRL-Based Service Function Chain Orchestration for IoT
AU  - Guo, S.
AU  - Qi, Y.
AU  - Jin, Y.
AU  - Li, W.
AU  - Qiu, X.
AU  - Meng, L.
T2  - IEEE Transactions on Computers
AB  - With the development of the Internet of Things, trust has become a limited factor in the integration of heterogeneous IoT networks. In this regard, we use the combination of blockchain technology and SDN/NFV to build a heterogeneous IoT network resource management model based on the consortium chain. In order to solve the efficiency problem caused by the full amount of data on the chain, we deploy light nodes and full nodes for the consortium chain. At the same time, we use the idea of identification to realize the separation of identification and resource information, build the application mode of on-chain identification and off-chain information, and realize resources endogenous trust management. We also propose a practical Byzantine fault-tolerant consensus mechanism based on reputation value to save consensus costs and improve efficiency. Combined with artificial intelligence technology, we introduce deep reinforcement learning for service function chain orchestration, and design a service function chain orchestration algorithm based on Asynchronous Advantage Actor-Critic to optimize orchestration costs. The final simulation results show that the consensus algorithm and service function chain orchestration algorithm we designed have good performance in terms of cost saving and efficiency improvement.  © 1968-2012 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/TC.2021.3051681
VL  - 71
IS  - 2
SP  - 397
EP  - 406
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099727569&doi=10.1109%2fTC.2021.3051681&partnerID=40&md5=49e53a9b6cd92ecfb13d19aa7cb3763d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 42</p>
KW  - Application modes
KW  - Artificial intelligence technologies
KW  - blockchain
KW  - Byzantine fault
KW  - Consensus algorithms
KW  - Deep learning
KW  - Efficiency
KW  - Efficiency improvement
KW  - endogenous trust
KW  - Information management
KW  - Internet of things
KW  - Internet of Things
KW  - Reinforcement learning
KW  - Resource information
KW  - resource management
KW  - service function chain
KW  - Service functions
KW  - Trust management
ER  - 

TY  - JOUR
TI  - Joint Deployment and Request Routing for Microservice Call Graphs in Data Centers
AU  - Hu, Y.
AU  - Wang, H.
AU  - Wang, L.
AU  - Hu, M.
AU  - Peng, K.
AU  - Veeravalli, B.
T2  - IEEE Transactions on Parallel and Distributed Systems
AB  - Microservices are an architectural and organizational paradigm for Internet application development. In cloud data centers, delay-sensitive applications receive massive user requests, which are fed into multiple queues and subsequently served by multiple microservice instances. Accordingly, effective deployment of multiple queues and containers can significantly reduce queuing delay, processing delay, and communication delay. Due to the increased complexity of call dependencies and probabilistic routing paths, the deployment of service instances fully interacts with request routing, bringing great difficulties to service orchestration. In this case, it is valuable to simultaneously consider service deployment and request routing in a fine-grained manner. However, most existing studies considered them as two independent components with local optimization, while data dependencies and the instance-level deployment are ignored. Therefore, this paper proposes to jointly optimize the deployment and request routing of microservice call graphs based on fine-grained queuing network analysis and container orchestration. We first formulate the problem as a mixed-integer nonlinear program and exploit open Jackson queuing networks to model intrinsic data dependencies and analyze response latency. To optimize the overall cost and latency, this paper presents an efficient two-stage heuristic algorithm, which consists of a resource-splitting-based deployment approach and a partition-mapping-based routing method. Further, this paper also provides mathematical analysis on the performance and complexity of the proposed algorithm. Finally, comprehensive trace-driven experiments demonstrate that the overall performance of our approach is better than existing microservice benchmarks. The average deployment cost is reduced by 27.4% and end-to-end response latency is reduced by 15.1% on average. © 2023 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/TPDS.2023.3311767
VL  - 34
IS  - 11
SP  - 2994
EP  - 3011
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171593998&doi=10.1109%2fTPDS.2023.3311767&partnerID=40&md5=82cdb534f807349b9a3d1ebe6273d335
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 30</p>
KW  - Benchmarking
KW  - Call graphs
KW  - Cloud data centers
KW  - Complex networks
KW  - Containers
KW  - Datacenter
KW  - Delay
KW  - Delay-sensitive applications
KW  - Integer programming
KW  - Microservice architecture
KW  - Microservice call graph
KW  - microservice call graphs
KW  - Nonlinear programming
KW  - Optimisations
KW  - Queueing analysis
KW  - Queueing networks
KW  - Queueing theory
KW  - queuing theory
KW  - Queuing theory
KW  - request routing
KW  - Request routing
KW  - Routings
KW  - Service computing
KW  - service deployment
KW  - Service deployment
KW  - services computing
ER  - 

TY  - JOUR
TI  - Delay-Aware Virtual Network Function Placement and Routing in Edge Clouds
AU  - Yang, S.
AU  - Li, F.
AU  - Trajanovski, S.
AU  - Chen, X.
AU  - Wang, Y.
AU  - Fu, X.
T2  - IEEE Transactions on Mobile Computing
AB  - Mobile Edge Computing (MEC) offers a way to shorten the cloud servicing delay by building the small-scale cloud infrastructures at the network edge, which are in close proximity to the end users. Moreover, Network Function Virtualization (NFV) has been an emerging technology that transforms from traditional dedicated hardware implementations to software instances running in a virtualized environment. In NFV, the requested service is implemented by a sequence of Virtual Network Functions (VNF) that can run on generic servers by leveraging the virtualization technology. Service Function Chaining (SFC) is defined as a chain-ordered set of placed VNFs that handles the traffic of the delivery and control of a specific application. NFV therefore allows to allocate network resources in a more scalable and elastic manner, offer a more efficient and agile management and operation mechanism for network functions and hence can largely reduce the overall costs in MEC. In this paper, we study the problem of how to place VNFs on edge and public clouds and route the traffic among adjacent VNF pairs, such that the maximum link load ratio is minimized and each user's requested delay is satisfied. We consider this problem for both totally ordered SFCs and partially ordered SFCs. We prove that this problem is NP-hard, even for the special case when only one VNF is requested. We subsequently propose an efficient randomized rounding approximation algorithm to solve this problem. Extensive simulation results show that the proposed approximation algorithm can achieve close-to-optimal performance in terms of acceptance ratio and maximum link load ratio.  © 2002-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/TMC.2019.2942306
VL  - 20
IS  - 2
SP  - 445
EP  - 459
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099536735&doi=10.1109%2fTMC.2019.2942306&partnerID=40&md5=b380ca694fed9df8a50b0120e29be872
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 131</p>
KW  - Approximation algorithms
KW  - Cloud infrastructures
KW  - delay
KW  - Emerging technologies
KW  - Extensive simulations
KW  - mobile edge computing
KW  - Network function virtualization
KW  - NP-hard
KW  - Optimal performance
KW  - placement
KW  - Placement and routing
KW  - Randomized rounding
KW  - routing
KW  - Transfer functions
KW  - Virtualization technologies
KW  - Virtualized environment
ER  - 

TY  - JOUR
TI  - READ: Robustness-Oriented Edge Application Deployment in Edge Computing Environment
AU  - Li, B.
AU  - He, Q.
AU  - Cui, G.
AU  - Xia, X.
AU  - Chen, F.
AU  - Jin, H.
AU  - Yang, Y.
T2  - IEEE Transactions on Services Computing
AB  - In recent years, edge computing has emerged as a prospective distributed computing paradigm that overcomes several limitations of cloud computing. In the edge computing environment, a service provider can deploy its application instances on edge servers at the edge of the network to serve its own users with low latency. Given a limited budget KK for deploying applications on the edge servers in a particular geographical area, a number of approaches have been proposed very recently to determine the optimal deployment strategy that achieves various optimization objectives, e.g., to maximize the servers' coverage, to minimize the average network latency, etc. However, the robustness of the services collectively delivered by the service provider's applications deployed on the edge servers has not been considered at all. This is a critical issue, especially in the highly distributed, dynamic and volatile edge computing environment. In this article, we make the first attempt to tackle this challenge. Specifically, we formulate this Robustness-oriented Edge Application Deployment (READ) problem as a constrained optimization problem and prove its NP NP-hardness. Then, we provide an integer programming based approach named READ-O for solving this problem precisely. We also provide an approximation algorithm, namely READ-A, for finding near-optimal solutions to large-scale READ problems efficiently. We prove its approximation ratio is not worse than K/2K/2, which is a constant regardless of the total number of edge servers. We evaluate our approaches experimentally on a widely-used real-world dataset against five representative approaches. The experiment results demonstrate that our approaches can solve the READ problem effectively and efficiently.  © 2008-2012 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/TSC.2020.3015316
VL  - 15
IS  - 3
SP  - 1746
EP  - 1759
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089450751&doi=10.1109%2fTSC.2020.3015316&partnerID=40&md5=431de19bf0c171d769c3d3b5c9344b23
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 88</p>
KW  - application deployment
KW  - Application deployment
KW  - Approximation algorithms
KW  - approximation approach
KW  - Approximation ratios
KW  - Budget control
KW  - Computing environments
KW  - Constrained optimi-zation problems
KW  - Constrained optimization
KW  - Edge computing
KW  - Geographical area
KW  - integer programming
KW  - Integer programming
KW  - Large-scale problem
KW  - Near-optimal solutions
KW  - Network latencies
KW  - optimal approach
KW  - robustness
ER  - 

TY  - JOUR
TI  - Energy-Efficient Computation Offloading With DVFS Using Deep Reinforcement Learning for Time-Critical IoT Applications in Edge Computing
AU  - Panda, S.K.
AU  - Lin, M.
AU  - Zhou, T.
T2  - IEEE Internet of Things Journal
AB  - Internet of Things (IoT) is a technology that allows ordinary physical devices to collect, process, and share data with other physical devices and systems over the Internet. It provides pervasively connected infrastructures to support innovative applications and services that can automate otherwise intensely laborious manual effort. Edge computing (EC) complements the powerful centralized cloud servers by providing powerful computation capability close to the data source, minimizing communication latency, and securing data privacy. The energy consumption problem has continued to receive much attention from the IoT community in applying various techniques to reduce energy consumption while still meeting the computational demand. In this article, we propose an application-deadline-aware data offloading scheme using deep reinforcement learning and dynamic voltage and frequency scaling (DVFS) in an EC environment to reduce the energy consumption of IoT devices. The proposed scheme learns the optimal data distribution policies and local computation DVFS frequency scaling by interacting with the system environment and learning the behavior of the device, network, and edge servers. The proposed scheme was tested on multiple EC environments with different IoT devices. Experimental results show that this scheme can reduce energy consumption while achieving the IoT application and services timing and computational goals. The proposed scheme has substantial energy savings when compared with the native Linux governors.  © 2014 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/JIOT.2022.3153399
VL  - 10
IS  - 8
SP  - 6611
EP  - 6621
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125328925&doi=10.1109%2fJIOT.2022.3153399&partnerID=40&md5=a26d92e26b17118fa1b89b019c11670c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 38</p>
KW  - computation offloading
KW  - Computational modelling
KW  - Computer operating systems
KW  - Deep learning
KW  - Deep reinforcement learning (DRL)
KW  - Deep reinforcement learning.
KW  - Dynamic frequency scaling
KW  - dynamic voltage and frequency scaling
KW  - Dynamic voltage and frequency scaling
KW  - Edge computing
KW  - edge computing (EC)
KW  - edge server
KW  - Edge server
KW  - energy consumption
KW  - Energy efficiency
KW  - Energy utilization
KW  - Energy-consumption
KW  - Green computing
KW  - Interactive computer systems
KW  - Internet of things
KW  - Internet of Things (IoT)
KW  - Job analysis
KW  - offloading
KW  - Offloading
KW  - Performances evaluation
KW  - Real - Time system
KW  - Real time systems
KW  - Reinforcement learning
KW  - Reinforcement learnings
KW  - Task analysis
KW  - Voltage scaling
ER  - 

TY  - JOUR
TI  - Energy-quality-time optimized task mapping on DVFS-enabled multicores
AU  - Mo, L.
AU  - Kritikakou, A.
AU  - Sentieys, O.
T2  - IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems
AB  - Multicore architectures have great potential for energy-constrained embedded systems, such as energy-harvesting wireless sensor networks. Some embedded applications, especially the real-time ones, can be modeled as imprecise computation tasks. A task is divided into a mandatory subtask that provides a baseline quality-of-service (QoS) and an optional subtask that refines the result to increase the QoS. Combining dynamic voltage and frequency scaling, task allocation, and task adjustment, we can maximize the system QoS under real-time and energy supply constraints. However, the nonlinear and combinatorial nature of this problem makes it difficult to solve. This paper first formulates a mixed-integer nonlinear programming problem to concurrently carry out task-to-processor allocation, frequency-to-task assignment and optional task adjustment. We provide a mixed-integer linear programming form of this formulation without performance degradation and we propose a novel decomposition algorithm to provide an optimal solution with reduced computation time compared to state-of-the-art optimal approaches (22.6% in average). We also propose a heuristic version that has negligible computation time. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/TCAD.2018.2857300
VL  - 37
IS  - 11
SP  - 2428
EP  - 2439
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050245250&doi=10.1109%2fTCAD.2018.2857300&partnerID=40&md5=671caf21b52f9a6a05957a2fca5e4fdc
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 35</p>
KW  - Computer architecture
KW  - Dynamic frequency scaling
KW  - Embedded systems
KW  - Energy harvesting
KW  - Integer programming
KW  - Mapping
KW  - MILP
KW  - Mixed-integer linear programming (MILP)
KW  - multicore architectures
KW  - Multicore architectures
KW  - Network architecture
KW  - Nonlinear programming
KW  - problem decomposition
KW  - Problem decomposition
KW  - Quality of service
KW  - quality-of-service (QoS)
KW  - Real time
KW  - real-time and energy constraints
KW  - Software architecture
KW  - task mapping
KW  - Task mapping
KW  - Voltage scaling
KW  - Wireless sensor networks
ER  - 

TY  - JOUR
TI  - Mobile Edge Computing: A Survey on Architecture and Computation Offloading
AU  - Mach, P.
AU  - Becvar, Z.
T2  - IEEE Communications Surveys and Tutorials
AB  - Technological evolution of mobile user equipment (UEs), such as smartphones or laptops, goes hand-in-hand with evolution of new mobile applications. However, running computationally demanding applications at the UEs is constrained by limited battery capacity and energy consumption of the UEs. A suitable solution extending the battery life-time of the UEs is to offload the applications demanding huge processing to a conventional centralized cloud. Nevertheless, this option introduces significant execution delay consisting of delivery of the offloaded applications to the cloud and back plus time of the computation at the cloud. Such a delay is inconvenient and makes the offloading unsuitable for real-time applications. To cope with the delay problem, a new emerging concept, known as mobile edge computing (MEC), has been introduced. The MEC brings computation and storage resources to the edge of mobile network enabling it to run the highly demanding applications at the UE while meeting strict delay requirements. The MEC computing resources can be exploited also by operators and third parties for specific purposes. In this paper, we first describe major use cases and reference scenarios where the MEC is applicable. After that we survey existing concepts integrating MEC functionalities to the mobile networks and discuss current advancement in standardization of the MEC. The core of this survey is, then, focused on user-oriented use case in the MEC, i.e., computation offloading. In this regard, we divide the research on computation offloading to three key areas: 1) decision on computation offloading; 2) allocation of computing resource within the MEC; and 3) mobility management. Finally, we highlight lessons learned in area of the MEC and we discuss open research challenges yet to be addressed in order to fully enjoy potentials offered by the MEC. © 1998-2012 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/COMST.2017.2682318
VL  - 19
IS  - 3
SP  - 1628
EP  - 1656
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028360031&doi=10.1109%2fCOMST.2017.2682318&partnerID=40&md5=f8175210396621bb960b65c83bb5e929
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2848</p>
KW  - allocation of computing resources
KW  - Allocation of computing resources
KW  - computation offloading
KW  - Computation offloading
KW  - Computer architecture
KW  - Edge computing
KW  - Electric batteries
KW  - Energy utilization
KW  - Mobile applications
KW  - Mobile edge computing
KW  - mobile network architecture
KW  - Mobile telecommunication systems
KW  - mobility management
KW  - Mobility management
KW  - Network architecture
KW  - Real-time application
KW  - Research challenges
KW  - standardization
KW  - Standardization
KW  - Surveys
KW  - Technological evolution
KW  - use-cases
KW  - Wireless networks
ER  - 

TY  - JOUR
TI  - Multi-Objective Deep Reinforcement Learning Assisted Service Function Chains Placement
AU  - Bi, Y.
AU  - Meixner, C.C.
AU  - Bunyakitanon, M.
AU  - Vasilakos, X.
AU  - Nejabati, R.
AU  - Simeonidou, D.
T2  - IEEE Transactions on Network and Service Management
AB  - The study of Service Function Chains (SFCs) placement problem is crucial to support services flexibly and use resources efficiently. Solutions should satisfy various Quality of Service requirements, avoid edge resource congestion, and improve service acceptance ratio (SAR). This work presents a novel approach to address these challenges by solving a multi-objective SFCs placement problem based on the Pointer Network in multi-layer edge and cloud networks. We design a Deep Reinforcement Learning algorithm, called Chebyshev-assisted Actor-Critic SFCs Placement Algorithm, to overcome the limitations of traditional heuristic and evolutionary algorithms. Then, we run this algorithm iteratively with a set of weights to obtain non-dominated fronts, which have much higher hypervolume values than those obtained from other state-of-the-art algorithms. Moreover, running our algorithm individually with selected weights from non-dominated fronts can avoid edge resource congestion and achieve 98% SARs of low-latency services during high-workload periods. Finally, based on both simulation and real testbed experimental results, it is validated that the proposed algorithm fits for pragmatic service deployment while achieving 100% of SARs in the use cases deployed on the testbed. © 2004-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/TNSM.2021.3127685
VL  - 18
IS  - 4
SP  - 4134
EP  - 4150
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118978329&doi=10.1109%2fTNSM.2021.3127685&partnerID=40&md5=4dca8f90e49383b3a798f5e5150acaf9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 48</p>
KW  - Approximation algorithms
KW  - Chains
KW  - Cloud-computing
KW  - Computational modelling
KW  - Deep learning
KW  - Distributed computer systems
KW  - Edge computing
KW  - Evolutionary algorithms
KW  - Heuristic algorithms
KW  - Heuristics algorithm
KW  - Iterative methods
KW  - Learning algorithms
KW  - Markov processes
KW  - Multi objective
KW  - Multi-access edge computing
KW  - Multi-objective deep reinforcement learning
KW  - Multiaccess
KW  - Network function virtualisation
KW  - Network function virtualization
KW  - Network layers
KW  - Optical network
KW  - Optical network.
KW  - Optimisations
KW  - Optimization
KW  - Quality of service
KW  - Quality-of-service
KW  - Reinforcement learning
KW  - Service function chaining
KW  - Service functions
KW  - Testbeds
ER  - 

TY  - JOUR
TI  - Energy-Efficient Provisioning for Service Function Chains to Support Delay-Sensitive Applications in Network Function Virtualization
AU  - Sun, G.
AU  - Zhou, R.
AU  - Sun, J.
AU  - Yu, H.
AU  - Vasilakos, A.V.
T2  - IEEE Internet of Things Journal
AB  - The efficient deployment of virtual network functions (VNFs) for network service provisioning is key for achieving network function virtualization (NFV); however, most existing studies address only offline or one-off deployments of service function chains (SFCs) while neglecting the dynamic (i.e., online) deployment and expansion requirements. In particular, many methods of energy/resource cost reduction are achieved by merging VNFs. However, the energy waste and device wear for large-scale collections of servers (e.g., cloud networks and data centers) caused by sporadic request updating are ignored. To solve these problems, we propose an energy-aware routing and adaptive delayed shutdown (EAR-ADS) algorithm for dynamic SFC deployment, which includes the following features: 1) energy-aware routing (EAR): by considering a practical deployment environment, a flexible solution is developed based on reusing open servers and selecting paths with the aims of balancing energy and resources and minimizing the total cost and 2) adaptive delayed shutdown (ADS): the delayed shutdown time of the servers can be flexibly adjusted in accordance with the usage of each device in each time slot, thus eliminating the no-load wait time of the servers and frequent on/off switching. Therefore, the EAR-ADS can achieve dual-energy savings by both decreasing the number of open servers and reducing the idle/switching energy consumption of these servers. The simulation results show that EAR-ADS not only minimizes the cost of energy and resources but also achieves an excellent success rate and stability. Moreover, EAR-ADS is efficient compared with an improved Markov algorithm (SAMA), reducing the average deployment time by more than a factor of 40. © 2014 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/JIOT.2020.2970995
VL  - 7
IS  - 7
SP  - 6116
EP  - 6131
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082944215&doi=10.1109%2fJIOT.2020.2970995&partnerID=40&md5=d5b0857caed3ac4216a59a27af859485
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 79</p>
KW  - Balancing energy
KW  - Cost of energies
KW  - Cost reduction
KW  - Deceleration
KW  - Delay-sensitive applications
KW  - Dynamic deployment
KW  - Energy efficiency
KW  - energy efficient
KW  - Energy efficient
KW  - Energy utilization
KW  - Energy-aware routing
KW  - Green computing
KW  - Network function virtualization
KW  - network function virtualization (NFV)
KW  - Network services
KW  - Power management (telecommunication)
KW  - service function chain (SFC)
KW  - Service functions
KW  - Transfer functions
KW  - Virtual addresses
KW  - Virtual networks
ER  - 

TY  - JOUR
TI  - Fast packet processing with EBPF and XDP: Concepts, code, challenges, and applications
AU  - Vieira, M.A.M.
AU  - Castanho, M.S.
AU  - Pacífico, R.D.G.
AU  - Santos, E.R.S.
AU  - Câmara, E.P.M.
AU  - Vieira, L.F.M.
T2  - ACM Computing Surveys
AB  - Extended Berkeley Packet Filter (eBPF) is an instruction set and an execution environment inside the Linux kernel. It enables modification, interaction, and kernel programmability at runtime. eBPF can be used to program the eXpress Data Path (XDP), a kernel network layer that processes packets closer to the NIC for fast packet processing. Developers can write programs in C or P4 languages and then compile to eBPF instructions, which can be processed by the kernel or by programmable devices (e.g., SmartNICs). Since its introduction in 2014, eBPF has been rapidly adopted by major companies such as Facebook, Cloudflare, and Netronome. Use cases include network monitoring, network traffic manipulation, load balancing, and system profiling. This work aims to present eBPF to an inexpert audience, covering the main theoretical and fundamental aspects of eBPF and XDP, as well as introducing the reader to simple examples to give insight into the general operation and use of both technologies. © 2020 Association for Computing Machinery.
DA  - 2021///
PY  - 2021
DO  - 10.1145/3371038
VL  - 53
IS  - 1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079572561&doi=10.1145%2f3371038&partnerID=40&md5=ff564b94cfa31c849985ed61e8350d5e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 200</p>
KW  - Berkeley packet filters
KW  - C (programming language)
KW  - Computer networking
KW  - Computer operating systems
KW  - Data paths
KW  - Execution environments
KW  - Instruction set
KW  - Linux kernel
KW  - Network coding
KW  - Network functions
KW  - Network layers
KW  - Packet networks
KW  - Packet processing
KW  - Programmability
KW  - Runtimes
ER  - 

TY  - JOUR
TI  - Toward 5G cloud radio access network: An energy and latency perspective
AU  - Bassoli, R.
AU  - Granelli, F.
AU  - Arzo, S.T.
AU  - Di Renzo, M.
T2  - Transactions on Emerging Telecommunications Technologies
AB  - Future generation networks will entirely deploy virtualization paradigms to enhance performance and capabilities of current cellular networks. In order to achieve the vision of fifth-generation networks, software-defined networking and network function virtualization will be applied not only at the core network but also at the radio access network. That will help to achieve significant reduction in power consumption while increasing energy efficiency, flexibility, and scalability. This article proposes a general mathematical model that can correctly and accurately describe spatial/topological characteristics, power consumption, and latency of Cloud radio access network in future generation networks. Thanks to the development of this novel model based on stochastic geometry, tessellation theory, and random multilayer hypergraphs, we can numerically estimate the overall energy efficiency (in bit per Joule) of Cloud radio access network in 5G (considering either edge or cloud computing), and we can compare that to energy efficiency of legacy radio access network of current 4G cellular networks. Moreover, the analysis includes a preliminary discussion about latency; that shows edge computing to be the best paradigm for 5G radio access network, which can concurrently satisfy energy efficiency and latency requirements. © 2019 John Wiley & Sons, Ltd.
DA  - 2021///
PY  - 2021
DO  - 10.1002/ett.3669
VL  - 32
IS  - 1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067701113&doi=10.1002%2fett.3669&partnerID=40&md5=f55c7a9e02ad4fd27ce30a21b752a9b4
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 16</p>
KW  - 5G mobile communication systems
KW  - Cellular network
KW  - Computation theory
KW  - Core networks
KW  - Electric power utilization
KW  - Energy efficiency
KW  - Future generations
KW  - Graph theory
KW  - Green computing
KW  - Hyper graph
KW  - Mobile telecommunication systems
KW  - Model-based OPC
KW  - Network function virtualization
KW  - Overall energy efficiency
KW  - Radio
KW  - Radio access networks
KW  - Random multilayers
KW  - Stochastic geometry
KW  - Stochastic models
KW  - Stochastic systems
KW  - Wireless networks
ER  - 

TY  - CONF
TI  - Analysis and evaluation of kubernetes based NFV management and orchestration
AU  - Gawel, M.
AU  - Zielinski, K.
T2  - IEEE International Conference on Cloud Computing, CLOUD
AB  - The paper confronts MANO specification aspects with Kubernetes fundamental containers orchestration mechanisms. It evaluates in which degree Containerized Network Functions (CNFs) can be managed with the use of Kubernetes platform. To confirm the analysis, a series of stress and chaos tests are conducted on the Kubernetes testbed with the use of an exemplary virtual IP Multimedia System. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/CLOUD.2019.00094
VL  - 2019-July
SP  - 511
EP  - 513
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072319638&doi=10.1109%2fCLOUD.2019.00094&partnerID=40&md5=37e6fcd4a2f614a54ca05fa436954b05
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 15</p>
KW  - Analysis and evaluation
KW  - Cloud computing
KW  - Containers
KW  - IP multimedia system
KW  - Kubernetes
KW  - Management
KW  - MANO
KW  - Multimedia systems
KW  - Network function virtualization
KW  - Network functions
KW  - NFV
KW  - Orchestration
ER  - 

TY  - JOUR
TI  - Re-architecting nfv ecosystem with microservices: State of the art and research challenges
AU  - Chowdhury, S.R.
AU  - Salahuddin, M.A.
AU  - Limam, N.
AU  - Boutaba, R.
T2  - IEEE Network
AB  - Network Function Virtualization (NFV), considered a key enabler of network "softwarization", promises to reduce capital and operational expenditures for network operators by moving packet processing from purpose-built hardware to software running on commodity servers. However, the state-of-the-art in NFV is merely replacing monolithic hardware with monolithic VNFs, the software that realizes different network functions (e.g., firewalls, WAN optimizers, and so on). Although this is a first step toward deploying NFV, common functionality is repeatedly implemented in monolithic VNFs. Repeated execution of such redundant functionality introduces processing overhead when VNFs are chained to realize Service Function Chains and leads to sub-optimal usage of infrastructure resources. This stresses the need for re-architecting the NFV ecosystem, from VNFs to their orchestration, through modular VNF design and flexible service composition. In that perspective, we make the case for using the microservice software architecture, proven to be effective for building large-scale cloud applications from reusable and independently deployable components, to re-architect the NFV ecosystem. We also discuss the state-of-the-art in realizing modular VNFs from both industry and academia. Finally, we outline a set of research challenges for microservice-based NFV platforms. © 1986-2012 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/MNET.2019.1800082
VL  - 33
IS  - 3
SP  - 168
EP  - 176
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068862049&doi=10.1109%2fMNET.2019.1800082&partnerID=40&md5=72276c7566e74c7b6b4d51b16055eeae
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 60</p>
KW  - Application programs
KW  - Cloud applications
KW  - Computer hardware
KW  - Computer software reusability
KW  - Computer system firewalls
KW  - Ecosystems
KW  - Infrastructure resources
KW  - Network function virtualization
KW  - Network functions
KW  - Operational expenditures
KW  - Packet processing
KW  - Processing overhead
KW  - Research challenges
KW  - Service functions
KW  - Transfer functions
ER  - 

TY  - JOUR
TI  - Service-Oriented Energy-Latency Tradeoff for IoT Task Partial Offloading in MEC-Enhanced Multi-RAT Networks
AU  - Qin, M.
AU  - Cheng, N.
AU  - Jing, Z.
AU  - Yang, T.
AU  - Xu, W.
AU  - Yang, Q.
AU  - Rao, R.R.
T2  - IEEE Internet of Things Journal
AB  - The development of the 5G network is envisioned to offer various types of services like virtual reality/augmented reality and autonomous vehicles applications with low-latency requirements in Internet-of-Things (IoT) networks. Mobile-edge computing (MEC) has become a promising solution for enhancing the computation capacity of mobile devices at the edge of the network in a 5G wireless network. Additionally, multiple radio access technologies (multi-RATs) have been verified with the potential in lowering the transmission latency and energy consumption, while improving the Quality of Services (QoS). Benefiting from the cooperation of multi-RATs, large latency-sensitive computing service tasks (L2SC) can be offloaded by different RATs simultaneously, which has great practical significance for data partitioned oriented applications with large task sizes. In this article, to enhance the L2SC offloading services for satisfying low-latency requirements with low energy consumption, we investigate the energy-latency tradeoff problem for partial task offloading in the MEC-enhanced multi-RAT network, considering the limitation of energy and computing in capability-constrained end devices in IoT networks. Specifically, we formulated the L2SC task computation offloading problem to minimize the weighted sum of the latency cost and the energy consumption by jointly optimizing the local computing frequency, task splitting, and transmit power, while guaranteeing the stringent latency requirement and the residual energy constraint. Due to the nonsmoothness and nonconvexity of the formulated problem with high complexity, we convert the tradeoff problem into a smooth biconvex problem and propose an alternate convex search-based algorithm, which can greatly reduce the computational complexity. Numerical simulation results show the effectiveness of the proposed algorithm with various performance parameters.  © 2014 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/JIOT.2020.3015970
VL  - 8
IS  - 3
SP  - 1896
EP  - 1907
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100243699&doi=10.1109%2fJIOT.2020.3015970&partnerID=40&md5=10240033a60eeb1dd0cfc960c8baac34
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 120</p>
KW  - 5G mobile communication systems
KW  - Complex networks
KW  - Computation capacity
KW  - Computation offloading
KW  - Computing services
KW  - Energy utilization
KW  - Formulated problems
KW  - Green computing
KW  - Internet of things
KW  - Internet of Things (IoT)
KW  - Internet of Things (IOT)
KW  - latency-sensitive services
KW  - Low energy consumption
KW  - Low power electronics
KW  - mobile-edge computing (MEC)
KW  - multiple radio access technology (multi-RAT)
KW  - partial offloading
KW  - Performance parameters
KW  - Quality of service
KW  - Radio access networks
KW  - Radio transmission
KW  - Rats
KW  - resource allocation
KW  - Search-based algorithms
KW  - task splitting
ER  - 

TY  - JOUR
TI  - Modeling and analyzing the performance of high-speed packet I/O
AU  - Li, X.
AU  - Ren, F.
AU  - Yang, B.
T2  - Tsinghua Science and Technology
AB  - Recently, 10 Gbps or higher speed links are being widely deployed in data centers. Novel high-speed packet I/O frameworks have emerged to keep pace with such high-speed links. These frameworks mainly use techniques, such as memory preallocation, busy polling, zero copy, and batch processing, to replace costly operations (e.g., interrupts, packet copy, and system call) in native OS kernel stack. For high-speed packet I/O frameworks, costs per packet, saturation throughput, and latency are performance metrics that are of utmost concern, and various factors have an effect on these metrics. To acquire a comprehensive understanding of high-speed packet I/O, we propose an analytical model to formulate its packet forwarding (receiving-processing-sending) flow. Our model takes the four main techniques adopted by the frameworks into consideration, and the concerned performance metrics are derived from it. The validity and correctness of our model are verified by real system experiments. Moreover, we explore how each factor impacts the three metrics through a model analysis and then provide several useful insights and suggestions for performance tuning. © 1996-2012 Tsinghua University Press.
DA  - 2021///
PY  - 2021
DO  - 10.26599/TST.2019.9010080
VL  - 26
IS  - 4
SP  - 426
EP  - 439
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099145278&doi=10.26599%2fTST.2019.9010080&partnerID=40&md5=0eb4d6e26b07a3519c3f911c0e21829c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 6</p>
KW  - Batch data processing
KW  - costs per packet
KW  - High-speed links
KW  - high-speed packet I/O
KW  - latency
KW  - Model analysis
KW  - modeling
KW  - Packet forwarding
KW  - Performance metrics
KW  - Performance tuning
KW  - Real systems
KW  - saturation throughput
KW  - Saturation throughput
KW  - Speed
KW  - System calls
ER  - 

TY  - JOUR
T2  - Vector Packet Processor (VPP)
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132019193&partnerID=40&md5=4855a08e2a9a84df103a50dbd2ca1321
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Evaluation of forwarding efficiency in NFV-nodes toward predictable service chain performance
AU  - Kawashima, R.
AU  - Nakayama, H.
AU  - Hayashi, T.
AU  - Matsuo, H.
T2  - IEEE Transactions on Network and Service Management
AB  - The concept of network functions virtualization (NFV) has been embodied in commercial networks over the past years. Software-based virtual network functions have forwarding performance concerns in general, and various acceleration technologies have been developed so far, such as DPDK and vhost-user. Existence of several alternatives requires network engineers or operators to select appropriate technologies; however, no pragmatic criterion exists for constructing high-performance NFV-nodes. From their points of view, a lack of common benchmark and understanding of performance characteristics makes it difficult to predict hop-by-hop performance in a service chain, which results in prevention of NFV deployment in mission-critical networks. In this paper, we clarify performance characteristics of packet forwarding in NFV nodes focusing on three types of acceleration technologies; packet I/O architecture, virtual network I/O, and forwarding engine in a practical stage. We examined three packet I/O architectures (NAPI, netmap, and DPDK), three virtual I/O mechanisms (vhost-net, vhost-user, and SR-IOV), and four practical forwarding programs (Open vSwitch, OVS-DPDK, xDPd-DPDK, and Lagopus) with three referential programs (Linux Bridge, VALE, and L2FWD-DPDK). The experiment was conducted on a 40 GbE environment and we examined two device-under-Test machines having different CPU performance. We argue performance characteristics of each technology and give quantitative analyses of the result. The key findings are: 1) CPU core speed has impact on both throughput and latency/jitter; 2) DPDK can allow performance prediction; 3) vhost-user is appropriate for real environment; and 4) OVS-DPDK provides a good combination of performance and functionality. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/TNSM.2017.2734560
VL  - 14
IS  - 4
SP  - 920
EP  - 933
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028990365&doi=10.1109%2fTNSM.2017.2734560&partnerID=40&md5=2c6d1521a5372ee3c2f8b93677f45df8
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 30</p>
KW  - Acceleration
KW  - Benchmark testing
KW  - Benchmarking
KW  - Chains
KW  - Computer operating systems
KW  - DPDK
KW  - Engines
KW  - Function evaluation
KW  - Kernel
KW  - Linux
KW  - Network architecture
KW  - Network function virtualization
KW  - Network functions
KW  - Network functions virtualization
KW  - Performance evaluation
KW  - Service chaining
KW  - Software defined networking
KW  - Software testing
KW  - Throughput
KW  - Transfer functions
KW  - Virtual reality
KW  - Virtualization
ER  - 

TY  - CONF
TI  - In-network computation is a dumb idea whose time has come
AU  - Sapio, A.
AU  - Abdelaziz, I.
AU  - Aldilaijan, A.
AU  - Canini, M.
AU  - Kalnis, P.
T2  - HotNets 2017 - Proceedings of the 16th ACM Workshop on Hot Topics in Networks
AB  - Programmable data plane hardware creates new opportunities for infusing intelligence into the network. This raises a fundamental question: what kinds of computation should be delegated to the network? In this paper, we discuss the opportunities and challenges for co-designing data center distributed systems with their network layer. We believe that the time has finally come for offloading part of their computation to execute in-network. However, in-network computation tasks must be judiciously crafted to match the limitations of the network machine architecture of programmable devices. With the help of our experiments on machine learning and graph analytics workloads, we identify that aggregation functions raise opportunities to exploit the limited computation power of networking hardware to lessen network congestion and improve the overall application performance. Moreover, as a proof-of-concept, we propose DAIET, a system that performs in-network data aggregation. Experimental results with an initial prototype show a large data reduction ratio (86.9%-89.3%) and a similar decrease in the workers' computation time. © 2017 Copyright held by the owner/author(s).
DA  - 2017///
PY  - 2017
DO  - 10.1145/3152434.3152461
SP  - 150
EP  - 156
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041220007&doi=10.1145%2f3152434.3152461&partnerID=40&md5=68566ea2a85969713f49bdc889901815
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 230</p>
KW  - Aggregation functions
KW  - Application performance
KW  - Computation power
KW  - Distributed systems
KW  - Hardware
KW  - In-network computations
KW  - In-network data aggregation
KW  - Learning systems
KW  - Network congestions
KW  - Network layers
KW  - Programmable devices
ER  - 

TY  - CONF
TI  - Accurate characterization of the variability in power consumption in modern mobile processors
AU  - Balaji, B.
AU  - McCullough, J.
AU  - Gupta, R.K.
AU  - Agarwal, Y.
T2  - 2012 Workshop on Power-Aware Computing Systems, HotPower 2012
AB  - The variability in performance and power consumption is slated to grow further with continued scaling of process technologies. While this variability has been studied and modeled before, there is lack of empirical data on its extent, as well as the factors affecting it, especially for modern general purpose microprocessors. Using detailed power measurements we show that the part to part variability for modern processors utilizing the Nehalem microarchitecture is indeed significant. We chose six Core i5-540M laptop processors marketed in the same frequency bins - thus presumed to be identical - and characterized their power consumption for a variety of representative single-threaded and multi-threaded application workloads. Our data shows processor power variation ranging from 7% - 17% across different applications and configuration options such as Hyper-Threading and Turbo Boost. We present our hypotheses on the underlying causes of this observed power variation and discuss its potential implications. © 2012 Power-Aware Computing Systems, HotPower 2012.All right reserved.
DA  - 2012///
PY  - 2012
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093861219&partnerID=40&md5=08c0bf64735e6f429e539fa38714e0ef
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 22</p>
KW  - Configuration options
KW  - Electric power utilization
KW  - General-purpose microprocessors
KW  - Micro architectures
KW  - Mobile processors
KW  - Modern processors
KW  - Multi- threaded applications
KW  - Power management
KW  - Process Technologies
KW  - Underlying cause
ER  - 

TY  - JOUR
AU  - Karlsson, M.
T2  - Fundamental Technologies We Need to Work on for Cloud-Native Networking
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132031351&partnerID=40&md5=0fea8c2858fdafc451967ea7d2fe1461
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Assessing Container Network Interface Plugins: Functionality, Performance, and Scalability
AU  - Qi, S.
AU  - Kulkarni, S.G.
AU  - Ramakrishnan, K.K.
T2  - IEEE Transactions on Network and Service Management
AB  - Kubernetes, an open-source container orchestration platform, has been widely adopted by cloud service providers (CSPs) for its advantages in simplifying container deployment, scalability, and scheduling. Networking is one of the central components of Kubernetes, providing connectivity between different Pods (a group of containers) both within the same host and across hosts. To bootstrap Kubernetes networking, the Container Network Interface (CNI) provides a unified interface for the interaction between container runtimes. There are several CNI implementations, available as open-source 'CNI plugins'. While they differ in functionality and performance, it is a challenge for a cloud provider to differentiate and choose the appropriate plugin for their environment. In this article, we compare the various open-source CNI plugins available from the community, qualitatively, and through detailed quantitative measurements. With our experimental evaluation, we analyze the overheads and bottlenecks for each CNI plugin, especially because of the interaction with the datapath/iptables as well as the host network stack. Overlay tunnel offload support in the network interface card plays a significant role in achieving the good performance of CNIs that use overlay tunnels for inter-host Pod-to-Pod communication. We also study scalability with an increasing number of Pods, as well as with HTTP workloads, and briefly evaluate Pod startup latency. Our measurement results inform the outline of an ideal CNI environment for Kubernetes.  © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/TNSM.2020.3047545
VL  - 18
IS  - 1
SP  - 656
EP  - 671
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098759828&doi=10.1109%2fTNSM.2020.3047545&partnerID=40&md5=c3729316286e0f8f37d923a43c7838c6
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 46</p>
KW  - Central component
KW  - Cloud providers
KW  - Cloud service providers
KW  - container networking interface
KW  - Containers
KW  - Experimental evaluation
KW  - Kubernetes
KW  - Network interface cards
KW  - Network stack
KW  - Open sources
KW  - performance
KW  - Petroleum reservoir evaluation
KW  - Quantitative measurement
KW  - scalability
KW  - Scalability
ER  - 

TY  - JOUR
TI  - Performance benchmarking of state-of-the-art software switches for NFV
AU  - Zhang, T.
AU  - Linguaglossa, L.
AU  - Giaccone, P.
AU  - Iannone, L.
AU  - Roberts, J.
T2  - Computer Networks
AB  - With the ultimate goal of replacing proprietary hardware appliances with Virtual Network Functions (VNFs) implemented in software, Network Function Virtualization (NFV) has gained popularity in the past few years. Software switches are widely employed to route traffic between VNFs and physical Network Interface Cards (NICs). It is thus of paramount importance to compare the performance of different switch designs and architectures. In this paper, we propose a methodology to compare fairly and comprehensively the performance of software switches. We first explore the design spaces of 7 state-of-the-art software switches and then compare their performance under four representative test scenarios. Each scenario corresponds to a specific case of routing NFV traffic between NICs and/or VNFs. In our experiments, we evaluate the throughput and latency between VNFs in two of the most popular virtualization environments, namely virtual machines (VMs) and containers. Our experimental results show that no single software switch prevails in all scenarios. It is, therefore, crucial to choose the most suitable solution for the given use case. At the same time, the presented results and analysis provide a more in-depth insight into the design tradeoffs and identify potential performance bottlenecks that could inspire new designs. © 2021 Elsevier B.V.
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.comnet.2021.107861
VL  - 188
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100639701&doi=10.1016%2fj.comnet.2021.107861&partnerID=40&md5=d435b6f042235e3109f150bb5791fa67
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 14</p>
KW  - Benchmarking
KW  - Design tradeoff
KW  - High-speed packet processing
KW  - Network function virtualization
KW  - Network Function Virtualization (NFV)
KW  - Performance benchmarking
KW  - Performance benchmarking methodology
KW  - Performance bottlenecks
KW  - Physical network
KW  - Service Function Chain (SFC)
KW  - Software switch
KW  - Software switches
KW  - Software testing
KW  - State of the art
KW  - Suitable solutions
KW  - Transfer functions
KW  - Virtual Network Functions (VNF)
KW  - Virtual networks
KW  - Virtual switch
ER  - 

TY  - JOUR
T2  - Mizar Project Documentation
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132007808&partnerID=40&md5=196319cc000590ddb9f9b9adc04622c2
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - SoftNIC: A software NIC to augment hardware
AU  - Han, S.
AU  - Jang, K.
AU  - Panda, A.
AU  - Palkar, S.
AU  - Han, D.
AU  - Ratnasamy, S.
T2  - SoftNIC: A Software NIC to Augment Hardware
DA  - 2015///
PY  - 2015
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042347159&partnerID=40&md5=a9e1357bee38f8e1fb1e3bc780a04ebf
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 162</p>
ER  - 

TY  - JOUR
AU  - Daly, L.
AU  - Hunt, D.
AU  - MacNamara, C.
AU  - Ramakrishnan, K.
T2  - Intel Speed Select Technology-Base Frequency (Intel_ SSTBF) with Kubernetes
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132026618&partnerID=40&md5=a754ff05c65d6e919abbfbbd8928deb9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - MEC-Enabled Wireless VR Video Service: A Learning-Based Mixed Strategy for Energy-Latency Tradeoff
AU  - Zheng, C.
AU  - Liu, S.
AU  - Huang, Y.
AU  - Yang, L.
T2  - IEEE Wireless Communications and Networking Conference, WCNC
AB  - Mobile edge computing (MEC) has received broad attention as an effective network architecture and a key enabler of the wireless virtual reality (VR) video service which is expected to take a huge share of communication traffic. In this work, we investigate the scenario of multi-tiles-based wireless VR video service with the aid of MEC network, where the primary objective is to minimize the system energy consumption and the latency as well as to arrive at a tradeoff between these two metrics. To this end, we first cast the time-varying view popularity as a modelfree Markov chain and use a long short-term memory autoencoder network to predict its dynamics. Then, a mixed strategy, which jointly considers the dynamic caching replacement and the deterministic offloading, is designed to fully utilize the caching and computing resource in the system. The underlying multiobjective optimization problem is reformulated as a partially observable Markov decision process and solved by using a deep deterministic policy gradient algorithm. The effectiveness of the proposed scheme is confirmed by numerical simulations. © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/WCNC45663.2020.9120529
VL  - 2020-May
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087278990&doi=10.1109%2fWCNC45663.2020.9120529&partnerID=40&md5=d683a113fa599f4ffd1ceaa6a61bfd82
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 26</p>
KW  - Clustering algorithms
KW  - Computing resource
KW  - Dynamic caching
KW  - Energy utilization
KW  - Learning algorithms
KW  - Markov chains
KW  - Multi-objective optimization problem
KW  - Multiobjective optimization
KW  - Network architecture
KW  - Partially observable Markov decision process
KW  - Policy gradient
KW  - Primary objective
KW  - System energy consumption
KW  - Video services
ER  - 

TY  - JOUR
AU  - Töpel, B.
T2  - Introducing AFXDP Support
DA  - 2021///
PY  - 2021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132045110&partnerID=40&md5=51c3f99d8be2516018bf26a8b8b50e85
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - A disaggregated packet processing architecture for network function virtualization
AU  - Chowdhury, S.R.
AU  - Bian, H.
AU  - Bai, T.
AU  - Boutaba, R.
T2  - IEEE Journal on Selected Areas in Communications
AB  - Network Function Virtualization (NFV) promises to reduce the capital and operational expenditure for network operators by moving packet processing from purpose-built hardware to software running on commodity servers. However, the state-of-the-art in NFV is merely replacing monolithic hardware with monolithic Virtual Network Functions (VNFs), i.e., software that realizes different network functions. This is a good first step towards transitioning to NFV, however, common functionality is repeatedly implemented in monolithic VNFs. Repeated execution of such redundant functionality is particularly common when VNFs are chained to realize Service Function Chains (SFCs) and results in wasted infrastructure resources. This stresses the need for re-architecting the NFV ecosystem, through modular VNF design and flexible service composition. From this perspective, we propose MicroNF (μ NF in short), a disaggregated packet processing architecture facilitating the deployment of VNFs and SFCs using reusable, loosely-coupled, and independently deployable components. We have implemented the proposed system, including the different architecture components and optimizations for improving packet processing throughput and latency. Extensive experiments on a testbed demonstrate that: (i) compared to monolithic VNF based SFCs, those composed of μ NFs achieve the same packet processing throughput while using less CPU cycles per packet on average; and (ii) μ NF-based SFCs can sustain the same packet processing throughput as those based on state-of-the-art run-to-completion VNF architecture while using lesser number of CPU cores. © 1983-2012 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/JSAC.2020.2986611
VL  - 38
IS  - 6
SP  - 1075
EP  - 1088
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085591728&doi=10.1109%2fJSAC.2020.2986611&partnerID=40&md5=3ac93be323fcd1189dac8ecc61f77349
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 27</p>
KW  - Computer architecture
KW  - Computer hardware
KW  - Infrastructure resources
KW  - microservices
KW  - middleboxes
KW  - Network function virtualization
KW  - Network functions
KW  - Network operator
KW  - Operational expenditures
KW  - Packet networks
KW  - Packet processing
KW  - Packet-processing architectures
KW  - Service functions
KW  - Transfer functions
KW  - virtual network function decomposition
KW  - Virtual networks
ER  - 

TY  - JOUR
TI  - Virtualization in Cloud Computing: Moving from Hypervisor to Containerization—A Survey
AU  - Bhardwaj, A.
AU  - Krishna, C.R.
T2  - Arabian Journal for Science and Engineering
AB  - Containers emerged as a lightweight alternative to virtual machines that offer better microservice architecture support. They are widely used by organizations to deploy their increasingly diverse workloads derived from modern applications such as big data, IoT, and edge/fog computing in either proprietary clusters or private, public cloud data centers. With the growing interest in container-based virtualization technologies, the requirement to explore the deployment and orchestration of clusters of containers has become a central research problem. Although progress has been made to study containerization, systematic consolidation of the existing literature with a summative evaluation is still missing. To fill this gap, in this paper, we first taxonomically classify the existing research studies on the performance comparison between hypervisor and container technology and then analyze state-of-the-art for container cluster management orchestration systems, its performance monitoring tools, and finally future research trends. This results in a better understanding of container technology with attention to provide summative analysis in terms of (i) how much performance overhead is generated by a hypervisor compared to container-based virtualization, (ii) which container technology is suited for a cloud application deployment based on the type of benchmark executing, (iii) how to provide management of containers deployed in a cluster environment, (iv) container performance monitoring tools, and (v) finally emerging concerns for future research directions. © 2021, King Fahd University of Petroleum & Minerals.
DA  - 2021///
PY  - 2021
DO  - 10.1007/s13369-021-05553-3
VL  - 46
IS  - 9
SP  - 8585
EP  - 8601
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104544140&doi=10.1007%2fs13369-021-05553-3&partnerID=40&md5=826155bb661c7d28b85c62aff15913d6
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 87</p>
KW  - Cloud computing
KW  - Containerization
KW  - Hypervisor
KW  - Virtualization
ER  - 

TY  - JOUR
TI  - Energy and Cost Efficient Resource Allocation for Blockchain-Enabled NFV
AU  - Taskou, S.K.
AU  - Rasti, M.
AU  - Nardelli, P.H.J.
T2  - IEEE Transactions on Services Computing
AB  - Network function virtualization (NFV) is a promising technology to make 5G networks flexible and agile. NFV decreases operators' OPEX and CAPEX by decoupling the physical hardware from the functions they perform. In NFV, users' service request can be viewed as a service function chain (SFC) consisting of several virtual network functions (VNFs) which are connected through virtual links. Resource allocation in NFV is done through a centralized authority called NFV Orchestrator (NFVO). This centralized authority suffers from some drawbacks such as single point of failure and security. Blockchain (BC) technology is able to address these problems by decentralizing resource allocation. The drawbacks of NFVO in NFV architecture and the exceptional BC characteristics to address these problems motivate us to focus on NFV resource allocation to users' SFCs without the need for an NFVO. To this end, we assume there are two types of users: users who send SFC requests (SFC requesting users) and users who perform mining process (miner users). For SFC requesting users, we formulate NFV resource allocation (NFV-RA) problem as a multi-objective problem to minimize the energy consumption and utilized resource cost, simultaneously. To address this problem, we propose an Approximation-based Resource Allocation algorithm (ARA) using Majorization-Minimization approximation method to convexify NFV-RA problem. Furthermore, due to the high complexity of ARA algorithm, we propose a low complexity Hungarian-based Resource Allocation (HuRA) algorithm using Hungarian algorithm for server allocation. Through the simulation results, we show that our proposed ARA and HuRA algorithms achieve near-optimal performance with lower computational complexity. Also, ARA algorithm outperforms the existing algorithms in terms of number of active servers, energy consumption, and average latency. Moreover, the mining process is the foundation of BC technology. In wireless networks, mining is performed by resource-limited mobile users. Since the mining process requires high computational complexity, miner users cannot perform it alone. So, in this article, we assume that miner users can perform mining process with participating of other users. For mining process, the problem of minimizing the energy consumption and cost of users' processing resources is formulated as a linear programming problem that can be optimally solved in polynomial time.  © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/TSC.2021.3050717
VL  - 15
IS  - 4
SP  - 2328
EP  - 2341
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099549098&doi=10.1109%2fTSC.2021.3050717&partnerID=40&md5=75997ec3595b2d1785c1155d1e7bc446
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 17</p>
KW  - 5G mobile communication systems
KW  - blockchain
KW  - Blockchain
KW  - consensus mechanism
KW  - Energy and cost
KW  - Energy utilization
KW  - mining
KW  - Mining process
KW  - Multi-objective problem
KW  - Network function virtualization
KW  - Resource allocation
KW  - Service functions
KW  - Service requests
KW  - Single point
KW  - Transfer functions
KW  - Utilized resources
KW  - virtual network function
KW  - Virtual networks
ER  - 

TY  - JOUR
TI  - AI-Driven Provisioning in the 5G Core
AU  - Sheoran, A.
AU  - Fahmy, S.
AU  - Cao, L.
AU  - Sharma, P.
T2  - IEEE Internet Computing
AB  - Network slicing enables communication service providers to partition physical infrastructure into logically independent networks. Network slices must be provisioned to meet the service-level objectives (SLOs) of disparate offerings, such as enhanced mobile broadband, ultrareliable low-latency communications, and massive machine-type communications. Network orchestrators must customize service placement and scaling to achieve the SLO of each network slice. In this article, we discuss the challenges encountered by network orchestrators in allocating resources to disparate 5G network slices, and propose the use of artificial intelligence to make core placement and scaling decisions that meet the requirements of network slices deployed on shared infrastructure. We explore how artificial intelligence-driven scaling algorithms, coupled with functionality-aware placement, can enable providers to design closed-loop solutions to meet the disparate SLOs of future network slices. © 1997-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/MIC.2021.3056230
VL  - 25
IS  - 2
SP  - 18
EP  - 25
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100778760&doi=10.1109%2fMIC.2021.3056230&partnerID=40&md5=4c6a71ee3f21d41b9a5454fcc83c26bd
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 8</p>
KW  - 5G
KW  - 5G mobile communication systems
KW  - AI
KW  - Artificial intelligence
KW  - Closed-loop solution
KW  - Communication service
KW  - Low-latency communication
KW  - Machine type communications
KW  - Network Functions Virtualization
KW  - Scaling algorithm
KW  - Service level objective
KW  - Service placements
KW  - Shared infrastructure
KW  - Telecommunications
ER  - 

TY  - JOUR
AU  - Höweler, M.
T2  - XDP-Monitoring Energy-Adaptive Network Functions (X-MAN) Source Code
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132024950&partnerID=40&md5=b95de512e50a43b117f975cd82b05c05
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - An empirical model of packet processing delay of the Open vSwitch
AU  - Sattar, D.
AU  - Matrawy, A.
T2  - Proceedings - International Conference on Network Protocols, ICNP
AB  - Network virtualization offers flexibility by decoupling virtual network from the underlying physical network. Software-Defined Network (SDN) could utilize the virtual network. For example, in Software-Defined Networks, the entire network can be run on commodity hardware and operating systems that use virtual elements. However, this could present new challenges of data plane performance. In this paper, we present an empirical model of the packet processing delay of a widely used OpenFlow virtual switch, the Open vSwitch. In the empirical model, we analyze the effect of varying Random Access Memory (RAM) and network parameters on the performance of the Open vSwitch. Our empirical model captures the non-network processing delays, which could be used in enhancing the network modeling and simulation. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/ICNP.2017.8117597
VL  - 2017-October
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041384741&doi=10.1109%2fICNP.2017.8117597&partnerID=40&md5=578d2b621a2aca548c826637b9fc2d15
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 10</p>
KW  - Commodity hardware
KW  - Internet protocols
KW  - Network modeling and simulations
KW  - Network parameters
KW  - Network processing
KW  - Network protocols
KW  - Network virtualization
KW  - Packet processing
KW  - Random access memory
KW  - Random access storage
KW  - Underlying physical networks
ER  - 

TY  - JOUR
TI  - Importance of Internet Exchange Point (IXP) infrastructure for 5G: Estimating the impact of 5G use cases
AU  - Hoeschele, T.
AU  - Dietzel, C.
AU  - Kopp, D.
AU  - Fitzek, F.H.P.
AU  - Reisslein, M.
T2  - Telecommunications Policy
AB  - While the Internet is ubiquitous in most parts of the world today, the dominant network access technology is gradually shifting from wired to wireless connections. Notably, the ever increasing bandwidth, together with reduced latency in mobile networks, enables a multitude of new use cases for a wide range of industries. These new use cases have typically increased quality requirements for the wireless connection. Accordingly, the Internet architecture needs to keep pace with the recent developments, particularly in light of the holistic approach of the new fifth generation (5G) mobile communication standard. In this study, we shed light on the effects of 5G on the Internet's core, i.e., on the Internet interdomain traffic. The interdomain traffic between distinct autonomous systems (ASs) is exchanged by transit networks as well as through peerings at private network interconnects (PNIs) and public Internet Exchange Points (IXPs). 5G interdomain traffic estimation is especially important for IXPs as these are focal points that aggregate high traffic volumes. We coherently derive twelve 5G use case groups and present an overview of their individual associated 5G use cases. We characterize the communication networking (connection) requirements and the resulting implications of these use cases. Further, we develop and apply a methodology to assess the 5G use cases, so as to rank them in terms of their projected impact on the overall internet traffic growth. Based on this systematic approach, we conclude that the traffic for the use case groups Video in 5G, Health, and Virtual & Augmented Reality will have the largest impact. We identify a large number of other use case groups, e.g., Live Events, Tactile Internet, and Manufacturing, that will contribute rather small individual fractions to the overall growth of Internet traffic. However, their aggregated contribution to the internet traffic growth will be significant. Our traffic estimation can inform interdomain infrastructure providers, such as IXPs, and policy makers about the likely impact of the introduction of 5G on the interdomain traffic in the Internet. © 2020 Elsevier Ltd
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.telpol.2020.102091
VL  - 45
IS  - 3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100436753&doi=10.1016%2fj.telpol.2020.102091&partnerID=40&md5=b7945017a0e2892d044d4b5c3725daac
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 51</p>
KW  - 5G mobile communication systems
KW  - 5G wireless use cases
KW  - Augmented reality
KW  - Autonomous system (ASs)
KW  - Communication networking
KW  - Infrastructure providers
KW  - Inter-domain traffic
KW  - Internet architecture
KW  - Internet Exchange Point (IXP)
KW  - Internet exchange points
KW  - Internet traffic growth
KW  - Mobile communication standards
KW  - Mobile telecommunication systems
KW  - Quality requirements
KW  - Traffic projection model
ER  - 

TY  - JOUR
TI  - An Energy Aware Task Scheduling Model Using Ant-Mating Optimization in Fog Computing Environment
AU  - Ghanavati, S.
AU  - Abawajy, J.
AU  - Izadi, D.
T2  - IEEE Transactions on Services Computing
AB  - Fog computing has become a platform of choice for executing emerging applications with low latency requirements. Since the devices in fog computing tend to be resource constraint and highly distributed, how fog computing resources can be effectively utilized for executing delay-sensitive tasks is a fundamental challenge. To address this problem, we propose and evaluate a new task scheduling algorithm with the aim of reducing the total system makespan and energy consumption for fog computing platform. The proposed approach consists of two key components: 1) a new bio-inspired optimization approach called Ant Mating Optimization (AMO) and 2) optimized distribution of a set of tasks among the fog nodes within proximity. The objective is to find an optimal trade-off between the system makespan and the consumed energy required by the fog computing services, established by end-user devices. Our empirical performance evaluation results demonstrate that the proposed approach outperforms the bee life algorithm, traditional particle swarm optimization and genetic algorithm in terms of makespan and consumed energy.  © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/TSC.2020.3028575
VL  - 15
IS  - 4
SP  - 2007
EP  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135806143&doi=10.1109%2fTSC.2020.3028575&partnerID=40&md5=1f9a67831fc25e069961511a1cb47863
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 69</p>
KW  - ant mating optimization
KW  - Ant mating optimization
KW  - Biomimetics
KW  - Computing environments
KW  - Consumed energy
KW  - Delay-sensitive applications
KW  - Economic and social effects
KW  - energy consumption
KW  - Energy utilization
KW  - Energy-aware task scheduling
KW  - Energy-consumption
KW  - Fog
KW  - Fog computing
KW  - Genetic algorithms
KW  - Internet of things
KW  - Internet of Things
KW  - Makespan
KW  - Matings
KW  - Multitasking
KW  - Optimisations
KW  - Particle swarm optimization (PSO)
KW  - Power management
KW  - Scheduling algorithms
KW  - Scheduling models
KW  - task offloading
KW  - Task offloading
ER  - 

TY  - JOUR
TI  - High-speed data plane and network functions virtualization by vectorizing packet processing
AU  - Linguaglossa, L.
AU  - Rossi, D.
AU  - Pontarelli, S.
AU  - Barach, D.
AU  - Marjon, D.
AU  - Pfister, P.
T2  - Computer Networks
AB  - In the last decade, a number of frameworks started to appear that implement, directly in user-space with kernel-bypass mode, high-speed software data plane functionalities on commodity hardware. This may be the key to replace specific hardware-based middleboxes with custom pieces of software, as advocated by the recent Network Function Virtualization (NFV) paradigm. Vector Packet Processor (VPP) is one of such frameworks, representing an interesting point in the design space in that it offers: (i) in user-space networking, (ii) the flexibility of a modular router (Click and variants) with (iii) high-speed performance (several millions of packets per second on a single CPU core), achieved through techniques such as batch processing that have become commonplace in high-speed networking stacks (e.g. netmap or DPDK). Similarly to Click, VPP lets users arrange functions as a processing graph, providing a full-blown stack of network functions. However, unlike Click where the whole tree is traversed for each packet, in VPP each traversed node processes all packets in the batch (or vector) before moving to the next node. This design choice enables several code optimizations that greatly improve the achievable throughput. This paper introduces the main VPP concepts and architecture, and experimentally evaluates the impact of its design choices (such as batch packet processing) on its performance. © 2018 Elsevier B.V.
DA  - 2019///
PY  - 2019
DO  - 10.1016/j.comnet.2018.11.033
VL  - 149
SP  - 187
EP  - 199
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058036682&doi=10.1016%2fj.comnet.2018.11.033&partnerID=40&md5=56ed9d2011411a8ff24e3c82f7a7a581
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 25</p>
KW  - Achievable throughputs
KW  - Batch data processing
KW  - Hardware
KW  - High-speed networking
KW  - High-speed performance
KW  - Interesting points
KW  - Kernel bypass
KW  - Kernel-bypass
KW  - Network function virtualization
KW  - Packet networks
KW  - Packet processing
KW  - Packets per seconds
KW  - Software routers
KW  - Speed
KW  - Transfer functions
KW  - Vector packet processing
KW  - Vector spaces
KW  - Virtual reality
ER  - 

TY  - CONF
TI  - Fast userspace packet processing
AU  - Barbette, T.
AU  - Soldani, C.
AU  - Mathy, L.
T2  - ANCS 2015 - 11th 2015 ACM/IEEE Symposium on Architectures for Networking and Communications Systems
AB  - In recent years, we have witnessed the emergence of high speed packet I/O frameworks, bringing unprecedented network performance to userspace. Using the Click modular router, we rst review and quantitatively compare several such packet I/O frameworks, showing their superiority to kernel-based forwarding. We then reconsider the issue of software packet processing, in the context of modern commodity hardware with hardware multi-queues, multi-core processors and non-uniform memory access. Through a combination of existing techniques and improvements of our own, we derive modern general principles for the design of software packet processors. Our implementation of a fast packet processor framework, integrating a faster Click with both Netmap and DPDK, ex-hibits up-to about 2.3x speed-up compared to other software implementations, when used as an IP router. © 2015 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/ANCS.2015.7110116
SP  - 5
EP  - 16
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936120771&doi=10.1109%2fANCS.2015.7110116&partnerID=40&md5=273947133afdfb20766bd5058fbf291a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 180</p>
KW  - Click modular router
KW  - Commodity hardware
KW  - Design of softwares
KW  - Memory architecture
KW  - Multi-core processor
KW  - Non uniform memory access
KW  - Packet processing
KW  - Packet processors
KW  - Software implementation
ER  - 

TY  - CONF
TI  - FlowMon-DPDK: Parsimonious Per-Flow Software Monitoring at Line Rate
AU  - Zhang, T.
AU  - Linguaglossa, L.
AU  - Gallo, M.
AU  - Giaccone, P.
AU  - Rossi, D.
T2  - TMA 2018 - Proceedings of the 2nd Network Traffic Measurement and Analysis Conference
AB  - Testing experimental network devices requires deep performance analysis, which is usually performed with expensive, not flexible, hardware equipment. With the advent of highspeed packet I/O frameworks, general purpose equipments have narrowed the performance gap in respect of dedicated hardware and a variety of software-based solutions have emerged for handling traffic at very high speed. While the literature abounds with software traffic generators, existing monitoring solutions do not target worst-case scenarios (i.e., 64B packets at line rate) that are particularly relevant for stress-testing high-speed network functions, or occupy too many resources. In this paper we first analyse the design space for high-speed traffic monitoring that leads us to specific choices characterizing FlowMon-DPDK, a DPDK-based software traffic monitor that we release as an open source project. In a nutshell, FlowMon-DPDK provides tunable fine-grained statistics at both packet and flow levels. Experimental results demonstrate that our traffic monitor is able to provide per-flow statistics with 5-nines precision at high-speed (14.88 Mpps) using an exiguous amount of resources. Finally, we showcase FlowMon-DPDK usage by testing two open source prototypes for stateful flow-level end-host and in-network packet processing. © 2018 IFIP.
DA  - 2018///
PY  - 2018
DO  - 10.23919/TMA.2018.8506565
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056453581&doi=10.23919%2fTMA.2018.8506565&partnerID=40&md5=9a03bc267901072d037cdb936a646e61
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 18</p>
KW  - Dedicated hardware
KW  - Hardware
KW  - HIgh speed networks
KW  - Open source projects
KW  - Open source software
KW  - Open systems
KW  - Performance analysis
KW  - Software monitoring
KW  - Software testing
KW  - Software-based solutions
KW  - Traffic generators
KW  - Traffic monitoring
KW  - Worst case scenario
ER  - 

TY  - JOUR
TI  - Energy-Efficient Workflow Scheduling Using Container-Based Virtualization in Software-Defined Data Centers
AU  - Ranjan, R.
AU  - Thakur, I.S.
AU  - Aujla, G.S.
AU  - Kumar, N.
AU  - Zomaya, A.Y.
T2  - IEEE Transactions on Industrial Informatics
AB  - Workflow scheduling is one of the most difficult tasks due to the variation in the traffic flows generated from diverse cloud applications. Hence, in this article, a container-based virtualization is used to design an energy-efficient workflow scheduling in software-defined data centers. The containers provide the flexibility to the applications to access the underlying resource as per their requirements. Moreover, a runtime scheduler is responsible to handle all the scheduling decisions in the proposed workflow scheduling scheme. Even more, a doubly linked list-based access mechanism is used to provide access to the servers and virtual machines by traversing both ways. Finally, a hashing scheme is used to select an ideal location for the allocation of the containers. The proposed scheme is evaluated with respect to different performance metrics (makespan, execution time, fault tolerance, energy consumption, etc.) on the real data traces. The results obtained depict the superiority of the proposed scheme in comparison to the other existing schemes of its category.  © 2005-2012 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/TII.2020.2985030
VL  - 16
IS  - 12
SP  - 7646
EP  - 7657
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092197336&doi=10.1109%2fTII.2020.2985030&partnerID=40&md5=838e6b08755cc6a14abbd8d21dfbd57d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 33</p>
KW  - Access mechanism
KW  - Cloud applications
KW  - Cloud computing
KW  - containers
KW  - Containers
KW  - Data centers
KW  - Energy efficiency
KW  - Energy efficient
KW  - Energy utilization
KW  - Fault tolerance
KW  - Performance metrics
KW  - Scheduling
KW  - Scheduling decisions
KW  - software-defined data centers (SDDCs)
KW  - Traffic flow
KW  - virtual machine
KW  - Virtualization
KW  - workflow scheduling
KW  - Workflow scheduling
ER  - 

TY  - JOUR
TI  - A Framework for eBPF-Based Network Functions in an Era of Microservices
AU  - Miano, S.
AU  - Risso, F.
AU  - Bernal, M.V.
AU  - Bertrone, M.
AU  - Lu, Y.
T2  - IEEE Transactions on Network and Service Management
AB  - By moving network functionality from dedicated hardware to software running on end-hosts, Network Functions Virtualization (NFV) pledges the benefits of cloud computing to packet processing. While most of the NFV frameworks today rely on kernel-bypass approaches, no attention has been given to kernel packet processing, which has always proved hard to evolve and to program. In this article, we present Polycube, a software framework whose main goal is to bring the power of NFV to in-kernel packet processing applications, enabling a level of flexibility and customization that was unthinkable before. Polycube enables the creation of arbitrary and complex network function chains, where each function can include an efficient in-kernel data plane and a flexible user-space control plane with strong characteristics of isolation, persistence, and composability. Polycube network functions, called Cubes, can be dynamically generated and injected into the kernel networking stack, without requiring custom kernels or specific kernel modules, simplifying the debugging and introspection, which are two fundamental properties in recent cloud environments. We validate the framework by showing significant improvements over existing applications, and we prove the generality of the Polycube programming model through the implementation of complex use cases such as a network provider for Kubernetes.  © 2004-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/TNSM.2021.3055676
VL  - 18
IS  - 1
SP  - 133
EP  - 151
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100455153&doi=10.1109%2fTNSM.2021.3055676&partnerID=40&md5=3c7fe2ca914b4808278cd8ceba2f90a0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 80</p>
KW  - Application programs
KW  - Cloud environments
KW  - Complex networks
KW  - Computer systems programming
KW  - Dedicated hardware
KW  - eBPF
KW  - Fundamental properties
KW  - linux
KW  - Network function virtualization
KW  - Network functions
KW  - Network provider
KW  - NFV
KW  - Packet processing
KW  - Programming models
KW  - Software frameworks
KW  - Transfer functions
KW  - XDP
ER  - 

TY  - JOUR
TI  - Intelligent and autonomous management in cloud-native future networks — A survey on related standards from an architectural perspective
AU  - Duan, Q.
T2  - Future Internet
AB  - Cloud-native network design, which leverages network virtualization and softwarization together with the service-oriented architectural principle, is transforming communication networks to a versatile platform for converged network-cloud/edge service provisioning. Intelligent and autonomous management is one of the most challenging issues in cloud-native future networks, and a wide range of machine learning (ML)-based technologies have been proposed for addressing different aspects of the management challenge. It becomes critical that the various management technologies are applied on the foundation of a consistent architectural framework with a holistic vision. This calls for standardization of new management architecture that supports seamless the integration of diverse ML-based technologies in cloud-native future networks. The goal of this paper is to provide a big picture of the recent developments of architectural frameworks for intelligent and autonomous management for future networks. The paper surveys the latest progress in the standardization of network management architectures including works by 3GPP, ETSI, and ITU-Tand analyzes how cloud-native network design may facilitate the architecture development for addressing management challenges. Open issues related to intelligent and autonomous management in cloud-native future networks are also discussed in this paper to identify some possible directions for future research and development. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
DA  - 2021///
PY  - 2021
DO  - 10.3390/fi13020042
VL  - 13
IS  - 2
SP  - 1
EP  - 20
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100709118&doi=10.3390%2ffi13020042&partnerID=40&md5=68153def97613d46fe5976dfdfaa148c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 29</p>
KW  - Architectural frameworks
KW  - Architectural principles
KW  - Autonomous managements
KW  - Cloud-native network design
KW  - Computer architecture
KW  - Intelligent and autonomous management
KW  - Machine learning
KW  - Management architectures
KW  - Management technologies
KW  - Network and service management
KW  - Network architecture
KW  - Network management architectures
KW  - Network virtualization
KW  - Quality of service
KW  - Research and development
KW  - Research and development management
KW  - Standardization
KW  - Surveys
ER  - 

TY  - JOUR
TI  - Software Physical/Virtual Rx Queue Mapping Toward High-Performance Containerized Networking
AU  - Kawashima, R.
T2  - IEEE Transactions on Network and Service Management
AB  - Softwarization of Network Functions (NFs) accelerates automated deployment and management of services on next-gen networks. Combining flexibility and high-performance is a vital requirement for Network Functions Virtualisation (NFV); however, many studies have demonstrated that containerization or virtualization of NFs severely degrades the fundamental efficiency of packet forwarding. Virtual network I/O, a mechanism of packet transferring between a guest and the host, has been seen as the performance bottleneck in the PVP (Physical-Virtual-Physical) datapath, and one of the main causes of this deterioration is packet copy between them. Various techniques, such as zero-copy, pass-through, and hardware offloading, have been examined to alleviate the performance overhead. However, existing designs and implementations incur pragmatic issues, such as compatibility, manageability, and insufficient quality of performance. We propose yet another design and implementation of zero-copy/pass-through acceleration (named IOVTee) to resolve real-world problems as well as to enhance the forwarding efficiency. IOVTee takes advantage of pre-processing of virtual switches with achieving zero-copy on the receive (Rx) path. The pluggable style of IOVTee for vhost-user (the de-facto virtual network I/O) enables our approach to be transparent to both containers/VMs and virtual switches. In this article, we explain the heart of IOVTee, a fully software-based Rx queue mapping mechanism (between physical and virtual) that enables a concept of Virtual DMA Write-through (to the NF). Our evaluation results showed that applying IOVTee to vhost-user drastically increased efficiency of packet forwarding in the PVP datapath (by 45% and 98% for traffic of 64-byte and 1514-byte packets respectively).  © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/TNSM.2020.3049053
VL  - 18
IS  - 1
SP  - 687
EP  - 700
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099425306&doi=10.1109%2fTNSM.2020.3049053&partnerID=40&md5=ab9a2ab50962b5186442412bccc4bf9f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 9</p>
KW  - container
KW  - Containers
KW  - Design and implementations
KW  - Deterioration
KW  - DPDK
KW  - Efficiency
KW  - Evaluation results
KW  - Mapping
KW  - Mapping mechanism
KW  - Network function virtualization
KW  - Network functions
KW  - Network functions virtualization
KW  - Packet forwarding
KW  - pass-through
KW  - Performance bottlenecks
KW  - Pragmatic issues
KW  - Real-world problem
KW  - Transfer functions
KW  - Vhost-user
KW  - zero-copy
ER  - 

TY  - JOUR
TI  - Evaluating the Upper Bound of Energy Cost Saving by Proactive Data Center Management
AU  - Milocco, R.
AU  - Minet, P.
AU  - Renault, E.
AU  - Boumerdassi, S.
T2  - IEEE Transactions on Network and Service Management
AB  - Data Centers (DCs) need to periodically configure their servers in order to meet user demands. Since appropriate proactive management to meet demands reduces the cost, either by improving Quality of Service (QoS) or saving energy, there is a great interest in studying different proactive strategies based on predictions of the energy used to serve CPU and memory requests. The amount of savings that can be achieved depends not only on the selected proactive strategy but also on user-demand statistics and the predictors used. Despite its importance, it is difficult to find theoretical studies that quantify the savings that can be made, due to the problem complexity. A proactive DC management strategy is presented together with its upper bound of energy cost savings obtained with respect to a purely reactive management. Using this method together with records of the recent past, it is possible to quantify the efficiency of different predictors. Both linear and nonlinear predictors are studied, using a Google data set collected over 29 days, to evaluate the benefits that can be obtained with these two predictors.  © 2004-2012 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/TNSM.2020.2988346
VL  - 17
IS  - 3
SP  - 1527
EP  - 1541
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091167277&doi=10.1109%2fTNSM.2020.2988346&partnerID=40&md5=5898a7c4839a629fb30177827109bf25
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 6</p>
KW  - Computer networks
KW  - Data center (DCs)
KW  - Data center management
KW  - energy cost
KW  - Energy cost savings
KW  - machine learning
KW  - Management strategies
KW  - Networks (circuits)
KW  - Nonlinear predictors
KW  - prediction
KW  - proactive management
KW  - Proactive management
KW  - Problem complexity
KW  - Quality of service
KW  - Theoretical study
ER  - 

TY  - CONF
TI  - MoonGen: A scriptable high-speed packet generator
AU  - Emmerich, P.
AU  - Gallenmüller, S.
AU  - Raumer, D.
AU  - Wohlfart, F.
AU  - Carle, G.
T2  - Proceedings of the ACM SIGCOMM Internet Measurement Conference, IMC
AB  - We present MoonGen, a flexible high-speed packet generator. It can saturate 10GbE links with minimum-sized packets while using only a single CPU core by running on top of the packet processing framework DPDK. Linear multicore scaling allows for even higher rates: We have tested MoonGen with up to 178.5 Mpps at 120Gbit/s. Moving the whole packet generation logic into user-controlled Lua scripts allows us to achieve the highest possible flexibility. In addition, we utilize hardware features of commodity NICs that have not been used for packet generators previously. A key feature is the measurement of latency with sub-microsecond precision and accuracy by using hardware timestamping capabilities of modern commodity NICs. We address timing issues with software-based packet generators and apply methods to mitigate them with both hardware support and with a novel method to control the inter-packet gap in software. Features that were previously only possible with hardware-based solutions are now provided by MoonGen on commodity hardware. MoonGen is available as free software under the MIT license in our git repository at https://github.com/emmericp/MoonGen. © 2015 ACM.
DA  - 2015///
PY  - 2015
DO  - 10.1145/2815675.2815692
VL  - 2015-October
SP  - 275
EP  - 287
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954101391&doi=10.1145%2f2815675.2815692&partnerID=40&md5=2287e9b1f85c0174101e0fa8aad8e788
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 317</p>
KW  - Commodity hardware
KW  - DPDK
KW  - Hardware
KW  - Hardware features
KW  - Hardware supports
KW  - HTTP
KW  - Internet
KW  - Lua
KW  - Packet generation
KW  - Packet generations
KW  - Packet networks
KW  - Packet processing
KW  - Reconfigurable hardware
KW  - User space networking
KW  - User spaces
ER  - 

TY  - CONF
TI  - Deploying Microservice Based Applications with Kubernetes: Experiments and Lessons Learned
AU  - Abdollahi Vayghan, L.
AU  - Saied, M.A.
AU  - Toeroe, M.
AU  - Khendek, F.
T2  - IEEE International Conference on Cloud Computing, CLOUD
AB  - Microservices represent a new architectural style where small and loosely coupled modules can be developed and deployed independently to compose an application. This architectural style brings various benefits such as maintainability and flexibility in scaling and aims at decreasing downtime in case of failure or upgrade. One of the enablers is Kubernetes, an open source platform that provides mechanisms for deploying, maintaining, and scaling containerized applications across a cluster of hosts. Moreover, Kubernetes enables healing through failure recovery actions to improve the availability of applications. As our ultimate goal is to devise architectures to enable high availability (HA) with Kubernetes for microservice based applications, in this paper we examine the availability achievable through Kubernetes under its default configuration. We have conducted a set of experiments which show that the service outage can be significantly higher than expected. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/CLOUD.2018.00148
VL  - 2018-July
SP  - 970
EP  - 973
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057488012&doi=10.1109%2fCLOUD.2018.00148&partnerID=40&md5=887f0d831b37008f1ba41fdff4a7f72a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 103</p>
KW  - Architectural style
KW  - Architecture
KW  - Availability
KW  - Cloud computing
KW  - Computer system recovery
KW  - Containers
KW  - Default configurations
KW  - Docker
KW  - Failure
KW  - High availability
KW  - Kubernetes
KW  - Microservices
KW  - Open source platforms
KW  - Orchestration
ER  - 

TY  - JOUR
T2  - Cloud-native Network Function (CNF) Testbed
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132012337&partnerID=40&md5=070af8103e252b3130a35a5dc5d7b658
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Performance exploration of software-based packet processing systems
AU  - Raumer, D.
AU  - Wohlfart, F.
AU  - Scholz, D.
AU  - Emmerich, P.
AU  - Carle, G.
T2  - Leistungs-, Zuverlässigkeits-und Verlässlichkeitsbewertung von Kommunikationsnetzen und verteilten Systemen
DA  - 2015///
PY  - 2015
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006995088&partnerID=40&md5=a8cb390b614c3a69ddc2e18b4e390920
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 13</p>
ER  - 

TY  - JOUR
TI  - DSEP Fulcrum: Dynamic sparsity and expansion packets for fulcrum network coding
AU  - Nguyen, V.
AU  - Tasdemir, E.
AU  - Nguyen, G.T.
AU  - Lucani, D.E.
AU  - Fitzek, F.H.P.
AU  - Reisslein, M.
T2  - IEEE Access
AB  - Fulcrum coding combines a high-field outer Random Linear Network Coding (RLNC) that generates outer coding expansion packets with a small-field inner RLNC that combines the source packets and the outer coding expansion packets. This two-layer Fulcrum coding allows flexible decoding in receivers with heterogeneous computational capabilities. Fulcrum coding has so far only been studied for conventional dense RLNC, which randomly selects all coding coefficients, and only for a statically fixed number of outer expansion packets. However, the probability that the coding coefficient row of a newly received packet is linearly independent of prior received coding coefficient rows (a prerequisite for successful decoding) is highly dynamic. We propose to exploit the dynamics of this probability to reduce the computational complexity of Fulcrum coding. In particular, we vary the density of non-zero coding coefficients, i.e., equivalently, the sparsity of coding coefficients, and the number of outer expansion packets to keep the complexity low while maintaining a reasonably high decoding probability. We introduce the general principles of dynamic sparsity and expansion packets (DSEP) for Fulcrum coding as well as two specific example DSEP policies. Our evaluations indicate that DSEP Fulcrum can increase the encoding throughput tenfold and increase the decoding throughput 1.4 to 4.3 fold while achieving decoding probabilities that are typically less than 1% lower than the conventional Fulcrum decoding probabilities. We also find that DSEP achieves somewhat higher encoding and decoding throughputs than the CodornicesRq (Release 2.1) implementation of RaptorQ block coding for small blocks (generations) of source packets, while RaptorQ is substantially faster for large generation sizes. Furthermore, we develop and evaluate an elementary DSEP recoding mechanism that achieves a recoding throughput more than double the decoding throughput. © 2013 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ACCESS.2020.2989619
VL  - 8
SP  - 78293
EP  - 78314
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084789500&doi=10.1109%2fACCESS.2020.2989619&partnerID=40&md5=e789a14a8638393a8fa69ce8c2cbaa5e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 36</p>
KW  - Block coding
KW  - Complex networks
KW  - Computational capability
KW  - Computational complexity
KW  - Decoding
KW  - Encoding (symbols)
KW  - Encoding and decoding
KW  - Encoding throughput
KW  - Expansion
KW  - Fixed numbers
KW  - heterogeneous devices
KW  - High field
KW  - Linear networks
KW  - Linearly independents
KW  - Network coding
KW  - Packet networks
KW  - Probability
KW  - Random Linear Network Coding
KW  - random linear network coding (RLNC)
KW  - RaptorQ
KW  - recoding
KW  - sparsity
KW  - throughput
ER  - 

TY  - JOUR
TI  - Workflow-Aware Automatic Fault Diagnosis for Microservice-Based Applications with Statistics
AU  - Wang, T.
AU  - Zhang, W.
AU  - Xu, J.
AU  - Gu, Z.
T2  - IEEE Transactions on Network and Service Management
AB  - Microservice architectures bring many benefits, e.g., faster delivery, improved scalability, and greater autonomy, so they are widely adopted to develop and operate Internet-based applications. How to effectively diagnose the faults of applications with lots of dynamic microservices has become a key to guarantee applications' performance and reliability. As a microservice performs various behaviors in different workflows of processing requests, existing approaches often cannot accurately locate the root cause of an application with interactive microservices in a dynamic deployment environment. We propose a workflow-aware automatic fault diagnosis approach for microservice-based applications with statistics. We characterize traces across microservices with calling trees, and then learn trace patterns as baselines. For the faults affecting the workflows of processing requests, we estimate the workflows' anomaly degrees, and then locate the microservices causing anomalies by comparing the difference between current traces and learned baselines with tree edit distance. For performance anomalies causing significantly increased response time, we employ principal component analysis to extract suspicious microservices with large fluctuation in response time. Finally, we evaluate our approach on three typical microservice-based applications with a series of experiments. The results show that our approach can accurately locate the microservices causing anomalies. © 2004-2012 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/TNSM.2020.3022028
VL  - 17
IS  - 4
SP  - 2350
EP  - 2363
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090950489&doi=10.1109%2fTNSM.2020.3022028&partnerID=40&md5=0e18c64082c18bf44f144799a4764966
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 45</p>
KW  - Anomaly degrees
KW  - Automatic fault diagnosis
KW  - Current traces
KW  - Dynamic deployment
KW  - Electric fault currents
KW  - execution traces
KW  - Failure analysis
KW  - Fault detection
KW  - Fault diagnosis
KW  - Forestry
KW  - Internet based application
KW  - microservice
KW  - Performance and reliabilities
KW  - Performance anomaly
KW  - statistics
KW  - Tree edit distance
KW  - workflow
ER  - 

TY  - JOUR
TI  - Sdn-based network slicing mechanism for a scalable 4g/5g core network: A kubernetes approach
AU  - Botez, R.
AU  - Costa-Requena, J.
AU  - Ivanciu, I.-A.
AU  - Strautiu, V.
AU  - Dobrota, V.
T2  - Sensors
AB  - Managing the large volumes of IoT and M2M traffic requires the evaluation of the scala-bility and reliability for all the components in the end-to-end system. This includes connectivity, mobile network functions, and application or services receiving and processing the data from end devices. Firstly, this paper discusses the design of a containerized IoT and M2M application and the mechanisms for delivering automated scalability and high availability when deploying it in: (1) the edge using balenaCloud; (2) the Amazon Web Services cloud with EC2 instances; and (3) the dedi-cated Amazon Web Services IoT service. The experiments showed that there are no significant differences between edge and cloud deployments regarding resource consumption. Secondly, the solutions for scaling the 4G/5G network functions and mobile backhaul that provide the connectivity between devices and IoT/M2M applications are analyzed. In this case, the scalability and high availability of the 4G/5G components are provided by Kubernetes. The experiments showed that our proposed scaling algorithm for network slicing managed with SDN guarantees the necessary radio and network resources for end-to-end high availability. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
DA  - 2021///
PY  - 2021
DO  - 10.3390/s21113773
VL  - 21
IS  - 11
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106739884&doi=10.3390%2fs21113773&partnerID=40&md5=8ba15b751674a9b759ad8e25ce633ad3
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 29</p>
KW  - 4G mobile communication systems
KW  - 5G
KW  - algorithm
KW  - Amazon web services
KW  - article
KW  - cloud computing
KW  - Cloud computing
KW  - Cloud deployments
KW  - Data handling
KW  - End-to-end systems
KW  - EPC
KW  - High availability
KW  - Internet of things
KW  - IoT
KW  - Kubernetes
KW  - Machine-to-machine communication
KW  - Network functions
KW  - Network resource
KW  - Network slicing
KW  - NFV
KW  - Resource consumption
KW  - Scalability
KW  - Scaling algorithm
KW  - SDN
KW  - Transfer functions
KW  - Web services
KW  - Websites
ER  - 

TY  - JOUR
TI  - Effects of the generation size and overlap on throughput and complexity in randomized linear network coding
AU  - Li, Y.
AU  - Soljanin, E.
AU  - Spasojevic, P.
T2  - IEEE Transactions on Information Theory
AB  - To reduce computational complexity and delay in randomized network coded content distribution, and for some other practical reasons, coding is not performed simultaneously over all content blocks, but over much smaller, possibly overlapping subsets of these blocks, known as generations. A penalty of this strategy is throughput reduction. To analyze the throughput loss, we model coding over generations with random generation scheduling as a coupon collector's brotherhood problem. This model enables us to derive the expected number of coded packets needed for successful decoding of the entire content as well as the probability of decoding failure (the latter only when generations do not overlap) and further, to quantify the tradeoff between computational complexity and throughput. Interestingly, with a moderate increase in the generation size, throughput quickly approaches link capacity. Overlaps between generations can further improve throughput substantially for relatively small generation sizes. © 2006 IEEE.
DA  - 2011///
PY  - 2011
DO  - 10.1109/TIT.2010.2095111
VL  - 57
IS  - 2
SP  - 1111
EP  - 1123
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79251485294&doi=10.1109%2fTIT.2010.2095111&partnerID=40&md5=d461fe42a5c6ecc5ae1cf0f2f54694ca
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 81</p>
KW  - Coded packet
KW  - Computational complexity
KW  - Content distribution
KW  - Coupon collector's problem
KW  - Decoding
KW  - Decoding failure
KW  - Information theory
KW  - Linear network coding
KW  - Link capacities
KW  - Network coding
KW  - Random generation
KW  - Rate-less codes
KW  - rateless codes
KW  - Throughput
ER  - 

TY  - JOUR
TI  - E2E QoS Guarantee for the Tactile Internet via Joint NFV and Radio Resource Allocation
AU  - Gholipoor, N.
AU  - Saeedi, H.
AU  - Mokari, N.
AU  - Jorswieck, E.A.
T2  - IEEE Transactions on Network and Service Management
AB  - The Tactile Internet (TI) is one of the next generation wireless network services with end to end (E2E) delay as low as 1 ms. Since this ultra low E2E delay cannot be met in the current 4G network architecture, it is necessary to investigate this service in the next generation wireless network by considering new technologies such as networks function virtualization (NFV). On the other hand, given the importance of E2E delay in the TI service, it is crucial to consider the delay of all parts of the network, including the radio access part and the NFV core part. In this paper, for the first time, we investigate the joint radio resource allocation (R-RA) and NFV resource allocation (NFV-RA) in a heterogeneous network where queuing delays, transmission delays, and delays resulting from virtual network function (VNF) execution are jointly considered. For this setup, we formulate a new resource allocation (RA) problem to minimize the total cost function subject to guaranteeing E2E delay of each connection. Since the proposed optimization problem is highly non-convex, we exploit alternative search method (ASM), successive convex approximation (SCA), and heuristic algorithms to solve it. Besides, for the NFV-RA, we propose an online heuristic algorithm, and analyze its performance for the TI service. Simulation results reveal that the proposed scheme can significantly reduce the network costs compared to the case where the two problems are optimized separately. Moreover, we compare the online algorithm with its offline counterpart as well as a baseline approach and it is shown that the online algorithm outperforms both of them.  © 2004-2012 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/TNSM.2020.3001359
VL  - 17
IS  - 3
SP  - 1788
EP  - 1804
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086725328&doi=10.1109%2fTNSM.2020.3001359&partnerID=40&md5=fab4f089eaba251d666d543e44db3284
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 24</p>
KW  - Approximation algorithms
KW  - Cost functions
KW  - end-to-end (E2E) delay
KW  - Heterogeneous networks
KW  - Heuristic algorithms
KW  - Heuristic methods
KW  - Network function virtualization
KW  - network function virtualization (NFV)
KW  - network service (NS)
KW  - Next generation networks
KW  - Next-generation wireless network
KW  - On-line algorithms
KW  - Optimization
KW  - Optimization problems
KW  - Quality of service
KW  - Queueing networks
KW  - queuing delay
KW  - Radio resource allocation
KW  - Radio transmission
KW  - Resource allocation
KW  - Successive convex approximations
KW  - Tactile Internet (TI)
KW  - Total cost function
KW  - transmission delay
KW  - Transmission delays
KW  - Virtual networks
KW  - virtualized network function (VNF)
KW  - Wireless networks
ER  - 

TY  - JOUR
TI  - Cloud-Native Network Slicing Using Software Defined Networking Based Multi-Access Edge Computing: A Survey
AU  - Shah, S.D.A.
AU  - Gregory, M.A.
AU  - Li, S.
T2  - IEEE Access
AB  - Fifth-Generation (5G) mobile cellular networks provide a promising platform for new, innovative and diverse IoT applications, such as ultra-reliable and low latency communication, real-time and dynamic data processing, intensive computation, and massive device connectivity. End-to-End (E2E) network slicing candidates present a promising approach to resource allocation and distribution that permit operators to flexibly provide scalable virtualized and dedicated logical networks over common physical infrastructure. Though network slicing promises the provision of services on demand, many of its use cases, such as self-driving cars and Google's Stadia, would require the integration of a Multi-Access Edge Computing (MEC) platform in 5G networks. Edge Computing is envisioned as one of the key drivers for 5G and Sixth-Generation (6G) mobile cellular networks, but its role in network slicing remains to be fully explored. We investigate MEC and network slicing for the provision of 5G service focused use cases. Recently, changes to the cloud-native 5G core are a focus with MEC use cases providing network scalability, elasticity, flexibility, and automation. A cloud-native microservices architecture, along with its potential use cases for 5G network slicing, is envisioned. This paper also elaborates on the recent advances made in enabling E2E network slicing, its enabling technologies, solutions, and current standardization efforts. Finally, this paper identifies open research issues and challenges and provides possible solutions and recommendations.  © 2013 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/ACCESS.2021.3050155
VL  - 9
SP  - 10903
EP  - 10924
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099553598&doi=10.1109%2fACCESS.2021.3050155&partnerID=40&md5=c8d253d87d2e8c0ca378ca2fb479f705
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 120</p>
KW  - 5G mobile communication systems
KW  - and low latency communication
KW  - cloud native
KW  - Data handling
KW  - Edge computing
KW  - Enabling technologies
KW  - IOT applications
KW  - Low-latency communication
KW  - Mobile cellular networks
KW  - Mobile telecommunication systems
KW  - multi-access edge computing
KW  - Network scalability
KW  - Network slicing
KW  - Queueing networks
KW  - Research issues
KW  - Services on demand
KW  - software defined networking
KW  - Software defined networking
KW  - ultra-reliable
KW  - Wireless networks
ER  - 

TY  - JOUR
TI  - Fine-Grained Cloud Resource Provisioning for Virtual Network Function
AU  - Yu, H.
AU  - Yang, J.
AU  - Fung, C.
T2  - IEEE Transactions on Network and Service Management
AB  - The deployment of Virtualized Network Functions is expected to be dynamic and swift when using Network Function Virtualization technology. The dynamic nature of workload from users requires the resource allocation of underlying infrastructure to be flexible to cope with the changes. Existing works investigated elastic NFV solutions by dynamically creating and dismantling Virtual Machine (VM) replicas, while maintaining balanced workload among VMs. However, those solutions are coarse-grained which may cause unnecessary resource over-provisioning as different network functions consume different amount of resources. In this paper, we present ElasticNFV, a dynamic and fine-grained cloud resource provisioning solution for VNF. ElasticNFV takes real-time resource demand of multiple service chains and allocates resources through an elastic provision mechanism. When a scaling conflict occurs, ElasticNFV provides a two-phase minimal migration algorithm to optimize the migration time and embedding cost of VNF instances. We implement ElasticNFV on top of the KVM platform to provide elastic VM for each VNF instance and Open vSwitch to form elastic intra-cloud network with virtual links between VNF instances. Our evaluation results show that ElasticNFV can improve VNF performance significantly, and achieve high resource utilization and fast migration time with low cost.  © 2004-2012 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/TNSM.2020.2986223
VL  - 17
IS  - 3
SP  - 1363
EP  - 1376
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083456353&doi=10.1109%2fTNSM.2020.2986223&partnerID=40&md5=bb9fe1a877ffd318d188fed2d478c885
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 34</p>
KW  - Balanced work-load
KW  - cloud computing
KW  - Evaluation results
KW  - Middlebox
KW  - Migration algorithms
KW  - Multiple services
KW  - Network function virtualization
KW  - network function virtualization (NFV)
KW  - Network functions
KW  - Over provisioning
KW  - resource allocation
KW  - resource scaling
KW  - Resource utilizations
KW  - service chain
KW  - Transfer functions
KW  - Virtual machine
KW  - Virtual networks
ER  - 

TY  - JOUR
T2  - Configure Traffic Capturing Options
DA  - 2021///
PY  - 2021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132006209&partnerID=40&md5=b7a648e16a3f0eb4bbe3c818a90e083c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
AU  - Joseph, O.
AU  - Micheal, C.
AU  - Tom, P.
T2  - Benefitting Power and Performance Sleep Loops
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132022475&partnerID=40&md5=19f655a73f3ffc6195c802d42ffb64f0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Accelerating Virtual Network Functions with Fast-Slow Path Architecture Using eXpress Data Path
AU  - Van Tu, N.
AU  - Yoo, J.-H.
AU  - Won-Ki Hong, J.
T2  - IEEE Transactions on Network and Service Management
AB  - By decoupling network functions from dedicated, proprietary hardware network devices, Network Function Virtualization (NFV) allows building Virtual Network Functions (VNFs) that can run on standard, commodity servers to reduce cost and gain flexibility in network deployment, operation, and management. However, building VNFs with high-throughput and low-latency is a big challenge. In this paper, we propose eVNF - a hybrid fast-slow path architecture to build and accelerate VNFs with eXpress Data Path (XDP), which is a Linux kernel framework that enables high performance and programmable network processing. The programmability of XDP is limited to ensure kernel safety, thus causing difficulties when using XDP to accelerate VNFs. eVNF solves this problem by taking a hybrid approach: leave the simple but critical tasks inside the kernel with XDP, and let complex tasks be processed outside XDP, e.g., in user-space. With the hybrid architecture, eVNF allows building fast and flexible VNFs. We applied eVNF to build four prototype VNFs: Flow Monitoring (eFM), Firewall (eFW), Deep Packet Inspection (eDPI), and Load Balancer (eLB). These VNFs are evaluated individually and in service function chains (SFCs) using OpenStack. Our experiments showed that eVNF can significantly improve service throughput as well as reduce latency and CPU usage. eVNF-based VNFs also can scale out with the number of CPU cores and can combine with Open vSwitch - Data Plane Development Kit (OvS-DPDK) for better performance.  © 2004-2012 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/TNSM.2020.3000255
VL  - 17
IS  - 3
SP  - 1474
EP  - 1486
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091158178&doi=10.1109%2fTNSM.2020.3000255&partnerID=40&md5=e657455c9ec1cfd0e097402e4684c7c9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 23</p>
KW  - AF_XDP
KW  - Decoupling network
KW  - Deep packet inspection
KW  - eBPF
KW  - eXpress data path
KW  - Flow monitoring
KW  - high-throughput packet processing
KW  - Hybrid architectures
KW  - Linux
KW  - low latency
KW  - network function virtualization
KW  - Network function virtualization
KW  - Network management
KW  - Programmability
KW  - Programmable network
KW  - Service functions
KW  - Transfer functions
KW  - virtual network function
KW  - Virtual networks
ER  - 

TY  - JOUR
AU  - Xiang, Z.
AU  - Höweler, M.
AU  - Zhang, R.
AU  - Xia, F.
T2  - Build-VNF A Collection of Utilities to Build, Test, and Benchmark Practical and High-Performance VNFs Using Containers
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132005087&partnerID=40&md5=13875e2f40bd37b4110bfecbe808ead6
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Latency and energy-aware provisioning of network slices in cloud networks
AU  - Borylo, P.
AU  - Tornatore, M.
AU  - Jaglarz, P.
AU  - Shahriar, N.
AU  - Chołda, P.
AU  - Boutaba, R.
T2  - Computer Communications
AB  - Modern network services are constantly increasing their requirements in terms of bandwidth, latency and cost efficiency. To satisfy these requirements, the concept of network slicing has been introduced in the context of next-generation 5G networks. However, to successfully provision resources to slices, a complex optimization problem must be addressed to allocate resources over a cloud network, i.e., a distributed computing infrastructure interconnected through high-capacity network links. In this study, we propose two new latency and energy-aware optimization models for provisioning 5G slices in cloud networks comprising both distributed computing and network resources. The proposed approaches differ from other existing solutions since we conduct our studies with respect to the end-to-end latency. Relevant models of latency and energy consumption are proposed based on a comprehensive review of the state-of-the-art. To effectively solve those optimization problems, a configurable heuristic is also proposed and investigated over different network topologies. Performance of the proposed heuristic is compared against near-optimal solutions. Moreover, we assess the importance of matching between resource provisioning algorithms and architectural assumptions related to 5G network slices and a proper problem modeling. © 2020 The Authors
DA  - 2020///
PY  - 2020
DO  - 10.1016/j.comcom.2020.03.050
VL  - 157
SP  - 1
EP  - 19
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082922044&doi=10.1016%2fj.comcom.2020.03.050&partnerID=40&md5=09956ea49da4735a0aa9f37843f50b1e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 22</p>
KW  - 5G mobile communication systems
KW  - 5g network slicing
KW  - 5G network slicing
KW  - Bandwidth efficiency
KW  - Cloud networks
KW  - Cost-efficiency
KW  - Distributed computing
KW  - Energy utilization
KW  - Energy-aware provisioning
KW  - Latency-aware
KW  - Multi-objective optimization
KW  - Multi-objectives optimization
KW  - Multiobjective optimization
KW  - Network slicing
KW  - Networks services
KW  - Queueing networks
KW  - VNF placement
ER  - 

TY  - JOUR
TI  - Toward Tactile Internet in beyond 5G Era: Recent Advances, Current Issues, and Future Directions
AU  - Sharma, S.K.
AU  - Woungang, I.
AU  - Anpalagan, A.
AU  - Chatzinotas, S.
T2  - IEEE Access
AB  - Tactile Internet (TI) is envisioned to create a paradigm shift from the content-oriented communications to steer/control-based communications by enabling real-time transmission of haptic information (i.e., touch, actuation, motion, vibration, surface texture) over Internet in addition to the conventional audiovisual and data traffics. This emerging TI technology, also considered as the next evolution phase of Internet of Things (IoT), is expected to create numerous opportunities for technology markets in a wide variety of applications ranging from teleoperation systems and Augmented/Virtual Reality (AR/VR) to automotive safety and eHealthcare towards addressing the complex problems of human society. However, the realization of TI over wireless media in the upcoming Fifth Generation (5G) and beyond networks creates various non-conventional communication challenges and stringent requirements in terms of ultra-low latency, ultra-high reliability, high data-rate connectivity, resource allocation, multiple access and quality-latency-rate tradeoff. To this end, this paper aims to provide a holistic view on wireless TI along with a thorough review of the existing state-of-the-art, to identify and analyze the involved technical issues, to highlight potential solutions and to propose future research directions. First, starting with the vision of TI and recent advances and a review of related survey/overview articles, we present a generalized framework for wireless TI in the Beyond 5G Era including a TI architecture, the main technical requirements, the key application areas and potential enabling technologies. Subsequently, we provide a comprehensive review of the existing TI works by broadly categorizing them into three main paradigms; namely, haptic communications, wireless AR/VR, and autonomous, intelligent and cooperative mobility systems. Next, potential enabling technologies across physical/Medium Access Control (MAC) and network layers are identified and discussed in detail. Also, security and privacy issues of TI applications are discussed along with some promising enablers. Finally, we present some open research challenges and recommend promising future research directions. © 2013 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ACCESS.2020.2980369
VL  - 8
SP  - 56948
EP  - 56991
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082972187&doi=10.1109%2fACCESS.2020.2980369&partnerID=40&md5=6181453932878af09e444a90f5914aac
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 159</p>
KW  - 5G
KW  - 5G mobile communication systems
KW  - Access control
KW  - Augmented reality
KW  - augmented reality (AR)
KW  - Augmented/virtual reality
KW  - beyond 5G
KW  - Cooperative communication
KW  - Future research directions
KW  - haptic communications
KW  - Haptic communications
KW  - Internet of things
KW  - Internet of Things (IOT)
KW  - IoT
KW  - Low-latency communication
KW  - Network layers
KW  - Real-time transmissions
KW  - Security and privacy issues
KW  - Tactile internet
KW  - Textures
KW  - ultra-reliable and low-latency communications (URLLC)
KW  - virtual reality (VR)
KW  - Visual servoing
ER  - 

TY  - JOUR
TI  - Machine learning methods for reliable resource provisioning in edge-cloud computing: A survey
AU  - Le Duc, T.
AU  - Leiva, R.G.
AU  - Casari, P.
AU  - Östberg, P.-O.
T2  - ACM Computing Surveys
AB  - Large-scale software systems are currently designed as distributed entities and deployed in cloud data centers. To overcome the limitations inherent to this type of deployment, applications are increasingly being supplemented with components instantiated closer to the edges of networks—a paradigm known as edge computing. The problem of how to efficiently orchestrate combined edge-cloud applications is, however, incompletely understood, and a wide range of techniques for resource and application management are currently in use. This article investigates the problem of reliable resource provisioning in joint edge-cloud environments, and surveys technologies, mechanisms, and methods that can be used to improve the reliability of distributed applications in diverse and heterogeneous network environments. Due to the complexity of the problem, special emphasis is placed on solutions to the characterization, management, and control of complex distributed applications using machine learning approaches. The survey is structured around a decomposition of the reliable resource provisioning problem into three categories of techniques: workload characterization and prediction, component placement and system consolidation, and application elasticity and remediation. Survey results are presented along with a problem-oriented discussion of the state-of-the-art. A summary of identified challenges and an outline of future research directions are presented to conclude the article. © 2019 Association for Computing Machinery.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3341145
VL  - 52
IS  - 5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072380854&doi=10.1145%2f3341145&partnerID=40&md5=47076a686bc04cca9a6254ea60f18b51
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 189</p>
KW  - Autoscaling
KW  - Cloud computing
KW  - Cloud-computing
KW  - Complex networks
KW  - Consolidation
KW  - Distributed applications
KW  - Distributed systems
KW  - Edge clouds
KW  - Edge computing
KW  - Heterogeneous networks
KW  - Machine learning
KW  - Machine learning methods
KW  - Machine-learning
KW  - Optimisations
KW  - Optimization
KW  - Placement
KW  - Reliability
KW  - Remediation
KW  - Surveys
ER  - 

TY  - JOUR
TI  - P8: P4 with Predictable Packet Processing Performance
AU  - Harkous, H.
AU  - Jarschel, M.
AU  - He, M.
AU  - Pries, R.
AU  - Kellerer, W.
T2  - IEEE Transactions on Network and Service Management
AB  - Data plane programmability brings network flexibility to a new level. However, it introduces the complexity of the data path's program as a new factor that influences packet forwarding latency and thus devices' performance. Accurate identification of the relation between data path complexity and packet forwarding latency enables the design and management of networks with predictable performance. In this article, we leverage the characteristics of P4 programming language to provide a method for estimating the packet forwarding latency as a function of the data path program. We analyze the impact of different P4 constructs on packet processing latency for three state-of-the-art P4 devices: Netronome SmartNIC, NetFPGA-SUME, and T4P4S DPDK-based software switch. Besides comparing the performance of these three targets, we use the derived results to propose a method for estimating the average packet latency, at compilation time, of arbitrary P4-based network functions implemented using the surveyed P4 constructs. The proposed method is finally validated using a set of realistic network functions, which shows that our method estimates the average packet latency with sub-microsecond precision.  © 2004-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/TNSM.2020.3030102
VL  - 18
IS  - 3
SP  - 2846
EP  - 2859
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108207879&doi=10.1109%2fTNSM.2020.3030102&partnerID=40&md5=fae627779ca282486737b020c7e5f22a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 36</p>
KW  - Average packet latencies
KW  - Complex networks
KW  - device benchmarking
KW  - modeling
KW  - Network flexibility
KW  - Network functions
KW  - P4
KW  - Packet forwarding
KW  - Packet processing
KW  - Performance evaluation
KW  - Programmability
KW  - programmable data plane
KW  - Software switches
KW  - software-defined networking
KW  - State of the art
KW  - Transfer functions
ER  - 

TY  - JOUR
TI  - Green networking with packet processing engines: Modeling and optimization
AU  - Bolla, R.
AU  - Bruschi, R.
AU  - Carrega, A.
AU  - Davoli, F.
T2  - IEEE/ACM Transactions on Networking
AB  - With the aim of controlling power consumption in metro/transport and core networks, we consider energy-aware devices able to reduce their energy requirements by adapting their performance. In particular, we focus on state-of-the-art packet processing engines, which generally represent the most energy-consuming components of network devices, and which are often composed of a number of parallel pipelines to 'divide and conquer' the incoming traffic load. Our goal is to control both the power configuration of pipelines and the way to distribute traffic flows among them. We propose an analytical model to accurately represent the impact of green network technologies (i.e., low power idle and adaptive rate) on network- and energy-aware performance indexes. The model has been validated with experimental results, performed by using energy-aware software routers loaded by real-world traffic traces. The achieved results demonstrate how the proposed model can effectively represent energy- and network-aware performance indexes. On this basis, we propose a constrained optimization policy, which seeks the best tradeoff between power consumption and packet latency times. The procedure aims at dynamically adapting the energy-aware device configuration to minimize energy consumption while coping with incoming traffic volumes and meeting network performance constraints. In order to deeply understand the impact of such policy, a number of tests have been performed by using experimental data from software router architectures and real-world traffic traces. © 2013 IEEE.
DA  - 2014///
PY  - 2014
DO  - 10.1109/TNET.2013.2242485
VL  - 22
IS  - 1
SP  - 110
EP  - 123
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873962195&doi=10.1109%2fTNET.2013.2242485&partnerID=40&md5=efbc5bbd40b15aab9a628993d298d263
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 48</p>
KW  - Adaptive rate
KW  - Forwarding engine
KW  - Green networking
KW  - Low power idle
KW  - Multipipeline
ER  - 

TY  - JOUR
AU  - Willis, K.
AU  - Walsh, T.
T2  - PowerEdge Servers and 2nd Generation Intel_ Xeon_ Scalable Processors Naming Convention and Special Use Case Offerings
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132030908&partnerID=40&md5=151d29a8a5d16dbe689416a2c534413f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
AU  - Pandit, K.
T2  - Modeling the Impact of CPU Properties to Optimize and Predict Packet-Processing Performance
DA  - 2018///
PY  - 2018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132017493&partnerID=40&md5=03524ff37d8af60c7819386ad5dc7981
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
T2  - Cloud-native network functions (CNFs)
DA  - 2018///
PY  - 2018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132038112&partnerID=40&md5=cdeef27a31fac998235b74cdda147ba4
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - Service Mesh: Challenges, state of the art, and future research opportunities
AU  - Li, W.
AU  - Lemieux, Y.
AU  - Gao, J.
AU  - Zhao, Z.
AU  - Han, Y.
T2  - Proceedings - 13th IEEE International Conference on Service-Oriented System Engineering, SOSE 2019, 10th International Workshop on Joint Cloud Computing, JCC 2019 and 2019 IEEE International Workshop on Cloud Computing in Robotic Systems, CCRS 2019
AB  - While the technology development towards microservices can significantly improve the speed and agility of software service delivery, it also raises the operational complexity associated with modern applications. This has led to the emergence of Service Mesh, a promising approach to mitigate this situation by introducing a dedicated infrastructure layer over microservices without imposing modification on the service implementations. Aiming to inspire more practical research work in this exploited area, we in this paper present a comprehensive review on the state of the art of Service Mesh and discuss the related challenges and its adoption. Finally, we highlight the opportunities for future research in this subject. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/SOSE.2019.00026
SP  - 122
EP  - 127
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065961725&doi=10.1109%2fSOSE.2019.00026&partnerID=40&md5=84fdf0c8ecda35cac57f5d6bfd63a5b8
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 148</p>
KW  - Application programs
KW  - Challenges
KW  - Cloud computing
KW  - DevOps
KW  - Edge computing
KW  - Edge Computing
KW  - Engineering research
KW  - Information services
KW  - Mesh generation
KW  - Microservices
KW  - Opportunities
KW  - Robotics
KW  - Service delivery
KW  - Service Delivery
KW  - Service Mesh
KW  - Service Oriented Architecture
KW  - Service oriented architecture (SOA)
KW  - Systems engineering
ER  - 

TY  - JOUR
TI  - Flowatcher-DPDK: Lightweight line-rate flow-level monitoring in software
AU  - Zhang, T.
AU  - Linguaglossa, L.
AU  - Gallo, M.
AU  - Giaccone, P.
AU  - Rossi, D.
T2  - IEEE Transactions on Network and Service Management
AB  - In the last few years, several software-based solutions have been proved to be very efficient for high-speed packet processing, traffic generation, and monitoring, and can be considered valid alternatives to expensive and non-flexible hardware-based solutions. In this paper, we first benchmark heterogeneous design choices for software-based packet monitoring systems in terms of achievable performance and required resources (i.e., the number of CPU cores). Building on this extensive analysis we design FloWatcher-DPDK, a DPDK-based high-speed software traffic monitor we provide to the community as an open source project. In a nutshell, FloWatcher-DPDK provides tunable fine-grained statistics at packet and flow levels. Experimental results demonstrate that FloWatcher-DPDK sustains per-flow statistics with 5-nines precision at high-speed (e.g., 14.88 Mpps) using a limited amount of resources. Finally, we showcase the usage of FloWatcher-DPDK by configuring it to analyze the performance of two open source prototypes for stateful flow-level end-host and in-network packet processing. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/TNSM.2019.2913710
VL  - 16
IS  - 3
SP  - 1143
EP  - 1156
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080533219&doi=10.1109%2fTNSM.2019.2913710&partnerID=40&md5=6b6f76f1533d5331eb49ae32e1f43c5b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 23</p>
KW  - Achievable performance
KW  - Benchmarking
KW  - Flow statistics
KW  - High-speed packet processing
KW  - Intel DPDK
KW  - Monitoring system
KW  - Network traffic monitoring
KW  - Open source projects
KW  - Open source software
KW  - Open systems
KW  - Per-flow packet measurement
KW  - Software-based solutions
KW  - Traffic generation
KW  - Traffic monitor
ER  - 

TY  - JOUR
T2  - Sample Virtualized Network Fuction (SampleVNF)
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132042614&partnerID=40&md5=5d9e08432ff92a46d4326fcd4d40e5c7
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - ENVI: Elastic resource flexing for network function virtualization
AU  - Cao, L.
AU  - Sharma, P.
AU  - Fahmy, S.
AU  - Saxena, V.
T2  - 9th USENIX Workshop on Hot Topics in Cloud Computing, HotCloud 2017, co-located with USENIX ATC 2017
AB  - Dynamic and elastic resource allocation to Virtual Network Functions (VNFs) in accordance with varying workloads is a must for realizing promised reductions in capital and operational expenses in Network Functions Virtualization (NFV). However, workload heterogeneity and complex relationships between resources allocated to a VNF and the resulting capacity makes elastic resource flexing a challenging task. We propose an NFV resource flexing system, ENVI, that uses a combination of VNF-level features and infrastructure-level features to construct a machine-learning-based decision engine for detecting resource flexing events. ENVI also extracts the dependence relationship among VNFs in deployed Service Function Chains (SFCs) to carefully plan the sequence of resource flexing steps upon scaling detection. We present preliminary results for the accuracy of ENVI’s resource flexing decision engine with two different VNFs, namely, the caching proxy Squid and the intrusion detection system Suricata. Our preliminary results show that using a combination of features to train a neural network model is a promising approach for scaling detection. © 2017 USENIX Association. All rights reserved.
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084163207&partnerID=40&md5=2d5d87131b9ee1535b7f746bf9657c76
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 25</p>
KW  - Cloud computing
KW  - Complex relationships
KW  - Decision engines
KW  - Dependence relationships
KW  - Deployed services
KW  - Engines
KW  - Feature extraction
KW  - Intrusion detection
KW  - Intrusion Detection Systems
KW  - Network function virtualization
KW  - Neural network model
KW  - Operational expense
KW  - Transfer functions
KW  - Virtual networks
KW  - Virtual reality
ER  - 

TY  - JOUR
TI  - Analysis of P4 and XDP for IoT Programmability in 6G and Beyond
AU  - Carrascal, D.
AU  - Rojas, E.
AU  - Alvarez-Horcajo, J.
AU  - Lopez-Pajares, D.
AU  - Martínez-Yelmo, I.
T2  - IoT
AB  - Recently, two technologies have emerged to provide advanced programmability in Software-Defined Networking (SDN) environments, namely P4 and XDP. At the same time, the Internet of Things (IoT) represents a pillar of future 6G networks, which will be also sustained by SDN. In this regard, there is a need to analyze the suitability of P4 and XDP for IoT. In this article, we aim to compare both technologies to help future research efforts in the field. For this purpose, we evaluate both technologies by implementing diverse use cases, assessing their performance and providing a quick qualitative overview. All tests and design scenarios are publicly available in GitHub to guarantee replication and serve as initial steps for researchers that want to initiate in the field. Results illustrate that currently XDP is the best option for constrained IoT devices, showing lower latency times, half the CPU usage, and reduced memory in comparison with P4. However, development of P4 programs is more straightforward and the amount of code lines is more similar regardless of the scenario. Additionally, P4 has a lot of potential in IoT if a special effort is made to improve the most common software target, BMv2. © 2020 by the authors.
DA  - 2020///
PY  - 2020
DO  - 10.3390/iot1020031
VL  - 1
IS  - 2
SP  - 605
EP  - 622
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111225752&doi=10.3390%2fiot1020031&partnerID=40&md5=a6cacee7fce0e09efd71e345d2ead448
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 9</p>
KW  - 5G
KW  - 6G
KW  - edge computing
KW  - IoT
KW  - P4
KW  - programmability
KW  - SDN
KW  - XDP
ER  - 

TY  - JOUR
TI  - Efficient Prediction of Network Traffic for Real-Time Applications
AU  - Iqbal, M.F.
AU  - Zahid, M.
AU  - Habib, D.
AU  - John, L.K.
T2  - Journal of Computer Networks and Communications
AB  - Accurate real-time traffic prediction is required in many networking applications like dynamic resource allocation and power management. This paper explores a number of predictors and searches for a predictor which has high accuracy and low computation complexity and power consumption. Many predictors from three different classes, including classic time series, artificial neural networks, and wavelet transform-based predictors, are compared. These predictors are evaluated using real network traces. Comparison of accuracy and cost, both in terms of computation complexity and power consumption, is presented. It is observed that a double exponential smoothing predictor provides a reasonable tradeoff between performance and cost overhead. © 2019 Muhammad Faisal Iqbal et al.
DA  - 2019///
PY  - 2019
DO  - 10.1155/2019/4067135
VL  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062350113&doi=10.1155%2f2019%2f4067135&partnerID=40&md5=2fb896ba6258eec7cb5c2f29469b648e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 72</p>
KW  - Complex networks
KW  - Computation complexity
KW  - Different class
KW  - Double exponential
KW  - Dynamic resource allocations
KW  - Efficient predictions
KW  - Electric power utilization
KW  - Networking applications
KW  - Neural networks
KW  - Real time traffics
KW  - Real-time application
KW  - Traffic control
KW  - Wavelet transforms
ER  - 

TY  - JOUR
TI  - A Comprehensive Survey of the Tactile Internet: State-of-the-Art and Research Directions
AU  - Promwongsa, N.
AU  - Ebrahimzadeh, A.
AU  - Naboulsi, D.
AU  - Kianpisheh, S.
AU  - Belqasmi, F.
AU  - Glitho, R.
AU  - Crespi, N.
AU  - Alfandi, O.
T2  - IEEE Communications Surveys and Tutorials
AB  - The Internet has made several giant leaps over the years, from a fixed to a mobile Internet, then to the Internet of Things, and now to a Tactile Internet. The Tactile Internet goes far beyond data, audio and video delivery over fixed and mobile networks, and even beyond allowing communication and collaboration among things. It is expected to enable haptic communications and allow skill set delivery over networks. Some examples of potential applications are tele-surgery, vehicle fleets, augmented reality and industrial process automation. Several papers already cover many of the Tactile Internet-related concepts and technologies, such as haptic codecs, applications, and supporting technologies. However, none of them offers a comprehensive survey of the Tactile Internet, including its architectures and algorithms. Furthermore, none of them provides a systematic and critical review of the existing solutions. To address these lacunae, we provide a comprehensive survey of the architectures and algorithms proposed to date for the Tactile Internet. In addition, we critically review them using a well-defined set of requirements and discuss some of the lessons learned as well as the most promising research directions.  © 1998-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/COMST.2020.3025995
VL  - 23
IS  - 1
SP  - 472
EP  - 523
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101801592&doi=10.1109%2fCOMST.2020.3025995&partnerID=40&md5=858188f084575fec72e04bd532b6bb3a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 125</p>
KW  - 5G/6G
KW  - artificial intelligence
KW  - Augmented reality
KW  - Communication and collaborations
KW  - Critical review
KW  - edge computing
KW  - Haptic communications
KW  - Industrial processs
KW  - ITS architecture
KW  - machine learning
KW  - Mobile Internet
KW  - Network architecture
KW  - State of the art
KW  - Supporting technology
KW  - Surveys
KW  - tactile Internet
ER  - 

TY  - JOUR
TI  - Energy aware edge computing: A survey
AU  - Jiang, C.
AU  - Fan, T.
AU  - Gao, H.
AU  - Shi, W.
AU  - Liu, L.
AU  - Cérin, C.
AU  - Wan, J.
T2  - Computer Communications
AB  - Edge computing is an emerging paradigm for the increasing computing and networking demands from end devices to smart things. Edge computing allows the computation to be offloaded from the cloud data centers to the network edge and edge nodes for lower latency, security and privacy preservation. Although energy efficiency in cloud data centers has been broadly investigated, energy efficiency in edge computing is largely left uninvestigated due to the complicated interactions between edge devices, edge servers, and cloud data centers. In order to achieve energy efficiency in edge computing, a systematic review on energy efficiency of edge devices, edge servers, and cloud data centers is required. In this paper, we survey the state-of-the-art research work on energy-aware edge computing, and identify related research challenges and directions, including architecture, operating system, middleware, applications services, and computation offloading. © 2020 Elsevier B.V.
DA  - 2020///
PY  - 2020
DO  - 10.1016/j.comcom.2020.01.004
VL  - 151
SP  - 556
EP  - 580
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078566025&doi=10.1016%2fj.comcom.2020.01.004&partnerID=40&md5=f1a80e4069ca1fa2606fc0756dcebe32
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 173</p>
KW  - Benchmarking
KW  - Cloud data centers
KW  - Computation offloading
KW  - Computation partitioning
KW  - Computing offloading
KW  - Edge computing
KW  - Energy efficiency
KW  - Green computing
KW  - Middleware
KW  - Network edges
KW  - Power management
KW  - Research challenges
KW  - Security and privacy
KW  - State of the art
KW  - Surveys
KW  - Systematic Review
ER  - 

TY  - JOUR
T2  - Linux Kernel User's and Administrator's Guide Power Management
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132019946&partnerID=40&md5=96f1ea3793863aa11d4dedafe70e4cf9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Agile Network Access Control in the Container Age
AU  - Diekmann, C.
AU  - Naab, J.
AU  - Korsten, A.
AU  - Carle, G.
T2  - IEEE Transactions on Network and Service Management
AB  - Linux containers, such as those managed by Docker, are an increasingly popular way to package and deploy complex applications. However, the fundamental security primitive of network access control for a distributed microservice deployment is often ignored or left to the network operations team. High-level application-specific security requirements are not appropriately enforced by low-level network access control lists. Apart from coarse-grained separation of virtual networks, Docker neither supports the application developer to specify nor the network operators to enforce fine-grained network access control between containers. In a fictional story, we follow DevOp engineer Alice through the lifecycle of a Web application. From the initial design and software engineering through network operations and automation, we show the task expected of Alice and propose tool-support to help. As a full-stack DevOp, Alice is involved in high-level design decisions as well as low-level network troubleshooting. Focusing on network access control, we demonstrate shortcomings in today's policy management and sketch a tool-supported solution. We survey related academic work and show that many existing tools fail to bridge between the different levels of abstractions a full-stack engineer is operating on. Our toolset is formally verified using Isabell/HOL and is available as an open source. © 2018 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/TNSM.2018.2889009
VL  - 16
IS  - 1
SP  - 41
EP  - 55
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058876640&doi=10.1109%2fTNSM.2018.2889009&partnerID=40&md5=1a172299c0ca96670d0b476a50d39b83
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 18</p>
KW  - access control
KW  - Access control
KW  - centralized management
KW  - Centralized management
KW  - Computer operating systems
KW  - container
KW  - Containers
KW  - docker
KW  - Docker
KW  - firewall
KW  - Firewall
KW  - formal methods
KW  - Formal methods
KW  - Isabelle/HOl
KW  - Isabelle/HOL
KW  - Network security
KW  - Open source software
KW  - operations & administration
KW  - policy
KW  - Public policy
KW  - Security management
KW  - tools
KW  - Tools
ER  - 

TY  - CONF
TI  - Container-based performance evaluation: A survey and challenges
AU  - Bachiega, N.G.
AU  - Souza, P.S.L.
AU  - Bruschi, S.M.
AU  - De Souza, S.D.R.S.
T2  - Proceedings - 2018 IEEE International Conference on Cloud Engineering, IC2E 2018
AB  - Virtualization is an expanding technology in all areas of research and development. It has been widely accepted because of characteristics such as elasticity and flexibility in delivering on-demand resources. Nowadays, containers are a new form of virtualization and they have been highlighted as large technology companies are giving more support to them. Containers are a type of OS-level virtualization, in which the kernel allows the existence of multiple isolated instances. This paper presents a survey of recent research and challenges involving containers, focusing on performance evaluation. To achieve this goal, we performed a systematic mapping considering the main databases (Springer, IEEE, ACM, and Scopus), raising the challenges of the area and prospects of future work, considering the recent use of containers. We have observed that there is still little research related to performance evaluation of containers and we consider this topic very important since performance is fundamental to the design and adoption of projects. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/IC2E.2018.00075
SP  - 398
EP  - 403
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048345603&doi=10.1109%2fIC2E.2018.00075&partnerID=40&md5=948531448ee4ea48f0e8b06a8ed48fb4
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 52</p>
KW  - Containers
KW  - New forms
KW  - On demands
KW  - Performance evaluation
KW  - Performance evaluations
KW  - Recent researches
KW  - Research and development
KW  - Surveys
KW  - Systematic mapping
KW  - Technology companies
KW  - Virtual reality
KW  - Virtualization
ER  - 

TY  - JOUR
T2  - Data Plane Development Kit (DPKD), Version 20.11.1, Sample Applications User Guides, Section 23. L3 Forwarding with Power Management Sample Application
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132037031&partnerID=40&md5=e12c44c15b15dfaf3ff664e1e510db5c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
AU  - Liu, Y.
T2  - Optimizing PAPI for Low-Overhead Counter Measurement
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080514388&partnerID=40&md5=e33fd7f1f7ec5d3937c87d209bfc1253
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3</p>
ER  - 

TY  - JOUR
TI  - MAPLE: A Machine Learning Approach for Efficient Placement and Adjustment of Virtual Network Functions
AU  - Wahab, O.A.
AU  - Kara, N.
AU  - Edstrom, C.
AU  - Lemieux, Y.
T2  - Journal of Network and Computer Applications
AB  - As one of the many advantages of cloud computing, Network Function Virtualization (NFV) has revolutionized the network and telecommunication industry through enabling the migration of network functions from expensive dedicated hardware to software-defined components that run in the form of Virtual Network Functions (VNFs). However, with NFV comes numerous challenges related mainly to the complexity of deploying and adjusting VNFs in the physical networks, owing to the huge number of nodes and links in today's datacenters, and the inter-dependency among VNFs forming a certain network service. Several contributions have been made in an attempt to answer these challenges, where most of the existing solutions focus on the static placement of VNFs and overlook the dynamic aspect of the problem, which arises mainly due to the ever-changing resource availability in the cloud datacenters and the continuous mobility of the users. Few attempts have been lately made to incorporate the dynamic aspect to the VNF deployment solutions. The main problem of these approaches lies in their reactive readjustment scheme which determines the placement/migration strategy upon the receipt of a new request or the happening of a certain event, thus resulting in high setup latencies. In this paper, we take advantage of machine learning to reduce the complexity of the placement and readjustment processes through designing a cluster-based proactive solution. The solution consists of (1) an Integer Linear Programming (ILP) model that considers a tradeoff between the minimization of the latency, Service-Level Objective (SLO) violation cost, hardware utilization, and VNF readjustment cost, (2) an optimized k-medoids clustering approach which proactively partitions the substrate network into a set of disjoint on-demand clusters and (3) data-driven cluster-based placement and readjustment algorithms that capitalize on machine learning to intelligently eliminate some cost functions from the optimization problem to boost its feasibility in large-scale networks. Simulation results show that the proposed solution considerably reduces the readjustment time and decrease the hardware utilization compared to the K-means, original k-medoids and migration without clustering approaches. © 2019 Elsevier Ltd
DA  - 2019///
PY  - 2019
DO  - 10.1016/j.jnca.2019.06.003
VL  - 142
SP  - 37
EP  - 50
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067443855&doi=10.1016%2fj.jnca.2019.06.003&partnerID=40&md5=a265fafa6da3450fa5aa1bc1fa2ea336
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 41</p>
KW  - Cloud computing
KW  - Complex networks
KW  - Cost functions
KW  - Data-driven optimization
KW  - E-learning
KW  - Hardware utilization
KW  - Inductive logic programming (ILP)
KW  - Integer linear programming models
KW  - Integer programming
KW  - K-means clustering
KW  - Learning algorithms
KW  - Learning systems
KW  - Machine learning
KW  - Machine learning approaches
KW  - Network function virtualization
KW  - Optimization problems
KW  - Resource availability
KW  - Semi- supervised learning
KW  - Semi-supervised learning
KW  - Service level objective
KW  - Supervised learning
KW  - Telecommunication industry
KW  - Transfer functions
KW  - Virtual reality
ER  - 

TY  - CONF
TI  - Building a power-proportional software router
AU  - Niccolini, L.
AU  - Iannaccone, G.
AU  - Ratnasamy, S.
AU  - Chandrashekar, J.
AU  - Rizzo, L.
T2  - Proceedings of the 2012 USENIX Annual Technical Conference, USENIX ATC 2012
AB  - We aim at improving the power efficiency of network routers without compromising their performance. Using server-based software routers as our prototyping vehicle, we investigate the design of a router that consumes power in proportion to the rate of incoming traffic. We start with an empirical study of power consumption in current software routers, decomposing the total power consumption into its component causes. Informed by this analysis, we develop software mechanisms that exploit the underlying hardware's power management features for more energy-efficient packet processing. We incorporate these mechanisms into Click and demonstrate a router that matches the peak performance of the original (unmodified) router while consuming up to half the power at low loads, with negligible impact on the packet forwarding latency. © 2012 by The USENIX Association. All Rights Reserved
DA  - 2019///
PY  - 2019
SP  - 89
EP  - 100
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077121508&partnerID=40&md5=13f43034d31a92be8730c0bd63298ffd
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 23</p>
KW  - Electric power utilization
KW  - Empirical studies
KW  - Energy efficiency
KW  - Incoming traffic
KW  - Packet forwarding
KW  - Packet processing
KW  - Peak performance
KW  - Routers
KW  - Software mechanisms
KW  - Software prototyping
KW  - Software routers
KW  - Total power consumption
ER  - 

TY  - JOUR
TI  - Efficient algorithms to minimize the end-to-end latency of edge network function virtualization
AU  - Ghai, K.S.
AU  - Choudhury, S.
AU  - Yassine, A.
T2  - Journal of Ambient Intelligence and Humanized Computing
AB  - In future wireless networks, network function virtualization will lay the foundation for establishing a new dynamic resource management framework to efficiently utilize network resources. The main problem discussed in this paper is the minimization of total latency for an edge network and how to solve it efficiently. A model of users, virtual network functions and hosting devices has been taken, and is used to find the minimum latency using integer linear programming. The problem is NP-hard and takes exponential time to return the optimal solution. We apply the stable matching based algorithm to solve the problem in polynomial time and then utilize local search to improve its efficiency further. From extensive performance evaluation, it is found that our proposed algorithm is very close to the optimal scheme in terms of latency and better in terms of time complexity. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.
DA  - 2020///
PY  - 2020
DO  - 10.1007/s12652-019-01630-6
VL  - 11
IS  - 10
SP  - 3963
EP  - 3974
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078343096&doi=10.1007%2fs12652-019-01630-6&partnerID=40&md5=8f899fc4e5c881d11cbf1182b8060763
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 14</p>
KW  - Dynamic resource management
KW  - End to end latencies
KW  - Future wireless networks
KW  - Hosting device
KW  - Integer Linear Programming
KW  - Integer programming
KW  - Latency
KW  - Local search
KW  - Local search (optimization)
KW  - Network function virtualization
KW  - Polynomial approximation
KW  - Stable matching
KW  - Transfer functions
KW  - Virtual reality
ER  - 

TY  - CONF
TI  - The turbo diaries: Application-controlled frequency scaling explained
AU  - Wamhoff, J.-T.
AU  - Marlier, P.
AU  - Dice, D.
AU  - Diestelhorst, S.
AU  - Felber, P.
AU  - Fetzer, C.
T2  - Proceedings of the 2014 USENIX Annual Technical Conference, USENIX ATC 2014
AB  - Most multi-core architectures nowadays support dynamic voltage and frequency scaling (DVFS) to adapt their speed to the system's load and save energy. Some recent architectures additionally allow cores to operate at boosted speeds exceeding the nominal base frequency but within their thermal design power. In this paper, we propose a general-purpose library that allows selective control of DVFS from user space to accelerate multi-threaded applications and expose the potential of heterogeneous frequencies. We analyze the performance and energy trade-offs using different DVFS configuration strategies on several benchmarks and real-world workloads. With the focus on performance, we compare the latency of traditional strategies that halt or busy-wait on contended locks and show the power implications of boosting of the lock owner. We propose new strategies that assign heterogeneous and possibly boosted frequencies while all cores remain fully operational. This allows us to leverage performance gains at the application level while all threads continuously execute at different speeds. We also derive a model to help developers decide on the optimal DVFS configuration strategy, e.g, for lock implementations. Our in-depth analysis and experimental evaluation of current hardware provides insightful guidelines for the design of future hardware power management and its operating system interface. © Proceedings of the 2014 USENIX Annual Technical Conference, USENIX ATC 2014. All rights reserved.
DA  - 2014///
PY  - 2014
SP  - 193
EP  - 204
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077433760&partnerID=40&md5=84be2f94ce36fa2a7dbed2626b459ec3
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 21</p>
KW  - Base frequencies
KW  - Benchmarking
KW  - Computer architecture
KW  - Controlled frequency
KW  - Dynamic frequency scaling
KW  - Dynamic voltage and frequency scaling
KW  - Economic and social effects
KW  - Frequency-scaling
KW  - Locks (fasteners)
KW  - Multicore architectures
KW  - Performance
KW  - Power
KW  - Save energy
KW  - System loads
KW  - Thermal designs
KW  - Voltage scaling
ER  - 

TY  - JOUR
TI  - Performance-Aware Energy Saving for Data Center Networks
AU  - Al-Tarazi, M.
AU  - Chang, J.M.
T2  - IEEE Transactions on Network and Service Management
AB  - Today's data center networks (DCNs) tend to have tens to hundreds of thousands of servers that provide massive and sophisticated services. The architectural design of DCNs are usually over-provisioned for peak workloads and fault tolerance. Statistically, DCNs remain highly under-utilized, with typical utilization of around 30%. Network over-provisioning and under-utilization can be exploited for energy-saving. Most research efforts on DCN energy saving focus on how to save maximum energy but have little or no consideration to the performance of the residual network. Thus, the DCN performance can become degraded and the network left vulnerable to sudden traffic surges. In this paper, we have studied the energy-saving problem in DCNs while preserving network performance. The problem was formulated as mixed integer linear problem (MILP) solvable by CPLEX in order to minimize the energy consumed by DCN; meanwhile, safety threshold constraints for links utilization are met. To overcome CPLEX high computational time, a heuristic algorithm to provide practical and efficient solution for the MILP is introduced. The heuristic algorithm uses switches grouping and links consolidation to switch the traffic to a small number of network devices and turn off unused switches and links. Valiant load balancing is used to distribute the loads over active links. Simulation experiments using synthetic and real packet traces were conducted to validate the heuristic in terms of energy consumption and network performance. The results show that the heuristic can save up to 45% of the network energy and improve the average imbalance scores for links and switches by more than 50% with minimal effect on network performance. © 2018 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/TNSM.2019.2891826
VL  - 16
IS  - 1
SP  - 206
EP  - 219
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062999304&doi=10.1109%2fTNSM.2019.2891826&partnerID=40&md5=cab31ef37ed02ce2dc89a5f62476eb21
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 22</p>
KW  - Computational efficiency
KW  - Computational time
KW  - Data center networks
KW  - Data center networks (DCNs)
KW  - Energy conservation
KW  - energy saving
KW  - Energy utilization
KW  - Energy-saving problems
KW  - Fault tolerance
KW  - Heuristic algorithms
KW  - Integer programming
KW  - load balancing
KW  - Mixed integer linear
KW  - Network performance
KW  - Over provisioning
KW  - Research efforts
KW  - Resource allocation
KW  - Valiant Load-balancing
ER  - 

TY  - JOUR
TI  - An Overview of Sustainable Green 5G Networks
AU  - Wu, Q.
AU  - Li, G.Y.
AU  - Chen, W.
AU  - Ng, D.W.K.
AU  - Schober, R.
T2  - IEEE Wireless Communications
AB  - The stringent requirements of a 1000x increase in data traffic and 1 ms round-trip latency have made limiting the potentially tremendous ensuing energy consumption one of the most challenging problems for the design of the upcoming 5G networks. To enable sustainable 5G networks, new technologies have been proposed to improve the system energy efficiency, and alternative energy sources are introduced to reduce our dependence on traditional fossil fuels. In particular, various 5G techniques target the reduction of the energy consumption without sacrificing the quality of service. Meanwhile, energy harvesting technologies, which enable communication transceivers to harvest energy from various renewable resources and ambient radio frequency signals for communication, have drawn significant interest from both academia and industry. In this article, we provide an overview of the latest research on both green 5G techniques and energy harvesting for communication. In addition, some technical challenges and potential research topics for realizing sustainable green 5G networks are also identified. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/MWC.2017.1600343
VL  - 24
IS  - 4
SP  - 72
EP  - 80
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028762867&doi=10.1109%2fMWC.2017.1600343&partnerID=40&md5=89d73fb2589aaf250e9c77fffda9413a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 514</p>
KW  - 5G mobile communication systems
KW  - Alternative energy source
KW  - Communication transceivers
KW  - Energy efficiency
KW  - Energy harvesting
KW  - Energy utilization
KW  - Fossil fuels
KW  - New technologies
KW  - Potential researches
KW  - Quality of service
KW  - Queueing networks
KW  - Radio transceivers
KW  - Radiofrequency signals
KW  - Renewable resource
KW  - Stringent requirement
KW  - Technical challenges
ER  - 

TY  - JOUR
TI  - On Dynamic service function chain deployment and readjustment
AU  - Liu, J.
AU  - Lu, W.
AU  - Zhou, F.
AU  - Lu, P.
AU  - Zhu, Z.
T2  - IEEE Transactions on Network and Service Management
AB  - Network function virtualization (NFV) is a promising technology to decouple the network functions from dedicated hardware elements, leading to the significant cost reduction in network service provisioning. As more and more users are trying to access their services wherever and whenever, we expect the NFV-related service function chains (SFCs) to be dynamic and adaptive, i.e., they can be readjusted to adapt to the service requests' dynamics for better user experience. In this paper, we study how to optimize SFC deployment and readjustment in the dynamic situation. Specifically, we try to jointly optimize the deployment of new users' SFCs and the readjustment of in-service users' SFCs while considering the trade-off between resource consumption and operational overhead. We first formulate an integer linear programming (ILP) model to solve the problem exactly. Then, to reduce the time complexity, we design a column generation (CG) model for the optimization. Simulation results show that the proposed CGbased algorithm can approximate the performance of the ILP and outperform an existing benchmark in terms of the profit from service provisioning. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/TNSM.2017.2711610
VL  - 14
IS  - 3
SP  - 543
EP  - 553
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028930403&doi=10.1109%2fTNSM.2017.2711610&partnerID=40&md5=4038ee32a096d49608a1956a4597f4b8
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 280</p>
KW  - Benchmarking
KW  - Chains
KW  - Column generation
KW  - Cost reduction
KW  - Dynamic support
KW  - Dynamic supports
KW  - Economic and social effects
KW  - Function virtualization (NFV)
KW  - In-network services
KW  - Integer linear programming models
KW  - Integer programming
KW  - Network function virtualization
KW  - Operational Overheads
KW  - Resource consumption
KW  - Service function chain (SFC)
KW  - Service functions
KW  - Service provisioning
KW  - Transfer functions
KW  - Virtual reality
KW  - Virtualization
ER  - 

TY  - JOUR
TI  - Reducing Latency in Virtual Machines: Enabling Tactile Internet for Human-Machine Co-Working
AU  - Xiang, Z.
AU  - Gabriel, F.
AU  - Urbano, E.
AU  - Nguyen, G.T.
AU  - Reisslein, M.
AU  - Fitzek, F.H.P.
T2  - IEEE Journal on Selected Areas in Communications
AB  - Software-defined networking (SDN) and network function virtualization (NFV) processed in multi-access edge computing (MEC) cloud systems have been proposed as critical paradigms for achieving the low latency requirements of the tactile Internet. While virtual network functions (VNFs) allow greater flexibility compared to hardware-based solutions, the VNF abstraction also introduces additional packet processing delays. In this paper, we investigate the practical feasibility of NFV with respect to the tactile Internet latency requirements. We develop, implement, and evaluate Chain-based Low latency VNF ImplemeNtation (CALVIN), a low-latency management framework for distributed Service Function Chains (SFCs). CALVIN classifies VNFs into elementary, basic, and advanced VNFs; moreover, CALVIN implements elementary and basic VNFs in the kernel space, while the advanced VNFs are implemented in the user space. Throughout, CALVIN employs a distributed mapping with one VNF per Virtual Machine (VM) in a MEC system. Furthermore, CALVIN avoids the metadata structure processing and batch processing of packets in the conventional Linux networking stack so as to achieve short per-packet latencies. Our rigorous measurements on off-the-shelf conventional networking and computing hardware demonstrate that CALVIN achieves round-trip times from a MEC ingress point via two elementary forwarding VNFs (one in kernel space and one in user space) and a MEC server to a MEC egress point on the order of 0.32 ms. Our measurements also indicate that MEC network coding and encryption are feasible for small 256 byte packets with an MEC latency budget of 0.35 ms; whereas, large 1400 byte packets can complete the network coding, but not the encryption within the 0.35 ms. © 1983-2012 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/JSAC.2019.2906788
VL  - 37
IS  - 5
SP  - 1098
EP  - 1116
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063097866&doi=10.1109%2fJSAC.2019.2906788&partnerID=40&md5=c6f601aac8dbfd6ca768e42bcb1e314b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 101</p>
KW  - Batch data processing
KW  - Bridges
KW  - Budget control
KW  - Cloud computing
KW  - Computer hardware
KW  - Computing hardware
KW  - Cryptography
KW  - Delays
KW  - Distributed service
KW  - Internet latencies
KW  - kernel space
KW  - Linux
KW  - Low-latency network softwarization
KW  - Management frameworks
KW  - Network coding
KW  - Network function virtualization
KW  - Network security
KW  - Packet latencies
KW  - Packet processing
KW  - Servers
KW  - service function chain (SFC)
KW  - Software defined networking (SDN)
KW  - tactile Internet
KW  - Telecommunication networks
KW  - Transfer functions
KW  - user space
KW  - Virtual machine
KW  - Virtual reality
KW  - virtualized network function (VNF)
ER  - 

TY  - JOUR
TI  - PacketUsher: Exploiting DPDK to accelerate compute-intensive packet processing
AU  - Ren, Q.
AU  - Zhou, L.
AU  - Xu, Z.
AU  - Zhang, Y.
AU  - Zhang, L.
T2  - Computer Communications
AB  - Many compute-intensive network applications such as application-layer traffic generator, Deep Packet Inspection(DPI) and web servers are widely deployed on commodity PC for the reason of flexibility and cheap price. However, how to improve their performance on general purpose OS is challenging due to the high packet I/O related overheads. This paper presents PacketUsher, a high-performance packets processing framework to remove these performance bottlenecks. In building PacketUsher, we constructed a DPDK wrapper as the underlying packet I/O engine to accelerate packet transmission, and utilized the strategies of zero copy, batch processing and parallelism to improve packet processing. Through RFC2544 benchmark, we demonstrate that DPDK wrapper has excellent packets transmission capability. As a case study of PacketUsher, we design and implement a commercial application-layer traffic generator. The experiment results show that the FPS (Flow Per Second) value of our traffic generator over PacketUsher is more than 4 times of that over standard Linux platform. By comparison, the FPS value over PacketUsher is about 3 times of that over existing methods (Netmap and PF_RING). © 2020 Elsevier B.V.
DA  - 2020///
PY  - 2020
DO  - 10.1016/j.comcom.2020.07.040
VL  - 161
SP  - 324
EP  - 333
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089230815&doi=10.1016%2fj.comcom.2020.07.040&partnerID=40&md5=70f29d9dac32dc213c115392678755d4
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 11</p>
KW  - Application Layer
KW  - Batch data processing
KW  - Commercial applications
KW  - Commodity PC
KW  - Computer operating systems
KW  - Deep packet inspection (DPI)
KW  - Design and implements
KW  - DPDK
KW  - Network applications
KW  - Packet processing
KW  - Packet transmissions
KW  - Performance bottlenecks
KW  - Performance improvement
KW  - Personal computers
KW  - Traffic generation
KW  - Traffic generators
KW  - Transmission capability
ER  - 

TY  - JOUR
TI  - Resource Aware Routing for Service Function Chains in SDN and NFV-Enabled Network
AU  - Pei, J.
AU  - Hong, P.
AU  - Xue, K.
AU  - Li, D.
T2  - IEEE Transactions on Services Computing
AB  - Owing to the Network Function Virtualization (NFV) and Software-Defined Networks (SDN), Service Function Chain (SFC) has become a popular service in SDN and NFV-enabled network. However, as the Virtual Network Function (VNF) of each type is generally multi-instance and flows with SFC requests must traverse a series of specified VNFs in predefined orders, it is a challenge for dynamic SFC formation to optimally select VNF instances and construct paths. Moreover, the load balancing and end-to-end delay need to be paid attention to, when routing flows with SFC requests. Additionally, fine-grained scheduling for traffic at flow level needs differentiated routing which should take flow features into consideration. Unfortunately, traditional algorithms cannot fulfill all these requirements. In this paper, we study the Differentiated Routing Problem considering SFC (DRP-SFC) in SDN and NFV-enabled network. We formulate the DRP-SFC as a Binary Integer Programming (BIP) model aiming to minimize the resource consumption costs of flows with SFC requests. Then a novel routing algorithm, Resource Aware Routing Algorithm (RA-RA), is proposed to solve the DRP-SFC. Performance evaluation shows that RA-RA can efficiently solve the DRP-SFC and surpass the performance of other existing algorithms in acceptance rate, throughput, hop count and load balancing.  © 2008-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/TSC.2018.2849712
VL  - 14
IS  - 4
SP  - 985
EP  - 997
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049062637&doi=10.1109%2fTSC.2018.2849712&partnerID=40&md5=5e6fda603b602fd7403730e7f61bd829
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 64</p>
KW  - Bandwidth
KW  - Cloud computing
KW  - Delays
KW  - differentiated routing
KW  - Differentiated Routing
KW  - Electric load management
KW  - flow feature
KW  - Flow features
KW  - Integer programming
KW  - Linear programming
KW  - network function virtualization
KW  - Network function virtualization
KW  - Routing
KW  - Routing algorithms
KW  - Service function chain
KW  - Service functions
KW  - Software defined networking
KW  - software-defined networks
KW  - Transfer functions
ER  - 

TY  - JOUR
T2  - Intel Streaming SIMD Extension
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949971730&partnerID=40&md5=0ce17c9e7bd498d2c311eebf3b3419f8
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 4</p>
ER  - 

TY  - JOUR
TI  - Service Function Chain Composition, Placement, and Assignment in Data Centers
AU  - Wang, Z.
AU  - Zhang, J.
AU  - Huang, T.
AU  - Liu, Y.
T2  - IEEE Transactions on Network and Service Management
AB  - With the development of network function virtualization (NFV), service function chains (SFCs) are deployed via virtual network functions (VNFs). In general, the SFCs are served via composition and then deployed into data center infrastructures. However, most of the existing works neglect SFC composition. Furthermore, they consider that VNF instances are independently deployed for each SFC, which may underutilize the computational power of servers. We consider, for each required VNF in the chain, the operator can either place it on a new instance or assign it to an established instance if the residual resource of that instance is sufficient. Such a deployment scheme can leverage resources more efficiently and we define it as SFC placement and assignment. In this paper, we first combine SFC composition, placement and assignment together to enhance resource allocation. We present the system model and formulate the problem as 0-1 integer programming. We aim to improve the VNF instance utilization as well as reduce the link consumption. A heuristic approach called Jcap is developed to solve the problem in two stages. The simulations show that Jcap achieves competitive performance with the optimal results obtained from mathematical model. © 2004-2012 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/TNSM.2019.2933872
VL  - 16
IS  - 4
SP  - 1638
EP  - 1650
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076701045&doi=10.1109%2fTNSM.2019.2933872&partnerID=40&md5=63970726335a5ce7c483886a20447349
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 42</p>
KW  - 0-1-Integer Programming
KW  - Chain composition
KW  - Competitive performance
KW  - Computational power
KW  - Heuristic approach
KW  - Heuristic methods
KW  - Integer programming
KW  - Network function virtualization
KW  - Residual resource
KW  - Service functions
KW  - SFC composition
KW  - Transfer functions
KW  - virtual network function assignment
KW  - virtual network function placement
KW  - Virtual networks
KW  - Virtual reality
ER  - 

TY  - JOUR
T2  - Data plane development kit power optimization on advantech*network appliance platform
DA  - 2021///
PY  - 2021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132017273&partnerID=40&md5=934669300c2ff13e3d58b928c5e753e3
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Towards Power Efficient High Performance Packet I/O
AU  - Li, X.
AU  - Cheng, W.
AU  - Zhang, T.
AU  - Ren, F.
AU  - Yang, B.
T2  - IEEE Transactions on Parallel and Distributed Systems
AB  - Recently, high performance packet I/O frameworks continue to flourish for their ability to process packets from high-speed links. To achieve high throughput and low latency, high performance packet I/O frameworks usually employ busy polling. As busy polling will burn all CPU cycles even if there's no packet to process, these frameworks are quite power inefficient. However, exploiting power management techniques such as DVFS and LPI in the frameworks is challenging, because neither the OS nor the frameworks can provide information (e.g., actual CPU utilization, available idle period, or the target frequency) required by these techniques. In this article, we establish a model that can formulate the packet processing flow of high performance packet I/O to help and address the above challenges. From the model, we can deduce the information needed for power management techniques, and gain the insights to balance the power and latency. After suggesting to use pause instruction to reduce CPU power within short idle period, we propose two approaches to conduct power conservation for high performance packet I/O: one with the aid of traffic information and the other without. Experiments with Intel DPDK show that both approaches can achieve significant power reduction with little latency increase. © 1990-2012 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/TPDS.2019.2957746
VL  - 31
IS  - 4
SP  - 981
EP  - 996
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078235600&doi=10.1109%2fTPDS.2019.2957746&partnerID=40&md5=78d36338bb0cde61ddc552f5aea3c420
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 7</p>
KW  - busy polling
KW  - DVFS
KW  - Electric power utilization
KW  - High performance packet I/O
KW  - Industrial management
KW  - latency
KW  - pause instruction
KW  - power consumption
KW  - Power management
KW  - Speed control
ER  - 

TY  - JOUR
T2  - ARMv8-A Power Management
DA  - 2021///
PY  - 2021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132022544&partnerID=40&md5=f1712db0ea6541aee8d7821a8b44810a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - NetVM: High performance and flexible networking using virtualization on commodity platforms
AU  - Hwang, J.
AU  - Ramakrishnan, K.K.
AU  - Wood, T.
T2  - IEEE Transactions on Network and Service Management
AB  - NetVM brings virtualization to the Network by enabling high bandwidth network functions to operate at near line speed, while taking advantage of the flexibility and customization of low cost commodity servers. NetVM allows customizable data plane processing capabilities such as firewalls, proxies, and routers to be embedded within virtual machines, complementing the control plane capabilities of Software Defined Networking. NetVM makes it easy to dynamically scale, deploy, and reprogram network functions. This provides far greater flexibility than existing purpose-built, sometimes proprietary hardware, while still allowing complex policies and full packet inspection to determine subsequent processing. It does so with dramatically higher throughput than existing software router platforms. NetVM is built on top of the KVM platform and Intel DPDK library. We detail many of the challenges we have solved such as adding support for high-speed inter-VM communication through shared huge pages and enhancing the CPU scheduler to prevent overheads caused by inter-core communication and context switching. NetVM allows true zero-copy delivery of data to VMs both for packet processing and messaging among VMs within a trust boundary. Our evaluation shows how NetVM can compose complex network functionality from multiple pipelined VMs and still obtain throughputs up to 10 Gbps, an improvement of more than 250% compared to existing techniques that use SR-IOV for virtualized networking. © 2004-2012 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/TNSM.2015.2401568
VL  - 12
IS  - 1
SP  - 34
EP  - 47
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926028205&doi=10.1109%2fTNSM.2015.2401568&partnerID=40&md5=c619fddc8f4e379a6e692e7fe9cc5e16
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 182</p>
KW  - Broadband networks
KW  - Cloud computing
KW  - Complex networks
KW  - Computer system firewalls
KW  - Data handling
KW  - Data-plane processing
KW  - Distributed computer systems
KW  - High-bandwidth networks
KW  - Inter-core communications
KW  - Network function virtualization
KW  - Network functionality
KW  - Network functions
KW  - Packet networks
KW  - Program processors
KW  - Routers
KW  - Software defined network
KW  - Software-defined networkings
KW  - Software-defined networks
KW  - Virtual reality
KW  - Virtualizations
ER  - 

TY  - CONF
TI  - An Energy Efficiency Feature Survey of the Intel Haswell Processor
AU  - Hackenberg, D.
AU  - Schöne, R.
AU  - Ilsche, T.
AU  - Molka, D.
AU  - Schuchart, J.
AU  - Geyer, R.
T2  - Proceedings - 2015 IEEE 29th International Parallel and Distributed Processing Symposium Workshops, IPDPSW 2015
AB  - The recently introduced Intel Xeon E5-1600 v3 and E5-2600 v3 series processors - codenamed Haswell-EP - implement major changes compared to their predecessors. Among these changes are integrated voltage regulators that enable individual voltages and frequencies for every core. In this paper we analyze a number of consequences of this development that are of utmost importance for energy efficiency optimization strategies such as dynamic voltage and frequency scaling (DVFS) and dynamic concurrency throttling (DCT). This includes the enhanced RAPL implementation and its improved accuracy as it moves from modeling to actual measurement. Another fundamental change is that every clock speed above AVX frequency - including nominal frequency - is opportunistic and unreliable, which vastly decreases performance predictability with potential effects on scalability. Moreover, we characterize significantly changed p-state transition behavior, and determine crucial memory performance data. © 2015 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/IPDPSW.2015.70
SP  - 896
EP  - 904
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962251153&doi=10.1109%2fIPDPSW.2015.70&partnerID=40&md5=582112a827fa8312102cfe118f3b0dba
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 191</p>
KW  - Actual measurements
KW  - Benchmark testing
KW  - Dynamic frequency scaling
KW  - Dynamic voltage and frequency scaling
KW  - Electric current regulators
KW  - Electric power measurement
KW  - Energy efficiency
KW  - Energy efficiency optimizations
KW  - Energy measurement
KW  - Frequency measurement
KW  - Frequency measurements
KW  - Fundamental changes
KW  - Parallel processing systems
KW  - Power demand
KW  - Power demands
KW  - Random access memory
KW  - Random access storage
KW  - Regulators
KW  - Voltage control
KW  - Voltage regulators
KW  - Voltage scaling
ER  - 

TY  - JOUR
T2  - Packetbeat Documentation
DA  - 2021///
PY  - 2021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132007624&partnerID=40&md5=23964327af4c0255958d698be2a14253
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - NetBricks: Taking the V out of NFV
AU  - Panda, A.
AU  - Han, S.
AU  - Jang, K.
AU  - Walls, M.
AU  - Ratnasamy, S.
AU  - Shenker, S.
T2  - Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016
AB  - The move from hardware middleboxes to software network functions, as advocated by NFV, has proven more challenging than expected. Developing new NFs remains a tedious process, requiring that developers repeatedly rediscover and reapply the same set of optimizations, while current techniques for providing isolation between NFs (using VMs or containers) incur high performance overheads. In this paper we describe NetBricks, a new NFV framework that tackles both these problems. For building NFs we take inspiration from modern data analytics frameworks (e.g., Spark and Dryad) and build a small set of customizable network processing elements. We also embrace type checking and safe runtimes to provide isolation in software, rather than rely on hardware isolation. NetBricks provides the same memory isolation as containers and VMs, without incurring the same performance penalties. To improve I/O efficiency, we introduce a novel technique called zero-copy software isolation. © 2016 by The USENIX Association All Rights Reserved.
DA  - 2016///
PY  - 2016
SP  - 203
EP  - 216
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077011241&partnerID=40&md5=092d2020118409b814bf52a92711d1c9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 241</p>
KW  - Containers
KW  - Customizable
KW  - Data Analytics
KW  - Memory isolation
KW  - Middleboxes
KW  - Network function virtualization
KW  - Network-processing elements
KW  - Novel techniques
KW  - Performance penalties
KW  - Software network
KW  - Systems analysis
KW  - Typechecking
ER  - 

TY  - CONF
TI  - Modeling IP traffic using the batch Markovian arrival process
AU  - Klemm, A.
AU  - Lindemann, C.
AU  - Lohmann, M.
T2  - Performance Evaluation
AB  - In this paper, we show how to utilize the expectation-maximization (EM) algorithm for efficient and numerical stable parameter estimation of the batch Markovian arrival process (BMAP). In fact, effective computational formulas for the E-step of the EM algorithm are presented, which utilize the well-known randomization technique and a stable calculation of Poisson jump probabilities. Moreover, we identify the BMAP as an analytically tractable model of choice for aggregated traffic modeling of IP networks. The key idea of this aggregated traffic model lies in customizing the BMAP such that different lengths of IP packets are represented by rewards of the BMAP. Using measured traffic data, a comparative study with the MMPP and the Poisson process illustrates the effectiveness of the customized BMAP for IP traffic modeling by visual inspection of sample paths over several time scales, by presenting important statistical properties as well as by investigations of queuing behavior. © 2003 Elsevier Science B.V. All rights reserved.
DA  - 2003///
PY  - 2003
DO  - 10.1016/S0166-5316(03)00067-1
VL  - 54
SP  - 149
EP  - 173
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042328345&doi=10.1016%2fS0166-5316%2803%2900067-1&partnerID=40&md5=903c5b09c8e6ad84cb69f3a34947cbc9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 237</p>
KW  - Algorithms
KW  - Analytical/numerical models of aggregated IP traffic
KW  - EM algorithm
KW  - Expectation-maximization (EM) algorithms
KW  - Markov processes
KW  - Numerical transient analysis of Markov chains
KW  - Parameter estimation
KW  - Probability
KW  - Telecommunication networks
KW  - Telecommunication traffic
ER  - 

TY  - JOUR
TI  - Hardware-Accelerated Platforms and Infrastructures for Network Functions: A Survey of Enabling Technologies and Research Studies
AU  - Shantharama, P.
AU  - Thyagaturu, A.S.
AU  - Reisslein, M.
T2  - IEEE Access
AB  - In order to facilitate flexible network service virtualization and migration, network functions (NFs) are increasingly executed by software modules as so-called 'softwarized NFs' on General-Purpose Computing (GPC) platforms and infrastructures. GPC platforms are not specifically designed to efficiently execute NFs with their typically intense Input/Output (I/O) demands. Recently, numerous hardware-based accelerations have been developed to augment GPC platforms and infrastructures, e.g., the central processing unit (CPU) and memory, to efficiently execute NFs. This article comprehensively surveys hardware-accelerated platforms and infrastructures for executing softwarized NFs. This survey covers both commercial products, which we consider to be enabling technologies, as well as relevant research studies. We have organized the survey into the main categories of enabling technologies and research studies on hardware accelerations for the CPU, the memory, and the interconnects (e.g., between CPU and memory), as well as custom and dedicated hardware accelerators (that are embedded on the platforms); furthermore, we survey hardware-accelerated infrastructures that connect GPC platforms to networks (e.g., smart network interface cards). We find that the CPU hardware accelerations have mainly focused on extended instruction sets and CPU clock adjustments, as well as cache coherency. Hardware accelerated interconnects have been developed for on-chip and chip-to-chip connections. Our comprehensive up-to-date survey identifies the main trade-offs and limitations of the existing hardware-accelerated platforms and infrastructures for NFs and outlines directions for future research. © 2013 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ACCESS.2020.3008250
VL  - 8
SP  - 132021
EP  - 132085
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089502667&doi=10.1109%2fACCESS.2020.3008250&partnerID=40&md5=5c5d820f5731abd41387747e740a65a4
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 78</p>
KW  - Central processing unit (CPU)
KW  - Chip-to-chip connections
KW  - Commercial products
KW  - Dedicated hardware
KW  - Economic and social effects
KW  - Enabling technologies
KW  - Flexible network services
KW  - General-purpose computing
KW  - Hardware acceleration
KW  - hardware accelerator
KW  - Hardware-accelerated
KW  - Infrastructure as a service (IaaS)
KW  - Integrated circuit interconnects
KW  - interconnect
KW  - memory
KW  - Program processors
KW  - software defined networking (SDN)
KW  - Surveys
KW  - Transfer functions
KW  - virtualized network function (VNF)
ER  - 

TY  - JOUR
TI  - Optimization of Virtualization Cost, Processing Power and Network Load of 5G Software-Defined Data Centers
AU  - Salhab, N.
AU  - Rahim, R.
AU  - Langar, R.
T2  - IEEE Transactions on Network and Service Management
AB  - Virtualization is getting unprecedented attention from Mobile Network Operators (MNOs) as it provides agility in deployment, especially when coupled with the Cloud that offers inherent elasticity and load-balancing of resources. MNOs have to ensure operational excellence by meeting several objectives. In this context, we propose in this paper, a framework for optimizing the mapping of next Generation Node-Bs (gNBs) to Software-Defined 5G Core (5GC) delay tolerant Network Functions (NFs). These NFs are considered to be deployed as a Virtual Machine (VM) pool, or containers, in order to minimize cloud computing cost, processing power and at the same time maximize network load. First, we formulate this problem as an integer linear program, while taking into account multiple constraints including Virtual Central Processing Unit (vCPU) capacity, central processing load limits and integrality of mapping relations between gNBs and 5GC NFs. Then, we propose an algorithm to solve large problem instances based on Branch, Cut and Price (BCP) combining all of 'Branch and Price', 'Branch and Cut' and 'Branch and Bound' frameworks. We present several schemes reflecting different optimization goals that the MNO can foster: virtualization cost, power minimization, network load or all. Simulation results demonstrate the good performance of our proposed algorithm to solve the gNBs-VM pool mapping for all evaluated schemes, while also emphasizing the advantages of a particular one (EWoS-333 for Equal Weight optimization Scheme) that can decrease virtualization cost by almost one order of magnitude compared to a static selection scheme, while considering the other two objectives.  © 2004-2012 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/TNSM.2020.2990664
VL  - 17
IS  - 3
SP  - 1542
EP  - 1553
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085129294&doi=10.1109%2fTNSM.2020.2990664&partnerID=40&md5=146993b049ffa8217325b4cab73744ba
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 10</p>
KW  - 5G
KW  - 5G mobile communication systems
KW  - Balancing
KW  - branch
KW  - Cloud computing costs
KW  - cut and price (BCP)
KW  - Delay tolerant networks
KW  - Integer linear programs
KW  - Integer programming
KW  - Mapping
KW  - mobile network operator (MNO)
KW  - Mobile network operators
KW  - Multi-objective optimization
KW  - Multiple constraint
KW  - Operational excellence
KW  - Optimization goals
KW  - Power minimization
KW  - Program processors
KW  - Virtual machine
KW  - Virtualization
KW  - virtualized network functions
KW  - Weight optimization
KW  - Wireless networks
ER  - 

TY  - CONF
TI  - Cloud-native Service Function Chaining for 5G based on Network Service Mesh
AU  - Dab, B.
AU  - Fajjari, I.
AU  - Rohon, M.
AU  - Auboin, C.
AU  - Diquelou, A.
T2  - IEEE International Conference on Communications
AB  - 5G will provide a flexible and programmable infrastructure, allowing different networks to share the same access network. A way to respond to the diverse service requirements of 5G while reducing both CAPEX and OPEX is to adopt cloud-native architectures. In this context, micro-services software design, the corner stone of cloud-native architecture, seems to be ideal for 5G. However, despite its several advantages, micro-services raise new challenges which slow its adoption down in the NFV ecosystem. Indeed, steering the expected 5G traffic between cloud-native network function is extremely challenging and is still under-investigated. In this paper, we address the service function chaining (SFC) in micro-service based network function virtualization (NFV) ecosystem from the view of the traffic steering. Specifically, we design and implement a cloud-native SFC framework offering efficient traffic steering mechanisms while considering the network state of the underlying NFV infrastructure. In this context, an optimized network-aware load balancing strategy is proposed. Based on extensive experiments, the results obtained show that our strategy achieved good results in terms of i) end-to-end latency and ii) deployment time. © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ICC40277.2020.9149045
VL  - 2020-June
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089411572&doi=10.1109%2fICC40277.2020.9149045&partnerID=40&md5=991065d62ed128673406bb673f31442c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 29</p>
KW  - 5G mobile communication systems
KW  - Cloud-native functions
KW  - Computer architecture
KW  - Design and implements
KW  - Ecosystems
KW  - End to end latencies
KW  - kubernetes
KW  - Load balancing strategy
KW  - Network function virtualization
KW  - Network functions
KW  - network service mesh
KW  - Network services
KW  - service function chaining
KW  - Service functions
KW  - Service requirements
KW  - Software design
KW  - Steering mechanisms
KW  - traffic steering
KW  - Transfer functions
ER  - 

TY  - JOUR
TI  - Traffic-Aware and Energy-Efficient vNF Placement for Service Chaining: Joint Sampling and Matching Approach
AU  - Pham, C.
AU  - Tran, N.H.
AU  - Ren, S.
AU  - Saad, W.
AU  - Hong, C.S.
T2  - IEEE Transactions on Services Computing
AB  - Although network function virtualization (NFV) is a promising approach for providing elastic network functions, it faces several challenges in terms of adaptation to diverse network appliances and reduction of the capital and operational expenses of the service providers. In particular, to deploy service chains, providers must consider different objectives, such as minimizing the network latency or the operational cost, which are coupled objectives that have traditionally been addressed separately. In this paper, the problem of virtual network function (vNF) placement for service chains is studied for the purpose of energy and traffic-aware cost minimization. This problem is formulated as an optimization problem named the joint operational and network traffic cost (OPNETOPNET) problem. First, a sampling-based Markov approximation (MA) approach is proposed to solve the combinatorial NP-hard problem, OPNETOPNET. Even though the MA approach can yield a near-optimal solution, it requires a long convergence time that can hinder its practical deployment. To overcome this issue, a novel approach that combines the MA with matching theory, named as SAMASAMA, is proposed to find an efficient solution for the original problem OPNETOPNET. Simulation results show that the proposed framework can reduce the total incurred cost by up to 19 percent compared to the existing non-coordinated approach. © 2008-2012 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/TSC.2017.2671867
VL  - 13
IS  - 1
SP  - 172
EP  - 185
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079575000&doi=10.1109%2fTSC.2017.2671867&partnerID=40&md5=2843a9cb9b4f3d22fd720f93b20cc9f0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 168</p>
KW  - Data centers
KW  - Datacenters
KW  - Energy efficiency
KW  - Markov approximation
KW  - Near-optimal solutions
KW  - Network appliances
KW  - network function virtualization
KW  - Network function virtualization
KW  - network traffic
KW  - Network traffic
KW  - NP-hard
KW  - Operational expense
KW  - Optimization
KW  - Optimization problems
KW  - resource allocation
KW  - Resource allocation
KW  - Transfer functions
KW  - virtual network function
KW  - Virtual networks
KW  - Virtual reality
ER  - 

TY  - CONF
TI  - A black-box approach for estimating utilization of polled Io network functions
AU  - Gupta, H.
AU  - Sharma, A.
AU  - Zelezniak, A.
AU  - Jang, M.
AU  - Ramachandran, U.
T2  - 11th USENIX Workshop on Hot Topics in Cloud Computing, HotCloud 2019, co-located with USENIX ATC 2019
AB  - Cloud management tasks such as performance diagnosis, workload placement, and power management depend critically on estimating the utilization of an application. But, it is challenging to measure actual utilization for polled IO network functions (NFs) without code instrumentation. We ask if CPU events (e.g., data cache misses) measured using hardware performance counters are good at estimating utilization for polled-IO NFs. We find a strong correlation between several CPU events and NF utilization for three representative types of network functions. Inspired by this finding, we explore the possibility of computing a universal estimation function that maps selected CPU events to NF utilization estimates for a wide-range of NFs, traffic profiles and traffic loads. Our NF-specific estimators and universal estimators achieve absolute estimation errors below 6% and 10% respectively. © 2019 USENIX Association. All rights reserved.
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084160867&partnerID=40&md5=1fa40eccf47ac5e71cca2524536bb2f1
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2</p>
KW  - Black box approach
KW  - Cloud computing
KW  - Cloud managements
KW  - Code instrumentation
KW  - Estimation errors
KW  - Hardware performance counters
KW  - Performance diagnosis
KW  - Strong correlation
KW  - Transfer functions
KW  - Universal estimation
ER  - 

TY  - JOUR
AU  - Trifonov, H.G.
T2  - Traffic-Aware Adaptive Polling Mechanism for High Performance Packet Processing
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080587284&partnerID=40&md5=a75704d121678886dc454603b6f48409
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3</p>
ER  - 

TY  - JOUR
T2  - Open vSwitch With DPDK
DA  - 2021///
PY  - 2021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132024182&partnerID=40&md5=35e7db0440d79dfe4c65fc950708c9a0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - Optimized scalable SFC traffic steering scheme for cloud native based applications
AU  - Bouridah, A.
AU  - Fajjari, I.
AU  - Aitsaadi, N.
AU  - Belhadef, H.
T2  - 2021 IEEE 18th Annual Consumer Communications and Networking Conference, CCNC 2021
AB  - Network Function Virtualization (NFV) has already proven its efficiency to deploy networking services in large-scale. Recent advances of cloud-native applications may bring new advantage by deploying and implementing Virtual Network Function (VNFs) as cloud-native Containers rather than virtual machines. Beside remarkable advantages such as lower overhead and faster running, microservices (cloud-native containers) intend to save costs while increasing the service agility. To this end, in this paper we extend consolidated state-of-the-art tools and technologies developed in two domains cloud-native applications and Network Function Virtualization (NFV). The proposed framework chains services provisioned across Kubernetes and Contiv/VPP domains and using containers. Our orchestration framework chain services across distributed CNFs. Furthermore, we propose K -TS scheme to load balance the traffic over services replicas. K -TS is based on Ketama Consistent hashing algorithm. Experimental simulations show very good results for both the service chaining framework in term of QoS satisfaction such as: packet error rate, throughput satisfaction and jitter. © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/CCNC49032.2021.9369583
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102979951&doi=10.1109%2fCCNC49032.2021.9369583&partnerID=40&md5=5283bc172f013d195f516ad251f4de31
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 11</p>
KW  - Consistent Hashing algorithms
KW  - Containers
KW  - Experimental simulations
KW  - Its efficiencies
KW  - Ketama algorithm
KW  - Kubernetes
KW  - Microservices
KW  - Network function virtualization
KW  - Networking services
KW  - NFV
KW  - Packet error rates
KW  - QoS
KW  - Service Function Chain
KW  - State of the art
KW  - Tools and technologies
KW  - Transfer functions
KW  - Virtual networks
ER  - 

TY  - CONF
TI  - Improving Energy Efficiency in NFV Clouds with Machine Learning
AU  - Moreira Zorello, L.M.
AU  - Torres Vieira, M.G.
AU  - Girani Tejos, R.A.
AU  - Torres Rojas, M.A.
AU  - Meirosu, C.
AU  - Melo De Brito Carvalho, T.C.
T2  - IEEE International Conference on Cloud Computing, CLOUD
AB  - Widespread deployments of Network Function Virtualization (NFV) technology will replace many physical appliances in telecommunication networks with software executed on cloud platforms. Setting compute servers continuously to high-performance operating modes is a common NFV approach for achieving predictable operations. However, this has the effect that large amounts of energy are consumed even when little traffic needs to be forwarded. The Dynamic Voltage-Frequency Scaling (DVFS) technology available in Intel processors is a known option for adapting the power consumption to the workload, but it is not optimized for network traffic processing workloads. We developed a novel control method for DVFS, based observing the ongoing traffic and online predictions using machine learning. Our results show that we can save up to 27% compared to commodity DVFS, even when including the computational overhead of machine learning. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/CLOUD.2018.00097
VL  - 2018-July
SP  - 710
EP  - 717
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057462359&doi=10.1109%2fCLOUD.2018.00097&partnerID=40&md5=6df104dfaaf267738458d631b4c32ee0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 6</p>
KW  - Artificial intelligence
KW  - Cloud computing
KW  - Cloud platforms
KW  - Computational overheads
KW  - Control methods
KW  - Dynamic frequency scaling
KW  - Dynamic voltage and frequency scaling
KW  - Dynamic Voltage and Frequency Scaling
KW  - Dynamic voltage frequency scaling
KW  - Energy efficiency
KW  - Intel processors
KW  - Learning systems
KW  - Machine learning
KW  - Network function virtualization
KW  - Network traffic
KW  - NFV
KW  - Online prediction
KW  - Voltage scaling
ER  - 

TY  - JOUR
AU  - DiGiglio, J.
AU  - Hunt, D.
AU  - Lim, A.
AU  - MacNamara, C.
AU  - Miskell, T.
T2  - Intel speed select technology-Base frequency-Enhancing performance
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132031546&partnerID=40&md5=353d036f063cde5b587d54003283e858
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
T2  - Power Management
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109580175&partnerID=40&md5=a2b355f1202d5102191bc3fc05ccd80f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3</p>
ER  - 

TY  - CHAP
TI  - Congestion control in data transmission networks: Historical perspective
AU  - Ignaciuk, P.
AU  - Bartoszewicz, A.
T2  - Communications and Control Engineering
AB  - The congestion occurs when the traffic generated by the network users exceeds the available bandwidth in the communication system. In such circumstances, not all the packets sent by the sources can be immediately relayed on the route towards their destination. Instead, they accumulate in the buffers at the intermediate nodes and wait for the bandwidth increase. If the incoming rate is not reduced (or stopped) before the queue of awaiting packets reaches its limit, typically defined by the amount of the reserved memory at the node, the new data pieces must be discarded. The lost fragments are retransmitted, which further deepens the congestion at the bottleneck point. At certain stage, the network becomes clogged with retransmissions and stops providing its services – this state is referred to as a deadlock or congestion collapse. In fact, the early communication networks frequently suffered from congestion collapse, until the development of the Jacobson’s scheme [73] for the Internet flow control. © Springer-Verlag London 2013.
DA  - 2013///
PY  - 2013
IS  - 9781447141464
SP  - 9
EP  - 44
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904723765&doi=10.1007%2f978-1-4471-4147-1_2&partnerID=40&md5=b496d88967f13ee48878c2e7d67e0eec
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3</p>
KW  - Available bandwidth
KW  - Bandwidth
KW  - Bide
KW  - Congestion collapse
KW  - Data communication systems
KW  - Expense
KW  - Historical perspective
KW  - Intermediate node
KW  - Nash
KW  - Settling
KW  - Traffic congestion
ER  - 

TY  - JOUR
TI  - Survey of Performance Acceleration Techniques for Network Function Virtualization
AU  - Linguaglossa, L.
AU  - Lange, S.
AU  - Pontarelli, S.
AU  - Retvari, G.
AU  - Rossi, D.
AU  - Zinner, T.
AU  - Bifulco, R.
AU  - Jarschel, M.
AU  - Bianchi, G.
T2  - Proceedings of the IEEE
AB  - The ongoing network softwarization trend holds the promise to revolutionize network infrastructures by making them more flexible, reconfigurable, portable, and more adaptive than ever. Still, the migration from hard-coded/hard-wired network functions toward their software-programmable counterparts comes along with the need for tailored optimizations and acceleration techniques so as to avoid or at least mitigate the throughput/latency performance degradation with respect to fixed function network elements. The contribution of this paper is twofold. First, we provide a comprehensive overview of the host-based network function virtualization (NFV) ecosystem, covering a broad range of techniques, from low-level hardware acceleration and bump-in-the-wire offloading approaches to high-level software acceleration solutions, including the virtualization technique itself. Second, we derive guidelines regarding the design, development, and operation of NFV-based deployments that meet the flexibility and scalability requirements of modern communication networks. © 1963-2012 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/JPROC.2019.2896848
VL  - 107
IS  - 4
SP  - 746
EP  - 764
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062998800&doi=10.1109%2fJPROC.2019.2896848&partnerID=40&md5=574fa440541d99475a04f004f0b02f8e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 104</p>
KW  - Acceleration
KW  - Acceleration technique
KW  - Fast packet processing
KW  - Network function virtualization
KW  - network function virtualization (NFV)
KW  - Network infrastructure
KW  - offloading
KW  - Packet processing
KW  - performance acceleration
KW  - Performance acceleration
KW  - Performance degradation
KW  - Software acceleration
KW  - Transfer functions
KW  - Virtual reality
KW  - virtualization
KW  - Virtualization
KW  - Virtualization Techniques
ER  - 

TY  - JOUR
TI  - Stock trend prediction using regression analysis-A data mining approach
AU  - Olaniyi, S.A.S.
AU  - Adewole, K.S.
AU  - Jimoh, R.G.
T2  - ARPN Journal of Systems and Software
DA  - 2011///
PY  - 2011
VL  - 1
IS  - 4
SP  - 154
EP  - 157
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894277180&partnerID=40&md5=3ea2f3e00d6581ef49700fb2227626e0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 39</p>
ER  - 

TY  - JOUR
TI  - The macroscopic behavior of the TCP congestion avoidance algorithm
AU  - Mathis, M.
AU  - Semke, J.
AU  - Mahdavi, J.
AU  - Ott, T.
T2  - Computer Communication Review
AB  - In this paper, we analyze a performance model for the TCP Congestion Avoidance algorithm. The model predicts the bandwidth of a sustained TCP connection subjected to light to moderate packet losses, such as loss caused by network congestion. It assumes that TCP avoids retransmission timeouts and always has sufficient receiver window and sender data. The model predicts the Congestion Avoidance performance of nearly all TCP implementations under restricted conditions and of TCP with Selective Acknowledgements over a much wider range of Internet conditions. We verify the model through both simulation and live Internet measurements. The simulations test several TCP implementations under a range of loss conditions and in environments with both drop-tail and RED queuing. The model is also compared to live Internet measurements using the TReno diagnostic and real TCP implementations. We also present several applications of the model to problems of bandwidth allocation in the Internet. We use the model to analyze networks with multiple congested gateways; this analysis shows strong agreement with prior work in this area. Finally, we present several important implications about the behavior of the Internet in the presence of high load from diverse user communities.
DA  - 1997///
PY  - 1997
DO  - 10.1145/263932.264023
VL  - 27
IS  - 3
SP  - 67
EP  - 82
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031175629&doi=10.1145%2f263932.264023&partnerID=40&md5=7b1e50888e37117a9d88b0459bdf55ff
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1103</p>
KW  - Algorithms
KW  - Bandwidth
KW  - Computer simulation
KW  - Congestion control (communication)
KW  - Internet
KW  - Mathematical models
KW  - Network protocols
KW  - Performance
KW  - Telecommunication systems
ER  - 

TY  - JOUR
TI  - Stress-ng
AU  - King, C.I.
T2  - Stress-ng
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941333121&partnerID=40&md5=c8b18e04b1c518cbeab5593003cf6529
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 91</p>
ER  - 

TY  - JOUR
T2  - XDP-Tools Utilities and Example Programs for Use With XDP
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132036047&partnerID=40&md5=12f668dfef36513483be8955e6dfa295
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - Measuring energy and power with PAPI
AU  - Weaver, V.M.
AU  - Johnson, M.
AU  - Kasichayanula, K.
AU  - Ralph, J.
AU  - Luszczek, P.
AU  - Terpstra, D.
AU  - Moore, S.
T2  - Proceedings of the International Conference on Parallel Processing Workshops
AB  - Energy and power consumption are becoming critical metrics in the design and usage of high performance systems. We have extended the Performance API (PAPI) analysis library to measure and report energy and power values. These values are reported using the existing PAPI API, allowing code previously instrumented for performance counters to also measure power and energy. Higher level tools that build on PAPI will automatically gain support for power and energy readings when used with the newest version of PAPI. We describe in detail the types of energy and power readings available through PAPI. We support external power meters, as well as values provided internally by recent CPUs and GPUs. Measurements are provided directly to the instrumented process, allowing immediate code analysis in real time. We provide examples showing results that can be obtained with our infrastructure. © 2012 IEEE.
DA  - 2012///
PY  - 2012
DO  - 10.1109/ICPPW.2012.39
SP  - 262
EP  - 268
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871142592&doi=10.1109%2fICPPW.2012.39&partnerID=40&md5=93d5b655b96279eac7fd539d711b2468
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 159</p>
KW  - Code analysis
KW  - Critical metrics
KW  - Electric power measurement
KW  - energy measurement
KW  - Energy readings
KW  - High performance systems
KW  - performance analysis
KW  - Performance analysis
KW  - Performance counters
KW  - power measurement
KW  - Power meters
KW  - Program processors
KW  - Real time
KW  - Technical presentations
ER  - 

TY  - JOUR
TI  - IMIX Genome Specification of variable packet sizes for additional testing
AU  - Morton, A.
T2  - IETF, Internet-Draft draft-morton-bmwg-imixgenome- 02
DA  - 2013///
PY  - 2013
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132011240&partnerID=40&md5=5de571abfba26763b2ab756c0ed0fd8c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - TCP congestion control
AU  - Allman, M.
AU  - Paxson, V.
AU  - Stevens, W.
T2  - TCP Congestion Control
DA  - 1999///
PY  - 1999
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003599162&partnerID=40&md5=3f10ecb9c3fa5e8ed5c28fd1b9a96e85
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1247</p>
ER  - 

TY  - JOUR
T2  - TRex Realistic Traffic Generator
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132027355&partnerID=40&md5=51e399ffbaaf2b01fed265c5e3a25c3e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
T2  - Turbostat-Report Processor Frequency and Idle Statistics
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132027542&partnerID=40&md5=9da5f0466389c5ef8282e98492591da5
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - An Open-Source Benchmark Suite for Microservices and Their Hardware-Software Implications for Cloud & Edge Systems
AU  - Gan, Y.
AU  - Zhang, Y.
AU  - Cheng, D.
AU  - Shetty, A.
AU  - Rathi, P.
AU  - Katarki, N.
AU  - Bruno, A.
AU  - Hu, J.
AU  - Ritchken, B.
AU  - Jackson, B.
AU  - Hu, K.
AU  - Pancholi, M.
AU  - He, Y.
AU  - Clancy, B.
AU  - Colen, C.
AU  - Wen, F.
AU  - Leung, C.
AU  - Wang, S.
AU  - Zaruvinsky, L.
AU  - Espinosa, M.
AU  - Lin, R.
AU  - Liu, Z.
AU  - Padilla, J.
AU  - Delimitrou, C.
T2  - International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS
AB  - Cloud services have recently started undergoing a major shift from monolithic applications, to graphs of hundreds of loosely-coupled microservices. Microservices fundamentally change a lot of assumptions current cloud systems are designed with, and present both opportunities and challenges when optimizing for quality of service (QoS) and utilization. In this paper we explore the implications microservices have across the cloud system stack. We first present Death- StarBench, a novel, open-source benchmark suite built with microservices that is representative of large end-to-end services, modular and extensible. DeathStarBench includes a social network, a media service, an e-commerce site, a banking system, and IoT applications for coordination control of UAV swarms. We then use DeathStarBench to study the architectural characteristics of microservices, their implications in networking and operating systems, their challenges with respect to cluster management, and their trade-offs in terms of application design and programming frameworks. Finally, we explore the tail at scale effects of microservices in real deployments with hundreds of users, and highlight the increased pressure they put on performance predictability. © 2019 Association for Computing Machinery.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3297858.3304013
SP  - 3
EP  - 18
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064688619&doi=10.1145%2f3297858.3304013&partnerID=40&md5=b9f662fba2a35d83eb40d32009cf7676
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 538</p>
KW  - acceleration
KW  - Benchmark suites
KW  - cloud computing
KW  - Cloud systems
KW  - Cloud-computing
KW  - Cluster computing
KW  - cluster management
KW  - Cluster management
KW  - Datacenter
KW  - datacenters
KW  - Economic and social effects
KW  - Field programmable gate arrays (FPGA)
KW  - fpga
KW  - Fpgum
KW  - Microservice
KW  - microservices
KW  - Open source software
KW  - Open systems
KW  - Open-source
KW  - QoS
KW  - Quality of service
KW  - Quality-of-service
KW  - serverless
KW  - Serverless
ER  - 

TY  - JOUR
TI  - Enriching computing simulators by generating realistic serverless traces
AU  - Sallo, D.H.
AU  - Kecskemeti, G.
T2  - Journal of Cloud Computing
AB  - Serverless computing is stepping forward to provide a cloud environment that mainly focuses on managing infrastructure, resources and configurations on the behalf of a user. Research in this field can’t rely on commercial providers such as AWS and Azure, as their inflexibility and cost often limits the required levels of reproducibility and scalability. Therefore, simulators have been opted as an alternative solution by the research community. They offer a reduced-cost and easy-setup environment. To get respectable precision, simulators use real traces collected and offered by commercial providers. These traces represent comprehensive information of executed tasks that reflect user behaviour. Due to serverless computing’s recency, typical workload traces employed by IaaS simulators are not well adoptable to the new computing model. In this paper, we propose an approach for generating realistic serverless traces. We enhance our previous generator approach that was based on the Azure Functions dataset. Our new, genetic algorithm based approach improves the statistical properties of the generated traces. We also enabled arbitrary scaling of the workload, while maintaining real users’ behaviour. These advances further support reproducibility in the serverless research community. We validated the results of our generator approach using the coefficient of determination (R2), which shows that our generated workload closely matches the original dataset’s characteristics in terms of execution time, memory utilisation as well as user participation percentage. To demonstrate the benefits of the reusability of the generated traces, we applied them with a diverse set of simulators and shown that they offer reproducible results independently of the simulator used. © 2023, The Author(s).
DA  - 2023///
PY  - 2023
DO  - 10.1186/s13677-023-00397-8
VL  - 12
IS  - 1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150208452&doi=10.1186%2fs13677-023-00397-8&partnerID=40&md5=68c93581a3135014027d6b9fa9d8b39f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3</p>
KW  - Alternative solutions
KW  - Behavioral research
KW  - Cloud environments
KW  - Faas
KW  - FaaS
KW  - Genetic Algorithm
KW  - Genetic algorithms
KW  - Infrastructure resources
KW  - Reduced cost
KW  - Reproducibilities
KW  - Research communities
KW  - Reusability
KW  - Serverless trace
KW  - Serverless workload
KW  - User behaviors
ER  - 

TY  - CONF
TI  - iFogSim: A toolkit for modeling and simulation of resource management techniques in the Internet of Things, Edge and Fog computing environments
AU  - Gupta, H.
AU  - Vahid Dastjerdi, A.
AU  - Ghosh, S.K.
AU  - Buyya, R.
T2  - Software - Practice and Experience
AB  - Internet of Things (IoT) aims to bring every object (eg, smart cameras, wearable, environmental sensors, home appliances, and vehicles) online, hence generating massive volume of data that can overwhelm storage systems and data analytics applications. Cloud computing offers services at the infrastructure level that can scale to IoT storage and processing requirements. However, there are applications such as health monitoring and emergency response that require low latency, and delay that is caused by transferring data to the cloud and then back to the application can seriously impact their performances. To overcome this limitation, Fog computing paradigm has been proposed, where cloud services are extended to the edge of the network to decrease the latency and network congestion. To realize the full potential of Fog and IoT paradigms for real-time analytics, several challenges need to be addressed. The first and most critical problem is designing resource management techniques that determine which modules of analytics applications are pushed to each edge device to minimize the latency and maximize the throughput. To this end, we need an evaluation platform that enables the quantification of performance of resource management policies on an IoT or Fog computing infrastructure in a repeatable manner. In this paper we propose a simulator, called iFogSim, to model IoT and Fog environments and measure the impact of resource management techniques in latency, network congestion, energy consumption, and cost. We describe two case studies to demonstrate modeling of an IoT environment and comparison of resource management policies. Moreover, scalability of the simulation toolkit of RAM consumption and execution time is verified under different circumstances. Copyright © 2017 John Wiley & Sons, Ltd.
DA  - 2017///
PY  - 2017
DO  - 10.1002/spe.2509
VL  - 47
SP  - 1275
EP  - 1296
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021248358&doi=10.1002%2fspe.2509&partnerID=40&md5=f5351d8461158bc62bf3f14acd916f12
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1313</p>
KW  - Computing environments
KW  - Computing infrastructures
KW  - Digital storage
KW  - Distributed computer systems
KW  - Domestic appliances
KW  - Edge computing
KW  - Energy utilization
KW  - Environmental management
KW  - Evaluation platforms
KW  - Fog
KW  - Fog computing
KW  - Industrial management
KW  - Internet of things
KW  - Internet of Things (IoT)
KW  - Internet of Things (IOT)
KW  - Model and simulation
KW  - modeling and simulation
KW  - Natural resources management
KW  - Online systems
KW  - Random access storage
KW  - Resource allocation
KW  - Resource management policy
KW  - Resource management techniques
ER  - 

TY  - JOUR
T2  - Google Data Center PUE performance
DA  - 2023///
PY  - 2023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209222184&partnerID=40&md5=b83f8cd2c9a3e2139d8459c113315a68
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2</p>
ER  - 

TY  - JOUR
TI  - Usage impact on data center electricity needs: A system dynamic forecasting model
AU  - Koot, M.
AU  - Wijnhoven, F.
T2  - Applied Energy
AB  - This article presents a forecasting model of data center electricity needs based on understanding usage growth and we conclude that this growth is not fully compensated by efficiency gains of data center technological innovations. We predict a combined growth of data center electricity needs of 286 TWh in 2016 until about 321 TWh in 2030, if all currently known growth factors remain the same. We next run simulations for the end of Moore's law and the growth of industrial Internet of Things (IoT). The end of Moore's law results in about 658 TWh for 2030 and an increase of the share of global data center electricity consumption from about 1.15% in 2016 to 1.86% in 2030. A rise of the Industrial IoT may result into total energy consumption of about 364 TWh (about 1.03%) in 2030. Moore's law and IoT combined cause data center energy needs going up to 752 TWh in 2030, and about 2.13% of global electricity available. Our sensitivity analysis reveals that the future impact of the data centers’ electricity consumption is vulnerable to behavioral usage trends, since the 95% confidence interval of [343, 1031] TWh is relatively wide. Our forecasts, however, exclude the energy needs of mobile devices, edge and fog computing. We offer a system dynamic model and simulation input data selected from the existing literature for replicating this study and applying alternative parameters to it. We further suggest multiple research directions on usage impact on data center energy consumption. © 2021 The Authors
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.apenergy.2021.116798
VL  - 291
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103102639&doi=10.1016%2fj.apenergy.2021.116798&partnerID=40&md5=e0af53444a66b5513927b52834a297ac
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 254</p>
KW  - confidence interval
KW  - Data center electricity need
KW  - Data center electricity needs
KW  - Data centers
KW  - Dynamic forecasting
KW  - Electric power utilization
KW  - Electricity-consumption
KW  - energy efficiency
KW  - Energy needs
KW  - Energy-consumption
KW  - Fog computing
KW  - Forecasting
KW  - forecasting method
KW  - Forecasting modeling
KW  - fuel consumption
KW  - Green computing
KW  - innovation
KW  - Internet
KW  - Internet of things
KW  - Internet of Things
KW  - Moore Law
KW  - Moore's law
KW  - parameterization
KW  - research work
KW  - sensitivity analysis
KW  - Sensitivity analysis
KW  - System dynamic forecasting
KW  - System Dynamics
KW  - vulnerability
ER  - 

TY  - CHAP
TI  - OMNeT++
AU  - Varga, A.
T2  - Modeling and Tools for Network Simulation
AB  - OMNeT++ (www.omnetpp.org) is an extensible, modular, component-based C++ simulation library and framework which also includes an integrated development and a graphical runtime environment. Domain-specific functionality (support for simulation of communication networks, queuing networks, performance evaluation, etc.) is provided by model frameworks, developed as independent projects. There are extensions for real-time simulation, network emulation, support for alternative programming languages (Java, C#), database integration, SystemC integration, HLA and several other functions. © 2010 Springer-Verlag Berlin Heidelberg.
DA  - 2010///
PY  - 2010
SP  - 35
EP  - 59
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960763273&doi=10.1007%2f978-3-642-12331-3_3&partnerID=40&md5=8bcb612c0aeff5cfc5b6583fe18ce30d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 416</p>
ER  - 

TY  - CHAP
TI  - The ns-3 network simulator
AU  - Riley, G.F.
AU  - Henderson, T.R.
T2  - Modeling and Tools for Network Simulation
AB  - As networks of computing devices grow larger and more complex, the need for highly accurate and scalable network simulation technologies becomes critical. Despite the emergence of large-scale testbeds for network research, simulation still plays a vital role in terms of scalability (both in size and in experimental speed), reproducibility, rapid prototyping, and education. With simulation based studies, the approach can be studied in detail at varying scales, with varying data applications, varying field conditions, and will result in reproducible and analyzable results. © 2010 Springer-Verlag Berlin Heidelberg.
DA  - 2010///
PY  - 2010
SP  - 15
EP  - 34
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876045426&doi=10.1007%2f978-3-642-12331-3_2&partnerID=40&md5=6a1bc8cf49e3f049dbe1337559ff44cf
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1131</p>
ER  - 

TY  - JOUR
TI  - DISSECT-CF: A simulator to foster energy-aware scheduling in infrastructure clouds
AU  - Kecskemeti, G.
T2  - Simulation Modelling Practice and Theory
AB  - Infrastructure as a Service (IaaS) systems offer on demand virtual infrastructures so reliably and flexibly that users expect a high service level. Therefore, even with regards to internal IaaS behaviour, production clouds only adopt novel ideas that are proven not to hinder established service levels. To analyse their expected behaviour, new ideas are often evaluated with simulators in production IaaS system-like scenarios. For instance, new research could enable collaboration amongst several layers of schedulers or could consider new optimisation objectives such as energy consumption. Unfortunately, current cloud simulators are hard to employ and they often have performance issues when several layers of schedulers interact in them. To target these issues, a new IaaS simulation framework (called DISSECT-CF) was designed. The new simulator's foundation has the following goals: easy extensibility, support energy evaluation of IaaSs and to enable fast evaluation of many scheduling and IaaS internal behaviour related scenarios. In response to the requirements of such scenarios, the new simulator introduces concepts such as: a unified model for resource sharing and a new energy metering framework with hierarchical and indirect metering options. Then, the comparison of several simulated situations to real-life IaaS behaviour is used to validate the simulator's functionality. Finally, a performance comparison is presented between DISSECT-CF and some currently available simulators. © 2015 Elsevier B.V.
DA  - 2015///
PY  - 2015
DO  - 10.1016/j.simpat.2015.05.009
VL  - 58
SP  - 188
EP  - 218
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947023292&doi=10.1016%2fj.simpat.2015.05.009&partnerID=40&md5=0bfb3658bac3c2137f2f246f7914374f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 86</p>
KW  - Cloud computing
KW  - Energy awareness
KW  - Energy utilization
KW  - Energy-aware scheduling
KW  - Energy-awareness
KW  - Infrastructure as a Service
KW  - Infrastructure as a service (IaaS)
KW  - Performance comparison
KW  - Performance issues
KW  - Power management
KW  - Resource management
KW  - Scheduling
KW  - Simulation
KW  - Simulation framework
KW  - Simulators
KW  - Virtual infrastructures
ER  - 

TY  - JOUR
TI  - iFogSim2: An extended iFogSim simulator for mobility, clustering, and microservice management in edge and fog computing environments
AU  - Mahmud, R.
AU  - Pallewatta, S.
AU  - Goudarzi, M.
AU  - Buyya, R.
T2  - Journal of Systems and Software
AB  - Internet of Things (IoT) has already proven to be the building block for next-generation Cyber–Physical Systems (CPSs). The considerable amount of data generated by the IoT devices needs latency-sensitive processing, which is not feasible by deploying the respective applications in remote Cloud datacentres. Edge/Fog computing, a promising extension of Cloud at the IoT-proximate network, can meet such requirements for smart CPSs. However, the structural and operational differences of Edge/Fog infrastructure resist employing Cloud-based service regulations directly to these environments. As a result, many research works have been recently conducted, focusing on efficient application and resource management in Edge/Fog computing environments. Scalable Edge/Fog infrastructure is a must to validate these policies, which is also challenging to accommodate in the real-world due to high cost and implementation time. Considering simulation as a key to this constraint, various software have been developed that can imitate the physical behavior of Edge/Fog computing environments. Nevertheless, the existing simulators often fail to support advanced service management features because of their monolithic architecture, lack of actual dataset, and limited scope for a periodic update. To overcome these issues, we have developed modular simulation models for service migration, dynamic distributed cluster formation, and microservice orchestration for Edge/Fog computing based on real datasets and extended the basic components of iFogSim, a widely used Edge/Fog computing simulator for their ease of adoption as iFogSim2. The performance of iFogSim2 and its built-in service management policies are evaluated using three use case scenarios and compared with the contemporary simulators and benchmark policies under different settings. Results indicate that our simulator consumes less memory and minimizes simulation time by an average of 28% when compared to other simulators. © 2022 Elsevier Inc.
DA  - 2022///
PY  - 2022
DO  - 10.1016/j.jss.2022.111351
VL  - 190
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130094579&doi=10.1016%2fj.jss.2022.111351&partnerID=40&md5=b650072584d3ec15cdfca92a7638dd74
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 167</p>
KW  - Benchmarking
KW  - Building blockes
KW  - Cluster computing
KW  - Clustering
KW  - Clusterings
KW  - Computer software
KW  - Computing environments
KW  - Cybe-physical systems
KW  - Cyber-physical systems
KW  - Datacenter
KW  - Edge/fog computing
KW  - Edge/Fog computing
KW  - Environmental management
KW  - Fog computing
KW  - Internet of things
KW  - Internet of Things
KW  - Microservice
KW  - Microservices
KW  - Mobility
KW  - Simulation
KW  - Simulators
ER  - 

TY  - JOUR
TI  - Accuracy study and improvement of network simulation in the SimGrid framework
AU  - Velho, P.
AU  - Legrand, A.
T2  - Int. Conf. on Simulation Tools and Techniques
DA  - 2010///
PY  - 2010
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209228922&partnerID=40&md5=77739bc8b4beb622e0a961c6803fec92
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - Adding Virtualization Capabilities to the Grid'5000 Testbed
AU  - Balouek, D.
AU  - Amarie, A.C.
AU  - Charrier, G.
AU  - Desprez, F.
AU  - Jeannot, E.
AU  - Jeanvoine, E.
AU  - Lèbre, A.
AU  - Margery, D.
AU  - Niclausse, N.
AU  - Nussbaum, L.
AU  - Richard, O.
AU  - Perez, C.
AU  - Quesnel, F.
AU  - Rohr, C.
AU  - Sarzyniec, L.
T2  - Communications in Computer and Information Science
AB  - Almost ten years after its premises, the Grid'5000 testbed has become one of the most complete testbed for designing or evaluating large-scale distributed systems. Initially dedicated to the study of High Performance Computing, the infrastructure has evolved to address wider concerns related to Desktop Computing, the Internet of Services and more recently the Cloud Computing paradigm. This paper present recent improvements of the Grid'5000 software and services stack to support large-scale experiments using virtualization technologies as building blocks. Such contributions include the deployment of customized software environments, the reservation of dedicated network domain and the possibility to isolate them from the others, and the automation of experiments with a REST API. We illustrate the interest of these contributions by describing three different use-cases of large-scale experiments on the Grid'5000 testbed. The first one leverages virtual machines to conduct larger experiments spread over 4000 peers. The second one describes the deployment of 10000 KVM instances over 4 Grid'5000 sites. Finally, the last use case introduces a one-click deployment tool to easily deploy major IaaS solutions. The conclusion highlights some important challenges of Grid'5000 related to the use of OpenFlow and to the management of applications dealing with tremendous amount of data. © Springer International Publishing Switzerland 2013.
DA  - 2013///
PY  - 2013
DO  - 10.1007/978-3-319-04519-1_1
VL  - 367 CCIS
SP  - 3
EP  - 20
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904646472&doi=10.1007%2f978-3-319-04519-1_1&partnerID=40&md5=27f9d0816df9aea694bb5a6b7f629bd3
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 247</p>
KW  - Cloud computing
KW  - Cloud Computing
KW  - Customized software
KW  - Distributed systems
KW  - Distributed Systems
KW  - Experiments
KW  - High performance computing
KW  - Internet of Services
KW  - Large scale experiments
KW  - Large-scale distributed system
KW  - Large-Scale Testbed
KW  - Testbeds
KW  - Virtual reality
KW  - Virtualization
KW  - Virtualization technologies
KW  - Virtualizations
ER  - 

TY  - JOUR
TI  - United States data center energy usage report
AU  - Shehabi, A.
T2  - United States Data Center Energy Usage Report
DA  - 2016///
PY  - 2016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010198598&partnerID=40&md5=ee3c20b0bf14d795f7df360fb342d3ad
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 894</p>
ER  - 

TY  - JOUR
T2  - Electricite : combien consomment les appareils de la maison ?, report
DA  - 2022///
PY  - 2022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209213020&partnerID=40&md5=d5830d557b580912ce614f0ab6cfdd2d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
T2  - Bilan electrique 2022
DA  - 2023///
PY  - 2023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209206938&partnerID=40&md5=3f923a68cba38e9daab641bdf3f544ec
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - How much carbon dioxide is produced per kilowatthour of U.S. electricity generation?
T2  - How much carbon dioxide is produced per kilowatthour of U.S. electricity generation?
DA  - 2023///
PY  - 2023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181673550&partnerID=40&md5=160ac3475f2e11d328477f8f71d09286
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 14</p>
ER  - 

TY  - CONF
TI  - Predicting the Energy-Consumption of MPI Applications at Scale Using only a Single Node
AU  - Heinrich, F.C.
AU  - Cornebize, T.
AU  - Degomme, A.
AU  - Legrand, A.
AU  - Carpen-Amarie, A.
AU  - Hunold, S.
AU  - Orgerie, A.-C.
AU  - Quinson, M.
T2  - Proceedings - IEEE International Conference on Cluster Computing, ICCC
AB  - Monitoring and assessing the energy efficiency of supercomputers and data centers is crucial in order to limit and reduce their energy consumption. Applications from the domain of High Performance Computing (HPC), such as MPI applications, account for a significant fraction of the overall energy consumed by HPC centers. Simulation is a popular approach for studying the behavior of these applications in a variety of scenarios, and it is therefore advantageous to be able to study their energy consumption in a cost-efficient, controllable, and also reproducible simulation environment. Alas, simulators supporting HPC applications commonly lack the capability of predicting the energy consumption, particularly when target platforms consist of multi-core nodes. In this work, we aim to accurately predict the energy consumption of MPI applications via simulation. Firstly, we introduce the models required for meaningful simulations: The computation model, the communication model, and the energy model of the target platform. Secondly, we demonstrate that by carefully calibrating these models on a single node, the predicted energy consumption of HPC applications at a larger scale is very close (within a few percents) to real experiments. We further show how to integrate such models into the SimGrid simulation toolkit. In order to obtain good execution time predictions on multi-core architectures, we also establish that it is vital to correctly account for memory effects in simulation. The proposed simulator is validated through an extensive set of experiments with wellknown HPC benchmarks. Lastly, we show the simulator can be used to study applications at scale, which allows researchers to save both time and resources compared to real experiments. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/CLUSTER.2017.66
VL  - 2017-September
SP  - 92
EP  - 102
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032620594&doi=10.1109%2fCLUSTER.2017.66&partnerID=40&md5=03eb8e05d82c1e29fe374761dbbfc782
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 33</p>
KW  - Cluster computing
KW  - Communication modeling
KW  - Computer architecture
KW  - Energy
KW  - Energy efficiency
KW  - Energy utilization
KW  - Execution time predictions
KW  - Forecasting
KW  - Green computing
KW  - High performance computing (HPC)
KW  - HPC
KW  - Message passing
KW  - MPI
KW  - Multicore architectures
KW  - Platform modeling
KW  - Platform models
KW  - Simulation
KW  - Simulation environment
KW  - Simulators
KW  - Supercomputers
ER  - 

TY  - CONF
TI  - CloudSim Plus: A cloud computing simulation framework pursuing software engineering principles for improved modularity, extensibility and correctness
AU  - Filho, M.C.S.
AU  - Oliveira, R.L.
AU  - Monteiro, C.C.
AU  - Inácio, P.R.M.
AU  - Freire, M.M.
T2  - Proceedings of the IM 2017 - 2017 IFIP/IEEE International Symposium on Integrated Network and Service Management
AB  - Cloud computing is an established technology to provide computing resources on demand that currently faces several challenges. Main challenges include management of shared resources, energy consumption, load balancing, resource provisioning and allocation, and fulfilment of service level agreements (SLAs). Due to its inherent complexity, cloud simulation is largely used to experiment new models and algorithms. This work presents CloudSim Plus, an open source simulation framework that pursues conformance to software engineering principles and object-oriented design in order to provide an extensible, modular and accurate tool. Based on the CloudSim framework, it aims to improve several engineering aspects, such as maintainability, reusability and extensibility. This work shows the benefits of CloudSim Plus, its particular features, how it ensures more accuracy, extension facility and usage simplicity. © 2017 IFIP.
DA  - 2017///
PY  - 2017
DO  - 10.23919/INM.2017.7987304
SP  - 400
EP  - 406
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029414646&doi=10.23919%2fINM.2017.7987304&partnerID=40&md5=bd419e1cfb51a75d065dde01882ce218
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 184</p>
KW  - Cloud computing
KW  - Computer software
KW  - Computing resource
KW  - Energy utilization
KW  - Engineering aspects
KW  - Inherent complexity
KW  - Models and algorithms
KW  - Network function virtualization
KW  - Object oriented design
KW  - Object oriented programming
KW  - Open source software
KW  - Open systems
KW  - Reusability
KW  - Service level agreement (SLAs)
KW  - Simulation framework
KW  - Software engineering
KW  - Software engineering principles
ER  - 

TY  - JOUR
TI  - Automated power modeling of computing devices: Implementation and use case for Raspberry Pis
AU  - Kanso, H.
AU  - Noureddine, A.
AU  - Exposito, E.
T2  - Sustainable Computing: Informatics and Systems
AB  - Monitoring the power consumption of smart and connected devices is a challenging task with heterogeneous devices and a variety of hardware and software configurations. In order to accurately monitor these devices and follow the speedy changes in their configuration, a new approach is needed. In this paper, we present an automated architecture and approach to empirically generate power models for a large set of devices. Our approach allows conducting automated benchmarks to collect power data and metrics, generating or updating accurate power models, and allowing software tools to query and retrieve the most accurate and up-to-date power model of a specific device configuration. We also present a proof-of-concept implementation for modeling the power consumption of Raspberry Pi devices. Finally, we conduct a comprehensive experiment, modeling the entire current lineup of Raspberry Pi devices with error margins as low as 0.3%, and then we discuss the impact of multiple device configurations on power consumption. © 2022 Elsevier Inc.
DA  - 2023///
PY  - 2023
DO  - 10.1016/j.suscom.2022.100837
VL  - 37
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145646703&doi=10.1016%2fj.suscom.2022.100837&partnerID=40&md5=55ca46d0737b822c65d90f26278c2792
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 10</p>
KW  - Automated software architecture
KW  - Automation
KW  - Computing devices
KW  - Computing power
KW  - Device configurations
KW  - Electric power utilization
KW  - Empirical experimentation
KW  - Hardware and software
KW  - Hardware configurations
KW  - Heterogeneous devices
KW  - Measurement
KW  - Performance
KW  - Power consumption
KW  - Power modeling
KW  - Software configuration
ER  - 

TY  - JOUR
TI  - IoTSim-Osmosis: A framework for modeling and simulating IoT applications over an edge-cloud continuum
AU  - Alwasel, K.
AU  - Jha, D.N.
AU  - Habeeb, F.
AU  - Demirbaga, U.
AU  - Rana, O.
AU  - Baker, T.
AU  - Dustdar, S.
AU  - Villari, M.
AU  - James, P.
AU  - Solaiman, E.
AU  - Ranjan, R.
T2  - Journal of Systems Architecture
AB  - The osmotic computing paradigm sets out the principles and algorithms for simplifying the deployment of Internet of Things (IoT) applications in integrated edge-cloud environments. Various existing simulation frameworks can be used to support integration of cloud and edge computing environments. However, none of these can directly support an osmotic computing environment due to the complexity of IoT applications and heterogeneity of integrated edge-cloud environments. Osmotic computing suggests the migration of workload to/from a cloud data center to edge devices, based on performance and security trigger events. We propose ‘IoTSim-Osmosis– a simulation framework to support the testing and validation of osmotic computing applications. In particular, our detailed related work analysis demonstrates that IoTSim-Osmosis is the first simulation framework to enable unified modeling and simulation of complex IoT applications over heterogeneous edge-cloud environments. IoTSim-Osmosis is demonstrated using an electricity management and billing application case study, for benchmarking various run-time QoS parameters, such as IoT battery use, execution time, network transmission time and consumed energy. © 2020 Elsevier B.V.
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.sysarc.2020.101956
VL  - 116
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097058270&doi=10.1016%2fj.sysarc.2020.101956&partnerID=40&md5=80b7425eee6f7a423f86b35b187435d6
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 59</p>
KW  - Benchmarking
KW  - Cloud computing
KW  - Cloud data centers
KW  - Complex networks
KW  - Computing applications
KW  - Computing environments
KW  - Computing paradigm
KW  - Edge computing
KW  - Internet of things
KW  - Internet of Things (IoT)
KW  - Internet of Things (IOT)
KW  - Modeling and simulating
KW  - Network transmission
KW  - Osmosis
KW  - Osmosis computing
KW  - Simulation framework
KW  - Simulation.
KW  - Software-Defined Network (SDN)
ER  - 

TY  - JOUR
T2  - IEEE 802.3 ae-2002 IEEE standard for information technology
DA  - 2002///
PY  - 2002
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209196011&partnerID=40&md5=15ffa366efc4f1bd657299185a253a42
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - An energy framework for the network simulator 3 (ns-3)
AU  - Wu, H.
AU  - Nabar, S.
AU  - Poovendran, R.
T2  - SIMUTools 2011 - 4th International ICST Conference on Simulation Tools and Techniques
AB  - The Network Simulator-3 (ns-3) is rapidly developing into a flexible and easy-to-use tool suitable for wireless network simulation. Since energy consumption is a key issue for wireless devices, wireless network researchers often need to investigate the energy consumption at a battery powered node or in the overall network, while running network simulations. This requires the underlying simulator to support energy consumption and energy source modeling. Currently however, ns-3 does not provide any support for modeling energy consumption or energy sources. In this paper, we introduce an integrated energy framework for ns-3, with models for energy source as well as energy consumption. We present the design and implementation of the overall framework and the specific models therein. Further, we show how the proposed framework can be used in ns-3 to simulate energy-aware protocols in a wireless network. Copyright © 2011 ICST.
DA  - 2011///
PY  - 2011
DO  - 10.4108/icst.simutools.2011.245584
SP  - 222
EP  - 230
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922764314&doi=10.4108%2ficst.simutools.2011.245584&partnerID=40&md5=1103577d31dcf3a8d9d847d0f39a0a9c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 92</p>
KW  - Battery management systems
KW  - Battery models
KW  - Design and implementations
KW  - Energy consumption
KW  - Energy utilization
KW  - Energy-aware protocol simulation
KW  - Energy-aware protocols
KW  - Network simulation
KW  - Network simulations
KW  - Network simulators
KW  - Ns-3
KW  - Overall networks
KW  - Power management (telecommunication)
KW  - Simulators
KW  - Wireless network simulation
KW  - Wireless networks
ER  - 

TY  - JOUR
AU  - Tirumala, A.
T2  - Iperf: The TCP/UDP Bandwidth Measurement Tool
DA  - 1999///
PY  - 1999
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169467871&partnerID=40&md5=6503187c98ca58285e8946d13ea10588
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 37</p>
ER  - 

TY  - CONF
TI  - A Large-Scale Wired Network Energy Model for Flow-Level Simulations
AU  - Guegan, L.
AU  - Amersho, B.L.
AU  - Orgerie, A.-C.
AU  - Quinson, M.
T2  - Advances in Intelligent Systems and Computing
AB  - The use of simulators to predict network energy consumption is a good way for scientists to improve and develop new algorithms and also to assess them. However, the average size of a network platforms is continuously increasing with the emergence of new technologies like the Internet Of Things and Fog Computing. Packet-level simulators start to reach their limits in terms of performance and this calls for newer solutions in the domain of large-scale platform energy models. In this paper, we propose two energy models for wired networks adapted to flow level simulators in order to estimate the energy consumption of large platforms. An evaluation of these models is proposed and it demonstrates their applicability in practice and their accuracy. Indeed, we obtain simulation results with a relative error lower than 4% compared to an ns-3-based solution, and our flow-based simulation is 120 times faster. © 2020, Springer Nature Switzerland AG.
DA  - 2020///
PY  - 2020
DO  - 10.1007/978-3-030-15032-7_88
VL  - 926
SP  - 1047
EP  - 1058
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063995808&doi=10.1007%2f978-3-030-15032-7_88&partnerID=40&md5=ecebd0379739e51297b52c2e0da22a46
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 5</p>
KW  - Energy utilization
KW  - Flow-level simulation
KW  - Flow-level simulators
KW  - Fog computing
KW  - Large scale platforms
KW  - Network energy consumption
KW  - Network platforms
KW  - Packet level
KW  - Relative errors
KW  - Simulators
KW  - Wired networks
ER  - 

TY  - JOUR
TI  - EnosLib: A Library for Experiment-Driven Research in Distributed Computing
AU  - Cherrueau, R.-A.
AU  - Delavergne, M.
AU  - van Kempen, A.
AU  - Lebre, A.
AU  - Pertin, D.
AU  - Balderrama, J.R.
AU  - Simonet, A.
AU  - Simonin, M.
T2  - IEEE Transactions on Parallel and Distributed Systems
AB  - Despite the importance of experiment-driven research in the distributed computing community, there has been little progress in helping researchers conduct their experiments. In most cases, they have to achieve tedious and time-consuming development and instrumentation activities to deal with the specifics of testbeds and the system under study. In order to relieve researchers of the burden of those efforts, we have developed ENOSLIB: a Python library that takes into account best experimentation practices and leverages modern toolkits on automatic deployment and configuration systems. ENOSLIB helps researchers not only in the process of developing their experimental artifacts, but also in running them over different infrastructures. To demonstrate the relevance of our library, we discuss three experimental engines built on top of ENOSLIB, and used to conduct empirical studies on complex software stacks between 2016 and 2019 (database systems, communication buses and OpenStack). By introducing ENOSLIB, our goal is to gather academic and industrial actors of our community around a library that aggregates everyday experiment-driven research operations. A library that has been already adopted by open-source projects and members of the scientific community thanks to its ease of use and extension. © 2021 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/TPDS.2021.3111159
VL  - 33
IS  - 6
SP  - 1464
EP  - 1477
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114742756&doi=10.1109%2fTPDS.2021.3111159&partnerID=40&md5=be6ea4f33bc372f075fb3c171ab0cf5a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 14</p>
KW  - Automatic configuration
KW  - Automatic deployments
KW  - Benchmark testing
KW  - Code
KW  - Computing community
KW  - Distributed computer systems
KW  - distributed computing experimentation library
KW  - Distributed computing experimentation library
KW  - Experiment-driven research
KW  - Industrial research
KW  - Job analysis
KW  - Libraries
KW  - Open source software
KW  - performance evaluation
KW  - Performances evaluation
KW  - Software
KW  - Software testing
KW  - Task analysis
ER  - 

TY  - CONF
TI  - Automated performance prediction of microservice applications using simulation
AU  - Courageux-Sudan, C.
AU  - Orgerie, A.-C.
AU  - Quinson, M.
T2  - Proceedings - IEEE Computer Society's Annual International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunications Systems, MASCOTS
AB  - Microservices transform monolithic applications into simple, scalable, and interacting services. It allows for faster development and fine-grained deployments. However, the cooperation of several services leads to intricate dependencies, hindering the detection of performance bottlenecks. Current microservice performance analysis methods require real deployments, a costly process both in time and resources, while performance prediction through simulation relies on models that are complex to develop and instantiate. In this paper, we propose a microservice performance analysis approach based on simulation. Our contribution first introduces a microservice performance model requiring few instantiation parameters. We then propose a methodology to automatically derive model instantiation values from a single execution trace. We evaluate this methodology on two benchmarks from the literature. Our approach accurately predicts the deployment performance of large-scale microservice applications in various configurations from a single execution trace. This provides valuable insights on the performance of an application prior to its deployment on real platform.  © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/MASCOTS53633.2021.9614260
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123177041&doi=10.1109%2fMASCOTS53633.2021.9614260&partnerID=40&md5=3de9b299e6d2200bff10b42d806e3af2
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 14</p>
KW  - Execution trace
KW  - Fine grained
KW  - Microservice
KW  - Model and simulation
KW  - Modeling and simulation
KW  - Monolithics
KW  - Performance
KW  - Performance bottlenecks
KW  - Performance evaluation
KW  - Performance prediction
KW  - Performances evaluation
KW  - Simple++
KW  - Web services
ER  - 

TY  - JOUR
TI  - Energy efficient fog-based healthcare monitoring infrastructure
AU  - Isa, I.S.B.M.
AU  - El-Gorashi, T.E.H.
AU  - Musa, M.O.I.
AU  - Elmirghani, J.M.H.
T2  - IEEE Access
AB  - Recent advances in mobile technologies and cloud computing services have inspired the development of cloud-based real-time health monitoring systems. However, the transfer of health-related data to the cloud contributes to the burden on the networking infrastructures, leading to high latency and increased power consumption. Fog computing is introduced to relieve this burden by bringing services to the users’ proximity. This study proposes a new fog computing architecture for health monitoring applications based on a Gigabit Passive Optical Network (GPON) access network. An Energy-Efficient Fog Computing (EEFC) model is developed using Mixed Integer Linear Programming (MILP) to optimize the number and location of fog devices at the network edge to process and analyze the health data for energy-efficient fog computing. The performance of the EEFC model at low data rates and high data rates health applications is studied. The outcome of the study reveals that a total energy saving of 36% and 52% are attained via processing and analysis the health data at the fog in comparison to conventional processing and analysis at the central cloud for low data rate application and high data rate application, respectively. We also developed a real-time heuristic; Energy Optimized Fog Computing (EOFC) heuristic, with energy consumption performance approaching the EEFC model. Furthermore, we examined the energy efficiency improvements under different scenarios of devices idle power consumption and traffic volume. © 2020 Lippincott Williams and Wilkins. All rights reserved.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ACCESS.2020.3033555
VL  - 8
SP  - 197828
EP  - 197852
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099259654&doi=10.1109%2fACCESS.2020.3033555&partnerID=40&md5=ae195e882e0fb8e42fc329dfeaf966c3
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 53</p>
KW  - Cloud computing
KW  - Cloud computing services
KW  - ECG signal
KW  - Electric power utilization
KW  - Energy consumption
KW  - Energy efficiency
KW  - Energy efficiency improvements
KW  - Energy utilization
KW  - Ethernet
KW  - Fog
KW  - Fog computing
KW  - Gigabit passive optical network
KW  - GPON
KW  - Green computing
KW  - Health
KW  - Health monitoring
KW  - High data rate applications
KW  - Integer programming
KW  - Internet of Things
KW  - Low-data-rate applications
KW  - Machine-to-machine (M2M)
KW  - Mixed-integer linear programming
KW  - Networking infrastructure
KW  - Passive optical networks
KW  - Real time systems
KW  - Real-time health monitoring
ER  - 

TY  - CONF
TI  - Estimating the end-to-end energy consumption of low-bandwidth IoT applications for WiFi devices
AU  - Guegan, L.
AU  - Orgerie, A.-C.
T2  - Proceedings of the International Conference on Cloud Computing Technology and Science, CloudCom
AB  - Information and Communication Technology takes a growing part in the worldwide energy consumption. One of the root causes of this increase lies in the multiplication of connected devices. Each object of the Internet-of-Things often does not consume much energy by itself. Yet, their number and the infrastructures they require to properly work have leverage. In this paper, we combine simulations and real measurements to study the energy impact of IoT devices. In particular, we analyze the energy consumption of Cloud and telecommunication infrastructures induced by the utilization of connected devices, and we propose an end-to-end energy consumption model for these devices. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/CloudCom.2019.00049
VL  - 2019-December
SP  - 287
EP  - 294
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079074638&doi=10.1109%2fCloudCom.2019.00049&partnerID=40&md5=cf6d488b221d750df919602f3e17f854
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 22</p>
KW  - Blockchain
KW  - Cloud computing
KW  - Clouds
KW  - End-to-end model
KW  - End-to-end models
KW  - Energy consumption
KW  - Energy consumption model
KW  - Energy impact
KW  - Energy utilization
KW  - Green computing
KW  - Information and Communication Technologies
KW  - Internet of things
KW  - IOT applications
KW  - IoT devices
KW  - Real measurements
KW  - Telecommunication infrastructures
KW  - Wireless local area networks (WLAN)
ER  - 

TY  - JOUR
TI  - End-to-end energy models for Edge Cloud-based IoT platforms: Application to data stream analysis in IoT
AU  - Li, Y.
AU  - Orgerie, A.-C.
AU  - Rodero, I.
AU  - Amersho, B.L.
AU  - Parashar, M.
AU  - Menaud, J.-M.
T2  - Future Generation Computer Systems
AB  - Internet of Things (IoT) is bringing an increasing number of connected devices that have a direct impact on the growth of data and energy-hungry services. These services are relying on Cloud infrastructures for storage and computing capabilities, transforming their architecture into more a distributed one based on edge facilities provided by Internet Service Providers (ISP). Yet, between the IoT device, communication network and Cloud infrastructure, it is unclear which part is the largest in terms of energy consumption. In this paper, we provide end-to-end energy models for Edge Cloud-based IoT platforms. These models are applied to a concrete scenario: data stream analysis produced by cameras embedded on vehicles. The validation combines measurements on real test-beds running the targeted application and simulations on well-known simulators for studying the scaling-up with an increasing number of IoT devices. Our results show that, for our scenario, the edge Cloud part embedding the computing resources consumes 3 times more than the IoT part comprising the IoT devices and the wireless access point. © 2017 Elsevier B.V.
DA  - 2018///
PY  - 2018
DO  - 10.1016/j.future.2017.12.048
VL  - 87
SP  - 667
EP  - 678
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039909917&doi=10.1016%2fj.future.2017.12.048&partnerID=40&md5=8d7d371ce4f2a169a5a837029454720a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 69</p>
KW  - Cloud infrastructures
KW  - Computing capability
KW  - Data stream
KW  - Data stream analysis
KW  - Digital storage
KW  - Edge Cloud computing
KW  - Edge clouds
KW  - End-to-end energy model
KW  - Energy efficiency
KW  - Energy model
KW  - Energy utilization
KW  - Energy-efficiency
KW  - Green computing
KW  - Internet of things
KW  - Internet of Things (IOT)
KW  - Internet service providers
KW  - Internet service providers (ISP)
KW  - IoT
KW  - Wireless access points
ER  - 

TY  - JOUR
TI  - Optimal Placement of Applications in the Fog Environment: A Systematic Literature Review
AU  - Islam, Mohammad Mainul
AU  - Ramezani, Fahimeh
AU  - Lu, Hai Yan
AU  - Naderpour, Mohsen
T2  - J. Parallel and Distrib. Comput
DA  - 2022///
PY  - 2022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150009781&partnerID=40&md5=e56bba9bccbb8c877c3942ff59b3d1f6
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3</p>
ER  - 

TY  - CONF
TI  - μqSim: Enabling Accurate and Scalable Simulation for Interactive Microservices
AU  - Zhang, Y.
AU  - Gan, Y.
AU  - Delimitrou, C.
T2  - Proceedings - 2019 IEEE International Symposium on Performance Analysis of Systems and Software, ISPASS 2019
AB  - Current cloud services are moving away from monolithic designs and towards graphs of many loosely-coupled, single-concerned microservices. Microservices have several advantages, including speeding up development and deployment, allowing specialization of the software infrastructure, and helping with debugging and error isolation. At the same time they introduce several hardware and software challenges. Given that most of the performance and efficiency implications of microservices happen at scales larger than what is available outside production deployments, studying such effects requires designing the right simulation infrastructures. We present j);qSim, a scalable and validated queueing network simulator designed specifically for interactive microser-vices. μqSim provides detailed intra- and inter-microservice models that allow it to faithfully reproduce the behavior of complex, many-tier applications. μqSim is also modular, allowing reuse of individual models across microservices and end-to-end applications. We have validated μqSim both against simple and more complex microservices graphs, and have shown that it accurately captures performance in terms of throughput and tail latency. Finally, we use μqSim to model the tail at scale effects of request fanout, and the performance impact of power management in latency -sensitive microservices. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/ISPASS.2019.00034
SP  - 212
EP  - 222
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065406209&doi=10.1109%2fISPASS.2019.00034&partnerID=40&md5=6d6d2dddbd199083af596f9b032e8407
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 17</p>
KW  - Complex networks
KW  - End-to-end application
KW  - Hardware and software
KW  - Individual models
KW  - Loosely coupled
KW  - Monolithic design
KW  - Network simulators
KW  - Performance impact
KW  - Program debugging
KW  - Software infrastructure
ER  - 

TY  - CONF
TI  - A Flow-Level Wi-Fi Model for Large Scale Network Simulation
AU  - Courageux-Sudan, C.
AU  - Guegan, L.
AU  - Orgerie, A.-C.
AU  - Quinson, M.
T2  - MSWiM 2022 - Proceedings of the International Conference on Modeling Analysis and Simulation of Wireless and Mobile Systems
AB  - Wi-Fi networks are extensively used to provide Internet access to end-users and to deploy applications at the edge. By playing a major role in modern networking, Wi-Fi networks are getting bigger and denser. However, studying their performance at large-scale and in a reproducible manner remains a challenging task. Current solutions include real experiments and simulations. While the size of experiments is limited by their financial cost and potential disturbance of commercial networks, the simulations also lack scalability due to their models' granularity and computational runtime. In this paper, we introduce a new Wi-Fi model for large-scale simulations. This model, based on flow-level simulation, requires fewer computations than state-of-the-art models to estimate bandwidth sharing over a wireless medium, leading to better scalability. Comparing our model to the already existing Wi-Fi implementation of ns-3, we show that our approach yields to close performance evaluations while improving the runtime of simulations by several orders of magnitude. Using this kind of model could allow researchers to obtain reproducible results for networks composed of thousands of nodes much faster than previously.  © 2022 ACM.
DA  - 2022///
PY  - 2022
DO  - 10.1145/3551659.3559022
SP  - 111
EP  - 119
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141711765&doi=10.1145%2f3551659.3559022&partnerID=40&md5=f2ab212a07e7cb574de31b0aff3d05b6
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3</p>
KW  - End-users
KW  - Flow-level
KW  - Internet access
KW  - Large-scale network simulation
KW  - Large-scales
KW  - Model and simulation
KW  - modeling and simulation
KW  - Performance
KW  - performance evaluation
KW  - Performances evaluation
KW  - Runtimes
KW  - Scalability
KW  - Wi-Fi
KW  - Wi-fi network
KW  - Wi-Fi networks
KW  - Wireless local area networks (WLAN)
ER  - 

TY  - JOUR
TI  - On the Validity of Flow-level TCP Network Models for Grid and Cloud Simulations
AU  - Velho, P.
AU  - Schnorr, L.M.
AU  - Casanova, H.
AU  - Legrand, A.
T2  - ACM Transactions on Modeling and Computer Simulation
AB  - Researchers in the area of grid/cloud computing perform many of their experiments using simulations that must capture network behavior. In this context, packet-level simulations, which are widely used to study network protocols, are too costly given the typical large scales of simulated systems and applications. An alternative is to implement network simulations with less costly flow-level models. Several flow-level models have been proposed and implemented in grid/cloud simulators. Surprisingly, published validations of these models, if any, consist of verifications for only a few simple cases. Consequently, even when they have been used to obtain published results, the ability of these simulators to produce scientifically meaningful results is in doubt. This work evaluates these state-of-the-art flow-level network models of TCP communication via comparison to packet-level simulation. While it is straightforward to show cases in which previously proposed models lead to good results, instead we follow the critical method, which places model refutation at the center of the scientific activity, and we systematically seek cases that lead to invalid results. Careful analysis of these cases reveals fundamental flaws and also suggests improvements. One contribution of this work is that these improvements lead to a new model that, while far from being perfect, improves upon all previously proposed models in the context of simulation of grids or clouds. A more important contribution, perhaps, is provided by the pitfalls and unexpected behaviors encountered in this work, leading to a number of enlightening lessons. In particular, this work shows that model validation cannot be achieved solely by exhibiting (possibly many) “good cases.” Confidence in the quality of a model can only be strengthened through an invalidation approach that attempts to prove the model wrong. © 2013, ACM. All rights reserved.
DA  - 2013///
PY  - 2013
DO  - 10.1145/2517448
VL  - 23
IS  - 4
SP  - 1
EP  - 26
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002716104&doi=10.1145%2f2517448&partnerID=40&md5=ac18458d87cdb90eaf754aad0005ae59
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 65</p>
KW  - Experimentation
KW  - Grid and cloud computing simulation
KW  - scalability
KW  - SimGrid
KW  - validation
ER  - 

TY  - JOUR
TI  - YAFS: A Simulator for IoT Scenarios in Fog Computing
AU  - Lera, I.
AU  - Guerrero, C.
AU  - Juiz, C.
T2  - IEEE Access
AB  - Fog computing is a paradigm that extends the cloud to intermediate network devices with computational and storage capacities. This allows the execution of applications closer to edge devices and end-users by allocating services in those intermediate devices. The placement of those services has an influence on the performance of the fog architecture. We propose a fog computing simulator for analyzing the design and deployment of applications through customized and dynamical strategies. We model the relationships among deployed applications, network connections, and infrastructure characteristics through complex network theory, enabling the integration of topological measures in dynamic and customizable strategies, such as the placement of application modules, workload location, and path routing and scheduling of services. We present a comparative analysis of the efficiency and the convergence of results of our simulator with the most referenced one, iFogSim. To highlight the YAFS functionalities, we model three scenarios that, to the best of our knowledge, cannot be implemented with current fog simulators: dynamic allocation of new application modules, dynamic failures of network nodes, and user mobility along with the topology. © 2013 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/ACCESS.2019.2927895
VL  - 7
SP  - 91745
EP  - 91758
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073893852&doi=10.1109%2fACCESS.2019.2927895&partnerID=40&md5=3a2fae3d226d69e4d3a1a6f4d75e397e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 193</p>
KW  - Application module
KW  - Comparative analysis
KW  - Complex networks
KW  - Computation theory
KW  - Deployed applications
KW  - Dynamic allocations
KW  - Dynamic failures
KW  - Fog
KW  - fog computing
KW  - Fog computing
KW  - Intermediate networks
KW  - Internet of things
KW  - Internet of Things
KW  - Network connection
KW  - New applications
KW  - simulator
KW  - Simulators
KW  - Topology
ER  - 

TY  - CONF
TI  - Modeling the End-to-End Energy Consumption of a Nation-Wide Smart Metering Infrastructure
AU  - Gougeon, A.
AU  - Lemercier, F.
AU  - Blavette, A.
AU  - Orgerie, A.-C.
T2  - Proceedings - IEEE Symposium on Computers and Communications
AB  - Several countries have deployed, or have started the deployment of a smart metering infrastructure in order to enable the Smart Grid. This infrastructure aims to provide new services to grid users and grid operators relying on several communication technologies. One of the goals of this infrastructure is to improve energy consumption, for instance by increasing the awareness of the users, or by enforcing energy management policies. Yet, this infrastructure also consumes energy. The objective of this work is to accurately characterize the energy consumption of each part of the smart metering infrastructure, at a nation-wide scale. We also explore several consumption scenarios highlighting the impact of legacy technologies on the energy consumption of the smart metering infrastructure.  © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ISCC55528.2022.9912949
VL  - 2022-June
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141128304&doi=10.1109%2fISCC55528.2022.9912949&partnerID=40&md5=4af824b291f8cb5b8d0102496bf3f964
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3</p>
KW  - Communicationtechnology
KW  - Electric measuring instruments
KW  - End to end
KW  - Energy
KW  - Energy management
KW  - Energy utilization
KW  - Energy-consumption
KW  - Grid operators
KW  - Grid users
KW  - Management policy
KW  - New services
KW  - Smart grid
KW  - Smart metering
ER  - 

TY  - CONF
TI  - Seven pitfalls in modeling and simulation research
AU  - Uhrmacher, A.M.
T2  - Proceedings - Winter Simulation Conference
AB  - Modeling and simulation is applied in many disciplines. While its multidisciplinarity is part of its fascination, its ubiquity also holds some dangers. Being not aware about these dangers might imply that resources are wasted, (PhD-) projects fail, and the overall progress of modeling and simulation is needlessly slowed down. Seven of these pitfalls are identified and tentative recommendations are made on how these pitfalls can be avoided. © 2012 IEEE.
DA  - 2012///
PY  - 2012
DO  - 10.1109/WSC.2012.6465321
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874748463&doi=10.1109%2fWSC.2012.6465321&partnerID=40&md5=7f826674d76c5e6f39b9d9f446936214
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 11</p>
KW  - Circuit simulation
KW  - Computer simulation
KW  - Modeling and simulation
KW  - Multidisciplinarity
KW  - Projects fail
KW  - Software engineering
ER  - 

TY  - CONF
TI  - Improving PSO-based Multi-Objective optimization using crowding, mutation and ε-dominance
AU  - Sierra, M.R.
AU  - Coello Coello, C.A.
T2  - Lecture Notes in Computer Science
AB  - In this paper, we propose a new Multi-Objective Particle Swarm Optimizer, which is based on Pareto dominance and the use of a crowding factor to filter out the list of available leaders. We also propose the use of different mutation (or turbulence) operators which act on different subdivisions of the swarm. Finally, the proposed approach also incorporates the ε-dominance concept to fix the size of the set of final solutions produced by the algorithm. Our approach is compared against five state-of-the-art algorithms, including three PSO-based approaches recently proposed. The results indicate that the proposed approach is highly competitive, being able to approximate the front even in cases where all the other PSO-based approaches fail. © Springer-Verlag Berlin Heidelberg 2005.
DA  - 2005///
PY  - 2005
DO  - 10.1007/978-3-540-31880-4_35
VL  - 3410
SP  - 505
EP  - 519
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-24344480582&doi=10.1007%2f978-3-540-31880-4_35&partnerID=40&md5=cde7f58a922047af031d2f9e8ef27d1d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 763</p>
KW  - Algorithms
KW  - Approximation theory
KW  - Crowding factors
KW  - Multiobjective optimization
KW  - Optimization
KW  - Optimizers
KW  - Pareto principle
ER  - 

TY  - JOUR
TI  - Docker Layer Placement for On-Demand Provisioning of Services on Edge Clouds
AU  - Smet, P.
AU  - Dhoedt, B.
AU  - Simoens, P.
T2  - IEEE Transactions on Network and Service Management
AB  - Driven by the increasing popularity of the microservice architecture, we see an increase in services with unknown demand pattern located in the edge network. Pre-deployed instances of such services would be idle most of the time, which is economically infeasible. Also, the finite storage capacity limits the amount of deployed instances we can offer. Instead, we present an on-demand deployment scheme using the Docker platform. In Docker, service images consist of layers, each layer adding specific functionality. This allows different services to reuse layers, avoiding cluttering the storages with redundant replicas. We propose a layer placement method which allows users to connect to a server, retrieve all necessary layers -possibly from multiple locations- and deploy an instance of the requested service within the desired response time. We search for the best layer placement which maximizes the satisfied demand given the storage and delay constraints. We developed an iterative optimization heuristic which is less exhaustive by dividing the global problem in smaller subproblems. Our simulation results show that our heuristic is able to solve the problem with less system resources. Last, we present interesting use-cases to use this approach in real-life scenarios. © 2004-2012 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/TNSM.2018.2844187
VL  - 15
IS  - 3
SP  - 1161
EP  - 1174
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048151684&doi=10.1109%2fTNSM.2018.2844187&partnerID=40&md5=c4e7bdb07766e3ce1e3794ca7058a7b4
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 36</p>
KW  - Cloud computing
KW  - Computer architecture
KW  - Containers
KW  - docker
KW  - Edge computing
KW  - Iterative methods
KW  - Long tail
KW  - long-tail
KW  - Network architecture
KW  - On demands
KW  - on-demand
KW  - placement algorithm
KW  - Placement algorithm
KW  - Servers
KW  - service-centric
KW  - Service-centric
KW  - services
KW  - Time factors
ER  - 

TY  - CONF
TI  - Elastic virtual machine for fine-grained cloud resource provisioning
AU  - Dawoud, W.
AU  - Takouna, I.
AU  - Meinel, C.
T2  - Communications in Computer and Information Science
AB  - Elasticity is one of the distinguishing characteristics associated with Cloud computing emergence. It enables cloud resources to auto-scale to cope with workload demand. Multi-instances horizontal scaling is the common scalability architecture in Cloud; however, its current implementation is coarse-grained, while it considers Virtual Machine (VM) as a scaling unit, this implies additional scaling-out overhead and limits it to specific applications. To overcome these limitations, we propose Elastic VM as a fine-grained vertical scaling architecture. Our results proved that Elastic VM architecture implies less consumption of resources, mitigates Service Level Objectives (SLOs) violation, and avoids scaling-up overhead. Furthermore, it scales broader range of applications including databases. © 2012 Springer-Verlag.
DA  - 2012///
PY  - 2012
DO  - 10.1007/978-3-642-29219-4_2
VL  - 269 CCIS
SP  - 11
EP  - 25
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865700853&doi=10.1007%2f978-3-642-29219-4_2&partnerID=40&md5=0a9ed8478d935e9cf164b85de141c4a8
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 49</p>
KW  - Architecture
KW  - Auto-scale
KW  - auto-scaling
KW  - cloud computing
KW  - Cloud computing
KW  - Coarse-grained
KW  - Communication systems
KW  - Computer simulation
KW  - elasticity
KW  - Elasticity
KW  - Heterojunction bipolar transistors
KW  - Horizontal scaling
KW  - Resource provisioning
KW  - Scaling-up
KW  - Service level objective
KW  - Vertical scaling
KW  - Virtual machines
KW  - virtualization
KW  - Virtualizations
ER  - 

TY  - JOUR
TI  - Resource optimization of container orchestration: a case study in multi-cloud microservices-based applications
AU  - Guerrero, C.
AU  - Lera, I.
AU  - Juiz, C.
T2  - Journal of Supercomputing
AB  - An approach to optimize the deployment of microservices-based applications using containers in multi-cloud architectures is presented. The optimization objectives are three: cloud service cost, network latency among microservices, and time to start a new microservice when a provider becomes unavailable. The decision variables are: the scale level of the microservices; their allocation in the virtual machines; the provider and virtual machine type selection; and the number of virtual machines. The experiments compare the optimization results between a Greedy First-Fit and a Non-dominated Sorting Genetic Algorithm II (NSGA-II). NSGA-II with a two-point crossover operator and three mutation operators obtained an overall improvement of 300% in regard to the greedy algorithm. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.
DA  - 2018///
PY  - 2018
DO  - 10.1007/s11227-018-2345-2
VL  - 74
IS  - 7
SP  - 2956
EP  - 2983
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044624170&doi=10.1007%2fs11227-018-2345-2&partnerID=40&md5=dd476bbb76f87c0ea30760bdc4b66184
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 85</p>
KW  - Cloud computing
KW  - Container orchestration
KW  - Containers
KW  - Crossover operator
KW  - Decision variables
KW  - Genetic algorithm
KW  - Genetic algorithms
KW  - Greedy algorithms
KW  - Microservices
KW  - Multi-objective optimization
KW  - Multiobjective optimization
KW  - Mutation operators
KW  - Network latencies
KW  - Network security
KW  - Non dominated sorting genetic algorithm ii (NSGA II)
KW  - Resource optimization
KW  - Virtual machine
ER  - 

TY  - JOUR
TI  - Resource Management in Clouds: Survey and Research Challenges
AU  - Jennings, B.
AU  - Stadler, R.
T2  - Journal of Network and Systems Management
AB  - Resource management in a cloud environment is a hard problem, due to: the scale of modern data centers; the heterogeneity of resource types and their interdependencies; the variability and unpredictability of the load; as well as the range of objectives of the different actors in a cloud ecosystem. Consequently, both academia and industry began significant research efforts in this area. In this paper, we survey the recent literature, covering 250+ publications, and highlighting key results. We outline a conceptual framework for cloud resource management and use it to structure the state-of-the-art review. Based on our analysis, we identify five challenges for future investigation. These relate to: providing predictable performance for cloud-hosted applications; achieving global manageability for cloud systems; engineering scalable resource management systems; understanding economic behavior and cloud pricing; and developing solutions for the mobile cloud paradigm. © 2014, Springer Science+Business Media New York.
DA  - 2015///
PY  - 2015
DO  - 10.1007/s10922-014-9307-7
VL  - 23
IS  - 3
SP  - 567
EP  - 619
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931011055&doi=10.1007%2fs10922-014-9307-7&partnerID=40&md5=cf605477e364b9c4bbe4faa4a35f370c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 414</p>
KW  - Cloud computing
KW  - Cloud environments
KW  - Conceptual frameworks
KW  - Developing solutions
KW  - Economics
KW  - Information management
KW  - Natural resources management
KW  - Research challenges
KW  - Resource allocation
KW  - Resource management
KW  - Resource management systems
KW  - State-of-the art reviews
KW  - Survey
KW  - Surveying
KW  - Surveys
KW  - Virtualization
KW  - Virtualizations
ER  - 

TY  - JOUR
TI  - Performance management for cluster-based web services
AU  - Pacifici, G.
AU  - Spreitzer, M.
AU  - Tantawi, A.N.
AU  - Youssef, A.
T2  - IEEE Journal on Selected Areas in Communications
AB  - We present an architecture and prototype implementation of a performance management system for cluster-based web services. The system supports multiple classes of web services traffic and allocates server resources dynamically so to maximize the expected value of a given cluster utility function in the face of fluctuating loads. The cluster utility is a function of the performance delivered to the various classes, and this leads to differentiated service. In this paper, we will use the average response time as the performance metric. The management system is transparent: it requires no changes in the client code, the server code, or the network interface between them. The system performs three performance management tasks: resource allocation, load balancing, and server overload protection. We use two nested levels of management. The inner level centers on queuing and scheduling of request messages. The outer level is a feedback control loop that periodically adjusts the scheduling weights and server allocations of the inner level. The feedback controller is based on an approximate first-principles model of the system, with parameters derived from continuous monitoring. We focus on SOAP-based web services. We report experimental results that show the dynamic behavior of the system. © 2005 IEEE.
DA  - 2005///
PY  - 2005
DO  - 10.1109/JSAC.2005.857208
VL  - 23
IS  - 12
SP  - 2333
EP  - 2343
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-29144473731&doi=10.1109%2fJSAC.2005.857208&partnerID=40&md5=c529802e51103ae7b22b5f29aed8a004
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 75</p>
KW  - Clustered computing
KW  - Performance management
KW  - Quality of service
KW  - Quality-of-service (QoS)
KW  - Queueing theory
KW  - Resource allocation
KW  - Servers
KW  - Service differentiation
KW  - Telecommunication traffic
KW  - Utility functions
KW  - Web services
KW  - World Wide Web
ER  - 

TY  - JOUR
TI  - Two Issues in Setting Call Centre Staffing Levels
AU  - Chen, B.P.K.
AU  - Henderson, S.G.
T2  - Annals of Operations Research
AB  - Motivated by a problem facing the Police Communication Centre in Auckland, New Zealand, we consider the setting of staffing levels in a call centre with priority customers. The choice of staffing level over any particular time period (e.g., Monday from 8 am-9 am) relies on accurate arrival rate information. The usual method for identifying the arrival rate based on historical data can, in some cases, lead to considerable errors in performance estimates for a given staffing level. We explain why, identify three potential causes of the difficulty, and describe a method for detecting and addressing such a problem.
DA  - 2001///
PY  - 2001
DO  - 10.1023/A:1016015213287
VL  - 108
IS  - 1-4
SP  - 175
EP  - 192
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035742615&doi=10.1023%2fA%3a1016015213287&partnerID=40&md5=03ce261824a313f04e69b3055782ff50
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 52</p>
KW  - Conditional poisson process
KW  - Convexity
KW  - Forecast error
KW  - Nonstationarity
ER  - 

TY  - JOUR
TI  - Self-Adaptive Trade-off Decision Making for Autoscaling Cloud-Based Services
AU  - Chen, T.
AU  - Bahsoon, R.
T2  - IEEE Transactions on Services Computing
AB  - Elasticity in the cloud is often achieved by on-demand autoscaling. In such context, the goal is to optimize the Quality of Service (QoS) and cost objectives for the cloud-based services. However, the difficulty lies in the facts that these objectives, e.g., throughput and cost, can be naturally conflicted; and the QoS of cloud-based services often interfere due to the shared infrastructure in cloud. Consequently, dynamic and effective trade-off decision making of autoscaling in the cloud is necessary, yet challenging. In particular, it is even harder to achieve well-compromised trade-offs, where the decision largely improves the majority of the objectives; while causing relatively small degradations to others. In this paper, we present a self-adaptive decision making approach for autoscaling in the cloud. It is capable to adaptively produce autoscaling decisions that lead to well-compromised trade-offs without heavy human intervention. We leverage on ant colony inspired multi-objective optimization for searching and optimizing the trade-offs decisions, the result is then filtered by compromise-dominance, a mechanism that extracts the decisions with balanced improvements in the trade-offs. We experimentally compare our approach to four state-of-the-arts autoscaling approaches: rule, heuristic, randomized and multi-objective genetic algorithm based solutions. The results reveal the effectiveness of our approach over the others, including better quality of trade-offs and significantly smaller violation of the requirements. © 2015 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/TSC.2015.2499770
VL  - 10
IS  - 4
SP  - 618
EP  - 632
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029351128&doi=10.1109%2fTSC.2015.2499770&partnerID=40&md5=0da55c2422403d112b7efdc918b9ab49
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 50</p>
KW  - Adaptive decision making
KW  - Ant colonies
KW  - Ant colony optimization
KW  - cloud computing
KW  - Cloud computing
KW  - Decision making
KW  - Economic and social effects
KW  - Genetic algorithms
KW  - Human intervention
KW  - Multi-objective genetic algorithm
KW  - multi-objective trade-offs
KW  - Multiobjective optimization
KW  - Optimization
KW  - QoS interference
KW  - Quality of service
KW  - Search based optimizations
KW  - Search-based optimization
KW  - Shared infrastructure
KW  - State of the art
KW  - Trade off
ER  - 

TY  - JOUR
TI  - CONVEXITY OF THE MEAN QUEUE SIZE OF THE M/M/c QUEUE WITH RESPECT TO THE TRAFFIC INTENSITY.
AU  - Grassmann, W.
T2  - Journal of Applied Probability
AB  - It is shown that the expected number in an M/M/c queue is convex with respect to the traffic intensity. The proof is conducted by expressing the second derivative of the expected queue size as the sum of non-negative terms.
DA  - 1983///
PY  - 1983
DO  - 10.1017/S0021900200024244
VL  - 20
IS  - 4
SP  - 916
EP  - 919
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020887727&doi=10.1017%2fS0021900200024244&partnerID=40&md5=87a6779b9a04878607ed482bb023c679
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 56</p>
KW  - Probability
ER  - 

TY  - JOUR
TI  - The differencing method of set partitioning
AU  - Karmarkar, N.
AU  - Karp, R.M.
T2  - The Differencing Method of Set Partitioning
DA  - 1982///
PY  - 1982
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003590131&partnerID=40&md5=5341bca0eee85b3d764e5afbbc6cb864
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 170</p>
ER  - 

TY  - JOUR
TI  - Write off-loading: Practical power management for enterprise storage
AU  - Narayanan, D.
AU  - Donnelly, A.
AU  - Rowstron, A.
T2  - ACM Transactions on Storage
AB  - In enterprise data centers power usage is a problem impacting server density and the total cost of ownership. Storage uses a significant fraction of the power budget and there are no widely deployed power-saving solutions for enterprise storage systems. The traditional view is that enterprise workloads make spinning disks down ineffective because idle periods are too short. We analyzed block-level traces from 36 volumes in an enterprise data center for one week and concluded that significant idle periods exist, and that they can be further increased by modifying the read/write patterns using write off-loading. Write off-loading allows write requests on spun-down disks to be temporarily redirected to persistent storage elsewhere in the data center. The key challenge is doing this transparently and efficiently at the block level, without sacrificing consistency or failure resilience. We describe our write off-loading design and implementation that achieves these goals. We evaluate it by replaying portions of our traces on a rack-based testbed. Results show that just spinning disks down when idle saves 28 - 36% of energy, and write off-loading further increases the savings to 45 - 60%. © 2008 ACM.
DA  - 2008///
PY  - 2008
DO  - 10.1145/1416944.1416949
VL  - 4
IS  - 3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-59449092139&doi=10.1145%2f1416944.1416949&partnerID=40&md5=fdf472ddb79d2aebeacd76b3f69e69b9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 487</p>
KW  - Disk spin-down
KW  - DiskEnergy
KW  - Disks (machine components)
KW  - Disks (structural components)
KW  - Energy
KW  - Energy management
KW  - Enterprise storage
KW  - Loading
KW  - Power
KW  - Satellite communication systems
KW  - Spin dynamics
KW  - Write off-loading
ER  - 

TY  - CONF
TI  - Economical and robust provisioning of N-tier cloud workloads: A Multi-level control approach
AU  - Xiong, P.
AU  - Wang, Z.
AU  - Malkowski, S.
AU  - Wang, Q.
AU  - Jayasinghe, D.
AU  - Pu, C.
T2  - Proceedings - International Conference on Distributed Computing Systems
AB  - Resource provisioning for N-tier web applications in Clouds is non-trivial due to at least two reasons. First, there is an inherent optimization conflict between cost of resources and Service Level Agreement (SLA) compliance. Second, the resource demands of the multiple tiers can be different from each other, and varying along with the time. Resources have to be allocated to multiple (virtual) containers to minimize the total amount of resources while meeting the end-to-end performance requirements for the application. In this paper we address these two challenges through the combination of the resource controllers on both application and container levels. On the application level, a decision maker (i.e., an adaptive feedback controller) determines the total budget of the resources that are required for the application to meet SLA requirements as the workload varies. On the container level, a second controller partitions the total resource budget among the components of the applications to optimize the application performance (i.e., to minimize the round trip time). We evaluated our method with three different workload models-open, closed, and semiopen- that were implemented in the RUBiS web application benchmark. Our evaluation indicates two major advantages of our method in comparison to previous approaches. First, fewer resources are provisioned to the applications to achieve the same performance. Second, our approach is robust enough to address various types of workloads with time-varying resource demand without reconfiguration. © 2011 IEEE.
DA  - 2011///
PY  - 2011
DO  - 10.1109/ICDCS.2011.88
SP  - 571
EP  - 580
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051864287&doi=10.1109%2fICDCS.2011.88&partnerID=40&md5=e471e1498fd23792af21885b2e739cdc
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 59</p>
KW  - Adaptive control
KW  - Adaptive Control
KW  - Adaptive feedback
KW  - Application level
KW  - Application performance
KW  - Benchmarking
KW  - Budget control
KW  - Cloud computing
KW  - Containers
KW  - Control approach
KW  - Controllers
KW  - Decision makers
KW  - End-to-end performance
KW  - Multi-level
KW  - N-tier web application
KW  - Non-trivial
KW  - Optimization
KW  - Quality of service
KW  - Resource budget
KW  - Resource controllers
KW  - Resource demands
KW  - Resource provisioning
KW  - Round-trip time
KW  - Service level agreement
KW  - Service Level Agreements
KW  - Time varying
KW  - Total budget
KW  - User interfaces
KW  - WEB application
KW  - World Wide Web
ER  - 

TY  - JOUR
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072523733&partnerID=40&md5=a4d9d0ca5c4e0a81742bd1b5fd06c9dc
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - The double Pareto-lognormal distribution - A new parametric model for size distributions
AU  - Reed, W.J.
AU  - Jorgensen, M.
T2  - Communications in Statistics - Theory and Methods
AB  - A family of probability densities, which has proved useful in modelling the size distributions of various phenomens, including incomes and earnings, human settlement sizes, oil-field volumes and particle sizes, is introduced. The distribution, named herein as the double Pareto-lognormal or dPlN distribution, arises as that of the state of a geometric Brownian motion (GBM), with lognormally distributed initial state, after an exponentially distributed length of time (or equivalently as the distribution of the killed state of such a GBM with constant killing rate). A number of phenomena can be viewed as resulting from such a process (e.g., incomes, settlement sizes), which explains the good fit. Properties of the distribution are derived and estimation methods discussed. The distribution exhibits Paretian (power-law) behaviour in both tails, and when plotted on logarithmic axes, its density exhibits hyperbolic-type behaviour.
DA  - 2004///
PY  - 2004
DO  - 10.1081/STA-120037438
VL  - 33
IS  - 8
SP  - 1733
EP  - 1753
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042550642&doi=10.1081%2fSTA-120037438&partnerID=40&md5=55a15aff7686aeb902a3de8923187c2f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 292</p>
KW  - Brownian movement
KW  - Computer simulation
KW  - EM algorithm
KW  - Fat tails
KW  - Financial returns
KW  - Laplace transforms
KW  - Parameter estimation
KW  - Pareto law
KW  - Pareto principle
KW  - Particle size analysis
KW  - Power-law distribution
KW  - Probability
KW  - Size distribution
KW  - WWW file size
ER  - 

TY  - JOUR
TI  - Dynamically scaling applications in the cloud
AU  - Vaquero, L.M.
AU  - Rodero-Merino, L.
AU  - Buyya, R.
T2  - Computer Communication Review
AB  - Scalability is said to be one of the major advantages brought by the cloud paradigm and, more specifically, the one that makes it different to an advanced outsourcing solution. However, there are some important pending issues before making the dreamed automated scaling for applications come true. In this paper, the most notable initiatives towards whole application scalability in cloud environments are presented. We present relevant efforts at the edge of state of the art technology, providing an encompassing overview of the trends they each follow. We also highlight pending challenges that will likely be addressed in new research efforts and present an ideal scalable cloud system.
DA  - 2011///
PY  - 2011
DO  - 10.1145/1925861.1925869
VL  - 41
IS  - 1
SP  - 45
EP  - 52
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551491370&doi=10.1145%2f1925861.1925869&partnerID=40&md5=985086bc116694cc75bc6bfbfbace65b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 288</p>
KW  - Application scalability
KW  - Cloud computing
KW  - Cloud systems
KW  - Computer systems
KW  - Outsourcing
KW  - Outsourcing solution
KW  - Research efforts
KW  - Scalability
KW  - State-of-the-art technology
ER  - 

TY  - JOUR
TI  - Resource allocation algorithms for virtualized service hosting platforms
AU  - Stillwell, M.
AU  - Schanzenbach, D.
AU  - Vivien, F.
AU  - Casanova, H.
T2  - Journal of Parallel and Distributed Computing
AB  - Commodity clusters are used routinely for deploying service hosting platforms. Due to hardware and operation costs, clusters need to be shared among multiple services. Crucial for enabling such shared hosting platforms is virtual machine (VM) technology, which allows consolidation of hardware resources. A key challenge, however, is to make appropriate decisions when allocating hardware resources to service instances. In this work we propose a formulation of the resource allocation problem in shared hosting platforms for static workloads with servers that provide multiple types of resources. Our formulation supports a mix of best-effort and QoS scenarios, and, via a precisely defined objective function, promotes performance, fairness, and cluster utilization. Further, this formulation makes it possible to compute a bound on the optimal resource allocation. We propose several classes of resource allocation algorithms, which we evaluate in simulation. We are able to identify an algorithm that achieves average performance close to the optimal across many experimental scenarios. Furthermore, this algorithm runs in only a few seconds for large platforms and thus is usable in practice. © 2010 Elsevier Inc. All rights reserved.
DA  - 2010///
PY  - 2010
DO  - 10.1016/j.jpdc.2010.05.006
VL  - 70
IS  - 9
SP  - 962
EP  - 974
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955515011&doi=10.1016%2fj.jpdc.2010.05.006&partnerID=40&md5=e2e2dd608677beaf7de2a190b3781bff
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 192</p>
KW  - Algorithms
KW  - Best-effort
KW  - Cluster
KW  - Cluster utilization
KW  - Commodity clusters
KW  - Computer hardware
KW  - Computer simulation
KW  - Hardware resources
KW  - Multiple services
KW  - Objective functions
KW  - Operation cost
KW  - Optimal resource allocation
KW  - Optimization
KW  - Resource allocation
KW  - Resource allocation algorithms
KW  - Resource allocation problem
KW  - Service hosting
KW  - Service instances
KW  - Virtual machine
KW  - Virtual machine technology
KW  - Virtual machines
KW  - Virtualized services
ER  - 

TY  - JOUR
TI  - Black-box and gray-box strategies for virtual machine migration
AU  - Wood, T.
AU  - Shenoy, P.
AU  - Venkataramani, A.
AU  - Yousif, M.
T2  - Proc. NSDI
DA  - 2007///
PY  - 2007
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-53349127069&partnerID=40&md5=fa5dcf25b8c23206d5b3b030ea72fac4
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 144</p>
ER  - 

TY  - JOUR
TI  - A Measurement-Based Characterization of the Energy Consumption in Data Center Servers
AU  - Aroca, J.A.
AU  - Chatzipapas, A.
AU  - Anta, A.F.
AU  - Mancuso, V.
T2  - IEEE Journal on Selected Areas in Communications
AB  - In this work, we present an exhaustive empirical characterization of the power requirements of multiple components of data center servers. To do so, we devise different experiments to stress these components, taking into account the multiple available frequencies and the fact that we are working with multicore servers. In these experiments, we measure energy consumption of server components and identify their optimal operational points. Our study proves that the curve defining the minimal CPU power utilization, as a function of the load in active cycles per second, is neither concave nor purely convex. Instead, it definitively shows a super-linear dependence on the load. Similarly, we present results on how to improve the efficiency of network cards and disks. Finally, we validate the accuracy of the model derived from our characterization by comparing the real energy consumed by two Hadoop applications-PageRank and WordCount-with the estimation from our model, obtaining errors below 4.1% on average. © 2015 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/JSAC.2015.2481198
VL  - 33
IS  - 12
SP  - 2863
EP  - 2877
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960129677&doi=10.1109%2fJSAC.2015.2481198&partnerID=40&md5=cfcd23421d1e295909acbaf7de6079f2
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 31</p>
KW  - Characterization
KW  - Cloud computing
KW  - Cloud Computing
KW  - CPU
KW  - data centers
KW  - Data centers
KW  - disk
KW  - Distributed computer systems
KW  - DVFS
KW  - Electric power measurement
KW  - energy efficiency
KW  - Energy efficiency
KW  - energy measurements
KW  - Energy utilization
KW  - Linear dependence
KW  - Measurement-based
KW  - Multi-core servers
KW  - Multiple components
KW  - network
KW  - Networks (circuits)
KW  - Power requirement
KW  - Program processors
ER  - 

TY  - CONF
TI  - Power provisioning for a warehouse-sized computer
AU  - Fan, X.
AU  - Weber, W.-D.
AU  - Barroso, L.A.
T2  - Proceedings - International Symposium on Computer Architecture
AB  - Large-scale Internet services require a computing infrastructure that can beappropriately described as a warehouse-sized computing system. The cost ofbuilding datacenter facilities capable of delivering a given power capacity tosuch a computer can rival the recurring energy consumption costs themselves.Therefore, there are strong economic incentives to operate facilities as closeas possible to maximum capacity, so that the non-recurring facility costs canbe best amortized. That is difficult to achieve in practice because ofuncertainties in equipment power ratings and because power consumption tends tovary significantly with the actual computing activity. Effective powerprovisioning strategies are needed to determine how much computing equipmentcan be safely and efficiently hosted within a given power budget. In this paper we present the aggregate power usage characteristics of largecollections of servers (up to 15 thousand) for different classes ofapplications over a period of approximately six months. Those observationsallow us to evaluate opportunities for maximizing the use of the deployed powercapacity of datacenters, and assess the risks of over-subscribing it. We findthat even in well-tuned applications there is a noticeable gap (7 - 16%)between achieved and theoretical aggregate peak power usage at the clusterlevel (thousands of servers). The gap grows to almost 40% in wholedatacenters. This headroom can be used to deploy additional compute equipmentwithin the same power budget with minimal risk of exceeding it. We use ourmodeling framework to estimate the potential of power management schemes toreduce peak power and energy usage. We find that the opportunities for powerand energy savings are significant, but greater at the cluster-level (thousandsof servers) than at the rack-level (tens). Finally we argue that systems needto be power efficient across the activity range, and not only at peakperformance levels. Copyright 2007 ACM.
DA  - 2007///
PY  - 2007
DO  - 10.1145/1250662.1250665
SP  - 13
EP  - 23
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-35348835964&doi=10.1145%2f1250662.1250665&partnerID=40&md5=0ba10bc53962f2321e9062dec64e3177
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1697</p>
KW  - Electric power utilization
KW  - Energy efficiency
KW  - Internet
KW  - Mathematical models
KW  - Power modeling
KW  - Power provisioning
KW  - Telecommunication services
KW  - Warehouses
ER  - 

TY  - JOUR
TI  - Multiobjective optimization problems with complicated pareto sets, MOEA/ D and NSGA-II
AU  - Li, H.
AU  - Zhang, Q.
T2  - IEEE Transactions on Evolutionary Computation
AB  - Partly due to lack of test problems, the impact of the Pareto set (PS) shapes on the performance of evolutionary algorithms has not yet attracted much attention. This paper introduces a general class of continuous multiobjective optimization test instances with arbitrary prescribed PS shapes, which could be used for studying the ability of multiobjective evolutionary algorithms for dealing with complicated PS shapes. It also proposes a new version of MOEA/D based on differential evolution (DE), i.e., MOEA/D-DE, and compares the proposed algorithm with NSGA-II with the same reproduction operators on the test instances introduced in this paper. The experimental results indicate that MOEA/D could significantly outperform NSGA-II on these test instances. It suggests that decomposition based multiobjective evolutionary algorithms are very promising in dealing with complicated PS shapes. © 2008 IEEE.
DA  - 2009///
PY  - 2009
DO  - 10.1109/TEVC.2008.925798
VL  - 13
IS  - 2
SP  - 284
EP  - 302
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-67349108023&doi=10.1109%2fTEVC.2008.925798&partnerID=40&md5=13559bf89e121c518aa6b21723f19a3a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2372</p>
KW  - Aggregation
KW  - Decomposition
KW  - Differential evolution
KW  - Evolutionary algorithms
KW  - General class
KW  - Multi-objective evolutionary algorithms
KW  - Multi-objective optimization problems
KW  - Multiobjective optimization
KW  - Nsga-ii
KW  - Pareto optimality
KW  - Pareto principle
KW  - Pareto sets
KW  - Reproduction operators
KW  - Shape optimization
KW  - Test instances
KW  - Test problems
KW  - Testing
ER  - 

TY  - JOUR
TI  - Multiobjective evolutionary algorithms: A comparative case study and the strength Pareto approach
AU  - Zitzler, E.
AU  - Thiele, L.
T2  - IEEE Transactions on Evolutionary Computation
AB  - Evolutionary algorithms (EA's) are often well-suited for optimization problems involving several, often conflicting objectives. Since 1985, various evolutionary approaches to multiobjective optimization have been developed that are capable of searching for multiple solutions concurrently in a single run. However, the few comparative studies of different methods presented up to now remain mostly qualitative and arc often restricted to a few approaches. In this paper, four multiobjective EA's are compared quantitatively where an extended 0/1 knapsack problem is taken as a basis. Furthermore, we introduce a new evolutionary approach to multicriteria optimization, the Strength Pareto EA (SPEA), that combines several features of previous multiobjective EA's in a unique manner. It is characterized by a) storing nondominated solutions externally in a second, continuously updated population, b) evaluating an individual's fitness dependent on the number of external nondominated points that dominate it, c) preserving population diversity using the Pareto dominance relationship, and d) incorporating a clustering procedure in order to reduce the nondominated set without destroying its characteristics. The proof-of-principle results obtained on two artificial problems as well as a larger problem, the synthesis of a digital hardware-software multiprocessor system, suggest that SPEA can be very effective in sampling from along the entire Pareto-optimal front and distributing the generated solutions over the tradeoff surface. Moreover, SPEA clearly outperforms the other four multiobjective EA's on the 0/1 knapsack problem. © 1999 IEEE.
DA  - 1999///
PY  - 1999
DO  - 10.1109/4235.797969
VL  - 3
IS  - 4
SP  - 257
EP  - 271
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033318858&doi=10.1109%2f4235.797969&partnerID=40&md5=beb167a6fd2d1daf7c4f1c7565a77000
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 7625</p>
KW  - Clustering
KW  - Computer simulation
KW  - Evolutionary algorithm
KW  - Genetic algorithms
KW  - Interfaces (computer)
KW  - Knapsack problem
KW  - Multiobjective optimization
KW  - Multiprocessing systems
KW  - Niching
KW  - Optimization
KW  - Pareto optimality
KW  - Problem solving
KW  - Sampling
KW  - Strength Pareto evolutionary algorithms
ER  - 

TY  - JOUR
TI  - An Energy Efficient Ant Colony System for Virtual Machine Placement in Cloud Computing
AU  - Liu, X.-F.
AU  - Zhan, Z.-H.
AU  - Deng, J.D.
AU  - Li, Y.
AU  - Gu, T.
AU  - Zhang, J.
T2  - IEEE Transactions on Evolutionary Computation
AB  - Virtual machine placement (VMP) and energy efficiency are significant topics in cloud computing research. In this paper, evolutionary computing is applied to VMP to minimize the number of active physical servers, so as to schedule underutilized servers to save energy. Inspired by the promising performance of the ant colony system (ACS) algorithm for combinatorial problems, an ACS-based approach is developed to achieve the VMP goal. Coupled with order exchange and migration (OEM) local search techniques, the resultant algorithm is termed an OEMACS. It effectively minimizes the number of active servers used for the assignment of virtual machines (VMs) from a global optimization perspective through a novel strategy for pheromone deposition which guides the artificial ants toward promising solutions that group candidate VMs together. The OEMACS is applied to a variety of VMP problems with differing VM sizes in cloud environments of homogenous and heterogeneous servers. The results show that the OEMACS generally outperforms conventional heuristic and other evolutionary-based approaches, especially on VMP with bottleneck resource characteristics, and offers significant savings of energy and more efficient use of different resources. © 1997-2012 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/TEVC.2016.2623803
VL  - 22
IS  - 1
SP  - 113
EP  - 128
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041323218&doi=10.1109%2fTEVC.2016.2623803&partnerID=40&md5=0bc23775671f54d3ee44222d97082fcd
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 365</p>
KW  - Ant colony optimization
KW  - Ant colony system (ACS)
KW  - Ant colony system algorithms
KW  - Ant colony systems
KW  - Bottleneck resources
KW  - cloud computing
KW  - Cloud computing
KW  - Combinatorial problem
KW  - Distributed computer systems
KW  - Energy efficiency
KW  - Evolutionary algorithms
KW  - Evolutionary computing
KW  - Global optimization
KW  - Green computing
KW  - Heterogeneous servers
KW  - Local search techniques
KW  - Network security
KW  - Optimization
KW  - Program compilers
KW  - Virtual machine
KW  - virtual machine placement (VMP)
KW  - Virtual machine placements
ER  - 

TY  - JOUR
AU  - Garey, M.R.
AU  - Johnson, D.S.
T2  - Computers and Intractability
DA  - 1979///
PY  - 1979
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0004127488&partnerID=40&md5=7acaf2208249ba347c4a2ae88ddf1e3d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3716</p>
ER  - 

TY  - JOUR
TI  - Energy-aware resource allocation heuristics for efficient management of data centers for Cloud computing
AU  - Beloglazov, A.
AU  - Abawajy, J.
AU  - Buyya, R.
T2  - Future Generation Computer Systems
AB  - Cloud computing offers utility-oriented IT services to users worldwide. Based on a pay-as-you-go model, it enables hosting of pervasive applications from consumer, scientific, and business domains. However, data centers hosting Cloud applications consume huge amounts of electrical energy, contributing to high operational costs and carbon footprints to the environment. Therefore, we need Green Cloud computing solutions that can not only minimize operational costs but also reduce the environmental impact. In this paper, we define an architectural framework and principles for energy-efficient Cloud computing. Based on this architecture, we present our vision, open research challenges, and resource provisioning and allocation algorithms for energy-efficient management of Cloud computing environments. The proposed energy-aware allocation heuristics provision data center resources to client applications in a way that improves energy efficiency of the data center, while delivering the negotiated Quality of Service (QoS). In particular, in this paper we conduct a survey of research in energy-efficient computing and propose: (a) architectural principles for energy-efficient management of Clouds; (b) energy-efficient resource allocation policies and scheduling algorithms considering QoS expectations and power usage characteristics of the devices; and (c) a number of open research challenges, addressing which can bring substantial benefits to both resource providers and consumers. We have validated our approach by conducting a performance evaluation study using the CloudSim toolkit. The results demonstrate that Cloud computing model has immense potential as it offers significant cost savings and demonstrates high potential for the improvement of energy efficiency under dynamic workload scenarios. © 2011 Elsevier B.V. All rights reserved.
DA  - 2012///
PY  - 2012
DO  - 10.1016/j.future.2011.04.017
VL  - 28
IS  - 5
SP  - 755
EP  - 768
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857370722&doi=10.1016%2fj.future.2011.04.017&partnerID=40&md5=404d8a6f96e7208ce49f7fb3ffc699cd
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2402</p>
KW  - Allocation algorithm
KW  - Architectural frameworks
KW  - Architectural principles
KW  - Business domain
KW  - Carbon footprint
KW  - Client applications
KW  - Cloud computing
KW  - Computer systems
KW  - Computing environments
KW  - Computing solutions
KW  - Cost saving
KW  - Data centers
KW  - Dynamic consolidation
KW  - Electrical energy
KW  - Energy aware
KW  - Energy efficiency
KW  - Energy efficient
KW  - Energy-efficient resource allocation
KW  - Environmental impact
KW  - Green IT
KW  - High potential
KW  - IT services
KW  - Operational costs
KW  - Pay-as-you-go
KW  - Performance evaluation
KW  - Pervasive applications
KW  - Power usage
KW  - Quality of service
KW  - Research
KW  - Research challenges
KW  - Resource allocation
KW  - Resource management
KW  - Resource providers
KW  - Resource provisioning
KW  - Virtualization
KW  - Virtualizations
ER  - 

TY  - CONF
TI  - Multi-objective particle swarm optimizers: An experimental comparison
AU  - Durillo, J.J.
AU  - García-Nieto, J.
AU  - Nebro, A.J.
AU  - Coello Coello, C.A.
AU  - Luna, F.
AU  - Alba, E.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Particle Swarm Optimization (PSO) has received increasing attention in the optimization research community since its first appearance in the mid-1990s. Regarding multi-objective optimization, a considerable number of algorithms based on Multi-Objective Particle Swarm Optimizers (MOPSOs) can be found in the specialized literature. Unfortunately, no experimental comparisons have been made in order to clarify which MOPSO version shows the best performance. In this paper, we use a benchmark composed of three well-known problem families (ZDT, DTLZ, and WFG) with the aim of analyzing the search capabilities of six representative state-of-the-art MOPSOs, namely, NSPSO, SigmaMOPSO, OMOPSO, AMOPSO, MOPSOpd, and CLMOPSO. We additionally propose a new MOPSO algorithm, called SMPSO, characterized by including a velocity constraint mechanism, obtaining promising results where the rest perform inadequately. © Springer-Verlag 2009.
DA  - 2010///
PY  - 2010
DO  - 10.1007/978-3-642-01020-0_39
VL  - 5467 LNCS
SP  - 495
EP  - 509
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650741651&doi=10.1007%2f978-3-642-01020-0_39&partnerID=40&md5=3eab3740d8b3622d9fc85b598a96864a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 137</p>
KW  - Algorithms
KW  - Comparative studies
KW  - Comparative study
KW  - Experimental comparison
KW  - Multi objective
KW  - Multi-objective optimization
KW  - Multiobjective optimization
KW  - Particle swarm
KW  - Particle swarm optimization
KW  - Particle swarm optimization (PSO)
KW  - Particle swarm optimizers
KW  - Research communities
KW  - Search capabilities
KW  - Velocity constraints
ER  - 

TY  - JOUR
TI  - Energy conservation policies for web servers
AU  - Elnozahy, M.
AU  - Kistler, M.
AU  - Rajamony, R.
T2  - Energy Conservation Policies for Web Servers
DA  - 2003///
PY  - 2003
SP  - 99
EP  - 112
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-1142307038&partnerID=40&md5=66b186478711087a76c76266458c3b17
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 7</p>
ER  - 

TY  - JOUR
TI  - A fast and elitist multiobjective genetic algorithm: NSGA-II
AU  - Deb, K.
AU  - Pratap, A.
AU  - Agarwal, S.
AU  - Meyarivan, T.
T2  - IEEE Transactions on Evolutionary Computation
AB  - Multiobjective evolutionary algorithms (EAs) that use nondominated sorting and sharing have been criticized mainly for their: 1) O(MN3) computational complexity (where M is the number of objectives and N is the population size); 2) nonelitism approach; and 3) the need for specifying a sharing parameter. In this paper, we suggest a nondominated sorting-based multiobjective EA (MOEA), called nondominated sorting genetic algorithm II (NSGA-II), which alleviates all the above three difficulties. Specifically, a fast nondominated sorting approach with O(MN2) computational complexity is presented. Also, a selection operator is presented that creates a mating pool by combining the parent and offspring populations and selecting the best (with respect to fitness and spread) N solutions. Simulation results on difficult test problems show that the proposed NSGA-II, in most problems, is able to find much better spread of solutions and better convergence near the true Pareto-optimal front compared to Pareto-archived evolution strategy and strength-Pareto EA - two other elitist MOEAs that pay special attention to creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multiobjective problems efficiently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective seven-constraint nonlinear problem, are compared with another constrained multiobjective optimizer and much better performance of NSGA-II is observed.
DA  - 2002///
PY  - 2002
DO  - 10.1109/4235.996017
VL  - 6
IS  - 2
SP  - 182
EP  - 197
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036530772&doi=10.1109%2f4235.996017&partnerID=40&md5=76831106ec37df04361f38a96f7c2508
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 43561</p>
KW  - Computational complexity
KW  - Computer simulation
KW  - Constraint handling
KW  - Convergence of numerical methods
KW  - Decision making
KW  - Elitism
KW  - Genetic algorithms
KW  - Multicriterion decision making
KW  - Multiobjective optimization
KW  - Optimization
KW  - Pareto-optimal solutions
KW  - Sorting
ER  - 

TY  - JOUR
TI  - Estimating the Distribution of a Sum of Independent Lognormal Random Variables
AU  - Beaulieu, N.C.
AU  - McLane, P.J.
AU  - Abu-Dayya, A.A.
T2  - IEEE Transactions on Communications
AB  - Four methods that can be used to approximate the distribution function (DF) of a sum of independent lognormal random variables (RV's) are compared. The aim is to determine the best method to compute the DF considering both accuracy and computational effort. The investigation focuses on values of the dB spread, σ, valid for practical problems in wireless transmission (6 dB ≤ σ ≤ 12 dB). Contrary to some previous reports, our results show that the simpler Wilkinson's approach gives a more accurate estimate, in some cases of interest, than Schwartz and Yeh's approach. © 1995 IEEE
DA  - 1995///
PY  - 1995
DO  - 10.1109/26.477480
VL  - 43
IS  - 12
SP  - 2869
EP  - 2873
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029488390&doi=10.1109%2f26.477480&partnerID=40&md5=5a18e034feb2cc251075310bdf8a3208
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 261</p>
KW  - Approximation theory
KW  - Complementary density function
KW  - Computational methods
KW  - Computer simulation
KW  - Estimation
KW  - Farley approximation
KW  - Fourier transforms
KW  - Integration
KW  - Inverse Fourier transform
KW  - Mobile radio systems
KW  - Probability density function
KW  - Random processes
KW  - Random variables
ER  - 

TY  - JOUR
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072518051&partnerID=40&md5=e46be198863af3cf8f4b61bb5b220d49
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Multi-objective particle swarm optimizers: A survey of the state-of-the-art
AU  - Reyes-Sierra, M.
AU  - Coello, C.A.C.
T2  - International Journal of Computational Intelligence Research
DA  - 2006///
PY  - 2006
VL  - 2
IS  - 3
SP  - 287
EP  - 308
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750267220&partnerID=40&md5=1a0128ddf932b774736624118a50213e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1364</p>
ER  - 

TY  - JOUR
TI  - Cloud computing resource scheduling and a survey of its evolutionary approaches
AU  - Zhan, Z.-H.
AU  - Liu, X.-F.
AU  - Gong, Y.-J.
AU  - Zhang, J.
AU  - Chung, H.S.-H.
AU  - Li, Y.
T2  - ACM Computing Surveys
AB  - A disruptive technology fundamentally transforming the way that computing services are delivered, cloud computing offers information and communication technology users a new dimension of convenience of resources, as services via the Internet. Because cloud provides a finite pool of virtualized on-demand resources, optimally scheduling them has become an essential and rewarding topic, where a trend of using Evolutionary Computation (EC) algorithms is emerging rapidly. Through analyzing the cloud computing architecture, this survey first presents taxonomy at two levels of scheduling cloud resources. It then paints a landscape of the scheduling problem and solutions. According to the taxonomy, a comprehensive survey of state-of-the-art approaches is presented systematically. Looking forward, challenges and potential future research directions are investigated and invited, including real-time scheduling, adaptive dynamic scheduling, large-scale scheduling, multiobjective scheduling, and distributed and parallel scheduling. At the dawn of Industry 4.0, cloud computing scheduling for cyber-physical integration with the presence of big data is also discussed. Research in this area is only in its infancy, but with the rapid fusion of information and data technology, more exciting and agenda-setting topics are likely to emerge on the horizon.
DA  - 2015///
PY  - 2015
DO  - 10.1145/2788397
VL  - 47
IS  - 4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939803867&doi=10.1145%2f2788397&partnerID=40&md5=51dd07826ce6c544fd26df28f90068de
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 436</p>
KW  - Ant colony optimization
KW  - Big data
KW  - Cloud computing
KW  - Cloud-computing
KW  - Computer architecture
KW  - Computing resource
KW  - Computing services
KW  - Disruptive technology
KW  - Evolutionary approach
KW  - Evolutionary Computation
KW  - Genetic algorithm
KW  - Genetic algorithms
KW  - Information and Communication Technologies
KW  - Particle swarm
KW  - Particle swarm optimization
KW  - Particle swarm optimization (PSO)
KW  - Resource scheduling
KW  - Resource-scheduling
KW  - Scheduling
KW  - Surveys
KW  - Swarm optimization
ER  - 

TY  - JOUR
TI  - Survey of multi-objective optimization methods for engineering
AU  - Marler, R.T.
AU  - Arora, J.S.
T2  - Structural and Multidisciplinary Optimization
AB  - A survey of current continuous nonlinear multi-objective optimization (MOO) concepts and methods is presented. It consolidates and relates seemingly different terminology and methods. The methods are divided into three major categories: methods with a priori articulation of preferences, methods with a posteriori articulation of preferences, and methods with no articulation of preferences. Genetic algorithms are surveyed as well. Commentary is provided on three fronts, concerning the advantages and pitfalls of individual methods, the different classes of methods, and the field of MOO as a whole. The Characteristics of the most significant methods are summarized. Conclusions are drawn that reflect often-neglected ideas and applicability to engineering problems. It is found that no single approach is superior. Rather, the selection of a specific method depends on the type of information that is provided in the problem, the user's preferences, the solution requirements, and the availability of software.
DA  - 2004///
PY  - 2004
DO  - 10.1007/s00158-003-0368-6
VL  - 26
IS  - 6
SP  - 369
EP  - 395
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442535151&doi=10.1007%2fs00158-003-0368-6&partnerID=40&md5=4139f4adec0a24676ea2d2762299e751
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 4087</p>
KW  - Computer software
KW  - Design variables
KW  - Engineering
KW  - Functions
KW  - Genetic algorithms
KW  - Multi-criteria
KW  - Multi-objective
KW  - Nonlinear systems
KW  - Optimization
KW  - Problem solving
KW  - Vectors
ER  - 

TY  - JOUR
TI  - Open Issues in Scheduling Microservices in the Cloud
AU  - Fazio, M.
AU  - Celesti, A.
AU  - Ranjan, R.
AU  - Liu, C.
AU  - Chen, L.
AU  - Villari, M.
T2  - IEEE Cloud Computing
AB  - The adoption of container-based microservices architectures is revolutionizing application design. By adopting a microservices architecture, developers can engineer applications that are composed of multiple lightweight, self-contained, and portable runtime components deployed across a large number of geodistributed servers. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/MCC.2016.112
VL  - 3
IS  - 5
SP  - 81
EP  - 88
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996569901&doi=10.1109%2fMCC.2016.112&partnerID=40&md5=d52198bb1cc22e10b6f9e5e3d6df7c95
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 139</p>
KW  - Application architecture
KW  - Application design
KW  - Application programs
KW  - Cloud computing
KW  - Cloud computing architecture
KW  - Cloud systems
KW  - Containers
KW  - Distributed applications
KW  - Federated clouds
KW  - Independent components
KW  - Internet of things
KW  - microservices
KW  - Runtimes
KW  - Service deployment
KW  - Software developer
ER  - 

TY  - CONF
TI  - Incentivizing microservices for online resource sharing in edge clouds
AU  - Samanta, A.
AU  - Jiao, L.
AU  - Muhlhauser, M.
AU  - Wang, L.
T2  - Proceedings - International Conference on Distributed Computing Systems
AB  - The microservice architecture provides high agility, making it a suitable choice for implementing edge cloud services. Provisioning microservices at the network edge requires the dynamic allocation of resources. However, due to the resource limitation in the edge cloud environment, there is no guarantee that enough resources are always available upon a microservice's requests. In this paper, we design an online auction-based mechanism to incentivize microservices to spare their occupied resources so that the edge cloud platform can reclaim them and reallocate them to other microservices that need resources. We firstly design a single-stage auction that determines the winning bids to satisfy the resource demands in polynomial time, while calculating the payments. Then, we design an online framework to tie a series of such single-stage auctions into a multi-stage online mechanism without requiring the knowledge of future bids and demands. Via rigorous analysis, we exhibit that our mechanism design achieves truthful bidding and individual rationality, with a constant competitive ratio regarding the social cost of the system in the long run. Finally, we verify the practical performance of our mechanism through extensive simulations. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/ICDCS.2019.00049
VL  - 2019-July
SP  - 420
EP  - 430
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072527786&doi=10.1109%2fICDCS.2019.00049&partnerID=40&md5=e42ef8d1c52446da52f38488cb2fb389
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 38</p>
KW  - Competitive ratio
KW  - Distributed computer systems
KW  - Dynamic allocations
KW  - Edge computing
KW  - Extensive simulations
KW  - Individual rationality
KW  - Machine design
KW  - Microservice
KW  - On-line algorithms
KW  - Online algorithm
KW  - Polynomial approximation
KW  - Resource limitations
KW  - Resource sharing
ER  - 

TY  - JOUR
TI  - Performance assessment of multiobjective optimizers: An analysis and review
AU  - Zitzler, E.
AU  - Thiele, L.
AU  - Laumanns, M.
AU  - Fonseca, C.M.
AU  - Da Fonseca, V.G.
T2  - IEEE Transactions on Evolutionary Computation
AB  - An important issue in multiobjective optimization is the quantitative comparison of the performance of different algorithms. In the case of multiobjective evolutionary algorithms, the outcome is usually an approximation of the Pareto-optimal set, which is denoted as an approximation set, and therefore the question arises of how to evaluate the quality of approximation sets. Most popular are methods that assign each approximation set a vector of real numbers that reflect different aspects of the quality. Sometimes, pairs of approximation sets are considered too. In this study, we provide a rigorous analysis of the limitations underlying this type of quality assessment. To this end, a mathematical frame-work is developed which allows to classify and discuss existing techniques.
DA  - 2003///
PY  - 2003
DO  - 10.1109/TEVC.2003.810758
VL  - 7
IS  - 2
SP  - 117
EP  - 132
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037936618&doi=10.1109%2fTEVC.2003.810758&partnerID=40&md5=16bb83388676a0ce9d36a3e3455cc5e1
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3749</p>
KW  - Approximation theory
KW  - Evolutionary algorithms
KW  - Multiobjective optimization
KW  - Multiobjective optimizers
KW  - Optimization
KW  - Pareto principle
KW  - Performance assessment
KW  - Quality indicator
KW  - Vectors
ER  - 

TY  - CONF
TI  - Verification in the Age of Microservices
AU  - Panda, A.
AU  - Sagiv, M.
AU  - Shenker, S.
T2  - Proceedings of the Workshop on Hot Topics in Operating Systems - HOTOS
AB  - Many large applications are now built using collections of microservices, each of which is deployed in isolated containers and which interact with each other through the use of remote procedure calls (RPCs). The use of microservices improves scalability - each component of an application can be scaled independently - and deployability. However, such applications are inherently distributed and current tools do not provide mechanisms to reason about and ensure their global behavior. In this paper we argue that recent advances in formal methods and software packet processing pave the path towards building mechanisms that can ensure correctness for such systems, both when they are being built and at runtime. These techniques impose minimal runtime overheads and are amenable to production deployments. © 2017 ACM.
DA  - 2017///
PY  - 2017
DO  - 10.1145/3102980.3102986
VL  - Part F129307
SP  - 30
EP  - 36
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028014958&doi=10.1145%2f3102980.3102986&partnerID=40&md5=2c48a4d4ee3c6bb7b997ad2cf5632346
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 33</p>
KW  - Deployability
KW  - Formal methods
KW  - Formal verification
KW  - Global behaviors
KW  - Packet processing
KW  - Remote Procedure Call
KW  - Runtime overheads
KW  - Runtimes
ER  - 

TY  - JOUR
TI  - The output of a queueing system
AU  - Burke, P.J.
T2  - Operations Research
DA  - 1956///
PY  - 1956
VL  - 4
IS  - 6
SP  - 699
EP  - 704
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001104312&partnerID=40&md5=2791e2c21cdc2e9d0a46397d0b298e2b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 454</p>
ER  - 

TY  - JOUR
TI  - Performance analysis of cloud computing centers using M/G/m/m+r queuing systems
AU  - Khazaei, H.
AU  - Misic, J.
AU  - Misic, V.B.
T2  - IEEE Transactions on Parallel and Distributed Systems
AB  - Successful development of cloud computing paradigm necessitates accurate performance evaluation of cloud data centers. As exact modeling of cloud centers is not feasible due to the nature of cloud centers and diversity of user requests, we describe a novel approximate analytical model for performance evaluation of cloud server farms and solve it to obtain accurate estimation of the complete probability distribution of the request response time and other important performance indicators. The model allows cloud operators to determine the relationship between the number of servers and input buffer size, on one side, and the performance indicators such as mean number of tasks in the system, blocking probability, and probability that a task will obtain immediate service, on the other. © 1990-2012 IEEE.
DA  - 2012///
PY  - 2012
DO  - 10.1109/TPDS.2011.199
VL  - 23
IS  - 5
SP  - 936
EP  - 943
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859728505&doi=10.1109%2fTPDS.2011.199&partnerID=40&md5=ee9314987ea6955752696748ee0a917b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 352</p>
KW  - Accurate estimation
KW  - Accurate performance
KW  - Benchmarking
KW  - Blocking probability
KW  - Cloud computing
KW  - Cloud data
KW  - Computing center
KW  - Computing paradigm
KW  - Embedded Markov chain
KW  - embedded Markov chain.
KW  - Input buffers
KW  - Markov processes
KW  - performance analysis
KW  - Performance analysis
KW  - Performance evaluation
KW  - Performance indicators
KW  - Probability distributions
KW  - Queueing theory
KW  - Queuing systems
KW  - queuing theory
KW  - Queuing theory
KW  - response time
KW  - Response time (computer systems)
KW  - Semi markov process
KW  - semi-Markov process
KW  - Server farms
ER  - 

TY  - CONF
TI  - Load Balancing Across Microservices
AU  - Niu, Y.
AU  - Liu, F.
AU  - Li, Z.
T2  - Proceedings - IEEE INFOCOM
AB  - With the advent of cloud container technology, enterprises develop applications through microservices, breaking monolithic software into a suite of small services whose instances run independently in containers. User requests are served by a series of microservices forming a chain, and the chains often share microservices. Existing load balancing strategies either incur significant networking overhead or ignore the competition for shared microservices across chains. Furthermore, typical load balancing solutions leverage a hybrid technique by combining HTTP with message queue to support microservice communications, bringing additional operational complexity. To address these challenges, we propose a chain-oriented load balancing algorithm (COLBA) based solely on message queues, which balances load based on microservice requirements of chains to minimize response time. We model the load balancing problem as a non-cooperative game, and leverage Nash bargaining to coordinate microservice allocation across chains. Employing convex optimization with rounding, we efficiently solve the problem that is proven NP-hard. Extensive trace-driven simulations demonstrate that COLBA reduces the overall average response time at least by 13% compared with existing load balancing strategies. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/INFOCOM.2018.8486300
VL  - 2018-April
SP  - 198
EP  - 206
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056174602&doi=10.1109%2fINFOCOM.2018.8486300&partnerID=40&md5=e6e54404d28ef87f04bcb09227fe99d0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 86</p>
KW  - Application programs
KW  - Chains
KW  - Containers
KW  - Convex optimization
KW  - Distributed computer systems
KW  - Game theory
KW  - Hybrid techniques
KW  - Load balancing algorithms
KW  - Load balancing problem
KW  - Load balancing strategy
KW  - Nash bargaining
KW  - Noncooperative game
KW  - Operational complexity
KW  - Trace driven simulation
ER  - 

TY  - JOUR
TI  - NFVnice: Dynamic backpressure and scheduling for NFV service chains
AU  - Kulkarni, S.G.
AU  - Zhang, W.
AU  - Hwang, J.
AU  - Rajagopalan, S.
AU  - Ramakrishnan, K.
AU  - Wood, T.
AU  - Arumaithurai, M.
AU  - Fu, X.
T2  - ACM SIGCOMM
DA  - 2017///
PY  - 2017
SP  - 71
EP  - 84
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060312991&partnerID=40&md5=192802b7d0fa098b515d2e1b88250d0b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 10</p>
ER  - 

TY  - JOUR
AU  - Medhi, J.
T2  - Stochastic Models in Queueing Theory
DA  - 1991///
PY  - 1991
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003686773&partnerID=40&md5=cc06e1d1814c07bf039459403e4e678e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 577</p>
ER  - 

TY  - JOUR
TI  - Online holiday shopping trends and traffic report for Europe and North America
AU  - Akamai, A.
DA  - 2015///
PY  - 2015
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072519220&partnerID=40&md5=3583a8c0a72d2784008b08e5507794af
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - Power Consumption of Virtualization Technologies: An Empirical Investigation
AU  - Morabito, R.
T2  - Proceedings - 2015 IEEE/ACM 8th International Conference on Utility and Cloud Computing, UCC 2015
AB  - Virtualization is growing rapidly as a result of the increasing number of alternative solutions in this area, and of the wide range of application field. Until now, hypervisor-based virtualization has been the de facto solution to perform server virtualization. Recently, container-based virtualization - an alternative to hypervisors - has gained more attention because of lightweight characteristics, attracting cloud providers that have already made use of it to deliver their services. However, a gap in the existing research on containers exists in the area of power consumption. This paper presents the results of a performance comparison in terms of power consumption of four different virtualization technologies: KVM and Xen, which are based on hypervisor virtualization, Docker and LXC which are based on container virtualization. The aim of this empirical investigation, carried out by means of a testbed, is to understand how these technologies react to particular workloads. Our initial results show how, despite of the number of virtual entities running, both kinds of virtualization alternatives behave similarly in idle state and in CPU/Memory stress test. Contrarily, the results on network performance show differences between the two technologies. © 2015 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/UCC.2015.93
SP  - 522
EP  - 527
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965029470&doi=10.1109%2fUCC.2015.93&partnerID=40&md5=8ec180bb7aeb56b6224c763c30a61187
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 68</p>
KW  - Alternative solutions
KW  - Cloud computing
KW  - Cloud Computing
KW  - container
KW  - Containers
KW  - Docker
KW  - Electric power utilization
KW  - Empirical investigation
KW  - hypervisor
KW  - Hypervisor
KW  - KVM
KW  - LXC
KW  - Performance
KW  - Performance comparison
KW  - power consumption
KW  - Virtual reality
KW  - virtualization
KW  - Virtualization technologies
KW  - Virtualizations
KW  - Xen
ER  - 

TY  - CONF
TI  - Load Balancing for Interdependent IoT Microservices
AU  - Yu, R.
AU  - Kilari, V.T.
AU  - Xue, G.
AU  - Yang, D.
T2  - Proceedings - IEEE INFOCOM
AB  - Advances in virtualization technologies and edge computing have inspired a new paradigm for Internet-of-Things (IoT) application development. By breaking a monolithic application into loosely coupled microservices, great gain can be achieved in performance, flexibility and robustness. In this paper, we study the important problem of load balancing across IoT microservice instances. A key difficulty in this problem is the interdependencies among microservices: the load on a successor microservice instance directly depends on the load distributed from its predecessor microservice instances. We propose a graph-based model for describing the load dependencies among microservices. Based on the model, we first propose a basic formulation for load balancing, which can be solved optimally in polynomial time. The basic model neglects the quality-of-service (QoS) of the IoT application. We then propose a QoS-aware load balancing model, based on a novel abstraction that captures a realization of the application's internal logic. The QoS-aware load balancing problem is NP-hard. We propose a fully polynomial-time approximation scheme for the QoS-aware problem. We show through simulation experiments that our proposed algorithm achieves enhanced QoS compared to heuristic solutions. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/INFOCOM.2019.8737450
VL  - 2019-April
SP  - 298
EP  - 306
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068225941&doi=10.1109%2fINFOCOM.2019.8737450&partnerID=40&md5=a2e0c52954d55f6b18b76371219b3245
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 56</p>
KW  - Application development
KW  - application graph
KW  - Computation theory
KW  - Fully polynomial time approximation schemes
KW  - fully polynomial-time approximation scheme
KW  - Graph-based modeling
KW  - Graphic methods
KW  - Internet of things
KW  - Internet of Things (IOT)
KW  - IoT
KW  - load balancing
KW  - Load balancing models
KW  - Load balancing problem
KW  - microservice
KW  - Polynomial approximation
KW  - Quality of service
KW  - Resource allocation
KW  - Virtualization technologies
ER  - 

TY  - JOUR
TI  - The origin of bursts and heavy tails in human dynamics
AU  - Barabási, A.-L.
T2  - Nature
AB  - The dynamics of many social, technological and economic phenomena are driven by individual human actions, turning the quantitative understanding of human behaviour into a central question of modern science. Current models of human dynamics, used from risk assessment to communications, assume that human actions are randomly distributed in time and thus well approximated by Poisson processes. In contrast, there is increasing evidence that the timing of many human activities, ranging from communication to entertainment and work patterns, follow non-Poisson statistics, characterized by bursts of rapidly occurring events separated by long periods of inactivity. Here I show that the bursty nature of human behaviour is a consequence of a decision-based queuing process: when individuals execute tasks based on some perceived priority, the timing of the tasks will be heavy tailed, with most tasks being rapidly executed, whereas a few experience very long waiting times. In contrast, random or priority blind execution is well approximated by uniform inter-event statistics. These finding have important implications, ranging from resource management to service allocation, in both communications and retail.
DA  - 2005///
PY  - 2005
DO  - 10.1038/nature03459
VL  - 435
IS  - 7039
SP  - 207
EP  - 211
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-18744406314&doi=10.1038%2fnature03459&partnerID=40&md5=ea851126856920836209855834ef27ff
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1860</p>
KW  - Approximation theory
KW  - article
KW  - behavior
KW  - Behavior
KW  - Behavioral research
KW  - biological model
KW  - Commerce
KW  - commercial phenomena
KW  - Communication
KW  - daily life activity
KW  - decision making
KW  - Decision Making
KW  - dynamics
KW  - Economic and social effects
KW  - human
KW  - human activities
KW  - Human Activities
KW  - Human behavior
KW  - Human dynamics
KW  - Human engineering
KW  - Humans
KW  - innovation
KW  - Internet
KW  - interpersonal communication
KW  - model
KW  - Models, Biological
KW  - Models, Statistical
KW  - Modern science
KW  - physiology
KW  - Poisson distribution
KW  - Poisson Distribution
KW  - priority journal
KW  - psychological aspect
KW  - Random Allocation
KW  - randomization
KW  - resource allocation
KW  - Resource Allocation
KW  - resource management
KW  - risk assessment
KW  - Risk assessment
KW  - Risk Assessment
KW  - science
KW  - science and technology
KW  - statistical model
KW  - statistics
KW  - Statistics
KW  - task performance
KW  - Technology
KW  - time
KW  - Time Factors
KW  - Work patterns
ER  - 

TY  - CONF
TI  - SMPSO: A new pso-based metaheuristic for multi-objective optimization
AU  - Nebro, A.J.
AU  - Durillo, J.J.
AU  - Nieto, G.
AU  - Coello, C.A.C.
AU  - Luna, F.
AU  - Alba, E.
T2  - 2009 IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making, MCDM 2009 - Proceedings
AB  - In this work, we present a new multi-objective particle swarm optimization algorithm (PSO) characterized by the use of a strategy to limit the velocity of the particles. The proposed approach, called Speed-constrained Multi-objective PSO (SMPSO) allows to produce new effective particle positions in those cases in which the velocity becomes too high. Other features of SMPSO include the use of polynomial mutation as a turbulence factor and an external archive to store the nondominated solutions found during the search. Our proposed approach is compared with respect to five multi-objective metaheuristics representative of the state-of-the-art in the area. For the comparison, two different criteria are adopted: the quality of the resulting approximation sets and the convergence speed to the Pareto front. The experiments carried out indicate that SMPSO obtains remarkable results in terms of both, accuracy and speed. ©2009 IEEE.
DA  - 2009///
PY  - 2009
DO  - 10.1109/MCDM.2009.4938830
SP  - 66
EP  - 73
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650565595&doi=10.1109%2fMCDM.2009.4938830&partnerID=40&md5=d4ae1703c5ac2dd82a52e83189cec23c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 682</p>
KW  - Approximation set
KW  - Artificial intelligence
KW  - Convergence speed
KW  - Decision making
KW  - Metaheuristic
KW  - Multi objective
KW  - Multi objective particle swarm optimization
KW  - Multi-objective metaheuristics
KW  - Multiobjective optimization
KW  - Nondominated solutions
KW  - Pareto front
KW  - Particle position
KW  - Particle swarm optimization (PSO)
KW  - Polynomial mutation
KW  - Turbulence factor
ER  - 

TY  - CONF
TI  - QoE analysis of NFV-based mobile edge computing video application
AU  - Li, S.
AU  - Guo, Z.
AU  - Shou, G.
AU  - Hu, Y.
AU  - Li, H.
T2  - Proceedings of 2016 5th International Conference on Network Infrastructure and Digital Content, IEEE IC-NIDC 2016
AB  - Mobile Edge Computing (MEC) provides mobile and cloud computing capabilities within the access network. Network Functions Virtualization (NFV) leverages standard IT Virtualization technology to decouple the network functions from the underlying physical infrastructure. Basing on the ICT demand, MEC can be consolidated into NFV, as a network element within access network. This paper presents an architecture of NFV-based MEC platform and analyzes its Quality of Service (QoS) compared with the remote servers (Shenzhen and Qingdao). Then, this paper measures the Quality of Experience (QoE) of HTTP videos deployed in the servers. The result shows MEC can offer a service environment with higher bandwidth, which supports 10-fold gains, and ultra-low latency, jitter and packet loss rate. Moreover, along with the higher resolution and bitrates, the range of the video QoE improvement on this platform rises compared with the remote servers. In a word, the NFV-based MEC can achieve better performance than the remote servers. © 2016 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/ICNIDC.2016.7974607
SP  - 411
EP  - 415
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027520812&doi=10.1109%2fICNIDC.2016.7974607&partnerID=40&md5=271821a67276b14597ba2d481d78cade
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 12</p>
KW  - 5G and future network
KW  - Cloud computing
KW  - Digital integrated circuits
KW  - Distributed computer systems
KW  - Future networks
KW  - Higher resolution
KW  - MEC
KW  - Network function virtualization
KW  - Network functions
KW  - NFV
KW  - Packet loss rates
KW  - QoE
KW  - Quality of experience (QoE)
KW  - Quality of service
KW  - Service environment
KW  - Transfer functions
KW  - Video applications
KW  - Virtual reality
KW  - Virtualization
KW  - Virtualization technologies
ER  - 

TY  - CONF
TI  - Latency and Availability Driven VNF Placement in a MEC-NFV Environment
AU  - Yala, L.
AU  - Frangoudis, P.A.
AU  - Ksentini, A.
T2  - Proceedings - IEEE Global Communications Conference, GLOBECOM
AB  - Multi-access Edge Computing (MEC) is gaining momentum as it is considered as one of the enablers of 5G ultra-Reliable Low-Latency Communications (uRLLC) services. MEC deploys computation resources close to the end user, enabling to reduce drastically the end-to-end latency. ETSI has recently leveraged the MEC architecture to run all MEC entities, including MEC applications, as Virtual Network Functions (VNF) in a Network Functions Virtualization (NFV) environment. This evolution allows taking advantage of the mature architecture and the enabling tools of NFV, including the potential to apply a variety of service-tailored function placement algorithms. However, the latter need to be carefully designed in case of MEC applications such as uRLLC, where service access latency is critical. In this paper, we propose a novel placement scheme applicable to a MEC in NFV environment. In particular, we propose a formulation of the problem of VNF placement tailored to uRLLC as an optimization problem of two conflicting objectives, namely minimizing access latency and maximizing service availability. To deal with the complexity of the problem, we propose a Genetic Algorithm to solve it, which we compare with a CPLEX implementation of our model. Our numerical results show that our heuristic algorithm runs efficiently and produces solutions that approximate well the optimal, reducing latency and providing a highly-available service. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/GLOCOM.2018.8647858
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063445904&doi=10.1109%2fGLOCOM.2018.8647858&partnerID=40&md5=1a2fd5c229e4fd75bb1bb15a4bbcd7d9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 107</p>
KW  - 5G mobile communication systems
KW  - Access latency
KW  - Availability
KW  - Communication service
KW  - Computation resources
KW  - Computing applications
KW  - Edge computing
KW  - Gaining momentum
KW  - Genetic algorithms
KW  - Heuristic algorithms
KW  - Low-latency communication
KW  - Multiaccess
KW  - Network function virtualization
KW  - Network functions
KW  - Transfer functions
KW  - Virtual networks
ER  - 

TY  - CONF
TI  - OpenNetVM: Flexible, high performance NFV (Demo)
AU  - Zhang, W.
AU  - Liu, G.
AU  - Zhang, W.
AU  - Shah, N.
AU  - Lopreiato, P.
AU  - Todeschi, G.
AU  - Ramakrishnan, K.K.
AU  - Wood, T.
T2  - IEEE Workshop on Local and Metropolitan Area Networks
AB  - Network Function Virtualization promises to enable dynamic management of software-based network functions. We envision a dynamic and flexible network that can support a smarter data plane than just simple switches that forward packets. This network architecture supports complex stateful rourtng of flows where processing by network functions (NFs) can transform packet data, customized on a per-flow basis, as it moves between end points. This demo will present OpenNetVM, a highly efficient packet processing framework that greatly simplifies the development of network functions, as well as their management and optimization. OpenNetVM runs network functions in lightweight Docker containers that start in less than a second. The OpenNetVM platform manager provides load balancing, flexible flow management, and service name abstractions. OpenNetVM uses DPDK for high performance I/O, and efficiently routes packets through dynamically created service chains. We will demonstrate how the research community can easily build new network functions and rapidly deploy them to see their effectiveness in high performance network environments. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/LANMAN.2016.7548875
VL  - 2016-August
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987771001&doi=10.1109%2fLANMAN.2016.7548875&partnerID=40&md5=871b8347af77397fd4bae929fb87c014
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 4</p>
KW  - Complex networks
KW  - Dynamic management
KW  - Flexible networks
KW  - Forward packets
KW  - High performance networks
KW  - Metropolitan area networks
KW  - Network architecture
KW  - Network functions
KW  - Network management
KW  - Packet networks
KW  - Packet processing
KW  - Research communities
KW  - Transfer functions
KW  - Virtualizations
ER  - 

TY  - JOUR
TI  - Deploying CPU-intensive applications on MEC in NFV systems: The immersive video use case
AU  - Cattaneo, G.
AU  - Giust, F.
AU  - Meani, C.
AU  - Munaretto, D.
AU  - Paglierani, P.
T2  - Computers
AB  - Multi-access Edge Computing (MEC) will be a technology pillar of forthcoming 5G networks. Nonetheless, there is a great interest in also deploying MEC solutions in current 4G infrastructures. MEC enables data processing in proximity to end users. Thus, latency can be minimized, high data rates locally achieved, and real-time information about radio link status or consumer geographical position exploited to develop high-value services. To consolidate network elements and edge applications on the same virtualization infrastructure, network operators aim to combine MEC with Network Function Virtualization (NFV). However, MEC in NFV integration is not fully established yet: in fact, various architectural issues are currently open, even at standardization level. This paper describes a novel MEC in an NFV system which successfully combines, at management level, MEC functional blocks with an NFV Orchestrator, and can neutrally support any “over the top” Mobile Edge application with minimal integration effort. A specific ME app combined with an end-user app for the provision of immersive video services is presented. To provide low latency, CPU-intensive services to end users, the proposed architecture exploits High-Performance Computing resources embedded in the edge infrastructure. Experimental results showing the effectiveness of the proposed architecture are reported and discussed. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.
DA  - 2018///
PY  - 2018
DO  - 10.3390/computers7040055
VL  - 7
IS  - 4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059256376&doi=10.3390%2fcomputers7040055&partnerID=40&md5=8c47d4cc30ba044b2744a9139ae336d9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 19</p>
KW  - Immersive video
KW  - Multi-access edge computing
KW  - Network function virtualization
KW  - Orchestration
KW  - Video processing
KW  - Virtualization
ER  - 

TY  - JOUR
TI  - DeepNFV: A Lightweight Framework for Intelligent Edge Network Functions Virtualization
AU  - Li, L.
AU  - Ota, K.
AU  - Dong, M.
T2  - IEEE Network
AB  - Traditional Network Functions Virtualization (NFV) implementations are somehow too heavy and do not have enough functionality to conduct complex tasks. In this work, we propose a lightweight NFV framework named DeepNFV, which is based on the Docker container running on the network edge, and integrates state-of-the-art deep learning models with NFV containers to address some complicated problems, such as traffic classification, link analysis, and so on. We compare the DeepNFV framework with several existing works, and detail its structures and functions. The most significant advantage of DeepNFV is its lightweight design, resulting from the virtualization and low-cost nature of the container technology. Also, we design this framework to be compatible with edge devices, in order to decrease the computational overhead of the central servers. Another merit is its strong analysis ability brought by deep learning models, which make it suitable for many more scenarios than traditional NFV approaches. In addition, we also describe some typical application scenarios, regarding how the NFV container works and how to utilize its learning ability. Simulations demonstrate its high efficiency, as well as the outstanding recognition performance in a typical use case. © 1986-2012 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/MNET.2018.1700394
VL  - 33
IS  - 1
SP  - 136
EP  - 141
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045631978&doi=10.1109%2fMNET.2018.1700394&partnerID=40&md5=23023759988cd6f7f02e921bcc8b16b1
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 41</p>
KW  - Computational model
KW  - Computational overheads
KW  - Computer hardware
KW  - Containers
KW  - Deep learning
KW  - Docker containers
KW  - Edge computing
KW  - Learning abilities
KW  - Learning systems
KW  - Lightweight design
KW  - Network function virtualization
KW  - Network functions
KW  - Servers
KW  - Telecommunication traffic
KW  - Traffic classification
KW  - Transfer functions
KW  - Typical application
KW  - Virtual reality
KW  - Virtualization
ER  - 

TY  - JOUR
TI  - Internet-of-Things-Based Smart Cities: Recent Advances and Challenges
AU  - Mehmood, Y.
AU  - Ahmad, F.
AU  - Yaqoob, I.
AU  - Adnane, A.
AU  - Imran, M.
AU  - Guizani, S.
T2  - IEEE Communications Magazine
AB  - The Internet of Things is a novel cutting edge technology that proffers to connect a plethora of digital devices endowed with several sensing, actuation, and computing capabilities with the Internet, thus offering manifold new services in the context of a smart city. The appealing IoT services and big data analytics are enabling smart city initiatives all over the world. These services are transforming cities by improving infrastructure and transportation systems, reducing traffic congestion, providing waste management, and improving the quality of human life. In this article, we devise a taxonomy to best bring forth a generic overview of the IoT paradigm for smart cities, integrated ICT, network types, possible opportunities and major requirements. Moreover, an overview of the up-to-date efforts from standard bodies is presented. Later, we give an overview of existing open source IoT platforms for realizing smart city applications followed by several exemplary case studies. In addition, we summarize the latest synergies and initiatives worldwide taken to promote IoT in the context of smart cities. Finally, we highlight several challenges in order to give future research directions. © 1979-2012 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/MCOM.2017.1600514
VL  - 55
IS  - 9
SP  - 16
EP  - 24
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029576329&doi=10.1109%2fMCOM.2017.1600514&partnerID=40&md5=1cfda6ec9fa8b431bba04b5b29883f1d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 586</p>
KW  - Big data
KW  - Computing capability
KW  - Cutting edge technology
KW  - Data analytics
KW  - Digital devices
KW  - Future research directions
KW  - Internet of things
KW  - Network types
KW  - Open sources
KW  - Smart city
KW  - Standard bodies
KW  - Traffic congestion
KW  - Transportation system
KW  - Waste management
ER  - 

TY  - JOUR
T2  - OpenNebula: Open Source Vcloud
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092693991&partnerID=40&md5=b9a0e50dc3ed8048bd13cdd55b6ec6e4
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - PRESENT: An ultra-lightweight block cipher
AU  - Bogdanov, A.
AU  - Knudsen, L.R.
AU  - Leander, G.
AU  - Paar, C.
AU  - Poschmann, A.
AU  - Robshaw, M.J.B.
AU  - Seurin, Y.
AU  - Vikkelsoe, C.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - With the establishment of the AES the need for new block ciphers has been greatly diminished; for almost all block cipher applications the AES is an excellent and preferred choice. However, despite recent implementation advances, the AES is not suitable for extremely constrained environments such as RFID tags and sensor networks. In this paper we describe an ultra-lightweight block cipher, PRESENT. Both security and hardware efficiency have been equally important during the design of the cipher and at 1570 GE, the hardware requirements for PRESENT are competitive with today's leading compact stream ciphers. © Springer-Verlag Berlin Heidelberg 2007.
DA  - 2007///
PY  - 2007
DO  - 10.1007/978-3-540-74735-2_31
VL  - 4727 LNCS
SP  - 450
EP  - 466
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-37149045263&doi=10.1007%2f978-3-540-74735-2_31&partnerID=40&md5=1df6f4bab163b6946899acf504a32822
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2347</p>
KW  - Cipher applications
KW  - Computer hardware
KW  - Cryptography
KW  - Lightweight block cipher
KW  - Security systems
KW  - Sensor networks
KW  - Systems analysis
ER  - 

TY  - JOUR
TI  - Technology pillars in the architecture of future 5G mobile networks: NFV, MEC and SDN
AU  - Blanco, B.
AU  - Fajardo, J.O.
AU  - Giannoulakis, I.
AU  - Kafetzakis, E.
AU  - Peng, S.
AU  - Pérez-Romero, J.
AU  - Trajkovska, I.
AU  - Khodashenas, P.S.
AU  - Goratti, L.
AU  - Paolino, M.
AU  - Sfakianakis, E.
AU  - Liberal, F.
AU  - Xilouris, G.
T2  - Computer Standards and Interfaces
AB  - This paper analyzes current standardization situation of 5G and the role network softwarization plays in order to address the challenges the new generation of mobile networks must face. This paper surveys recent documentation from the main stakeholders to pick out the use cases, scenarios and emerging vertical sectors that will be enabled by 5G technologies, and to identify future high-level service requirements. Driven by those service requirements 5G systems will support diverse radio access technology scenarios, meet end-to-end user experienced requirements and provide capability of flexible network deployment and efficient operations. Then, based on the identified requirements, the paper overviews the main 5G technology trends and design principles to address them. In particular, the paper emphasizes the role played by three main technologies, namely SDN, NFV and MEC, and analyzes the main open issues of these technologies in relation to 5G. © 2017 Elsevier B.V.
DA  - 2017///
PY  - 2017
DO  - 10.1016/j.csi.2016.12.007
VL  - 54
SP  - 216
EP  - 228
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009468168&doi=10.1016%2fj.csi.2016.12.007&partnerID=40&md5=0e437a6434c1ae84d9b0203d5b64250d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 190</p>
KW  - 5G
KW  - Design Principles
KW  - End to end
KW  - Flexible networks
KW  - High-level services
KW  - MEC
KW  - Mobile telecommunication systems
KW  - NFV
KW  - Paper surveys
KW  - Radio access technologies
KW  - SDN
KW  - Service requirements
KW  - Standardization
KW  - Technology trends
KW  - Wireless networks
ER  - 

TY  - JOUR
T2  - NetFPGA-10G Specifications
DA  - 2012///
PY  - 2012
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092727897&partnerID=40&md5=40bcf615c411a3c9a8296384c2ba9643
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - Serverless computing: economic and architectural impact
AU  - Adzic, G.
AU  - Chatley, R.
T2  - Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering
AB  - Amazon Web Services unveiled their "Lambda"platform in late 2014. Since then, each of the major cloud computing infrastructure providers has released services supporting a similar style of deployment and operation, where rather than deploying and running monolithic services, or dedicated virtual machines, users are able to deploy individual functions, and pay only for the time that their code is actually executing. These technologies are gathered together under the marketing term "serverless"and the providers suggest that they have the potential to significantly change how client/server applications are designed, developed and operated. This paper presents two case industrial studies of early adopters, showing how migrating an application to the Lambda deployment architecture reduced hosting costs-by between 66% and 95%-and discusses how further adoption of this trend might influence common software architecture design practices.  © 2017 ACM.
DA  - 2017///
PY  - 2017
DO  - 10.1145/3106237.3117767
VL  - 2017-January
SP  - 884
EP  - 889
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133872008&doi=10.1145%2f3106237.3117767&partnerID=40&md5=798a97d62635bfdaba6437601b4fa867
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 128</p>
KW  - Amazon web services
KW  - Application programs
KW  - Client/server application
KW  - Cloud computing
KW  - Cloud Computing
KW  - Cloud computing infrastructures
KW  - Cloud-computing
KW  - Deployment architecture
KW  - Economics
KW  - Infrastructure providers
KW  - Lambda's
KW  - Monolithics
KW  - Serverless
KW  - Service supporting
KW  - Web services
ER  - 

TY  - JOUR
TI  - NFV-Enabled IoT Service Provisioning in Mobile Edge Clouds
AU  - Xu, Z.
AU  - Gong, W.
AU  - Xia, Q.
AU  - Liang, W.
AU  - Rana, O.F.
AU  - Wu, G.
T2  - IEEE Transactions on Mobile Computing
AB  - Conventional Internet of Things (IoT) applications involve data capture from various sensors in environments, and the captured data then is processed in remote clouds. However, some critical IoT applications (e.g., autonomous vehicles) require a much lower response latency and more secure guarantees than those offered by remote clouds today. Mobile edge clouds (MEC) supported by the network function virtualization (NFV) technique have been envisioned as an ideal platform for supporting such IoT applications. Specifically, MECs enable to handle IoT applications in edge networks to shorten network latency, and NFV enables agile and low-cost network functions to run in low-cost commodity servers as virtual machines (VMs). One fundamental problem for the provisioning of IoT applications in an NFV-enabled MEC is where to place virtualized network functions (VNFs) for IoT applications in the MEC, such that the operational cost of provisioning IoT applications is minimized. In this paper, we first address this fundamental problem, by considering a special case of the IoT application placement problem, where the IoT application and VNFs of each service request are consolidated into a single location (gateway or cloudlet), for which we propose an exact solution and an approximation algorithm with a provable approximation ratio. We then develop a heuristic algorithm that controls the resource violation ratios of edge clouds in the network. For the IoT application placement problem for IoT applications where their VNFs can be placed to multiple locations, we propose an efficient heuristic that jointly places the IoT application and its VNFs. We finally study the performance of the proposed algorithms by simulations and implementations in a real test-bed, Experimental results show that the performance of the proposed algorithms outperform their counterparts by at least 10 percent.  © 2002-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/TMC.2020.2972530
VL  - 20
IS  - 5
SP  - 1892
EP  - 1906
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103960860&doi=10.1109%2fTMC.2020.2972530&partnerID=40&md5=d26f8f4d8b924c9ae27b8400824dc8f1
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 50</p>
KW  - algorithm design
KW  - approximation algorithms
KW  - Approximation algorithms
KW  - Approximation ratios
KW  - Costs
KW  - Exact solution
KW  - Gateways (computer networks)
KW  - Heuristic algorithms
KW  - Internet of things
KW  - Internet of Things (IOT)
KW  - Internet of Things (IoT) application
KW  - IOT applications
KW  - Mobile edge clouds
KW  - network function virtualization
KW  - Network function virtualization
KW  - Network functions
KW  - Network latencies
KW  - Service requests
KW  - Single location
KW  - Transfer functions
KW  - VNF placement
ER  - 

TY  - CONF
TI  - Netml: An nfv platform with efficient support for machine learning applications
AU  - Dhakal, A.
AU  - Ramakrishnan, K.K.
T2  - Proceedings of the 2019 IEEE Conference on Network Softwarization: Unleashing the Power of Network Softwarization, NetSoft 2019
AB  - Real-Time applications such as autonomous and connected cars, surveillance, and online learning applications have to train on streaming data. They require low-latency, high throughput machine learning (ML) functions resident in the network and in the cloud to perform learning and inference. NFV on edge cloud platforms can provide support for these applications by having heterogeneous computing including GPUs and other accelerators to offload ML-related computation. GPUs provide the necessary speedup for performing learning and inference to meet the needs of these latency sensitive real-Time applications. Supporting ML inference and learning efficiently for streaming data in NFV platforms has several challenges. In this paper, we present a framework, NetML, that runs existing ML applications on an heterogeneous NFV platform that includes both CPUs and GPUs. NetML efficiently transfers the appropriate packet payload to the GPU, minimizing overheads, avoiding locks, and avoiding CPU-based data copies. Additionally, NetML minimizes latency by maximizing overlap between the data movement and GPU computation. We evaluate the efficiency of our approach for training and inference using popular object detection algorithms on our platform. NetML reduces the latency for inferring images by more than 20% and increases the training throughput by 30% while reducing CPU utilization compared to other state-of-The-Art alternatives. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/NETSOFT.2019.8806698
SP  - 396
EP  - 404
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072010834&doi=10.1109%2fNETSOFT.2019.8806698&partnerID=40&md5=31489cc3433a2ac8f4adbcb19fa30a75
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 13</p>
KW  - CPU utilization
KW  - Heterogeneous computing
KW  - Inference engines
KW  - Machine learning
KW  - Machine learning applications
KW  - Network function virtualization
KW  - Object detection
KW  - Object detection algorithms
KW  - Online learning
KW  - Program processors
KW  - Real-time application
KW  - State of the art
KW  - Training throughputs
ER  - 

TY  - JOUR
TI  - A survey of mobile cloud computing: Architecture, applications, and approaches
AU  - Dinh, H.T.
AU  - Lee, C.
AU  - Niyato, D.
AU  - Wang, P.
T2  - Wireless Communications and Mobile Computing
AB  - Together with an explosive growth of the mobile applications and emerging of cloud computing concept, mobile cloud computing (MCC) has been introduced to be a potential technology for mobile services. MCC integrates the cloud computing into the mobile environment and overcomes obstacles related to the performance (e.g., battery life, storage, and bandwidth), environment (e.g., heterogeneity, scalability, and availability), and security (e.g., reliability and privacy) discussed in mobile computing. This paper gives a survey of MCC, which helps general readers have an overview of the MCC including the definition, architecture, and applications. The issues, existing solutions, and approaches are presented. In addition, the future research directions of MCC are discussed. Copyright © 2011 John Wiley & Sons, Ltd.
DA  - 2013///
PY  - 2013
DO  - 10.1002/wcm.1203
VL  - 13
IS  - 18
SP  - 1587
EP  - 1611
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888322916&doi=10.1002%2fwcm.1203&partnerID=40&md5=3c02ac47b24e8c9bfd9126167d40c94b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1820</p>
KW  - Battery life
KW  - Explosive growth
KW  - Future research directions
KW  - Mobile applications
KW  - Mobile cloud computing
KW  - Mobile computing
KW  - Mobile environments
KW  - Mobile service
KW  - Mobile services
KW  - Offloading
KW  - Potential technologies
KW  - Surveys
ER  - 

TY  - JOUR
TI  - Mobile edge computing: A key technology towards 5G
AU  - Hu, Y.C.
AU  - Patel, M.
AU  - Sabella, D.
AU  - Sprecher, N.
AU  - Young, V.
T2  - Mobile Edge Computing-A Key Technology Towards 5G
DA  - 2015///
PY  - 2015
IS  - 11
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963757817&partnerID=40&md5=333635cdcabb534a28c501ea6b0e8ae9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1947</p>
ER  - 

TY  - JOUR
T2  - Ring
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092712343&partnerID=40&md5=29e48261dfa16ff6ddda7221da5e13b0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - A serverless real-time data analytics platform for edge computing
AU  - Nastic, S.
AU  - Rausch, T.
AU  - Scekic, O.
AU  - Dustdar, S.
AU  - Gusev, M.
AU  - Koteska, B.
AU  - Kostoska, M.
AU  - Jakimovski, B.
AU  - Ristov, S.
AU  - Prodan, R.
T2  - IEEE Internet Computing
AB  - A novel approach implements cloud-supported, real-time data analytics in edge computing applications. The authors introduce their serverless edgedata analytics platform and application model and discuss their main design requirements and challenges, based on real-life healthcare use case scenarios. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/MIC.2017.2911430
VL  - 21
IS  - 4
SP  - 64
EP  - 71
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029365106&doi=10.1109%2fMIC.2017.2911430&partnerID=40&md5=9ec5bd564e219d20be5f7f495a7bf388
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 203</p>
KW  - Application modeling
KW  - Edge computing
KW  - Internet
KW  - Real-time data
KW  - Use case scenario
ER  - 

TY  - JOUR
TI  - Real-Time Video Analytics: The Killer App for Edge Computing
AU  - Ananthanarayanan, G.
AU  - Bahl, P.
AU  - Bodik, P.
AU  - Chintalapudi, K.
AU  - Philipose, M.
AU  - Ravindranath, L.
AU  - Sinha, S.
T2  - Computer
AB  - Video analytics will drive a wide range of applications with great potential to impact society. A geographically distributed architecture of public clouds and edges that extend down to the cameras is the only feasible approach to meeting the strict real-time requirements of large-scale live video analytics. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/MC.2017.3641638
VL  - 50
IS  - 10
SP  - 58
EP  - 67
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031670207&doi=10.1109%2fMC.2017.3641638&partnerID=40&md5=395ca02fefe64b7c91fd4acb62d9b615
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 467</p>
KW  - bandwidth
KW  - Bandwidth
KW  - Camera network
KW  - camera networks
KW  - cameras
KW  - Cameras
KW  - Computer science
KW  - Computers
KW  - edge computing
KW  - Edge computing
KW  - intelligent edge
KW  - latency
KW  - provisioning
KW  - real time
KW  - Real time
KW  - Real time videos
KW  - real-time video analytics
KW  - smart cameras
KW  - Smart cameras
KW  - video analytics
KW  - Video analytics
ER  - 

TY  - CONF
TI  - Seamless Support of Low Latency Mobile Applications with NFV-Enabled Mobile Edge-Cloud
AU  - Yang, B.
AU  - Chai, W.K.
AU  - Pavlou, G.
AU  - Katsaros, K.V.
T2  - Proceedings - 2016 5th IEEE International Conference on Cloud Networking, CloudNet 2016
AB  - Emerging mobile multimedia applications, such as augmented reality, have stringent latency requirements and high computational cost. To address this, mobile edge-cloud (MEC) has been proposed as an approach to bring resources closer to users. Recently, in contrast to conventional fixed cloud locations, the advent of network function virtualization (NFV) has, with some added cost due to the necessary decentralization, enhanced MEC with new flexibility in placing MEC services to any nodes capable of virtualizing their resources. In this work, we address the question on how to optimally place resources among NFV-enabled nodes to support mobile multimedia applications with low latency requirement and when to adapt the current resource placements to address workload changes. We first show that the placement optimization problem is NP-hard and propose an online dynamic resource allocation scheme that consists of an adaptive greedy heuristic algorithm and a detection mechanism to identify the time when the system will no longer be able to satisfy the applications' delay requirement. Our scheme takes into account the effect of current existing techniques (i.e., auto-scaling and load balancing). We design and implement a realistic NFV-enabled MEC simulated framework and show through extensive simulations that our proposal always manages to allocate sufficient resources on time to guarantee continuous satisfaction of the application latency requirements under changing workload while incurring up to 40% less cost in comparison to existing overprovisioning approaches. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/CloudNet.2016.21
SP  - 136
EP  - 141
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010685422&doi=10.1109%2fCloudNet.2016.21&partnerID=40&md5=fc8d1c0e614543d42b2fa4773f51d8d8
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 47</p>
KW  - Augmented reality
KW  - Computational costs
KW  - Costs
KW  - Design and implements
KW  - Detection mechanism
KW  - Dynamic resource allocations
KW  - Extensive simulations
KW  - Heuristic algorithms
KW  - Mobile applications
KW  - Mobile multimedia applications
KW  - Multimedia systems
KW  - Optimization
KW  - Placement optimization
ER  - 

TY  - JOUR
T2  - Mec in 5G Networks
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092739808&partnerID=40&md5=7a6a10bc40ad8a1bddfcf88e0c642a98
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Will Serverless Computing Revolutionize NFV?
AU  - Aditya, P.
AU  - Akkus, I.E.
AU  - Beck, A.
AU  - Chen, R.
AU  - Hilt, V.
AU  - Rimac, I.
AU  - Satzke, K.
AU  - Stein, M.
T2  - Proceedings of the IEEE
AB  - Communication networks need to be both adaptive and scalable. The last few years have seen an explosive growth of software-defined networking (SDN) and network function virtualization (NFV) to address this need. Both technologies help enable networking software to be decoupled from the hardware so that software functionality is no longer constrained by the underlying hardware and can evolve independently. Both SDN and NFV aim to advance a software-based approach to networking, where networking functionality is implemented in software modules and executed on a suitable cloud computing platform. Achieving this goal requires the virtualization paradigm used in these services that play an important role in the transition to software-based networks. Consequently, the corresponding computing platforms accompanying the virtualization technologies need to provide the required agility, robustness, and scalability for the services executed. Serverless computing has recently emerged as a new paradigm in virtualization and has already significantly changed the economics of offloading computations to the cloud. It is considered as a low-latency, resource-efficient, and rapidly deployable alternative to traditional virtualization approaches, such as those based on virtual machines and containers. Serverless computing provides scalability and cost reduction, without requiring any additional configuration overhead on the part of the developer. In this paper, we explore and survey how serverless computing technology can help building adaptive and scalable networks and show the potential pitfalls of doing so. © 1963-2012 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/JPROC.2019.2898101
VL  - 107
IS  - 4
SP  - 667
EP  - 678
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062408182&doi=10.1109%2fJPROC.2019.2898101&partnerID=40&md5=b536f61c170aa2ebd4371230e158c4ea
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 50</p>
KW  - Application programs
KW  - Application virtualization
KW  - cloud computing
KW  - Cloud computing
KW  - Cloud computing platforms
KW  - Computer hardware
KW  - Computing technology
KW  - Cost reduction
KW  - edge computing
KW  - Edge computing
KW  - Network function virtualization
KW  - network function virtualization (NFV)
KW  - Offloading computations
KW  - Scalability
KW  - serverless computing
KW  - Software defined networking
KW  - Software defined networking (SDN)
KW  - Software functionality
KW  - software-defined networking (SDN)
KW  - Transfer functions
KW  - Virtual reality
KW  - Virtualization technologies
ER  - 

TY  - JOUR
T2  - Apache Storm
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067659812&partnerID=40&md5=484c381b548d10a7295b00471baa67ca
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 28</p>
ER  - 

TY  - JOUR
TI  - Mobile Edge Computing: A Survey on Architecture and Computation Offloading
AU  - Mach, P.
AU  - Becvar, Z.
T2  - IEEE Communications Surveys and Tutorials
AB  - Technological evolution of mobile user equipment (UEs), such as smartphones or laptops, goes hand-in-hand with evolution of new mobile applications. However, running computationally demanding applications at the UEs is constrained by limited battery capacity and energy consumption of the UEs. A suitable solution extending the battery life-time of the UEs is to offload the applications demanding huge processing to a conventional centralized cloud. Nevertheless, this option introduces significant execution delay consisting of delivery of the offloaded applications to the cloud and back plus time of the computation at the cloud. Such a delay is inconvenient and makes the offloading unsuitable for real-time applications. To cope with the delay problem, a new emerging concept, known as mobile edge computing (MEC), has been introduced. The MEC brings computation and storage resources to the edge of mobile network enabling it to run the highly demanding applications at the UE while meeting strict delay requirements. The MEC computing resources can be exploited also by operators and third parties for specific purposes. In this paper, we first describe major use cases and reference scenarios where the MEC is applicable. After that we survey existing concepts integrating MEC functionalities to the mobile networks and discuss current advancement in standardization of the MEC. The core of this survey is, then, focused on user-oriented use case in the MEC, i.e., computation offloading. In this regard, we divide the research on computation offloading to three key areas: 1) decision on computation offloading; 2) allocation of computing resource within the MEC; and 3) mobility management. Finally, we highlight lessons learned in area of the MEC and we discuss open research challenges yet to be addressed in order to fully enjoy potentials offered by the MEC. © 1998-2012 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/COMST.2017.2682318
VL  - 19
IS  - 3
SP  - 1628
EP  - 1656
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028360031&doi=10.1109%2fCOMST.2017.2682318&partnerID=40&md5=f8175210396621bb960b65c83bb5e929
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2848</p>
KW  - allocation of computing resources
KW  - Allocation of computing resources
KW  - computation offloading
KW  - Computation offloading
KW  - Computer architecture
KW  - Edge computing
KW  - Electric batteries
KW  - Energy utilization
KW  - Mobile applications
KW  - Mobile edge computing
KW  - mobile network architecture
KW  - Mobile telecommunication systems
KW  - mobility management
KW  - Mobility management
KW  - Network architecture
KW  - Real-time application
KW  - Research challenges
KW  - standardization
KW  - Standardization
KW  - Surveys
KW  - Technological evolution
KW  - use-cases
KW  - Wireless networks
ER  - 

TY  - CONF
TI  - An execution model for serverless functions at the edge
AU  - Hall, A.
AU  - Ramachandran, U.
T2  - IoTDI 2019 - Proceedings of the 2019 Internet of Things Design and Implementation
AB  - Serverless computing platforms allow developers to host single-purpose applications that automatically scale with demand. In contrast to traditional long-running applications on dedicated, virtualized, or container-based platforms, serverless applications are intended to be instantiated when called, execute a single function, and shut down when finished. State-of-the-art serverless platforms achieve these goals by creating a new container instance to host a function when it is called and destroying the container when it completes. This design allows for cost and resource savings when hosting simple applications, such as those supporting IoT devices at the edge of the network. However, the use of containers introduces some overhead which may be unsuitable for applications requiring low-latency response or hardware platforms with limited resources, such as those served by edge computing environments. In this paper, we present a nomenclature for characterizing server-less function access patterns which allows us to derive the basic requirements of a serverless computing runtime. We then propose the use of WebAssembly as an alternative method for running serverless applications while meeting these requirements. Finally, we demonstrate how a WebAssembly-based serverless platform provides many of the same isolation and performance guarantees of container-based platforms while reducing average application start times and the resources needed to host them. © 2019 ACM.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3302505.3310084
SP  - 225
EP  - 236
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066038506&doi=10.1145%2f3302505.3310084&partnerID=40&md5=b5ea5977feb5c87dfc390668905e62cc
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 127</p>
KW  - Computing environments
KW  - Computing platform
KW  - Containers
KW  - edge computing
KW  - Edge computing
KW  - FaaS
KW  - fog computing
KW  - Fog computing
KW  - function-as-a-service
KW  - Hardware platform
KW  - Internet of things
KW  - Long-running applications
KW  - Performance guarantees
KW  - serverless
KW  - webassembly
ER  - 

TY  - CONF
TI  - An NFV and microservice based architecture for on-the-fly component provisioning in content delivery networks
AU  - Jahromi, N.T.
AU  - Glitho, R.H.
AU  - Larabi, A.
AU  - Brunner, R.
T2  - CCNC 2018 - 2018 15th IEEE Annual Consumer Communications and Networking Conference
AB  - Content Delivery Networks (CDNs) deliver content (e.g. Web pages, videos) to geographically distributed end-users over the Internet. Some contents do sometimes attract the attention of a large group of end-users. This often leads to flash crowds which can cause major issues such as outage in the CDN. Microservice architectural style aims at decomposing monolithic systems into smaller components which can be independently deployed, upgraded and disposed. Network Function Virtualization (NFV) is an emerging technology that aims to reduce costs and bring agility by decoupling network functions from the underlying hardware. This paper leverages the NFV and microservice architectural style to propose an architecture for on-the-fly CDN component provisioning to tackle issues such as flash crowds. In the proposed architecture, CDN components are designed as sets of microservices which interact via RESTFul Web services and are provisioned as Virtual Network Functions (VNFs), which are deployed and orchestrated on-the-fly. We have built a prototype in which a CDN surrogate server, designed as a set of microservices, is deployed on-the-fly. The prototype is deployed on SAVI, a Canadian distributed test bed for future Internet applications. The performance is also evaluated. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/CCNC.2018.8319227
VL  - 2018-January
SP  - 1
EP  - 7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046953095&doi=10.1109%2fCCNC.2018.8319227&partnerID=40&md5=ea4d830208d22e05baa00b82699ca110
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 13</p>
KW  - Architectural style
KW  - Architecture
KW  - CDN
KW  - Content delivery network
KW  - Content Delivery Network
KW  - Emerging technologies
KW  - Memory architecture
KW  - Microservice
KW  - Monolithic systems
KW  - Network function virtualization
KW  - Network Function Virtualization
KW  - NFV
KW  - On the flies
KW  - on-the-fly component provisioning
KW  - Proposed architectures
KW  - RESTful Web services
KW  - Transfer functions
KW  - Virtual reality
KW  - Web services
KW  - Websites
ER  - 

TY  - JOUR
T2  - Cities with the Most Surveillance Cameras in the World Mass Serveillance
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092720038&partnerID=40&md5=260725fb7bdf156577c8745745b5f957
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Mobile edge computing (MEC); deployment of mobile edge computing in an NFV environment v.1.1.1
T2  - Mobile Edge Computing (MEC); Deployment of Mobile Edge Computing in An NFV Environment
DA  - 2018///
PY  - 2018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050886380&partnerID=40&md5=482634895820ecacec335dcc883dc84d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 53</p>
ER  - 

TY  - JOUR
TI  - A survey on bitrate adaptation schemes for streaming media over HTTP
AU  - Bentaleb, A.
AU  - Taani, B.
AU  - Begen, A.C.
AU  - Timmerer, C.
AU  - Zimmermann, R.
T2  - IEEE Communications Surveys and Tutorials
AB  - In this survey, we present state-of-the-art bitrate adaptation algorithms for HTTP adaptive streaming (HAS). As a key distinction from other streaming approaches, the bitrate adaptation algorithms in HAS are chiefly executed at each client, i.e., in a distributed manner. The objective of these algorithms is to ensure a high quality of experience (QoE) for viewers in the presence of bandwidth fluctuations due to factors like signal strength, network congestion, network recon-vergence events, etc. While such fluctuations are common in public Internet, they can also occur in home networksor even managed networks where there is often admission control and QoS tools. Bitrate adaptation algorithms may take factors like bandwidth estimations, playback buffer fullness, device features, viewer preferences, and content features into account, albeit with different weights. Since the viewer’s QoE needs to be determined in real-time during playback, objective metrics are generally used including number of buffer stalls, duration of startup delay, frequency and amount of quality oscillations, and video instability. By design, the standards for HAS do not mandate any particular adaptation algorithm, leaving it to system builders to innovate and implement their own method. This survey provides an overview of the different methods proposed over the last several years. © 2018 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/COMST.2018.2862938
VL  - 21
IS  - 1
SP  - 562
EP  - 585
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051035075&doi=10.1109%2fCOMST.2018.2862938&partnerID=40&md5=b0d56e7ced58fa2dedc3199f3c243412
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 374</p>
KW  - ABR schemes
KW  - Adaptive video streaming
KW  - Bandwidth
KW  - Bit rates
KW  - Bitrate adaptation
KW  - Communication channels (information theory)
KW  - DASH
KW  - HAS
KW  - Home networks
KW  - HTTP
KW  - Image coding
KW  - Media
KW  - Media streaming
KW  - Network protocols
KW  - Quality of service
KW  - Servers
KW  - Standards
KW  - Streaming media
KW  - Surveys
KW  - Video signal processing
ER  - 

TY  - JOUR
T2  - Apache JMeter
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071968259&partnerID=40&md5=0da8c29c3fc2fa6e7a2bc39496d5b351
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 8</p>
ER  - 

TY  - JOUR
AU  - Hauptmann, A.
T2  - Video Content Analysis
DA  - 2009///
PY  - 2009
SP  - 3271
EP  - 3276
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092744121&partnerID=40&md5=59cfc39a558f3f71bde86a20aa110e95
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Multi-Factorial Energy Aware Resource Management in Edge Networks
AU  - Liu, P.
AU  - Chaudhry, S.R.
AU  - Huang, T.
AU  - Wang, X.
AU  - Collier, M.
T2  - IEEE Transactions on Green Communications and Networking
AB  - Edge networks deliver computing services close to the user, unlike centralized clouds. This improves service scalability and delay-sensitive functions can be offloaded to the edge, when the latency incurred by cloud services is too high. Since services in edge networks, by their nature, are not centralized, careful design is required to achieve efficient resource utilization and low power consumption. These issues are addressed in this paper. A network device power model is formulated to explore the power dissipation characteristics of frequency scalable CMOS devices (as measured using a NetFPGA testbed). An on-demand energy-efficient resource allocation model (OERA) is designed based on this model. OERA features acceptance ratios that are 11%-17% higher than existing solutions and 9% lower power consumption. A novel algorithm is presented for resource placement in edge networks, which can accommodate higher traffic flow demands and distribution distance than existing solutions. This uses mixed integer linear programming to simultaneously maximize the aggregate flow demands and to minimize the network energy consumption. An iterative algorithm and a heuristic greedy edge network device placement algorithm are implemented that not only solve this NP-Hard problem but also significantly reduce the network energy consumption. © 2017 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/TGCN.2018.2874397
VL  - 3
IS  - 1
SP  - 45
EP  - 56
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067529896&doi=10.1109%2fTGCN.2018.2874397&partnerID=40&md5=e5dfab09ec839bf3710c43b758dd4b2e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 12</p>
KW  - Cloud computing
KW  - Computational complexity
KW  - computer network performance
KW  - Computer networks
KW  - Computer resource management
KW  - Distributed computer systems
KW  - Dynamic frequency scaling
KW  - Electric losses
KW  - Electric power measurement
KW  - Electric power utilization
KW  - Embedded systems
KW  - Energy conservation
KW  - Energy dissipation
KW  - Energy efficiency
KW  - Energy utilization
KW  - Energy-efficient resource allocation
KW  - Green computing
KW  - Integer programming
KW  - Iterative methods
KW  - Low power electronics
KW  - Low-power consumption
KW  - Lower-power consumption
KW  - Mixed integer linear programming
KW  - Natural resources management
KW  - Network energy consumption
KW  - Power demands
KW  - Power management (telecommunication)
KW  - Resource allocation
KW  - resource management
KW  - Resource management
KW  - Resource utilisation
KW  - Semiconductor device models
KW  - Semiconductor devices
ER  - 

TY  - JOUR
TI  - Container Network Functions: Bringing NFV to the Network Edge
AU  - Cziva, R.
AU  - Pezaros, D.P.
T2  - IEEE Communications Magazine
AB  - In order to cope with the increasing network utilization driven by new mobile clients, and to satisfy demand for new network services and performance guarantees, telecommunication service providers are exploiting virtualization over their network by implementing network services in virtual machines, decoupled from legacy hardware accelerated appliances. This effort, known as NFV, reduces OPEX and provides new business opportunities. At the same time, next generation mobile, enterprise, and IoT networks are introducing the concept of computing capabilities being pushed at the network edge, in close proximity of the users. However, the heavy footprint of today's NFV platforms prevents them from operating at the network edge. In this article, we identify the opportunities of virtualization at the network edge and present Glasgow Network Functions (GNF), a container-based NFV platform that runs and orchestrates lightweight container VNFs, saving core network utilization and providing lower latency. Finally, we demonstrate three useful examples of the platform: IoT DDoS remediation, on-demand troubleshooting for telco networks, and supporting roaming of network functions. © 1979-2012 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/MCOM.2017.1601039
VL  - 55
IS  - 6
SP  - 24
EP  - 31
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020940399&doi=10.1109%2fMCOM.2017.1601039&partnerID=40&md5=e925375f7ec224aba683e1b2e7cca12f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 122</p>
KW  - Business opportunities
KW  - Computing capability
KW  - Containers
KW  - Hardware-accelerated
KW  - Internet of things
KW  - Net work utilization
KW  - Network function virtualization
KW  - Network functions
KW  - Network services
KW  - Performance guarantees
KW  - Telecommunication service provider
KW  - Telecommunication services
KW  - Transfer functions
KW  - Virtual machine
KW  - Virtual reality
KW  - Virtualization
ER  - 

TY  - JOUR
T2  - Nest
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092729135&partnerID=40&md5=c0410c1350c3d72c5fdb61febc15a106
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Multi-Access Edge Computing (MEC); Framework and Reference Architecture
T2  - Multi-access Edge Computing (MEC); Framework and Reference Architecture
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072052535&partnerID=40&md5=06f6d3fd16207c63b5ccdd0035f7a54f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 219</p>
ER  - 

TY  - JOUR
TI  - Middleware for internet of things: A survey
AU  - Razzaque, M.A.
AU  - Milojevic-Jevric, M.
AU  - Palade, A.
AU  - Cla, S.
T2  - IEEE Internet of Things Journal
AB  - The Internet of Things (IoT) envisages a future in which digital and physical things or objects (e.g., smartphones, TVs, cars) can be connected by means of suitable information and communication technologies, to enable a range of applications and services. The IoT's characteristics, including an ultra-largescale network of things, device and network level heterogeneity, and large numbers of events generated spontaneously by these things, will make development of the diverse applications and services a very challenging task. In general, middleware can ease a development process by integrating heterogeneous computing and communications devices, and supporting interoperability within the diverse applications and services. Recently, there have been a number of proposals for IoT middleware. These proposals mostly addressed wireless sensor networks (WSNs), a key component of IoT, but do not consider RF identification (RFID), machine-tomachine (M2M) communications, and supervisory control and data acquisition (SCADA), other three core elements in the IoT vision. In this paper, we outline a set of requirements for IoT middleware, and present a comprehensive review of the existing middleware solutions against those requirements. In addition, open research issues, challenges, and future research directions are highlighted. © 2015 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/JIOT.2015.2498900
VL  - 3
IS  - 1
SP  - 70
EP  - 95
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959420432&doi=10.1109%2fJIOT.2015.2498900&partnerID=40&md5=b004a6d236543ca3bc7b1f7ec7fe5c43
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 889</p>
KW  - Data acquisition
KW  - Internet
KW  - Internet of things
KW  - Internet of Things (IOT)
KW  - Internet of Things (IoT) characteristics
KW  - Interoperability
KW  - Machine-to-machine (M2M) communication
KW  - Machine-to-machine communications
KW  - Middleware
KW  - Middleware requirements
KW  - Radio frequency identification (RFID)
KW  - RF identification (RFID)
KW  - Rf identifications
KW  - Supervisory control and data acquisition
KW  - Supervisory control and data acquisition (SCADA)
KW  - Wireless sensor network (WSNs)
KW  - Wireless sensor networks
KW  - Wireless sensor networks (WSNs)
ER  - 

TY  - JOUR
TI  - Mobile Edge Computing Resources Optimization: A Geo-Clustering Approach
AU  - Bouet, M.
AU  - Conan, V.
T2  - IEEE Transactions on Network and Service Management
AB  - Mobile edge computing (MEC) is an emerging technology that aims at pushing applications and content close to the users (e.g., at base stations, access points, and aggregation networks) to reduce latency, improve quality of experience, and ensure highly efficient network operation and service delivery. It principally relies on virtualization-enabled MEC servers with limited capacity at the edge of the network. One key issue is to dimension such systems in terms of server size, server number, and server operation area to meet MEC goals. In this paper, we formulate this problem as a mixed integer linear program. We then propose a graph-based algorithm that, taking into account a maximum MEC server capacity, provides a partition of MEC clusters, which consolidates as many communications as possible at the edge. We use a dataset of mobile communications to extensively evaluate them with real world spatiooral human dynamics. In addition to quantifying macroscopic MEC benefits, the evaluation shows that our algorithm provides MEC area partitions that largely offload the core, thus pushing the load at the edge (e.g., with 10 small MEC servers between 55% and 64% of the traffic stay at the edge), and that are well balanced through time. © 2004-2012 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/TNSM.2018.2816263
VL  - 15
IS  - 2
SP  - 787
EP  - 796
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044070269&doi=10.1109%2fTNSM.2018.2816263&partnerID=40&md5=1193c3a0032b88b914cebe6561d05f2e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 124</p>
KW  - Base stations
KW  - clustering
KW  - Clustering algorithms
KW  - dimensioning
KW  - Edge computing
KW  - fog computing
KW  - Fog computing
KW  - Graphic methods
KW  - Heuristic algorithms
KW  - Integer programming
KW  - Mobile communications
KW  - Mobile edge computing
KW  - Mobile Edge Computing
KW  - Mobile telecommunication systems
KW  - multi-access edge computing
KW  - Multiaccess
KW  - network virtualization
KW  - Network virtualization
KW  - Partitioning algorithms
KW  - Quality of service
KW  - Servers
KW  - Virtual reality
KW  - Virtualization
ER  - 

TY  - JOUR
TI  - Lightweight cryptography methods
AU  - Buchanan, W.J.
AU  - Li, S.
AU  - Asif, R.
T2  - Journal of Cyber Security Technology
AB  - While our conventional cryptography methods, such for AES (encryption), SHA-256 (hashing) and RSA/Elliptic Curve (signing), work well on systems which have reasonable processing power and memory capabilities, these do not scale well into a world with embedded systems and sensor networks. Thus, lightweight cryptography methods are proposed to overcome many of the problems of conventional cryptography. This includes constraints related to physical size, processing requirements, memory limitation and energy drain. This paper outlines many of the techniques that are defined as replacements for conventional cryptography within an Internet of things space and discuss some trends in the design of lightweight algorithms. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.
DA  - 2017///
PY  - 2017
DO  - 10.1080/23742917.2017.1384917
VL  - 1
IS  - 3-4
SP  - 187
EP  - 201
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206596672&doi=10.1080%2f23742917.2017.1384917&partnerID=40&md5=22d0af4d890811dc5b012e0cf03625d7
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 152</p>
KW  - AES encryption
KW  - CLEFIA
KW  - Cryptography
KW  - Embedded systems
KW  - encryption
KW  - Enocoro
KW  - hashing functions
KW  - Hashing functions
KW  - Lesamanta-LW
KW  - Lesamantum-LW
KW  - Light-weight cryptography
KW  - Lightweight cryptography
KW  - PHOTONSPONGENT
KW  - PRESENT
KW  - resource-limited devices
KW  - Resource-limited devices
KW  - Sensor networks
KW  - Trivium
ER  - 

TY  - JOUR
TI  - Collaborative Agent Communities for Resilient Service Composition in Mobile Environments
AU  - Palade, A.
AU  - Clarke, S.
T2  - IEEE Transactions on Services Computing
AB  - Automatic planning, with dynamic binding and adaptive composition recovery, has been used to tackle complex service provisioning in mobile environments, but given frequent network topology changes, and services with time-dependent QoS, finding composites that can functionally and non-functionally satisfy a user's request remains difficult. Many service composition mechanisms either require a centralised perspective of the environment, or use optimisation mechanisms that trade off computational efficiency for optimality. Stigmergy-based approaches have been used to model decentralised service interactions between service providers, using a community of mobile software agents that share the same goal to approximate the set of QoS-optimal service compositions. Inspired by this model, this article addresses computational efficiency concerns using a collaborative approach to engage multiple communities of agents for provisioning QoS-optimal service compositions in mobile environments. New compositions can emerge from local decisions and interactions with agents from diverse communities. We assess whether having multiple communities improves the diversity and optimality of solutions. We also measure the proposed approach' efficiency in dealing with incomplete information. The results show that the proposed approach trades optimality for a more diverse set of solutions, at a cost of higher overhead. © 2008-2012 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/TSC.2020.2964753
VL  - 15
IS  - 2
SP  - 876
EP  - 890
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083743454&doi=10.1109%2fTSC.2020.2964753&partnerID=40&md5=f5c84da3826bcd512277c7d10719596b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 15</p>
KW  - Collaborative agents
KW  - Collaborative approach
KW  - Communication overheads
KW  - Computational efficiency
KW  - decentralised
KW  - Economic and social effects
KW  - Efficiency
KW  - flexible
KW  - Incomplete information
KW  - Mobile agents
KW  - Mobile environments
KW  - Mobile software agents
KW  - QoS-aware service composition
KW  - Quality of service
KW  - Service compositions
KW  - Service interaction
KW  - Stigmergic
ER  - 

TY  - CONF
TI  - Hardware-accelerated high-resolution video coding in Virtual Network Functions
AU  - Comi, P.
AU  - Secondo Crosta, P.
AU  - Beccari, M.
AU  - Paglierani, P.
AU  - Grossi, G.
AU  - Pedersini, F.
AU  - Petrini, A.
T2  - EUCNC 2016 - European Conference on Networks and Communications
AB  - Network Function Virtualization (NFV) has become a widely acclaimed approach to facilitate the management and orchestration of network services. However, after rapidly achieving a widespread success, NFV is now challenged by the overwhelming demand of computing power originated by the never-ending growth of innovative applications coming from the Internet world. To overcome this problem, the use of h/w acceleration combined with NFV has been proposed. This way, the computing performance of commodity servers can be greatly enhanced, without losing the advantages offered by NFV in service management. In this paper, to demonstrate the potentialities of NFV and h/w acceleration, a Virtual Network Function for video coding (video Transcoding Unit-vTU) is presented. The vTU is accelerated by a General Purpose GPU, and is based on Open Source software packages for media processing. The vTU architecture is firstly described in details. A thorough characterization of its computing performance is then reported, and the obtained results are compared to those achieved with non-accelerated and/or non-virtualized versions of the vTU itself. Also, the performance provided by an original, GPU accelerated version of the VP8 encoder is presented. The activities described in this paper have been carried out within the EU FP7 T-NOVA project. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/EuCNC.2016.7560999
SP  - 32
EP  - 36
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988936199&doi=10.1109%2fEuCNC.2016.7560999&partnerID=40&md5=e4755e04c082aa1f8511639697dd8af8
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 13</p>
KW  - Computing performance
KW  - General purpose gpu
KW  - Hardware-accelerated
KW  - In-service management
KW  - Media processing
KW  - Network functions
KW  - Network services
KW  - Open source software
KW  - Open systems
KW  - Program processors
KW  - Software engineering
KW  - Transfer functions
KW  - Video signal processing
KW  - Video-transcoding
ER  - 

TY  - CONF
TI  - An evaluation of open source serverless computing frameworks support at the Edge
AU  - Palade, A.
AU  - Kazmi, A.
AU  - Clarke, S.
T2  - Proceedings - 2019 IEEE World Congress on Services, SERVICES 2019
AB  - The proliferation of Internet of Things (IoT) and the success of resource-rich cloud services have pushed the data processing horizon towards the edge of the network. This has the potential to address bandwidth costs, and latency, availability and data privacy concerns. Serverless computing, a cloud computing model for stateless and event-driven applications, promises to further improve Quality of Service (QoS) by eliminating the burden of always-on infrastructure through ephemeral containers. Open source serverless frameworks have been introduced to avoid the vendor lock-in and computation restrictions of public cloud platforms and to bring the power of serverless computing to on-premises deployments. In an IoT environment, these frameworks can leverage the computational capabilities of devices in the local network to further improve QoS of applications delivered to the user. However, these frameworks have not been evaluated in a resource-constrained, edge computing environment. In this work we evaluate four open source serverless frameworks, namely, Kubeless, Apache OpenWhisk, OpenFaaS, Knative. Each framework is installed on a bare-metal, single master, Kubernetes cluster. We use the JMeter framework to evaluate the response time, throughput and success rate of functions deployed using these frameworks under different workloads. The evaluation results are presented and open research opportunities are discussed. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/SERVICES.2019.00057
SP  - 206
EP  - 211
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072771398&doi=10.1109%2fSERVICES.2019.00057&partnerID=40&md5=921ad78e9a7561588b837529881b53b9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 87</p>
KW  - Apache Openwhisk
KW  - Computing frameworks
KW  - Containers
KW  - Data privacy
KW  - Docker
KW  - FaaS
KW  - Function as a Service
KW  - Function composition
KW  - Function evaluation
KW  - Internet of things
KW  - JMeter
KW  - Knative
KW  - Kubeless
KW  - Kubernetes
KW  - Open systems
KW  - OpenFaaS
KW  - Opensource serverless computing frameworks
KW  - QoS
KW  - Quality of service
KW  - Quantitative evaluation
KW  - Serverless computing
ER  - 

TY  - JOUR
TI  - Multi-access Edge Computing (MEC); Framework and Reference Architecture: Tech. Rep. GS MEC 003
T2  - Multi-Access Edge Computing (MEC)
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101137764&partnerID=40&md5=69c98f52ad8c938df8d5a44fd2f4dfba
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 46</p>
ER  - 

TY  - CONF
TI  - Study of secure boot with a FPGA-based IoT device
AU  - Liu, Y.
AU  - Briones, J.
AU  - Zhou, R.
AU  - Magotra, N.
T2  - Midwest Symposium on Circuits and Systems
AB  - Internet of Things (loT) is network connected 'Things' such as vehicles, buildings, embedded systems, sensors, as well as people. IoT enables these objects to collect and exchange data of interest to complete various tasks including patient health monitoring, environmental monitoring, system condition prognostics and prediction, smart grid, smart buildings, smart cities, and do on. Due to the large scale of and the limited host processor computation power in an IoT system, effective security provisioning is shifting from software-based security implementation to hardware-based security implementation in terms of efficiency and effectiveness. Moreover, FPGA can take over the work of infrastructure components to preserve and protect critical components and minimize the negative impacts on these components. In this paper, we employ Xilinx Zynq-7000 Series System-on-Chip (SoC) ZC706 prototype board to design an IoT device. To defend against threats to FPGA design, we have studied Zynq-ZC706 to (1) encrypt FPGA bitstream to protect the IoT device from bitstream decoding; (2) encrypt system boot image to enhance system security; and (3) ensure the FPGA operates correctly as intended via authentication to avoid spoofing and Trojan Horse attacks. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/MWSCAS.2017.8053108
VL  - 2017-August
SP  - 1053
EP  - 1056
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034084065&doi=10.1109%2fMWSCAS.2017.8053108&partnerID=40&md5=9cba3d9340f4b2807acc0ab560f9da3f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 30</p>
KW  - Binary sequences
KW  - Computation power
KW  - Critical component
KW  - Cryptography
KW  - Embedded systems
KW  - Environmental Monitoring
KW  - Field programmable gate arrays (FPGA)
KW  - Hardware security
KW  - Image enhancement
KW  - Integrated circuit design
KW  - Internet of things
KW  - Malware
KW  - Programmable logic controllers
KW  - Prototype boards
KW  - Security implementations
KW  - Security Provisioning
KW  - Smart city
KW  - System conditions
KW  - System-on-chip
KW  - Trojan Horse attacks
ER  - 

TY  - JOUR
TI  - A Survey on Mobile Edge Computing: The Communication Perspective
AU  - Mao, Y.
AU  - You, C.
AU  - Zhang, J.
AU  - Huang, K.
AU  - Letaief, K.B.
T2  - IEEE Communications Surveys and Tutorials
AB  - Driven by the visions of Internet of Things and 5G communications, recent years have seen a paradigm shift in mobile computing, from the centralized mobile cloud computing toward mobile edge computing (MEC). The main feature of MEC is to push mobile computing, network control and storage to the network edges (e.g., base stations and access points) so as to enable computation-intensive and latency-critical applications at the resource-limited mobile devices. MEC promises dramatic reduction in latency and mobile energy consumption, tackling the key challenges for materializing 5G vision. The promised gains of MEC have motivated extensive efforts in both academia and industry on developing the technology. A main thrust of MEC research is to seamlessly merge the two disciplines of wireless communications and mobile computing, resulting in a wide-range of new designs ranging from techniques for computation offloading to network architectures. This paper provides a comprehensive survey of the state-of-the-art MEC research with a focus on joint radio-and-computational resource management. We also discuss a set of issues, challenges, and future research directions for MEC research, including MEC system deployment, cache-enabled MEC, mobility management for MEC, green MEC, as well as privacy-aware MEC. Advancements in these directions will facilitate the transformation of MEC from theory to practice. Finally, we introduce recent standardization efforts on MEC as well as some typical MEC application scenarios. © 1998-2012 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/COMST.2017.2745201
VL  - 19
IS  - 4
SP  - 2322
EP  - 2358
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028734877&doi=10.1109%2fCOMST.2017.2745201&partnerID=40&md5=6d1262992dcf95ea5c521e168f340065
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 4660</p>
KW  - 5G mobile communication systems
KW  - Cloud computing
KW  - computation offloading
KW  - Computation offloading
KW  - Computation theory
KW  - Edge computing
KW  - Energy utilization
KW  - fog computing
KW  - green computing
KW  - Green computing
KW  - mobile cloud computing
KW  - Mobile cloud computing
KW  - Mobile communications
KW  - Mobile computing
KW  - Mobile edge computing
KW  - Mobile telecommunication systems
KW  - Natural resources management
KW  - Network architecture
KW  - Resource allocation
KW  - resource management
KW  - Resource management
KW  - Surveys
KW  - Wireless communications
KW  - Wireless telecommunication systems
ER  - 

TY  - CONF
TI  - On-the-fly QoE-aware transcoding in the mobile edge
AU  - Dutta, S.
AU  - Taleb, T.
AU  - Frangoudis, P.A.
AU  - Ksentini, A.
T2  - Proceedings - IEEE Global Communications Conference, GLOBECOM
AB  - To enhance video streaming experience for mobile users, we propose an approach towards Quality-of-Experience (QoE) aware on-the-fly transcoding. The proposed approach relies on the concept of Mobile Edge Computing (MEC) as a key enabler in enhancing service quality. Our scheme involves an autonomic creation of a transcoding service as a Virtual Network Function (VNF) and ensures dynamic rate switching of the streamed video to maintain the desirable quality. This edge-assistive transcoding and adaptive streaming results in reduced computational loads and reduced core network traffic. The proposed solution represents a complete miniature content delivery network infrastructure on the edge, ensuring reduced latency and better quality of experience. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/GLOCOM.2016.7842074
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015411480&doi=10.1109%2fGLOCOM.2016.7842074&partnerID=40&md5=8a4b4fe6c630af4c774e112017dab37d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 46</p>
KW  - Autonomics
KW  - Dynamic rates
KW  - Mobile edge computing
KW  - Mobile users
KW  - Network functions
KW  - Quality of service
KW  - Rate switching
KW  - Service Quality
KW  - Streamed video
KW  - Transcoding
KW  - Video streaming
KW  - Video-streaming
KW  - Virtual networks
ER  - 

TY  - JOUR
TI  - Accelerating IPSec NFVs with arrive technologies on the Intel FPGA programmable acceleration card N3000
AU  - Doyle, S.
AU  - Tkachuk, G.
T2  - Netw. Function Virtual. (NFV) Workloads
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092720024&partnerID=40&md5=1e87d4816069a1b3d5b0ef047c65189d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - Will Serverless End the Dominance of Linux in the Cloud?
AU  - Koller, R.
AU  - Williams, D.
T2  - Proceedings of the Workshop on Hot Topics in Operating Systems - HOTOS
AB  - From the inception of the cloud, running multi-tenant workloads has put strain on the Linux kernel's abstractions. After years of having its abstractions bypassed via virtualization, the kernel has responded with a native container abstraction that is eagerly being applied in the cloud. In this paper, we point out that history is repeating itself: with the introduction of serverless computing, even the native container abstraction is ill-suited. We show that bypassing the kernel with unikernels can yield at least a factor of 6 better latency and throughput. Facing a more complex kernel than ever and a relatively undemanding computing model, we must revisit the question of whether the kernel should try to adapt, we should continue bypassing the kernel, or if it is finally time to try a new native OS for this important future cloud workload. © 2017 ACM.
DA  - 2017///
PY  - 2017
DO  - 10.1145/3102980.3103008
VL  - Part F129307
SP  - 169
EP  - 173
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027993116&doi=10.1145%2f3102980.3103008&partnerID=40&md5=94d77241c36530e85f9ecea7d744d3ce
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 40</p>
KW  - Abstracting
KW  - Computer operating systems
KW  - Computing model
KW  - Containers
KW  - Linux
KW  - Linux kernel
KW  - Multi tenants
ER  - 

TY  - JOUR
TI  - Big data analytics for video surveillance
AU  - Subudhi, B.N.
AU  - Rout, D.K.
AU  - Ghosh, A.
T2  - Multimedia Tools and Applications
AB  - This article addresses the usage and scope of Big Data Analytics in video surveillance and its potential application areas. The current age of technology provides the users, ample opportunity to generate data at every instant of time. Thus in general, a tremendous amount of data is generated every instant throughout the world. Among them, amount of video data generated is having a major share. Education, healthcare, tours and travels, food and culture, geographical exploration, agriculture, safety and security, entertainment etc., are the key areas where a tremendous amount of video data is generated every day. A major share among it are taken by the daily used surveillance data captured from the security purpose camera and are recorded everyday. Storage, retrieval, processing, and analysis of such gigantic data require some specific platform. Big Data Analytics is such a platform, which eases this analysis task. The aim of this article is to investigate the current trends in video surveillance and its applications using Big Data Analytics. It also aims to focus on the research opportunities for visual surveillance in Big Data frameworks. We have reported here the state-of-the-art surveillance schemes for four different imaging modalities: conventional video scene, remotely sensed video, medical diagnostics, and underwater surveillance. Several works were reported in this research field over recent years and are categorized based on the challenges solved by the researchers. A list of tools used for video surveillance using Big Data framework is presented. Finally, research gaps in this domain are discussed. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.
DA  - 2019///
PY  - 2019
DO  - 10.1007/s11042-019-07793-w
VL  - 78
IS  - 18
SP  - 26129
EP  - 26162
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067242076&doi=10.1007%2fs11042-019-07793-w&partnerID=40&md5=0bf9819047111b4bb8059083e8aa85ea
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 45</p>
KW  - Advanced Analytics
KW  - Big data
KW  - Big Data
KW  - Big Data Analytics for video
KW  - Data Analytics
KW  - Data Science
KW  - Diagnosis
KW  - Digital storage
KW  - Imaging modality
KW  - Medical diagnostics
KW  - Medical imaging
KW  - Monitoring
KW  - Research opportunities
KW  - Safety and securities
KW  - Security systems
KW  - Surveillance data
KW  - Underwater imaging
KW  - Underwater surveillance
KW  - Video recording
KW  - Video surveillance
KW  - Visual surveillance
ER  - 

TY  - JOUR
TI  - Everything you wanted to know about smart cities
AU  - Mohanty, S.P.
AU  - Choppali, U.
AU  - Kougianos, E.
T2  - IEEE Consumer Electronics Magazine
DA  - 2016///
PY  - 2016
DO  - 10.1109/MCE.2016.2556879
VL  - 5
IS  - 3
SP  - 60
EP  - 70
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983385301&doi=10.1109%2fMCE.2016.2556879&partnerID=40&md5=0c92228489190124f0bb1a6ef185bb8f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 881</p>
ER  - 

TY  - JOUR
T2  - Present-vhdl
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092694404&partnerID=40&md5=de10e60f72f0282575a9e38e043799d9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Network functions virtualisation (NFV)
AU  - ETSI, E.
T2  - Network Functions Virtualisation (NFV)
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089134546&partnerID=40&md5=fbbd1cedb284c9785ff6bbb46474f298
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 24</p>
ER  - 

TY  - JOUR
TI  - Kubernetes
AU  - Kubernetes, T.
T2  - Kubernetes
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063890881&partnerID=40&md5=192a9cea6a3b7ce32adc05133ff9ddd7
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 40</p>
ER  - 

TY  - CONF
TI  - Distributed video transcoding on a heterogeneous computing platform
AU  - Chang, Z.H.
AU  - Jong, B.F.
AU  - Wong, W.J.
AU  - Wong, M.L.D.
T2  - 2016 IEEE Asia Pacific Conference on Circuits and Systems, APCCAS 2016
AB  - The requirement for real-time video transcoding systems has significantly increased due to the easy and widely available access to high resolution video streams and large-scale applications in recent years. In this paper, we propose a real-time distributed video transcoding system working on heterogeneous environment to tackle the high requirement of such applications. It allows multiple computers to be networked together to execute the same transcoding task so that the system can process more video streams in real time. Most importantly, the proposed method emphasizes on the velocity of the video data which involves the continuous input video stream and outcomes of transcoded video output stream that is accessible on-the-fly in contrast to the batch-oriented approach such as the MapReduce framework, where output latency can be significant. The performance of the proposed system can be further improved by using a more intelligent scheduler for video frames distribution. © 2016 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/APCCAS.2016.7803998
SP  - 444
EP  - 447
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011103309&doi=10.1109%2fAPCCAS.2016.7803998&partnerID=40&md5=05f0a2d6f6e17ff89055fbcdd34f5910
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 14</p>
KW  - Continuous input
KW  - Heterogeneous computing
KW  - Heterogeneous environments
KW  - Image coding
KW  - Large-scale applications
KW  - Mapreduce frameworks
KW  - Multiple computers
KW  - Real time systems
KW  - Real time videos
KW  - Video signal processing
KW  - Video streaming
KW  - Video-transcoding
ER  - 

TY  - JOUR
TI  - Understanding the impact of cloud patterns on performance and energy consumption
AU  - Khomh, F.
AU  - Abtahizadeh, S.A.
T2  - Journal of Systems and Software
AB  - Cloud patterns are abstract solutions to recurrent design problems in the cloud. Previous work has shown that these patterns can improve the Quality of Service (QoS) of cloud applications but their impact on energy consumption is still unknown. In this work, we conduct an empirical study on two multi-processing and multi-threaded applications deployed in the cloud, to investigate the individual and the combined impact of six cloud patterns (Local Database Proxy, Local Sharding Based Router, Priority Queue, Competing Consumers, Gatekeeper and Pipes and Filters) on the energy consumption. We measure the energy consumption using Power-API; an application programming interface (API) written in Java to monitor the energy consumed at the process-level. Results show that cloud patterns can effectively reduce the energy consumption of a cloud-based application, but not in all cases. In general, there appear to be a trade-off between an improved response time of the application and the energy consumption. Moreover, our findings show that migrating an application to a microservices architecture can improve the performance of the application, while significantly reducing its energy consumption. We summarize our contributions in the form of guidelines that developers and software architects can follow during the implementation of a cloud-based application. © 2018 Elsevier Inc.
DA  - 2018///
PY  - 2018
DO  - 10.1016/j.jss.2018.03.063
VL  - 141
SP  - 151
EP  - 170
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045462134&doi=10.1016%2fj.jss.2018.03.063&partnerID=40&md5=0d07058d3d463220ec67779d9373bb32
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 27</p>
KW  - Application programming interfaces (API)
KW  - Application programs
KW  - Cloud applications
KW  - Cloud computing
KW  - Cloud pattern
KW  - Cloud patterns
KW  - Cloud-based applications
KW  - Economic and social effects
KW  - Empirical studies
KW  - Energy consumption
KW  - Energy efficiency
KW  - Energy utilization
KW  - Green computing
KW  - Multi- threaded applications
KW  - Multi-processing
KW  - Performance optimization
KW  - Performance optimizations
KW  - Quality of service
KW  - Software architects
KW  - Software architecture
ER  - 

TY  - JOUR
TI  - A model-driven approach for continuous performance engineering in microservice-based systems
AU  - Cortellessa, V.
AU  - Di Pompeo, D.
AU  - Eramo, R.
AU  - Tucci, M.
T2  - Journal of Systems and Software
AB  - Microservices are quite widely impacting on the software industry in recent years. Rapid evolution and continuous deployment represent specific benefits of microservice-based systems, but they may have a significant impact on non-functional properties like performance. Despite the obvious relevance of this property, there is still a lack of systematic approaches that explicitly take into account performance issues in the lifecycle of microservice-based systems. In such a context of evolution and re-deployment, Model-Driven Engineering techniques can provide major support to various software engineering activities, and in particular they can allow managing the relationships between a running system and its architectural model. In this paper, we propose a model-driven integrated approach that exploits traceability relationships between the monitored data of a microservice-based running system and its architectural model to derive recommended refactoring actions that lead to performance improvement. The approach has been applied and validated on two microservice-based systems, in the domain of e-commerce and ticket reservation, respectively, whose architectural models have been designed in UML profiled with MARTE. © 2021 The Author(s)
DA  - 2022///
PY  - 2022
DO  - 10.1016/j.jss.2021.111084
VL  - 183
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117098236&doi=10.1016%2fj.jss.2021.111084&partnerID=40&md5=47e70199c168eed6e6f288b9b64e7894
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 40</p>
KW  - Architectural modeling
KW  - Continuous deployment
KW  - Life cycle
KW  - Microservice
KW  - Microservices
KW  - Model driven approach
KW  - Model-driven engineering
KW  - Model-driven Engineering
KW  - Performance
KW  - Performance engineering
KW  - Running systems
KW  - Software engineering
KW  - Software evolution
KW  - Software Evolution
KW  - Software refactoring
ER  - 

TY  - JOUR
TI  - Benchmarking microservice systems for software engineering research
AU  - Zhou, X.
T2  - 40Th ACM/IEEE International Conference on Software Engineering (ICSE)
DA  - 2018///
PY  - 2018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178238757&partnerID=40&md5=eb38025223cf5577fffa47a2a516480b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2</p>
ER  - 

TY  - JOUR
TI  - Appropriate statistics for ordinal level data: Should we really be using t-test and Cohen's d for evaluating group differences on the NSSE and other surveys?
AU  - Romano, J.
AU  - Kromrey, J.D.
AU  - Coraggio, J.
AU  - Skowronek, J.
T2  - Annual Meeting of the Florida Association of Institutional Research
DA  - 2006///
PY  - 2006
SP  - 1
EP  - 33
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960837892&partnerID=40&md5=97f4aa2643d0ecbcd14a9aad2f79763a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 485</p>
ER  - 

TY  - JOUR
TI  - Cautionary note on reporting eta-squared values from multifactor anova designs
AU  - Pierce, C.A.
AU  - Block, R.A.
AU  - Aguinis, H.
T2  - Educational and Psychological Measurement
AB  - The authors provide a cautionary note on reporting accurate eta-squared values from multifactor analysis of variance (ANOVA) designs. They reinforce the distinction between classical and partial eta-squared as measures of strength of association. They provide examples from articles published in premier psychology journals in which the authors erroneously reported partial eta-squared values as representing classical eta-squared values. Finally, they discuss broader impacts of inaccurately reported eta-squared values for theory development, meta-analytic reviews, and intervention programs.
DA  - 2004///
PY  - 2004
DO  - 10.1177/0013164404264848
VL  - 64
IS  - 6
SP  - 916
EP  - 924
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-10044284358&doi=10.1177%2f0013164404264848&partnerID=40&md5=005c2d60a207aab3a21166aee76f966e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 482</p>
KW  - Analysis of variance
KW  - Effect size
KW  - Eta-squared
KW  - Partial eta-squared
ER  - 

TY  - JOUR
TI  - A comparative study of methods for measurement of energy of computing
AU  - Fahad, M.
AU  - Shahid, A.
AU  - Manumachu, R.R.
AU  - Lastovetsky, A.
T2  - Energies
AB  - Energy of computing is a serious environmental concern and mitigating it is an important technological challenge. Accurate measurement of energy consumption during an application execution is key to application-level energy minimization techniques. There are three popular approaches to providing it: (a) System-level physical measurements using external power meters; (b) Measurements using on-chip power sensors and (c) Energy predictive models. In this work, we present a comprehensive study comparing the accuracy of state-of-the-art on-chip power sensors and energy predictive models against system-level physical measurements using external power meters, which we consider to be the ground truth. We show that the average error of the dynamic energy profiles obtained using on-chip power sensors can be as high as 73% and the maximum reaches 300% for two scientific applications, matrix-matrix multiplication and 2D fast Fourier transform for a wide range of problem sizes. The applications are executed on three modern Intel multicore CPUs, two Nvidia GPUs and an Intel Xeon Phi accelerator. The average error of the energy predictive models employing performance monitoring counters (PMCs) as predictor variables can be as high as 32% and the maximum reaches 100% for a diverse set of seventeen benchmarks executed on two Intel multicore CPUs (one Haswell and the other Skylake). We also demonstrate that using inaccurate energy measurements provided by on-chip sensors for dynamic energy optimization can result in significant energy losses up to 84%. We show that, owing to the nature of the deviations of the energy measurements provided by on-chip sensors from the ground truth, calibration can not improve the accuracy of the on-chip sensors to an extent that can allow them to be used in optimization of applications for dynamic energy. Finally, we present the lessons learned, our recommendations for the use of on-chip sensors and energy predictive models and future directions. © 2019 by the authors.
DA  - 2019///
PY  - 2019
DO  - 10.3390/en12112204
VL  - 12
IS  - 11
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067106808&doi=10.3390%2fen12112204&partnerID=40&md5=1b7b5b544c44153b6b62a7d40ef60e20
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 63</p>
KW  - Benchmarking
KW  - Computing power
KW  - Dynamics
KW  - Energy
KW  - Energy dissipation
KW  - Energy efficiency
KW  - Energy predictive model
KW  - Energy predictive models
KW  - Energy utilization
KW  - Fast Fourier transforms
KW  - GPU
KW  - Graphics processing unit
KW  - Green computing
KW  - Multi-cores
KW  - Multicore CPU
KW  - NVML
KW  - Performance monitoring counter
KW  - Performance monitoring counters
KW  - Performance-monitoring
KW  - Power
KW  - Power aensor
KW  - Power aensors
KW  - Power meters
KW  - Powermeter
KW  - Predictive models
KW  - Program processors
KW  - RAPL
KW  - Xeon phi
KW  - Xeon Phi
ER  - 

TY  - JOUR
TI  - Crossover Designs in Software Engineering Experiments: Benefits and Perils
AU  - Vegas, S.
AU  - Apa, C.
AU  - Juristo, N.
T2  - IEEE Transactions on Software Engineering
AB  - In experiments with crossover design subjects apply more than one treatment. Crossover designs are widespread in software engineering experimentation: they require fewer subjects and control the variability among subjects. However, some researchers disapprove of crossover designs. The main criticisms are: the carryover threat and its troublesome analysis. Carryover is the persistence of the effect of one treatment when another treatment is applied later. It may invalidate the results of an experiment. Additionally, crossover designs are often not properly designed and/or analysed, limiting the validity of the results. In this paper, we aim to make SE researchers aware of the perils of crossover experiments and provide risk avoidance good practices. We study how another discipline (medicine) runs crossover experiments. We review the SE literature and discuss which good practices tend not to be adhered to, giving advice on how they should be applied in SE experiments. We illustrate the concepts discussed analysing a crossover experiment that we have run. We conclude that crossover experiments can yield valid results, provided they are properly designed and analysed, and that, if correctly addressed, carryover is no worse than other validity threats. © 2015 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/TSE.2015.2467378
VL  - 42
IS  - 2
SP  - 120
EP  - 135
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962866799&doi=10.1109%2fTSE.2015.2467378&partnerID=40&md5=235974c8d613ff75a08f5ca437a1ba1b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 115</p>
KW  - carryover
KW  - controlled experiment
KW  - Controlled experiment
KW  - crossover design
KW  - Crossover design
KW  - Crossover experiments
KW  - data analysis
KW  - Data reduction
KW  - Design
KW  - Experimental software engineering
KW  - Good practices
KW  - Risk avoidance
KW  - Software engineering
KW  - Software engineering experiments
ER  - 

TY  - JOUR
TI  - How does docker affect energy consumption? Evaluating workloads in and out of Docker containers
AU  - Santos, E.A.
AU  - McLean, C.
AU  - Solinas, C.
AU  - Hindle, A.
T2  - Journal of Systems and Software
AB  - Context: Virtual machines provide isolation of services at the cost of hypervisors and more resource usage. This spurred the growth of systems like Docker that enable single hosts to isolate several applications, similar to VMs, within a low-overhead abstraction called containers. Motivation: Although containers tout low overhead performance, how much do they increase energy use? Methodology: This work statistically compares the energy consumption of three application workloads in Docker and on bare-metal Linux. Results: In all cases, there was a statistically significant (t-test and Wilcoxon p <.05) increase in energy consumption when running tests in Docker, mostly due to the performance of I/O system calls. Developers worried about I/O overhead could consider baremetal deployments over Docker container deployments. © 2018
DA  - 2018///
PY  - 2018
DO  - 10.1016/j.jss.2018.07.077
VL  - 146
SP  - 14
EP  - 25
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053352568&doi=10.1016%2fj.jss.2018.07.077&partnerID=40&md5=a866674254e6fe1a8729a6ef3fd1a1fc
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 27</p>
KW  - Bare metal linux
KW  - Cloud computing
KW  - Computer operating systems
KW  - Containerization
KW  - Containers
KW  - Docker
KW  - Docker containers
KW  - Energy consumption
KW  - Energy utilization
KW  - Green computing
KW  - Microservice
KW  - Resource usage
KW  - Running tests
KW  - System calls
KW  - Virtualization
ER  - 

TY  - JOUR
TI  - Green IT and Green Software
AU  - Verdecchia, R.
AU  - Lago, P.
AU  - Ebert, C.
AU  - De Vries, C.
T2  - IEEE Software
AB  - Software and IT usage are continuously growing to keep our society active and manage our individual lives. But as they grow, their energy demand is exploding. By 2030, data centers alone will already consume some 10% of the global electricity.1 Including the Internet, telecommunications, and embedded devices, the energy consumption will be one-third of the global demand. Understanding that end users only consume what we offer, it is the community of software developers who must become active in ecologic behaviors. Green IT is the call of today. Each single line of code that we develop today may still be running years from now on zillions of processors, eating energy and contributing to global climate change. © 1984-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/MS.2021.3102254
VL  - 38
IS  - 6
SP  - 7
EP  - 15
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118591280&doi=10.1109%2fMS.2021.3102254&partnerID=40&md5=755c4fe29383d9c14162132cd01c9457
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 63</p>
KW  - Climate change
KW  - Datacenter
KW  - Embedded device
KW  - End-users
KW  - Energy demands
KW  - Energy utilization
KW  - Energy-consumption
KW  - Global demand
KW  - Green computing
KW  - Internet devices
KW  - Line of codes
KW  - Software developer
KW  - Telecommunication devices
ER  - 

TY  - JOUR
TI  - Watts up? Pro AC power meter for automated energy recording
AU  - Hirst, J.M.
AU  - Miller, J.R.
AU  - Kaplan, B.A.
AU  - Reed, D.D.
T2  - Behavior Analysis in Practice
DA  - 2013///
PY  - 2013
VL  - 6
IS  - 1
SP  - 82
EP  - 95
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889803805&partnerID=40&md5=a40fa7422f155c37bc1d5e5fc75b13bf
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 25</p>
ER  - 

TY  - CONF
TI  - Metrics of energy consumption in software systems: A systematic literature review
AU  - Ergasheva, S.
AU  - Khomyakov, I.
AU  - Kruglov, A.
AU  - Succil, G.
T2  - IOP Conference Series: Earth and Environmental Science
AB  - The current situation when using tight time frames and frequently changing requirements when creating software dictates the need to create a system for monitoring energy consumption at any stage of production of a software product. At the first stage, we need to evaluate the state-of-the-art on this topic. To this goal, we conducted a systematic literature review. During the review more than 500 studies were observed and 124 of them were selected for detailed analysis. Among these papers, 169 metrics were derived and assessed from the point of their applicability within invasive software development process analysis. The study demonstrates the relevance of the questions posed and shows the immaturity of the area. There is no evolutionary study and the possibility of assessment at any stage of the development of a software product. The data show the importance and relevance of technical work and the importance of its further development. © 2020 IOP Publishing Ltd. All rights reserved.
DA  - 2020///
PY  - 2020
DO  - 10.1088/1755-1315/431/1/012051
VL  - 431
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080894336&doi=10.1088%2f1755-1315%2f431%2f1%2f012051&partnerID=40&md5=30df7296a80987f34df5dc24e03f247c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 17</p>
KW  - Current situation
KW  - Energy utilization
KW  - Invasive software
KW  - Software design
KW  - Software products
KW  - Software systems
KW  - State of the art
KW  - Systematic literature review
KW  - Technical work
KW  - Time frame
ER  - 

TY  - JOUR
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178229172&partnerID=40&md5=517c77504ebd4ed6df0e1af428066f78
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - E3: Energy-efficient microservices on smartnic-accelerated servers
AU  - Liu, M.
AU  - Peter, S.
AU  - Krishnamurthy, A.
AU  - Phothilimthana, P.M.
T2  - Proceedings of the 2019 USENIX Annual Technical Conference, USENIX ATC 2019
AB  - We investigate the use of SmartNIC-accelerated servers to execute microservice-based applications in the data center. By offloading suitable microservices to the SmartNIC’s low-power processor, we can improve server energy-efficiency without latency loss. However, as a heterogeneous computing substrate in the data path of the host, SmartNICs bring several challenges to a microservice platform: network traffic routing and load balancing, microservice placement on heterogeneous hardware, and contention on shared SmartNIC resources. We present E3, a microservice execution platform for SmartNIC-accelerated servers. E3 follows the design philosophies of the Azure Service Fabric microservice platform and extends key system components to a SmartNIC to address the above-mentioned challenges. E3 employs three key techniques: ECMP-based load balancing via SmartNICs to the host, network topology-aware microservice placement, and a data-plane orchestrator that can detect SmartNIC overload. Our E3 prototype using Cavium LiquidIO SmartNICs shows that SmartNIC offload can improve cluster energy-efficiency up to 3× and cost efficiency up to 1.9× at up to 4% latency cost for common microservices, including real-time analytics, an IoT hub, and virtual network functions. © Proceedings of the 2019 USENIX Annual Technical Conference, USENIX ATC 2019. All rights reserved.
DA  - 2019///
PY  - 2019
SP  - 363
EP  - 378
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074451146&partnerID=40&md5=2fc9b5be570d88098c87c71ce4f4a3b6
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 119</p>
KW  - Design philosophy
KW  - Energy efficiency
KW  - Execution platforms
KW  - Green computing
KW  - Heterogeneous computing
KW  - Heterogeneous hardware
KW  - Key system components
KW  - Low power processors
KW  - Philosophical aspects
KW  - Real-time analytics
KW  - Servers
KW  - Virtual networks
ER  - 

TY  - JOUR
TI  - Software modeling and measurement: The goal/question/metric paradigm
AU  - Basili, V.R.
T2  - Software Modeling and Measurement: The Goal/Question/Metric Paradigm
DA  - 1992///
PY  - 1992
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0005382508&partnerID=40&md5=3853a47ef5cf11bbbe765454cf429ebf
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 472</p>
ER  - 

TY  - CONF
TI  - Assessing the performance impact of service monitoring
AU  - Heward, G.
AU  - Müller, I.
AU  - Han, J.
AU  - Schneider, J.-G.
AU  - Versteeg, S.
T2  - Proceedings of the Australian Software Engineering Conference, ASWEC
AB  - Service monitoring is an essential part of service-oriented software systems and is required for meeting regulatory requirements, verifying compliance to service-level agreements, optimising system performance, and minimising the cost of hosting Web services. However, service monitoring comes with a cost, including a performance impact on the monitored services and systems. Therefore, it is important to deploy the right level of monitoring at the appropriate time and location in order to achieve the objectives of monitoring whilst minimising its impact on services and systems. Although there have been many efforts to create Web services monitoring techniques and frameworks, there has been limited work in quantifying the impact of Web service monitoring. In this paper, we report on experiments assessing the performance impact of service monitoring under typical system monitoring settings. The performance impact of monitoring method, monitor location, monitor processing capability, and monitoring mode are taken into consideration. Based on the experimental results, we advise on the most appropriate ways to deploy service monitoring. © 2010 IEEE.
DA  - 2010///
PY  - 2010
DO  - 10.1109/ASWEC.2010.28
SP  - 192
EP  - 201
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954345094&doi=10.1109%2fASWEC.2010.28&partnerID=40&md5=c113ed9e093c2cba6090e1d5fbae6d4f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 15</p>
KW  - Computer software
KW  - Monitor location
KW  - Monitoring
KW  - Monitoring methods
KW  - Monitoring mode
KW  - Monitoring techniques
KW  - Performance impact
KW  - Processing capability
KW  - Regulatory requirements
KW  - Service interceptors
KW  - Service Level Agreements
KW  - Service monitoring
KW  - Service Oriented
KW  - Software systems
KW  - System monitoring
KW  - Web service monitoring
KW  - Web services
ER  - 

TY  - JOUR
TI  - Architecting with microservices: A systematic mapping study
AU  - Di Francesco, P.
AU  - Lago, P.
AU  - Malavolta, I.
T2  - Journal of Systems and Software
AB  - Context: A microservice architecture is composed of a set of small services, each running in its own process and communicating with lightweight mechanisms. Many aspects on architecting with microservices are still unexplored and existing research is still far from being crispy clear. Objective: We aim at identifying, classifying, and evaluating the state of the art on architecting with microservices from the following perspectives: publication trends, focus of research, and potential for industrial adoption. Method: We apply the systematic mapping methodology. We rigorously selected 103 primary studies and we defined and applied a classification framework to them for extracting key information for subsequent analysis. We synthesized the obtained data and produced a clear overview of the state of the art. Results: This work contributes with (i) a classification framework for research studies on architecting with microservices, (ii) a systematic map of current research of the field, (iii) an evaluation of the potential for industrial adoption of research results, and (iv) a discussion of emerging findings and implications for future research. Conclusion: This study provides a solid, rigorous, and replicable picture of the state of the art on architecting with microservices. Its results can benefit both researchers and practitioners of the field. © 2019 Elsevier Inc.
DA  - 2019///
PY  - 2019
DO  - 10.1016/j.jss.2019.01.001
VL  - 150
SP  - 77
EP  - 97
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060521992&doi=10.1016%2fj.jss.2019.01.001&partnerID=40&md5=95d7beacebfc79566f4a506b7a7180ba
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 194</p>
KW  - Classification (of information)
KW  - Classification framework
KW  - Focus of researches
KW  - Implications for futures
KW  - Industrial adoption
KW  - Industrial research
KW  - Mapping
KW  - Microservices
KW  - Software architecture
KW  - State of the art
KW  - Systematic mapping
KW  - Systematic mapping studies
KW  - Systematic mapping study
ER  - 

TY  - JOUR
TI  - Docker: Lightweight linux containers for consistent development and deployment
AU  - Merkel, D.
T2  - Linux Journal
DA  - 2014///
PY  - 2014
VL  - 2014
IS  - 239
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926444656&partnerID=40&md5=ffabb5619a892434a0212f02e9fd31f5
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2855</p>
ER  - 

TY  - JOUR
TI  - Efficient Multi-User Computation Offloading for Mobile-Edge Cloud Computing
AU  - Chen, X.
AU  - Jiao, L.
AU  - Li, W.
AU  - Fu, X.
T2  - IEEE/ACM Transactions on Networking
AB  - Mobile-edge cloud computing is a new paradigm to provide cloud computing capabilities at the edge of pervasive radio access networks in close proximity to mobile users. In this paper, we first study the multi-user computation offloading problem for mobile-edge cloud computing in a multi-channel wireless interference environment. We show that it is NP-hard to compute a centralized optimal solution, and hence adopt a game theoretic approach for achieving efficient computation offloading in a distributed manner. We formulate the distributed computation offloading decision making problem among mobile device users as a multi-user computation offloading game. We analyze the structural property of the game and show that the game admits a Nash equilibrium and possesses the finite improvement property. We then design a distributed computation offloading algorithm that can achieve a Nash equilibrium, derive the upper bound of the convergence time, and quantify its efficiency ratio over the centralized optimal solutions in terms of two important performance metrics. We further extend our study to the scenario of multi-user computation offloading in the multi-channel wireless contention environment. Numerical results corroborate that the proposed algorithm can achieve superior computation offloading performance and scale well as the user size increases. © 1993-2012 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/TNET.2015.2487344
VL  - 24
IS  - 5
SP  - 2795
EP  - 2808
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015384872&doi=10.1109%2fTNET.2015.2487344&partnerID=40&md5=2576cf988405bfd1006fe6eb101acf74
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2461</p>
KW  - Computation offloading
KW  - Computation theory
KW  - Computational efficiency
KW  - Computer games
KW  - Decision making
KW  - Decision-making problem
KW  - Distributed computations
KW  - Edge clouds
KW  - Efficient computation
KW  - game theory
KW  - Game theory
KW  - Mobile cloud computing
KW  - Mobile devices
KW  - Mobile telecommunication systems
KW  - mobile-edge cloud computing
KW  - Nash equilibria
KW  - Nash equilibrium
KW  - Optimal systems
KW  - Optimization
KW  - Radio access networks
KW  - Wireless interference
ER  - 

TY  - CONF
TI  - CloneCloud: Elastic execution between mobile device and cloud
AU  - Chun, B.-G.
AU  - Ihm, S.
AU  - Maniatis, P.
AU  - Naik, M.
AU  - Patti, A.
T2  - EuroSys'11 - Proceedings of the EuroSys 2011 Conference
AB  - Mobile applications are becoming increasingly ubiquitous and provide ever richer functionality on mobile devices. At the same time, such devices often enjoy strong connectivity with more powerful machines ranging from laptops and desktops to commercial clouds. This paper presents the design and implementation of CloneCloud, a system that automatically transforms mobile applications to benefit from the cloud. The system is a flexible application partitioner and execution runtime that enables unmodified mobile applications running in an application-level virtual machine to seamlessly off-load part of their execution from mobile devices onto device clones operating in a computational cloud. CloneCloud uses a combination of static analysis and dynamic profiling to partition applications automatically at a fine granularity while optimizing execution time and energy use for a target computation and communication environment. At runtime, the application partitioning is effected by migrating a thread from the mobile device at a chosen point to the clone in the cloud, executing there for the remainder of the partition, and re-integrating the migrated thread back to the mobile device. Our evaluation shows that CloneCloud can adapt application partitioning to different environments, and can help some applications achieve as much as a 20x execution speed-up and a 20-fold decrease of energy spent on the mobile device. Copyright © 2011 ACM.
DA  - 2011///
PY  - 2011
DO  - 10.1145/1966445.1966473
SP  - 301
EP  - 314
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955951954&doi=10.1145%2f1966445.1966473&partnerID=40&md5=10d63d8e01e56a340a75c8b6d7c61f02
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1664</p>
KW  - Cloning
KW  - Cloud computing
KW  - Dynamic analysis
KW  - Laptop computers
KW  - Migration
KW  - Mobile cloud computing
KW  - Mobile devices
KW  - Mobile telecommunication systems
KW  - Offloading
KW  - Partitioning
KW  - Portable equipment
KW  - Smartphones
KW  - Static analysis
ER  - 

TY  - CONF
TI  - CoMET: Code offload by migrating execution transparently
AU  - Gordon, M.S.
AU  - Anoushe Jamshidi, D.
AU  - Mahlke, S.
AU  - Morley Mao, Z.
AU  - Chen, X.
T2  - Proceedings of the 10th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2012
AB  - In this paper we introduce a runtime system to allow unmodified multi-threaded applications to use multiple machines. The system allows threads to migrate freely between machines depending on the workload. Our prototype, COMET (Code Offload by Migrating Execution Transparently), is a realization of this design built on top of the Dalvik Virtual Machine. COMET leverages the underlying memory model of our runtime to implement distributed shared memory (DSM) with as few interactions between machines as possible. Making use of a new VM-synchronization primitive, COMET imposes little restriction on when migration can occur. Additionally, enough information is maintained so one machine may resume computation after a network failure. We target our efforts towards augmenting smartphones or tablets with machines available in the network. We demonstrate the effectiveness of COMET on several real applications available on Google Play. These applications include image editors, turn-based games, a trip planner, and math tools. Utilizing a server-class machine, COMET can offer significant speed-ups on these real applications when run on a modern smartphone. With WiFi and 3G networks, we observe geometric mean speed-ups of 2.88X and 1.27X relative to the Dalvik interpreter across the set of applications with speed-ups as high as 15X on some applications. © 2012 by The USENIX Association. All Rights Reserved.
DA  - 2012///
PY  - 2012
SP  - 93
EP  - 106
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076923128&partnerID=40&md5=9a5060db8b9efcdfd44fe12dad2d251e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 328</p>
KW  - Dalvik virtual machines
KW  - Distributed shared memory
KW  - Machine shops
KW  - Memory architecture
KW  - Multi- threaded applications
KW  - Multiple machine
KW  - Network failure
KW  - Real applications
KW  - Runtime systems
KW  - Smartphones
KW  - Synchronization primitive
KW  - Systems analysis
KW  - Wi-Fi
ER  - 

TY  - JOUR
TI  - AIOLOS: Middleware for improving mobile application performance through cyber foraging
AU  - Verbelen, T.
AU  - Simoens, P.
AU  - De Turck, F.
AU  - Dhoedt, B.
T2  - Journal of Systems and Software
AB  - As the popularity of smartphones and tablets increases, the mobile platform is becoming a very important target for application developers. Despite recent advances in mobile hardware, most mobile devices fail to execute complex multimedia applications (such as image processing) with an acceptable level of user experience. Cyber foraging is a well-known computing technique to enhance the capabilities of mobile devices, where the mobile device offloads parts of the application to a nearby discovered server in the network. Although first introduced in 2001, cyber foraging is still not widely adopted in current smartphone platforms or applications. In this respect, two major challenges are to be tackled. First, a suitable adaptive decision engine is needed to determine the optimal offloading decision, that takes into account the potentially high and variable latency between the device and the server. Second, an integrated cyber foraging platform with sufficient support for application developers is not publicly available on popular mobile platforms such as Android. In this paper, we present AIOLOS, a mobile middleware framework for cyber foraging on the Android platform. AIOLOS uses an estimation model that takes into account server resources and network state to decide at runtime whether or not a method call should be offloaded. We also introduce developer tools to integrate the AIOLOS framework in the Android platform, enabling easy development of cyber foraging enabled applications. A prototype implementation is presented and evaluated in detail by means of both a chess application and a newly developed photo editor application. © 2012 Elsevier Inc. All rights reserved.
DA  - 2012///
PY  - 2012
DO  - 10.1016/j.jss.2012.06.011
VL  - 85
IS  - 11
SP  - 2629
EP  - 2639
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865204463&doi=10.1016%2fj.jss.2012.06.011&partnerID=40&md5=dee2b4b554997ad0987b734a2861603d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 74</p>
KW  - Application developers
KW  - Computing techniques
KW  - Cyber foraging
KW  - Decision engines
KW  - Distributed systems
KW  - Estimation models
KW  - Image processing
KW  - Middleware
KW  - Mobile applications
KW  - Mobile computing
KW  - Mobile devices
KW  - Mobile hardware
KW  - Mobile middleware
KW  - Mobile phones
KW  - Mobile platform
KW  - Multimedia applications
KW  - Network state
KW  - Photo editors
KW  - Prototype implementations
KW  - Robots
KW  - Runtimes
KW  - Server resources
KW  - Smartphones
KW  - User experience
ER  - 

TY  - JOUR
TI  - Fog Computing: Current Research and Future Challenges
AU  - Gedeon, J.
AU  - Heuschkel, J.
AU  - Wang, L.
AU  - Mühlhäuser, M.
T2  - KuVS-Fachgespräch Fog Comput
DA  - 2018///
PY  - 2018
VL  - 1
SP  - 1
EP  - 4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055787884&partnerID=40&md5=f3ed5d093c35fbb1e5f2369913f054ca
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 13</p>
ER  - 

TY  - CONF
TI  - MAUI: Making smartphones last longer with code offload
AU  - Cuervoy, E.
AU  - Balasubramanian, A.
AU  - Cho, D.-K.
AU  - Wolman, A.
AU  - Saroiu, S.
AU  - Chandra, R.
AU  - Bahlx, P.
T2  - MobiSys'10 - Proceedings of the 8th International Conference on Mobile Systems, Applications, and Services
AB  - This paper presents MAUI, a system that enables fine-grained energy-aware offload of mobile code to the infrastructure. Previous approaches to these problems either relied heavily on programmer support to partition an application, or they were coarse-grained requiring full process (or full VM) migration. MAUI uses the benefits of a managed code environment to offer the best of both worlds: it supports fine-grained code offload to maximize energy savings with minimal burden on the programmer. MAUI decides at runtime which methods should be remotely executed, driven by an optimization engine that achieves the best energy savings possible under the mobile device's current connectivity constrains. In our evaluation, we show that MAUI enables: 1) a resource-intensive face recognition application that consumes an order of magnitude less energy, 2) a latency-sensitive arcade game application that doubles its refresh rate, and 3) a voice-based language translation application that bypasses the limitations of the smartphone environment by executing unsupported components remotely. Copyright 2010 ACM.
DA  - 2010///
PY  - 2010
DO  - 10.1145/1814433.1814441
SP  - 49
EP  - 62
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954993714&doi=10.1145%2f1814433.1814441&partnerID=40&md5=c41bbe80921b5bbf2f63ed0803a1c662
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2078</p>
KW  - Coarse-grained
KW  - Code offload
KW  - Energy
KW  - Energy  savings
KW  - Energy aware
KW  - Energy management
KW  - Energy-savings
KW  - Fine grained
KW  - Mobile codes
KW  - Partitioning
KW  - Smart phones
KW  - Smartphones
ER  - 

TY  - JOUR
TI  - Microservices: A definition of this new architectural term
AU  - Lewis, J.
AU  - Fowler, M.
T2  - Microservices: A Definition of This New Architectural Term
DA  - 2014///
PY  - 2014
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964234114&partnerID=40&md5=fd3de0bd7b34545e47485cf075757737
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 579</p>
ER  - 

TY  - CONF
TI  - ThinkAir: Dynamic resource allocation and parallel execution in the cloud for mobile code offloading
AU  - Kosta, S.
AU  - Aucinas, A.
AU  - Hui, P.
AU  - Mortier, R.
AU  - Zhang, X.
T2  - Proceedings - IEEE INFOCOM
AB  - Smartphones have exploded in popularity in recent years, becoming ever more sophisticated and capable. As a result, developers worldwide are building increasingly complex applications that require ever increasing amounts of computational power and energy. In this paper we propose ThinkAir, a framework that makes it simple for developers to migrate their smartphone applications to the cloud. ThinkAir exploits the concept of smartphone virtualization in the cloud and provides method-level computation offloading. Advancing on previous work, it focuses on the elasticity and scalability of the cloud and enhances the power of mobile cloud computing by parallelizing method execution using multiple virtual machine (VM) images. We implement ThinkAir and evaluate it with a range of benchmarks starting from simple micro-benchmarks to more complex applications. First, we show that the execution time and energy consumption decrease two orders of magnitude for a N-queens puzzle application and one order of magnitude for a face detection and a virus scan application. We then show that a parallelizable application can invoke multiple VMs to execute in the cloud in a seamless and on-demand manner such as to achieve greater reduction on execution time and energy consumption. We finally use a memory-hungry image combiner tool to demonstrate that applications can dynamically request VMs with more computational power in order to meet their computational requirements. © 2012 IEEE.
DA  - 2012///
PY  - 2012
DO  - 10.1109/INFCOM.2012.6195845
SP  - 945
EP  - 953
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861596582&doi=10.1109%2fINFCOM.2012.6195845&partnerID=40&md5=2f2ea3dde7f0b12d746a891601f06d9f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1037</p>
KW  - Benchmarking
KW  - Complex applications
KW  - Computation offloading
KW  - Computational power
KW  - Computational requirements
KW  - Dynamic resource allocations
KW  - Elasticity
KW  - Energy utilization
KW  - Execution time
KW  - Mobile codes
KW  - Orders of magnitude
KW  - Parallel executions
KW  - Parallelizing
KW  - Signal encoding
KW  - Smartphones
KW  - Virtual machines
KW  - Virtualizations
KW  - Virus scan
ER  - 

TY  - CONF
TI  - Bringing the cloud to the edge
AU  - Chang, H.
AU  - Hari, A.
AU  - Mukherjee, S.
AU  - Lakshman, T.V.
T2  - Proceedings - IEEE INFOCOM
AB  - Edge services become increasingly important as the Internet transforms into an Internet of Things (IoT). Edge services require bounded latency, bandwidth reduction between the edge and the core, service resiliency with graceful degradation, and access to resources visible only inside the NATed and secured edge networks. While the data center based cloud excels at providing general purpose computation/storage at scale, it is not suitable for edge services. We present a new model for cloud computing, which we call the Edge Cloud, that addresses edge computing specific issues by augmenting the traditional data center cloud model with service nodes placed at the network edges. We describe the architecture of the Edge Cloud and its implementation as an overlay hybrid cloud using the industry standard OpenStack cloud management framework. We demonstrate the advantages garnered by two new classes of applications enabled by the Edge Cloud - a highly accurate indoor localization that saves on latency, and a scalable and resilient video monitoring that saves on bandwidth. © 2014 IEEE.
DA  - 2014///
PY  - 2014
DO  - 10.1109/INFCOMW.2014.6849256
SP  - 346
EP  - 351
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904465488&doi=10.1109%2fINFCOMW.2014.6849256&partnerID=40&md5=6dd481d13ecb005f79ba94ceadcf19cd
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 165</p>
KW  - Access to resources
KW  - Bandwidth
KW  - Bandwidth reductions
KW  - Cloud computing
KW  - General-purpose computations
KW  - Graceful degradation
KW  - Indoor localization
KW  - Industry standards
KW  - Internet of things
KW  - Internet of Things (IOT)
KW  - Traditional data centers
ER  - 

TY  - JOUR
TI  - A survey and taxonomy of cyber foraging of mobile devices
AU  - Sharifi, M.
AU  - Kafaie, S.
AU  - Kashefi, O.
T2  - IEEE Communications Surveys and Tutorials
AB  - With the ever-increasing advancement of mobile device technology and their pervasive usage, users expect to run their applications on mobile devices and get the same performance as if they used to run their applications on powerful non-mobile computers. There is a challenge though in that mobile devices deliver lower performance than traditional less-constrained and non-mobile computers because they are constrained by weight, size, and mobility in spite of all their advancements in recent years. One of the most common solutions that has ameliorated this performance disparity is cyber foraging, wherein nearby non-mobile computers called surrogates are utilized to run the whole or parts of applications on behalf of mobile devices. In this paper, we present a survey of cyber foraging as a solution to resolve the challenges of computing on resource-constrained mobile devices. We also explain the most notable cyber foraging systems and present a categorization of existing cyber foraging approaches considering their type of dynamicity, granularity, metrics used, surrogate types and scale, location of their decision maker unit, remoteness of execution, migration support, and their overheads. © 2012 IEEE.
DA  - 2012///
PY  - 2012
DO  - 10.1109/SURV.2011.111411.00016
VL  - 14
IS  - 4
SP  - 1232
EP  - 1243
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897999903&doi=10.1109%2fSURV.2011.111411.00016&partnerID=40&md5=a4055a5a82325e2ee3d717288f5c8301
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 82</p>
KW  - Cyber foraging
KW  - Cyber Foraging
KW  - Decision makers
KW  - Device technologies
KW  - Mobile devices
KW  - Mobile Devices
KW  - Resource-Constrained Computing
KW  - Surveys
KW  - Taxonomies
KW  - Taxonomy
ER  - 

TY  - JOUR
TI  - Microservices Architecture Enables DevOps: Migration to a Cloud-Native Architecture
AU  - Balalaie, A.
AU  - Heydarnoori, A.
AU  - Jamshidi, P.
T2  - IEEE Software
AB  - When DevOps started gaining momentum in the software industry, one of the first service-based architectural styles to be introduced, be applied in practice, and become popular was microservices. Migrating monolithic architectures to cloud-native architectures such as microservices reaps many benefits, such as adaptability to technological changes and independent resource management for different system components. This article reports on experiences and lessons learned during incremental migration and architectural refactoring of a commercial MBaaS (mobile back end as a service) to microservices. It explains how adopting DevOps facilitated a smooth migration. Furthermore, the researchers transformed their experiences in different projects into reusable migration practices, resulting in microservices migration patterns. This article is part of a theme issue on DevOps. The Web extra at https://youtu.be/MF3-dKTCQ88 is an audio recording of Brian Brannon speaking with author Pooyan Jamshidi and James Lewis, principal engineer at ThoughtWorks, about DevOps and microservices architecture. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/MS.2016.64
VL  - 33
IS  - 3
SP  - 42
EP  - 52
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968796741&doi=10.1109%2fMS.2016.64&partnerID=40&md5=428311b5b8f1d1fed8654690c5a60fe0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 590</p>
KW  - architectural refactoring
KW  - cloud computing
KW  - Cloud computing
KW  - Computer architecture
KW  - DevOps
KW  - HTTP
KW  - microservices
KW  - migration pattern
KW  - Migration patterns
KW  - mobile back end as a service
KW  - Refactorings
KW  - Software design
KW  - software development
KW  - software engineering
KW  - Software engineering
ER  - 

TY  - JOUR
TI  - The Design and Architecture of Microservices
AU  - Sill, A.
T2  - IEEE Cloud Computing
AB  - Microservices are sweeping through cloud design architectures, at once embodying new trends and making use of previous paradigms. This column explores the basis for these trends in both modern and historical standards, and sets out a direction for the future of microservices development. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/MCC.2016.111
VL  - 3
IS  - 5
SP  - 76
EP  - 80
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997402736&doi=10.1109%2fMCC.2016.111&partnerID=40&md5=f4a9d927dbbe0c63632bfb31c6c8a4d1
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 114</p>
KW  - architecture
KW  - Architecture
KW  - automation
KW  - Automation
KW  - cloud
KW  - Clouds
KW  - Computer architecture
KW  - containers
KW  - Containers
KW  - data
KW  - design
KW  - Design
KW  - Design architecture
KW  - Microservices
KW  - Network architecture
KW  - networks
KW  - Networks (circuits)
KW  - standards
KW  - Standards
ER  - 

TY  - JOUR
TI  - The emergence of edge computing
AU  - Satyanarayanan, M.
T2  - Computer
AB  - Industry investment and research interest in edge computing, in which computing and storage nodes are placed at the Internet's edge in close proximity to mobile devices or sensors, have grown dramatically in recent years. This emerging technology promises to deliver highly responsive cloud services for mobile computing, scalability and privacy-policy enforcement for the Internet of Things, and the ability to mask transient cloud outages. The web extra at www.youtube.com/playlist?list=PLmrZVvFtthdP3fwHPy-4d61oDvQY-RBgS includes a five-video playlist demonstrating proof-of-concept implementations for three tasks: assembling 2D Lego models, freehand sketching, and playing Ping-Pong. © 1970-2012 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/MC.2017.9
VL  - 50
IS  - 1
SP  - 30
EP  - 39
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009446769&doi=10.1109%2fMC.2017.9&partnerID=40&md5=cd3428df30a93363cd7e8db5cabdde75
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2049</p>
KW  - Close proximity
KW  - Cloud services
KW  - Computer science
KW  - Computers
KW  - Edge computing
KW  - Emerging technologies
KW  - Freehand sketching
KW  - nocv1
KW  - Privacy policies
KW  - Proof of concept
KW  - Research interests
ER  - 

TY  - CONF
TI  - ParaDrop: Enabling lightweight multi-tenancy at the network's extreme edge
AU  - Liu, P.
AU  - Willis, D.
AU  - Banerjee, S.
T2  - Proceedings - 1st IEEE/ACM Symposium on Edge Computing, SEC 2016
AB  - We introduce ParaDrop, a specific edge computing platform that provides computing and storage resources at the 'extreme' edge of the network allowing third-party developers to flexibly create new types of services. This extreme edge of the network is the WiFi Access Point (AP) or the wireless gateway through which all end-device traffic (personal devices, sensors, etc.) passes through. ParaDrop's focus on WiFi APs also stems from the fact that the WiFi AP has unique contextual knowledge of its end-devices (e.g., proximity, channel characteristics) that are lost as we get deeper into the network. While different variations and implementations of edge computing platforms have been created over the last decade, ParaDrop focuses on specific design issues around how to structure an architecture, a programming interface, and orchestration framework through which such edge computing services can be dynamically created, installed, and revoked. ParaDrop consists of the following three main components: a flexible hosting substrate in the WiFi APs that supports multi-tenancy, a cloud-based backend through which such computations are orchestrated across many ParaDrop APs, and an API through which third-party developers can deploy and manage their computing functions across such different ParaDrop APs. We have implemented and deployed the entire ParaDrop framework and, in this paper, describe its overall architecture and our initial experiences using it as an edge computing platform. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/SEC.2016.39
SP  - 1
EP  - 13
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010831253&doi=10.1109%2fSEC.2016.39&partnerID=40&md5=9f17c934cdc3359f5b704e175ca7ef65
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 120</p>
KW  - Channel characteristics
KW  - Computer architecture
KW  - Computing functions
KW  - Contextual knowledge
KW  - Gateways (computer networks)
KW  - Network architecture
KW  - Personal devices
KW  - Programming interface
KW  - Storage resources
KW  - Wi-Fi
KW  - Wi-fi access points
KW  - Wireless gateways
KW  - Wireless local area networks (WLAN)
ER  - 

TY  - JOUR
TI  - Cyber Foraging: Fifteen Years Later
AU  - Balan, R.K.
AU  - Flinn, J.
T2  - IEEE Pervasive Computing
AB  - Revisiting Mahadev Satyanarayanan's original vision of cyber foraging and reflecting on the last 15 years of related research, the authors discuss the major accomplishments achieved as well as remaining challenges. They also look to current and future applications that could provide compelling application scenarios for making cyber foraging a widely deployed technology. This article is part of a special issue on pervasive computing revisited. © 2002-2012 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/MPRV.2017.2940972
VL  - 16
IS  - 3
SP  - 24
EP  - 30
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029352267&doi=10.1109%2fMPRV.2017.2940972&partnerID=40&md5=ba1a595b8310c9a083ba8f9cf8cd2bc1
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 11</p>
KW  - Application scenario
KW  - communication
KW  - Communication
KW  - Computer applications
KW  - computer systems organization
KW  - Computer systems organization
KW  - Cyber foraging
KW  - Future applications
KW  - Information technology
KW  - Information Technology
KW  - mobile
KW  - networking
KW  - pervasive computing
KW  - Software engineering
KW  - special-purpose and application-based systems
KW  - ubiquitous computing
KW  - Ubiquitous computing
ER  - 

TY  - CONF
TI  - Odessa: Enabling interactive perception applications on mobile devices
AU  - Ra, M.-R.
AU  - Sheth, A.
AU  - Mummert, L.
AU  - Pillai, P.
AU  - Wetherall, D.
AU  - Govindan, R.
T2  - MobiSys'11 - Compilation Proceedings of the 9th International Conference on Mobile Systems, Applications and Services and Co-located Workshops
AB  - Resource constrained mobile devices need to leverage computation on nearby servers to run responsive applications that recognize objects, people, or gestures from real-time video. The two key questions that impact performance are what computation to offload, and how to structure the parallelism across the mobile device and server. To answer these questions, we develop and evaluate three interactive perceptual applications. We find that offloading and parallelism choices should be dynamic, even for a given application, as performance depends on scene complexity as well as environmental factors such as the network and device capabilities. To this end we develop Odessa, a novel, lightweight, runtime that automatically and adaptively makes offloading and parallelism decisions for mobile interactive perception applications. Our evaluation shows that the incremental greedy strategy of Odessa converges to an operating point that is close to an ideal offline partitioning. It provides more than a 3x improvement in application performance over partitioning suggested by domain experts. Odessa works well across a variety of execution environments, and is agile to changes in the network, device and application inputs. © 2011 ACM.
DA  - 2011///
PY  - 2011
DO  - 10.1145/1999995.2000000
SP  - 43
EP  - 56
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79961048093&doi=10.1145%2f1999995.2000000&partnerID=40&md5=31c898117bb8d28e0bd9e3e02d38a1dd
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 313</p>
KW  - incremental partitioning
KW  - Incremental partitioning
KW  - Mobile devices
KW  - mobile perception application
KW  - offloading
KW  - parallel processing
KW  - Parallel processing
KW  - Portable equipment
KW  - Real time systems
KW  - User interfaces
KW  - video processing
KW  - Video processing
ER  - 

TY  - JOUR
T2  - BPF Compiler Collection (BCC). Retrieved from
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097236527&partnerID=40&md5=a68ad0ae4f81617eb27c49b2c4c2e97e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - A Kubernetes-Based Monitoring Platform for Dynamic Cloud Resource Provisioning
AU  - Chang, C.-C.
AU  - Yang, S.-R.
AU  - Yeh, E.-H.
AU  - Lin, P.
AU  - Jeng, J.-Y.
T2  - Proceedings - IEEE Global Communications Conference, GLOBECOM
AB  - Recently, more and more network operators have deployed cloud environment to implement network operations centers that monitor the status of their large-scale mobile or wireline networks. Typically, the cloud environment adopts container-based virtualization that uses Docker for container packaging with Kubernetes for multihost Docker container management. In such a container-based environment, it is important that the Kubernetes can dynamically monitor the resource requirements and/or usage of the running applications, and then adjust the resource provisioned to the managed containers accordingly. Currently, Kubernetes provides a naive dynamic resource-provisioning mechanism which only considers CPU utilization and thus is not effective. This paper aims at developing a generic platform to facilitate dynamic resource-provisioning based on Kubernetes. Our platform contains the following three features. First, our platform includes a comprehensive monitoring mechanism that integrates and provides the relatively complete system resource utilization and application QoS metrics to the resource-provisioning algorithm to make the better provisioning strategy. Second, our platform modularizes the operation of dynamic resource- provisioning operation so that the users can easily deploy a newly designed algorithm to replace an existing one in our platform. Third, the dynamic resource-provisioning operation in our platform is implemented as a control loop which can consequently be applied to all the running application following a user-defined time interval without other manual configuration. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/GLOCOM.2017.8254046
VL  - 2018-January
SP  - 1
EP  - 6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046372373&doi=10.1109%2fGLOCOM.2017.8254046&partnerID=40&md5=eccb2611e5a1df030d85f4a3a041d21a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 89</p>
KW  - Cloud environments
KW  - Container
KW  - Containers
KW  - Docker
KW  - dynamic resource provisioning
KW  - Dynamic resource provisioning
KW  - Dynamics
KW  - Kubernetes
KW  - Large-scales
KW  - monitoring
KW  - Monitoring platform
KW  - Network operations centers
KW  - Network operator
KW  - Quality of service
KW  - Running applications
KW  - Wire line networks
ER  - 

TY  - CONF
TI  - An Open-Source Benchmark Suite for Microservices and Their Hardware-Software Implications for Cloud & Edge Systems
AU  - Gan, Y.
AU  - Zhang, Y.
AU  - Cheng, D.
AU  - Shetty, A.
AU  - Rathi, P.
AU  - Katarki, N.
AU  - Bruno, A.
AU  - Hu, J.
AU  - Ritchken, B.
AU  - Jackson, B.
AU  - Hu, K.
AU  - Pancholi, M.
AU  - He, Y.
AU  - Clancy, B.
AU  - Colen, C.
AU  - Wen, F.
AU  - Leung, C.
AU  - Wang, S.
AU  - Zaruvinsky, L.
AU  - Espinosa, M.
AU  - Lin, R.
AU  - Liu, Z.
AU  - Padilla, J.
AU  - Delimitrou, C.
T2  - International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS
AB  - Cloud services have recently started undergoing a major shift from monolithic applications, to graphs of hundreds of loosely-coupled microservices. Microservices fundamentally change a lot of assumptions current cloud systems are designed with, and present both opportunities and challenges when optimizing for quality of service (QoS) and utilization. In this paper we explore the implications microservices have across the cloud system stack. We first present Death- StarBench, a novel, open-source benchmark suite built with microservices that is representative of large end-to-end services, modular and extensible. DeathStarBench includes a social network, a media service, an e-commerce site, a banking system, and IoT applications for coordination control of UAV swarms. We then use DeathStarBench to study the architectural characteristics of microservices, their implications in networking and operating systems, their challenges with respect to cluster management, and their trade-offs in terms of application design and programming frameworks. Finally, we explore the tail at scale effects of microservices in real deployments with hundreds of users, and highlight the increased pressure they put on performance predictability. © 2019 Association for Computing Machinery.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3297858.3304013
SP  - 3
EP  - 18
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064688619&doi=10.1145%2f3297858.3304013&partnerID=40&md5=b9f662fba2a35d83eb40d32009cf7676
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 538</p>
KW  - acceleration
KW  - Benchmark suites
KW  - cloud computing
KW  - Cloud systems
KW  - Cloud-computing
KW  - Cluster computing
KW  - cluster management
KW  - Cluster management
KW  - Datacenter
KW  - datacenters
KW  - Economic and social effects
KW  - Field programmable gate arrays (FPGA)
KW  - fpga
KW  - Fpgum
KW  - Microservice
KW  - microservices
KW  - Open source software
KW  - Open systems
KW  - Open-source
KW  - QoS
KW  - Quality of service
KW  - Quality-of-service
KW  - serverless
KW  - Serverless
ER  - 

TY  - JOUR
TI  - The nas parallel benchmarks
AU  - Bailey, D.H.
AU  - Barszcz, E.
AU  - Barton, J.T.
AU  - Browning, D.S.
AU  - Carter, R.L.
AU  - Dagum, L.
AU  - Fatoohi, R.A.
AU  - Frederickson, P.O.
AU  - Lasinski, T.A.
AU  - Schreiber, R.S.
AU  - Simon, H.D.
AU  - Venkatakrishnan, V.
AU  - Weeratunga, S.K.
T2  - International Journal of High Performance Computing Applications
AB  - A new set of benchmarks has been developed for the performance evaluation of highly parallel supercom puters. These consist of five “parallel kernel” bench marks and three “simulated application” benchmarks. Together they mimic the computation and data move ment characteristics of large-scale computational fluid dynamics applications. The principal distinguishing feature of these benchmarks is their “pencil and paper” specification—all details of these benchmarks are specified only algorithmically. In this way many of the difficulties associated with conventional bench- marking approaches on highly parallel systems are avoided. © 1991, Sage Publications. All rights reserved.
DA  - 1991///
PY  - 1991
DO  - 10.1177/109434209100500306
VL  - 5
IS  - 3
SP  - 63
EP  - 73
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973836157&doi=10.1177%2f109434209100500306&partnerID=40&md5=12f2b27c9f81379811534a509bccaa5c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 809</p>
ER  - 

TY  - CONF
TI  - Grid infrastructure monitoring system based on Nagios
AU  - Imamagic, E.
AU  - Dobrenic, D.
T2  - Proceedings of the 2007 Workshop on Grid Monitoring, GMW'07
AB  - Monitoring in distributed environment such as a grid is crucial for normal operation of all subsystems. Constant gathering of information enables efficient security auditing, failure detection, maintenance, job scheduling, accounting, resource performance tuning, debugging, etc. In this paper we focus on monitoring of resources in the grid with the purpose of failure detection, notifications and automatic recovery. We introduce our system based on open source monitoring framework Nagios that achieves these functionalities. We describe grid specific features we implemented in order to achieve efficient grid monitoring system, namely sensors for various grid services, advanced sensor hierarchy and certificate-based authorization on web interface. Finally, we give overview of the implementation of our system for monitoring EGEE grid infrastructure. Copyright 2007 ACM.
DA  - 2007///
PY  - 2007
DO  - 10.1145/1272680.1272685
SP  - 23
EP  - 28
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548251405&doi=10.1145%2f1272680.1272685&partnerID=40&md5=a1e9003ac921d02cfacc5fa08fb58cd7
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 40</p>
KW  - Automatic recovery
KW  - Computer resource management
KW  - Computer system recovery
KW  - Condition monitoring
KW  - Failure analysis
KW  - Failure detection
KW  - Grid computing
KW  - Grid monitoring architecture
KW  - Knowledge acquisition
KW  - Network monitoring
ER  - 

TY  - JOUR
T2  - Datadog
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097248158&partnerID=40&md5=26ee91c13c2aa399a9357087964a3a52
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - A Framework and Algorithm for Energy Efficient Container Consolidation in Cloud Data Centers
AU  - Piraghaj, S.F.
AU  - Dastjerdi, A.V.
AU  - Calheiros, R.N.
AU  - Buyya, R.
T2  - Proceedings - 2015 IEEE International Conference on Data Science and Data Intensive Systems; 8th IEEE International Conference Cyber, Physical and Social Computing; 11th IEEE International Conference on Green Computing and Communications and 8th IEEE International Conference on Internet of Things, DSDIS/CPSCom/GreenCom/iThings 2015
AB  - One of the major challenges that cloud providers face is minimizing power consumption of their data centers. To this point, majority of current research focuses on energy efficient management of resources in the Infrastructure as a Service model and through virtual machine consolidation. However, containers are increasingly gaining popularity and going to be major deployment model in cloud environment and specifically in Platform as a Service. This paper focuses on improving the energy efficiency of servers for this new deployment model by proposing a framework that consolidates containers on virtual machines. We first formally present the container consolidation problem and then we compare a number of algorithms and evaluate their performance against metrics such as energy consumption, Service Level Agreement violations, average container migrations rate, and average number of created virtual machines. Our proposed framework and algorithms can be utilized in a private cloud to minimize energy consumption, or alternatively in a public cloud to minimize the total number of hours the virtual machines leased. © 2015 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/DSDIS.2015.67
SP  - 368
EP  - 375
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964497471&doi=10.1109%2fDSDIS.2015.67&partnerID=40&md5=27ae17192e919f9725e5b8b246ba5422
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 106</p>
KW  - Algorithms
KW  - Cloud computing
KW  - Cloud Computing
KW  - Cloud data centers
KW  - Cloud environments
KW  - Container as a Service
KW  - Containers
KW  - Deployment models
KW  - Energy efficiency
KW  - Energy Efficiency
KW  - Energy efficient
KW  - Energy utilization
KW  - Framework and algorithms
KW  - Infrastructure as a service (IaaS)
KW  - Internet
KW  - Internet of things
KW  - Java programming language
KW  - Platform as a Service (PaaS)
KW  - Service Level Agreements
KW  - Virtual machine consolidations
KW  - Virtualization
KW  - Virtualizations
ER  - 

TY  - JOUR
T2  - Docker
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097211828&partnerID=40&md5=6708e505656f42ef935adfac738f56ce
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
AU  - Barak, N.
T2  - JgraphT
DA  - 2008///
PY  - 2008
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097213865&partnerID=40&md5=292c2031b1402d6d1660c4377aaf4bdf
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - The ganglia distributed monitoring system: Design, implementation, and experience
AU  - Massie, M.L.
AU  - Chun, B.N.
AU  - Culler, D.E.
T2  - Parallel Computing
AB  - Ganglia is a scalable distributed monitoring system for high performance computing systems such as clusters and Grids. It is based on a hierarchical design targeted at federations of clusters. It relies on a multicast-based listen/announce protocol to monitor state within clusters and uses a tree of point-to-point connections amongst representative cluster nodes to federate clusters and aggregate their state. It leverages widely used technologies such as XML for data representation, XDR for compact, portable data transport, and RRDtool for data storage and visualization. It uses carefully engineered data structures and algorithms to achieve very low per-node overheads and high concurrency. The implementation is robust, has been ported to an extensive set of operating systems and processor architectures, and is currently in use on over 500 clusters around the world. This paper presents the design, implementation, and evaluation of Ganglia along with experience gained through real world deployments on systems of widely varying scale, configurations, and target application domains over the last two and a half years. © 2004 Elsevier B.V. All rights reserved.
DA  - 2004///
PY  - 2004
DO  - 10.1016/j.parco.2004.04.001
VL  - 30
IS  - 7
SP  - 817
EP  - 840
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-3342966061&doi=10.1016%2fj.parco.2004.04.001&partnerID=40&md5=bb124c87b75271923533a6a58693e765
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 945</p>
KW  - Algorithms
KW  - Clusters
KW  - Computer applications
KW  - Computer operating systems
KW  - Computing systems
KW  - Condition monitoring
KW  - Data structures
KW  - Distributed computer systems
KW  - Distributed systems
KW  - Grids
KW  - Hierarchical systems
KW  - Monitoring
KW  - Multicasting
KW  - Parallel processing systems
KW  - Structural design
KW  - Trees (mathematics)
KW  - XML
ER  - 

TY  - CONF
TI  - DDSketch: A fast and fully-mergeable quantile sketch with relative-error guarantees
AU  - Masson, C.
AU  - Rim, J.E.
AU  - Lee, H.K.
T2  - Proceedings of the VLDB Endowment
AB  - Summary statistics such as the mean and variance are easily maintained for large, distributed data streams, but order statistics (i.e., sample quantiles) can only be approximately summarized. There is extensive literature on maintaining quantile sketches where the emphasis has been on bounding the rank error of the sketch while using little memory. Unfortunately, rank error guarantees do not preclude arbitrarily large relative errors, and this often occurs in practice when the data is heavily skewed. Given the distributed nature of contemporary large-scale systems, another crucial property for quantile sketches is mergeablility, i.e., several combined sketches must be as accurate as a single sketch of the same data. We present the first fully-mergeable, relative-error quantile sketching algorithm with formal guarantees. The sketch is extremely fast and accurate, and is currently being used by Datadog at a wide-scale. © 2019 VLDB Endowment.
DA  - 2018///
PY  - 2018
DO  - 10.14778/3352063.3352135
VL  - 12
SP  - 2195
EP  - 2205
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074537736&doi=10.14778%2f3352063.3352135&partnerID=40&md5=1fb08193b7fd72eedcd3b307ecbbc834
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 46</p>
KW  - Distributed data streams
KW  - Errors
KW  - Large scale systems
KW  - Order statistics
KW  - Relative errors
KW  - Summary statistic
ER  - 

TY  - JOUR
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097241211&partnerID=40&md5=b95368a35bc942ec457f838b0bb97a89
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - A framework for monitoring microservice-oriented cloud applications in heterogeneous virtualization environments
AU  - Noor, A.
AU  - Jha, D.N.
AU  - Mitra, K.
AU  - Jayaraman, P.P.
AU  - Souza, A.
AU  - Ranjan, R.
AU  - Dustdar, S.
T2  - IEEE International Conference on Cloud Computing, CLOUD
AB  - Microservices have emerged as a new approach for developing and deploying cloud applications that require higher levels of agility, scale, and reliability. To this end, a microservice-based cloud application architecture advocates decomposition of monolithic application components into independent software components called 'microservices'. As the independent microservices can be developed, deployed, and updated independently of each other, it leads to complex run-time performance monitoring and management challenges. To solve this problem, we propose a generic monitoring framework, Multi-microservices Multi-virtualization Multi-cloud (M3) that monitors the performance of microservices deployed across heterogeneous virtualization platforms in a multi-cloud environment. We validated the efficacy and efficiency of M3 using a Book-Shop application executing across AWS and Azure. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/CLOUD.2019.00035
VL  - 2019-July
SP  - 156
EP  - 163
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067381634&doi=10.1109%2fCLOUD.2019.00035&partnerID=40&md5=e9fb328dc0f72a148e478f437ddfc1f3
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 33</p>
KW  - Application components
KW  - Application programs
KW  - Cloud applications
KW  - Cloud computing
KW  - Container
KW  - Containers
KW  - Microservices
KW  - Monitoring
KW  - Monitoring frameworks
KW  - Multi-clouds
KW  - New approaches
KW  - Run-time performance
KW  - Software component
KW  - Virtual reality
KW  - Virtualization
KW  - VM
ER  - 

TY  - CONF
TI  - A dashboard for microservice monitoring and management
AU  - Mayer, B.
AU  - Weinreich, R.
T2  - Proceedings - 2017 IEEE International Conference on Software Architecture Workshops, ICSAW 2017: Side Track Proceedings
AB  - We present an experimental dashboard for microservice monitoring and management. The dashboard can be adapted to different stakeholder needs and it supports the integration of different monitoring infrastructures for collecting microservice runtime data. Aside from runtime information, the dashboard also supports the integration of other information sources for providing static information about microservices and microservice development. We describe the main motivation for developing the dashboard, explain the basic concepts and present important usage scenarios and views currently supported in the dashboard. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/ICSAW.2017.44
SP  - 66
EP  - 69
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025678915&doi=10.1109%2fICSAW.2017.44&partnerID=40&md5=0cd137af2b581c3c6b0e0eca710982a5
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 51</p>
KW  - Basic concepts
KW  - Information sources
KW  - Microservice dashboard
KW  - Microservice management
KW  - Microservice monitoring
KW  - Microservices
KW  - Monitoring and management
KW  - Run-time information
KW  - Software architecture
KW  - Static information
KW  - Usage scenarios
ER  - 

TY  - CONF
TI  - ConMon: An automated container based network performance monitoring system
AU  - Moradi, F.
AU  - Flinta, C.
AU  - Johnsson, A.
AU  - Meirosu, C.
T2  - Proceedings of the IM 2017 - 2017 IFIP/IEEE International Symposium on Integrated Network and Service Management
AB  - The popularity of container technologies and their widespread usage for building microservices demands solutions dedicated for efficient monitoring of containers and their interactions. In this paper we present ConMon, an automated system for monitoring the network performance of container-based applications. It automatically identifies newly instantiated application containers and observes passively their traffic. Based on these observations, it configures and executes monitoring functions inside adjacent monitoring containers. The system adapts the monitoring containers to changes driven by either the application or the execution platform. The evaluation results validate the feasibility of the ConMon approach and illustrate its scalability in terms of low overhead on compute resources, moderate impact on applications, and negligible impact on the background network traffic. © 2017 IFIP.
DA  - 2017///
PY  - 2017
DO  - 10.23919/INM.2017.7987264
SP  - 54
EP  - 62
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029438453&doi=10.23919%2fINM.2017.7987264&partnerID=40&md5=7272fa543fabca82836d72e452f5b4d3
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 28</p>
KW  - Automated systems
KW  - Automation
KW  - Binary alloys
KW  - Compute resources
KW  - Containers
KW  - Efficient monitoring
KW  - Evaluation results
KW  - Execution platforms
KW  - Monitoring
KW  - Monitoring functions
KW  - Network performance
KW  - Network performance monitoring
KW  - Network traffic
ER  - 

TY  - JOUR
T2  - Prometheus
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097219831&partnerID=40&md5=73d5a12c929c7db7e75b077d3d52c8c4
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 4</p>
ER  - 

TY  - JOUR
T2  - CNCF Cloud Native Landscape
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097217996&partnerID=40&md5=3566126478b39a6ec0f2ef94c8c60cb2
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097239840&partnerID=40&md5=b10068408ce36dbb29ee5e0484f227f8
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - The BSD packet filter: A new architecture for user-level packet capture
AU  - McCanne, S.
AU  - Jacobson, V.
T2  - Proceedings of the Winter 1993 USENIX Conference
AB  - Many versions of Unix provide facilities for user-level packet capture, making possible the use of general purpose workstations for network monitoring. Because network monitors run as user-level processes, packets must be copied across the kernel/user-space protection boundary. This copying can be minimized by deploying a kernel agent called a packet filter, which discards unwanted packets as early as possible. The original Unix packet filter was designed around a stack-based filter evaluator that performs sub-optimally on current RISC CPUs. The BSD Packet Filter (BPF) uses a new, register-based filter evaluator that is up to 20 times faster than the original design. BPF also uses a straightforward buffering strategy that makes its overall performance up to 100 times faster than Sun's NIT running on the same hardware. © 1993 USENIX.All right reserved.
DA  - 1993///
PY  - 1993
SP  - 259
EP  - 269
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077206746&partnerID=40&md5=bfe7b22125776923195480159edf78bc
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 701</p>
KW  - Buffering strategy
KW  - Computer workstations
KW  - Network Monitoring
KW  - Network monitors
KW  - On currents
KW  - Original design
KW  - Packet capture
KW  - Packet filters
KW  - Program processors
KW  - UNIX
KW  - Unwanted packets
ER  - 

TY  - JOUR
T2  - New Relic
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032364452&partnerID=40&md5=035c21ebc0c2e6e8722e8b235dd6756a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2</p>
ER  - 

TY  - CONF
TI  - Seer: Leveraging Big Data to Navigate the Complexity of Performance Debugging in Cloud Microservices
AU  - Gan, Y.
AU  - Zhang, Y.
AU  - Hu, K.
AU  - Cheng, D.
AU  - He, Y.
AU  - Pancholi, M.
AU  - Delimitrou, C.
T2  - International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS
AB  - Performance unpredictability is a major roadblock towards cloud adoption, and has performance, cost, and revenue ramifications. Predictable performance is even more critical as cloud services transition from monolithic designs to microservices. Detecting QoS violations after they occur in systems with microservices results in long recovery times, as hotspots propagate and amplify across dependent services. We present Seer, an online cloud performance debugging system that leverages deep learning and the massive amount of tracing data cloud systems collect to learn spatial and temporal patterns that translate to QoS violations. Seer combines lightweight distributed RPC-level tracing, with detailed low-level hardware monitoring to signal an upcoming QoS violation, and diagnose the source of unpredictable performance. Once an imminent QoS violation is detected, Seer notifies the cluster manager to take action to avoid performance degradation altogether.We evaluate Seer both in local clusters, and in large-scale deployments of end-to-end applications built with microservices with hundreds of users. We show that Seer correctly anticipates QoS violations 91% of the time, and avoids the QoS violation to begin with in 84% of cases. Finally, we show that Seer can identify applicationlevel design bugs, and provide insights on how to better architect microservices to achieve predictable performance. © 2019 Association for Computing Machinery.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3297858.3304004
SP  - 19
EP  - 33
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064659746&doi=10.1145%2f3297858.3304004&partnerID=40&md5=67a5e54db5ebf1ecee4bb2bc5f5c7790
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 252</p>
KW  - Big data
KW  - Cloud adoptions
KW  - cloud computing
KW  - Cloud computing
KW  - Cloud-computing
KW  - data mining
KW  - Data mining
KW  - datacenter
KW  - Datacenter
KW  - deep learning
KW  - Deep learning
KW  - Information management
KW  - Microservice
KW  - microservices
KW  - monitoring
KW  - Online systems
KW  - Performance
KW  - Performance costs
KW  - performance debugging
KW  - Performance debugging
KW  - Program debugging
KW  - QoS
KW  - Quality of service
KW  - resource management
KW  - Resource management
KW  - tracing
KW  - Tracing
ER  - 

TY  - CONF
TI  - Microservices Monitoring with Event Logs and Black Box Execution Tracing
AU  - Cinque, M.
AU  - Corte, R.D.
AU  - Pecchia, A.
T2  - Proceedings - 2021 IEEE World Congress on Services, SERVICES 2021
AB  - Monitoring is a core practice in any software system, and entails gathering a variety of data sources that pertain the execution of a given system. Trends in microservices systems exacerbate the role of monitoring. Microservices put forth reduced size, independency, flexibility and modularity principles, which well cope with ever-changing business environments. However, as real-world applications are decomposed, they can easily reach hundreds of microservices. This inherent complexity determines an increasing difficulty in debugging, monitoring and forensics, and poses novel challenges to monitoring data sources, such as event logs.  © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/SERVICES51467.2021.00023
SP  - 12
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123717085&doi=10.1109%2fSERVICES51467.2021.00023&partnerID=40&md5=b47f6212972444f8cbf91ea1484a88ae
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 13</p>
KW  - Black boxes
KW  - Clearwater
KW  - Data-source
KW  - Docker
KW  - Event logs
KW  - Execution tracing
KW  - Kubernetes
KW  - Log analysis
KW  - Microservice
KW  - Microservices
KW  - Monitoring
KW  - REST
ER  - 

TY  - JOUR
TI  - BPF+: Exploiting global data-flow optimization in a generalized packet filter architecture
AU  - Begel, A.
AU  - McCanne, S.
AU  - Graham, S.L.
T2  - Computer Communication Review
AB  - A packet filler is a programmable selection criterion for classifying or selecting packets from a packet stream in a generic, reusable fashion. Previous work on packet filters falls roughly into two categories, namely those efforts that investigate flexible and extensible filter abstractions but sacrifice performance, and those that focus on low-level, optimized filtering representations but sacrifice flexibility. Applications like network monitoring and intrusion detection, however, require both high-level expressiveness and raw performance. In this paper, we propose a fully general packet filter framework that affords both a high degree of flexibility and good performance. In our framework, a packet filter is expressed in a high-level language that is compiled into a highly efficient native implementation. The optimization phase of the compiler uses a flowgraph set relation called edge dominators and the novel application of an optimization technique that we call "redundant predicate elimination," in which we interleave partial redundancy elimination, predicate assertion propagation, and flowgraph edge elimination to carry out the filter predicate optimization. Our resulting packet-filtering framework, which we call BPF+, derives from the BSD packet filter (BPF), and includes a filter program translator, a byte code optimizer, a byte code safety verifier to allow code to migrate across protection boundaries, and a just-in-time assembler to convert byte codes to efficient native code. Despite the high degree of flexibility afforded by our generalized framework, our performance measurements show that our system achieves performance comparable to state-of-the-art packet filter architectures and better than hand-coded filters written in C. © 1999 ACM.
DA  - 1999///
PY  - 1999
DO  - 10.1145/316194.316214
VL  - 29
IS  - 4
SP  - 123
EP  - 134
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033204392&doi=10.1145%2f316194.316214&partnerID=40&md5=cf36885f99deda499012c5ad05b666d9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 83</p>
KW  - Codes (symbols)
KW  - Data communication systems
KW  - Data flow analysis
KW  - High level languages
KW  - Optimization
KW  - Packet filters
KW  - Packet switching
KW  - Program compilers
KW  - Signal filtering and prediction
ER  - 

TY  - JOUR
TI  - The new linux perf tools
AU  - De Melo, A.C.
T2  - Slides from Linux Kongress
DA  - 2010///
PY  - 2010
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937476715&partnerID=40&md5=32d69aa6271f145acae831317024ae7b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 263</p>
ER  - 

TY  - JOUR
TI  - Microservices: A definition of this new architectural term
AU  - Lewis, J.
AU  - Fowler, M.
T2  - Microservices: A Definition of This New Architectural Term
DA  - 2014///
PY  - 2014
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964234114&partnerID=40&md5=fd3de0bd7b34545e47485cf075757737
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 579</p>
ER  - 

TY  - JOUR
TI  - Rusty: Runtime Interference-Aware Predictive Monitoring for Modern Multi-Tenant Systems
AU  - Masouros, D.
AU  - Xydis, S.
AU  - Soudris, D.
T2  - IEEE Transactions on Parallel and Distributed Systems
AB  - Modern micro-service and container-based cloud-native applications have leveraged multi-tenancy as a first class system design concern. The increasing number of co-located services/workloads into server facilities stresses resource availability and system capability in an unconventional and unpredictable manner. To efficiently manage resources in such dynamic environments, run-time observability and forecasting are required to capture workload sensitivities under differing interference effects, according to applied co-location scenarios. While several research efforts have emerged on interference-aware performance modelling, they are usually applied at a very coarse-grained manner e.g., estimating the overall performance degradation of an application, thus failing to effectively quantify, predict or provide educated insights on the impact of continuous runtime interference on per-resource allocations. In this paper, we present Rusty, a predictive monitoring system that leverages the power of Long Short-Term Memory networks to enable fast and accurate runtime forecasting of key performance metrics and resource stresses of cloud-native applications under interference. We evaluate Rusty under a diverse set of interference scenarios for a plethora of representative cloud workloads, showing that Rusty i) achieves extremely high prediction accuracy, average $R^2$R2 value of 0.98, ii) enables very deep prediction horizons retaining high accuracy, e.g., $R^2$R2 of around 0.99 for a horizon of 1 sec ahead and around 0.94 for an horizon of 5 sec ahead, while iii) satisfying, at the same time, the strict latency constraints required to make Rusty practical for continuous predictive monitoring at runtime.  © 1990-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/TPDS.2020.3013948
VL  - 32
IS  - 1
SP  - 184
EP  - 198
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090449884&doi=10.1109%2fTPDS.2020.3013948&partnerID=40&md5=ef543cd52dc6f2508a315444a314d165
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 37</p>
KW  - Dynamic environments
KW  - Forecasting
KW  - interference aware
KW  - Interference effects
KW  - Latency constraints
KW  - LSTM networks
KW  - multi-tenant systems
KW  - Performance degradation
KW  - Performance metrics
KW  - Performance modelling
KW  - predictive monitoring
KW  - Predictive monitoring
KW  - Resource availability
KW  - Signal processing
KW  - system predictability
ER  - 

TY  - CONF
TI  - Power containers: An OS facility for fine-grained power and energy management on multicore servers
AU  - Shen, K.
AU  - Shriraman, A.
AU  - Dwarkadas, S.
AU  - Zhang, X.
AU  - Chen, Z.
T2  - ACM SIGPLAN Notices
AB  - Energy efficiency and power capping are critical concerns in server and cloud computing systems. They face growing challenges due to dynamic power variations from new client-directed web applications, as well as complex behaviors due to multicore resource sharing and hardware heterogeneity. This paper presents a new operating system facility called "power containers" that accounts for and controls the power and energy usage of individual fine-grained requests in multicore servers. This facility relies on three key techniques-1) online model that attributes multicore power (including shared maintenance power) to concurrently running tasks, 2) alignment of actual power measurements and model estimates to enable online model recalibration, and 3) on-the-fly applicationtransparent request tracking in multi-stage servers to isolate the power and energy contributions and customize per-request control. Our mechanisms enable new multicore server management capabilities including fair power capping that only penalizes powerhungry requests, and energy-aware request distribution between heterogeneous servers. Our evaluation uses three multicore processors (Intel Woodcrest, Westmere, and SandyBridge) and a variety of server and cloud computing (Google App Engine) workloads. Our results demonstrate the high accuracy of our request power accounting (no more than 11% errors) and the effectiveness of container-enabled power virus isolation and throttling. Our request distribution case study shows up to 25% energy saving compared to an alternative approach that recognizes machine heterogeneity but not fine-grained workload affinity.
DA  - 2013///
PY  - 2013
DO  - 10.1145/2499368.2451124
VL  - 48
SP  - 65
EP  - 76
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880084057&doi=10.1145%2f2499368.2451124&partnerID=40&md5=112375f1784ddee3e8da2b03c015e68e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 47</p>
KW  - Computer hardware
KW  - Containers
KW  - Embedded systems
KW  - Energy efficiency
KW  - Hardware
KW  - Hardware counters
KW  - Multi core
KW  - Multicore
KW  - Operating system
KW  - Platform as a Service (PaaS)
KW  - Power modeling
KW  - Power virus
KW  - Server and cloud computing
ER  - 

TY  - CONF
TI  - Happy: Hyperthread-aware power profiling dynamically
AU  - Zhai, Y.
AU  - Zhang, X.
AU  - Eranian, S.
AU  - Tang, L.
AU  - Mars, J.
T2  - Proceedings of the 2014 USENIX Annual Technical Conference, USENIX ATC 2014
AB  - Quantifying the power consumption of individual applications co-running on a single server is a critical component for software-based power capping, scheduling, and provisioning techniques in modern datacenters. However, with the proliferation of hyperthreading in the last few generations of server-grade processor designs, the challenge of accurately and dynamically performing this power attribution to individual threads has been significantly exacerbated. Due to the sharing of core-level resources such as functional units, prior techniques are not suitable to attribute the power consumption between hyperthreads sharing a physical core. In this paper, we present a runtime mechanism that quantifies and attributes power consumption to individual jobs at fine granularity. Specifically, we introduce a hyperthread-aware power model that differentiates between the states when both hardware threads of a core are in use, and when only one thread is in use. By capturing these two different states, we are able to accurately attribute power to each logical CPU in modern servers. We conducted experiments with several Google production workloads on an Intel Sandy Bridge server. Compared to prior hyperthread-oblivious model, HaPPy is substantially more accurate, reducing the prediction error from 20.5% to 7.5% on average and from 31.5% to 9.4% in the worst case. © Proceedings of the 2014 USENIX Annual Technical Conference, USENIX ATC 2014. All rights reserved.
DA  - 2014///
PY  - 2014
SP  - 211
EP  - 217
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077459426&partnerID=40&md5=2097f753ffcf901217994ca6b5f04c69
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 39</p>
KW  - Application programs
KW  - Critical component
KW  - Datacenter
KW  - Electric power utilization
KW  - Functional units
KW  - Hyperthreading
KW  - Multitasking
KW  - Physical core
KW  - Power
KW  - Power capping
KW  - Power profiling
KW  - Processor design
KW  - Single server
ER  - 

TY  - JOUR
AU  - Lazowska, E.D.
AU  - Zahorjan, J.
AU  - Graham, G.S.
AU  - Sevcik, K.C.
T2  - Quantitative System Performance: Computer System Analysis Using Queueing Network Models
DA  - 1984///
PY  - 1984
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003834102&partnerID=40&md5=2510094fe765c8f8f6d6acde2842053a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1047</p>
ER  - 

TY  - JOUR
TI  - Enabling power-awareness for the Xen hypervisor
AU  - Ferroni, M.
AU  - Colmenares, J.A.
AU  - Hofmeyr, S.
AU  - Kubiatowicz, J.D.
AU  - Santambrogio, M.D.
T2  - ACM SIGBED Review
AB  - Virtualization allows simultaneous execution of multi-tenant workloads on the same platform, either a server or an embedded system. Unfortunately, it is non-trivial to attribute hardware events to multiple virtual tenants, as some system's metrics relate to the whole system (e.g., RAPL energy counters). Virtualized environments have then a rather incomplete picture of how tenants use the hardware, limiting their optimization capabilities. Thus, we propose XeM-Power, a lightweight monitoring solution for Xen that precisely accounts hardware events to guest workloads. It also enables attribution of CPU power consumption to individual tenants. We show that XeMPower introduces negligible overhead in power consumption, aiming to be a reference design for power-aware virtualized environments. © 2018 Authors.
DA  - 2018///
PY  - 2018
DO  - 10.1145/3199610.3199615
VL  - 15
IS  - 1
SP  - 36
EP  - 42
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089730588&doi=10.1145%2f3199610.3199615&partnerID=40&md5=ba0885caf8d819a4e0b46b4fc0e243e3
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3</p>
KW  - Computer hardware
KW  - Electric power utilization
KW  - Hypervisor
KW  - Multi tenants
KW  - Non-trivial
KW  - Optimization capabilities
KW  - Power awareness
KW  - Power management
KW  - Power-aware
KW  - Reference designs
KW  - Virtualized environment
ER  - 

TY  - JOUR
T2  - Weave Scope
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071420868&partnerID=40&md5=bb45823128e041f19b65cab560af010b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 5</p>
ER  - 

TY  - CONF
TI  - Realtime high-speed network traffic monitoring using ntopng
AU  - Deri, L.
AU  - Martinelli, M.
AU  - Cardigliano, A.
T2  - 28th Large Installation System Administration Conference, LISA 2014
AB  - Monitoring network traffic has become increasingly challenging in terms of number of hosts, protocol proliferation and probe placement topologies. Virtualised environments and cloud services shifted the focus from dedicated hardware monitoring devices to virtual machine based, software traffic monitoring applications. This paper covers the design and implementation of ntopng, an open-source traffic monitoring application designed for high-speed networks. ntopng's key features are large networks real-time analytics and the ability to characterise application protocols and user traffic behaviour. ntopng was extensively validated in various monitoring environments ranging from small networks to.it ccTLD traffic analysis. © LISA 2014.All right reserved.
DA  - 2014///
PY  - 2014
SP  - 69
EP  - 79
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949487110&partnerID=40&md5=6dc0ba001f6a954b4b4e7d7fdc86575f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 27</p>
KW  - Application programs
KW  - Application protocols
KW  - Behavioral research
KW  - Dedicated hardware
KW  - Design and implementations
KW  - High speed network traffic
KW  - HIgh speed networks
KW  - Internet protocols
KW  - Monitoring
KW  - Monitoring environment
KW  - Open source software
KW  - Real-time analytics
KW  - Traffic monitoring
KW  - Virtualised environments
ER  - 

TY  - CONF
TI  - The benefits of event-driven energy accounting in power-sensitive systems
AU  - Bellosa, F.
T2  - Proceedings of the 9th Workshop on ACM SIGOPS European Workshop: Beyond the PC: New Challenges for the Operating System, EW 2000
AB  - A prerequisite of energy-aware scheduling is precise knowledge of any activity inside the computer system. Embedded hardware monitors (e.g., processor performance counters) have proved to offer valuable information in the field of performance analysis. The same approach can be applied to investigate the energy usage patterns of individual threads. We use information about active hardware units (e.g., integer/floating point unit, cache/memory interface) gathered by event counters to establish a thread-specific energy accounting. The evaluation shows that the correlation of events and energy values provides the necessary information for energy-aware scheduling policies. Our approach to OS-directed power management adds the energy usage pattern to the runtime context of a thread. Depending on the field of application we present two scenarios that benefit from applying energy usage patterns: Workstations with passive cooling on the one hand and battery-powered mobile systems on the other hand. Energy-aware scheduling evaluates the energy usage of each thread and throttles the system activity so that the scheduling goal is achieved. In workstations we throttle the system if the average energy use exceeds a predefined power-dissipation capacity. This makes a compact, noiseless and affordable system design possible that meets sporadic yet high demands in computing power. Nowadays, more and more mobile systems offer the features of reducible clock speed and dynamic voltage scaling. Energy-aware scheduling can employ these features to yield a longer battery life by slowing down low-priority threads while preserving a certain quality of service.
DA  - 2000///
PY  - 2000
DO  - 10.1145/566726.566736
SP  - 37
EP  - 42
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944384544&doi=10.1145%2f566726.566736&partnerID=40&md5=acdcf8e369e9024a3056aa6cdfc18124
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 279</p>
KW  - Computer hardware
KW  - Computer systems programming
KW  - Cooling systems
KW  - Electric batteries
KW  - Embedded hardware
KW  - Energy accounting
KW  - Energy-aware scheduling
KW  - Hardware
KW  - Passive cooling
KW  - Performance analysis
KW  - Power management
KW  - Power-sensitive systems
KW  - Processor performance
KW  - Quality of service
KW  - Scheduling
KW  - Specific energy
KW  - Voltage scaling
ER  - 

TY  - JOUR
TI  - Borg, omega, and kubernetes
AU  - Burns, B.
AU  - Grant, B.
AU  - Oppenheimer, D.
AU  - Brewer, E.
AU  - Wilkes, J.
T2  - Queue
DA  - 2016///
PY  - 2016
DO  - 10.1145/2898442.2898444
VL  - 14
IS  - 1
SP  - 70
EP  - 93
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960969836&doi=10.1145%2f2898442.2898444&partnerID=40&md5=d0defa676806718c1a9b91c2e9c70de1
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 355</p>
ER  - 

TY  - JOUR
TI  - Performance-aware load shedding for monitoring events in container based environments
AU  - Brondolin, R.
AU  - Ferroni, M.
AU  - Santambrogio, M.
T2  - ACM SIGBED Review
AB  - Runtime monitoring tools have become fundamental to assess the correct operation of complex systems and applications. Unfortunately, the more precise is the monitoring (sampling rate, information granularity, and so on), the higher is the overhead introduced in the system itself. In this paper, we propose a new load shedding framework that enables runtime adaptation of monitoring agents under heavy system load, exploiting an heuristic Load Manager to control the agent status and a runtime support for domain-specific policies. We implemented the proposed methodology on Sysdig, with an average control error improvement of 3.51x (12.25x at most), w.r.t. previous solutions. Copyright held by Owner/Author
DA  - 2019///
PY  - 2019
DO  - 10.1145/3373400.3373404
VL  - 16
IS  - 3
SP  - 27
EP  - 32
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075802823&doi=10.1145%2f3373400.3373404&partnerID=40&md5=cd5ff103db1c4e292adb5a6975238249
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 4</p>
KW  - Data-stream processing
KW  - Load Shedding
KW  - Monitoring
ER  - 

TY  - CONF
TI  - Nonintrusive monitoring of microservice-based systems
AU  - Pina, F.
AU  - Correia, J.
AU  - Filipe, R.
AU  - Araujo, F.
AU  - Cardroom, J.
T2  - NCA 2018 - 2018 IEEE 17th International Symposium on Network Computing and Applications
AB  - Breaking large software systems into smaller functionally interconnected components is a trend on the rise. This architectural style, known as 'microservices', simplifies development, deployment and management at the expense of complexity and observability. In fact, in large scale systems, it is particularly difficult to determine the set of microservices responsible for delaying a client's request, when one module impacts several other microservices in a cascading effect. Components cannot be analyzed in isolation, and without instrumenting their source code extensively, it is difficult to find the bottlenecks and trace their root causes. To mitigate this problem, we propose a much simpler approach: log gateway activity, to register all calls to and between microservices, as well as their responses, thus enabling the extraction of topology and performance metrics, without changing source code. For validation, we implemented the proposed platform, with a microservices-based application that we observe under load. Our results show that we can extract relevant performance information with a negligible effort, even in legacy systems, where instrumenting modules may be a very expensive task. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/NCA.2018.8548311
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059972659&doi=10.1109%2fNCA.2018.8548311&partnerID=40&md5=a4c3f4f828fddb283ee6b003982d66a8
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 26</p>
KW  - Architectural style
KW  - Black boxes
KW  - Black-box monitoring
KW  - Cascading effects
KW  - Gateway
KW  - Gateways (computer networks)
KW  - Large scale systems
KW  - Large software systems
KW  - Legacy systems
KW  - Microservices
KW  - Non-intrusive monitoring
KW  - Performance metrics
KW  - Source codes
ER  - 

TY  - CONF
TI  - DockerCap: A Software-Level Power Capping Orchestrator for Docker Containers
AU  - Asnaghi, A.
AU  - Ferroni, M.
AU  - Santambrogio, M.D.
T2  - Proceedings - 19th IEEE International Conference on Computational Science and Engineering, 14th IEEE International Conference on Embedded and Ubiquitous Computing and 15th International Symposium on Distributed Computing and Applications to Business, Engineering and Science, CSE-EUC-DCABES 2016
AB  - Internet of Things (IoT) is experiencing a huge hype these days, thanks to the increasing capabilities of embedded devices that enable their adoption in new fields of application (e.g. Wireless Sensor Networks, Connected Cars, Health Care, etc.). On the one hand, this is leading to an increasing adoption of multi-tenancy solutions for Cloud and Fog Computing, to analyze and store the data produced. On the other hand, power consumption has become a major concern for almost every digital system, from the smallest embedded circuits to the biggest computer clusters, with all the shades in between. Fine-grain control mechanisms are then needed to cap power consumption at each level of the stack, still guaranteeing Service Level Agreements (SLA) to the hosted applications. In this work, we propose DockerCap, a software-level power capping orchestrator for Docker containers that follows an Observe-Decide-Act loop structure: this allows to quickly react to changes that impact on the power consumption by managing resources of each container at run-time, to ensure the desired power cap. We show how we are able to obtain results comparable with the state of the art power capping solution provided by Intel RAPL, still being able to tune the performances of the containers and even guarantee SLA constraints. © 2016 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/CSE-EUC-DCABES.2016.166
SP  - 90
EP  - 97
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026642569&doi=10.1109%2fCSE-EUC-DCABES.2016.166&partnerID=40&md5=19be1729ebe7a99678ff8c78e63497c8
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 21</p>
KW  - Computer clusters
KW  - Containers
KW  - Distributed computer systems
KW  - Docker
KW  - Electric power utilization
KW  - Embedded circuits
KW  - Fine-grain controls
KW  - Fog Computing
KW  - Green computing
KW  - Internet of things
KW  - Internet of Things (IOT)
KW  - Managing resources
KW  - Power management
KW  - Power Management
KW  - Service Level Agreements
KW  - State of the art
KW  - Ubiquitous computing
KW  - Wireless sensor networks
ER  - 

TY  - CONF
TI  - DEEP-mon: Dynamic and energy efficient power monitoring for container-based infrastructures
AU  - Brondolin, R.
AU  - Sardelli, T.
AU  - Santambrogio, M.D.
T2  - Proceedings - 2018 IEEE 32nd International Parallel and Distributed Processing Symposium Workshops, IPDPSW 2018
AB  - In the last few years energy efficiency of large scale infrastructures gained a lot of attention, as power consumption became one of the most impacting factors of the operative costs of a data-center and of its Total Cost of Ownership (TCO). Power consumption can be observed at different layers of the data-center, from the overall power grid, moving to each rack and arriving to each machine and system. Given the rise of application containers both in the cloud computing and High Performance Computing (HPC) scenarios, it becomes more and more important to measure power consumption also at the application level, where power-aware schedulers and orchestrators can optimize the execution of the workloads not only from a performance perspective, but also considering performance/power trade-offs. In this paper we propose DEEP-mon, a novel monitoring tool able to measure power consumption and attribute it for each thread and application container running in the system. Moreover, we show how the proposed approach has a negligible impact on the monitored system and on the running workloads, overcoming the limitations of the previous works in the field. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/IPDPSW.2018.00110
SP  - 676
EP  - 684
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052236003&doi=10.1109%2fIPDPSW.2018.00110&partnerID=40&md5=b3fc988b43b9d7d4cf6b652e09435256
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 20</p>
KW  - Application containers
KW  - Application level
KW  - Containers
KW  - Distributed computer systems
KW  - Economic and social effects
KW  - Electric power transmission networks
KW  - Electric power utilization
KW  - Energy efficiency
KW  - Green computing
KW  - High performance computing (HPC)
KW  - Impacting factor
KW  - Large scale infrastructures
KW  - Monitored systems
KW  - Monitoring
KW  - Power attribution
KW  - Power awareness
KW  - Power management
KW  - Total cost of ownership
ER  - 

TY  - JOUR
TI  - Power-management architecture of the intel microarchitecture code-named Sandy Bridge
AU  - Rotem, E.
AU  - Naveh, A.
AU  - Ananthakrishnan, A.
AU  - Weissmann, E.
AU  - Rajwan, D.
T2  - IEEE Micro
AB  - Modern microprocessors are evolving into system-on-a-chip designs with high integration levels, catering to ever-shrinking form factors. Portability without compromising performance is a driving market need. An architectural approach that's adaptive to and cognizant of workload behavior and platform physical constraints is indispensable to meeting these performance and efficiency goals. This article describes power-management innovations introduced on Intel's Sandy Bridge microprocessor. © 2012 IEEE.
DA  - 2012///
PY  - 2012
DO  - 10.1109/MM.2012.12
VL  - 32
IS  - 2
SP  - 20
EP  - 27
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859729360&doi=10.1109%2fMM.2012.12&partnerID=40&md5=ac411761c3d90741a5eebafa6fed9716
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 387</p>
KW  - Application specific integrated circuits
KW  - Approximation theory
KW  - Architectural approach
KW  - energy management
KW  - Energy management
KW  - Form factors
KW  - High integration level
KW  - Market needs
KW  - Micro architectures
KW  - Microprocessor chips
KW  - Modern microprocessor
KW  - Physical constraints
KW  - power management
KW  - Power managements
KW  - Program processors
KW  - Sandy Bridge
KW  - System-on-a-chip designs
KW  - Turbo Boost
ER  - 

TY  - JOUR
TI  - Combining system visibility and security using eBPF
AU  - Deri, L.
AU  - Sabella, S.
AU  - Mainardi, S.
T2  - Italian Conference on Cybersecurity
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102386111&partnerID=40&md5=4c6391aeed60593ddea37f793f19591e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 23</p>
ER  - 

TY  - JOUR
TI  - PAPI: A portable interface to hardware performance counters
AU  - Mucci, P.J.
AU  - Browne, S.
AU  - Deane, C.
AU  - Ho, G.
T2  - Proceedings of the Department of Defense HPCMP Users Group Conference
DA  - 1999///
PY  - 1999
SP  - 7
EP  - 10
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646222013&partnerID=40&md5=24440d72bf4d73c520b743720ddc1dae
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 335</p>
ER  - 

TY  - CONF
TI  - Energy-aware Provisioning of Microservices for Serverless Edge Computing
AU  - Adeppady, M.
AU  - Conte, A.
AU  - Karl, H.
AU  - Giaccone, P.
AU  - Chiasserini, C.F.
T2  - Proceedings - IEEE Global Communications Conference, GLOBECOM
AB  - Serverless edge computing allows for highly efficient resource utilization, reducing the energy footprint of edge data centers. Indeed, the containers can be dynamically created and destroyed, allowing to adapt the workload to the available resources. Creating containers upon arrivals of service requests entails, however, a high start-up latency, which may be unsuitable for time-critical services. As alternative solution, pre-started containers ('warm containers') are used to decrease start-up latency, but incurring in higher resource costs. In this work, we minimize the energy consumption of the active servers in the data center by optimally managing the various container states while meeting the target delay of the requested services. Further, in light of the problem complexity, we investigate how a simple threshold-based algorithm performs and show that it can closely match the optimum. © 2023 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/GLOBECOM54140.2023.10437798
SP  - 3070
EP  - 3075
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187344836&doi=10.1109%2fGLOBECOM54140.2023.10437798&partnerID=40&md5=3407e06752dd06c798af012e211b4933
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 5</p>
KW  - Alternative solutions
KW  - Containers
KW  - Critical service
KW  - Datacenter
KW  - Edge computing
KW  - Edge data
KW  - Energy
KW  - Energy utilization
KW  - Energy-aware provisioning
KW  - Green computing
KW  - Power management
KW  - Resources utilizations
KW  - Service requests
KW  - Time-critical
ER  - 

TY  - CONF
TI  - FunctionBench: A suite of workloads for serverless cloud function service
AU  - Kim, J.
AU  - Lee, K.
T2  - IEEE International Conference on Cloud Computing, CLOUD
AB  - Serverless computing is attracting considerable attention recently, but many published papers use micro-benchmarks for evaluation that might result in impracticality. To address this, we present FunctionBench, a suite of practical function workloads for public services. It contains realistic data-oriented applications that utilize various resources during execution. The source codes customized for various cloud service providers are publicly available. We are positive that it suggests opportunities for new function applications with lessen experiment setup overheads. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/CLOUD.2019.00091
VL  - 2019-July
SP  - 502
EP  - 504
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072317217&doi=10.1109%2fCLOUD.2019.00091&partnerID=40&md5=d0cba5a320a7466c6ad22981b03f0d34
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 169</p>
KW  - Benchmark
KW  - Benchmarking
KW  - Cloud
KW  - Cloud computing
KW  - Cloud service providers
KW  - Clouds
KW  - Computer programming
KW  - Computer science
KW  - Experiment set-up
KW  - FaaS
KW  - Micro-benchmark
KW  - New functions
KW  - Public services
KW  - Serverless
KW  - Workload
ER  - 

TY  - JOUR
T2  - Vmstat—Report virtual memory statistics
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003949265&partnerID=40&md5=18ba8b5b142f75196a2e82c9a1ff961e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Rhythm: Component-distinguishable workload deployment in datacenters
AU  - Zhao, L.
T2  - Proc. 15th Eur. Conf. Comput. Syst
DA  - 2020///
PY  - 2020
SP  - 1
EP  - 17
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121740906&partnerID=40&md5=71a7f3c61786dd341a562b21bcf4a600
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 11</p>
ER  - 

TY  - CONF
TI  - SEUSS: Skip redundant paths to make serverless fast
AU  - Cadden, J.
AU  - Unger, T.
AU  - Awad, Y.
AU  - Dong, H.
AU  - Krieger, O.
AU  - Appavoo, J.
T2  - Proceedings of the 15th European Conference on Computer Systems, EuroSys 2020
AB  - This paper presents a system-level method for achieving the rapid deployment and high-density caching of serverless functions in a FaaS environment. For reduced start times, functions are deployed from unikernel snapshots, bypassing expensive initialization steps. To reduce the memory footprint of snapshots we apply page-level sharing across the entire software stack that is required to run a function. We demonstrate the effects of our techniques by replacing Linux on the compute node of a FaaS platform architecture. With our prototype OS, the deployment time of a function drops from 100s of milliseconds to under 10 ms. Platform throughput improves by 51x on workload composed entirely of new functions. We are able to cache over 50,000 function instances in memory as opposed to 3,000 using standard OS techniques. In combination, these improvements give the FaaS platform a new ability to handle large-scale bursts of requests. © 2020 ACM.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3342195.3392698
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087107959&doi=10.1145%2f3342195.3392698&partnerID=40&md5=7c393950590d0b7cd76c31891e4148d7
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 134</p>
KW  - Cache memory
KW  - Computer operating systems
KW  - Deployment time
KW  - Initialization step
KW  - Memory footprint
KW  - New functions
KW  - Platform architecture
KW  - Rapid deployments
KW  - Redundant paths
KW  - Software stacks
ER  - 

TY  - JOUR
T2  - Apache OpenWhisk
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052391921&partnerID=40&md5=ab5ddcbe8d8cbd17177a5e410c26c70a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 68</p>
ER  - 

TY  - JOUR
T2  - Cloud Functions
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014238761&partnerID=40&md5=dc60b1e7e73ef6a1d4f3d133131e79cd
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 60</p>
ER  - 

TY  - JOUR
TI  - Agile cold starts for scalable serverless
AU  - Mohan, Anup
AU  - Sane, Harshad
AU  - Doshi, Kshitij
AU  - Edupuganti, Saikrishna
AU  - Nayak, Naren
AU  - Sukhomlinov, Vadim
T2  - Proc. HotCloud
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181985770&partnerID=40&md5=b4f008f48c40d00af00858bb2ebb5c86
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2</p>
ER  - 

TY  - JOUR
T2  - Memory and computing power at Amazon lambda
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003943978&partnerID=40&md5=9267b8050649fe4b7a0892c05281bab8
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - GrandSLAm: Guaranteeing SLAs for jobs in microservices execution frameworks
AU  - Kannan, R.S.
AU  - Subramanian, L.
AU  - Raju, A.
AU  - Ahn, J.
AU  - Mars, J.
AU  - Tang, L.
T2  - Proceedings of the 14th EuroSys Conference 2019
AB  - The microservice architecture has dramatically reduced user effort in adopting and maintaining servers by providing a catalog of functions as services that can be used as building blocks to construct applications. This has enabled datacenter operators to look at managing datacenter hosting microservices quite differently from traditional infrastructures. Such a paradigm shift calls for a need to rethink resource management strategies employed in such execution environments. We observe that the visibility enabled by a microservices execution framework can be exploited to achieve high throughput and resource utilization while still meeting Service Level Agreements, especially in multi-tenant execution scenarios. In this study, we present GrandSLAm, a microservice execution framework that improves utilization of datacenters hosting microservices. GrandSLAm estimates time of completion of requests propagating through individual microservice stages within an application. It then leverages this estimate to drive a runtime system that dynamically batches and reorders requests at each microservice in a manner where individual jobs meet their respective target latency while achieving high throughput. GrandSLAm significantly increases throughput by up to 3× compared to the our baseline, without violating SLAs for a wide range of real-world AI and ML applications. © 2019 Association for Computing Machinery.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3302424.3303958
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063902706&doi=10.1145%2f3302424.3303958&partnerID=40&md5=3421b52ef10bf2d471d82be441436ba2
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 130</p>
KW  - Building blockes
KW  - Computer systems
KW  - Execution environments
KW  - Execution framework
KW  - Execution scenario
KW  - Learning systems
KW  - Machine Learning
KW  - Microservice
KW  - Resource management
KW  - Resource utilizations
KW  - Service Level Agreements
KW  - Systems
ER  - 

TY  - JOUR
TI  - SAND: Towards High-Performance Serverless Computing
AU  - Akkus, I.E.
AU  - Chen, R.
AU  - Rimac, I.
AU  - Stein, M.
AU  - Satzke, K.
AU  - Beck, A.
AU  - Aditya, P.
AU  - Hilt, V.
T2  - USENIX ATC
DA  - 2018///
PY  - 2018
SP  - 923
EP  - 935
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062596504&partnerID=40&md5=69803d1ae53e00a17efa1267107cd265
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 44</p>
ER  - 

TY  - JOUR
TI  - Fifer: Tackling resource underutilization in the serverless era
AU  - Gunasekaran, Jashwant Raj
AU  - Thinakaran, Prashanth
AU  - Nachiappan, Nachiappan C
AU  - Kandemir, Mahmut Taylan
AU  - Das, Chita R
T2  - Proceedings of the ACM/IFIP/USENIX International Middleware Conference (Middleware)
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099724585&partnerID=40&md5=f426d3314eb0a2c92cd7bd633091a5b7
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 7</p>
ER  - 

TY  - CONF
TI  - Help Rather Than Recycle: Alleviating Cold Startup in Serverless Computing Through Inter-Function Container Sharing
AU  - Li, Z.
AU  - Guo, L.
AU  - Chen, Q.
AU  - Cheng, J.
AU  - Xu, C.
AU  - Zeng, D.
AU  - Song, Z.
AU  - Ma, T.
AU  - Yang, Y.
AU  - Li, C.
AU  - Guo, M.
T2  - Proceedings of the 2022 USENIX Annual Technical Conference, ATC 2022
AB  - In serverless computing, each function invocation is executed in a container (or a Virtual Machine), and container cold startup results in long response latency. We observe that some functions suffer from cold container startup, while the warm containers of other functions are idle. Based on the observation, other than booting a new container for a function from scratch, we propose to alleviate the cold startup by re-purposing a warm but idle container from another function. We implement a container management scheme, named Pagurus, to achieve the purpose. Pagurus comprises an intra-function manager for replacing an idle warm container to be a container that other functions can use without introducing additional security issues, an inter-function scheduler for scheduling containers between functions, and a sharingaware function balancer at the cluster-level for balancing the workload across different nodes. Experiments using Azure serverless traces show that Pagurus alleviates 84.6% of the cold startup, and the cold startup latency is reduced from hundreds of milliseconds to 16 milliseconds if alleviated. © 2022 USENIX Annual Technical Conference, ATC 2022.All rights reserved.
DA  - 2022///
PY  - 2022
SP  - 69
EP  - 84
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137787065&partnerID=40&md5=cae1d5bf3a230156bd84ee6d47324708
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 87</p>
KW  - Balancing
KW  - Container management
KW  - Containers
KW  - Management scheme
KW  - Scheduling
KW  - Security issues
ER  - 

TY  - JOUR
TI  - Workflow Scheduling in Serverless Edge Computing for the Industrial Internet of Things: A Learning Approach
AU  - Xie, R.
AU  - Gu, D.
AU  - Tang, Q.
AU  - Huang, T.
AU  - Yu, F.R.
T2  - IEEE Transactions on Industrial Informatics
AB  - Serverless edge computing is seen as a promising enabler to execute differentiated Industrial Internet of Things (IIoT) applications without managing the underlying servers and clusters. In IIoT serverless edge computing, IIoT workflow scheduling for cloud-edge collaborative processing is closely related to the service quality of users. However, serverless functions decomposed by IIoT applications are limited in their deployment at the edge due to the resource-constrained nature of edge infrastructures. In addition, the scheduling of complex IIoT applications supported by serverless computing is more challenging. Therefore, considering the limited function deployment and the complex dependencies of serverless workflows, we model the workflow application as directed acyclic graph and formulate the scheduling problem as a multiobjective optimization problem. A dueling double deep Q-network-based solution is proposed to make scheduling decisions under dynamically changing systems. Extensive simulation experiments are conducted to validate the superiority of the proposed scheme.  © 2005-2012 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/TII.2022.3217477
VL  - 19
IS  - 7
SP  - 8242
EP  - 8252
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141466873&doi=10.1109%2fTII.2022.3217477&partnerID=40&md5=66543484125a586a3e40a1558926b76a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 43</p>
KW  - Complex networks
KW  - Computational modelling
KW  - Deep learning
KW  - Deep reinforcement learning
KW  - Deep reinforcement learning (DRL)
KW  - Directed graphs
KW  - Edge computing
KW  - Industrial internet of thing
KW  - Internet of things
KW  - Job analysis
KW  - Job shop scheduling
KW  - Job-Shop scheduling
KW  - Multi-objectives optimization
KW  - multiobjective optimization
KW  - Multiobjective optimization
KW  - Processor scheduling
KW  - Quality of service
KW  - Reinforcement learning
KW  - Reinforcement learnings
KW  - serverless edge computing
KW  - Serverless edge computing
KW  - Task analysis
KW  - Wireless communications
KW  - workflow scheduling
KW  - Workflow scheduling
ER  - 

TY  - JOUR
TI  - Cloud programming simplified: A Berkeley view on serverless computing
AU  - Jonas, E.
T2  - Cloud Programming Simplified: A Berkeley View on Serverless Computing
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067433472&partnerID=40&md5=85287d7260661386b411bc1efadbde9b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 556</p>
ER  - 

TY  - CONF
TI  - Deadline-aware dynamic resource management in serverless computing environments
AU  - Mampage, A.
AU  - Karunasekera, S.
AU  - Buyya, R.
T2  - Proceedings - 21st IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing, CCGrid 2021
AB  - Serverless computing enables rapid application development and deployment by composing loosely coupled microservices at a scale. This emerging paradigm greatly unburdens the users of cloud environments, from the need to provision and manage the underlying cloud resources. With this shift in responsibility, the cloud provider faces the challenge of providing acceptable performance to the user without compromising on reliability, while having minimal knowledge of the application requirements. Sub-optimal resource allocations, specifically the CPU resources, could result in the violation of performance requirements of applications. Further, the fine-grained serverless billing model only charges for resource usage in terms of function execution time. At the same time, the provider has to maintain the underlying infrastructure in always-on mode to facilitate asynchronous function calls. Thus, achieving optimum utilization of cloud resources without compromising on application requirements is of high importance to the provider. Most of the current works only focus on minimizing function execution times caused by delays in infrastructure set up and reducing resource costs for the end-user. However, in this paper, we focus on both the provider and user's perspective and propose a function placement policy and a dynamic resource management policy for applications deployed in serverless computing environments. The policies minimize the resource consumption cost for the service provider while meeting the user's application requirement, i.e., deadline. The proposed solutions are sensitive to deadline and efficiently increase the resource utilization for the provider, while dynamically managing resources to improve function response times. We implement and evaluate our approach through simulation using ContainerCloudSim toolkit. The proposed function placement policy when compared with baseline scheduling techniques can reduce resource consumption by up to three times. The dynamic resource allocation policy when evaluated with a fixed resource allocation policy and a proportional CPU-shares policy shows improvements of up to 25% in meeting the required function deadlines. © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/CCGrid51090.2021.00058
SP  - 483
EP  - 492
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108510704&doi=10.1109%2fCCGrid51090.2021.00058&partnerID=40&md5=890ec416032c734fae061d3a62717af1
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 37</p>
KW  - Application requirements
KW  - Cluster computing
KW  - Computing environments
KW  - dy-namic resource management
KW  - Dynamic resource allocations
KW  - Dynamic resource management
KW  - Environmental management
KW  - function placement
KW  - Natural resources management
KW  - Optimal resource allocation
KW  - Performance requirements
KW  - Rapid application development
KW  - Resource allocation
KW  - Resource allocation policy
KW  - resource efficiency
KW  - serverless computing
ER  - 

TY  - CONF
TI  - SOCK: Rapid task provisioning with serverless-optimized containers
AU  - Oakes, E.
AU  - Yang, L.
AU  - Zhou, D.
AU  - Houck, K.
AU  - Harter, T.
AU  - Arpaci-Dusseau, A.C.
AU  - Arpaci-Dusseau, R.H.
T2  - Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018
AB  - Serverless computing promises to provide applications with cost savings and extreme elasticity. Unfortunately, slow application and container initialization can hurt common-case latency on serverless platforms. In this work, we analyze Linux container primitives, identifying scalability bottlenecks related to storage and network isolation. We also analyze Python applications from GitHub and show that importing many popular libraries adds about 100 ms to startup. Based on these findings, we implement SOCK, a container system optimized for serverless workloads. Careful avoidance of kernel scalability bottlenecks gives SOCK an 18× speedup over Docker. A generalized-Zygote provisioning strategy yields an additional 3× speedup. A more sophisticated three-tier caching strategy based on Zygotes provides a 45× speedup over SOCK without Zygotes. Relative to AWS Lambda and OpenWhisk, OpenLambda with SOCK reduces platform overheads by 2.8× and 5.3× respectively in an image processing case study. © Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018. All rights reserved.
DA  - 2020///
PY  - 2020
SP  - 57
EP  - 69
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077468687&partnerID=40&md5=4e3d486e4ba7e8e882f63bf763e28a81
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 305</p>
KW  - Caching strategy
KW  - Computer operating systems
KW  - Containers
KW  - Cost saving
KW  - Image processing
KW  - Network isolation
KW  - Scalability
ER  - 

TY  - CONF
TI  - Retention-Aware Container Caching for Serverless Edge Computing
AU  - Pan, L.
AU  - Wang, L.
AU  - Chen, S.
AU  - Liu, F.
T2  - Proceedings - IEEE INFOCOM
AB  - Serverless edge computing adopts an event-based model where Internet-of-Things (IoT) services are executed in lightweight containers only when requested, leading to significantly improved edge resource utilization. Unfortunately, the startup latency of containers degrades the responsiveness of IoT services dramatically. Container caching, while masking this latency, requires retaining resources thus compromising resource efficiency. In this paper, we study the retention-aware container caching problem in serverless edge computing. We leverage the distributed and heterogeneous nature of edge platforms and propose to optimize container caching jointly with request distribution. We reveal step by step that this joint optimization problem can be mapped to the classic ski-rental problem. We first present an online competitive algorithm for a special case where request distribution and container caching are based on a set of carefully designed probability distribution functions. Based on this algorithm, we propose an online algorithm called O-RDC for the general case, which incorporates the resource capacity and network latency by opportunistically distributing requests. We conduct extensive experiments to examine the performance of the proposed algorithms with both synthetic and real-world serverless computing traces. Our results show that ORDC outperforms existing caching strategies of current serverless computing platforms by up to 94.5% in terms of the overall system cost. © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/INFOCOM48880.2022.9796705
VL  - 2022-May
SP  - 1069
EP  - 1078
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133285886&doi=10.1109%2fINFOCOM48880.2022.9796705&partnerID=40&md5=5d8c244a23c67dea264b5e9f05ee86de
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 78</p>
KW  - container caching
KW  - Container caching
KW  - Containers
KW  - Distribution functions
KW  - edge computing
KW  - Edge computing
KW  - Edge resources
KW  - Event-based modeling
KW  - Internet of things
KW  - Joint optimization
KW  - Request distributions
KW  - Resource efficiencies
KW  - Resources utilizations
KW  - serverless computing
KW  - Serverless computing
KW  - ski-rental problem
KW  - Ski-rental problems
ER  - 

TY  - JOUR
T2  - Perf: Linux Profiling with Performance Counters
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877683951&partnerID=40&md5=e8f2b976737a97fabea50bca634d8503
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 125</p>
ER  - 

TY  - CONF
TI  - Catalyzer: Sub-millisecond startup for serverless computing with initialization-less booting
AU  - Du, D.
AU  - Yu, T.
AU  - Xia, Y.
AU  - Zang, B.
AU  - Yan, G.
AU  - Qin, C.
AU  - Wu, Q.
AU  - Chen, H.
T2  - International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS
AB  - Serverless computing promises cost-efficiency and elasticity for high-productive software development. To achieve this, the serverless sandbox system must address two challenges: strong isolation between function instances, and low startup latency to ensure user experience. While strong isolation can be provided by virtualization-based sandboxes, the initialization of sandbox and application causes non-negligible startup overhead. Conventional sandbox systems fall short in low-latency startup due to their application-agnostic nature: they can only reduce the latency of sandbox initialization through hypervisor and guest kernel customization, which is inadequate and does not mitigate the majority of startup overhead. This paper proposes Catalyzer, a serverless sandbox system design providing both strong isolation and extremely fast function startup. Instead of booting from scratch, Catalyzer restores a virtualization-based function instance from a well-formed checkpoint image and thereby skips the initialization on the critical path (init-less). Catalyzer boosts the restore performance by on-demand recovering both user-level memory state and system state. We also propose a new OS primitive, sfork (sandbox fork), to further reduce the startup latency by directly reusing the state of a running sandbox instance. Fundamentally, Catalyzer removes the initialization cost by reusing state, which enables general optimizations for diverse serverless functions. The evaluation shows that Catalyzer reduces startup latency by orders of magnitude, achieves <1ms latency in the best case, and significantly reduces the end-to-end latency for real-world workloads. Catalyzer has been adopted by Ant Financial, and we also present lessons learned from industrial development. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3373376.3378512
SP  - 467
EP  - 481
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082400852&doi=10.1145%2f3373376.3378512&partnerID=40&md5=7efa51b655f8910603f7a378b26b339f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 261</p>
KW  - Checkpoint and restore
KW  - Cost elasticities
KW  - Cost-efficiency
KW  - Low latency
KW  - Operating system
KW  - Restoration
KW  - Serverless computing
KW  - Software design
KW  - Start-up overheads
KW  - Startup latency
KW  - Users' experiences
KW  - Virtual reality
KW  - Virtualization
KW  - Virtualizations
ER  - 

TY  - CONF
TI  - Serverless in the wild: Characterizing and optimizing the serverless workload at a large cloud provider
AU  - Shahrad, M.
AU  - Fonseca, R.
AU  - Goiri, Í.
AU  - Chaudhry, G.
AU  - Batum, P.
AU  - Cooke, J.
AU  - Laureano, E.
AU  - Tresness, C.
AU  - Russinovich, M.
AU  - Bianchini, R.
T2  - Proceedings of the 2020 USENIX Annual Technical Conference, ATC 2020
AB  - Function as a Service (FaaS) has been gaining popularity as a way to deploy computations to serverless backends in the cloud. This paradigm shifts the complexity of allocating and provisioning resources to the cloud provider, which has to provide the illusion of always-available resources (i.e., fast function invocations without cold starts) at the lowest possible resource cost. Doing so requires the provider to deeply understand the characteristics of the FaaS workload. Unfortunately, there has been little to no public information on these characteristics. Thus, in this paper, we first characterize the entire production FaaS workload of Azure Functions. We show for example that most functions are invoked very infrequently, but there is an 8-order-of-magnitude range of invocation frequencies. Using observations from our characterization, we then propose a practical resource management policy that significantly reduces the number of function cold starts, while spending fewer resources than state-of-the-practice policies. Copyright © Proc. of the 2020 USENIX Annual Technical Conference, ATC 2020. All rights reserved.
DA  - 2020///
PY  - 2020
SP  - 205
EP  - 218
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091934381&partnerID=40&md5=16dd6343f7a2bb6ef374f99e1bb924da
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 566</p>
KW  - Cloud providers
KW  - Cold start
KW  - Paradigm shifts
KW  - Public information
KW  - Resource costs
KW  - Resource management policy
KW  - State of the practice
ER  - 

TY  - JOUR
TI  - Minimal Cost Server Configuration for Meeting Time-Varying Resource Demands in Cloud Centers
AU  - Liu, C.
AU  - Li, K.
AU  - Li, K.
T2  - IEEE Transactions on Parallel and Distributed Systems
AB  - We consider the minimal cost server configuration for meeting resource demands over multiple time slots. Specifically, there are some heterogeneous servers. Each server is specified by a cost, certain amounts of several resources, and an active interval, i.e., the time interval that the server is planed to work. There are different overall demands for each type of resource over different time slots. A feasible solution is a set of servers such that at any time slot, the resources provided by the selected servers are at least their corresponding demands. Notice that, a selected server can not provide resources for the time slots out of its active interval. The total cost of the solution is the summation of the costs of all selected servers. The goal is to find a feasible solution with minimal total cost. This problem is proved to be NP-hard due to a reduction from the multidimensional knapsack problem (MKP), which is a well-known NP-hard combinational optimization problem. To solve our problem, we present a randomized approximation algorithm called partial rounding algorithm (PRA), which guarantees O (KT)-approximation, i.e., η (KT)-approximation, where K is the number of kinds of resources, T is the number of time slots, and η is a positive constant. Furthermore, to minimize η as much as possible, we propose a varied Chernoff bound and apply it in PRA. We perform extensive experiments with random inputs and a specific application input. The results show that PRA with our varied Chernoff conclusion can find solutions closing to the optimal one. © 1990-2012 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/TPDS.2018.2836452
VL  - 29
IS  - 11
SP  - 2503
EP  - 2513
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047012237&doi=10.1109%2fTPDS.2018.2836452&partnerID=40&md5=fb9dc8f6a33375053bb3a7402c4e5d74
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 30</p>
KW  - Approximation algorithms
KW  - Bandwidth
KW  - Chernoff bounds
KW  - Cloud computing
KW  - Cost minimization
KW  - Costs
KW  - meeting resource demands
KW  - Optimization
KW  - Randomized approximation
KW  - randomized approximation algorithm
KW  - Resource demands
KW  - Resource management
KW  - server configuration
KW  - Servers
KW  - varied Chernoff bound
ER  - 

TY  - CONF
TI  - Prebaking functions to warm the serverless cold start
AU  - Silva, P.
AU  - Fireman, D.
AU  - Pereira, T.E.
T2  - Middleware 2020 - Proceedings of the 2020 21st International Middleware Conference
AB  - Function-as-service (FaaS) platforms promise a simpler programming model for cloud computing, in which the developers concentrate on writing its applications. In contrast, platform providers take care of resource management and administration. As FaaS users are billed based on the execution of the functions, platform providers have a natural incentive not to keep idle resources running at the platform's expense. However, this strategy may lead to the cold start issue, in which the execution of a function is delayed because there is no ready resource to host the execution. Cold starts can take hundreds of milliseconds to seconds and have been a prohibitive and painful disadvantage for some applications. This work describes and evaluates a technique to start functions, which restores snapshots from previously executed function processes. We developed a prototype of this technique based on the CRIU process checkpoint/restore Linux tool. We evaluate this prototype by running experiments that compare its start-up time against the standard Unix process creation/start-up procedure. We analyze the following three functions: i) a "do-nothing" function, ii) an Image Resizer function, and iii) a function that renders Markdown files. The results attained indicate that the technique can improve the start-up time of function replicas by 40% (in the worst case of a "do-nothing" function) and up to 71% for the Image Resizer one. Further analysis indicates that the runtime initialization is a key factor, and we confirmed it by performing a sensitivity analysis based on synthetically generated functions of different code sizes. These experiments demonstrate that it is critical to decide when to create a snapshot of a function. When one creates the snapshots of warm functions, the speed-up achieved by the prebaking technique is even higher: the speed-up increases from 127.45% to 403.96%, for a small, synthetic function; and for a bigger, synthetic function, this ratio increases from 121.07% to 1932.49%. © 2020 Association for Computing Machinery.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3423211.3425682
SP  - 1
EP  - 13
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098482026&doi=10.1145%2f3423211.3425682&partnerID=40&md5=a504f228433628dd70e0d3a74365fdba
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 98</p>
KW  - Cloud
KW  - Cold start
KW  - Computer operating systems
KW  - Faas
KW  - Generated function
KW  - Image enhancement
KW  - ITS applications
KW  - Key factors
KW  - Middleware
KW  - Performance evaluation
KW  - Process creation
KW  - Programming models
KW  - Resource management
KW  - Sensitivity analysis
KW  - Serverless
KW  - Startup time
ER  - 

TY  - JOUR
T2  - Amazon Lambda
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971535952&partnerID=40&md5=c81820cd628831a2fb7ca371091d198f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 20</p>
ER  - 

TY  - CONF
TI  - Peeking behind the curtains of serverless platforms
AU  - Wang, L.
AU  - Li, M.
AU  - Zhang, Y.
AU  - Ristenpart, T.
AU  - Swift, M.
T2  - Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018
AB  - Serverless computing is an emerging paradigm in which an application's resource provisioning and scaling are managed by third-party services. Examples include AWS Lambda, Azure Functions, and Google Cloud Functions. Behind these services' easy-to-use APIs are opaque, complex infrastructure and management ecosystems. Taking on the viewpoint of a serverless customer, we conduct the largest measurement study to date, launching more than 50,000 function instances across these three services, in order to characterize their architectures, performance, and resource management efficiency. We explain how the platforms isolate the functions of different accounts, using either virtual machines or containers, which has important security implications. We characterize performance in terms of scalability, coldstart latency, and resource efficiency, with highlights including that AWS Lambda adopts a bin-packing-like strategy to maximize VM memory utilization, that severe contention between functions can arise in AWS and Azure, and that Google had bugs that allow customers to use resources for free. © Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018. All rights reserved.
DA  - 2020///
PY  - 2020
SP  - 133
EP  - 145
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077448177&partnerID=40&md5=4c9a798d44b920fec75b104bd8d7da8f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 444</p>
KW  - Bin packing
KW  - Complex infrastructures
KW  - Efficiency
KW  - Measurement study
KW  - Memory architecture
KW  - Memory utilization
KW  - Resource efficiencies
KW  - Resource management
KW  - Security implications
KW  - Third party services
ER  - 

TY  - JOUR
TI  - Mobile edge computing: A key technology towards 5G
AU  - Hu, Y.C.
AU  - Patel, M.
AU  - Sabella, D.
AU  - Sprecher, N.
AU  - Young, V.
T2  - Mobile Edge Computing-A Key Technology Towards 5G
DA  - 2015///
PY  - 2015
IS  - 11
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963757817&partnerID=40&md5=333635cdcabb534a28c501ea6b0e8ae9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1947</p>
ER  - 

TY  - JOUR
TI  - Navigating Performance-Efficiency Tradeoffs in Serverless Computing: Deduplication to the Rescue!
AU  - Saxena, D.
AU  - Ji, T.
AU  - Singhvi, A.
AU  - Khalid, J.
AU  - Akella, A.
T2  - Operating Systems Review (ACM)
AB  - Navigating the performance and efficiency trade-offs is critical for serverless platforms, where the providers ideally want to give the illusion of warm function startups while maintaining low resource costs. Limited controls, provided via toggling sandboxes between warm and cold states and keepalives, force operators to sacrifice significant resources to achieve good performance.  © 2023 Copyright is held by the owner/author(s).
DA  - 2023///
PY  - 2023
DO  - 10.1145/3606557.3606564
VL  - 57
IS  - 1
SP  - 47
EP  - 53
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164254628&doi=10.1145%2f3606557.3606564&partnerID=40&md5=39e9c8f5f6fbb95bc0a523163a00380b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
KW  - Cloud Computing
KW  - Cloud-computing
KW  - Cold state
KW  - Commerce
KW  - Deduplication
KW  - Economic and social effects
KW  - Efficiency
KW  - Memory deduplication
KW  - Memory Deduplication
KW  - Performance
KW  - Performance efficiency
KW  - Resource costs
KW  - Serverless
KW  - Trade off
KW  - Virtualization
KW  - Virtualizations
ER  - 

TY  - CONF
TI  - Architectural implications of function-as-a-service computing
AU  - Shahrad, M.
AU  - Balkind, J.
AU  - Wentzlaff, D.
T2  - Proceedings of the Annual International Symposium on Microarchitecture, MICRO
AB  - Serverless computing is a rapidly growing cloud application model, popularized by Amazon's Lambda platform. Serverless cloud services provide fine-grained provisioning of resources, which scale automatically with user demand. Function-as-a-Service (FaaS) applications follow this serverless model, with the developer providing their application as a set of functions which are executed in response to a user- or system-generated event. Functions are designed to be short-lived and execute inside containers or virtual machines, introducing a range of system-level overheads. This paper studies the architectural implications of this emerging paradigm. Using the commercial-grade Apache OpenWhisk FaaS platform on real servers, this work investigates and identifies the architectural implications of FaaS serverless computing. The workloads, along with the way that FaaS inherently interleaves short functions from many tenants frustrates many of the locality-preserving architectural structures common in modern processors. In particular, we find that: FaaS containerization brings up to 20x slowdown compared to native execution, cold-start can be over 10x a short function's execution time, branch mispredictions per kilo-instruction are 20x higher for short functions, memory bandwidth increases by 6x due to the invocation pattern, and IPC decreases by as much as 35% due to inter-function interference. We open-source FaaSProfiler, the FaaS testing and profiling platform that we developed for this work. © 2019 Association for Computing Machinery.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3352460.3358296
SP  - 1063
EP  - 1075
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074454583&doi=10.1145%2f3352460.3358296&partnerID=40&md5=a1c5e1ae40346c4b02458962a3f50a50
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 176</p>
KW  - Architectural structure
KW  - Architecture
KW  - Branch mispredictions
KW  - Cloud
KW  - Cloud applications
KW  - Clouds
KW  - Computer architecture
KW  - Containers
KW  - Faas
KW  - Function-as-a-service
KW  - Locality-preserving
KW  - Modern processors
KW  - Open whisk
KW  - Serverless
ER  - 

TY  - CONF
TI  - Serverless Computing: Design, Implementation, and Performance
AU  - McGrath, G.
AU  - Brenner, P.R.
T2  - Proceedings - IEEE 37th International Conference on Distributed Computing Systems Workshops, ICDCSW 2017
AB  - We present the design of a novel performance-oriented serverless computing platform implemented in. NET, deployed in Microsoft Azure, and utilizing Windows containers as function execution environments. Implementation challenges such as function scaling and container discovery, lifecycle, and reuse are discussed in detail. We propose metrics to evaluate the execution performance of serverless platforms and conduct tests on our prototype as well as AWS Lambda, Azure Functions, Google Cloud Functions, and IBM's deployment of Apache OpenWhisk. Our measurements show the prototype achieving greater throughput than other platforms at most concurrency levels, and we examine the scaling and instance expiration trends in the implementations. Additionally, we discuss the gaps and limitations in our current design, propose possible solutions, and highlight future research. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/ICDCSW.2017.36
SP  - 405
EP  - 410
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027531918&doi=10.1109%2fICDCSW.2017.36&partnerID=40&md5=7922eccabb0c16d5ecf2a4aba6079cf0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 282</p>
KW  - Apache OpenWhisk
KW  - AWS Lambda
KW  - Azure Functions
KW  - Containers
KW  - Distributed computer systems
KW  - FaaS
KW  - Function-as-a-Service
KW  - Google Cloud Functions
KW  - IBM OpenWhisk
KW  - serverless computing
KW  - serverless performance
KW  - Web services
KW  - Windows operating system
ER  - 

TY  - JOUR
TI  - Performance modeling of serverless computing platforms
AU  - Mahmoudi, N.
AU  - Khazaei, H.
T2  - IEEE Transactions on Cloud Computing
DA  - 2020///
PY  - 2020
VL  - 10
IS  - 4
SP  - 2834
EP  - 2847
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108777806&partnerID=40&md5=5a99e5f8d064071dd6c17aeb8dee1182
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 78</p>
ER  - 

TY  - JOUR
T2  - Energy saving strategy
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003935101&partnerID=40&md5=b793f4c6d2a34ba2807b73603c50d9bc
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Vector bin packing with multiple-choice
AU  - Patt-Shamir, B.
AU  - Rawitz, D.
T2  - Discrete Applied Mathematics
AB  - We consider a variant of bin packing called multiple-choice vector bin packing. In this problem, we are given a set of n items, where each item can be selected in one of several D-dimensional incarnations. We are also given T bin types, each with its own cost andD-dimensional size. Our goal is to pack the items in a set of bins of minimum overall cost. The problem is motivated by scheduling in networks with guaranteed quality of service (QoS), but due to its general formulation it has many other applications as well. We present an approximation algorithm that is guaranteed to produce a solution whose cost is about lnD times the optimum. For the running time to be polynomial we require D=O(1) and T=O(logn). This extends previous results for vector bin packing, in which each item has a single incarnation and there is only one bin type. To obtain our result we also present a PTAS for the multiple-choice version of multidimensional knapsack, where we are given only one bin and the goal is to pack a maximum weight set of (incarnations of) items in that bin. © 2012 Elsevier Ltd. All rights reserved.
DA  - 2012///
PY  - 2012
DO  - 10.1016/j.dam.2012.02.020
VL  - 160
IS  - 10-11
SP  - 1591
EP  - 1600
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859922814&doi=10.1016%2fj.dam.2012.02.020&partnerID=40&md5=d42743c934e12933297b86bcf7932576
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 25</p>
KW  - Approximation algorithms
KW  - Bin packing
KW  - Bins
KW  - Costs
KW  - Guaranteed quality
KW  - Multidimensional knapsack
KW  - Multiple-choice multidimensional knapsack
KW  - Multiple-choice vector bin packing
KW  - Other applications
KW  - Overall costs
KW  - Quality of service
KW  - Running time
KW  - Vectors
KW  - Weight set
ER  - 

TY  - JOUR
T2  - Azure Functions
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041179679&partnerID=40&md5=4b7dcbec72e81f7c2321f9ba9504130e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 109</p>
ER  - 

TY  - JOUR
TI  - On the energy-proportionality of data center networks
AU  - Ruiu, P.
AU  - Fiandrino, C.
AU  - Giaccone, P.
AU  - Bianco, A.
AU  - Kliazovich, D.
AU  - Bouvry, P.
T2  - IEEE Transactions on Sustainable Computing
AB  - Data centers provision industry and end users with the necessary computing and communication resources to access the vast majority of services online and on a pay-as-you-go basis. In this paper, we study the problem of energy proportionality in data center networks (DCNs). Devices are energy proportional when any increase of the load corresponds to a proportional increase of energy consumption. In data centers, energy consumption is concern as it considerably impacts on the operational expenses (OPEX) of the operators. In our analysis, we investigate the impact of three different allocation policies on the energy proportionality of computing and networking equipment for different DCNs, including 2-Tier, 3-Tier, and Jupiter topologies. For evaluation, the size of the DCNs varies to accommodate up to several thousands of computing servers. Validation of the analysis is conducted through simulations. We propose new metrics with the objective to characterize in a holistic manner the energy proportionality in data centers. The experiments unveil that, when consolidation policies are in place and regardless of the type of architecture, the size of the DCN plays a key role, i.e., larger DCNs containing thousands of servers are more energy proportional than small DCNs. © 2016 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/TSUSC.2017.2711967
VL  - 2
IS  - 2
SP  - 197
EP  - 210
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061511614&doi=10.1109%2fTSUSC.2017.2711967&partnerID=40&md5=3c022d9a61ff6d9c6cf847627cdfca62
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 18</p>
KW  - Allocation policies
KW  - Communication resources
KW  - data center networking
KW  - Data center networkings
KW  - Data center networks
KW  - Data center networks (DCNs)
KW  - Energy efficiency
KW  - Energy proportionalities
KW  - Energy utilization
KW  - Energy-efficiency
KW  - energy-proportionality
KW  - Green computing
KW  - Networking equipment
KW  - Operational expense
ER  - 

TY  - JOUR
TI  - A survey of virtual machine management in edge computing
AU  - Tao, Z.
AU  - Xia, Q.
AU  - Hao, Z.
AU  - Li, C.
AU  - Ma, L.
AU  - Yi, S.
AU  - Li, Q.
T2  - Proc. IEEE
DA  - 2019///
PY  - 2019
VL  - 107
IS  - 8
SP  - 1482
EP  - 1499
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082883374&partnerID=40&md5=4e72932e2486eef6c7c69ed2ee0ff079
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 106</p>
ER  - 

TY  - JOUR
TI  - Firecracker: Lightweight virtualization for serverless applications
AU  - Agache, A.
T2  - USENIX NSDI
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100498805&partnerID=40&md5=c2c620de699387405713dd03d0904cde
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 10</p>
ER  - 

TY  - CONF
TI  - An Open-Source Benchmark Suite for Microservices and Their Hardware-Software Implications for Cloud & Edge Systems
AU  - Gan, Y.
AU  - Zhang, Y.
AU  - Cheng, D.
AU  - Shetty, A.
AU  - Rathi, P.
AU  - Katarki, N.
AU  - Bruno, A.
AU  - Hu, J.
AU  - Ritchken, B.
AU  - Jackson, B.
AU  - Hu, K.
AU  - Pancholi, M.
AU  - He, Y.
AU  - Clancy, B.
AU  - Colen, C.
AU  - Wen, F.
AU  - Leung, C.
AU  - Wang, S.
AU  - Zaruvinsky, L.
AU  - Espinosa, M.
AU  - Lin, R.
AU  - Liu, Z.
AU  - Padilla, J.
AU  - Delimitrou, C.
T2  - International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS
AB  - Cloud services have recently started undergoing a major shift from monolithic applications, to graphs of hundreds of loosely-coupled microservices. Microservices fundamentally change a lot of assumptions current cloud systems are designed with, and present both opportunities and challenges when optimizing for quality of service (QoS) and utilization. In this paper we explore the implications microservices have across the cloud system stack. We first present Death- StarBench, a novel, open-source benchmark suite built with microservices that is representative of large end-to-end services, modular and extensible. DeathStarBench includes a social network, a media service, an e-commerce site, a banking system, and IoT applications for coordination control of UAV swarms. We then use DeathStarBench to study the architectural characteristics of microservices, their implications in networking and operating systems, their challenges with respect to cluster management, and their trade-offs in terms of application design and programming frameworks. Finally, we explore the tail at scale effects of microservices in real deployments with hundreds of users, and highlight the increased pressure they put on performance predictability. © 2019 Association for Computing Machinery.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3297858.3304013
SP  - 3
EP  - 18
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064688619&doi=10.1145%2f3297858.3304013&partnerID=40&md5=b9f662fba2a35d83eb40d32009cf7676
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 538</p>
KW  - acceleration
KW  - Benchmark suites
KW  - cloud computing
KW  - Cloud systems
KW  - Cloud-computing
KW  - Cluster computing
KW  - cluster management
KW  - Cluster management
KW  - Datacenter
KW  - datacenters
KW  - Economic and social effects
KW  - Field programmable gate arrays (FPGA)
KW  - fpga
KW  - Fpgum
KW  - Microservice
KW  - microservices
KW  - Open source software
KW  - Open systems
KW  - Open-source
KW  - QoS
KW  - Quality of service
KW  - Quality-of-service
KW  - serverless
KW  - Serverless
ER  - 

TY  - CONF
TI  - Sparrow: Distributed, low latency scheduling
AU  - Ousterhout, K.
AU  - Wendell, P.
AU  - Zaharia, M.
AU  - Stoica, I.
T2  - SOSP 2013 - Proceedings of the 24th ACM Symposium on Operating Systems Principles
AB  - Large-scale data analytics frameworks are shifting towards shorter task durations and larger degrees of parallelism to provide low latency. Scheduling highly parallel jobs that complete in hundreds of milliseconds poses a major challenge for task schedulers, which will need to schedule millions of tasks per second on appropriate machines while offering millisecond-level latency and high availability. We demonstrate that a decentralized, randomized sampling approach provides near-optimal performance while avoiding the throughput and availability limitations of a centralized design. We implement and deploy our scheduler, Sparrow, on a 110-machine cluster and demonstrate that Sparrow performs within 12% of an ideal scheduler. © 2013 ACM.
DA  - 2013///
PY  - 2013
DO  - 10.1145/2517349.2522716
SP  - 69
EP  - 84
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889640333&doi=10.1145%2f2517349.2522716&partnerID=40&md5=3453ba1344af99cae7b8501554703616
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 513</p>
KW  - High availability
KW  - Highly parallels
KW  - Large-scale datum
KW  - Low latency
KW  - Near-optimal performance
KW  - Randomized sampling
KW  - Scheduling
ER  - 

TY  - JOUR
T2  - Apache OpenWhisk
DA  - 2016///
PY  - 2016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071948507&partnerID=40&md5=cae79e308d1a4a884fbd05575f40dcfd
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 9</p>
ER  - 

TY  - JOUR
AU  - Simon, Eismann
AU  - Joel, Scheuner
T2  - A Review of Serverless Use Cases and their Characteristics
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098402238&partnerID=40&md5=1ef65986f58542e24905bb400e205491
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - Agile cold starts for scalable serverless
AU  - Mohan, A.
AU  - Sane, H.
AU  - Doshi, K.
AU  - Edupuganti, S.
AU  - Nayak, N.
AU  - Sukhomlinov, V.
T2  - 11th USENIX Workshop on Hot Topics in Cloud Computing, HotCloud 2019, co-located with USENIX ATC 2019
AB  - The Serverless or Function-as-a-Service (FaaS) model capitalizes on lightweight execution by packaging code and dependencies together for just-in-time dispatch. Often a container environment has to be set up afresh– a condition called “cold start", and in such cases, performance suffers and overheads mount, both deteriorating rapidly under high concurrency. Caching and reusing previously employed containers ties up memory and risks information leakage. Latency for cold starts is frequently due to work and wait-times in setting up various dependencies – such as in initializing networking elements. This paper proposes a solution that pre-crafts such resources and then dynamically reassociates them with baseline containers. Applied to networking, this approach demonstrates an order of magnitude gain in cold starts, negligible memory consumption, and flat startup time under rising concurrency. © 2019 USENIX Association. All rights reserved.
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084160535&partnerID=40&md5=78d805902ca3dc9483d26084f74a76cf
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 145</p>
KW  - Cloud computing
KW  - Cold start
KW  - Containers
KW  - High concurrencies
KW  - Information leakage
KW  - Just in time
KW  - Memory consumption
KW  - Startup time
KW  - Wait time
ER  - 

TY  - JOUR
T2  - Brigade-workflows
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098406206&partnerID=40&md5=a6596e197849ea4e104d741f94a2badb
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2</p>
ER  - 

TY  - CONF
TI  - Proteus: Agile ML elasticity through tiered reliability in dynamic resource markets
AU  - Harlap, A.
AU  - Tumanov, A.
AU  - Chung, A.
AU  - Ganger, G.R.
AU  - Gibbons, P.B.
T2  - Proceedings of the 12th European Conference on Computer Systems, EuroSys 2017
AB  - Many shared computing clusters allow users to utilize excess idle resources at lower cost or priority, with the proviso that some or all may be taken away at any time. But, exploiting such dynamic resource availability and the often fluctuating markets for them requires agile elasticity and effective acquisition strategies. Proteus aggressively exploits such transient revocable resources to do machine learning (ML) cheaper and/or faster. Its parameter server framework, AgileML, efficiently adapts to bulk additions and revocations of transient machines, through a novel 3-stage active-backup approach, with minimal use of more costly non-transient resources. Its BidBrain component adaptively allocates resources from multiple EC2 spot markets to minimize average cost per work as transient resource availability and cost change over time. Our evaluations show that Proteus reduces cost by 85% relative to non-transient pricing, and by 43% relative to previous approaches, while simultaneously reducing runtimes by up to 37%. © 2017 Copyright held by the owner/author(s).
DA  - 2017///
PY  - 2017
DO  - 10.1145/3064176.3064182
SP  - 589
EP  - 604
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019230467&doi=10.1145%2f3064176.3064182&partnerID=40&md5=ae793a495f02a4472e2a897874109dc2
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 75</p>
KW  - Acquisition strategies
KW  - Agile manufacturing systems
KW  - Average cost
KW  - Commerce
KW  - Computing clusters
KW  - Cost changes
KW  - Costs
KW  - Dynamic resource availabilities
KW  - Dynamic resources
KW  - Elasticity
KW  - Learning systems
KW  - Spot market
KW  - Transient resources
ER  - 

TY  - CONF
TI  - FNSched: An efficient scheduler for serverless functions
AU  - Suresh, A.
AU  - Gandhi, A.
T2  - WOSC 2019 - Proceedings of the 2019 5th International Workshop on Serverless Computing, Part of Middleware 2019
AB  - An imminent challenge in the serverless computing landscape is the escalating cost of infrastructure needed to handle the growing traffic at scale. This work presents FnSched, a function-level scheduler designed to minimize provider resource costs while meeting customer performance requirements. FnSched works by carefully regulating the resource usage of colocated functions on each invoker, and autoscaling capacity by concentrating load on few invokers in response to varying traffic. We implement a prototype of FnSched and show that, compared to existing baselines, FnSched significantly improves resource efficiency, by as much as 36%–55%, while providing acceptable application latency. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3366623.3368136
SP  - 19
EP  - 24
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078922442&doi=10.1145%2f3366623.3368136&partnerID=40&md5=68847dba229c4bab2960be8b3fe1f625
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 67</p>
KW  - Autoscaling
KW  - Co-located
KW  - Escalating costs
KW  - Function levels
KW  - Middleware
KW  - Performance requirements
KW  - Resource costs
KW  - Resource efficiencies
KW  - Resource usage
KW  - Scheduling
ER  - 

TY  - JOUR
T2  - Serverless Functions
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098525877&partnerID=40&md5=cb5dd68616db3c7763503fa350dfbdc5
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - System response time and user satisfaction: An experimental study of browser-based applications
AU  - Hoxmeier, J.A.
AU  - DiCesare, C.
T2  - AMCIS 2000 Proceedings
DA  - 2000///
PY  - 2000
SP  - 140
EP  - 145
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0242530345&partnerID=40&md5=133e2518e2900702e5a142408a03bb44
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 138</p>
ER  - 

TY  - JOUR
TI  - Exploiting intra-request slack to improve SSD performance
AU  - Elyasi, N.
AU  - Arjomand, M.
AU  - Sivasubramaniam, A.
AU  - Kandemir, M.T.
AU  - Das, C.R.
AU  - Jung, M.
T2  - ACM SIGPLAN Notices
AB  - With Solid State Disks (SSDs) offering high degrees of parallelism, SSD controllers place data and direct requests to exploit the maximum offered hardware parallelism. In the quest to maximize parallelism and utilization, sub-requests of a request that are directed to different flash chips by the scheduler can experience differential wait times since their individual queues are not coordinated and load balanced at all times. Since the macro request is considered complete only when its last sub-request completes, some of its sub-requests that complete earlier have to necessarily wait for this last sub-request. This paper opens the door to a new class of schedulers to leverage such slack between sub-requests in order to improve response times. Specifically, the paper presents the design and implementation of a slack-enabled re-ordering scheduler, called Slacker, for sub-requests issued to each flash chip. Layered under a modern SSD request scheduler, Slacker estimates the slack of each incoming sub-request to a flash chip and allows them to jump ahead of existing sub-requests with sufficient slack so as to not detrimentally impact their response times. Slacker is simple to implement and imposes only marginal additions to the hardware. Using a spectrum of 21 workloads with diverse read-write characteristics, we show that Slacker provides as much as 19.5%, 13% and 14.5% improvement in response times, with average improvements of 12%, 6.5% and 8.5%, for write-intensive, read-intensive and read-write balanced workloads, respectively. © 2017 ACM.
DA  - 2017///
PY  - 2017
DO  - 10.1145/3037697.3037728
VL  - 52
IS  - 4
SP  - 375
EP  - 388
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066424914&doi=10.1145%2f3037697.3037728&partnerID=40&md5=c998df4eaeae91a0d7036dd13eded94c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 37</p>
KW  - Balanced work-load
KW  - Computer programming
KW  - Computer science
KW  - Design and implementations
KW  - Flash chips
KW  - Hardware parallelisms
KW  - Intra-request slack
KW  - Load-balanced
KW  - Re orderings
KW  - Scheduling
KW  - Solid state disks
KW  - Ssd
KW  - Wait time
ER  - 

TY  - CONF
TI  - Mark: Exploiting cloud services for cost-effective, slo-aware machine learning inference serving
AU  - Zhang, C.
AU  - Yu, M.
AU  - Wang, W.
AU  - Yan, F.
T2  - Proceedings of the 2019 USENIX Annual Technical Conference, USENIX ATC 2019
AB  - The advances of Machine Learning (ML) have sparked a growing demand of ML-as-a-Service: developers train ML models and publish them in the cloud as online services to provide low-latency inference at scale. The key challenge of ML model serving is to meet the response-time Service-Level Objectives (SLOs) of inference workloads while minimizing the serving cost. In this paper, we tackle the dual challenge of SLO compliance and cost effectiveness with MArk (Model Ark), a general-purpose inference serving system built in Amazon Web Services (AWS). MArk employs three design choices tailor-made for inference workload. First, MArk dynamically batches requests and opportunistically serves them using expensive hardware accelerators (e.g., GPU) for improved performance-cost ratio. Second, instead of relying on feedback control scaling or over-provisioning to serve dynamic workload, which can be too slow or too expensive for inference serving, MArk employs predictive autoscaling to hide the provisioning latency at low cost. Third, given the stateless nature of inference serving, MArk exploits the flexible, yet costly serverless instances to cover the occasional load spikes that are hard to predict. We evaluated the performance of MArk using several state-of-the-art ML models trained in popular frameworks including TensorFlow, MXNet, and Keras. Compared with the premier industrial ML serving platform SageMaker, MArk reduces the serving cost up to 7.8× while achieving even better latency performance. © Proceedings of the 2019 USENIX Annual Technical Conference, USENIX ATC 2019. All rights reserved.
DA  - 2019///
PY  - 2019
SP  - 1049
EP  - 1062
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076183718&partnerID=40&md5=de8d4f1dca43c789793276270808d25f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 258</p>
KW  - Amazon web services
KW  - Cost effectiveness
KW  - Costs
KW  - Expensive hardware
KW  - Latency performance
KW  - Machine learning
KW  - On-line service
KW  - Over provisioning
KW  - Performance-cost ratio
KW  - Service level objective
KW  - State of the art
KW  - Web services
ER  - 

TY  - JOUR
T2  - Microsoft Azure Serverless Computing
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072040842&partnerID=40&md5=0510241c379401e49ccb91a3f847e916
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 6</p>
ER  - 

TY  - CONF
TI  - Hawk: Hybrid datacenter scheduling
AU  - Delgado, P.
AU  - Dinu, F.
AU  - Kermarrec, A.-M.
AU  - Zwaenepoel, W.
T2  - Proceedings of the 2015 USENIX Annual Technical Conference, USENIX ATC 2015
AB  - This paper addresses the problem of efficient scheduling of large clusters under high load and heterogeneous workloads. A heterogeneousworkload typically consists of many short jobs and a small number of large jobs that consume the bulk of the cluster's resources. Recent work advocates distributed scheduling to overcome the limitations of centralized schedulers for large clusters with many competing jobs. Such distributed schedulers are inherently scalable, but may make poor scheduling decisions because of limited visibility into the overall resource usage in the cluster. In particular, we demonstrate that under high load, short jobs can fare poorly with such a distributed scheduler. We propose instead a new hybrid centralized/distributed scheduler, called Hawk. In Hawk, long jobs are scheduled using a centralized scheduler, while short ones are scheduled in a fully distributed way. Moreover, a small portion of the cluster is reserved for the use of short jobs. In order to compensate for the occasional poor decisions made by the distributed scheduler, we propose a novel and efficient randomized work-stealing algorithm. We evaluate Hawk using a trace-driven simulation and a prototype implementation in Spark. In particular, using a Google trace, we show that under high load, compared to the purely distributed Sparrow scheduler, Hawk improves the 50th and 90th percentile runtimes by 80% and 90% for short jobs and by 35% and 10% for long jobs, respectively. Measurements of a prototype implementation using Spark on a 100-node cluster confirm the results of the simulation. © 2015 USENIX Annual Technical Conference.
DA  - 2015///
PY  - 2015
SP  - 499
EP  - 510
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992029827&partnerID=40&md5=e541b7894adc128e8f152137264b2d38
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 178</p>
KW  - Centralized schedulers
KW  - Distributed schedulers
KW  - Distributed scheduling
KW  - Efficient scheduling
KW  - Heterogeneous workloads
KW  - Prototype implementations
KW  - Scheduling
KW  - Scheduling decisions
KW  - Trace driven simulation
ER  - 

TY  - JOUR
AU  - Iandola, F.N.
AU  - Han, S.
AU  - Moskewicz, M.W.
AU  - Ashraf, K.
AU  - Dally, W.J.
AU  - Keutzer, K.
T2  - SqueezeNet: AlexNet-level Accuracy with 50x Fewer Parameters and <1MB Model Size
DA  - 2016///
PY  - 2016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988340112&partnerID=40&md5=9ba329ae98ab30a684b6086fe17a2cb9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 6927</p>
ER  - 

TY  - CONF
TI  - Load Balancing Across Microservices
AU  - Niu, Y.
AU  - Liu, F.
AU  - Li, Z.
T2  - Proceedings - IEEE INFOCOM
AB  - With the advent of cloud container technology, enterprises develop applications through microservices, breaking monolithic software into a suite of small services whose instances run independently in containers. User requests are served by a series of microservices forming a chain, and the chains often share microservices. Existing load balancing strategies either incur significant networking overhead or ignore the competition for shared microservices across chains. Furthermore, typical load balancing solutions leverage a hybrid technique by combining HTTP with message queue to support microservice communications, bringing additional operational complexity. To address these challenges, we propose a chain-oriented load balancing algorithm (COLBA) based solely on message queues, which balances load based on microservice requirements of chains to minimize response time. We model the load balancing problem as a non-cooperative game, and leverage Nash bargaining to coordinate microservice allocation across chains. Employing convex optimization with rounding, we efficiently solve the problem that is proven NP-hard. Extensive trace-driven simulations demonstrate that COLBA reduces the overall average response time at least by 13% compared with existing load balancing strategies. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/INFOCOM.2018.8486300
VL  - 2018-April
SP  - 198
EP  - 206
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056174602&doi=10.1109%2fINFOCOM.2018.8486300&partnerID=40&md5=e6e54404d28ef87f04bcb09227fe99d0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 86</p>
KW  - Application programs
KW  - Chains
KW  - Containers
KW  - Convex optimization
KW  - Distributed computer systems
KW  - Game theory
KW  - Hybrid techniques
KW  - Load balancing algorithms
KW  - Load balancing problem
KW  - Load balancing strategy
KW  - Nash bargaining
KW  - Noncooperative game
KW  - Operational complexity
KW  - Trace driven simulation
ER  - 

TY  - CONF
TI  - ZooKeeper: Wait-free coordination for internet-scale systems
AU  - Hunt, P.
AU  - Konar, M.
AU  - Junqueira, F.P.
AU  - Reed, B.
T2  - Proceedings of the 2010 USENIX Annual Technical Conference, USENIX ATC 2010
AB  - In this paper, we describe ZooKeeper, a service for coordinating processes of distributed applications. Since ZooKeeper is part of critical infrastructure, ZooKeeper aims to provide a simple and high performance kernel for building more complex coordination primitives at the client. It incorporates elements from group messaging, shared registers, and distributed lock services in a replicated, centralized service. The interface exposed by ZooKeeper has the wait-free aspects of shared registers with an event-driven mechanism similar to cache invalidations of distributed file systems to provide a simple, yet powerful coordination service. The ZooKeeper interface enables a high-performance service implementation. In addition to the wait-free property, ZooKeeper provides a per client guarantee of FIFO execution of requests and linearizability for all requests that change the ZooKeeper state. These design decisions enable the implementation of a high performance processing pipeline with read requests being satisfied by local servers. We show for the target workloads, 2:1 to 100:1 read to write ratio, that ZooKeeper can handle tens to hundreds of thousands of transactions per second. This performance allows ZooKeeper to be used extensively by client applications. © 2019 USENIX Annual Technical Conference. All rights reserved.
DA  - 2019///
PY  - 2019
SP  - 145
EP  - 158
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077109271&partnerID=40&md5=54d7bb3044aaba79d144ebaeae2094ff
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 500</p>
KW  - Cache invalidation
KW  - Client applications
KW  - Coordination reactions
KW  - Distributed applications
KW  - Distributed file systems
KW  - Event driven mechanisms
KW  - File organization
KW  - High-performance processing
KW  - Internet-scale systems
KW  - Target workloads
ER  - 

TY  - JOUR
TI  - Firecracker: Lightweight Virtualization for Serverless Applications
AU  - Brooker, M.
AU  - Florescu, A.
AU  - Popa, D.-M.
AU  - Neugebauer, R.
AU  - Agache, A.
AU  - Iordache, A.
AU  - Liguori, A.
AU  - Piwonka, P.
T2  - NSDI
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095497238&partnerID=40&md5=acdd7c262baa0e03669c6c09ec95784f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 5</p>
ER  - 

TY  - CONF
TI  - Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective
AU  - Hazelwood, K.
AU  - Bird, S.
AU  - Brooks, D.
AU  - Chintala, S.
AU  - Diril, U.
AU  - Dzhulgakov, D.
AU  - Fawzy, M.
AU  - Jia, B.
AU  - Jia, Y.
AU  - Kalro, A.
AU  - Law, J.
AU  - Lee, K.
AU  - Lu, J.
AU  - Noordhuis, P.
AU  - Smelyanskiy, M.
AU  - Xiong, L.
AU  - Wang, X.
T2  - Proceedings - International Symposium on High-Performance Computer Architecture
AB  - Machine learning sits at the core of many essential products and services at Facebook. This paper describes the hardware and software infrastructure that supports machine learning at global scale. Facebook's machine learning workloads are extremely diverse: services require many different types of models in practice. This diversity has implications at all layers in the system stack. In addition, a sizable fraction of all data stored at Facebook flows through machine learning pipelines, presenting significant challenges in delivering data to high-performance distributed training flows. Computational requirements are also intense, leveraging both GPU and CPU platforms for training and abundant CPU capacity for real-time inference. Addressing these and other emerging challenges continues to require diverse efforts that span machine learning algorithms, software, and hardware design. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/HPCA.2018.00059
VL  - 2018-February
SP  - 620
EP  - 629
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046720962&doi=10.1109%2fHPCA.2018.00059&partnerID=40&md5=f62b6bf978a70f022b04ae0378243e08
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 487</p>
KW  - Applied machine learning
KW  - Artificial intelligence
KW  - Computational requirements
KW  - Computer architecture
KW  - Computer hardware
KW  - Computer software
KW  - Facebook
KW  - Hardware
KW  - Hardware and software
KW  - Hardware design
KW  - Hardware software codesign
KW  - Hardware-software codesign
KW  - Learning algorithms
KW  - Learning systems
KW  - Machine learning
KW  - Products and services
KW  - Real-time inference
KW  - Social networking (online)
KW  - Supercomputers
KW  - System stacks
ER  - 

TY  - JOUR
TI  - Aérgia: A network-on-chip exploiting packet latency slack
AU  - Das, R.
AU  - Mutlu, O.
AU  - Moscibroda, T.
AU  - Das, C.
T2  - IEEE Micro
AB  - A traditional Network-on-Chip (NoC) employs simple arbitration strategies, such as round robin or oldest first, which treat packets equally regardless of the source applications' characteristics. This is suboptimal because packets can have different effects on system performance. We define slack as a key measure for characterizing a packet's relative importance. Aérgia introduces new router prioritization policies that exploit interfering packets' available slack to improve overall system performance and fairness. © 2011 IEEE.
DA  - 2011///
PY  - 2011
DO  - 10.1109/MM.2010.98
VL  - 31
IS  - 1
SP  - 29
EP  - 41
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951833094&doi=10.1109%2fMM.2010.98&partnerID=40&md5=45e017b0ee41c62dbb3ded0e528ae3dd
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 14</p>
KW  - arbitration
KW  - criticality
KW  - Criticality (nuclear fission)
KW  - memory systems
KW  - Multi core
KW  - multicore
KW  - On-chip networks
KW  - Packet networks
KW  - packet scheduling
KW  - prioritization
KW  - Prioritization
KW  - Routers
KW  - Scheduling
KW  - Servers
KW  - slack
KW  - VLSI circuits
ER  - 

TY  - CONF
TI  - Cash: Compiler assisted hardware design for improving DRAM energy efficiency in CNN inference
AU  - Sarma, A.
AU  - Jiang, H.
AU  - Pattnaik, A.
AU  - Kotra, J.
AU  - Kandemir, M.T.
AU  - Das, C.R.
T2  - ACM International Conference Proceeding Series
AB  - The advent of machine learning (ML) and deep learning applications has led to the development of a multitude of hardware accelerators and architectural optimization techniques for parallel architectures. This is due in part to the regularity and parallelism exhibited by the ML workloads, especially convolutional neural networks (CNNs). However, CPUs continue to be one of the dominant compute fabric in data-centers today, thereby also being widely deployed for inference tasks. As CNNs grow larger, the inherent limitations of a CPU-based system become apparent, specifically in terms of main memory data movement. In this paper, we present CASH, a compiler-assisted hardware solution that eliminates redundant data-movement to and from the main memory and, therefore, reduces main memory bandwidth and energy consumption. Our experimental evaluations on a set of four different state-of-the-art CNN workloads indicate that CASH provides, on average, ∼40% and ∼18% reductions in main memory bandwidth and energy consumption, respectively. © 2019 Association for Computing Machinery.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3357526.3357536
SP  - 396
EP  - 408
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075853474&doi=10.1145%2f3357526.3357536&partnerID=40&md5=3db3c7b0a49d5760a62307515f1dc67e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 5</p>
KW  - Bandwidth
KW  - Compiler-assisted
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Dynamic random access storage
KW  - Energy efficiency
KW  - Energy utilization
KW  - Experimental evaluation
KW  - Hardware accelerators
KW  - Hardware design
KW  - Hardware solutions
KW  - Inherent limitations
KW  - Integrated circuit design
KW  - Optimization techniques
KW  - Parallel architectures
KW  - Program compilers
KW  - State of the art
ER  - 

TY  - CONF
TI  - Happiness index: Right-sizing the cloud’s tenant-provider interface
AU  - Dukic, V.
AU  - Singla, A.
T2  - 11th USENIX Workshop on Hot Topics in Cloud Computing, HotCloud 2019, co-located with USENIX ATC 2019
AB  - Cloud providers and their tenants have a mutual interest in identifying optimal configurations in which to run tenant jobs, i.e., ones that achieve tenants’ performance goals at minimum cost; or ones that maximize performance within a specified budget. However, different tenants may have different performance goals that are opaque to the provider. A consequence of this opacity is that providers today typically offer fixed bundles of cloud resources, which tenants must themselves explore and choose from. This is burdensome for tenants and can lead to choices that are sub-optimal for both parties. We thus explore a simple, minimal interface, which lets tenants communicate their happiness with cloud infrastructure to the provider, and enables the provider to explore resource configurations that maximize this happiness. Our early results indicate that this interface could strike a good balance between enabling efficient discovery of application resource needs and the complexity of communicating a full description of tenant utility from different configurations to the provider. © 2019 USENIX Association. All rights reserved.
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088357108&partnerID=40&md5=75519c7457fec4a289e4b3cacbb55bd2
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 8</p>
KW  - Budget control
KW  - Cloud computing
KW  - Cloud infrastructures
KW  - Cloud providers
KW  - Happiness Index
KW  - Minimum cost
KW  - Resource configurations
ER  - 

TY  - JOUR
T2  - Fission Workflows
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080320465&partnerID=40&md5=fec721763da7b786d80c12b0f47d5ea1
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3</p>
ER  - 

TY  - JOUR
T2  - Amazon Web Services
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349752533&partnerID=40&md5=027313702d289eeb7d91fe9d61390c12
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1157</p>
ER  - 

TY  - JOUR
TI  - Serverless computing: One step forward, two steps back
AU  - Hellerstein, J.M.
AU  - Faleiro, J.
AU  - Gonzalez, J.E.
AU  - Schleier-Smith, J.
AU  - Sreekanti, V.
AU  - Tumanov, A.
AU  - Wu, C.
T2  - Serverless Computing: One Step Forward, Two Steps Back
DA  - 2018///
PY  - 2018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063882696&partnerID=40&md5=5c1ff7e752d2999d4dd74ad63968c46f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 182</p>
ER  - 

TY  - JOUR
AU  - Mauro, T.
T2  - Adopting Microservices at Netflix: Lessons for Architectural Design
DA  - 2015///
PY  - 2015
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963909685&partnerID=40&md5=410fc7ee0bb445a33f63b3e673f2dbb4
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 90</p>
ER  - 

TY  - CONF
TI  - KylinX: A dynamic library operating system for simplified and efficient cloud virtualization
AU  - Zhang, Y.
AU  - Crowcroft, J.
AU  - Li, D.
AU  - Zhang, C.
AU  - Li, H.
AU  - Wang, Y.
AU  - Yu, K.
AU  - Xiong, Y.
AU  - Chen, G.
T2  - Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018
AB  - Unikernel specializes a minimalistic LibOS and a target application into a standalone single-purpose virtual machine (VM) running on a hypervisor, which is referred to as (virtual) appliance. Compared to traditional VMs, Unikernel appliances have smaller memory footprint and lower overhead while guaranteeing the same level of isolation. On the downside, Unikernel strips off the process abstraction from its monolithic appliance and thus sacrifices flexibility, efficiency, and applicability. This paper examines whether there is a balance embracing the best of both Unikernel appliances (strong isolation) and processes (high flexibility/efficiency). We present KylinX, a dynamic library operating system for simplified and efficient cloud virtualization by providing the pVM (process-like VM) abstraction. A pVM takes the hypervisor as an OS and the Unikernel appliance as a process allowing both page-level and library-level dynamic mapping. At the page level, KylinX supports pVM fork plus a set of API for inter-pVM communication (IpC). At the library level, KylinX supports shared libraries to be linked to a Unikernel appliance at runtime. KylinX enforces mapping restrictions against potential threats. KylinX can fork a pVM in about 1.3 ms and link a library to a running pVM in a few ms, both comparable to process fork on Linux (about 1 ms). Latencies of KylinX IpCs are also comparable to that of UNIX IPCs. © Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018. All rights reserved.
DA  - 2020///
PY  - 2020
SP  - 173
EP  - 185
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077459733&partnerID=40&md5=ce16e0d3fd31f98b9e0dacc83a6f3a1a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 35</p>
KW  - Abstracting
KW  - Computer operating systems
KW  - Dynamic mapping
KW  - High flexibility
KW  - Hypervisor
KW  - Mapping
KW  - Memory footprint
KW  - Potential threats
KW  - Runtimes
KW  - Shared libraries
KW  - Target application
KW  - Virtual machine
KW  - Virtual reality
KW  - Virtualization
ER  - 

TY  - CONF
TI  - Race-To-sleep + content caching + display caching: A recipe for energy-efficient video streaming on handhelds
AU  - Zhang, H.
AU  - Rengasamy, P.V.
AU  - Zhao, S.
AU  - Nachiappan, N.C.
AU  - Sivasubramaniam, A.
AU  - Kandemir, M.T.
AU  - Iyer, R.
AU  - Das, C.R.
T2  - Proceedings of the Annual International Symposium on Microarchitecture, MICRO
AB  - Video streaming has become the most common application in handhelds and this trend is expected to grow in future to account for about 75% of all mobile data traffic by 2021. Thus, optimizing the performance and energy consumption of video processing in mobile devices is critical for sustaining the handheld market growth. In this paper, we propose three complementary techniques, race-to-sleep, content caching and display caching, to minimize the energy consumption of the video processing flows. Unlike the state-of-the-art frame-by-frame processing of a video decoder, the first scheme, race-to-sleep, uses two approaches, called batching of frames and frequency boosting to prolong its sleep state for saving energy, while avoiding any frame drops. The second scheme, content caching, exploits the content similarity of smaller video blocks, called macroblocks, to design a novel cache organization for reducing the memory pressure. The third scheme, in turn, takes advantage of content similarity at the display controller to facilitate display caching further improving energy efficiency. We integrate these three schemes for developing an end-to-end video processing framework and evaluate our design on a comprehensive mobile system design platform with a variety of video processing workloads. Our evaluations show that the proposed three techniques complement each other in improving performance by avoiding frame drops and reducing the energy consumption of video streaming applications by 21%, on average, compared to the current baseline design. © 2017 Association for Computing Machinery.
DA  - 2017///
PY  - 2017
DO  - 10.1145/3123939.3123948
VL  - Part F131207
SP  - 517
EP  - 531
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034109414&doi=10.1145%2f3123939.3123948&partnerID=40&md5=51c7075c40a5db98799b2f661f261b8c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 36</p>
KW  - Cache memory
KW  - Cache organization
KW  - Caching
KW  - Complementary techniques
KW  - Computer architecture
KW  - Content similarity
KW  - Data storage equipment
KW  - Display
KW  - Display controller
KW  - Display devices
KW  - Drops
KW  - Energy efficiency
KW  - Energy utilization
KW  - Improving performance
KW  - Integrated circuit design
KW  - Memory
KW  - Mobile data traffic
KW  - Mobile SoC
KW  - Mobile telecommunication systems
KW  - Sleep research
KW  - SoC
KW  - System-on-chip
KW  - Video signal processing
KW  - Video streaming
KW  - Video Streaming Applications
ER  - 

TY  - JOUR
TI  - SAND: Towards High-Performance Serverless Computing
AU  - Akkus, I.E.
AU  - Chen, R.
AU  - Rimac, I.
AU  - Stein, M.
AU  - Satzke, K.
AU  - Beck, A.
AU  - Aditya, P.
AU  - Hilt, V.
T2  - USENIX ATC
DA  - 2018///
PY  - 2018
SP  - 923
EP  - 935
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062596504&partnerID=40&md5=69803d1ae53e00a17efa1267107cd265
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 44</p>
ER  - 

TY  - CONF
TI  - SpotCheck: Designing a derivative IaaS cloud on the spot market
AU  - Sharma, P.
AU  - Lee, S.
AU  - Guo, T.
AU  - Irwin, D.
AU  - Shenoy, P.
T2  - Proceedings of the 10th European Conference on Computer Systems, EuroSys 2015
AB  - Infrastructure-as-a-Service (IaaS) cloud platforms rent resources, in the form of virtual machines (VMs), under a variety of contract terms that offer different levels of risk and cost. For example, users may acquire VMs in the spot market that are often cheap but entail significant risk, since their price varies over time based on market supply and demand and they may terminate at any time if the price rises too high. Currently, users must manage all the risks associated with using spot servers. As a result, conventional wisdom holds that spot servers are only appropriate for delay-tolerant batch applications. In this paper, we propose a derivative cloud platform, called SpotCheck, that transparently manages the risks associated with using spot servers for users. SpotCheck provides the illusion of an IaaS platform that offers always-available VMs on demand for a cost near that of spot servers, and supports all types of applications, including interactive ones. SpotCheck's design combines the use of nested VMs with live bounded-time migration and novel server pool management policies to maximize availability, while balancing risk and cost. We implement SpotCheck on Amazon's EC2 and show that it i) provides nested VMs to users that are 99.9989% available, ii) achieves nearly 5× cost savings compared to using equivalent types of ondemand VMs, and iii) eliminates any risk of losing VM state. Copyright © 2015 ACM.
DA  - 2015///
PY  - 2015
DO  - 10.1145/2741948.2741953
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929584823&doi=10.1145%2f2741948.2741953&partnerID=40&md5=569b7364fd80a039909d1a56c1af465f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 95</p>
KW  - Balancing
KW  - Cloud platforms
KW  - Commerce
KW  - Contract terms
KW  - Delay tolerant
KW  - Iaas clouds
KW  - Infrastructure as a service (IaaS)
KW  - Management policy
KW  - Market supply and demand
KW  - On the spots
KW  - Platform as a Service (PaaS)
KW  - Spot market
ER  - 

TY  - JOUR
T2  - IBM Serverless Functions
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095546931&partnerID=40&md5=3cdaa5c2be56fe7ab21039b8a6cd784f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2</p>
ER  - 

TY  - CONF
TI  - CritICs critiquing criticality in mobile apps
AU  - Rengasamy, P.V.
AU  - Zhang, H.
AU  - Zhao, S.
AU  - Nachiappan, N.C.
AU  - Sivasubramaniam, A.
AU  - Kandemir, M.T.
AU  - Das, C.R.
T2  - Proceedings of the Annual International Symposium on Microarchitecture, MICRO
AB  - In this paper, we conduct a systematic analysis to show that existing CPU optimizations targeting scientific/server workloads are not always well suited for mobile apps. In particular, we observe that the well-known and very important concept of identifying and accelerating individual critical instructions in workloads such as SPEC, are not as effective for mobile apps. Several differences in mobile app characteristics including (i) dependencies between critical instructions interspersed with non-critical instructions in the dependence chain, (ii) temporal proximity of the critical instructions in the dynamic stream, and (iii) the bottleneck shifting to the front from the rear of the datapath pipeline, are key contributors to the ineffectiveness of traditional criticality based optimizations. Instead, we propose the concept of Critical Instruction Chains (CritICs)-which are short, critical and self contained sequences of instructions, for aggregate level optimization. With motivating results, we show that an offline profiler/analysis framework can easily identify these CritICs, and we propose a very simple software mechanism in the compiler that exploits ARM's 16-bit ISA format to nearly double the fetch bandwidth of these instructions. We have implemented this entire framework-both profiler and compiler passes, and evaluated its effectiveness for 10 popular apps from the Play Store. Experimental evaluations show that our approach is much more effective than two previously studied criticality optimizations, yielding a speedup of 12.65%, and energy savings of 15% in the CPU (translating to a system wide energy savings of 4.6%), requiring very little additional hardware support. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/MICRO.2018.00075
VL  - 2018-October
SP  - 867
EP  - 880
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060005773&doi=10.1109%2fMICRO.2018.00075&partnerID=40&md5=f6ce916e62036f960c6a76c924b0a891
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 7</p>
KW  - Computer architecture
KW  - CPU
KW  - Criticality
KW  - Criticality (nuclear fission)
KW  - Energy
KW  - Energy conservation
KW  - Experimental evaluation
KW  - Fetch bandwidth
KW  - Hardware supports
KW  - Mobile
KW  - Mobile apps
KW  - Program compilers
KW  - Program processors
KW  - Software mechanisms
KW  - Systematic analysis
ER  - 

TY  - CONF
TI  - DMA-aware memory energy management
AU  - Pandey, V.
AU  - Jiang, W.
AU  - Zhou, Y.
AU  - Blanching, R.
T2  - Proceedings - International Symposium on High-Performance Computer Architecture
AB  - As increasingly larger memories are used to bridge the widening gap between processor and disk speeds, main memory energy consumption is becoming increasingly dominant. Even though much prior research has been conducted on memory energy management, no study has focused on data servers, where main memory is predominantly accessed by DMAs instead of processors. In this paper, we study DMA-aware techniques for memory energy management in data servers. We first characterize the effect of DMA accesses on memory energy and show that, due to the mismatch between memory and I/O bus band-widths, significant energy is wasted when memory is idle but still active during DMA transfers. To reduce this waste, we propose two novel performance-directed energy management techniques that maximize the utilization of memory devices by increasing the level of concurrency between multiple DMA transfers from different I/O buses to the same memory device. We evaluate our techniques using a detailed trace-driven simulator, and storage and database server traces. The results show that our techniques can effectively minimize the amount of idle energy waste during DMA transfers and, consequently, conserve up to 38.6% more memory energy than previous approaches while providing similar performance. © 2006 IEEE.
DA  - 2006///
PY  - 2006
DO  - 10.1109/HPCA.2006.1598120
VL  - 2006
SP  - 134
EP  - 145
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748852203&doi=10.1109%2fHPCA.2006.1598120&partnerID=40&md5=d28838b3aaa649420a770679b9a25b3f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 51</p>
KW  - Data servers
KW  - Data storage equipment
KW  - Database systems
KW  - Disk speeds
KW  - DMA transfers
KW  - Dynamic mechanical analysis
KW  - Energy management
KW  - Energy utilization
KW  - Magnetic disk storage
KW  - Memory energy management
KW  - Program processors
KW  - Servers
KW  - Storage allocation (computer)
ER  - 

TY  - CONF
TI  - Job-aware scheduling in eagle: Divide and stick to your probes
AU  - Delgado, P.
AU  - Didona, D.
AU  - Dinu, F.
AU  - Zwaenepoel, W.
T2  - Proceedings of the 7th ACM Symposium on Cloud Computing, SoCC 2016
AB  - We present Eagle, a new hybrid data center scheduler for data-parallel programs. Eagle dynamically divides the nodes of the data center in partitions for the execution of long and short jobs, thereby avoiding head-of-line blocking. Furthermore, it provides job awareness and avoids stragglers by a new technique, called Sticky Batch Probing (SBP). The dynamic partitioning of the data center nodes is accomplished by a technique called Succinct State Sharing (SSS), in which the distributed schedulers are informed of the locations where long jobs are executing. SSS is particularly easy to implement with a hybrid scheduler, in which the centralized scheduler places long jobs. With SBP, when a distributed scheduler places a probe for a job on a node, the probe stays there until all tasks of the job have been completed. When finishing the execution of a task corresponding to probe P, rather than executing a task corresponding to the next probe P' in its queue, the node may choose to execute another task corresponding to P. We use SBP in combination with a distributed approximation of Shortest Remaining Processing Time (SRPT) with starvation prevention. We have implemented Eagle as a Spark plugin, and we have measured job completion times for a subset of the Google trace on a 100-node cluster for a variety of cluster loads. We provide simulation results for larger clusters, different traces, and for comparison with other scheduling disciplines. We show that Eagle outperforms other state-oftheart scheduling solutions at most percentiles, and is more robust against mis-estimation of task duration.
DA  - 2016///
PY  - 2016
DO  - 10.1145/2987550.2987563
SP  - 497
EP  - 509
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995553833&doi=10.1145%2f2987550.2987563&partnerID=40&md5=1df124fce5bd418eaf82e5878d991f28
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 93</p>
KW  - Centralized schedulers
KW  - Cloud computing
KW  - Data center
KW  - Data centers
KW  - Data handling
KW  - Data parallel programs
KW  - Distributed schedulers
KW  - Dynamic partitioning
KW  - Finishing
KW  - Head of line blocking
KW  - Scheduling
KW  - Scheduling discipline
KW  - Shortest remaining processing time
ER  - 

TY  - CONF
TI  - Using burstable instances in the public cloud: Why, when and how?
AU  - Wang, C.
AU  - Urgaonkar, B.
AU  - Nasiriani, N.
AU  - Kesidis, G.
T2  - SIGMETRICS 2017 Abstracts - Proceedings of the 2017 ACM SIGMETRICS / International Conference on Measurement and Modeling of Computer Systems
DA  - 2017///
PY  - 2017
DO  - 10.1145/3078505.3078591
SP  - 56
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021822573&doi=10.1145%2f3078505.3078591&partnerID=40&md5=9e00f6949b8937ff15bc91824be21271
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 16</p>
KW  - Burstable instance
KW  - Capacity dynamism
KW  - Resource allocation
ER  - 

TY  - JOUR
TI  - The kaldi speech recognition toolkit
AU  - Povey, D.
AU  - Ghoshal, A.
AU  - Boulianne, G.
AU  - Burget, L.
AU  - Glembek, O.
AU  - Goel, N.
AU  - Hannemann, M.
AU  - Motlicek, P.
AU  - Qian, Y.
AU  - Schwarz, P.
AU  - Silovsky, J.
AU  - Stemmer, G.
AU  - Vesely, K.
T2  - Proc. ASRU
DA  - 2011///
PY  - 2011
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858953642&partnerID=40&md5=a2114d55aa5c88fb66e91dcc67602237
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 4259</p>
ER  - 

TY  - JOUR
TI  - Online experiments: Lessons learned
AU  - Kohavi, R.
AU  - Longbotham, R.
T2  - Computer
AB  - Web experiments generate insights and promote innovation. © 2007 IEEE.
DA  - 2007///
PY  - 2007
DO  - 10.1109/MC.2007.328
VL  - 40
IS  - 9
SP  - 103
EP  - 105
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-34748868419&doi=10.1109%2fMC.2007.328&partnerID=40&md5=4b11067d910674a269972b00efdfaa6b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 99</p>
KW  - Online experiments
KW  - Web technologies
ER  - 

TY  - JOUR
T2  - IBM Composer
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061814553&partnerID=40&md5=6ef0f6d4678e0850e05bc07c0ae45e35
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3</p>
ER  - 

TY  - CONF
TI  - Cloud-Based Disaster Management as a Service: A Microservice Approach for Hurricane Twitter Data Analysis
AU  - Khaleq, A.A.
AU  - Ra, I.
T2  - GHTC 2018 - IEEE Global Humanitarian Technology Conference, Proceedings
AB  - Disasters whether natural or man-made have great impact on countries and civilians. Proper information across the main disaster phases need to be delivered on time and to the right people to minimize the impact and provide needed resources. Social media and Twitter in particular, is an important mean of information sharing in real-Time as part of a complete cyber-physical emergency management system during a disaster. Twitter can be used in any place in the world through smartphones or other mediums with an internet access connection. The vast and varied number of tweets produced during a disaster will benefit from the cloud scalable storage and processing resources. As a centralized processing system is more vulnerable when a disaster strikes, there is a need for a more resilient distributed system architecture that allows for the distribution of both processing and storage resources. The goal of our study is to develop and evaluate a prototype of a microservice architecture for twitter data analytics during a disaster that meets the requirements of disaster management. In this paper, we design a cloud-based microservices twitter analytics framework for disaster management and implement a basic prototype system. Our prototype system demonstrates that the microservices approach allows for a distributed, dynamic, reliable and scalable system architecture on cloud platform that goes in hand with disaster domain requirements. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/GHTC.2018.8601887
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061838462&doi=10.1109%2fGHTC.2018.8601887&partnerID=40&md5=ff951a00244499db2772fb34bfd80e89
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 11</p>
KW  - Architecture
KW  - Big data
KW  - Cloud analytics
KW  - cloud applications
KW  - Cloud applications
KW  - Cloud-based
KW  - Computer architecture
KW  - Data Analytics
KW  - Digital storage
KW  - disaster management
KW  - Disaster management
KW  - Disaster prevention
KW  - Disasters
KW  - Hurricanes
KW  - Information management
KW  - Information sharing
KW  - microservice architecture
KW  - Microservice architecture
KW  - Processing resources
KW  - Prototype system
KW  - Risk management
KW  - Social media
KW  - Social networking (online)
KW  - Storage resources
KW  - Twitter analytic
KW  - twitter analytics
ER  - 

TY  - CONF
TI  - Serverless computation with OpenLambda
AU  - Hendrickson, S.
AU  - Sturdevant, S.
AU  - Harter, T.
AU  - Venkataramani, V.
AU  - Arpaci-Dusseau, A.C.
AU  - Arpaci-Dusseau, R.H.
T2  - 8th USENIX Workshop on Hot Topics in Cloud Computing, HotCloud 2016
AB  - We present OpenLambda, a new, open-source platform for building next-generation web services and applications in the burgeoning model of serverless computation. We describe the key aspects of serverless computation, and present numerous research challenges that must be addressed in the design and implementation of such systems. We also include a brief study of current web applications, so as to better motivate some aspects of serverless application construction. © 2016 USENIX Association. All rights reserved.
DA  - 2016///
PY  - 2016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084164509&partnerID=40&md5=1a21e36f6f88c72da58021cdea14a5d5
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 327</p>
KW  - Cloud computing
KW  - Design and implementations
KW  - Open source platforms
KW  - Research challenges
KW  - WEB application
KW  - Web services
ER  - 

TY  - JOUR
TI  - Cloudburst: Stateful functions-as-a-service
AU  - Sreekanti, V.
AU  - Wu, C.
AU  - Lin, X.C.
AU  - Schleier-Smith, J.
AU  - Faleiro, J.M.
AU  - Gonzalez, J.E.
AU  - Hellerstein, J.M.
AU  - Tumanov, A.
T2  - Cloudburst: Stateful functions-as-a-service
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086281019&partnerID=40&md5=ab57b28a082c003185256d0439147e50
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 61</p>
ER  - 

TY  - JOUR
TI  - TensorFlow: Learning functions at scale
AU  - Abadi, M.
T2  - ACM SIGPLAN Notices
DA  - 2016///
PY  - 2016
VL  - 51
IS  - 9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039716878&partnerID=40&md5=5e3e0918218e91fc4e1a1a4f9aeb5b21
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 388</p>
ER  - 

TY  - JOUR
T2  - Kubernetes
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098487549&partnerID=40&md5=5ae153ebc160adfdb02cd449da782c20
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - Tarcil: Reconciling scheduling speed and quality in large shared clusters
AU  - Delimitrou, C.
AU  - Sanchez, D.
AU  - Kozyrakis, C.
T2  - ACM SoCC 2015 - Proceedings of the 6th ACM Symposium on Cloud Computing
AB  - Scheduling diverse applications in large, shared clusters is particularly challenging. Recent research on cluster scheduling focuses either on scheduling speed, using sampling to quickly assign resources to tasks, or on scheduling quality, using centralized algorithms that search for the resources that improve both task performance and cluster utilization. We present Tarcil, a distributed scheduler that targets both scheduling speed and quality. Tarcil uses an analytically derived sampling framework that adjusts the sample size based on load, and provides statistical guarantees on the quality of allocated resources. It also implements admission control when sampling is unlikely to find suitable resources. This makes it appropriate for large, shared clusters hosting shortand long-running jobs. We evaluate Tarcil on clusters with hundreds of servers on EC2. For highly-loaded clusters running short jobs, Tarcil improves task execution time by 41% over a distributed, sampling-based scheduler. For more general scenarios, Tarcil achieves near-optimal performance for 4× and 2× more jobs than sampling-based and centralized schedulers respectively. © 2015 ACM.
DA  - 2015///
PY  - 2015
DO  - 10.1145/2806777.2806779
SP  - 97
EP  - 110
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959036568&doi=10.1145%2f2806777.2806779&partnerID=40&md5=523707d41f70d4102eb4334422502869
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 137</p>
KW  - Centralized algorithms
KW  - Centralized schedulers
KW  - Cloud computing
KW  - Data centers
KW  - Datacenters
KW  - Distributed schedulers
KW  - Diverse applications
KW  - Near-optimal performance
KW  - QoS
KW  - Quality of service
KW  - Resource efficiencies
KW  - Resource-efficiency
KW  - Sampling
KW  - Scalability
KW  - Scheduling
KW  - Statistical guarantee
ER  - 

TY  - JOUR
TI  - Step Functions
AU  - Buddha, J.P.
AU  - Beesetty, R.
T2  - The Definitive Guide to AWS Application Integration
DA  - 2019///
PY  - 2019
SP  - 263
EP  - 342
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098411795&partnerID=40&md5=27877b13000e4908df896f0ccf53acf2
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 5</p>
ER  - 

TY  - JOUR
TI  - Function-as-a-Service Market by User Type (Developer-Centric and Operator-Centric), Application (Web and Mobile Based, Research and Academic), Service Type, Deployment Model, Organization Size, Industry Vertical, and Region - Global Forecast to 2021
T2  - Research and Markets
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098371439&partnerID=40&md5=8f4bf12242acb341652aca38b65c48f9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - Occupy the cloud: Distributed computing for the 99%
AU  - Jonas, E.
AU  - Pu, Q.
AU  - Venkataraman, S.
AU  - Stoica, I.
AU  - Recht, B.
T2  - SoCC 2017 - Proceedings of the 2017 Symposium on Cloud Computing
AB  - Distributed computing remains inaccessible to a large number of users, in spite of many open source platforms and extensive commercial offerings. While distributed computation frameworks have moved beyond a simple map-reduce model, many users are still left to struggle with complex cluster management and configuration tools, even for running simple embarrassingly parallel jobs. We argue that stateless functions represent a viable platform for these users, eliminating cluster management overhead, fulfilling the promise of elasticity. Furthermore, using our prototype implementation, Py-Wren, we show that this model is general enough to implement a number of distributed computing models, such as BSP, efficiently. Extrapolating from recent trends in network bandwidth and the advent of disaggregated storage, we suggest that stateless functions are a natural fit for data processing in future computing environments. © 2017 Association for Computing Machinery.
DA  - 2017///
PY  - 2017
DO  - 10.1145/3127479.3128601
SP  - 445
EP  - 451
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032457554&doi=10.1145%2f3127479.3128601&partnerID=40&md5=297fd2995b565f6919acd69e150d9936
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 352</p>
KW  - AWS lambda
KW  - Cloud computing
KW  - Computing environments
KW  - Data handling
KW  - Digital storage
KW  - Distributed computation framework
KW  - Distributed computer systems
KW  - Distributed computing
KW  - Distributed computing models
KW  - Open source platforms
KW  - Open systems
KW  - Prototype implementations
KW  - PyWren
KW  - Serverless
ER  - 

TY  - CONF
TI  - Stratus: Cost-aware container scheduling in the public cloud
AU  - Chung, A.
AU  - Park, J.W.
AU  - Ganger, G.R.
T2  - SoCC 2018 - Proceedings of the 2018 ACM Symposium on Cloud Computing
AB  - Stratus is a new cluster scheduler specialized for orchestrating batch job execution on virtual clusters, dynamically allocated collections of virtual machine instances on public IaaS platforms. Unlike schedulers for conventional clusters, Stratus focuses primarily on dollar cost considerations, since public clouds provide effectively unlimited, highly heterogeneous resources allocated on demand. But, since resources are charged-for while allocated, Stratus aggressively packs tasks onto machines, guided by job runtime estimates, trying to make allocated resources be either mostly full (highly utilized) or empty (so they can be released to save money). Simulation experiments based on cluster workload traces from Google and TwoSigma show that Stratus reduces cost by 17–44% compared to state-of-the-art approaches to virtual cluster scheduling. © 2018 Association for Computing Machinery.
DA  - 2018///
PY  - 2018
DO  - 10.1145/3267809.3267819
SP  - 121
EP  - 134
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058982961&doi=10.1145%2f3267809.3267819&partnerID=40&md5=92b2e003340b423f8524c57fc0a3dace
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 70</p>
KW  - Cloud computing
KW  - Cluster computing
KW  - Cluster scheduling
KW  - Container scheduling
KW  - Cost-aware
KW  - Heterogeneous resources
KW  - Public clouds
KW  - Runtime estimates
KW  - Scheduling
KW  - State-of-the-art approach
KW  - Transient server
KW  - Virtual clusters
ER  - 

TY  - CONF
TI  - On the Network Performance of Amazon S3 Cloud-Storage Service
AU  - Persico, V.
AU  - Montieri, A.
AU  - Pescape, A.
T2  - Proceedings - 2016 5th IEEE International Conference on Cloud Networking, CloudNet 2016
AB  - The advances in networking technologies and the increase in the need for storage resources have prompted many companies to outsource their storage needs. Cloud-storage providers offer clean and simple file-system interfaces, abstracting away the complexities of direct hardware management. At the same time, however, such services eliminate the direct oversight of performance that final users with high service-level requirements traditionally expect. While several works in literature have addressed security-related issues (such as privacy, integrity, availability, etc.) few of them have targeted the network performance of this kind of services. In this work we propose the analysis of the performance of the network associated to the storage service offered by Amazon: S3. Thanks to a large-scale distributed campaign performed by leveraging the Bismark measurement platform, we have characterized how the performance of the network may impact the quality of service experienced by final users on the basis of their location and the configuration of services. We found how performance heavily changes (up to 1553 KiB/s) accordingto the location of the customers and the cloud region they rely on (up to 2117 KiB/s), also deriving a number of usageguidelines for the customers. In addition we characterize the impact of leveraging the Amazon CDN service to distributecontents, finding that while it guarantees up to a 275-percent performance improvement, cases exist for which additional costs may lead to worse performance. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/CloudNet.2016.16
SP  - 113
EP  - 118
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010679733&doi=10.1109%2fCloudNet.2016.16&partnerID=40&md5=5910d41581a95a87b71ff3cd0066672a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 35</p>
KW  - Additional costs
KW  - AWS
KW  - Cloud networks
KW  - Cloud Networks
KW  - Cloud performance
KW  - Cloud Performance
KW  - Cloud Storage
KW  - Cloud storage services
KW  - Cloud storages
KW  - Hardware management
KW  - Network performance
KW  - Networking technology
KW  - Quality of service
KW  - Storage resources
ER  - 

TY  - JOUR
TI  - SEDA: An architecture for well-conditioned, scalable internet services
AU  - Welsh, M.
AU  - Culler, D.
AU  - Brewer, E.
T2  - Operating Systems Review (ACM)
AB  - We propose a new design for highly concurrent Internet services, which we call the staged event-driven architecture (SEDA). SEDA is intended to support massive concurrency demands and simplify the construction of well-conditioned services. In SEDA, applications consist of a network of event-driven stages connected by explicit queues. This architecture allows services to be well-conditioned to load, preventing resources from being overcommitted when demand exceeds service capacity. SEDA makes use of a set of dynamic resource controllers to keep stages within their operating regime despite large fluctuations in load. We describe several control mechanisms for automatic tuning and load conditioning, including thread pool sizing, event batching, and adaptive load shedding. We present the SEDA design and an implementation of an Internet services platform based on this architecture. We evaluate the use of SEDA through two applications: a high-performance HTTP server and a packet router for the Gnutella peer-to-peer file sharing network. These results show that SEDA applications exhibit higher performance than traditional service designs, and are robust to huge variations in load.
DA  - 2001///
PY  - 2001
DO  - 10.1145/502059.502057
VL  - 35
IS  - 5
SP  - 230
EP  - 243
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036041527&doi=10.1145%2f502059.502057&partnerID=40&md5=39444708ee3c15a575845675f72f1c4d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 520</p>
KW  - Client server computer systems
KW  - Computer architecture
KW  - Congestion control (communication)
KW  - Dynamic resource controllers
KW  - File organization
KW  - HTTP
KW  - Internet
KW  - Internet services
KW  - Packet router
KW  - Packet switching
KW  - Peer to peer file sharing network
KW  - Queueing networks
KW  - Routers
KW  - Staged event driven architecture
KW  - Telecommunication services
ER  - 

TY  - CONF
TI  - Déjà View: Spatio-Temporal Compute Reuse for' Energy-Efficient 360° VR Video Streaming
AU  - Zhao, S.
AU  - Zhang, H.
AU  - Bhuyan, S.
AU  - Mishra, C.S.
AU  - Ying, Z.
AU  - Kandemir, M.T.
AU  - Sivasubramaniam, A.
AU  - Das, C.R.
T2  - Proceedings - International Symposium on Computer Architecture
AB  - The emergence of virtual reality (VR) and augmented reality (AR) has revolutionized our lives by enabling a 360° artificial sensory stimulation across diverse domains, including, but not limited to, sports, media, healthcare, and gaming. Unlike the conventional planar video processing, where memory access is the main bottleneck, in 360° VR videos the compute is the primary bottleneck and contributes to more than 50% energy consumption in battery-operated VR headsets. Thus, improving the computational efficiency of the video processing pipeline in a VR is critical. While prior efforts have attempted to address this problem through acceleration using a GPU or FPGA, none of them has analyzed the 360° VR pipeline to examine if there is any scope to optimize the computation with known techniques such as memoization. Thus, in this paper, we analyze the VR computation pipeline and observe that there is significant scope to skip computations by leveraging the temporal and spatial locality in head orientation and eye correlations, respectively, resulting in computation reduction and energy efficiency. The proposed Déjà View design takes advantage of temporal reuse by memoizing head orientation and spatial reuse by establishing a relationship between left and right eye projection, and can be implemented either on a GPU or an FPGA. We propose both software modifications for existing compute pipeline and microarchitectural additions for further enhancement. We evaluate our design by implementing the software enhancements on an NVIDIA Jetson TX2 GPU board and our microarchitectural additions on a Xilinx Zynq-7000 FPGA model using five video workloads. Experimental results show that Déjà View can provide 34% computation reduction and 17% energy saving, compared to the state-of-the-art design. © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ISCA45697.2020.00030
VL  - 2020-May
SP  - 241
EP  - 253
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091991854&doi=10.1109%2fISCA45697.2020.00030&partnerID=40&md5=f38f67d5801024d67b9baf3306fc494d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 29</p>
KW  - 360° Video Processing
KW  - Augmented reality
KW  - Computation reduction
KW  - Computational efficiency
KW  - Edge Computing
KW  - Energy efficiency
KW  - Energy efficient
KW  - Energy utilization
KW  - Field programmable gate arrays (FPGA)
KW  - Graphics processing unit
KW  - Integrated circuit design
KW  - IoT
KW  - Memory architecture
KW  - Pipeline processing systems
KW  - Pipelines
KW  - Sensory stimulation
KW  - Software enhancements
KW  - Software modification
KW  - State of the art
KW  - Temporal and spatial
KW  - Video processing
KW  - Video streaming
KW  - Virtual reality
KW  - Virtual Reality
ER  - 

TY  - CONF
TI  - BurScale: Using Burstable Instances for Cost-Effective Autoscaling in the Public Cloud
AU  - Baarzi, A.F.
AU  - Zhu, T.
AU  - Urgaonkar, B.
T2  - SoCC 2019 - Proceedings of the ACM Symposium on Cloud Computing
AB  - Cloud providers have recently introduced burstable instances-virtual machines whose CPU capacity is rate limited by token-bucket mechanisms. A user of a burstable instance is able to burst to a much higher resource capacity ("peak rate") than the instance's long-Term average capacity ("sustained rate"), provided the bursts are short and infrequent. A burstable instance tends to be much cheaper than a conventional instance that is always provisioned for the peak rate. Consequently, cloud providers advertise burstable instances as cost-effective options for customers with intermittent needs and small (e.g., single VM) clusters. By contrast, this paper presents two novel usage scenarios for burstable instances in larger clusters with sustained usage. We demonstrate (i) how burstable instances can be utilized alongside conventional instances to handle the transient queueing arising from variability in traffic, and (ii) how burstable instances can mask the VM startup/warmup time when autoscaling to handle flash crowds. We implement our ideas in a system called BurScale and use it to demonstrate cost-effective autoscaling for two important workloads: (i) a stateless web server cluster, and (ii) a stateful Memcached caching cluster. Results from our prototype system show that via its careful combination of burstable and regular instances, BurScale can ensure similar application performance as traditional autoscaling systems that use all regular instances while reducing cost by up to 50%. © 2019 ACM.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3357223.3362706
SP  - 126
EP  - 138
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091803998&doi=10.1145%2f3357223.3362706&partnerID=40&md5=1353a8e9add9407b766677f9a3d1e3e6
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 23</p>
KW  - Application performance
KW  - autoscaling
KW  - Average capacities
KW  - burstable instances
KW  - cloud computing
KW  - Cloud computing
KW  - Cloud providers
KW  - Cost effectiveness
KW  - Prototype system
KW  - Reducing costs
KW  - Resource capacity
KW  - resource provisioning
KW  - Usage scenarios
KW  - Virtual machine
KW  - Web server cluster
ER  - 

TY  - JOUR
TI  - Swayam: Distributed Autoscaling to Meet SLAs of Machine Learning Inference Services with Resource Efficiency
AU  - Gujarati, A.
AU  - Elnikety, S.
AU  - He, Y.
AU  - McKinley, K. S.
AU  - Brandenburg, B. B.
T2  - USENIX Middleware Conference
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095484895&partnerID=40&md5=220c1fa5fda624b6df2f84c7fcf6f4e0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 9</p>
ER  - 

TY  - CONF
TI  - Machine learning at facebook: Understanding inference at the edge
AU  - Wu, C.-J.
AU  - Brooks, D.
AU  - Chen, K.
AU  - Chen, D.
AU  - Choudhury, S.
AU  - Dukhan, M.
AU  - Hazelwood, K.
AU  - Isaac, E.
AU  - Jia, Y.
AU  - Jia, B.
AU  - Leyvand, T.
AU  - Lu, H.
AU  - Lu, Y.
AU  - Qiao, L.
AU  - Reagen, B.
AU  - Spisak, J.
AU  - Sun, F.
AU  - Tulloch, A.
AU  - Vajda, P.
AU  - Wang, X.
AU  - Wang, Y.
AU  - Wasti, B.
AU  - Wu, Y.
AU  - Xian, R.
AU  - Yoo, S.
AU  - Zhang, P.
T2  - Proceedings - 25th IEEE International Symposium on High Performance Computer Architecture, HPCA 2019
AB  - At Facebook, machine learning provides a wide range of capabilities that drive many aspects of user experience including ranking posts, content understanding, object detection and tracking for augmented and virtual reality, speech and text translations. While machine learning models are currently trained on customized datacenter infrastructure, Facebook is working to bring machine learning inference to the edge. By doing so, user experience is improved with reduced latency (inference time) and becomes less dependent on network connectivity. Furthermore, this also enables many more applications of deep learning with important features only made available at the edge. This paper takes a datadriven approach to present the opportunities and design challenges faced by Facebook in order to enable machine learning inference locally on smartphones and other edge platforms. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/HPCA.2019.00048
SP  - 331
EP  - 344
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064189318&doi=10.1109%2fHPCA.2019.00048&partnerID=40&md5=fce9ebef85a5b311645a3d1523a74cb6
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 390</p>
KW  - Augmented and virtual realities
KW  - Computer architecture
KW  - Data-driven approach
KW  - Deep learning
KW  - Edge Inference
KW  - Important features
KW  - Learning systems
KW  - Machine learning
KW  - Machine learning models
KW  - Network architecture
KW  - Network connectivity
KW  - Object detection
KW  - Object detection and tracking
KW  - Reduced latencies
KW  - Social networking (online)
KW  - Supercomputers
KW  - Virtual reality
ER  - 

TY  - CONF
TI  - Static call graph construction in AWS lambda serverless applications
AU  - Obetz, M.
AU  - Patterson, S.
AU  - Milanova, A.
T2  - 11th USENIX Workshop on Hot Topics in Cloud Computing, HotCloud 2019, co-located with USENIX ATC 2019
AB  - We present new means for performing static program analysis on serverless programs. We propose a new type of call graph that captures the stateless, event-driven nature of such programs and describe a method for constructing these new extended service call graphs. Next, we survey applications of program analysis that can leverage our extended service call graphs to answer questions about code that executes on a serverless platform. We present findings on the applicability of our techniques to real open source serverless programs. Finally, we close with several open questions about how to best incorporate static analysis in problem solving for developing serverless applications. © 2019 USENIX Association. All rights reserved.
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084163844&partnerID=40&md5=35c6f8ebd1837e9ea5344f84212d6117
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 23</p>
KW  - Application programs
KW  - Call graph construction
KW  - Call graphs
KW  - Cloud computing
KW  - Event-driven
KW  - Open sources
KW  - Program analysis
KW  - Static analysis
KW  - Static program analysis
KW  - XML
ER  - 

TY  - CONF
TI  - Infrastructure Cost Comparison of Running Web Applications in the Cloud Using AWS Lambda and Monolithic and Microservice Architectures
AU  - Villamizar, M.
AU  - Garces, O.
AU  - Ochoa, L.
AU  - Castro, H.
AU  - Salamanca, L.
AU  - Verano, M.
AU  - Casallas, R.
AU  - Gil, S.
AU  - Valencia, C.
AU  - Zambrano, A.
AU  - Lang, M.
T2  - Proceedings - 2016 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing, CCGrid 2016
AB  - Large Internet companies like Amazon, Netflix, and LinkedIn are using the microservice architecture pattern to deploy large applications in the cloud as a set of small services that can be developed, tested, deployed, scaled, operated and upgraded independently. However, aside from gaining agility, independent development, and scalability, infrastructure costs are a major concern for companies adopting this pattern. This paper presents a cost comparison of a web application developed and deployed using the same scalable scenarios with three different approaches: 1) a monolithic architecture, 2) a microservice architecture operated by the cloud customer, and 3) a microservice architecture operated by the cloud provider. Test results show that microservices can help reduce infrastructure costs in comparison to standard monolithic architectures. Moreover, the use of services specifically designed to deploy and scale microservices reduces infrastructure costs by 70% or more. Lastly, we also describe the challenges we faced while implementing and deploying microservice applications. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/CCGrid.2016.37
SP  - 179
EP  - 182
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983458085&doi=10.1109%2fCCGrid.2016.37&partnerID=40&md5=794e884aa90085ecc838644b0e4b9f15
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 159</p>
KW  - Amazon web services
KW  - Amazon Web Services
KW  - Application programs
KW  - Architecture patterns
KW  - AWS Lambda
KW  - cloud computing
KW  - Cloud computing
KW  - Cluster computing
KW  - Computer architecture
KW  - Cost comparisons
KW  - Cost reduction
KW  - Costs
KW  - Distributed computer systems
KW  - Grid computing
KW  - Information services
KW  - Infrastructure costs
KW  - microservice architecture
KW  - microservices
KW  - Monolithic architecture
KW  - scalable applications
KW  - Service oriented architecture (SOA)
KW  - service oriented architectures
KW  - software architecture
KW  - Software architecture
KW  - software engineering
KW  - Software engineering
KW  - WEB application
KW  - Web services
KW  - World Wide Web
ER  - 

TY  - CONF
TI  - PARTIES: QoS-Aware Resource Partitioning for Multiple Interactive Services
AU  - Chen, S.
AU  - Delimitrou, C.
AU  - Martinez, J.F.
T2  - International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS
AB  - Multi-tenancy in modern datacenters is currently limited to a single latency-critical, interactive service, running alongside one or more low-priority, best-efort jobs. This limits the eiciency gains from multi-tenancy, especially as an increasing number of cloud applications are shifting from batch jobs to services with strict latency requirements. We present PARTIES, a QoS-aware resource manager that enables an arbitrary number of interactive, latency-critical services to share a physical node without QoS violations. PARTIES leverages a set of hardware and software resource partitioning mechanisms to adjust allocations dynamically at runtime, in a way that meets the QoS requirements of each co-scheduled workload, and maximizes throughput for the machine. We evaluate PARTIES on state-of-the-art server platforms across a set of diverse interactive services. Our results show that PARTIES improves throughput under QoS by 61% on average, compared to existing resource managers, and that the rate of improvement increases with the number of co-scheduled applications per physical host. © 2019 Association for Computing Machinery.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3297858.3304005
SP  - 107
EP  - 120
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064601154&doi=10.1145%2f3297858.3304005&partnerID=40&md5=4055a3f1ad5f004e9dfb025ceb4c7fc2
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 259</p>
KW  - Cloud computing
KW  - Cloud-computing
KW  - Datacenter
KW  - datacenters
KW  - Interactive services
KW  - interference
KW  - Interference
KW  - isolation
KW  - Isolation
KW  - Managers
KW  - Multi tenancies
KW  - QoS-aware
KW  - quality of service
KW  - Quality of service
KW  - Quality-of-service
KW  - resource management
KW  - Resource management
KW  - resource partitioning
KW  - Resource partitioning
ER  - 

TY  - JOUR
TI  - Bay-max: QoS Awareness and Increased Utilization for Non-Preemptive Accelerators in Warehouse Scale Computers
AU  - Chen, Quan
AU  - Yang, Hailong
AU  - Mars, Jason
AU  - Tang, Lingjia
T2  - SIGARCH Computer Architecture News
DA  - 2016///
PY  - 2016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098370232&partnerID=40&md5=13168b4daffbc5318b6b5371997a63a5
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - PowerChief: Intelligent power allocation for multi-stage applications to improve responsiveness on power constrained CMP
AU  - Yang, H.
AU  - Chen, Q.
AU  - Riaz, M.
AU  - Luan, Z.
AU  - Tang, L.
AU  - Mars, J.
T2  - Proceedings - International Symposium on Computer Architecture
AB  - Modern user facing applications consist of multiple processing stages with a number of service instances in each stage. The latency profle of these multi-stage applications is intrinsically variable, making it challenging to provide satisfactory responsiveness. Given a limited power budget, improving the end-to-end latency requires intelligently boosting the bottleneck service across stages using multiple boosting techniques. However, prior work fail to acknowledge the multi-stage nature of user-facing applications and perform poorly in improving responsiveness on power constrained CMP, as they are unable to accurately identify bottleneck service and apply the boosting techniques adaptively. In this paper, we present PowerChief, a runtime framework that 1) provides joint design of service and query to monitor the latency statistics across service stages and accurately identifes the bottleneck service during runtime; 2) adaptively chooses the boosting technique to accelerate the bottleneck service with improved responsiveness; 3) dynamically reallocates the constrained power budget across service stages to accommodate the chosen boosting technique. Evaluated with real world multi-stage applications, PowerChief improves the average latency by 20.3× and 32.4× (99% tail latency by 13.3× and 19.4×) for Sirius and Natural Language Processing applications respectively compared to stage-agnostic power allocation. In addition, for the given QoS target, PowerChief reduces the power consumption of Sirius and Web Search applications by 23% and 33% respectively over prior work. © 2017 Association for Computing Machinery.
DA  - 2017///
PY  - 2017
DO  - 10.1145/3079856.3080224
VL  - Part F128643
SP  - 133
EP  - 146
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025619452&doi=10.1145%2f3079856.3080224&partnerID=40&md5=1b3762d3742b803bb5b2ea594b587a90
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 41</p>
KW  - Budget control
KW  - Computer architecture
KW  - End to end latencies
KW  - Facings
KW  - Intelligent Service Boosting
KW  - Intelligent Services
KW  - Multi stage
KW  - Multi-Stage Application
KW  - Multiple processing
KW  - Natural language processing systems
KW  - Number of services
KW  - Power allocations
KW  - Power Constrained CMP
KW  - Quality of service
KW  - Runtime frameworks
ER  - 

TY  - CONF
TI  - Towards serverless as commodity: A case of Knative
AU  - Kaviani, N.
AU  - Kalinin, D.
AU  - Maximilien, M.
T2  - WOSC 2019 - Proceedings of the 2019 5th International Workshop on Serverless Computing, Part of Middleware 2019
AB  - Serverless computing promises to evolve cloud computing architecture from VMs and containers-as-a-service (CaaS) to function-as-a-service (FaaS). This takes away complexities of managing and scaling underlying infrastructure and can result in simpler code, cheaper realization of services, and higher availability. Nonetheless, one of the primary drawbacks customers face when making decision to move their software to a serverless platform is the potential for getting locked-in with a particular provider. This used to be a concern with Platform-as-a-Service (PaaS) offerings too. However with Kubernetes emerging as the industry standard PaaS layer, PaaS is closer to becoming commodity with the Kubernetes API as its common interface. The question is if a similar unification for the API interface layer and runtime contracts can be achieved for serverless. If achieved, this would free up serverless users from their fears of platform lock-in. Our goal in this paper is to extract a minimal common denominator model of execution that can move us closer to a unified serverless platform. As contributors to Knative [13] with in-depth understanding of its internal design, we use Knative as the baseline for this comparison and contrast its API interface and runtime contracts against other prominent serverless platforms to identify commonalities and differences. Influenced by the work in Knative, we also discuss challenges as well as the necessary evolution we expect to see as serverless platforms themselves reach commodity status. © 2019 Association for Computing Machinery.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3366623.3368135
SP  - 13
EP  - 18
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078952724&doi=10.1145%2f3366623.3368135&partnerID=40&md5=646110fad1b668fef322752c42165706
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 21</p>
KW  - Application programming interfaces (API)
KW  - Cloud
KW  - Cloud computing architectures
KW  - Clouds
KW  - Common denominators
KW  - Common interfaces
KW  - Computer architecture
KW  - In-depth understanding
KW  - Industry standards
KW  - Locks (fasteners)
KW  - Middleware
KW  - Model of executions
KW  - Performance
KW  - Platform as a Service (PaaS)
KW  - Scalability
KW  - Serverless
ER  - 

TY  - CONF
TI  - Opportunistic computing in GPU architectures
AU  - Pattnaik, A.
AU  - Tang, X.
AU  - Kayiran, O.
AU  - Jog, A.
AU  - Mishra, A.
AU  - Kandemir, M.T.
AU  - Sivasubramaniam, A.
AU  - Das, C.R.
T2  - Proceedings - International Symposium on Computer Architecture
AB  - Data transfer overhead between computing cores and memory hierarchy has been a persistent issue for von Neumann architectures and the problem has only become more challenging with the emergence of manycore systems. A conceptually powerful approach to mitigate this overhead is to bring the computation closer to data, known as Near Data Computing (NDC). Recently, NDC has been investigated in different flavors for CPU-based multicores, while the GPU domain has received little attention. In this paper, we present a novel NDC solution for GPU architectures with the objective of minimizing on-chip data transfer between the computing cores and Last-Level Cache (LLC). To achieve this, we first identify frequently occurring Load-Compute-Store instruction chains in GPU applications. These chains, when offloaded to a compute unit closer to where the data resides, can significantly reduce data movement. We develop two offloading techniques, called LLC-Compute and Omni-Compute. The first technique, LLC-Compute, augments the LLCs with computational hardware for handling the computation offloaded to them. The second technique (Omni-Compute) employs simple bookkeeping hardware to enable GPU cores to compute instructions offloaded by other GPU cores. Our experimental evaluations on nine GPGPU workloads indicate that the LLC-Compute technique provides, on an average, 19% performance improvement (IPC), 11% performance/watt improvement, and 29% reduction in on-chip data movement compared to the baseline GPU design. The Omni-Compute design boosts these benefits to 31%, 16% and 44%, respectively. © 2019 ACM.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3307650.3322212
SP  - 210
EP  - 223
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067674010&doi=10.1145%2f3307650.3322212&partnerID=40&md5=064f3aa92292124b1033b830a7ca4ba3
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 44</p>
KW  - Computation offloading
KW  - Computer hardware
KW  - Data transfer
KW  - Experimental evaluation
KW  - GPU
KW  - Graphics processing unit
KW  - Lastlevel caches (LLC)
KW  - Manycore systems
KW  - Memory architecture
KW  - Memory hierarchy
KW  - Near data computing
KW  - Neumann architecture
KW  - Opportunistic computing
KW  - Program processors
ER  - 

TY  - JOUR
T2  - WITS: Waikato Internet Traffic Storage
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870668438&partnerID=40&md5=fdeae5e938dbe2ee5bbc11117f041ae1
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 25</p>
ER  - 

TY  - JOUR
TI  - Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems
AU  - Chen, T.
AU  - Li, M.
AU  - Li, Y.
AU  - Lin, M.
AU  - Wang, N.
AU  - Wang, M.
AU  - Xiao, T.
AU  - Xu, B.
AU  - Zhang, C.
AU  - Zhang, Z.
T2  - CoRR
DA  - 2015///
PY  - 2015
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018888481&partnerID=40&md5=f556582559a70d9c8f2f07d9bd0c56d0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 219</p>
ER  - 

TY  - JOUR
TI  - Wikipedia workload analysis for decentralized hosting
AU  - Urdaneta, G.
AU  - Pierre, G.
AU  - van Steen, M.
T2  - Computer Networks
AB  - We study an access trace containing a sample of Wikipedia's traffic over a 107-day period aiming to identify appropriate replication and distribution strategies in a fully decentralized hosting environment. We perform a global analysis of the whole trace, and a detailed analysis of the requests directed to the English edition of Wikipedia. In our study, we classify client requests and examine aspects such as the number of read and save operations, significant load variations and requests for nonexisting pages. We also review proposed decentralized wiki architectures and discuss how they would handle Wikipedia's workload. We conclude that decentralized architectures must focus on applying techniques to efficiently handle read operations while maintaining consistency and dealing with typical issues on decentralized systems such as churn, unbalanced loads and malicious participating nodes. © 2009 Elsevier B.V. All rights reserved.
DA  - 2009///
PY  - 2009
DO  - 10.1016/j.comnet.2009.02.019
VL  - 53
IS  - 11
SP  - 1830
EP  - 1845
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-67349226826&doi=10.1016%2fj.comnet.2009.02.019&partnerID=40&md5=b5fc02e0d8213494d9a714f5be814510
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 304</p>
KW  - Decentralized architecture
KW  - Decentralized hosting
KW  - Decentralized system
KW  - Distribution strategies
KW  - Global analysis
KW  - Internet
KW  - P2P
KW  - Unbalanced loads
KW  - Websites
KW  - Wikipedia
KW  - Workload analysis
ER  - 

TY  - CONF
TI  - Centralized Core-granular Scheduling for Serverless Functions
AU  - Kaffes, K.
AU  - Yadwadkar, N.J.
AU  - Kozyrakis, C.
T2  - SoCC 2019 - Proceedings of the ACM Symposium on Cloud Computing
AB  - In recent years, many applications have started using serverless computing platforms primarily due to the ease of deployment and cost efficiency they offer. However, the existing scheduling mechanisms of serverless platforms fall short in catering to the unique characteristics of such applications: burstiness, short and variable execution times, statelessness and use of a single core. Specifically, the existing mechanisms fall short in meeting the requirements generated due to the combined effect of these characteristics: scheduling at a scale of millions of function invocations per second while achieving predictable performance. In this paper, we argue for a cluster-level centralized and core-granular scheduler for serverless functions. By maintaining a global view of the cluster resources, the centralized approach eliminates queue imbalances while the core granularity reduces interference; together these properties enable reduced performance variability. We expect such a scheduler to increase the adoption of serverless computing platforms by various latency and throughput sensitive applications. © 2019 ACM.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3357223.3362709
SP  - 158
EP  - 164
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091770578&doi=10.1145%2f3357223.3362709&partnerID=40&md5=55e6a8163cf50fe7af99df21b5104efe
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 95</p>
KW  - Centralized approaches
KW  - cloud computing
KW  - Cloud computing
KW  - Combined effect
KW  - Computing platform
KW  - Cost efficiency
KW  - Performance variability
KW  - resource allocation
KW  - scheduling
KW  - Scheduling
KW  - Scheduling mechanism
KW  - Sensitive application
KW  - serverless computing
KW  - Variable execution time
ER  - 

TY  - CONF
TI  - Exploring Serverless Computing for Neural Network Training
AU  - Feng, L.
AU  - Kudva, P.
AU  - Da Silva, D.
AU  - Hu, J.
T2  - IEEE International Conference on Cloud Computing, CLOUD
AB  - Serverless or functions as a service runtimes have shown significant benefits to efficiency and cost for event-driven cloud applications. Although serverless runtimes are limited to applications requiring lightweight computation and memory, such as machine learning prediction and inference, they have shown improvements on these applications beyond other cloud runtimes. Training deep learning can be both compute and memory intensive. We investigate the use of serverless runtimes while leveraging data parallelism for large models, show the challenges and limitations due to the tightly coupled nature of such models, and propose modifications to the underlying runtime implementations that would mitigate them. For hyperparameter optimization of smaller deep learning models, we show that serverless runtimes can provide significant benefit. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/CLOUD.2018.00049
VL  - 2018-July
SP  - 334
EP  - 341
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057438808&doi=10.1109%2fCLOUD.2018.00049&partnerID=40&md5=f06e623d4d0cec6601a12dd5fbed07cb
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 72</p>
KW  - Cloud applications
KW  - Cloud computing
KW  - Cloud cost and performance
KW  - Cloud scaling
KW  - Data parallelism
KW  - Deep learning
KW  - Hyper-parameter optimizations
KW  - Large models
KW  - Learning models
KW  - Neural network training
KW  - Neural networks
KW  - Serverless computing
KW  - Tightly-coupled
ER  - 

TY  - CONF
TI  - Mercury: Hybrid Centralized and Distributed Scheduling in Large Shared Clusters
AU  - Karanasos, K.
AU  - Raol, S.
AU  - Curino, C.
AU  - Douglas, C.
AU  - Chaliparambil, K.
AU  - Fumarola, G.M.
AU  - Ramakrishnan, S.H.R.
AU  - Sakalanaga, S.
T2  - Proceedings of the 2015 USENIX Annual Technical Conference, USENIX ATC 2015
AB  - Datacenter-scale computing for analytics workloads is increasingly common. High operational costs force heterogeneous applications to share cluster resources for achieving economy of scale. Scheduling such large and diverse workloads is inherently hard, and existing approaches tackle this in two alternative ways: 1) centralized solutions offer strict, secure enforcement of scheduling invariants (e.g., fairness, capacity) for heterogeneous applications, 2) distributed solutions offer scalable, efficient scheduling for homogeneous applications. We argue that these solutions are complementary, and advocate a blended approach. Concretely, we propose Mercury, a hybrid resource management framework that supports the full spectrum of scheduling, from centralized to distributed. Mercury exposes a programmatic interface that allows applications to trade-off between scheduling overhead and execution guarantees. Our framework harnesses this flexibility by opportunistically utilizing resources to improve task throughput. Experimental results on production-derived workloads show gains of over 35% in task throughput. These benefits can be translated by appropriate application and framework policies into job throughput or job latency improvements. We have implemented and contributed Mercury as an extension of Apache Hadoop/YARN.1. © 2015 USENIX Annual Technical Conference.
DA  - 2015///
PY  - 2015
SP  - 485
EP  - 497
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077131857&partnerID=40&md5=2eda27e797887c37a269c5dd9bcc7365
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 155</p>
KW  - Blended approach
KW  - Computer software
KW  - Distributed solutions
KW  - Economic and social effects
KW  - Economy of scale
KW  - Efficient scheduling
KW  - Full spectrum
KW  - Hybrid centralized
KW  - Hybrid resource managements
KW  - Job throughput
KW  - Mercury (metal)
KW  - Scheduling
KW  - Throughput
ER  - 

TY  - JOUR
TI  - VIP: Virtualizing IP chains on handheld platforms
AU  - Nachiappan, N.C.
AU  - Zhang, H.
AU  - Ryoo, J.
AU  - Soundararajan, N.
AU  - Sivasubramaniam, A.
AU  - Kandemir, M.T.
AU  - Iyer, R.
AU  - Das, C.R.
T2  - ACM SIGARCH Computer Architecture News
DA  - 2015///
PY  - 2015
VL  - 43
IS  - 3
SP  - 655
EP  - 667
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027250997&partnerID=40&md5=615aeb13614e0bf7ddb079231c0d8cb9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 4</p>
ER  - 

TY  - JOUR
AU  - Konsor, P.
T2  - Intel Power Gadget
DA  - 2024///
PY  - 2024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876078201&partnerID=40&md5=a51df9ce968f03e7e79f6cd83263fa4d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 24</p>
ER  - 

TY  - JOUR
TI  - Deep Learning mit Python und Keras: Das Praxis-Handbuch vom Entwickler der Keras-Bibliothek
AU  - Chollet, F.
T2  - Deep Learning mit Python und Keras: Das Praxis-Handbuch vom Entwickler der Keras-Bibliothek
DA  - 2018///
PY  - 2018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082530819&partnerID=40&md5=40bb660655d66326ab9d500c767b123f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 166</p>
ER  - 

TY  - JOUR
TI  - DLV: Exploiting Device Level Latency Variations for Performance Improvement on Flash Memory Storage Systems
AU  - Cui, J.
AU  - Zhang, Y.
AU  - Wu, W.
AU  - Yang, J.
AU  - Wang, Y.
AU  - Huang, J.
T2  - IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems
AB  - NAND flash has been widely adopted in storage systems due to its better read and write performance and lower power consumption over traditional mechanical hard drives. To meet the increasing performance demand of modern applications, recent studies speed up flash accesses by exploiting access latency variations at the device level. Unfortunately, existing flash access schedulers are still oblivious to such variations, leading to suboptimal I/O performance improvements. In this paper, we propose DLV, a novel flash access scheduler for exploring scheduling opportunities due to device level access latency variations. DLV improves flash access speeds based on process variations and data retention time difference across flash blocks. More importantly, DLV integrates access speed optimization with access scheduling such that the average access response time can be effectively reduced on flash memory storage systems. Our experimental results show that DLV achieves an average of 41.5% performance improvement over the state-of-the-art. © 1982-2012 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/TCAD.2017.2766156
VL  - 37
IS  - 8
SP  - 1546
EP  - 1559
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032436449&doi=10.1109%2fTCAD.2017.2766156&partnerID=40&md5=57cd4cef0a332c000a6a6af92112dc0a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 19</p>
KW  - Error correction
KW  - Error correction codes
KW  - Flash memories
KW  - Flash memory
KW  - LDPC
KW  - low-density parity-check code (LDPC)
KW  - Mathematical programming
KW  - Mechanical drives
KW  - Out of order
KW  - out-of-order scheduler
KW  - Parity check codes
KW  - Performance evaluations
KW  - Process Variation
KW  - process variation (PV)
KW  - Random access memory
KW  - Random access storage
KW  - raw bit error rate (RBER)
KW  - RBER
KW  - Retention age
KW  - retention age (RA)
KW  - Scheduling
KW  - Sensors
KW  - Time factors
ER  - 

TY  - CONF
TI  - Serverless Programming (Function as a Service)
AU  - Castro, P.
AU  - Ishakian, V.
AU  - Muthusamy, V.
AU  - Slominski, A.
T2  - Proceedings - International Conference on Distributed Computing Systems
DA  - 2017///
PY  - 2017
DO  - 10.1109/ICDCS.2017.305
SP  - 2658
EP  - 2659
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027280573&doi=10.1109%2fICDCS.2017.305&partnerID=40&md5=28b8e93ea54cb314f955313130e9c407
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 51</p>
ER  - 

TY  - JOUR
T2  - Azure Durable Functions
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095083784&partnerID=40&md5=73d56b97fab2f3e3d5e4da83dc7c87ff
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 22</p>
ER  - 

TY  - CONF
TI  - The Fast and the Frugal: Tail Latency Aware Provisioning for Coping with Load Variations
AU  - Kumar, A.
AU  - Narayanan, I.
AU  - Zhu, T.
AU  - Sivasubramaniam, A.
T2  - The Web Conference 2020 - Proceedings of the World Wide Web Conference, WWW 2020
AB  - Small and medium sized enterprises use the cloud for running online, user-facing, tail latency sensitive applications with well-defined fixed monthly budgets. For these applications, adequate system capacity must be provisioned to extract maximal performance despite the challenges of uncertainties in load and request-sizes. In this paper, we address the problem of capacity provisioning under fixed budget constraints with the goal of minimizing tail latency. To tackle this problem, we propose building systems using a heterogeneous mix of low latency expensive resources and cheap resources that provide high throughput per dollar. As load changes through the day, we use more faster resources to reduce tail latency during low load periods and more cheaper resources to handle the high load periods. To achieve these tail latency benefits, we introduce novel heterogeneity-aware scheduling and autoscaling algorithms that are designed for minimizing tail latency. Using software prototypes and by running experiments on the public cloud, we show that our approach can outperform existing capacity provisioning systems by reducing the tail latency by as much as 45% under fixed-budget settings. © 2020 ACM.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3366423.3380117
SP  - 314
EP  - 326
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086574713&doi=10.1145%2f3366423.3380117&partnerID=40&md5=9548743118b8b136fc81d22acc45edef
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 15</p>
KW  - Budget control
KW  - Building systems
KW  - Cheap resources
KW  - Existing capacity
KW  - Load variations
KW  - Low load periods
KW  - Scheduling algorithms
KW  - Sensitive application
KW  - Small and medium sized enterprise
KW  - Software prototyping
KW  - System Capacity
KW  - World Wide Web
ER  - 

TY  - CONF
TI  - Smart refresh: An enhanced memory controller design for reducing energy in conventional and 3D die-stacked DRAMs
AU  - Ghosh, M.
AU  - Lee, H.-H.S.
T2  - Proceedings of the Annual International Symposium on Microarchitecture, MICRO
AB  - DRAMs require periodic refresh for preserving data stored in them. The refresh interval for DRAMs depends on the vendor and the design technology they use. For each refresh in a DRAM row, the stored information in each cell is read out and then written back to itself as each DRAM bit read is self-destructive. The refresh process is inevitable for maintaining data correctness, unfortunately, at the expense of power and bandwidth overhead. The future trend to integrate layers of 3D die-stacked DRAMs on top of a processor further exacerbates the situation as accesses to these DRAMs will be more frequent and hiding refresh cycles in the available slack becomes increasingly difficult. Moreover, due to the implication of temperature increase, the refresh interval of 3D die-stacked DRAMs will become shorter than those of conventional ones. This paper proposes an innovative scheme to alleviate the energy consumed in DRAMs. By employing a time-out counter for each memory row of a DRAM module, all the unnecessary periodic refresh operations can be eliminated. The basic concept behind our scheme is that a DRAM row that was recently read or written to by the processor (or other devices that share the same DRAM) does not need to be refreshed again by the periodic refresh operation, thereby eliminating excessive refreshes and the energy dissipated. Based on this concept, we propose a low-cost technique in the memory controller for DRAM power reduction. The simulation results show that our technique can reduce up to 86% of all refresh operations and 59.3% on the average for a 2GB DRAM. This in turn results in a 52.6% energy savings for refresh operations. The overall energy saving in the DRAM is up to 25.7% with an average of 12.13% obtained for SPLASH-2, SPECint2000, and Biobench benchmark programs simulated on a 2GB DRAM. For a 64MB 3D DRAM, the energy saving is up to 21% and 9.37% on an average when the refresh rate is 64 ms. For a faster 32ms refresh rate the maximum and average savings are 12% and 6.8% respectively. © 2007 IEEE.
DA  - 2007///
PY  - 2007
DO  - 10.1109/MICRO.2007.13
SP  - 134
EP  - 145
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-47349120126&doi=10.1109%2fMICRO.2007.13&partnerID=40&md5=11b5090eb7847c4d65a19e6e517976dd
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 178</p>
KW  - BASIC (programming language)
KW  - Benchmark programs
KW  - Biobench
KW  - Canning
KW  - Cost reduction
KW  - Design technologies
KW  - Dies
KW  - Dynamic random access storage
KW  - Energy conservation
KW  - Energy savings
KW  - International symposium
KW  - Ketones
KW  - Memory controllers
KW  - Micro architectures
KW  - Power reductions
KW  - Reducing energy
KW  - Simulation results
KW  - Temperature increase
KW  - Time outs
ER  - 

TY  - JOUR
TI  - Enabling cost-aware and adaptive elasticity of multi-tier cloud applications
AU  - Han, R.
AU  - Ghanem, M.M.
AU  - Guo, L.
AU  - Guo, Y.
AU  - Osmond, M.
T2  - Future Generation Computer Systems
AB  - Elasticity (on-demand scaling) of applications is one of the most important features of cloud computing. This elasticity is the ability to adaptively scale resources up and down in order to meet varying application demands. To date, most existing scaling techniques can maintain applications' Quality of Service (QoS) but do not adequately address issues relating to minimizing the costs of using the service. In this paper, we propose an elastic scaling approach that makes use of cost-aware criteria to detect and analyse the bottlenecks within multi-tier cloud-based applications. We present an adaptive scaling algorithm that reduces the costs incurred by users of cloud infrastructure services, allowing them to scale their applications only at bottleneck tiers, and present the design of an intelligent platform that automates the scaling process. Our approach is generic for a wide class of multi-tier applications, and we demonstrate its effectiveness against other approaches by studying the behaviour of an example e-commerce application using a standard workload benchmark.
DA  - 2014///
PY  - 2014
DO  - 10.1016/j.future.2012.05.018
VL  - 32
IS  - 1
SP  - 82
EP  - 98
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891600220&doi=10.1016%2fj.future.2012.05.018&partnerID=40&md5=2b7a30910070a629e11affb87f30275c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 125</p>
KW  - Adaptive elasticity
KW  - Adaptive scaling
KW  - Adaptive scaling algorithm
KW  - Algorithms
KW  - Benchmarking
KW  - Cloud computing
KW  - Cloud infrastructures
KW  - Cloud-based applications
KW  - Cost-aware
KW  - Cost-aware criteria
KW  - Costs
KW  - E-Commerce applications
KW  - Elasticity
KW  - Intelligent platform
KW  - Multi-tier applications
KW  - Quality of service
ER  - 

TY  - JOUR
TI  - Tributary: Spot-dancing for elastic services with latency SLOs
AU  - Harlap, A.
AU  - Chung, A.
AU  - Tumanov, A.
AU  - Ganger, G.R.
AU  - Gibbons, P.B.
T2  - 2018 USENIX Annual Technical Conference (USENIX ATC 18)
DA  - 2018///
PY  - 2018
SP  - 1
EP  - 14
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058985076&partnerID=40&md5=a8186c9bf0d66cef5aa332d36884d8be
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 12</p>
ER  - 

TY  - CONF
TI  - Kube-Knots: Resource Harvesting through Dynamic Container Orchestration in GPU-based Datacenters
AU  - Thinakaran, P.
AU  - Gunasekaran, J.R.
AU  - Sharma, B.
AU  - Kandemir, M.T.
AU  - Das, C.R.
T2  - Proceedings - IEEE International Conference on Cluster Computing, ICCC
AB  - Compute heterogeneity is increasingly gaining prominence in modern datacenters due to the addition of accelerators like GPUs and FPGAs. We observe that datacenter schedulers are agnostic of these emerging accelerators, especially their resource utilization footprints, and thus, not well equipped to dynamically provision them based on the application needs. We observe that the state-of-the-art datacenter schedulers fail to provide fine-grained resource guarantees for latency-sensitive tasks that are GPU-bound. Specifically for GPUs, this results in resource fragmentation and interference leading to poor utilization of allocated GPU resources. Furthermore, GPUs exhibit highly linear energy efficiency with respect to utilization and hence proactive management of these resources is essential to keep the operational costs low while ensuring the end-to-end Quality of Service (QoS) in case of user-facing queries.Towards addressing the GPU orchestration problem, we build Knots, a GPU-aware resource orchestration layer and integrate it with the Kubernetes container orchestrator to build Kube-Knots. Kube-Knots can dynamically harvest spare compute cycles through dynamic container orchestration enabling co-location of latency-critical and batch workloads together while improving the overall resource utilization. We design and evaluate two GPU-based scheduling techniques to schedule datacenter-scale workloads through Kube-Knots on a ten node GPU cluster. Our proposed Correlation Based Prediction (CBP) and Peak Prediction (PP) schemes together improves both average and 99th percentile cluster-wide GPU utilization by up to 80% in case of HPC workloads. In addition, CBP+PP improves the average job completion times (JCT) of deep learning workloads by up to 36% when compared to state-of-the-art schedulers. This leads to 33% cluster-wide energy savings on an average for three different workloads compared to state-of-the-art GPU-agnostic schedulers. Further, the proposed PP scheduler guarantees the end-to-end QoS for latency-critical queries by reducing QoS violations by up to 53% when compared to state-of-the-art GPU schedulers. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/CLUSTER.2019.8891040
VL  - 2019-September
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075268776&doi=10.1109%2fCLUSTER.2019.8891040&partnerID=40&md5=c15599158265a13428a55a593c69384d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 40</p>
KW  - Batch workloads
KW  - Cluster computing
KW  - Computer architecture
KW  - Containers
KW  - Deep learning
KW  - End-to-end QoS
KW  - End-to-end quality of service
KW  - Energy efficiency
KW  - Graphics processing unit
KW  - Job completion
KW  - Proactive management
KW  - Program processors
KW  - Quality of service
KW  - Resource utilizations
KW  - Scheduling
KW  - Scheduling techniques
KW  - State of the art
ER  - 

TY  - CONF
TI  - DjiNN and Tonic: DNN as a service and its implications for future warehouse scale computers
AU  - Hauswald, J.
AU  - Kang, Y.
AU  - Laurenzano, M.A.
AU  - Chen, Q.
AU  - Li, C.
AU  - Mudge, T.
AU  - Dreslinski, R.G.
AU  - Mars, J.
AU  - Tang, L.
T2  - Proceedings - International Symposium on Computer Architecture
AB  - As applications such as Apple Siri, Google Now, Microsoft Cortana, and Amazon Echo continue to gain traction, web-service companies are adopting large deep neural networks (DNN) for machine learning challenges such as image processing, speech recognition, natural language processing, among others. A number of open questions arise as to the design of a server platform specialized for DNN and how modern warehouse scale computers (WSCs) should be outfitted to provide DNN as a service for these applications. In this paper, we present DjiNN, an open infrastructure for DNN as a service in WSCs, and Tonic Suite, a suite of 7 end-to-end applications that span image, speech, and language processing. We use DjiNN to design a high throughput DNN system based on massive GPU server designs and provide insights as to the varying characteristics across applications. After studying the throughput, bandwidth, and power properties of DjiNN and Tonic Suite, we investigate several design points for future WSC architectures. We investigate the total cost of ownership implications of having a WSC with a disaggregated GPU pool versus a WSC composed of homogeneous integrated GPU servers. We improve DNN throughput by over 120x for all but one application (40x for Facial Recognition) on an NVIDIA K40 GPU. On a GPU server composed of 8 NVIDIA K40s, we achieve near-linear scaling (around 1000x throughput improvement) for 3 of the 7 applications. Through our analysis, we also find that GPU-enabled WSCs improve total cost of ownership over CPU-only designs by 4-20x, depending on the composition of the workload. © 2015 ACM.
DA  - 2015///
PY  - 2015
DO  - 10.1145/2749469.2749472
VL  - 13-17-June-2015
SP  - 27
EP  - 40
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960091813&doi=10.1145%2f2749469.2749472&partnerID=40&md5=09fb751b876e19fca3a0ad4caa395fe0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 119</p>
KW  - Artificial intelligence
KW  - Computational linguistics
KW  - Computer architecture
KW  - Cost benefit analysis
KW  - Deep neural networks
KW  - Design
KW  - End-to-end application
KW  - Face recognition
KW  - Facial recognition
KW  - Image processing
KW  - Implications for futures
KW  - Internet
KW  - Language processing
KW  - Learning algorithms
KW  - Learning systems
KW  - NAtural language processing
KW  - Natural language processing systems
KW  - Network architecture
KW  - Speech recognition
KW  - Throughput
KW  - Throughput improvement
KW  - Total cost of ownership
KW  - Warehouses
KW  - Web services
ER  - 

TY  - JOUR
TI  - Long Short-Term Memory
AU  - Hochreiter, S.
AU  - Schmidhuber, J.
T2  - Neural Computation
AB  - Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.
DA  - 1997///
PY  - 1997
DO  - 10.1162/neco.1997.9.8.1735
VL  - 9
IS  - 8
SP  - 1735
EP  - 1780
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031573117&doi=10.1162%2fneco.1997.9.8.1735&partnerID=40&md5=6e4ee65c4bc5399487e5a65f4186aa19
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 84944</p>
KW  - algorithm
KW  - Algorithms
KW  - article
KW  - artificial neural network
KW  - biological model
KW  - learning
KW  - Learning
KW  - memory
KW  - Memory
KW  - Memory, Short-Term
KW  - Models, Neurological
KW  - Models, Psychological
KW  - nerve cell network
KW  - Nerve Net
KW  - Neural Networks (Computer)
KW  - physiology
KW  - psychological model
KW  - short term memory
KW  - time
KW  - Time Factors
ER  - 

TY  - CONF
TI  - From laptop to Lambda: Outsourcing everyday jobs to thousands of transient functional containers
AU  - Fouladi, S.
AU  - Romero, F.
AU  - Iter, D.
AU  - Li, Q.
AU  - Chatterjee, S.
AU  - Kozyrakis, C.
AU  - Zaharia, M.
AU  - Winstein, K.
T2  - Proceedings of the 2019 USENIX Annual Technical Conference, USENIX ATC 2019
AB  - We present gg, a framework and a set of command-line tools that helps people execute everyday applications—e.g., software compilation, unit tests, video encoding, or object recognition—using thousands of parallel threads on a cloud-functions service to achieve near-interactive completion times. In the future, instead of running these tasks on a laptop, or keeping a warm cluster running in the cloud, users might push a button that spawns 10,000 parallel cloud functions to execute a large job in a few seconds from start. gg is designed to make this practical and easy. With gg, applications express a job as a composition of lightweight OS containers that are individually transient (lifetimes of 1–60 seconds) and functional (each container is hermetically sealed and deterministic). gg takes care of instantiating these containers on cloud functions, loading dependencies, minimizing data movement, moving data between containers, and dealing with failure and stragglers. We ported several latency-sensitive applications to run on gg and evaluated its performance. In the best case, a distributed compiler built on gg outperformed a conventional tool (icecc) by 2–5×, without requiring a warm cluster running continuously. In the worst case, gg was within 20% of the hand-tuned performance of an existing tool for video encoding (ExCamera). © Proceedings of the 2019 USENIX Annual Technical Conference, USENIX ATC 2019. All rights reserved.
DA  - 2019///
PY  - 2019
SP  - 475
EP  - 488
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077087748&partnerID=40&md5=ec153ea5f9118d7208c1a52e894fa867
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 175</p>
KW  - Application programs
KW  - Command line
KW  - Completion time
KW  - Containers
KW  - Data movements
KW  - Distributed compilers
KW  - Encoding (symbols)
KW  - Hermetically sealed
KW  - Laptop computers
KW  - Object recognition
KW  - Sensitive application
KW  - Signal encoding
KW  - Software compilation
KW  - Software testing
KW  - Video encodings
KW  - Video signal processing
ER  - 

TY  - CONF
TI  - GrandSLAm: Guaranteeing SLAs for jobs in microservices execution frameworks
AU  - Kannan, R.S.
AU  - Subramanian, L.
AU  - Raju, A.
AU  - Ahn, J.
AU  - Mars, J.
AU  - Tang, L.
T2  - Proceedings of the 14th EuroSys Conference 2019
AB  - The microservice architecture has dramatically reduced user effort in adopting and maintaining servers by providing a catalog of functions as services that can be used as building blocks to construct applications. This has enabled datacenter operators to look at managing datacenter hosting microservices quite differently from traditional infrastructures. Such a paradigm shift calls for a need to rethink resource management strategies employed in such execution environments. We observe that the visibility enabled by a microservices execution framework can be exploited to achieve high throughput and resource utilization while still meeting Service Level Agreements, especially in multi-tenant execution scenarios. In this study, we present GrandSLAm, a microservice execution framework that improves utilization of datacenters hosting microservices. GrandSLAm estimates time of completion of requests propagating through individual microservice stages within an application. It then leverages this estimate to drive a runtime system that dynamically batches and reorders requests at each microservice in a manner where individual jobs meet their respective target latency while achieving high throughput. GrandSLAm significantly increases throughput by up to 3× compared to the our baseline, without violating SLAs for a wide range of real-world AI and ML applications. © 2019 Association for Computing Machinery.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3302424.3303958
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063902706&doi=10.1145%2f3302424.3303958&partnerID=40&md5=3421b52ef10bf2d471d82be441436ba2
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 130</p>
KW  - Building blockes
KW  - Computer systems
KW  - Execution environments
KW  - Execution framework
KW  - Execution scenario
KW  - Learning systems
KW  - Machine Learning
KW  - Microservice
KW  - Resource management
KW  - Resource utilizations
KW  - Service Level Agreements
KW  - Systems
ER  - 

TY  - CONF
TI  - Slack-based resource arbitration for real-time Networks-on-Chip
AU  - Kostrzewa, A.
AU  - Saidi, S.
AU  - Ernst, R.
T2  - Proceedings of the 2016 Design, Automation and Test in Europe Conference and Exhibition, DATE 2016
AB  - Networks-on-Chip (NoCs) designed for real-time systems must efficiently deal with a broad diversity of traffic requirements. This requires providing latency guarantees for hard real-time transmissions with minimum impact on performance sensitive best-effort traffic. In this work, we present a novel mechanism which achieves this goal through a slack-based global and dynamic prioritization of data streams. This is performed using an overlay network and a scheduling unit combining local arbitration performed in routers with global scheduling of entire logical transmissions for end to end guarantees. Consequently, our approach allows to decrease both hardware and temporal overhead when compared with existing solutions and to achieve a performance improvement up to around 60%. © 2016 EDAA.
DA  - 2016///
PY  - 2016
DO  - 10.3850/9783981537079_0233
SP  - 1012
EP  - 1017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973629992&doi=10.3850%2f9783981537079_0233&partnerID=40&md5=a4fe9a5e520f78dcc0c39cc40eb7a85f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 16</p>
KW  - Best-Effort Traffic
KW  - Data stream
KW  - Global scheduling
KW  - Hard real-time
KW  - Interactive computer systems
KW  - Network-on-chip
KW  - Networks on chips
KW  - Prioritization
KW  - Real time network
KW  - Real time systems
KW  - Scheduling
KW  - Traffic requirements
ER  - 

TY  - CONF
TI  - Portfolio-driven Resource Management for Transient Cloud Servers
AU  - Sharma, P.
AU  - Irwin, D.
AU  - Shenoy, P.
T2  - Performance Evaluation Review
AB  - Cloud providers have begun to offer their surplus capacity in the form of low-cost transient servers, which can be revoked unilaterally at any time. While the low cost of transient servers makes them attractive for a wide range of applications, such as data processing and scientific computing, failures due to server revocation can severely degrade application performance. Since different transient server types offer different cost and availability tradeoffs, we present the notion of server portfolios that is based on financial portfolio modeling. Server portfolios enable construction of an "optimal" mix of severs to meet an application's sensitivity to cost and revocation risk. We implement model-driven portfolios in a system called ExoSphere, and show how diverse applications can use portfolios and application-specific policies to gracefully handle transient servers. We show that ExoSphere enables widely-used parallel applications such as Spark, MPI, and BOINC to be made transiency-aware with modest effort. Our experiments show that allowing the applications to use suitable transiency-aware policies, ExoSphere is able to achieve 80% cost savings when compared to on-demand servers and greatly reduces revocation risk compared to existing approaches. © 2017 Owner/Author.
DA  - 2017///
PY  - 2017
DO  - 10.1145/3078505.3078511
VL  - 45
SP  - 59
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084586768&doi=10.1145%2f3078505.3078511&partnerID=40&md5=8629788bfdd8c27c9ca88bc61755102c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 7</p>
KW  - Application performance
KW  - Application specific
KW  - cloud computing
KW  - Cloud providers
KW  - Costs
KW  - Data handling
KW  - Diverse applications
KW  - Financial portfolio
KW  - Ionosphere
KW  - Parallel application
KW  - Resource management
KW  - Surplus capacity
KW  - transient servers
ER  - 

TY  - CONF
TI  - TimeTrader: Exploiting latency tail to save datacenter energy for online search
AU  - Vamanan, B.
AU  - Sohail, H.B.
AU  - Hasan, J.
AU  - Vijaykumar, T.N.
T2  - Proceedings of the Annual International Symposium on Microarchitecture, MICRO
AB  - Online Search (OLS) is a key component of many popular Internet services. Datacenters running OLS consume significant amounts of energy. However, reducing their energy is challenging due to their tight response time requirements. A key aspect of OLS is that each user query goes to all or many of the nodes in the cluster, so that the overall time budget is dictated by the tail of the replies' latency distribution; replies see latency variations both in the network and compute. Previous work proposes to achieve load-proportional energy by slowing down the computation at lower datacenter loads based directly on response times (i.e., at lower loads, the proposal exploits the average slack in the time budget provisioned for the peak load). In contrast, we propose TimeTrader to reduce energy by exploiting the latency slack in the sub-critical replies which arrive before the deadline (e.g., 80% of replies are 3-4x faster than the tail). This slack is present at all loads and subsumes the previous work's load-related slack. While the previous work shifts the leaves' response time distribution to consume the slack at lower loads, TimeTrader reshapes the distribution at all loads by slowing down individual sub-critical nodes without increasing missed deadlines. TimeTrader exploits slack in both the network and compute budgets. Further, TimeTrader leverages Earliest Deadline First scheduling to largely decouple critical requests from the queuing delays of sub-critical requests which can then be slowed down without hurting critical requests. A combination of real-system measurements and at-scale simulations shows that without adding to missed deadlines, TimeTrader saves 15% and 40% energy at 90% and 30% loading, respectively, in a datacenter with 512 nodes, whereas previous work saves 0% and 30%. Further, as a proof-of-concept, we build a small-scale real implementation to evaluate TimeTrader and show 10-30% energy savings. © 2015 ACM.
DA  - 2015///
PY  - 2015
DO  - 10.1145/2830772.2830779
VL  - 05-09-December-2015
SP  - 585
EP  - 597
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959912236&doi=10.1145%2f2830772.2830779&partnerID=40&md5=d87e1398cbc5d247ceaf45713e3345b5
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 48</p>
KW  - Budget control
KW  - Computer architecture
KW  - datacenter
KW  - Datacenter
KW  - Earliest deadline first scheduling
KW  - Energy conservation
KW  - incast
KW  - Internet
KW  - latency tail
KW  - Latency variations
KW  - Online data
KW  - online data-intensive (OLDI) applications
KW  - online search (OLS)
KW  - Online searching
KW  - Response time distribution
KW  - Scheduling algorithms
KW  - Time requirements
ER  - 

TY  - CONF
TI  - Putting the “micro” back in microservice
AU  - Boucher, S.
AU  - Kalia, A.
AU  - Andersen, D.G.
AU  - Kaminsky, M.
T2  - Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018
AB  - Modern cloud computing environments strive to provide users with fine-grained scheduling and accounting, as well as seamless scalability. The most recent face to this trend is the “serverless” model, in which individual functions, or microservices, are executed on demand. Popular implementations of this model, however, operate at a relatively coarse granularity, occupying resources for minutes at a time and requiring hundreds of milliseconds for a cold launch. In this paper, we describe a novel design for providing “functions as a service” (FaaS) that attempts to be truly micro: cold launch times in microseconds that enable even finer-grained resource accounting and support latency-critical applications. Our proposal is to eschew much of the traditional serverless infrastructure in favor of language-based isolation. The result is microsecond-granularity launch latency, and microsecond-scale preemptive scheduling using high-precision timers. © Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018. All rights reserved.
DA  - 2020///
PY  - 2020
SP  - 645
EP  - 650
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060929156&partnerID=40&md5=01b3057f899a9e50397254abe5b659ed
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 76</p>
KW  - Cloud computing environments
KW  - Critical applications
KW  - Fine grained
KW  - High-precision
KW  - Novel design
KW  - On demands
KW  - Pre-emptive scheduling
KW  - Resource accountings
KW  - Scheduling
ER  - 

TY  - CONF
TI  - Capriccio: Scalable threads for internet services
AU  - Von Behren, R.
AU  - Condit, J.
AU  - Zhou, F.
AU  - Necula, G.C.
AU  - Brewer, E.
T2  - Operating Systems Review (ACM)
AB  - This paper presents Capriccio, a scalable thread package for use with high-concurrency servers. While recent work has advocated event-based systems, we believe that thread-based systems can provide a simpler programming model that achieves equivalent or superior performance. By implementing Capriccio as a user-level thread package, we have decoupled the thread package implementation from the underlying operating system. As a result, we can take advantage of cooperative threading, new asynchronous I/O mechanisms, and compiler support. Using this approach, we are able to provide three key features: (1) scalability to 100,000 threads, (2) efficient stack management, and (3) resource-aware scheduling. We introduce linked stack management, which minimizes the amount of wasted stack space by providing safe, small, and non-contiguous stacks that can grow or shrink at run time. A compiler analysis makes our stack implementation efficient and sound. We also present resource-aware scheduling, which allows thread scheduling and admission control to adapt to the system's current resource usage. This technique uses a blocking graph that is automatically derived from the application to describe the flow of control between blocking points in a cooperative thread package. We have applied our techniques to the Apache 2.0.44 web server, demonstrating that we can achieve high performance and scalability despite using a simple threaded programming model. Copyright 2003 ACM.
DA  - 2003///
PY  - 2003
DO  - 10.1145/1165389.945471
VL  - 37
SP  - 268
EP  - 281
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-21644460050&doi=10.1145%2f1165389.945471&partnerID=40&md5=5c1555e4f215e242d73e61edc32d1779
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 172</p>
KW  - Blocking graph
KW  - Computer programming
KW  - Data structures
KW  - Dynamic stack growth
KW  - Graph theory
KW  - Internet
KW  - Linked stack management
KW  - Mathematical models
KW  - Problem solving
KW  - Program compilers
KW  - Resource-aware scheduling
KW  - Scheduling
KW  - Servers
KW  - User-level threads
ER  - 

TY  - CONF
TI  - Short-Circuiting Memory Traffic in Handheld Platforms
AU  - Yedlapalli, P.
AU  - Nachiappan, N.C.
AU  - Soundararajan, N.
AU  - Sivasubramaniam, A.
AU  - Kandemir, M.T.
AU  - Das, C.R.
T2  - Proceedings of the Annual International Symposium on Microarchitecture, MICRO
AB  - Handheld devices are ubiquitous in today's world. With their advent, we also see a tremendous increase in device-user interactivity and real-time data processing needs. Media (audio/video/camera) and gaming use-cases are gaining substantial user attention and are defining product successes. The combination of increasing demand from these use-cases and having to run them at low power (from a battery) means that architects have to carefully study the applications and optimize the hardware and software stack together to gain significant optimizations. In this work, we study workloads from these domains and identify the memory subsystem (system agent) to be a critical bottleneck to performance scaling. We characterize the lifetime of the "frame-based" data used in these workloads through the system and show that, by communicating at frame granularity, we miss significant performance optimization opportunities, caused by large IP-to-IP data reuse distances. By carefully breaking these frames into sub-frames, while maintaining correctness, we demonstrate substantial gains with limited hardware requirements. Specifically, we evaluate two techniques, flow-buffering and IP-IP short-circuiting, and show that these techniques bring both power-performance benefits and enhanced user experience. © 2014 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/MICRO.2014.60
VL  - 2015-January
SP  - 166
EP  - 177
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937697128&doi=10.1109%2fMICRO.2014.60&partnerID=40&md5=6461beb89e2a6277714ca99b3b02bd76
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 10</p>
KW  - Application programs
KW  - Buffer
KW  - Computer architecture
KW  - Data handling
KW  - Data storage equipment
KW  - Frames
KW  - Hardware and software
KW  - Locality
KW  - Memory
KW  - Mobile
KW  - Performance optimizations
KW  - Power performance
KW  - Real-time data processing
KW  - SoC
KW  - System-on-chip
KW  - Timing circuits
KW  - User interfaces
ER  - 

TY  - JOUR
T2  - Brigade-Azure
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098487850&partnerID=40&md5=ea53304fd7d94372fb76afffaefa61fb
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
T2  - Google Cloud Functions
DA  - 2018///
PY  - 2018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058988852&partnerID=40&md5=3754edb5c4abbd9a67b401f83d0d33ec
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 27</p>
ER  - 

TY  - JOUR
AU  - Carlson, J.L.
T2  - Redis in Action
DA  - 2013///
PY  - 2013
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907072305&partnerID=40&md5=2eb374aef30da01750e69449d26943b1
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 229</p>
ER  - 

TY  - JOUR
TI  - SOCK: Rapid task provisioning with serverless-optimized containers
AU  - Oakes, E.
AU  - Yang, L.
AU  - Zhou, D.
AU  - Houck, K.
AU  - Harter, T.
AU  - Arpaci-Dusseau, A.
AU  - Arpaci-Dusseau, R.
T2  - USENIX ATC
DA  - 2018///
PY  - 2018
SP  - 57
EP  - 70
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062616330&partnerID=40&md5=9b225926df24cf1dbc5436bcf966f5ca
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 39</p>
ER  - 

TY  - CONF
TI  - Sirius: An open end-to-end voice and vision personal assistant and its implications for future warehouse scale computers
AU  - Hauswald, J.
AU  - Laurenzano, M.A.
AU  - Zhang, Y.
AU  - Li, C.
AU  - Rovinski, A.
AU  - Khurana, A.
AU  - Dreslinski, R.G.
AU  - Mudge, T.
AU  - Petrucci, V.
AU  - Tang, L.
AU  - Mars, J.
T2  - International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS
AB  - As user demand scales for intelligent personal assistants (IPAs) such as Apple's Siri, Google's Google Now, and Microsoft's Cortana, we are approaching the computational limits of current datacenter architectures. It is an open question how future server architectures should evolve to enable this emerging class of applications, and the lack of an open-source IPA workload is an obstacle in addressing this question. In this paper, we present the design of Sirius, an open end-to-end IPA web-service application that accepts queries in the form of voice and images, and responds with natural language. We then use this workload to investigate the implications of four points in the design space of future accelerator-based server architectures spanning traditional CPUs, GPUs, manycore throughput co-processors, and FP-GAs. To investigate future server designs for Sirius, we decompose Sirius into a suite of 7 benchmarks (Sirius Suite) comprising the computationally intensive bottlenecks of Sirius. We port Sirius Suite to a spectrum of accelerator platforms and use the performance and power trade-offs across these platforms to perform a total cost of ownership (TCO) analysis of various server design points. In our study, we find that accelerators are critical for the future scalability of IPA services. Our results show that GPU- and FPGA-accelerated servers improve the query latency on average by 10 × and 16 ×. For a given throughput, GPU- and FPGA-accelerated servers can reduce the TCO of datacenters by 2.6 × and 1.4 ×, respectively. Copyright © 2015 ACM.
DA  - 2015///
PY  - 2015
DO  - 10.1145/2694344.2694347
VL  - 2015-January
SP  - 223
EP  - 238
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939202658&doi=10.1145%2f2694344.2694347&partnerID=40&md5=1e5c86dc98b6a15e7c6049c7ffdcb0d6
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 82</p>
KW  - Benchmarking
KW  - Computer graphics equipment
KW  - Datacenter
KW  - Datacenters
KW  - Digital storage
KW  - Emerging workload
KW  - Emerging workloads
KW  - End to end
KW  - Google+
KW  - Graphics processing unit
KW  - Implications for futures
KW  - Integrated circuit design
KW  - Intelligent personal assistants
KW  - MicroSoft
KW  - Personal assistants
KW  - Personal digital assistants
KW  - Query languages
KW  - Server architecture
KW  - Time sharing systems
KW  - User demands
KW  - Warehouse scale computer
KW  - Warehouse scale computers
ER  - 

TY  - CONF
TI  - Recognizing facial expression: Machine learning and application to spontaneous behavior
AU  - Bartlett, M.S.
AU  - Littlewort, G.
AU  - Frank, M.
AU  - Lainscsek, C.
AU  - Fasel, I.
AU  - Movellan, J.
T2  - Proceedings - 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, CVPR 2005
AB  - We present a systematic comparison of machine learning methods applied to the problem of fully automatic recognition of facial expressions. We report results on a series of experiments comparing recognition engines, including AdaBoost, support vector machines, linear discriminant analysis. We also explored feature selection techniques, including the use of AdaBoost for feature selection prior to classification by SVM or LDA. Best results were obtained by selecting a subset of Gabor filters using AdaBoost followed by classification with Support Vector Machines. The system operates in real-time, and obtained 93% correct generalization to novel subjects for a 7-way forced choice on the Cohn-Kanade expression dataset. The outputs of the classifiers change smoothly as a function of time and thus can be used to measure facial expression dynamics. We applied the system to to fully automated recognition of facial actions (FACS). The present system classifies 17 action units, whether they occur singly or in combination with other actions, with a mean accuracy of 94.8%. We present preliminary results for applying this system to spontaneous facial expressions.
DA  - 2005///
PY  - 2005
DO  - 10.1109/CVPR.2005.297
VL  - II
SP  - 568
EP  - 573
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-24644474838&doi=10.1109%2fCVPR.2005.297&partnerID=40&md5=e27b251400f3321ed4a63b54202a8338
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 541</p>
KW  - Database systems
KW  - Facial expression dynamics
KW  - Facial expressions
KW  - Feature selection techniques
KW  - Learning systems
KW  - Optical filters
KW  - Pattern recognition
KW  - Real time systems
KW  - Support vector machines
ER  - 

TY  - CONF
TI  - Serverless Computing: Design, Implementation, and Performance
AU  - McGrath, G.
AU  - Brenner, P.R.
T2  - Proceedings - IEEE 37th International Conference on Distributed Computing Systems Workshops, ICDCSW 2017
AB  - We present the design of a novel performance-oriented serverless computing platform implemented in. NET, deployed in Microsoft Azure, and utilizing Windows containers as function execution environments. Implementation challenges such as function scaling and container discovery, lifecycle, and reuse are discussed in detail. We propose metrics to evaluate the execution performance of serverless platforms and conduct tests on our prototype as well as AWS Lambda, Azure Functions, Google Cloud Functions, and IBM's deployment of Apache OpenWhisk. Our measurements show the prototype achieving greater throughput than other platforms at most concurrency levels, and we examine the scaling and instance expiration trends in the implementations. Additionally, we discuss the gaps and limitations in our current design, propose possible solutions, and highlight future research. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/ICDCSW.2017.36
SP  - 405
EP  - 410
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027531918&doi=10.1109%2fICDCSW.2017.36&partnerID=40&md5=7922eccabb0c16d5ecf2a4aba6079cf0
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 282</p>
KW  - Apache OpenWhisk
KW  - AWS Lambda
KW  - Azure Functions
KW  - Containers
KW  - Distributed computer systems
KW  - FaaS
KW  - Function-as-a-Service
KW  - Google Cloud Functions
KW  - IBM OpenWhisk
KW  - serverless computing
KW  - serverless performance
KW  - Web services
KW  - Windows operating system
ER  - 

TY  - JOUR
TI  - Peeking behind the curtains of serverless platforms
AU  - Wang, L.
AU  - Li, M.
AU  - Zhang, Y.
AU  - Ristenpart, T.
AU  - Swift, M.
T2  - ATC
DA  - 2018///
PY  - 2018
SP  - 133
EP  - 146
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059031873&partnerID=40&md5=caa34109dbbbd9ed3461168088924e09
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 75</p>
ER  - 

TY  - JOUR
AU  - Chodorow, K.
AU  - Dirolf, M.
T2  - MongoDB: The Definitive Guide
DA  - 2010///
PY  - 2010
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053414450&partnerID=40&md5=91309a4540699bdd8c19f002b62d9fdb
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 602</p>
ER  - 

TY  - CONF
TI  - SoftSKU: Optimizing server architectures for microservice diversity @scale
AU  - Sriraman, A.
AU  - Dhanotia, A.
AU  - Wenisch, T.F.
T2  - Proceedings - International Symposium on Computer Architecture
AB  - The variety and complexity of microservices in warehouse-scale data centers has grown precipitously over the last few years to support a growing user base and an evolving product portfolio. Despite accelerating microservice diversity, there is a strong requirement to limit diversity in underlying server hardware to maintain hardware resource fungibility, preserve procurement economies of scale, and curb qualification/test overheads. As such, there is an urgent need for strategies that enable limited server CPU architectures (a.k.a "SKUs") to provide performance and energy efficiency over diverse microservices. To this end, we first undertake a comprehensive characterization of the top seven microservices that run on the compute-optimized data center fleet at Facebook. Our characterization reveals profound diversity in OS and I/O interaction, cache misses, memory bandwidth utilization, instruction mix, and CPU stall behavior. Whereas customizing a CPU SKU for each microservice might be beneficial, it is prohibitive. Instead, we argue for "soft SKUs", wherein we exploit coarse-grain (e.g., boot time) configuration knobs to tune the platform for a particular microservice. We develop a tool, μSKU, that automates search over a soft-SKU design space using A/B testing in production and demonstrate how it can obtain statistically significant gains (up to 7.2% and 4.5% performance improvement over stock and production servers, respectively) with no additional hardware requirements. © 2019 ACM.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3307650.3322227
SP  - 513
EP  - 526
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069454735&doi=10.1145%2f3307650.3322227&partnerID=40&md5=ab1d2deda28dc255ad24bd10331349d9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 93</p>
KW  - Computer architecture
KW  - Computer hardware
KW  - Economics
KW  - Economies of scale
KW  - Energy efficiency
KW  - Green computing
KW  - Hardware resources
KW  - Memory bandwidths
KW  - Microservice
KW  - Product portfolios
KW  - Resource fungibility
KW  - Server architecture
KW  - Soft SKU
ER  - 

TY  - JOUR
AU  - Singhvi, A.
AU  - Houck, K.
AU  - Balasubramanian, A.
AU  - Shaikh, M.D.
AU  - Venkataraman, S.
AU  - Akella, A.
T2  - Archipelago: A Scalable Low-Latency Serverless Platform
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086275868&partnerID=40&md5=74375a642a1e604dccda8238e9406f1a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 30</p>
ER  - 

TY  - JOUR
TI  - Kubernetes
T2  - Kubernetes
DA  - 2024///
PY  - 2024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193979418&partnerID=40&md5=f7b85be2a65bdd29ab785bed2cf78d41
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 57</p>
ER  - 

TY  - CONF
TI  - Brownout: Building more robust cloud applications
AU  - Klein, C.
AU  - Maggio, M.
AU  - Arzén, K.-E.
AU  - Hernández-Rodriguez, F.
T2  - Proceedings - International Conference on Software Engineering
AB  - Self-adaptation is a first class concern for cloud applications, which should be able to withstand diverse runtime changes. Variations are simultaneously happening both at the cloud infrastructure level - for example hardware failures - and at the user workload level - flash crowds. However, robustly withstanding extreme variability, requires costly hardware over-provisioning. In this paper, we introduce a self-adaptation programming paradigm called brownout. Using this paradigm, applications can be designed to robustly withstand unpredictable runtime variations, without over-provisioning. The paradigm is based on optional code that can be dynamically deactivated through decisions based on control theory. We modified two popular web application prototypes - RUBiS and RUBBoS - with less than 170 lines of code, to make them brownout-compliant. Experiments show that brownout self-adaptation dramatically improves the ability to withstand flash-crowds and hardware failures. © 2014 ACM.
DA  - 2014///
PY  - 2014
DO  - 10.1145/2568225.2568227
SP  - 700
EP  - 711
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994065778&doi=10.1145%2f2568225.2568227&partnerID=40&md5=65399bd44b01bdd11e43ebcda0ddb8ab
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 120</p>
KW  - Adaptive software
KW  - Adaptive Software
KW  - Brownout
KW  - Cloud
KW  - Cloud applications
KW  - Cloud infrastructures
KW  - Clouds
KW  - Control theory
KW  - Control Theory
KW  - Hardware
KW  - Hardware failures
KW  - Over provisioning
KW  - Programming paradigms
KW  - Reconfigurable hardware
KW  - Run-time variations
KW  - Software engineering
ER  - 

TY  - JOUR
T2  - Locust
DA  - 2024///
PY  - 2024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212237688&partnerID=40&md5=ba928c4331d587e56c2d376188e52943
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Harnessing renewable energy in cloud                      datacenters: Opportunities and challenges
AU  - Deng, W.
AU  - Liu, F.
AU  - Jin, H.
AU  - Li, B.
AU  - Li, D.
T2  - IEEE Network
AB  - The proliferation of cloud computing has promoted the wide deployment                         of largescale datacenters with tremendous power consumption and high carbon                         emission. To reduce power cost and carbon footprint, an increasing number of                         cloud service providers have considered green datacenters with renewable                         energy sources, such as solar or wind. However, unlike the stable supply of                         grid energy, it is challenging to utilize and realize renewable energy due                         to the uncertain, intermittent and variable nature. In this article, we                         provide a taxonomy of the state-of-the-art research in applying renewable                         energy in cloud computing datacenters from five key aspects, including                         generation models and prediction methods of renewable energy, capacity                         planning of green datacenters, intra-datacenter workload scheduling and load                         balancing across geographically distributed datacenters. By exploring new                         research challenges involved in managing the use of renewable energy in                         datacenters, this article attempts to address why, when, where and how to                         leverage renewable energy in datacenters, also with a focus on future                         research avenues. © 2014 IEEE.
DA  - 2014///
PY  - 2014
DO  - 10.1109/MNET.2014.6724106
VL  - 28
IS  - 1
SP  - 48
EP  - 55
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893874351&doi=10.1109%2fMNET.2014.6724106&partnerID=40&md5=cb6de00c7eb3b66d85598cfc7b2e0d49
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 145</p>
KW  - Capacity planning
KW  - Carbon footprint
KW  - Cloud computing
KW  - Cloud service providers
KW  - Computer supported cooperative work
KW  - Environmental impact
KW  - Green computing
KW  - Prediction methods
KW  - Renewable energies
KW  - Renewable energy resources
KW  - Renewable energy source
KW  - Research challenges
KW  - State of the art
KW  - Use of renewable energies
ER  - 

TY  - CONF
TI  - Quality-Elasticity: Improved Resource Utilization, Throughput, and Response Times Via Adjusting Output Quality to Current Operating Conditions
AU  - Larsson, L.
AU  - Tarneberg, W.
AU  - Klein, C.
AU  - Elmroth, E.
T2  - Proceedings - 2019 IEEE International Conference on Autonomic Computing, ICAC 2019
AB  - This work addresses two related problems for on-line services, namely poor resource utilization during regular operating conditions, and low throughput, long response times, or poor performance under periods of high system load. To address these problems, we introduce our notion of quality-elasticity as a manner of dynamically adapting response qualities from software services along a fine-grained spectrum. When resources are abundant, response quality can be increased, and when resources are scarce, responses are delivered at a lower quality to prioritize throughput and response times. We present an example of how a complex online shopping site can be made quality-elastic. Experiments show that, compared to state of the art, improvements in throughput (57% more served queries), lowered response times (8 time reduction for 95th percentile responses), and an estimated 40% profitability increase can be made using our quality-elastic approach. When resources are abundant, our approach may achieve upwards of twice as high resource utilization as prior work in this field. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/ICAC.2019.00017
SP  - 52
EP  - 62
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073231355&doi=10.1109%2fICAC.2019.00017&partnerID=40&md5=e88f0d1ee213ca825a9c8773645623b7
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3</p>
KW  - adaptive software
KW  - Adaptive software
KW  - brownout
KW  - cloud computing
KW  - Cloud computing
KW  - Computer programming
KW  - Computer science
KW  - Elasticity
KW  - Online shopping sites
KW  - Operating condition
KW  - Poor performance
KW  - Resource utilizations
KW  - service delivery
KW  - Service delivery
KW  - Software services
ER  - 

TY  - CONF
TI  - SweetspotVM: Oversubscribing CPU without Sacrificing VM Performance
AU  - Jacquet, P.
AU  - Ledoux, T.
AU  - Rouvoy, R.
T2  - Proceedings - 2024 IEEE 24th International Symposium on Cluster, Cloud and Internet Computing, CCGrid 2024
AB  - The adoption of computing resources oversubscription in cloud environments is conventionally limited to a restricted subset of Virtual Machines (VMs) within the providers' offerings, primarily driven by performance considerations. So far, VMs schedulers mostly implement all-or-nothing oversubscription strategies, wherein all VM resources are either oversubscribed or remain unaltered. While the former strategy offers higher consolidation rates, the latter delivers better performance guarantees.In this paper, we conducted an empirical study of the individual usage of virtual CPUs (vCPUs) in the OVHCloud production environment and we demonstrate that, as they are not uniformly utilized, the current holistic approach may not be appropriate. Based on these observations, we introduce a novel approach, named SweetspotVM, where oversubscription ratios are applied at the granularity of individual vCPU, instead of the whole VMs. This novel paradigm unlocks a more flexible oversubscription management strategy, pinning oversubscription ratios per vCPU within VMs. We present a prototype of SweetspotVM to illustrate the feasibility of accommodating multiple oversubscription levels within a single host and assigning them to individual vCPU.We assess the viability of our approach on a physical platform, demonstrating the possibility of dividing the cost of hosting VMs by 3, while maintaining the VMs performance at the level of non-oversubscribed platforms. We, therefore, believe that SweetspotVM opens new avenues to boost the consolidation of VMs on a reduced number of servers, with positive impacts on the environmental footprint of cloud computing.  © 2024 IEEE.
DA  - 2024///
PY  - 2024
DO  - 10.1109/CCGrid59990.2024.00026
SP  - 148
EP  - 157
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207968601&doi=10.1109%2fCCGrid59990.2024.00026&partnerID=40&md5=be3bbe3e8eb33bfab68e6d5c7e0b13cc
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 3</p>
KW  - All or nothings
KW  - Cloud
KW  - Cloud computing
KW  - Cloud environments
KW  - Cloud platforms
KW  - Computing resource
KW  - Empirical studies
KW  - Iaa
KW  - IaaS
KW  - Machine performance
KW  - Machine resources
KW  - Oversubscription
KW  - Performance
KW  - Performance guarantees
KW  - Virtual machine
ER  - 

TY  - JOUR
TI  - EnosLib: A Library for Experiment-Driven Research in Distributed Computing
AU  - Cherrueau, R.-A.
AU  - Delavergne, M.
AU  - van Kempen, A.
AU  - Lebre, A.
AU  - Pertin, D.
AU  - Balderrama, J.R.
AU  - Simonet, A.
AU  - Simonin, M.
T2  - IEEE Transactions on Parallel and Distributed Systems
AB  - Despite the importance of experiment-driven research in the distributed computing community, there has been little progress in helping researchers conduct their experiments. In most cases, they have to achieve tedious and time-consuming development and instrumentation activities to deal with the specifics of testbeds and the system under study. In order to relieve researchers of the burden of those efforts, we have developed ENOSLIB: a Python library that takes into account best experimentation practices and leverages modern toolkits on automatic deployment and configuration systems. ENOSLIB helps researchers not only in the process of developing their experimental artifacts, but also in running them over different infrastructures. To demonstrate the relevance of our library, we discuss three experimental engines built on top of ENOSLIB, and used to conduct empirical studies on complex software stacks between 2016 and 2019 (database systems, communication buses and OpenStack). By introducing ENOSLIB, our goal is to gather academic and industrial actors of our community around a library that aggregates everyday experiment-driven research operations. A library that has been already adopted by open-source projects and members of the scientific community thanks to its ease of use and extension. © 2021 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/TPDS.2021.3111159
VL  - 33
IS  - 6
SP  - 1464
EP  - 1477
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114742756&doi=10.1109%2fTPDS.2021.3111159&partnerID=40&md5=be6ea4f33bc372f075fb3c171ab0cf5a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 14</p>
KW  - Automatic configuration
KW  - Automatic deployments
KW  - Benchmark testing
KW  - Code
KW  - Computing community
KW  - Distributed computer systems
KW  - distributed computing experimentation library
KW  - Distributed computing experimentation library
KW  - Experiment-driven research
KW  - Industrial research
KW  - Job analysis
KW  - Libraries
KW  - Open source software
KW  - performance evaluation
KW  - Performances evaluation
KW  - Software
KW  - Software testing
KW  - Task analysis
ER  - 

TY  - JOUR
TI  - The real climate and transformative impact of ICT: A critique of estimates, trends, and regulations
AU  - Freitag, C.
AU  - Berners-Lee, M.
AU  - Widdicks, K.
AU  - Knowles, B.
AU  - Blair, G.S.
AU  - Friday, A.
T2  - Patterns
AB  - In this paper, we critique ICT's current and projected climate impacts. Peer-reviewed studies estimate ICT's current share of global greenhouse gas (GHG) emissions at 1.8%–2.8% of global GHG emissions; adjusting for truncation of supply chain pathways, we find that this share could actually be between 2.1% and 3.9%. For ICT's future emissions, we explore assumptions underlying analysts' projections to understand the reasons for their variability. All analysts agree that ICT emissions will not reduce without major concerted efforts involving broad political and industrial action. We provide three reasons to believe ICT emissions are going to increase barring intervention and find that not all carbon pledges in the ICT sector are ambitious enough to meet climate targets. We explore the underdevelopment of policy mechanisms for enforcing sector-wide compliance, and contend that, without a global carbon constraint, a new regulatory framework is required to keep the ICT sector's footprint aligned with the Paris Agreement. © 2021 The Authors
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.patter.2021.100340
VL  - 2
IS  - 9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122638594&doi=10.1016%2fj.patter.2021.100340&partnerID=40&md5=d7af9a116bbc52d9530b7e2158984995
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 341</p>
KW  - 'current
KW  - AI
KW  - big data
KW  - Block-chain
KW  - blockchain
KW  - Blockchain
KW  - carbon footprint
KW  - Carbon footprint
KW  - Climate impacts
KW  - Current share
KW  - data science
KW  - Gas emissions
KW  - Greenhouse gas emissions
KW  - Greenhouse gases
KW  - ICT
KW  - Industrial emissions
KW  - Internet of things
KW  - IoT
KW  - policy
KW  - Political actions
KW  - Regulation
KW  - regulations
KW  - Regulatory compliance
KW  - Supply chains
KW  - Trend
KW  - trends
ER  - 

TY  - JOUR
TI  - A Self-Adaptive Approach for Managing Applications and Harnessing Renewable Energy for Sustainable Cloud Computing
AU  - Xu, M.
AU  - Toosi, A.N.
AU  - Buyya, R.
T2  - IEEE Transactions on Sustainable Computing
AB  - Rapid adoption of Cloud computing for hosting services and its success is primarily attributed to its attractive features such as elasticity, availability and pay-as-you-go pricing model. However, the huge amount of energy consumed by cloud data centers makes it to be one of the fastest growing sources of carbon emissions. Approaches for improving the energy efficiency include enhancing the resource utilization to reduce resource wastage and applying the renewable energy as the energy supply. This work aims to reduce the carbon footprint of the data centers by reducing the usage of brown energy and maximizing the usage of renewable energy. Taking advantage of microservices and renewable energy, we propose a self-adaptive approach for the resource management of interactive workloads and batch workloads. To ensure the quality of service of workloads, a brownout-based algorithm for interactive workloads and a deferring algorithm for batch workloads are proposed. We have implemented the proposed approach in a prototype system and evaluated it with web services under real traces. The results illustrate our approach can reduce the brown energy usage by 21 percent and improve the renewable energy usage by 10 percent.  © 2020 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/TSUSC.2020.3014943
VL  - 6
IS  - 4
SP  - 544
EP  - 558
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089367582&doi=10.1109%2fTSUSC.2020.3014943&partnerID=40&md5=2f433e3f8ac589be591826822a62d110
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 52</p>
KW  - Brownout
KW  - Carbon emissions
KW  - Carbon footprint
KW  - Cloud computing
KW  - Cloud data centers
KW  - Energy efficiency
KW  - Energy supplies
KW  - Environmental impact
KW  - Microservices
KW  - Prototype system
KW  - QoS
KW  - Quality of service
KW  - Renewable energies
KW  - Renewable energy efficiency
KW  - Renewable energy resources
KW  - Resource management
KW  - Resource utilizations
KW  - Self adaptive approach
KW  - Web services
ER  - 

TY  - JOUR
T2  - Image resizing application
DA  - 2024///
PY  - 2024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212220341&partnerID=40&md5=fca921ae4340cb6eb60d8c3752f546e9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - Grid'5000: A large scale and highly reconfigurable Grid experimental testbed
AU  - Cappello, F.
AU  - Caron, E.
AU  - Dayde, M.
AU  - Desprez, F.
AU  - Jegou, Y.
AU  - Primet, P.
AU  - Jeannot, E.
AU  - Lanteri, S.
AU  - Leduc, J.
AU  - Melab, N.
AU  - Mornet, G.
AU  - Namyst, R.
AU  - Quetier, B.
AU  - Richard, O.
T2  - Proceedings - IEEE/ACM International Workshop on Grid Computing
AB  - Large scale distributed systems like Grids are difficult to study only from theoretical models and simulators. Most Grids deployed at large scale are production platforms that are inappropriate research tools because of their limited reconfiguration, control and monitoring capabilities. In this paper, we present Grid'5000, a 5000 CPUs nation-wide infrastructure for research in Grid computing. Grid'5000 is designed to provide a scientific tool for computer scientists similar to the large-scale instruments used by physicists, astronomers and biologists. We describe the motivations, design, architecture, configuration examples of Grid'5000 and performance results for the reconfiguration subsystem. © 2005 IEEE.
DA  - 2005///
PY  - 2005
DO  - 10.1109/GRID.2005.1542730
VL  - 2005
SP  - 99
EP  - 106
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749669647&doi=10.1109%2fGRID.2005.1542730&partnerID=40&md5=bf7e0a13b21c93e622ae73fd2c3728cf
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 154</p>
KW  - Astronomy
KW  - Distributed computer systems
KW  - Large scale systems
KW  - Large-scale instruments
KW  - Mathematical models
KW  - Research and development management
KW  - Research tools
KW  - Theoretical models
ER  - 

TY  - JOUR
TI  - Brownout approach for adaptive management of resources and applications in cloud computing systems: A taxonomy and future directions
AU  - Xu, M.
AU  - Buyya, R.
T2  - ACM Computing Surveys
AB  - Cloud computing has been regarded as an emerging approach to provisioning resources and managing applications. It provides attractive features, such as an on-demand model, scalability enhancement, and management cost reduction. However, cloud computing systems continue to face problems such as hardware failures, overloads caused by unexpected workloads, or the waste of energy due to inefficient resource utilization, which all result in resource shortages and application issues such as delays or saturation. A paradigm, the brownout, has been applied to handle these issues by adaptively activating or deactivating optional parts of applications or services to manage resource usage in cloud computing system. Brownout has successfully shown that it can avoid overloads due to changes in workload and achieve better load balancing and energy saving effects. This article proposes a taxonomy of the brownout approach for managing resources and applications adaptively in cloud computing systems and carries out a comprehensive survey. It identifies open challenges and offers future research directions. © 2019 Association for Computing Machinery.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3234151
VL  - 52
IS  - 1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061091847&doi=10.1145%2f3234151&partnerID=40&md5=ffe6d0023e39b43b51be5143ff1727c6
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 64</p>
KW  - Adaptive management
KW  - Adaptive Management
KW  - Brownout
KW  - Cloud computing
KW  - Cloud-computing
KW  - Computing system
KW  - Cost reduction
KW  - Costs reduction
KW  - Demand modelling
KW  - Energy conservation
KW  - Management costs
KW  - On demands
KW  - Optional service
KW  - Optional services
KW  - Quality of service
KW  - Quality-of-service
KW  - Taxonomies
ER  - 

TY  - CONF
TI  - Characterization of Different User Behaviors for Demand Response in Data Centers
AU  - Madon, M.
AU  - Da Costa, G.
AU  - Pierson, J.-M.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Digital technologies are becoming ubiquitous while their impact increases. A growing part of this impact happens far away from the end users, in networks or data centers, contributing to a rebound effect. A solution for a more responsible use is therefore to involve the user. As a first step in this quest, this work considers the users of a data center and characterizes their contribution to curtail the computing load for a short period of time by solely changing their job submission behavior. The contributions are: (i) an open-source plugin for the simulator Batsim to simulate users based on real data; (ii) the exploration of four types of user behaviors to curtail the load during a time window, namely delaying, degrading, reconfiguring or renouncing their job submissions. We study the impact of these behaviors on four different metrics: the energy consumed during and after the time window, the mean waiting time and the mean slowdown. We also characterize the conditions under which the involvement of users is the most beneficial. © 2022, Springer Nature Switzerland AG.
DA  - 2022///
PY  - 2022
DO  - 10.1007/978-3-031-12597-3_4
VL  - 13440 LNCS
SP  - 53
EP  - 68
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135769642&doi=10.1007%2f978-3-031-12597-3_4&partnerID=40&md5=2c805d05ac929b7271f8698f27354b7e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 4</p>
KW  - Behavioral research
KW  - Data center
KW  - Datacenter
KW  - Demand response
KW  - Digital technologies
KW  - Job submission
KW  - Parallel workload
KW  - Parallel workloads
KW  - Reproducible research
KW  - Time windows
KW  - User behaviors
KW  - User involvement
KW  - User-aware
ER  - 

TY  - JOUR
T2  - Resampling filters
DA  - 2024///
PY  - 2024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212188539&partnerID=40&md5=607b04b45ab9cab1ee8311194bdaab08
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - Involving users in energy conservation: A case study in scientific clouds
AU  - Guyon, D.
AU  - Orgerie, A.-C.
AU  - Morin, C.
AU  - Agarwal, D.
T2  - International Journal of Grid and Utility Computing
AB  - Services offered by cloud computing are convenient to users for reasons such as their ease of use, flexibility, and financial model. Yet data centres used for their execution are known to consume massive amounts of energy. The growing resource utilisation following the cloud success highlights the importance of the reduction of its energy consumption. This paper investigates a way to reduce the footprint of HPC cloud users by varying the size of the virtual resources they request. We analyse the influence of concurrent applications with different resources sizes on the system energy consumption. Simulation results show that resources with larger size are more energy consuming regardless of faster completion of applications. Although smaller-sized resources offer energy savings, it is not always favourable in terms of energy to reduce too much the size. High energy savings depend on the user profiles’ distribution. Copyright © 2019 Inderscience Enterprises Ltd.
DA  - 2019///
PY  - 2019
DO  - 10.1504/IJGUC.2019.099667
VL  - 10
SP  - 272
EP  - 282
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066116496&doi=10.1504%2fIJGUC.2019.099667&partnerID=40&md5=47a3e60f59319ec76eb555898ca5494e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 8</p>
KW  - Cloud computing
KW  - Data centres
KW  - Energy conservation
KW  - Energy savings
KW  - Energy utilization
KW  - Financial modeling
KW  - Green computing
KW  - High performance computing
KW  - HPC applications
KW  - Resource utilisation
KW  - System energy consumption
KW  - User profile
KW  - Users involvement
KW  - Virtual resource
ER  - 

TY  - JOUR
DA  - 2024///
PY  - 2024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212220117&partnerID=40&md5=62d97aefbd0c41cefc44fc735f82fa6e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
AU  - Gamma, E.
AU  - Helm, R.
AU  - Johnson, R.
AU  - Vlissides, J.
T2  - Design Patterns: Elements of Reusable Object-Oriented Software
DA  - 1995///
PY  - 1995
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003880013&partnerID=40&md5=a6dd1fec98664d3b577c87cb9905d476
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 15714</p>
ER  - 

TY  - JOUR
TI  - Investigating energy consumption and performance trade-off for interactive cloud application
AU  - Hasan, M.S.
AU  - Alvares, F.
AU  - Ledoux, T.
AU  - Pazat, J.-L.
T2  - IEEE Transactions on Sustainable Computing
AB  - With the ever growing demand and popularity of cloud based services, data centers have to urgently face energy consumption issue. Similar to other large consumers of power, data centers find themselves increasingly pressured to reduce their carbon footprint. In response, cloud providers have started to set sustainability goals to reduce carbon emissions by using renewable sources to their services. Traditionally, batch processing cloud applications are deadline oriented, hence they can be easily adapted with the different green energy profile. Whereas, interactive cloud applications are imposed with several performance criteria. This paper, the first of its kind, investigates a thorough analysis of energy consumption and performance trade-off by allowing smart usage of green energy for interactive cloud application. Moreover, we propose an auto-scaler, named SaaScaler, that implements several control loop based application controllers to satisfy different performance (i.e., response time, availability, and user experience) and resource aware metrics (i.e., quality of energy). Based on extensive experiments with RUBiS benchmark and real workload traces using single compute node in Openstack/Grid'5000, results suggest that 13 percent brown energy consumption can be reduced without deprovisioning any physical or virtual resources at IaaS layer while 29 percent more users can access the application by dynamically adjusting capacity requirements. Furthermore, our investigation verifies that the energy consumption deviates as little as.07 percent when our approach is scaled using several physical nodes. © 2016 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/TSUSC.2017.2714959
VL  - 2
IS  - 2
SP  - 113
EP  - 126
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028694906&doi=10.1109%2fTSUSC.2017.2714959&partnerID=40&md5=e7ad009f49f86a7a36283d0a0e834618
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 31</p>
KW  - autonomic computing
KW  - Autonomic Computing
KW  - Batch data processing
KW  - Benchmarking
KW  - Capacity requirement
KW  - Carbon footprint
KW  - Cloud application
KW  - Cloud applications
KW  - Economic and social effects
KW  - energy consumption
KW  - Energy utilization
KW  - Environmental impact
KW  - Green computing
KW  - green IT
KW  - performance analysis
KW  - Performance analysis
KW  - Performance criterion
KW  - Performance trade-off
KW  - Quality control
KW  - Quality of energies
KW  - sustainable computing
KW  - Sustainable computing
ER  - 

TY  - JOUR
TI  - A review of energy efficiency evaluation technologies in cloud data centers
AU  - Long, S.
AU  - Li, Y.
AU  - Huang, J.
AU  - Li, Z.
AU  - Li, Y.
T2  - Energy and Buildings
AB  - The energy consumption by data centers is expanding in tandem with the rapid rise of the digital economy. Data centers, as high-energy-consumption organizations, have garnered extensive attention from society in order to accomplish energy conservation and emission reduction. As a result, improving the energy efficiency of cloud data centers has become a major topic of research. Researchers are working hard to develop practical energy efficiency evaluation methodologies and metrics in order to attain this goal. This article summarizes data center energy efficiency evaluation methods, classifies existing energy efficiency evaluation metrics, examines the current state and challenges of data center energy efficiency evaluation, and makes recommendations for improving energy efficiency evaluation technology to assist cloud operators, decision-makers, and researchers in developing appropriate energy efficiency evaluation strategies. We give data center researchers a better grasp of energy efficiency evaluation and encourage them to combine theory and practice in energy efficiency evaluation and utilize more advanced metrics to assess data center energy efficiency. This is a critical step in the quest for the most advanced green technology, as well as a significant step toward reaching sustainable development goals. © 2022 Elsevier B.V.
DA  - 2022///
PY  - 2022
DO  - 10.1016/j.enbuild.2022.111848
VL  - 260
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125382557&doi=10.1016%2fj.enbuild.2022.111848&partnerID=40&md5=d25a3ad812b00dc2909c72992dfa5997
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 57</p>
KW  - Cloud data centers
KW  - Data center
KW  - Data center energy efficiencies
KW  - Datacenter
KW  - Decision making
KW  - Digital economy
KW  - Emission control
KW  - Energy consumption
KW  - Energy efficiency
KW  - Energy efficiency evaluation
KW  - Energy utilization
KW  - Energy-consumption
KW  - Evaluation
KW  - Evaluation metrics
KW  - Green computing
KW  - High energy consumption
KW  - Metric
KW  - Metrics
ER  - 

TY  - CONF
TI  - SeDuCe: Toward a testbed for research on thermal and power management in datacenters
AU  - Pastor, J.
AU  - Menaud, J.M.
T2  - e-Energy 2018 - Proceedings of the 9th ACM International Conference on Future Energy Systems
AB  - With the advent of Cloud Computing, the size of datacenters is ever increasing and the management of servers and their power consumption and heat production have become challenges. The management of the heat produced by servers has been experimentally less explored than the management of their power consumption. It can be partly explained by the lack of a public testbed that provides reliable access to both thermal and power metrics of server rooms. In this paper we propose SeDuCe, a testbed that targets research on power and thermal management of servers, by providing public access to precise data about the power consumption and the thermal dissipation of 48 servers integrated in Grid’5000 as the new ecotype cluster. We present the chosen software and hardware architecture for the rst version of the SeDuCe testbed, highlighting its current limitation and proposing some improvements that will increase its relevance. © 2018 Copyright held by the owner/author(s).
DA  - 2018///
PY  - 2018
DO  - 10.1145/3208903.3213523
SP  - 513
EP  - 518
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050245662&doi=10.1145%2f3208903.3213523&partnerID=40&md5=c55368b81e22fa331ecea1f1662a794e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2</p>
KW  - Current limitation
KW  - Data centers
KW  - Electric power utilization
KW  - Green computing
KW  - Heat production
KW  - Power management
KW  - Power metrics
KW  - Public Access
KW  - Smart power grids
KW  - Software and hardwares
KW  - Testbeds
KW  - Thermal dissipation
ER  - 

TY  - JOUR
T2  - Ecotype
DA  - 2024///
PY  - 2024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212181198&partnerID=40&md5=cf3338a479a8d6ef61d9165d3e5b9951
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Real time power estimation and thread scheduling via performance counters
AU  - Singh, K.
AU  - Bhadauria, M.
AU  - McKee, S.A.
T2  - SIGARCH Comput. Archit. News
DA  - 2009///
PY  - 2009
VL  - 37
IS  - 2
SP  - 46
EP  - 55
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954733934&partnerID=40&md5=6ede599fd480ff4c4984431a506a6fc3
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 224</p>
ER  - 

TY  - JOUR
TI  - Complete System Power Estimation Using Processor Performance Events
AU  - Bircher, W.L.
AU  - John, L.K.
T2  - IEEE Transactions on Computers
AB  - This paper proposes the use of microprocessor performance counters for online measurement of complete system powe consumption. The approach takes advantage of the “trickle-down” effect of performance events in microprocessors. While it has bee known that CPU power consumption is correlated to processor performance, the use of well-known performance-related events withi a microprocessor such as cache misses and DMA transactions to estimate power consumption in memory and disk and other subsystems outside of the microprocessor is new. Using measurement of actual systems running scientific, commercial and productivity workloads, power models for six subsystems (CPU, memory, chipset, I/O, disk, and GPU) on two platforms (server an desktop) are developed and validated. These models are shown to have an average error of less than nine percent per subsystem across the considered workloads. Through the use of these models and existing on-chip performance event counters, it is possible 1 estimate system power consumption without the need for power sensing hardware. © 2012, IEEE
DA  - 2012///
PY  - 2012
DO  - 10.1109/TC.2011.47
VL  - 61
IS  - 4
SP  - 563
EP  - 577
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008569578&doi=10.1109%2fTC.2011.47&partnerID=40&md5=b0b54bc5d6ee43fe0edaca148f26cb21
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 188</p>
KW  - Energy-aware systems
KW  - evaluation
KW  - measurement
KW  - modeling
KW  - power management
ER  - 

TY  - CONF
TI  - Energy efficiency and server virtualization in data centers: An empirical investigation
AU  - Jin, Y.
AU  - Wen, Y.
AU  - Chen, Q.
T2  - Proceedings - IEEE INFOCOM
AB  - With a growing concern on the considerable energy consumed by data centers, research efforts are targeting toward green data centers with higher energy efficiency. In particular, server virtualization is emerging as the prominent approach to consolidate applications from multiple applications to one server, with an objective to save energy usage. However, little understanding has been obtained about the potential overhead in energy consumption and the throughput reduction for virtualized servers in data centers. In this research, we take the initiative to characterize the energy usage on virtualized servers. An empirical approach is adopted to investigate how server virtualization affects the energy usage in physical servers. Through intensive data collection and analysis, we identify a fundamental trade-off between the energy saving from server consolidation and the detrimental effects (e.g., energy overhead and throughput reduction) from server virtualization. This characterization lays a mathematical foundation for server consolidation in green data center architecture. © 2012 IEEE.
DA  - 2012///
PY  - 2012
DO  - 10.1109/INFCOMW.2012.6193474
SP  - 133
EP  - 138
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862075555&doi=10.1109%2fINFCOMW.2012.6193474&partnerID=40&md5=89c392d0ab77b4eefd6218e1d9d4cfe4
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 82</p>
KW  - Cost reduction
KW  - Data center architecture
KW  - Data centers
KW  - Data collection
KW  - Detrimental effects
KW  - Empirical approach
KW  - Empirical investigation
KW  - Energy efficiency
KW  - Energy usage
KW  - Energy utilization
KW  - Mathematical foundations
KW  - Multiple applications
KW  - Research efforts
KW  - Save energy
KW  - Server consolidation
KW  - Virtual reality
KW  - Virtualizations
ER  - 

TY  - JOUR
TI  - E-AMOM: An energy-aware modeling and optimization methodology for scientific applications
AU  - Lively, C.
AU  - Taylor, V.
AU  - Wu, X.
AU  - Chang, H.-C.
AU  - Su, C.-Y.
AU  - Cameron, K.
AU  - Moore, S.
AU  - Terpstra, D.
T2  - Computer Science - Research and Development
AB  - In this paper, we present the Energy-Aware Modeling and Optimization Methodology (E-AMOM) framework, which develops models of runtime and power consumption based upon performance counters and uses these models to identify energy-based optimizations for scientific applications. E-AMOM utilizes predictive models to employ run-time Dynamic Voltage and Frequency Scaling (DVFS) and Dynamic Concurrency Throttling (DCT) to reduce power consumption of the scientific applications, and uses cache optimizations to further reduce runtime and energy consumption of the applications. The models and optimization are done at the level of the kernels that comprise the application. Our models resulted in an average error rate of at most 6.79 % for Hybrid MPI/OpenMP and MPI implementations of six scientific applications. With respect to optimizations, we were able to reduce the energy consumption by up to 21 %, with a reduction in runtime by up to 14.15 %, and a reduction in power consumption by up to 12.50 %. © 2013 Springer-Verlag Berlin Heidelberg.
DA  - 2014///
PY  - 2014
DO  - 10.1007/s00450-013-0239-3
VL  - 29
IS  - 3-4
SP  - 197
EP  - 210
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904729129&doi=10.1007%2fs00450-013-0239-3&partnerID=40&md5=361553ed0a0550f132e7207fdff05edf
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 23</p>
KW  - Dynamic frequency scaling
KW  - Electric power utilization
KW  - Embedded systems
KW  - Energy aware
KW  - Energy consumption
KW  - Energy-consumption
KW  - Hybrid MPI/openmp
KW  - Hybrid MPI/OpenMP
KW  - Modeling and optimization
KW  - Modeling methodology
KW  - MPI
KW  - Optimization methodology
KW  - Performance modeling
KW  - Performance Modeling
KW  - Performance optimization
KW  - Performance optimizations
KW  - Power consumption
KW  - Power prediction
KW  - Power predictions
KW  - Voltage scaling
ER  - 

TY  - CONF
TI  - Runtime power monitoring in high-end processors: Methodology and empirical data
AU  - Isci, C.
AU  - Martonosi, M.
T2  - Proceedings of the Annual International Symposium on Microarchitecture, MICRO
AB  - With power dissipation becoming an increasingly vexing problem across many classes of computer systems, measuring power dissipation of real, running systems has become crucial for hardware and software system research and design. Live power measurements are imperative for studies requiring execution times too long for simulation, such as thermal analysis. Furthermore, as processors become more complex and include a host of aggressive dynamic power management techniques, per-component estimates of power dissipation have become both more challenging as well as more important. In this paper we describe our technique for a coordinated measurement approach that combines real total power measurement with performance-counter-based, per-unit power estimation. The resulting tool offers live total power measurements for Intel Pentium 4 processors, and also provides power breakdowns for 22 of the major CPU subunits over minutes of SPEC2000 and desktop workload execution. As an example application, we use the generated component power breakdowns to identify program power phase behaviour. Overall, this paper demonstrates a processor power measurement and estimation methodology and also gives experiences and empirical application results that can provide a basis for future power-aware research. © 2003 IEEE.
DA  - 2003///
PY  - 2003
DO  - 10.1109/MICRO.2003.1253186
VL  - 2003-January
SP  - 93
EP  - 104
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944414165&doi=10.1109%2fMICRO.2003.1253186&partnerID=40&md5=99f5cf447bd12c523bf066899b63992d
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 449</p>
KW  - Analytical models
KW  - Application programs
KW  - Computational model
KW  - Computational modeling
KW  - Computer architecture
KW  - Computer hardware
KW  - Computer software
KW  - Computerized monitoring
KW  - Dynamic power management
KW  - Electric breakdown
KW  - Electric losses
KW  - Electric power measurement
KW  - Electric power system measurement
KW  - Energy dissipation
KW  - Estimation methodologies
KW  - Hardware
KW  - Hardware and software
KW  - Microprocessor chips
KW  - Pentium 4 Processors
KW  - Power dissipation
KW  - Power management
KW  - Power measurement
KW  - Runtime
KW  - Runtimes
KW  - Software engineering
KW  - Software measurement
KW  - Software Measurement
KW  - Software systems
KW  - Thermoanalysis
ER  - 

TY  - CONF
TI  - A simplified and accurate model of power-performance efficiency on emergent GPU architectures
AU  - Song, S.
AU  - Su, C.
AU  - Rountree, B.
AU  - Cameron, K.W.
T2  - Proceedings - IEEE 27th International Parallel and Distributed Processing Symposium, IPDPS 2013
AB  - Emergent heterogeneous systems must be optimized for both power and performance at exascale. Massive parallelism combined with complex memory hierarchies form a barrier to efficient application and architecture design. These challenges are exacerbated with GPUs as parallelism increases orders of magnitude and power consumption can easily double. Models have been proposed to isolate power and performance bottlenecks and identify their root causes. However, no current models combine simplicity, accuracy, and support for emergent GPU architectures (e.g. NVIDIA Fermi). We combine hardware performance counter data with machine learning and advanced analytics to model power-performance efficiency for modern GPU-based systems. Our performance counter based approach is simpler than previous approaches and does not require detailed understanding of the underlying architecture. The resulting model is accurate for predicting power (within 2.1%) and performance (within 6.7%) for application kernels on modern GPUs. Our model can identify power-performance bottlenecks and their root causes for various complex computation and memory access patterns (e.g. global, shared, texture). We measure the accuracy of our power and performance models on a NVIDIA Fermi C2075 GPU for more than a dozen CUDA applications. We show our power model is more accurate and robust than the best available GPU power models - multiple linear regression models MLR and MLR+. We demonstrate how to use our models to identify power-performance bottlenecks and suggest optimization strategies for high-performance codes such as GEM, a biomolecular electrostatic analysis application. We verify our power-performance model is accurate on clusters of NVIDIA Fermi M2090s and useful for suggesting optimal runtime configurations on the Keeneland supercomputer at Georgia Tech. © 2013 IEEE.
DA  - 2013///
PY  - 2013
DO  - 10.1109/IPDPS.2013.73
SP  - 673
EP  - 686
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884823629&doi=10.1109%2fIPDPS.2013.73&partnerID=40&md5=2071e2ab79d99755c974c84584a17062
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 134</p>
KW  - Biomolecular electrostatics
KW  - Distributed parameter networks
KW  - Hardware performance counters
KW  - Heterogeneous systems
KW  - Linear regression
KW  - Memory access patterns
KW  - Multiple linear regression models
KW  - Optimization
KW  - Performance bottlenecks
KW  - Power-performance efficiency
KW  - Program processors
KW  - Run-time configuration
KW  - Supercomputers
ER  - 

TY  - CONF
TI  - Component-level energy consumption estimation for distributed java-based software systems
AU  - Seo, C.
AU  - Malek, S.
AU  - Medvidovic, N.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Efficiency with respect to energy consumption has increasingly been recognized as an important quality attribute for distributed software systems in embedded and pervasive environments. In this paper we present a framework for estimating the energy consumption of distributed software systems implemented in Java. Our primary objective in devising the framework is to enable an engineer to make informed decisions when adapting a system's architecture, such that the energy consumption on hardware devices with a finite battery life is reduced, and the lifetime of the system's key software services increases. Our framework explic itly takes a component-based perspective, which renders it well suited for a large class of today's distributed, embedded, and pervasive applications. The framework allows the engineer to estimate the distributed system's energy consumption at sys tem construction-time and refine it at runtime. In a large number of distributed application scenarios, the framework showed very good precision on the whole, giving results that were within 5% (and often less) of the actual energy consump tion incurred by executing the software. Our work to date has also highlighted the framework's practical applications and a number of possible enhancements. © 2008 Springer.
DA  - 2008///
PY  - 2008
DO  - 10.1007/978-3-540-87891-9_7
VL  - 5282 LNCS
SP  - 97
EP  - 113
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350536310&doi=10.1007%2f978-3-540-87891-9_7&partnerID=40&md5=a0fe76b07b2ce1c8c458dab3cacb0951
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 35</p>
KW  - Application programs
KW  - Component based software
KW  - Component-based software
KW  - Distributed applications
KW  - Distributed computer systems
KW  - Distributed database systems
KW  - Distributed software system
KW  - Distributed systems
KW  - Embedded systems
KW  - Energy consumption
KW  - Energy utilization
KW  - Java
KW  - Java programming language
KW  - Pervasive applications
KW  - Pervasive environments
KW  - Quality attributes
ER  - 

TY  - CONF
TI  - Impact of developer choices on energy consumption of software on servers
AU  - Singh, J.
AU  - Naik, K.
AU  - Mahinthan, V.
T2  - Procedia Computer Science
AB  - The power cost of running a data center is a significant portion of its total annual operating budget. With the aim of reducing power bills of data centers, "Green Computing" has emerged with the primary goal of making software more energy efficient without compromising the performance. Developers play an important role in controlling the energy cost of data center software while writing code. In this paper, we show how software developers can contribute to energy efficiency of servers by choosing energy efficient APIs (Application Programming Interface) with the optimal choice of parameters while implementing file reading, file copy, file compression and file decompression operations in Java; that are performed extensively on large scale servers in data centers. We performed extensive measurements of energy cost of those operations on a Dell Power Edge 2950 machine running Linux and Windows servers. Measurement results show that energy costs of various APIs for those operations are sensitive to the buffer size selection. The choice of a particular Java API for file reading with different buffer sizes has significant impact on the energy cost, giving an opportunity to save up to 76%. To save energy while copying files, it is important to use APIs with tunable buffer sizes, rather than APIs using fixed size buffers. In addition, there is a trade off between compression ratio and energy cost: because of more compression ratio, xz compression API consumes more energy than zip and gzip compression APIs. Finally, we model the energy costs of APIs by polynomial regression to avoid repeated measurements. © 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license.
DA  - 2015///
PY  - 2015
DO  - 10.1016/j.procs.2015.08.423
VL  - 62
SP  - 385
EP  - 394
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962609274&doi=10.1016%2fj.procs.2015.08.423&partnerID=40&md5=b387e232bd2ea964415da1d5a56b88a9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 16</p>
KW  - Application programming interfaces (API)
KW  - Application programs
KW  - Budget control
KW  - Computer operating systems
KW  - Computer software
KW  - Costs
KW  - Data centers
KW  - Developer choices
KW  - Economic and social effects
KW  - Energy efficiency
KW  - Energy efficient
KW  - Energy efficient software
KW  - Energy performance evaluation
KW  - Energy performance evaluations
KW  - Energy utilization
KW  - Ion beams
KW  - Java programming language
KW  - Operating budgets
KW  - Polynomial regression
KW  - Repeated measurements
KW  - Soft computing
KW  - Software developer
KW  - Software engineering
KW  - Windows operating system
ER  - 

TY  - JOUR
TI  - The case for energy-proportional computing
AU  - Barroso, L.A.
AU  - Hölzle, U.
T2  - Computer
AB  - Energy-proportional designs would enable large energy savings in servers, potentially doubling their efficiency in real-life use. Achieving energy-proportionality will require significant improvements in the energy usage profile of every system component, particularly the memory and disk subsystems. © 2007 IEEE.
DA  - 2007///
PY  - 2007
DO  - 10.1109/MC.2007.443
VL  - 40
IS  - 12
SP  - 33
EP  - 37
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-47249127725&doi=10.1109%2fMC.2007.443&partnerID=40&md5=402a0a6da009e78f271427bb0569ba91
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 2024</p>
KW  - Energy-proportional computing
KW  - Green computing
ER  - 

TY  - BOOK
TI  - Green in software engineering
AU  - Calero, C.
AU  - Piattini, M.
T2  - Green in Software Engineering
AB  - This is the first book that presents a comprehensive overview of sustainability aspects in software engineering. Its format follows the structure of the SWEBOK and covers the key areas involved in the incorporation of green aspects in software engineering, encompassing topics from requirement elicitation to quality assurance and maintenance, while also considering professional practices and economic aspects. The book consists of thirteen chapters, which are structured in five parts. First the “Introduction” gives an overview of the primary general concepts related to Green IT, discussing what Green in Software Engineering is and how it differs from Green by Software Engineering. Next "Environments, Processes and Construction" presents green software development environments, green software engineering processes and green software construction in general. The third part, “Economic and Other Qualities,” details models for measuring how well software supports green software engineering techniques and for performing trade-off analyses between alternative green practices from an economic perspective. "Software Development Process" then details techniques for incorporating green aspects at various stages of software development, including requirements engineering, design, testing, and maintenance. In closing, “Practical Issues” addresses the repercussions of green software engineering on decision-making, stakeholder participation and innovation management. The audience for this book includes software engineering researchers in academia and industry seeking to understand the challenges and impact of green aspects in software engineering, as well as practitioners interested in learning about the state of the art in Green in Software Engineering. © Springer International Publishing Switzerland 2015.
DA  - 2015///
PY  - 2015
SP  - 1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944526351&doi=10.1007%2f978-3-319-08581-4&partnerID=40&md5=4bda434062430f89a16e131e8102074f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 77</p>
KW  - Computer software maintenance
KW  - Decision making
KW  - Economic and social effects
KW  - Engineering education
KW  - Engineering techniques
KW  - Professional aspects
KW  - Professional practices
KW  - Quality assurance
KW  - Requirement elicitation
KW  - Requirements engineering
KW  - Software construction
KW  - Software design
KW  - Software development environment
KW  - Software development process
KW  - Software engineering
KW  - Software engineering process
KW  - Software testing
KW  - Stakeholder participation
ER  - 

TY  - CONF
TI  - Power provisioning for a warehouse-sized computer
AU  - Fan, X.
AU  - Weber, W.-D.
AU  - Barroso, L.A.
T2  - Proceedings - International Symposium on Computer Architecture
AB  - Large-scale Internet services require a computing infrastructure that can beappropriately described as a warehouse-sized computing system. The cost ofbuilding datacenter facilities capable of delivering a given power capacity tosuch a computer can rival the recurring energy consumption costs themselves.Therefore, there are strong economic incentives to operate facilities as closeas possible to maximum capacity, so that the non-recurring facility costs canbe best amortized. That is difficult to achieve in practice because ofuncertainties in equipment power ratings and because power consumption tends tovary significantly with the actual computing activity. Effective powerprovisioning strategies are needed to determine how much computing equipmentcan be safely and efficiently hosted within a given power budget. In this paper we present the aggregate power usage characteristics of largecollections of servers (up to 15 thousand) for different classes ofapplications over a period of approximately six months. Those observationsallow us to evaluate opportunities for maximizing the use of the deployed powercapacity of datacenters, and assess the risks of over-subscribing it. We findthat even in well-tuned applications there is a noticeable gap (7 - 16%)between achieved and theoretical aggregate peak power usage at the clusterlevel (thousands of servers). The gap grows to almost 40% in wholedatacenters. This headroom can be used to deploy additional compute equipmentwithin the same power budget with minimal risk of exceeding it. We use ourmodeling framework to estimate the potential of power management schemes toreduce peak power and energy usage. We find that the opportunities for powerand energy savings are significant, but greater at the cluster-level (thousandsof servers) than at the rack-level (tens). Finally we argue that systems needto be power efficient across the activity range, and not only at peakperformance levels. Copyright 2007 ACM.
DA  - 2007///
PY  - 2007
DO  - 10.1145/1250662.1250665
SP  - 13
EP  - 23
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-35348835964&doi=10.1145%2f1250662.1250665&partnerID=40&md5=0ba10bc53962f2321e9062dec64e3177
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1697</p>
KW  - Electric power utilization
KW  - Energy efficiency
KW  - Internet
KW  - Mathematical models
KW  - Power modeling
KW  - Power provisioning
KW  - Telecommunication services
KW  - Warehouses
ER  - 

TY  - JOUR
T2  - IntelR 64 and IA-32 Architectures Software Developer's Manual
DA  - 2018///
PY  - 2018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956506076&partnerID=40&md5=44d6a3448690ccb63f34cea1126ce4bd
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 153</p>
ER  - 

TY  - JOUR
TI  - Measuring application software energy efficiency
AU  - Capra, E.
AU  - Francalanci, C.
AU  - Slaughter, S.A.
T2  - IT Professional
AB  - Researchers have studied the energy efficiency of hardware, but what about application software? Using an experimental approach, the authors show how applications affect total energy consumption and discuss design factors that could influence software energy efficiency. © 2006 IEEE.
DA  - 2012///
PY  - 2012
DO  - 10.1109/MITP.2012.39
VL  - 14
IS  - 2
SP  - 54
EP  - 61
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859168181&doi=10.1109%2fMITP.2012.39&partnerID=40&md5=bbb95e5c1bbd02331ef1647aac5524d3
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 39</p>
KW  - application development environment
KW  - Application development environment
KW  - Application softwares
KW  - Computer software
KW  - Design factors
KW  - Energy efficiency
KW  - Energy utilization
KW  - Experimental approaches
KW  - green IT
KW  - Green IT
KW  - information technology
KW  - Information technology
KW  - Measuring applications
KW  - software design
KW  - Software design
KW  - software energy efficiency
KW  - Total energy consumption
ER  - 

TY  - JOUR
AU  - Ad Anton Beloglazov, R.B.
AU  - Abawajy, J.H.
T2  - Energy-efficient Management of Data Center Resources for Cloud Computing: A Vision, Architectural Elements, Open Challenges
DA  - 2010///
PY  - 2010
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073221577&partnerID=40&md5=676b5f08192f5c4c6893493b6fa83d15
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - JOUR
TI  - Power, pollution and the internet
AU  - Glanz, J.
T2  - Power, Pollution and the Internet
DA  - 2012///
PY  - 2012
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874546725&partnerID=40&md5=e2f609209af82f944ff852005f1c111e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 60</p>
ER  - 

TY  - JOUR
TI  - Exploiting performance counters to predict and improve energy performance of HPC systems
AU  - Tsafack Chetsa, G.L.
AU  - Lefèvre, L.
AU  - Pierson, J.M.
AU  - Stolf, P.
AU  - Da Costa, G.
T2  - Future Generation Computer Systems
AB  - Hardware monitoring through performance counters is available on almost all modern processors. Although these counters are originally designed for performance tuning, they have also been used for evaluating power consumption. We propose two approaches for modelling and understanding the behaviour of high performance computing (HPC) systems relying on hardware monitoring counters. We evaluate the effectiveness of our system modelling approach considering both optimizing the energy usage of HPC systems and predicting HPC applications' energy consumption as target objectives. Although hardware monitoring counters are used for modelling the system, other methods-including partial phase recognition and cross platform energy prediction-are used for energy optimization and prediction. Experimental results for energy prediction demonstrate that we can accurately predict the peak energy consumption of an application on a target platform; whereas, results for energy optimization indicate that with no a priori knowledge of workloads sharing the platform we can save up to 24% of the overall HPC system's energy consumption under benchmarks and real-life workloads. © 2013 Elsevier B.V. All rights reserved.
DA  - 2014///
PY  - 2014
DO  - 10.1016/j.future.2013.07.010
VL  - 36
SP  - 287
EP  - 298
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899622479&doi=10.1016%2fj.future.2013.07.010&partnerID=40&md5=fbee48a53e2626db15c3679abfd67cf7
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 34</p>
KW  - Benchmarking
KW  - Electric load forecasting
KW  - Electric power utilization
KW  - Energy efficiency
KW  - Energy optimization
KW  - Energy performance
KW  - Energy utilization
KW  - Forecasting
KW  - Green IT
KW  - Hardware
KW  - Hardware performance counters
KW  - High performance computing
KW  - High performance computing systems
KW  - Optimization
KW  - Performance counters
KW  - Performance tuning
KW  - Power consumption
ER  - 

TY  - CONF
TI  - Accurate online power estimation and automatic battery behavior based power model generation for smartphones
AU  - Zhang, L.
AU  - Tiwana, B.
AU  - Qian, Z.
AU  - Wang, Z.
AU  - Dick, R.P.
AU  - Mao, Z.M.
AU  - Yang, L.
T2  - Embedded Systems Week 2010 - Proceedings of the 8th IEEE/ACM/IFIP International Conference on Compilers, Architecture and Synthesis for Embedded Systems, CODES+ISSS'2010
AB  - This paper describes PowerBooter, an automated power model construction technique that uses built-in battery voltage sensors and knowledge of battery discharge behavior to monitor power consumption while explicitly controlling the power management and activity states of individual components. It requires no external measurement equipment. We also describe PowerTutor, a component power management and activity state introspection based tool that uses the model generated by PowerBooter for online power estimation. PowerBooter is intended to make it quick and easy for application developers and end users to generate power models for new smartphone variants, which each have different power consumption properties and therefore require different power models. PowerTutor is intended to ease the design and selection of power efficient software for embedded systems. Combined, PowerBooter and PowerTutor have the goal of opening power modeling and analysis for more smartphone variants and their users.
DA  - 2010///
PY  - 2010
DO  - 10.1145/1878961.1878982
SP  - 105
EP  - 114
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650632079&doi=10.1145%2f1878961.1878982&partnerID=40&md5=b2a1cafc966a30341aee60665a3db9e7
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 946</p>
KW  - Application developers
KW  - battery
KW  - Battery
KW  - Battery discharge
KW  - Battery voltages
KW  - Behavior-based
KW  - Behavioral research
KW  - Embedded software
KW  - Embedded systems
KW  - End users
KW  - Energy management
KW  - Human computer interaction
KW  - Individual components
KW  - Measurement equipment
KW  - Mobile devices
KW  - Mobile phones
KW  - Power Consumption
KW  - Power efficient
KW  - Power estimations
KW  - Power managements
KW  - Power model
KW  - Power modeling
KW  - Signal encoding
KW  - Smart phones
KW  - Telecommunication equipment
KW  - Telephone
KW  - Telephone sets
ER  - 

TY  - JOUR
TI  - Mobile code offloading: From concept to practice and beyond
AU  - Flores, H.
AU  - Hui, P.
AU  - Tarkoma, S.
AU  - Li, Y.
AU  - Srirama, S.
AU  - Buyya, R.
T2  - IEEE Communications Magazine
AB  - The emerging mobile cloud has expanded the horizon of application development and deployment with techniques such as code offloading. While offloading has been widely considered for saving energy and increasing responsiveness of mobile devices, the technique still faces many challenges pertaining to practical usage. In this article, we adopt a systemic approach for analyzing the components of a generic code offloading architecture. Based on theoretical and experimental analysis, we identify the key limitations for code offloading in practice and then propose solutions to mitigate these limitations. We develop a generic architecture to evaluate the proposed solutions. The results provide insights regarding the evolution and deployment of code offloading. © 2015 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/MCOM.2015.7060486
VL  - 53
IS  - 3
SP  - 80
EP  - 88
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925843954&doi=10.1109%2fMCOM.2015.7060486&partnerID=40&md5=05ceca4e6ad341560d992bc838aab01a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 159</p>
KW  - Application development
KW  - Codes (symbols)
KW  - Experimental analysis
KW  - Generic architecture
KW  - Generic codes
KW  - Mobile clouds
KW  - Mobile codes
KW  - Mobile devices
KW  - Saving energy
KW  - Systemic approach
ER  - 

TY  - CONF
TI  - Calculating source line level energy information for Android applications
AU  - Li, D.
AU  - Hao, S.
AU  - Halfond, W.G.J.
AU  - Govindan, R.
T2  - 2013 International Symposium on Software Testing and Analysis, ISSTA 2013 - Proceedings
AB  - The popularity of mobile apps continues to grow as developers take advantage of the sensors and data available on mobile devices. However, the increased functionality comes with a higher energy cost, which can cause a problem for users on battery constrained mobile devices. To improve the energy consumption of mobile apps, developers need detailed information about the energy consumption of their applications. Existing techniques have drawbacks that limit their usefulness or provide information at too high of a level of granularity, such as components or methods. Our approach is able to calculate source line level energy consumption information. It does this by combining hardware-based power measurements with program analysis and statistical modeling. Our empirical evaluation of the approach shows that it is fast and accurate. © 2013 ACM.
DA  - 2013///
PY  - 2013
DO  - 10.1145/2483760.2483780
SP  - 78
EP  - 89
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881289459&doi=10.1145%2f2483760.2483780&partnerID=40&md5=14c81c262e9c760849491698a43c893f
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 175</p>
KW  - Android app
KW  - Android applications
KW  - Electric power measurement
KW  - Empirical evaluations
KW  - Energy cost
KW  - Energy information
KW  - Energy measurement
KW  - Energy utilization
KW  - Mobile devices
KW  - Program analysis
KW  - Software testing
KW  - Source line level
KW  - Statistical modeling
ER  - 

TY  - CONF
TI  - Utilizing hardware performance counters to model and optimize the energy and performance of large scale scientific applications on power-aware supercomputers
AU  - Wu, X.
AU  - Taylor, V.
T2  - Proceedings - 2016 IEEE 30th International Parallel and Distributed Processing Symposium, IPDPS 2016
AB  - Hardware performance counters are used as effective proxies to estimate power consumption and runtime. In this paper we present a performance counter-based power and performance modeling and optimization method, and use the method to model four metrics: runtime, system power, CPU power and memory power. The performance counters that compose the models are used to explore some counter-guided optimizations with two large-scale scientific applications: an earthquake simulation and an aerospace application. We demonstrate the use of the method using two power-aware supercomputers, Mira at Argonne National Laboratory and SystemG at Virginia Tech. The counter-guided optimizations result in a reduction in energy by an average of 18.28% on up to 32,768 cores on Mira and 11.28% on up to 128 cores on SystemG for the aerospace application. For the earthquake simulation, the average energy reductions achieved are 48.65% on up to 4,096 cores on Mira and 30.67% on up to 256 cores on SystemG. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/IPDPSW.2016.78
SP  - 1180
EP  - 1189
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991693802&doi=10.1109%2fIPDPSW.2016.78&partnerID=40&md5=dc13a7c94f118199d6a28e0956e8a96b
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 14</p>
KW  - Aerospace applications
KW  - Argonne National Laboratory
KW  - Counter-guided optimization
KW  - Earthquake engineering
KW  - Earthquake simulation
KW  - Earthquakes
KW  - Geophysics
KW  - Hardware
KW  - Hardware performance counters
KW  - Performance counters
KW  - Performance Model
KW  - Performance modeling
KW  - Power management
KW  - Power model
KW  - Power modeling
KW  - Power-aware
KW  - Power-aware supercomputers
KW  - Reconfigurable hardware
KW  - Scientific applications
KW  - Supercomputers
ER  - 

TY  - JOUR
TI  - The internet of fewer things [News]
AU  - Nordrum, A.
T2  - IEEE Spectrum
AB  - If you follow discussions about the Internet of Things, you've probably heard this stunning prediction at least once: The world will have 50 billion Internet-connected devices by 2020. Ericsson's former CEO, Hans Vestburg, was among the first to toss out that number, when he gave a 2010 presentation to shareholders. The following year, Dave Evans, who worked for Cisco at the time, published the same prediction in a white paper. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/MSPEC.2016.7572524
VL  - 53
IS  - 10
SP  - 12
EP  - 13
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991205682&doi=10.1109%2fMSPEC.2016.7572524&partnerID=40&md5=72f8e91eb18439c3aa0fe979e2ca33be
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 75</p>
KW  - Electrical engineering
KW  - Ericsson
KW  - Internet
KW  - Technology
KW  - White papers
ER  - 

TY  - JOUR
TI  - Running average power limit-rapl
AU  - Pandruvada, S.
T2  - Running Average Power Limit
DA  - 2014///
PY  - 2014
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009149645&partnerID=40&md5=5c9c804e9f3f35ba6132a3022fa70d78
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 31</p>
ER  - 

TY  - JOUR
TI  - Continuous monitoring of software services: Design and application of the kieker framework Kiel University
AU  - Van Hoorn, A.
AU  - Rohr, M.
AU  - Hasselbring, W.
AU  - Waller, J.
AU  - Ehlers, J.
AU  - Frey, S.
AU  - Kieselhorst, D.
T2  - Forschungsbericht
DA  - 2009///
PY  - 2009
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073249021&partnerID=40&md5=0f4c475908ece05ea7f8d3967c675ad8
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 1</p>
ER  - 

TY  - CONF
TI  - A Top-Down method for performance analysis and counters architecture
AU  - Yasin, A.
T2  - ISPASS 2014 - IEEE International Symposium on Performance Analysis of Systems and Software
AB  - Optimizing an application's performance for a given microarchitecture has become painfully difficult. Increasing microarchitecture complexity, workload diversity, and the unmanageable volume of data produced by performance tools increase the optimization challenges. At the same time resource and time constraints get tougher with recently emerged segments. This further calls for accurate and prompt analysis methods. In this paper a Top-Down Analysis is developed - a practical method to quickly identify true bottlenecks in out-of-order processors. The developed method uses designated performance counters in a structured hierarchical approach to quickly and, more importantly, correctly identify dominant performance bottlenecks. The developed method is adopted by multiple in-production tools including VTune. Feedback from VTune average users suggests that the analysis is made easier thanks to the simplified hierarchy which avoids the high-learning curve associated with microarchitecture details. Characterization results of this method are reported for the SPEC CPU2006 benchmarks as well as key enterprise workloads. Field case studies where the method guides software optimization are included, in addition to architectural exploration study for most recent generations of Intel Core™ products. The insights from this method guide a proposal for a novel performance counters architecture that can determine the true bottlenecks of a general out-of-order processor. Unlike other approaches, our analysis method is low-cost and already featured in in-production systems - it requires just eight simple new performance events to be added to a traditional PMU. It is comprehensive - no restriction to predefined set of performance issues. It accounts for granular bottlenecks in super-scalar cores, missed by earlier approaches. © 2014 IEEE.
DA  - 2014///
PY  - 2014
DO  - 10.1109/ISPASS.2014.6844459
SP  - 35
EP  - 44
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904497623&doi=10.1109%2fISPASS.2014.6844459&partnerID=40&md5=324610a9db40b3b289d029e4e71abe22
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 275</p>
KW  - Computer architecture
KW  - Hierarchical approach
KW  - Micro architectures
KW  - Out-of-order processors
KW  - Performance analysis
KW  - Performance bottlenecks
KW  - Performance counters
KW  - Software optimization
KW  - Workload diversities
ER  - 

TY  - CONF
TI  - Accuracy of performance counter measurements
AU  - Zaparanuks, D.
AU  - Jovic, M.
AU  - Hauswirth, M.
T2  - ISPASS 2009 - International Symposium on Performance Analysis of Systems and Software
AB  - Many experimental performance evaluations depend on accurate measurements of the cost of executing a piece of code. Often these measurements are conducted using infrastructures to access hardware performance counters. Most modern processors provide such counters to count microarchitectural events such as retired instructions or clock cycles. These counters can be difficult to configure, may not be programmable or readable from user-level code, and can not discriminate between events caused by different software threads. Various software infrastructures address this problem, providing access to per-thread counters from application code. This paper constitutes the first comparative study of the accuracy of three commonly used measurement infrastructures (perfctr, perfmon2, and PAPI) on three common processors (Pentium D, Core 2 Duo, and AMD ATHLON 64 ×2). © 2009 IEEE.
DA  - 2009///
PY  - 2009
DO  - 10.1109/ISPASS.2009.4919635
SP  - 23
EP  - 32
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349170794&doi=10.1109%2fISPASS.2009.4919635&partnerID=40&md5=493fe0ec70461b798a5906edffc39ab2
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 54</p>
KW  - Accurate measurement
KW  - Application codes
KW  - Athlon 64
KW  - Clock cycles
KW  - Comparative studies
KW  - Computer hardware
KW  - Computer software
KW  - Experimental performance evaluations
KW  - Hardware performance counters
KW  - Modern processors
KW  - Pentium
KW  - Performance counters
KW  - Software infrastructure
KW  - Software threads
KW  - User-level codes
ER  - 

TY  - CONF
TI  - RAPL: Memory power estimation and capping
AU  - David, H.
AU  - Gorbatov, E.
AU  - Hanebutte, U.R.
AU  - Khanna, R.
AU  - Le, C.
T2  - Proceedings of the International Symposium on Low Power Electronics and Design
AB  - The drive for higher performance and energy efficiency in data-centers has influenced trends toward increased power and cooling requirements in the facilities. Since enterprise servers rarely operate at their peak capacity, efficient power capping is deemed as a critical component of modern enterprise computing environments. In this paper we propose a new power measurement and power limiting architecture for main memory. Specifically, we describe a new approach for measuring memory power and demonstrate its applicability to a novel power limiting algorithm. We implement and evaluate our approach in the modern servers and show that we achieve up to 40% lower performance impact when compared to the stateof- art baseline across the power limiting range. Copyright 2010 ACM.
DA  - 2010///
PY  - 2010
DO  - 10.1145/1840845.1840883
SP  - 189
EP  - 194
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957942221&doi=10.1145%2f1840845.1840883&partnerID=40&md5=335285081760d0cd206366592a9687b9
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 501</p>
KW  - Critical component
KW  - Data centers
KW  - Design
KW  - Efficient power
KW  - Energy efficiency
KW  - Enterprise computing environment
KW  - Enterprise servers
KW  - Experimentation
KW  - Main memory
KW  - Measurement
KW  - New approaches
KW  - Peak capacity
KW  - Performance
KW  - Performance impact
KW  - Power electronics
KW  - Power estimations
KW  - Power limiting
KW  - Power measurement
ER  - 

TY  - CONF
TI  - Performance and power modeling in a multi-programmed multi-core environment
AU  - Chen, X.
AU  - Xu, C.
AU  - Dick, R.P.
AU  - Mao, Z.M.
T2  - Proceedings - Design Automation Conference
AB  - This paper describes a fast, automated technique for accurate on-line estimation of the performance and power consumption of interacting processes in a multi-programmed, multi-core environment. The proposed technique does not require modifying hardware or applications. The performance model uses reuse distance histograms, cache access frequencies, and the relationship between the throughput and cache miss rate of each process to predict throughput. The system-level power model is derived using multi-variable linear regression, accounting for cache contention. Both models are validated on multiple real multi-core systems using SPEC CPU2000 benchmarks; their performance and power estimates are within 3.5% of measured values on average. We explain how to integrate the two models for power estimation during process assignment, helpful for power-aware assignment. Copyright 2010 ACM.
DA  - 2010///
PY  - 2010
DO  - 10.1145/1837274.1837479
SP  - 813
EP  - 818
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956210093&doi=10.1145%2f1837274.1837479&partnerID=40&md5=2fdb1f99d1e78194d838f0f4790992a8
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 41</p>
KW  - Assignment
KW  - Automated techniques
KW  - Automation
KW  - Cache access
KW  - Cache miss rates
KW  - Computer aided design
KW  - Electric power utilization
KW  - Interacting process
KW  - Microprocessor chips
KW  - Multi core
KW  - Multi-core systems
KW  - Multi-variable linear regression
KW  - On-line estimation
KW  - Performance Model
KW  - Performance modeling
KW  - Power Consumption
KW  - Power estimations
KW  - Power model
KW  - Power modeling
KW  - Power-aware
KW  - Process assignment
KW  - Reuse distance
KW  - System levels
ER  - 

TY  - CONF
TI  - TeaStore: A micro-service reference application for benchmarking, modeling and resource management research
AU  - Von Kistowski, J.
AU  - Eismann, S.
AU  - Schmitt, N.
AU  - Bauer, A.
AU  - Grohmann, J.
AU  - Kounev, S.
T2  - Proceedings - 26th IEEE International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems, MASCOTS 2018
AB  - Modern distributed applications offer complex performance behavior and many degrees of freedom regarding deployment and configuration. Researchers employ various methods of analysis, modeling, and management that leverage these degrees of freedom to predict or improve non-functional properties of the software under consideration. In order to demonstrate and evaluate their applicability in the real world, methods resulting from such research areas require test and reference applications that offer a range of different behaviors, as well as the necessary degrees of freedom. Existing production software is often inaccessible for researchers or closed off to instrumentation. Existing testing and benchmarking frameworks, on the other hand, are either designed for specific testing scenarios, or they do not offer the necessary degrees of freedom. Further, most test applications are difficult to deploy and run, or are outdated. In this paper, we introduce the TeaStore, a state-of-The-Art micro-service-based test and reference application. TeaStore offers services with different performance characteristics and many degrees of freedom regarding deployment and configuration to be used as a benchmarking framework for researchers. The TeaStore allows evaluating performance modeling and resource management techniques; it also offers instrumented variants to enable extensive run-Time analysis. We demonstrate TeaStore's use in three contexts: performance modeling, cloud resource management, and energy efficiency analysis. Our experiments show that TeaStore can be used for evaluating novel approaches in these contexts and also motivates further research in the areas of performance modeling and resource management. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/MASCOTS.2018.00030
SP  - 223
EP  - 236
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058299875&doi=10.1109%2fMASCOTS.2018.00030&partnerID=40&md5=b9ee5510d42b5ac1ead035c7c172885c
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 124</p>
KW  - Auto Scaler
KW  - Benchmarking
KW  - Cloud
KW  - Clouds
KW  - Container
KW  - Containers
KW  - Degrees of freedom (mechanics)
KW  - Deployment and configuration
KW  - Energy efficiency
KW  - Energy Efficiency
KW  - Energy efficiency analysis
KW  - Green computing
KW  - Microservice
KW  - Models
KW  - Natural resources management
KW  - Performance
KW  - Performance characteristics
KW  - Power
KW  - Resource allocation
KW  - Resource management techniques
ER  - 

TY  - CONF
TI  - Power budgeting for virtualized data centers
AU  - Lim, H.
AU  - Kansal, A.
AU  - Liu, J.
T2  - Proceedings of the 2011 USENIX Annual Technical Conference, USENIX ATC 2011
AB  - Power costs are very significant for data centers. To maximally utilize the provisioned power capacity, data centers often employ over-subscription, that is, the sum of peak consumptions of individual servers may be greater than the provisioned capacity. Power budgeting methods are employed to ensure that actual consumption never exceeds capacity. However, current power budgeting methods enforce capacity limits in hardware and are not well suited for virtualized servers because the hardware is shared among multiple applications. We present a power budgeting system for virtualized infrastructures that enforces power limits on individual distributed applications. Our system enables multiple applications to share the same servers but operate with their individual quality of service guarantees. It responds to workload and power availability changes, by dynamically allocating appropriate amount of power to different applications and tiers within applications. The design is mindful of practical constraints such the data center's limited visibility into hosted application performance. We evaluate the system using workloads derived from real world data center traces.
DA  - 2019///
PY  - 2019
SP  - 59
EP  - 72
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077062938&partnerID=40&md5=67c8e58f613f0d894bcb843d0257e605
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 35</p>
KW  - Application performance
KW  - Budget control
KW  - Computer hardware
KW  - Distributed applications
KW  - Limited visibility
KW  - Multiple applications
KW  - Power budgeting
KW  - Power capacity
KW  - Quality of service
KW  - Quality of service guarantees
KW  - Virtualized data centers
ER  - 

TY  - CONF
TI  - SoftPower: Fine-grain power estimations using performance counters
AU  - Lim, M.Y.
AU  - Porterfield, A.
AU  - Fowler, R.
T2  - HPDC 2010 - Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing
AB  - We present and evaluate a surrogate model, based on hardware performance counter measurements, to estimate computer system power consumption. Power and energy are especially important in the design and operation of large data centers and of clusters used for scientific computing. Tradeoffs are made between performance and power consumption, this needs to be dynamic because activity varies over time. While it is possible to instrument systems for fine-grain power monitoring, such instrumentation is costly and not commonly available. Furthermore, the latency and sampling periods of hardware power monitors can be large compared to time scales at which workloads can change and dynamic power controls can operate. Given these limitations, we argue that surrogate models of the kind we present here can provide low-cost and accurate estimates of power consumption to drive on-line dynamic control mechanisms and for use in off-line tuning. In this brief paper, we discuss a general approach to building system power estimation models based on hardware performance counters. Using this technique, we then present a model for an Intel Core i7 system that has an absolute estimation error of 5.32 percent (median) and acceptable data collection overheads on varying workloads, CPU power states (frequency and voltage), and number of active cores. Since this method is based on event sampling of hardware counters, one can make a tradeoff between estimation accuracy and data-collection overhead. Copyright 2010 ACM.
DA  - 2010///
PY  - 2010
DO  - 10.1145/1851476.1851517
SP  - 308
EP  - 311
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650033616&doi=10.1145%2f1851476.1851517&partnerID=40&md5=b633b0aa54c8e639bdd4a0f30ad292f1
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 55</p>
KW  - Building systems
KW  - Computer hardware
KW  - CPU power
KW  - Data acquisition
KW  - Data collection
KW  - Design and operations
KW  - Dynamic controls
KW  - Dynamic power control
KW  - Estimation
KW  - Estimation errors
KW  - Frequency estimation
KW  - Fuel additives
KW  - General approach
KW  - Hardware counters
KW  - Hardware performance counters
KW  - Instrument systems
KW  - Instruments
KW  - Large data
KW  - Performance counters
KW  - Power Consumption
KW  - Power estimation
KW  - Power estimations
KW  - Power monitor
KW  - Power monitoring
KW  - Sampling period
KW  - Scientific computing
KW  - Surrogate model
KW  - Time-scales
ER  - 

TY  - CONF
TI  - Where is the energy spent inside my app?: Fine grained energy accounting on smartphones with eprof
AU  - Pathak, A.
AU  - Hu, Y.C.
AU  - Zhang, M.
T2  - EuroSys'12 - Proceedings of the EuroSys 2012 Conference
AB  - Where is the energy spent inside my app? Despite the immense popularity of smartphones and the fact that energy is the most crucial aspect in smartphone programming, the answer to the above question remains elusive. This paper first presents epro f, the first fine-grained energy profiler for smartphone apps. Compared to profiling the runtime of applications running on conventional computers, profiling energy consumption of applications running on smartphones faces a unique challenge, asynchronous power behavior, where the effect on a component's power state due to a program entity lasts beyond the end of that program entity. We present the design, implementation and evaluation of eprof on two mobile OSes, Android and Windows Mobile. We then present an in-depth case study, the first of its kind, of six popular smartphones apps (including Angry-Birds, Facebook and Browser). Eprof sheds lights on internal energy dissipation of these apps and exposes surprising findings like 65%-75% of energy in free apps is spent in third-party advertisement modules. Eprof also reveals several "wakelock bugs", a family of "energy bugs" in smart-phone apps, and effectively pinpoints their location in the source code. The case study highlights the fact that most of the energy in smartphone apps is spent in I/O, and I/O events are clustered, often due to a few routines. This motivates us to propose bundles, a new accounting presentation of app I/O energy, which helps the developer to quickly understand and optimize the energy drain of her app. Using the bundle presentation, we reduced the energy consumption of four apps by 20% to 65%. © 2012 ACM.
DA  - 2012///
PY  - 2012
DO  - 10.1145/2168836.2168841
SP  - 29
EP  - 42
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860588880&doi=10.1145%2f2168836.2168841&partnerID=40&md5=7e5e7c394e2c4c1aa4599acddc188741
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 512</p>
KW  - Conventional computers
KW  - Energy
KW  - Energy accounting
KW  - Energy dissipation
KW  - Energy drain
KW  - Energy utilization
KW  - Eprof
KW  - Facebook
KW  - Fine grained
KW  - Mobile
KW  - Power state
KW  - Runtimes
KW  - Signal encoding
KW  - Smartphones
KW  - Source codes
KW  - Windows mobiles
ER  - 

TY  - JOUR
TI  - A study on the use of performance counters to estimate power in microprocessors
AU  - Rodrigues, R.
AU  - Annamalai, A.
AU  - Koren, I.
AU  - Kundu, S.
T2  - IEEE Transactions on Circuits and Systems II: Express Briefs
AB  - We present a study on estimating the dynamic power consumption of a processor based on performance counters. Today's processors feature a large number of such counters to monitor various CPU and memory parameters, such as utilization, occupancy, bandwidth, page, cache, and branch buffer hit rates. The use of various sets of performance counters to estimate the power consumed by the processor has been demonstrated in the past. Our goal is to find out whether there exists a subset of counters that can be used to estimate, with sufficient accuracy, the dynamic power consumption of processors with varying microarchitecture. To this end, we consider two recent processor configurations representing two extremes of the performance spectrum, one targeting low power and the other high performance. Our results indicate that only three counters measuring 1) the number of fetched instructions, 2) level-1 cache hits, and 3) dispatch stalls are sufficient to achieve adequate precision. These counters are shown to be effective in predicting the dynamic power consumption across processors of varying resource sizes achieving a prediction accuracy of 95%. © 2004-2012 IEEE.
DA  - 2013///
PY  - 2013
DO  - 10.1109/TCSII.2013.2285966
VL  - 60
IS  - 12
SP  - 882
EP  - 886
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890856183&doi=10.1109%2fTCSII.2013.2285966&partnerID=40&md5=42d883289d120d43e48a715684651370
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 61</p>
KW  - Dynamic power consumption
KW  - Electric power utilization
KW  - Electrical engineering
KW  - Electronics engineering
KW  - High-performance core (HPerf)
KW  - Low Power
KW  - low-power core (LP)
KW  - Micro architectures
KW  - performance counters
KW  - Performance counters
KW  - Performance spectrum
KW  - power estimation
KW  - Power estimations
KW  - Prediction accuracy
ER  - 

TY  - JOUR
TI  - Data center efficiency assessment
AU  - Whitney, J.
AU  - Delforge, P.
T2  - Data Center Efficiency Assessment
DA  - 2014///
PY  - 2014
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955280182&partnerID=40&md5=c2aff8c790412a8447c9a15d4555890a
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 155</p>
ER  - 

TY  - JOUR
TI  - Power-management architecture of the intel microarchitecture code-named Sandy Bridge
AU  - Rotem, E.
AU  - Naveh, A.
AU  - Ananthakrishnan, A.
AU  - Weissmann, E.
AU  - Rajwan, D.
T2  - IEEE Micro
AB  - Modern microprocessors are evolving into system-on-a-chip designs with high integration levels, catering to ever-shrinking form factors. Portability without compromising performance is a driving market need. An architectural approach that's adaptive to and cognizant of workload behavior and platform physical constraints is indispensable to meeting these performance and efficiency goals. This article describes power-management innovations introduced on Intel's Sandy Bridge microprocessor. © 2012 IEEE.
DA  - 2012///
PY  - 2012
DO  - 10.1109/MM.2012.12
VL  - 32
IS  - 2
SP  - 20
EP  - 27
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859729360&doi=10.1109%2fMM.2012.12&partnerID=40&md5=ac411761c3d90741a5eebafa6fed9716
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 387</p>
KW  - Application specific integrated circuits
KW  - Approximation theory
KW  - Architectural approach
KW  - energy management
KW  - Energy management
KW  - Form factors
KW  - High integration level
KW  - Market needs
KW  - Micro architectures
KW  - Microprocessor chips
KW  - Modern microprocessor
KW  - Physical constraints
KW  - power management
KW  - Power managements
KW  - Program processors
KW  - Sandy Bridge
KW  - System-on-a-chip designs
KW  - Turbo Boost
ER  - 

TY  - CONF
TI  - Self-constructive high-rate system energy modeling for battery-powered mobile systems
AU  - Dong, M.
AU  - Zhong, L.
T2  - MobiSys'11 - Compilation Proceedings of the 9th International Conference on Mobile Systems, Applications and Services and Co-located Workshops
AB  - System energy models are important for energy optimization and management in mobile systems. However, existing system energy models are built in a lab setting with the help from a second computer. Not only are they labor-intensive; but also they do not adequately account for the great diversity in the hardware and usage of mobile systems. Moreover, existing system energy models are intended for energy estimation for time intervals of one second or longer; they do not provide the required rate for fine-grain use such as per-application energy accounting. In this work, we study a self-modeling paradigm in which a mobile system automatically generates its energy model without any external assistance. Our solution, Sesame, leverages the possibility of self power measurement through the smart battery interface and employs a suite of novel techniques to achieve accuracy and rate much higher than that of the smart battery interface. We report the implementation and evaluation of Sesame on a laptop and a smartphone. The experiment results show that Sesame is able to generate system energy models of 95% accuracy at one estimation per second and of 88% accuracy at one estimation per 10ms, without any external assistance. Two five-day field studies with four laptop and four smartphone users further demonstrate the effectiveness, efficiency, and non-invasiveness of Sesame. © 2011 ACM.
DA  - 2011///
PY  - 2011
DO  - 10.1145/1999995.2000027
SP  - 335
EP  - 348
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79961068480&doi=10.1145%2f1999995.2000027&partnerID=40&md5=657d48bfa00a439eb38824f38d639126
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 197</p>
KW  - Energy accounting
KW  - Energy estimation
KW  - Energy model
KW  - energy modeling
KW  - Energy modeling
KW  - Energy optimization
KW  - Estimation
KW  - Existing systems
KW  - Field studies
KW  - High-rate systems
KW  - Laptop computers
KW  - low power
KW  - Low Power
KW  - mobile systems
KW  - Mobile systems
KW  - Novel techniques
KW  - Signal encoding
KW  - Smart phones
KW  - System energy
KW  - Time interval
ER  - 

TY  - CONF
TI  - Estimating mobile application energy consumption using program analysis
AU  - Hao, S.
AU  - Li, D.
AU  - Halfond, W.G.J.
AU  - Govindan, R.
T2  - Proceedings - International Conference on Software Engineering
AB  - Optimizing the energy efficiency of mobile applications can greatly increase user satisfaction. However, developers lack viable techniques for estimating the energy consumption of their applications. This paper proposes a new approach that is both lightweight in terms of its developer requirements and provides fine-grained estimates of energy consumption at the code level. It achieves this using a novel combination of program analysis and per-instruction energy modeling. In evaluation, our approach is able to estimate energy consumption to within 10% of the ground truth for a set of mobile applications from the Google Play store. Additionally, it provides useful and meaningful feedback to developers that helps them to understand application energy consumption behavior. © 2013 IEEE.
DA  - 2013///
PY  - 2013
DO  - 10.1109/ICSE.2013.6606555
SP  - 92
EP  - 101
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886424945&doi=10.1109%2fICSE.2013.6606555&partnerID=40&md5=d1be15a97911487f3e5c18a2be35bb4e
DB  - Scopus
N1  - <p>Export Date: 21 October 2025; Cited By: 252</p>
KW  - Computer applications
KW  - Energy efficiency
KW  - Energy estimation
KW  - Energy utilization
KW  - Estimation
KW  - fine-grained energy estimation
KW  - Google plays
KW  - Ground truth
KW  - Mobile app
KW  - Mobile applications
KW  - Mobile computing
KW  - New approaches
KW  - program analysis
KW  - Program analysis
KW  - Software engineering
KW  - User satisfaction
ER  - 

TY  - CONF
TI  - The straw that broke the camel's back: Safe cloud overbooking with application brownout
AU  - Tomas, L.
AU  - Klein, C.
AU  - Tordsson, J.
AU  - Hernandez-Rodriguez, F.
T2  - Proceedings - 2014 International Conference on Cloud and Autonomic Computing, ICCAC 2014
AB  - Resource overbooking is an admission control technique to increase utilization in cloud environments. However, due to uncertainty about future application workloads, overbooking may result in overload situations and deteriorated performance. We mitigate this using brownout, a feedback approach to application performance steering, that ensures graceful degradation during load spikes and thus avoids overload. Additionally, brownout management information is included into the overbooking system, enabling the development of improved reactive methods to overload situations. Our combined brownout-overbooking approach is evaluated based on real-life interactive workloads and non-interactive batch applications. The results show that our approach achieves an improvement of resource utilization of 11 to 37 percentage points, while keeping response times lower than the set target of 1 second, with negligible application degradation. © 2014 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/ICCAC.2014.10
SP  - 151
EP  - 160
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923972497&doi=10.1109%2fICCAC.2014.10&partnerID=40&md5=8b2ea2c3f695f04eef39fc7e39e61500
DB  - Scopus
KW  - Information management
KW  - Cloud environments
KW  - Computer science
KW  - Resource utilizations
KW  - Future applications
KW  - Application performance
KW  - Computer programming
KW  - Graceful degradation
KW  - Management information
KW  - Over-booking approaches
KW  - Resource overbooking
ER  - 

TY  - JOUR
AU  - Docker, I.
T2  - Docker Compose File Version 3 Reference
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061106799&partnerID=40&md5=0f4f39d246cd3efb265b611c962dfbe5
DB  - Scopus
ER  - 

TY  - CONF
TI  - Brownout: Building more robust cloud applications
AU  - Klein, C.
AU  - Maggio, M.
AU  - Arzén, K.-E.
AU  - Hernández-Rodriguez, F.
T2  - Proceedings - International Conference on Software Engineering
AB  - Self-adaptation is a first class concern for cloud applications, which should be able to withstand diverse runtime changes. Variations are simultaneously happening both at the cloud infrastructure level - for example hardware failures - and at the user workload level - flash crowds. However, robustly withstanding extreme variability, requires costly hardware over-provisioning. In this paper, we introduce a self-adaptation programming paradigm called brownout. Using this paradigm, applications can be designed to robustly withstand unpredictable runtime variations, without over-provisioning. The paradigm is based on optional code that can be dynamically deactivated through decisions based on control theory. We modified two popular web application prototypes - RUBiS and RUBBoS - with less than 170 lines of code, to make them brownout-compliant. Experiments show that brownout self-adaptation dramatically improves the ability to withstand flash-crowds and hardware failures. © 2014 ACM.
DA  - 2014///
PY  - 2014
DO  - 10.1145/2568225.2568227
SP  - 700
EP  - 711
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994065778&doi=10.1145%2f2568225.2568227&partnerID=40&md5=65399bd44b01bdd11e43ebcda0ddb8ab
DB  - Scopus
KW  - Cloud
KW  - Software engineering
KW  - Hardware
KW  - Brownout
KW  - Cloud applications
KW  - Cloud infrastructures
KW  - Clouds
KW  - Adaptive software
KW  - Adaptive Software
KW  - Control theory
KW  - Control Theory
KW  - Hardware failures
KW  - Over provisioning
KW  - Programming paradigms
KW  - Reconfigurable hardware
KW  - Run-time variations
ER  - 

TY  - CONF
TI  - Renewable and cooling aware workload management for sustainable data centers
AU  - Liu, Z.
AU  - Chen, Y.
AU  - Bash, C.
AU  - Wierman, A.
AU  - Gmach, D.
AU  - Wang, Z.
AU  - Marwah, M.
AU  - Hyser, C.
T2  - Performance Evaluation Review
AB  - Recently, the demand for data center computing has surged, increasing the total energy footprint of data centers worldwide. Data centers typically comprise three subsystems: IT equipment provides services to customers; power infrastructure supports the IT and cooling equipment; and the cooling infrastructure removes heat generated by these subsystems. This work presents a novel approach to model the energy flows in a data center and optimize its operation. Traditionally, supply-side constraints such as energy or cooling availability were treated independently from IT workload management. This work reduces electricity cost and environmental impact using a holistic approach that integrates renewable supply, dynamic pricing, and cooling supply including chiller and outside air cooling, with IT workload planning to improve the overall sustainability of data center operations. Specifically, we first predict renewable energy as well as IT demand. Then we use these predictions to generate an IT workload management plan that schedules IT workload and allocates IT resources within a data center according to time varying power supply and cooling efficiency. We have implemented and evaluated our approach using traces from real data centers and production systems. The results demonstrate that our approach can reduce both the recurring power costs and the use of non-renewable energy by as much as 60% compared to existing techniques, while still meeting the Service Level Agreements. © 2012 ACM.
DA  - 2012///
PY  - 2012
DO  - 10.1145/2254756.2254779
VL  - 40
SP  - 175
EP  - 186
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864700984&doi=10.1145%2f2254756.2254779&partnerID=40&md5=dfdf71c33e8d6ec1bb243fe5c0840045
DB  - Scopus
KW  - scheduling
KW  - Scheduling
KW  - Information technology
KW  - Data centers
KW  - Optimization
KW  - Information management
KW  - Cooling
KW  - Total energy
KW  - Renewable energies
KW  - Power infrastructures
KW  - Cost reduction
KW  - Computer systems
KW  - Cooling equipment
KW  - Service Level Agreements
KW  - Non-renewable energy
KW  - Time varying
KW  - Air cooling
KW  - Cooling efficiency
KW  - cooling optimization
KW  - Cooling optimization
KW  - Data center operations
KW  - demand shaping
KW  - Dynamic pricing
KW  - Electricity costs
KW  - Energy flow
KW  - Holistic approach
KW  - IT equipment
KW  - IT resources
KW  - Power costs
KW  - Power supply
KW  - Production system
KW  - renewable energy
KW  - sustainable data center
KW  - Workload management
KW  - Workload planning
ER  - 

TY  - JOUR
AU  - Luzzardi, A.
T2  - Scale Testing Docker Swarm to 30,000 Containers
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994686335&partnerID=40&md5=84cfef52310b7eb2ac410692044cc6aa
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Probabilistic Consolidation of Virtual Machines in Self-Organizing Cloud Data Centers
AU  - Mastroianni, C.
AU  - Meo, M.
AU  - Papuzzo, G.
T2  - IEEE Transactions on Cloud Computing
AB  - Power efficiency is one of the main issues that will drive the design of data centers, especially of those devoted to provide Cloud computing services. In virtualized data centers, consolidation of Virtual Machines (VMs) on the minimum number of physical servers has been recognized as a very efficient approach, as this allows unloaded servers to be switched off or used to accommodate more load, which is clearly a cheaper alternative to buy more resources. The consolidation problem must be solved on multiple dimensions, since in modern data centers CPU is not the only critical resource: depending on the characteristics of the workload other resources, for example, RAM and bandwidth, can become the bottleneck. The problem is so complex that centralized and deterministic solutions are practically useless in large data centers with hundreds or thousands of servers. This paper presents ecoCloud a self-organizing and adaptive approach for the consolidation of VMs on two resources, namely CPU and RAM. Decisions on the assignment and migration of VMs are driven by probabilistic processes and are based exclusively on local information, which makes the approach very simple to implement. Both a fluid-like mathematical model and experiments on a real data center show that the approach rapidly consolidates the workload, and CPU-bound and RAM-bound VMs are balanced, so that both resources are exploited efficiently. © 2013 IEEE.
DA  - 2013///
PY  - 2013
DO  - 10.1109/TCC.2013.17
VL  - 1
IS  - 2
SP  - 215
EP  - 228
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969916663&doi=10.1109%2fTCC.2013.17&partnerID=40&md5=ca388967b1d387943d840d8ba2d0e6ec
DB  - Scopus
KW  - Cloud computing
KW  - data center
KW  - Network security
KW  - Data centers
KW  - Energy conservation
KW  - Cloud data centers
KW  - Cloud computing services
KW  - Virtual machine
KW  - Digital storage
KW  - Virtualized data centers
KW  - Critical resources
KW  - energy saving
KW  - Multiple dimensions
KW  - Probabilistic consolidations
KW  - Probabilistic process
KW  - VM consolidation
ER  - 

TY  - JOUR
T2  - Docker Documentation | Docker Documentation
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061078756&partnerID=40&md5=a9b8991f6c6d87bfda7ef2335cf3c527
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Power-aware provisioning of virtual machines for real-time Cloud services
AU  - Kim, K.H.
AU  - Beloglazov, A.
AU  - Buyya, R.
T2  - Concurrency and Computation: Practice and Experience
AB  - Reducing power consumption has been an essential requirement for Cloud resource providers not only to decrease operating costs, but also to improve the system reliability. As Cloud computing becomes emergent for the Anything as a Service (XaaS) paradigm, modern real-time services also become available through Cloud computing. In this work, we investigate power-aware provisioning of virtual machines for real-time services. Our approach is (i) to model a real-time service as a real-time virtual machine request; and (ii) to provision virtual machines in Cloud data centers using dynamic voltage frequency scaling schemes. We propose several schemes to reduce power consumption by hard real-time services and power-aware profitable provisioning of soft real-time services. © 2011 John Wiley & Sons, Ltd.
DA  - 2011///
PY  - 2011
DO  - 10.1002/cpe.1712
VL  - 23
IS  - 13
SP  - 1491
EP  - 1505
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051798127&doi=10.1002%2fcpe.1712&partnerID=40&md5=99b529f15e78a203deecd0580efda5df
DB  - Scopus
KW  - cloud computing
KW  - Cloud computing
KW  - Energy efficiency
KW  - green data centers
KW  - Network security
KW  - Green computing
KW  - Dynamic frequency scaling
KW  - Dynamic voltage frequency scaling
KW  - Voltage scaling
KW  - Cloud data centers
KW  - Virtual machine
KW  - Web services
KW  - Information services
KW  - Electric power utilization
KW  - Resource providers
KW  - Operating costs
KW  - Real time systems
KW  - System reliability
KW  - Energy efficient computing
KW  - energy-efficient computing
KW  - Green data centers
KW  - Hard real-time
KW  - Real time service
KW  - real-time services
ER  - 

TY  - JOUR
T2  - Apache JMeter-Apache JMeter™
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050792283&partnerID=40&md5=659268e5695b4f76e5bbd078230dca60
DB  - Scopus
ER  - 

TY  - CONF
TI  - Market-oriented cloud computing: Vision, hype, and reality for delivering IT services as computing utilities
AU  - Buyya, R.
AU  - Yeo, C.S.
AU  - Venugopal, S.
T2  - Proceedings - 10th IEEE International Conference on High Performance Computing and Communications, HPCC 2008
AB  - This keynote paper: presents a 21st century vision of computing; identifies various computing paradigms promising to deliver the vision of computing utilities; defines Cloud computing and provides the architecture for creating market-oriented Clouds by leveraging technologies such as VMs; provides thoughts on market-based resource management strategies that encompass both customer-driven service management and computational risk management to sustain SLA-oriented resource allocation; presents some representative Cloud platforms especially those developed in industries along with our current work towards realising market-oriented resource allocation of Clouds by leveraging the 3 rd generation Aneka enterprise Grid technology; reveals our early thoughts on interconnecting Clouds for dynamically creating an atmospheric computing environment along with pointers to future community research; and concludes with the need for convergence of competing IT paradigms for delivering our 21st century vision. © 2008 IEEE.
DA  - 2008///
PY  - 2008
DO  - 10.1109/HPCC.2008.172
SP  - 5
EP  - 13
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349150824&doi=10.1109%2fHPCC.2008.172&partnerID=40&md5=6c72d9bf489047e57ce934a25be09075
DB  - Scopus
KW  - Resource allocation
KW  - Information management
KW  - Computing environments
KW  - Planning
KW  - Clouds
KW  - Computational risk managements
KW  - Computer systems
KW  - Computing paradigms
KW  - Grid computing
KW  - GRID technologies
KW  - High performance liquid chromatography
KW  - It services
KW  - Management
KW  - Marketing
KW  - Resource Management strategies
KW  - Risk analysis
KW  - Risk management
KW  - Service managements
ER  - 

TY  - JOUR
TI  - Managing overloaded hosts for dynamic consolidation of virtual machines in cloud data centers under quality of service constraints
AU  - Beloglazov, A.
AU  - Buyya, R.
T2  - IEEE Transactions on Parallel and Distributed Systems
AB  - Dynamic consolidation of virtual machines (VMs) is an effective way to improve the utilization of resources and energy efficiency in cloud data centers. Determining when it is best to reallocate VMs from an overloaded host is an aspect of dynamic VM consolidation that directly influences the resource utilization and quality of service (QoS) delivered by the system. The influence on the QoS is explained by the fact that server overloads cause resource shortages and performance degradation of applications. Current solutions to the problem of host overload detection are generally heuristic based, or rely on statistical analysis of historical data. The limitations of these approaches are that they lead to suboptimal results and do not allow explicit specification of a QoS goal. We propose a novel approach that for any known stationary workload and a given state configuration optimally solves the problem of host overload detection by maximizing the mean intermigration time under the specified QoS goal based on a Markov chain model. We heuristically adapt the algorithm to handle unknown nonstationary workloads using the Multisize Sliding Window workload estimation technique. Through simulations with workload traces from more than a thousand PlanetLab VMs, we show that our approach outperforms the best benchmark algorithm and provides approximately 88 percent of the performance of the optimal offline algorithm. © 1990-2012 IEEE.
DA  - 2013///
PY  - 2013
DO  - 10.1109/TPDS.2012.240
VL  - 24
IS  - 7
SP  - 1366
EP  - 1379
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878476684&doi=10.1109%2fTPDS.2012.240&partnerID=40&md5=01be4a531f0874fe562725f0dc176ae7
DB  - Scopus
KW  - cloud computing
KW  - Cloud computing
KW  - energy efficiency
KW  - Energy efficiency
KW  - virtualization
KW  - Distributed systems
KW  - Quality of service
KW  - Performance degradation
KW  - Benchmarking
KW  - Algorithms
KW  - Resource utilizations
KW  - Computer simulation
KW  - Dynamic consolidation
KW  - Utilization of resources
KW  - dynamic consolidation
KW  - host overload detection
KW  - Overload detection
KW  - Quality of Service constraints
KW  - Virtualizations
ER  - 

TY  - JOUR
TI  - Global Warming: Data Centres to Consume Three Times as Much Energy in Next Decade, Experts Warn. Independent
AU  - Bawden, T.
T2  - Global Warming: Data Centres to Consume Three Times As Much Energy in Next Decade, Experts Warn
DA  - 2016///
PY  - 2016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027445487&partnerID=40&md5=123e9283abd4476f899e2d3f932614ad
DB  - Scopus
ER  - 

TY  - JOUR
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081760324&partnerID=40&md5=415a49868af0ccf4777abd03d2c145e8
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Data center efficiency assessment-scaling up energy efficiency across the data center industry: Evaluating key drivers and barriers
AU  - Whitney, J.
AU  - Delforge, P.
T2  - Data Center Efficiency Assessment-Scaling Up Energy Efficiency Across the Data Center Industry: Evaluating Key Drivers and Barriers
DA  - 2014///
PY  - 2014
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957375958&partnerID=40&md5=74d0323715ac988c9cdcb79ae0c9b082
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Energy-Centered and QoS-Aware Services Selection for Internet of Things
AU  - Khanouche, M.E.
AU  - Amirat, Y.
AU  - Chibani, A.
AU  - Kerkar, M.
AU  - Yachir, A.
T2  - IEEE Transactions on Automation Science and Engineering
AB  - An important challenge to be addressed in the domain of Internet of Things (IoT) is the development of efficient services selection algorithms for an optimal management of both energy and Quality of Service (QoS) in the context of IoT services composition. This issue becomes crucial in the case of large-scale IoT environments composed of thousands of distributed entities. In this paper, an energy-centered and QoS-aware services selection algorithm (EQSA) is proposed for IoT services composition. The proposed selection approach consists of preselecting the services offering the QoS level required for user's satisfaction using a lexicographic optimization strategy and QoS constraints relaxation technique. In order to reduce the energy consumption of a composite service without affecting the user's satisfaction, the most suitable services among the preselected ones are then selected using the concept of relative dominance of services in the sense of Pareto. The relative dominance of a candidate service depends on its energy profile and QoS attributes, and user's preferences. The proposed algorithm has been evaluated through several simulation scenarios. The obtained results show clearly the good performances of the EQSA algorithm in terms of selection time, energy efficiency, composition lifetime, and optimality and its added value in comparison with algorithms dealing separately with QoS and energy consumption. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/TASE.2016.2539240
VL  - 13
IS  - 3
SP  - 1256
EP  - 1269
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979723409&doi=10.1109%2fTASE.2016.2539240&partnerID=40&md5=3d14b30a4a317db17e3e8c18f342f0a5
DB  - Scopus
KW  - Energy efficiency
KW  - multi-objective optimization
KW  - Internet of Things (IoT)
KW  - Internet of things
KW  - Quality of service
KW  - Energy utilization
KW  - Multiobjective optimization
KW  - Web services
KW  - Services composition
KW  - Internet of Things (IOT)
KW  - Telecommunication services
KW  - Linear programming
KW  - Energy profile
KW  - lexicographic optimization
KW  - Lexicographic optimization
KW  - quality of service (QoS)
KW  - services composition
KW  - services selection
KW  - Services selection
ER  - 

TY  - CONF
TI  - Utilization-based VM consolidation scheme for power efficiency in cloud data centers
AU  - Chen, Q.
AU  - Chen, J.
AU  - Zheng, B.
AU  - Cui, J.
AU  - Qian, Y.
T2  - 2015 IEEE International Conference on Communication Workshop, ICCW 2015
AB  - Cloud computing offers utility-oriented services to users, which is supported by large-scale data center. Although virtualized data centers provide high performance computing service, they also consume enormous amount of power. To solve the problem, dynamic consolidation of Virtual Machines (VMs) is considered as an efficient way to reduce power consumption and guarantee Quality of Service (QoS). Live migration is applied into the dynamic consolidation, which allows VMs to be migrated to other hosts and aims to minimize the number of hosts in data centers. However, the migration overhead is essential to be taken into account and massive migrations will lead to performance degradation and extra power consumption. In this paper, we propose a utilization-based migration algorithm (UMA) to migrate VMs to stable hosts, which efficiently reduces migration time and power consumption. Experiment results show that our UMA can reduce about 77.5%-82.4% migrations and save up to 39.3% -42.2% power consumption compared with the MinPower policy. © 2015 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/ICCW.2015.7247462
SP  - 1928
EP  - 1933
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947776140&doi=10.1109%2fICCW.2015.7247462&partnerID=40&md5=c79891a7cdb7bf5e860a097a90b342c3
DB  - Scopus
KW  - Energy efficiency
KW  - Quality of service
KW  - Performance degradation
KW  - Cloud data centers
KW  - Virtual machine
KW  - Power efficiency
KW  - Electric power utilization
KW  - Large scale data
KW  - Dynamic consolidation
KW  - Virtualized data centers
KW  - High performance computing
KW  - Migration algorithms
ER  - 

TY  - JOUR
TI  - Energy efficiency of vm consolidation in iaas clouds
AU  - Teng, F.
AU  - Yu, L.
AU  - Li, T.
AU  - Deng, D.
AU  - Magoulès, F.
T2  - J. Supercomput.
DA  - 2016///
PY  - 2016
SP  - 1
EP  - 28
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054315522&partnerID=40&md5=68e6992837656e7866c72685d36c854d
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Energy-aware resource allocation heuristics for efficient management of data centers for Cloud computing
AU  - Beloglazov, A.
AU  - Abawajy, J.
AU  - Buyya, R.
T2  - Future Generation Computer Systems
AB  - Cloud computing offers utility-oriented IT services to users worldwide. Based on a pay-as-you-go model, it enables hosting of pervasive applications from consumer, scientific, and business domains. However, data centers hosting Cloud applications consume huge amounts of electrical energy, contributing to high operational costs and carbon footprints to the environment. Therefore, we need Green Cloud computing solutions that can not only minimize operational costs but also reduce the environmental impact. In this paper, we define an architectural framework and principles for energy-efficient Cloud computing. Based on this architecture, we present our vision, open research challenges, and resource provisioning and allocation algorithms for energy-efficient management of Cloud computing environments. The proposed energy-aware allocation heuristics provision data center resources to client applications in a way that improves energy efficiency of the data center, while delivering the negotiated Quality of Service (QoS). In particular, in this paper we conduct a survey of research in energy-efficient computing and propose: (a) architectural principles for energy-efficient management of Clouds; (b) energy-efficient resource allocation policies and scheduling algorithms considering QoS expectations and power usage characteristics of the devices; and (c) a number of open research challenges, addressing which can bring substantial benefits to both resource providers and consumers. We have validated our approach by conducting a performance evaluation study using the CloudSim toolkit. The results demonstrate that Cloud computing model has immense potential as it offers significant cost savings and demonstrates high potential for the improvement of energy efficiency under dynamic workload scenarios. © 2011 Elsevier B.V. All rights reserved.
DA  - 2012///
PY  - 2012
DO  - 10.1016/j.future.2011.04.017
VL  - 28
IS  - 5
SP  - 755
EP  - 768
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857370722&doi=10.1016%2fj.future.2011.04.017&partnerID=40&md5=404d8a6f96e7208ce49f7fb3ffc699cd
DB  - Scopus
KW  - Cloud computing
KW  - Energy efficiency
KW  - Resource management
KW  - Virtualization
KW  - Carbon footprint
KW  - Data centers
KW  - Performance evaluation
KW  - Quality of service
KW  - Resource allocation
KW  - Energy efficient
KW  - Energy aware
KW  - Computing environments
KW  - Green IT
KW  - Environmental impact
KW  - Research challenges
KW  - Resource provisioning
KW  - Computer systems
KW  - Dynamic consolidation
KW  - Resource providers
KW  - Research
KW  - Virtualizations
KW  - Allocation algorithm
KW  - Architectural frameworks
KW  - Architectural principles
KW  - Business domain
KW  - Client applications
KW  - Computing solutions
KW  - Cost saving
KW  - Electrical energy
KW  - Energy-efficient resource allocation
KW  - High potential
KW  - IT services
KW  - Operational costs
KW  - Pay-as-you-go
KW  - Pervasive applications
KW  - Power usage
ER  - 

TY  - CONF
TI  - Virtual machine consolidation in cloud data centers using ACO metaheuristic
AU  - Ferdaus, M.H.
AU  - Murshed, M.
AU  - Calheiros, R.N.
AU  - Buyya, R.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - In this paper, we propose the AVVMC VM consolidation scheme that focuses on balanced resource utilization of servers across different computing resources (CPU, memory, and network I/O) with the goal of minimizing power consumption and resource wastage. Since the VM consolidation problem is strictly NP-hard and computationally infeasible for large data centers, we propose adaptation and integration of the Ant Colony Optimization (ACO) metaheuristic with balanced usage of computing resources based on vector algebra. Our simulation results show that AVVMC outperforms existing methods and achieves improvement in both energy consumption and resource wastage reduction. © 2014 Springer International Publishing Switzerland.
DA  - 2014///
PY  - 2014
DO  - 10.1007/978-3-319-09873-9_26
VL  - 8632 LNCS
SP  - 306
EP  - 317
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958546557&doi=10.1007%2f978-3-319-09873-9_26&partnerID=40&md5=8d940660c30a5c80492576433b3d1e30
DB  - Scopus
KW  - Artificial intelligence
KW  - Energy utilization
KW  - Cloud data centers
KW  - Algorithms
KW  - Resource utilizations
KW  - Metaheuristic
KW  - Virtual machine consolidations
KW  - Computing resource
KW  - Ant colony optimization
KW  - Ant Colony Optimization (ACO)
KW  - Network I/O
KW  - Vector algebra
ER  - 

TY  - JOUR
TI  - Cost-Aware Multimedia Data Allocation for Heterogeneous Memory Using Genetic Algorithm in Cloud Computing
AU  - Gai, K.
AU  - Qiu, L.
AU  - Zhao, H.
AU  - Qiu, M.
T2  - IEEE Transactions on Cloud Computing
AB  - Recent expansions of Internet-of-Things (IoT) applying cloud computing have been growing at a phenomenal rate. As one of the developments, heterogeneous cloud computing has enabled a variety of cloud-based infrastructure solutions, such as multimedia big data. Numerous prior researches have explored the optimizations of on-premise heterogeneous memories. However, the heterogeneous cloud memories are facing constraints due to the performance limitations and cost concerns caused by the hardware distributions and manipulative mechanisms. Assigning data tasks to distributed memories with various capacities is a combinatorial NP-hard problem. This paper focuses on this issue and proposes a novel approach, Cost-Aware Heterogeneous Cloud Memory Model (CAHCM), aiming to provision a high performance cloud-based heterogeneous memory service offerings. The main algorithm supporting CAHCM is Dynamic Data Allocation Advance (2DA) Algorithm that uses genetic programming to determine the data allocations on the cloud-based memories. In our proposed approach, we consider a set of crucial factors impacting the performance of the cloud memories, such as communication costs, data move operating costs, energy performance, and time constraints. Finally, we implement experimental evaluations to examine our proposed model. The experimental results have shown that our approach is adoptable and feasible for being a cost-aware cloud-based solution. © 2013 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/TCC.2016.2594172
VL  - 8
IS  - 4
SP  - 1212
EP  - 1222
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097852644&doi=10.1109%2fTCC.2016.2594172&partnerID=40&md5=cd4c7bebee53fb44b787663a80271aec
DB  - Scopus
KW  - Cloud computing
KW  - Performance
KW  - Big data
KW  - Internet of things
KW  - Optimisations
KW  - Cloud-computing
KW  - Genetic algorithms
KW  - Cloud-based
KW  - Heterogeneous memory
KW  - genetic algorithm
KW  - Computational complexity
KW  - Cost-aware
KW  - Operating costs
KW  - data allocation
KW  - Data allocation
KW  - Genetic programming
KW  - heterogeneous memory
KW  - Memory modeling
KW  - multimedia big data
KW  - Multimedia data
KW  - Multimedium big data
ER  - 

TY  - CONF
TI  - Joint power optimization of data center network and servers with correlation analysis
AU  - Zheng, K.
AU  - Wang, X.
AU  - Li, L.
AU  - Wang, X.
T2  - Proceedings - IEEE INFOCOM
AB  - Data center power optimization has recently received a great deal of research attention. For example, server consolidation has been demonstrated as one of the most effective energy saving methodologies. Likewise, traffic consolidation has also been recently proposed to save energy for data center networks (DCNs). However, current research on data center power optimization focuses on servers and DCN separately. As a result, the optimization results are often inferior, because server consolidation without considering the DCN may cause traffic congestion and thus degraded network performance. On the other hand, server consolidation may change the DCN topology, allowing new opportunities for energy savings. In this paper, we propose PowerNetS, a power optimization strategy that leverages workload correlation analysis to jointly minimize the total power consumption of servers and the DCN. The design of PowerNetS is based on the key observations that the workloads of different servers and DCN traffic flows do not peak at exactly the same time. Thus, more energy savings can be achieved if the workload correlations are considered in server and traffic consolidations. In addition, PowerNetS considers the DCN topology during server consolidation, which leads to less inter-server traffic and thus more energy savings and shorter network delays. We implement PowerNetS on a hardware testbed composed of 10 virtual switches configured with a production 48-port OpenFlow switch and 6 servers. Our empirical results with Wikipedia, Yahoo!, and IBM traces demonstrate that PowerNetS can save up to 51.6% of energy for a data center. PowerNetS also outperforms two state-of-the-art baselines by 44.3% and 15.8% on energy savings, respectively. Our simulation results with 72 switches and 122 servers also show the superior energy efficiency of PowerNetS over the baselines. © 2014 IEEE.
DA  - 2014///
PY  - 2014
DO  - 10.1109/INFOCOM.2014.6848207
SP  - 2598
EP  - 2606
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904410981&doi=10.1109%2fINFOCOM.2014.6848207&partnerID=40&md5=ec3b4daf09fd8b2c667563df1cd786e4
DB  - Scopus
KW  - Energy efficiency
KW  - Topology
KW  - Traffic congestion
KW  - Power Optimization
KW  - Server consolidation
KW  - Correlation analysis
KW  - Correlation methods
KW  - Data center networks
KW  - Data center networks (DCNs)
KW  - Hardware testbeds
KW  - Total power consumption
KW  - Workload correlations
ER  - 

TY  - CONF
TI  - Dynamic virtual machine management via approximate Markov decision process
AU  - Han, Z.
AU  - Tan, H.
AU  - Chen, G.
AU  - Wang, R.
AU  - Chen, Y.
AU  - Lau, F.C.M.
T2  - Proceedings - IEEE INFOCOM
AB  - Efficient virtual machine (VM) management can dramatically reduce energy consumption in data centers. Existing VM management algorithms fall into two categories based on whether the VMs' resource demands are assumed to be static or dynamic. The former category fails to maximize the resource utilization as they cannot adapt to the dynamic nature of VMs' resource demands. Most approaches in the latter category are heuristical and lack theoretical performance guarantees. In this work, we formulate dynamic VM management as a large-scale Markov Decision Process (MDP) problem and derive an optimal solution. Our analysis of real-world data traces supports our choice of the modeling approach. However, solving the large-scale MDP problem suffers from the curse of dimensionality. Therefore, we further exploit the special structure of the problem and propose an approximate MDP-based dynamic VM management method, called MadVM. We prove the convergence of MadVM and analyze the bound of its approximation error. Moreover, MadVM can be implemented in a distributed system, which should suit the needs of real data centers. Extensive simulations based on two real-world workload traces show that MadVM achieves significant performance gains over two existing baseline approaches in power consumption, resource shortage and the number of VM migrations. Specifically, the more intensely the resource demands fluctuate, the more MadVM outperforms. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/INFOCOM.2016.7524384
VL  - 2016-July
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983261312&doi=10.1109%2fINFOCOM.2016.7524384&partnerID=40&md5=ad72f6d581877dfd92d061df60b3b28e
DB  - Scopus
KW  - Markov processes
KW  - Decision making
KW  - Energy utilization
KW  - Markov Decision Processes
KW  - Resource utilizations
KW  - Extensive simulations
KW  - Reduce energy consumption
KW  - Approximation errors
KW  - Curse of dimensionality
KW  - Telecommunication networks
KW  - Theoretical performance
KW  - Virtual machine management
ER  - 

TY  - JOUR
AU  - Newman, S.
T2  - Building Microservices
DA  - 2015///
PY  - 2015
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950338538&partnerID=40&md5=aec25db8f81564a4ab82f370c5e620cc
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Cloud container technologies: A state-of-the-art review
AU  - Pahl, C.
AU  - Brogi, A.
AU  - Soldani, J.
AU  - Jamshidi, P.
T2  - IEEE Transactions on Cloud Computing
AB  - —Containers as a lightweight technology to virtualise applications have recently been successful, particularly to manage applications in the cloud. Often, the management of clusters of containers becomes essential and the orchestration of the construction and deployment becomes a central problem. This emerging topic has been taken up by researchers, but there is currently no secondary study to consolidate this research. We aim to identify, taxonomically classify and systematically compare the existing research body on containers and their orchestration and specifically the application of this technology in the cloud. We have conducted a systematic mapping study of 46 selected studies. We classified and compared the selected studies based on a characterisation framework. This results in a discussion of agreed and emerging concerns in the container orchestration space, positioning it within the cloud context, but also moving it closer to current concerns in cloud platforms, microservices and continuous development. © 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
DA  - 2019///
PY  - 2019
DO  - 10.1109/TCC.2017.2702586
VL  - 7
IS  - 3
SP  - 677
EP  - 692
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050374433&doi=10.1109%2fTCC.2017.2702586&partnerID=40&md5=ba46e14ad12f81852ca97648461e4504
DB  - Scopus
KW  - Container
KW  - Containers
KW  - Orchestration
KW  - Systematic review
KW  - Systematic Review
KW  - Continuous development
KW  - Container technologies
KW  - Cluster
KW  - State-of-the art reviews
KW  - Systematic mapping studies
KW  - Characterisation framework
KW  - Index terms
KW  - Index Terms—Cloud
KW  - Space platforms
ER  - 

TY  - CONF
TI  - Optimal online deterministic algorithms and adaptive heuristics for energy and performance efficient dynamic consolidation of virtual machines in Cloud data centers
AU  - Beloglazov, A.
AU  - Buyya, R.
T2  - Concurrency and Computation: Practice and Experience
AB  - The rapid growth in demand for computational power driven by modern service applications combined with the shift to the Cloud computing model have led to the establishment of large-scale virtualized data centers. Such data centers consume enormous amounts of electrical energy resulting in high operating costs and carbon dioxide emissions. Dynamic consolidation of virtual machines (VMs) using live migration and switching idle nodes to the sleep mode allows Cloud providers to optimize resource usage and reduce energy consumption. However, the obligation of providing high quality of service to customers leads to the necessity in dealing with the energy-performance trade-off, as aggressive consolidation may lead to performance degradation. Because of the variability of workloads experienced by modern applications, the VM placement should be optimized continuously in an online manner. To understand the implications of the online nature of the problem, we conduct a competitive analysis and prove competitive ratios of optimal online deterministic algorithms for the single VM migration and dynamic VM consolidation problems. Furthermore, we propose novel adaptive heuristics for dynamic consolidation of VMs based on an analysis of historical data from the resource usage by VMs. The proposed algorithms significantly reduce energy consumption, while ensuring a high level of adherence to the service level agreement. We validate the high efficiency of the proposed algorithms by extensive simulations using real-world workload traces from more than a thousand PlanetLab VMs. Copyright © 2011 John Wiley & Sons, Ltd. Copyright © 2011 John Wiley & Sons, Ltd.
DA  - 2012///
PY  - 2012
DO  - 10.1002/cpe.1867
VL  - 24
SP  - 1397
EP  - 1420
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864511735&doi=10.1002%2fcpe.1867&partnerID=40&md5=980cb3c4add37bca4eeb9ddb689564e6
DB  - Scopus
KW  - Cloud computing
KW  - resource management
KW  - virtualization
KW  - Resource management
KW  - Virtualization
KW  - Network security
KW  - Quality of service
KW  - Green computing
KW  - Carbon dioxide
KW  - Optimization
KW  - Economic and social effects
KW  - Energy utilization
KW  - Performance degradation
KW  - Virtual machine
KW  - Green IT
KW  - Service Level Agreements
KW  - Reduce energy consumption
KW  - Dynamic consolidation
KW  - Virtualized data centers
KW  - Operating costs
KW  - dynamic consolidation
KW  - Carbon dioxide emissions
KW  - Deterministic algorithms
KW  - Global warming
ER  - 

TY  - JOUR
TI  - Renewable-aware geographical load balancing of web applications for sustainable data centers
AU  - Nadjaran Toosi, A.
AU  - Qu, C.
AU  - de Assunção, M.D.
AU  - Buyya, R.
T2  - Journal of Network and Computer Applications
AB  - The ever-increasing demand for web applications deployed across multiple data centers results in large electricity costs for service providers and significant impact on the environment. This has motivated service providers to move towards more sustainable data centers powered by renewable or green sources of energy, such as solar or wind. However, efficient utilization of green energy to service web applications is a challenging problem due to intermittency and unpredictability of both application workload and renewable energy availability. One possible solution to reduce cost and increase renewable energy utilization is to exploit the spatio-temporal variations in on-site power and grid power prices by balancing the load among multiple data centers geographically distributed. In this paper, we propose a framework for reactive load balancing of web application requests among Geo-distributed sustainable data centers based on the availability of renewable energy sources on each site. A system prototype is developed, its underlying design and algorithms are described, and experiments are conducted with it using real infrastructure (Grid'5000 in France) and workload traces (real traffic to English Wikipedia). The experimental results demonstrate that our approach can reduce cost and brown energy usage with efficient utilization of green energy and without a priori knowledge of future workload, availability of renewable energy, and grid electricity prices. © 2017 Elsevier Ltd
DA  - 2017///
PY  - 2017
DO  - 10.1016/j.jnca.2017.01.036
VL  - 83
SP  - 155
EP  - 168
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013218316&doi=10.1016%2fj.jnca.2017.01.036&partnerID=40&md5=fc714bb8bc882da6f70ef53c4f5bac7f
DB  - Scopus
KW  - Energy efficiency
KW  - Auto-scaling
KW  - Costs
KW  - Green energy
KW  - Green computing
KW  - Energy utilization
KW  - Renewable energies
KW  - Renewable energy resources
KW  - Electric power transmission networks
KW  - Cost reduction
KW  - Energy policy
KW  - WEB application
KW  - Cost saving
KW  - Wikipedia
KW  - Brown energy
KW  - Geographical load balancing
KW  - Green Energy
KW  - Renewable energy
KW  - System prototype
KW  - Web applications
ER  - 

TY  - CONF
TI  - Energy-Aware Workflow Scheduling Using Frequency Scaling
AU  - Pietri, I.
AU  - Sakellariou, R.
T2  - Proceedings of the International Conference on Parallel Processing Workshops
AB  - Dynamic Voltage and Frequency Scaling (DVFS) is a power management technique used to decrease the processor frequency and minimize power consumption in modern computing systems. This may lead to higher energy savings for large-scale computational problems, with scientific workflows comprising an important category of applications among these. However, as frequency scaling may result in increased execution time overall, idle time on the processors may also increase, to such a degree that any gains in power are annulled, this depends on the system and workflow characteristics. In this paper, we propose a scheduling algorithm that adopts frequency scaling to reduce overall energy consumption of scientific workflows given an allocation of tasks onto machines and a deadline to complete the execution. Based on the observation that using the lowest possible frequency may not necessarily be energy-efficient, the proposed algorithm works iteratively to scale the frequency further and distribute any slack time, only when overall energy consumption can be decreased. Synthetic data based on parameters of real scientific workflows are used in the evaluation. The results show that the proposed algorithm can achieve energy savings, sometimes at the expense of execution time to reduce the idle time of the processors and decrease overall energy consumption. © 2014 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/ICPPW.2014.26
VL  - 2015-May
SP  - 104
EP  - 113
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946568373&doi=10.1109%2fICPPW.2014.26&partnerID=40&md5=78127246419cc2193a6bbad390d1d0fb
DB  - Scopus
KW  - Energy efficiency
KW  - workflow
KW  - Scheduling
KW  - DAG scheduling
KW  - Green computing
KW  - Energy utilization
KW  - Dynamic frequency scaling
KW  - Frequency-scaling
KW  - Voltage scaling
KW  - DVFS
KW  - Scheduling algorithms
KW  - Energy-aware scheduling
KW  - Iterative methods
KW  - Parallel processing systems
KW  - energy-aware scheduling
KW  - frequency scaling
ER  - 

TY  - JOUR
TI  - Provision of Data-Intensive Services Through Energy-and QoS-Aware Virtual Machine Placement in National Cloud Data Centers
AU  - Wang, S.
AU  - Zhou, A.
AU  - Hsu, C.-H.
AU  - Xiao, X.
AU  - Yang, F.
T2  - IEEE Transactions on Emerging Topics in Computing
AB  - Many data-intensive services (e.g., planet analysis, gene analysis, and so on) are becoming increasingly reliant on national cloud data centers (NCDCs) because of growing scientific collaboration among countries. In NCDCs, tens of thousands of virtual machines (VMs) are assigned to physical servers to provide data-intensive services with a quality-of-service (QoS) guarantee, and consume a massive amount of energy in the process. Although many VM placement schemes have been proposed to solve this problem of energy consumption, most of these assume that all the physical servers are homogeneous. However, the physical server configurations of NCDCs often differ significantly, which leads to varying energy consumption characteristics. In this paper, we explore an alternative VM placement approach to minimize energy consumption during the provision of data-intensive services with a global QoS guarantee in NCDCs. We use an improved particle swarm optimization algorithm to develop an optimal VM placement approach involving a tradeoff between energy consumption and global QoS guarantee for data-intensive services. Experimental results show that our approach significantly outperforms other approaches to energy optimization and global QoS guarantee in NCDCs. © 2013 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/TETC.2015.2508383
VL  - 4
IS  - 2
SP  - 290
EP  - 300
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976510805&doi=10.1109%2fTETC.2015.2508383&partnerID=40&md5=d87e788b1dcb97d7055c050a64508194
DB  - Scopus
KW  - cloud computing
KW  - Cloud computing
KW  - Energy consumption
KW  - QoS
KW  - Quality of service
KW  - Optimization
KW  - Energy utilization
KW  - Cloud data centers
KW  - Virtual machines
KW  - Particle swarm optimization (PSO)
KW  - Algorithms
KW  - Java programming language
KW  - Virtual machine placement
KW  - Virtual machine placements
KW  - Energy optimization
KW  - Data-intensive service
KW  - Data-intensive services
KW  - Improved particle swarm optimization algorithms
KW  - National cloud data center
KW  - Quality of service (QoS) guarantees
KW  - Scientific collaboration
ER  - 

TY  - CONF
TI  - CoScale: Coordinating CPU and memory system DVFS in server systems
AU  - Deng, Q.
AU  - Meisner, D.
AU  - Bhattacharjee, A.
AU  - Wenisch, T.F.
AU  - Bianchini, R.
T2  - Proceedings - 2012 IEEE/ACM 45th International Symposium on Microarchitecture, MICRO 2012
AB  - Recent work has introduced memory system dynamic voltage and frequency scaling (DVFS), and has suggested that balanced scaling of both CPU and the memory system is the most promising approach for conserving energy in server systems. In this paper, we first demonstrate that CPU and memory system DVFS often conflict when performed independently by separate controllers. In response, we propose Co Scale, the first method for effectively coordinating these mechanisms under performance constraints. Co Scale relies on execution profiling of each core via (existing and new) performance counters, and models of core and memory performance and power consumption. Co Scale explores the set of possible frequency settings in such a way that it efficiently minimizes the full-system energy consumption within the performance bound. Our results demonstrate that, by effectively coordinating CPU and memory power management, Co Scale conserves a significant amount of system energy compared to existing approaches, while consistently remaining within the prescribed performance bounds. The results also show that Co Scale conserves almost as much system energy as an offline, idealized approach. © 2012 IEEE.
DA  - 2012///
PY  - 2012
DO  - 10.1109/MICRO.2012.22
SP  - 143
EP  - 154
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876526315&doi=10.1109%2fMICRO.2012.22&partnerID=40&md5=58eb4b43654b660b4b23401474a3658a
DB  - Scopus
KW  - Computer architecture
KW  - Energy utilization
KW  - Energy conservation
KW  - energy conservation
KW  - dynamic voltage and frequency scaling
KW  - Dynamic voltage and frequency scaling
KW  - Performance counters
KW  - coordination
KW  - Memory performance
KW  - Memory power management
KW  - Performance bounds
KW  - Performance constraints
KW  - Prescribed performance bounds
ER  - 

TY  - JOUR
TI  - Energy-aware task assignment for mobile cyber-enabled applications in heterogeneous cloud computing
AU  - Gai, K.
AU  - Qiu, M.
AU  - Zhao, H.
T2  - Journal of Parallel and Distributed Computing
AB  - Recent remarkable growth of mobile computing has led to an exceptional hardware upgrade, including the adoption of the multiple core processors. Along with this trend, energy consumptions are becoming greater when the computation capacity or workload grows. As one of the solutions, using cloud computing can mitigate energy costs due to the centralized computation. However, simply offloading the workloads to the remote side cannot efficiently reduce the energy consumptions when the energy costs caused by wireless communications are greater than that of on mobile devices. In this paper, we focus on the energy-saving problem and consider the energy wastes when tasks are assigned to remote cloud servers or heterogeneous core processors. Our solution aims to reduce the total energy cost of the mobile heterogeneous embedded systems by a novel task assignment to heterogeneous cores and mobile clouds. The proposed model is called Energy-Aware Heterogeneous Cloud Management (EA-HCM) model and the main algorithm is Heterogeneous Task Assignment Algorithm (HTA2). Our experimental evaluations have proved that our approach is effective to save energy when deploying heterogeneous embedded systems in mobile cloud systems. © 2017 Elsevier Inc.
DA  - 2018///
PY  - 2018
DO  - 10.1016/j.jpdc.2017.08.001
VL  - 111
SP  - 126
EP  - 135
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028741784&doi=10.1016%2fj.jpdc.2017.08.001&partnerID=40&md5=a2823efa0fbd16efa745da8e5c83f491
DB  - Scopus
KW  - Cloud computing
KW  - Energy conservation
KW  - Energy aware
KW  - Mobile cloud computing
KW  - Energy-aware
KW  - Power management (telecommunication)
KW  - Experimental evaluation
KW  - Cost reduction
KW  - Wireless communications
KW  - NP-hard
KW  - Embedded systems
KW  - Computation capacity
KW  - Energy-saving problems
KW  - Centralized computation
KW  - Cyber-enabled applications
KW  - Heterogeneous embedded system
KW  - Mobile embedded systems
KW  - Task assignment
ER  - 

TY  - JOUR
AU  - Lavallée, Brian
T2  - Data Center Energy: Reducing Your Carbon Footprint | Data Center Knowledge
DA  - 2014///
PY  - 2014
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081759269&partnerID=40&md5=d98cf00e066a6d60f49a7a7c5768e8b5
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Energy Efficient Scheduling of Cloud Application Components with Brownout
AU  - Xu, M.
AU  - Dastjerdi, A.V.
AU  - Buyya, R.
T2  - IEEE Transactions on Sustainable Computing
AB  - It is common for cloud data centers meeting unexpected loads like request bursts, which may lead to overloaded situation and performance degradation. Dynamic Voltage Frequency Scaling and VM consolidation have been proved effective to manage overloads. However, they cannot function when the whole data center is overloaded. Brownout provides a promising direction to avoid overloads through configuring applications to temporarily degrade user experience. Additionally, brownout can also be applied to reduce data center energy consumption. As a complementary option for Dynamic Voltage Frequency Scaling and VM consolidation, our combined brownout approach reduces energy consumption through selectively and dynamically deactivating application optional components, which can also be applied to self-contained microservices. The results show that our approach can save more than 20 percent energy consumption and there are trade-offs between energy saving and discount offered to users. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/TSUSC.2017.2661339
VL  - 1
IS  - 2
SP  - 40
EP  - 43
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081713746&doi=10.1109%2fTSUSC.2017.2661339&partnerID=40&md5=e00446747da4f43b0f291cc8f5ee0154
DB  - Scopus
KW  - Energy efficiency
KW  - microservices
KW  - Green computing
KW  - Economic and social effects
KW  - Energy utilization
KW  - Dynamic frequency scaling
KW  - Voltage scaling
KW  - Application components
KW  - application component
KW  - brownout
KW  - Cloud data centers
KW  - energy efficient
KW  - Energy efficient
ER  - 

TY  - JOUR
T2  - Weaveshop-microservices-demo
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081759485&partnerID=40&md5=6b703dba0fc256353541fe6285ca937e
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Ansible Is Simple IT Automation
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042654270&partnerID=40&md5=e699d71a8dd588cf5f2b444fc17375c3
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Dynamic energy-aware cloudlet-based mobile cloud computing model for green computing
AU  - Gai, K.
AU  - Qiu, M.
AU  - Zhao, H.
AU  - Tao, L.
AU  - Zong, Z.
T2  - Journal of Network and Computer Applications
AB  - Employing mobile cloud computing (MCC) to enable mobile users to acquire benefits of cloud computing by an environmental friendly method is an efficient strategy for meeting current industrial demands. However, the restrictions of wireless bandwidth and device capacity have brought various obstacles, such as extra energy waste and latency delay, when deploying MCC. Addressing this issue, we propose a dynamic energy-aware cloudlet-based mobile cloud computing model (DECM) focusing on solving the additional energy consumptions during the wireless communications by leveraging dynamic cloudlets (DCL)-based model. In this paper, we examine our model by a simulation of practical scenario and provide solid results for the evaluations. The main contributions of this paper are twofold. First, this paper is the first exploration in solving energy waste problems within the dynamic networking environment. Second, the proposed model provides future research with a guideline and theoretical supports. © 2015 Elsevier Ltd. All rights reserved.
DA  - 2016///
PY  - 2016
DO  - 10.1016/j.jnca.2015.05.016
VL  - 59
SP  - 46
EP  - 54
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949627105&doi=10.1016%2fj.jnca.2015.05.016&partnerID=40&md5=5abb99d76ead892a44038e02e5a26544
DB  - Scopus
KW  - Cloud computing
KW  - Power management
KW  - Green computing
KW  - Energy aware
KW  - Mobile cloud computing
KW  - Efficient strategy
KW  - Energy-aware
KW  - Power management (telecommunication)
KW  - Cloudlets
KW  - Dynamic models
KW  - Dynamic program
KW  - Dynamic programs
KW  - Environmental friendly methods
KW  - Wireless bandwidth
KW  - Wireless communications
KW  - Wireless telecommunication systems
ER  - 

TY  - CONF
TI  - Energy efficient scheduling of application components via brownout and approximate markov decision process
AU  - Xu, M.
AU  - Buyya, R.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Unexpected loads in Cloud data centers may trigger overloaded situation and performance degradation. To guarantee system performance, cloud computing environment is required to have the ability to handle overloads. The existing approaches, like Dynamic Voltage Frequency Scaling and VM consolidation, are effective in handling partial overloads, however, they cannot function when the whole data center is overloaded. Brownout has been proved to be a promising approach to relieve the overloads through deactivating application non-mandatory components or microservices temporarily. Moreover, brownout has been applied to reduce data center energy consumption. It shows that there are trade-offs between energy saving and discount offered to users (revenue loss) when one or more services are not provided temporarily. In this paper, we propose a brownout-based approximate Markov Decision Process approach to improve the aforementioned trade-offs. The results based on real trace demonstrate that our approach saves 20% energy consumption than VM consolidation approach. Compared with existing energy-efficient brownout approach, our approach reduces the discount amount given to users while saving similar energy consumption. © Springer International Publishing AG 2017.
DA  - 2017///
PY  - 2017
DO  - 10.1007/978-3-319-69035-3_14
VL  - 10601 LNCS
SP  - 206
EP  - 220
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034095398&doi=10.1007%2f978-3-319-69035-3_14&partnerID=40&md5=9bb690276a53bfcfee72db0e06c927ac
DB  - Scopus
KW  - Energy efficiency
KW  - Microservices
KW  - Markov processes
KW  - Decision making
KW  - Green computing
KW  - Economic and social effects
KW  - Energy utilization
KW  - Dynamic frequency scaling
KW  - Dynamic voltage frequency scaling
KW  - Voltage scaling
KW  - Application component
KW  - Application components
KW  - Brownout
KW  - Cloud computing environments
KW  - Cloud energy efficiency
KW  - Commerce
KW  - Distributed computer systems
KW  - Energy conservation
KW  - Energy-Efficient Scheduling
KW  - Markov decision process
KW  - Markov Decision Processes
KW  - Performance degradation
ER  - 

TY  - JOUR
TI  - Recalibrating global data center energy-use estimates: Growth in energy use has slowed owing to efficiency gains that smart policies can help maintain in the near term
AU  - Masanet, E.
AU  - Shehabi, A.
AU  - Lei, N.
AU  - Smith, S.
AU  - Koomey, J.
T2  - Science
DA  - 2020///
PY  - 2020
DO  - 10.1126/science.aba3758
VL  - 367
IS  - 6481
SP  - 984
EP  - 986
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080913294&doi=10.1126%2fscience.aba3758&partnerID=40&md5=e714910a4e298ff7a30d72190beae6c0
DB  - Scopus
KW  - cloud computing
KW  - energy efficiency
KW  - energy
KW  - data processing
KW  - electricity
KW  - priority journal
KW  - policy
KW  - power supply
KW  - information technology
KW  - calibration
KW  - client server application
KW  - data assimilation
KW  - energy use
KW  - funding
KW  - information storage
KW  - internet protocol
KW  - investment
KW  - policy making
KW  - power usage effectiveness
KW  - Review
ER  - 

TY  - CONF
TI  - Renewable and cooling aware workload management for sustainable data centers
AU  - Liu, Z.
AU  - Chen, Y.
AU  - Bash, C.
AU  - Wierman, A.
AU  - Gmach, D.
AU  - Wang, Z.
AU  - Marwah, M.
AU  - Hyser, C.
T2  - Performance Evaluation Review
AB  - Recently, the demand for data center computing has surged, increasing the total energy footprint of data centers worldwide. Data centers typically comprise three subsystems: IT equipment provides services to customers; power infrastructure supports the IT and cooling equipment; and the cooling infrastructure removes heat generated by these subsystems. This work presents a novel approach to model the energy flows in a data center and optimize its operation. Traditionally, supply-side constraints such as energy or cooling availability were treated independently from IT workload management. This work reduces electricity cost and environmental impact using a holistic approach that integrates renewable supply, dynamic pricing, and cooling supply including chiller and outside air cooling, with IT workload planning to improve the overall sustainability of data center operations. Specifically, we first predict renewable energy as well as IT demand. Then we use these predictions to generate an IT workload management plan that schedules IT workload and allocates IT resources within a data center according to time varying power supply and cooling efficiency. We have implemented and evaluated our approach using traces from real data centers and production systems. The results demonstrate that our approach can reduce both the recurring power costs and the use of non-renewable energy by as much as 60% compared to existing techniques, while still meeting the Service Level Agreements. © 2012 ACM.
DA  - 2012///
PY  - 2012
DO  - 10.1145/2254756.2254779
VL  - 40
SP  - 175
EP  - 186
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864700984&doi=10.1145%2f2254756.2254779&partnerID=40&md5=dfdf71c33e8d6ec1bb243fe5c0840045
DB  - Scopus
KW  - scheduling
KW  - Scheduling
KW  - Information technology
KW  - Data centers
KW  - Optimization
KW  - Information management
KW  - Cooling
KW  - Total energy
KW  - Renewable energies
KW  - Power infrastructures
KW  - Cost reduction
KW  - Computer systems
KW  - Cooling equipment
KW  - Service Level Agreements
KW  - Non-renewable energy
KW  - Time varying
KW  - Air cooling
KW  - Cooling efficiency
KW  - cooling optimization
KW  - Cooling optimization
KW  - Data center operations
KW  - demand shaping
KW  - Dynamic pricing
KW  - Electricity costs
KW  - Energy flow
KW  - Holistic approach
KW  - IT equipment
KW  - IT resources
KW  - Power costs
KW  - Power supply
KW  - Production system
KW  - renewable energy
KW  - sustainable data center
KW  - Workload management
KW  - Workload planning
ER  - 

TY  - JOUR
TI  - An energy-efficient, QoS-aware and cost-effective scheduling approach for real-time workflow applications in cloud computing systems utilizing DVFS and approximate computations
AU  - Stavrinides, G.L.
AU  - Karatza, H.D.
T2  - Future Generation Computer Systems
AB  - Green cloud computing attracts significant attention from both academia and industry. One of the major challenges involved, is to provide a high level of Quality of Service (QoS) in a cost-effective way for the end users and in an energy-efficient manner for the cloud providers. Towards this direction, this paper presents an energy-efficient, QoS-aware and cost-effective scheduling strategy for real-time workflow applications in cloud computing systems. The proposed approach utilizes per-core Dynamic Voltage and Frequency Scaling (DVFS) on the underlying heterogeneous multi-core processors, as well as approximate computations, in order to fill in schedule gaps. At the same time, it takes into account the effects of input error on the processing time of the component tasks. Our goal is to provide timeliness and energy efficiency by trading off result precision, while keeping the result quality of the completed jobs at an acceptable standard and the monetary cost required for the execution of the jobs at a reasonable level. The proposed scheduling heuristic is compared to two other baseline policies, under the impact of various QoS requirements. The simulation experiments reveal that our approach outperforms the other examined policies, providing promising results. © 2019 Elsevier B.V.
DA  - 2019///
PY  - 2019
DO  - 10.1016/j.future.2019.02.019
VL  - 96
SP  - 216
EP  - 226
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061666574&doi=10.1016%2fj.future.2019.02.019&partnerID=40&md5=76a3a60640ec03022fd8f31b42d67a7a
DB  - Scopus
KW  - Cloud computing
KW  - Energy efficiency
KW  - Cost effectiveness
KW  - Scheduling
KW  - Quality of service
KW  - Green computing
KW  - Dynamic frequency scaling
KW  - Voltage scaling
KW  - Job shop scheduling
KW  - Scheduling heuristics
KW  - Work-flows
KW  - Real time systems
KW  - Quality of Service
KW  - Approximate computation
KW  - Approximate computations
KW  - Cost effective scheduling strategy
KW  - Heterogeneous multi core processors
KW  - Per-core DVFS
KW  - QoS requirements
KW  - Real-time workflows
KW  - Workflow applications
ER  - 

TY  - CONF
TI  - DRACeo: A smart simulator to deploy energy saving methods in microservices based networks
AU  - Valera, H.H.A.
AU  - Dalmau, M.
AU  - Roose, P.
AU  - Larracoechea, J.
AU  - Herzog, C.
T2  - Proceedings of the Workshop on Enabling Technologies: Infrastructure for Collaborative Enterprises, WETICE
AB  - Nowadays, many researchers work to identify microservices-based application deployments and scheduling solutions to save energy without decreasing functional QoS. In this work, we present DRACeo: A simulator that allows facing this challenge in a simple and efficient way, enabling its users to focus uniquely on microservices deployment/scheduling algorithms and its hardware/software repercussions (load vs. energy consumption) without worrying about low-level network configurations or operating system issues. DRACeo is able to deploy and schedule (move, duplicate, start/stop) microservices and their dependencies on various devices with software and hardware heterogeneity (CPU, bandwidth, RAM, Battery, etc.), taking into account various scheduling heuristics algorithms: centralized vs non-centralized. To do this, DRACeo allows deploying custom network topologies based on client-server schemes or p2p distributions, where devices can (dis)appear, turn on/off obeying random circumstances or user strategies. Finally, the simulator performs relevant operations such as QoS definition, resource monitoring, calculation of energy saved and consumption tracking (at device and network level). We tested some ideas based on our previous work 'Kaligreen' to demonstrate the effectiveness of DRACeo. © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/WETICE49692.2020.00026
VL  - 2020-September
SP  - 94
EP  - 99
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100714002&doi=10.1109%2fWETICE49692.2020.00026&partnerID=40&md5=6dde8162ca929d1cfeb833956e56d257
DB  - Scopus
KW  - microservices
KW  - middleware
KW  - energy
KW  - Scheduling
KW  - Energy utilization
KW  - Energy conservation
KW  - consumption
KW  - CPU
KW  - hard disk
KW  - network
KW  - prototype
KW  - simulator
KW  - Network topology
KW  - Application deployment
KW  - Computer hardware
KW  - Energy saving methods
KW  - Hardware/software
KW  - Network configuration
KW  - Resource monitoring
KW  - Scheduling heuristics
KW  - Simulators
KW  - Software and hardwares
ER  - 

TY  - JOUR
TI  - Goal-oriented decision modeling: A position paper
AU  - Guizzardi, R.S.
AU  - Perini, A.
T2  - Istar@ Caise
DA  - 2018///
PY  - 2018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132719762&partnerID=40&md5=10fe90c15390f479b4fff311e9efce23
DB  - Scopus
ER  - 

TY  - CONF
TI  - Game-based negotiation between power demand and supply in green datacenters
AU  - Thi, M.-T.
AU  - Pierson, J.-M.
AU  - Da Costa, G.
T2  - Proceedings - 2020 IEEE International Symposium on Parallel and Distributed Processing with Applications, 2020 IEEE International Conference on Big Data and Cloud Computing, 2020 IEEE International Symposium on Social Computing and Networking and 2020 IEEE International Conference on Sustainable Computing and Communications, ISPA-BDCloud-SocialCom-SustainCom 2020
AB  - The power consumption of datacenters is growing rapidly and becoming a major concern. For reducing carbon footprint and increasing energy efficiency, a promising solution is to locally supply datacenters with renewable energies. However, a challenging problem in building such green datacenter is the coordinating between the power demand and the intermittent power supply. To address this problem, we propose to model the green datacenter as two subsystems, namely, Information Technology (IT) subsystem which consumes energy, and electrical subsystem which supplies energy. Then we aim to find an efficient compromise between the power supply and power demand, taking into account the constraints of both subsystems. Based on buyer-supplier game, we introduce a negotiation approach, in which the two subsystems are modeled as the energy buyer and energy supplier. A negotiation algorithm is proposed, allowing the two subsystems to negotiate and reach an efficient trade-off, while respecting their own utility/monetary gain. The algorithm is evaluated in our middleware of renewable energies-powered datacenter. The experimental results show that the proposed algorithm allows the negotiation process to reach stable points. This algorithm also obtains significant improvement in the datacenter's utility and quality of service (QoS), compared to the algorithms in which joint IT-energy management is not considered. © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ISPA-BDCloud-SocialCom-SustainCom51426.2020.00112
SP  - 690
EP  - 697
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108028950&doi=10.1109%2fISPA-BDCloud-SocialCom-SustainCom51426.2020.00112&partnerID=40&md5=3b71c813352ce106a7569fb763e4acbc
DB  - Scopus
KW  - Cloud computing
KW  - Energy efficiency
KW  - Game theory
KW  - Big data
KW  - Carbon footprint
KW  - Quality of service
KW  - Green computing
KW  - Economic and social effects
KW  - Commerce
KW  - Middleware
KW  - Electric power utilization
KW  - Renewable energies
KW  - Power demands
KW  - Social networking (online)
KW  - Power supply
KW  - Electric power systems
KW  - Electrical subsystems
KW  - Energy suppliers
KW  - Green Datacenter
KW  - Negotiation
KW  - Negotiation algorithm
KW  - Negotiation process
KW  - Stable points
ER  - 

TY  - JOUR
TI  - Latency-aware failover strategies for containerized web applications in distributed clouds
AU  - Aldwyan, Y.
AU  - Sinnott, R.O.
T2  - Future Generation Computer Systems
AB  - Despite advances in Cloud computing, ensuring high availability (HA) remains a challenge due to varying loads and the potential for Cloud outages. Deploying applications in distributed Clouds can help overcome this challenge by geo-replicating applications across multiple Cloud data centers (DCs). However, this distributed deployment can be a performance bottleneck due to network latencies between users and DCs as well as inter-DC latencies incurred during the geo-replication process. For most web applications, both HA and Performance (HAP) are essential and need to meet pre-agreed Service Level Objectives (SLOs). Efficiently placing and managing primary and backup replicas of applications in distributed Clouds to achieve HAP is a challenging task. Existing solutions consider either HA or performance but not both. In this paper we propose an approach for automating the process of providing a latency-aware failover strategy through a server placement algorithm leveraging genetic algorithms that factor in the proximity of users and inter-DC latencies. To facilitate the distributed deployment of applications and avoid the overheads of Clouds, we utilize container technologies. To evaluate our proposed approach, we conduct experiments on the Australia-wide National eResearch Collaboration Tools and Resources (NeCTAR - www.nectar.org.au) Research Cloud. Our results show at least a 23.3% and 22.6% improvement in response times under normal and failover conditions respectively compared to traditional, latency-unaware approaches. Also, the 95th percentile of response times in our approach are at most1.5 ms above the SLO compared to 11–32 ms using other approaches. © 2019 Elsevier B.V.
DA  - 2019///
PY  - 2019
DO  - 10.1016/j.future.2019.07.032
VL  - 101
SP  - 1081
EP  - 1095
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069934073&doi=10.1016%2fj.future.2019.07.032&partnerID=40&md5=df05d6c2a0efbc2d6468b98536b5eacf
DB  - Scopus
KW  - Performance
KW  - Containers
KW  - Genetic algorithms
KW  - High availability
KW  - Container technologies
KW  - WEB application
KW  - Web applications
KW  - Cloud outages
KW  - Distributed clouds
KW  - Distributed Clouds
KW  - Distributed deployment
ER  - 

TY  - JOUR
TI  - Reducing the energy consumption of large-scale computing systems through combined shutdown policies with multiple constraints
AU  - Benoit, A.
AU  - Lefèvre, L.
AU  - Orgerie, A.-C.
AU  - Raïs, I.
T2  - International Journal of High Performance Computing Applications
AB  - Large-scale distributed systems (high-performance computing centers, networks, data centers) are expected to consume huge amounts of energy. In order to address this issue, shutdown policies constitute an appealing approach able to dynamically adapt the resource set to the actual workload. However, multiple constraints have to be taken into account for such policies to be applied on real infrastructures: the time and energy cost of switching on and off, the power and energy consumption bounds caused by the electricity grid or the cooling system, and the availability of renewable energy. In this article, we propose models translating these various constraints into different shutdown policies that can be combined for a multiconstraint purpose. Our models and their combinations are validated through simulations on a real workload trace. © 2017, © The Author(s) 2017.
DA  - 2018///
PY  - 2018
DO  - 10.1177/1094342017714530
VL  - 32
IS  - 1
SP  - 176
EP  - 188
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039869052&doi=10.1177%2f1094342017714530&partnerID=40&md5=16390b43f86e710b523417082387d1fb
DB  - Scopus
KW  - simulation
KW  - Green computing
KW  - Energy utilization
KW  - Distributed computer systems
KW  - Energy policy
KW  - High performance computing
KW  - Energy model
KW  - Multiple constraint
KW  - Large-scale distributed system
KW  - energy models
KW  - Large-scale computing systems
KW  - Large-scale distributed systems
KW  - Power and energy consumption
KW  - shutdown policies
KW  - Shutdown policies
ER  - 

TY  - JOUR
TI  - Green IT: A matter of business and information systems engineering?
AU  - Loos, P.
AU  - Nebel, W.
AU  - Gómez, J.M.
AU  - Hasan, H.
AU  - Watson, R.T.
AU  - Brocke, J.V.
AU  - Seidel, S.
AU  - Recker, J.
T2  - Business and Information Systems Engineering
DA  - 2011///
PY  - 2011
DO  - 10.1007/s12599-011-0165-5
VL  - 3
IS  - 4
SP  - 245
EP  - 252
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856547763&doi=10.1007%2fs12599-011-0165-5&partnerID=40&md5=a05fcbd7691c30e7c1bfc3e67ed799de
DB  - Scopus
ER  - 

TY  - JOUR
AU  - Acton, M.
T2  - 2018 Best Practice Guidelines for the EU Code of Conduct on Data Centre Energy Efficiency. Technical Report. EUR 29103 EN
DA  - 2017///
PY  - 2017
VL  - 2018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132728098&partnerID=40&md5=8e92befe639c7eebba74f9cb3ccde267
DB  - Scopus
ER  - 

TY  - JOUR
TI  - A novel energy-aware resource management technique using joint VM and container consolidation approach for green computing in cloud data centers
AU  - Gholipour, N.
AU  - Arianyan, E.
AU  - Buyya, R.
T2  - Simulation Modelling Practice and Theory
AB  - Cloud computing is being rapidly adopted for managing IT services as a notable solution due to diverse beneficiaries such as automatically optimized resource management as well as modern service delivery models. The container as a service has been recently introduced by cloud providers as a new service apart from traditional cloud services. Containers enable applications to run and deploy on isolated virtual space, and the operating system kernel is shared among them. Also, containerization has some attributes such as scalability, highly portable properties, and lightweight, for those reasons, it is applied for running isolated applications. Reducing energy consumption, as well as their CO2 emissions, are great deals for cloud providers. In this direction, consolidation is recommended as a vital energy-aware approach in cloud data centers. Previously, independent virtual machine migration or container migration was proposed in the literature for green computing in cloud data centers. However, this paper proposes a new cloud resource management procedure based on a multi-criteria decision-making method that takes advantage of a joint virtual machine and container migration approach concurrently. The results of simulations using ContainerCloudsim simulator validates the applicability of the proposed approach which shows notable reductions in energy consumption, SLA violation, and number of migrations in comparison with the state-of-the-art algorithms. © 2020 Elsevier B.V.
DA  - 2020///
PY  - 2020
DO  - 10.1016/j.simpat.2020.102127
VL  - 104
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086469770&doi=10.1016%2fj.simpat.2020.102127&partnerID=40&md5=f6c07590171b1d1c04d2407d84f70c54
DB  - Scopus
KW  - Cloud computing
KW  - Resource management
KW  - Containers
KW  - Energy consumption
KW  - Power management
KW  - Decision making
KW  - Network security
KW  - Computer aided software engineering
KW  - Datacenter
KW  - Green computing
KW  - Energy utilization
KW  - Reducing energy consumption
KW  - Resource allocation
KW  - Natural resources management
KW  - Information management
KW  - Virtual machine
KW  - Consolidation
KW  - Containerization
KW  - Virtual machine migrations
KW  - Resource management techniques
KW  - State-of-the-art algorithms
KW  - Energy aware approaches
KW  - Multi-criteria decision making methods
KW  - Operating system kernel
ER  - 

TY  - JOUR
TI  - On Achieving Energy Efficiency and Reducing CO2 Footprint in Cloud Computing
AU  - Wajid, U.
AU  - Cappiello, C.
AU  - Plebani, P.
AU  - Pernici, B.
AU  - Mehandjiev, N.
AU  - Vitali, M.
AU  - Gienger, M.
AU  - Kavoussanakis, K.
AU  - Margery, D.
AU  - Perez, D.G.
AU  - Sampaio, P.
T2  - IEEE Transactions on Cloud Computing
AB  - With the increasing popularity of the cloud computing model and rapid proliferation of cloud infrastructures there are increasing concerns about energy consumption and consequent impact of cloud computing as a contributor to global CO2 emissions. To date, little is known about how to incorporate energy consumption and CO2 concerns into cloud application development and deployment decision models. In this respect, this paper describes an eco-aware approach that relies on the definition, monitoring and utilization of energy and CO2 metrics combined with the use of innovative application scheduling and runtime adaptation techniques to optimize energy consumption and CO2 footprint of cloud applications as well as the underlying infrastructure. The eco-aware approach involves measuring or quantifying the energy consumption and CO2 at different levels of cloud computing, using that information to create scheduling and adaptation techniques that contribute towards reducing the energy consumption and CO2 emissions, and finally testing and validating the developed solutions in a multi-site cloud environment with the help of challenging case study applications. The experimental and validation results show the potential of the eco-aware approach to significantly reduce the CO2 footprint and consequent environmental impact of cloud applications. © 2013 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/TCC.2015.2453988
VL  - 4
IS  - 2
SP  - 138
EP  - 151
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976563351&doi=10.1109%2fTCC.2015.2453988&partnerID=40&md5=8950a4ac081e6f203a8245bdc8c7372b
DB  - Scopus
KW  - Cloud computing
KW  - Energy efficiency
KW  - Power management
KW  - Scheduling
KW  - Carbon footprint
KW  - Carbon dioxide
KW  - Energy utilization
KW  - Application scheduling
KW  - Cloud applications
KW  - Environmental impact
KW  - Cloud infrastructures
KW  - Evaluation
KW  - Energy-aware systems
KW  - Adaptation techniques
KW  - Runtime adaptation
KW  - Scheduling and task partitioning
ER  - 

TY  - JOUR
TI  - Big Data, Big Waste? A Reflection on the Environmental Sustainability of Big Data Initiatives
AU  - Lucivero, F.
T2  - Science and Engineering Ethics
AB  - This paper addresses a problem that has so far been neglected by scholars investigating the ethics of Big Data and policy makers: that is the ethical implications of Big Data initiatives’ environmental impact. Building on literature in environmental studies, cultural studies and Science and Technology Studies, the article draws attention to the physical presence of data, the material configuration of digital service, and the space occupied by data. It then explains how this material and situated character of data raises questions concerning the ethics of the increasingly fashionable Big Data discourses. It argues that attention should be paid to (1) the vocabulary currently used when discussing the governance of data initiatives; (2) the internal tension between current data initiatives and environmental policies; (3) issues of fair distribution. The article explains how taking into account these aspects would allow for a more responsible behaviour in the context of data storage and production. © 2019, The Author(s).
DA  - 2020///
PY  - 2020
DO  - 10.1007/s11948-019-00171-7
VL  - 26
IS  - 2
SP  - 1009
EP  - 1030
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077193813&doi=10.1007%2fs11948-019-00171-7&partnerID=40&md5=3c4562e342b30f8c1641e0966ddb6eb2
DB  - Scopus
KW  - Big Data
KW  - Sustainability
KW  - Technology
KW  - Environmental impacts
KW  - Materiality
KW  - Responsibility
KW  - technology
ER  - 

TY  - JOUR
TI  - OMG: Decision model and notation version 1.3
T2  - OMG
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132781047&partnerID=40&md5=986acfbc393a1b6137cf90fd9dac5d8f
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Data center maturity model. Techn. Ber
AU  - Singh, H.
T2  - The Green Grid
DA  - 2011///
PY  - 2011
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124062848&partnerID=40&md5=14bf3faf41290d038c42d365ae3b7975
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Minimizing Energy Cost for Green Data Center by Exploring Heterogeneous Energy Resource
AU  - Hu, X.
AU  - Li, P.
AU  - Sun, Y.
T2  - Journal of Modern Power Systems and Clean Energy
AB  - With the deteriorating effects resulting from global warming in many areas, geographically distributed data centers contribute greatly to carbon emissions, because the major energy supply is fossil fuels. Considering this issue, many geographically distributed data centers are attempting to use clean energy as their energy supply, such as fuel cells and renewable energy sources. However, not all workloads can be powered by a single power sources, since different workloads exhibit different characteristics. In this paper, we propose a fine-grained heterogeneous power distribution model with an objective of minimizing the total energy costs and the sum of the energy gap generated by the geographically distributed data centers powered by multiple types of energy resources. In order to achieve these two goals, we design a two-stage online algorithm to leverage the power supply of each energy source. In each time slot, we also consider a chance-constraint problem and use the Bernstein approximation to solve the problem. Finally, simulation results based on real-world traces illustrate that the proposed algorithm can achieve satisfactory performance.  © 2021 State Grid Electric Power Research Institute.
DA  - 2021///
PY  - 2021
DO  - 10.35833/MPCE.2019.000052
VL  - 9
IS  - 1
SP  - 148
EP  - 159
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100023467&doi=10.35833%2fMPCE.2019.000052&partnerID=40&md5=381fb359c26bf2289314afb380e5b16d
DB  - Scopus
KW  - Carbon emissions
KW  - Green computing
KW  - Data center
KW  - Renewable energy resources
KW  - On-line algorithms
KW  - Minimizing energy
KW  - Green data centers
KW  - Fossil fuels
KW  - energy management
KW  - Renewable energy source
KW  - Global warming
KW  - Bernstein approximation
KW  - Chance constraint
KW  - Distributed data
KW  - Fossil fuel power plants
KW  - Fuel cells
KW  - heterogeneous energy resources
KW  - power distribution algorithm
KW  - Power distribution modeling
ER  - 

TY  - JOUR
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132753568&partnerID=40&md5=65010e3a8816238a36c1ba1ffad3f862
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Energy usage reports: Environmental awareness as part of algorithmic accountability
AU  - Lottick, K.
AU  - Susai, S.
AU  - Friedler, S.A.
AU  - Wilson, J.P.
T2  - Energy usage reports: Environmental awareness as part of algorithmic accountability
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102644245&partnerID=40&md5=2893d7fe594fc83783af1041ab63593a
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Business Process Model and Notation (BPMN) Version 2.0
DA  - 2011///
PY  - 2011
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79956069069&partnerID=40&md5=91913c5f0d92775fa13cc2d3d2285d8a
DB  - Scopus
ER  - 

TY  - CONF
TI  - Parasol and greenswitch: Managing datacenters powered by renewable energy
AU  - Goiri, I.
AU  - Katsak, W.
AU  - Ley, K.
AU  - Nguyen, T.D.
AU  - Bianchini, R.
T2  - ACM SIGPLAN Notices
AB  - Several companies have recently announced plans to build "green" datacenters, i.e. datacenters partially or completely powered by renewable energy. These datacenters will either generate their own renewable energy or draw it directly from an existing nearby plant. Besides reducing carbon footprints, renewable energy can potentially reduce energy costs, reduce peak power costs, or both. However, certain renewable fuels are intermittent, which requires approaches for tackling the energy supply variability. One approach is to use batteries and/or the electrical grid as a backup for the renewable energy. It may also be possible to adapt the workload to match the renewable energy supply. For highest benefits, green datacenter operators must intelligently manage their workloads and the sources of energy at their disposal. In this paper, we first discuss the tradeoffs involved in building green datacenters today and in the future. Second, we present Parasol, a prototype green datacenter that we have built as a research platform. Parasol comprises a small container, a set of solar panels, a battery bank, and a grid-tie. Third, we describe GreenSwitch, our model-based approach for dynamically scheduling the workload and selecting the source of energy to use. Our real experiments with Parasol, GreenSwitch, and MapReduce workloads demonstrate that intelligent workload and energy source management can produce significant cost reductions. Our results also isolate the cost implications of peak power management, storing energy on the grid, and the ability to delay the MapReduce jobs. Finally, our results demonstrate that careful workload and energy source management can minimize the negative impact of electrical grid outages.
DA  - 2013///
PY  - 2013
DO  - 10.1145/2499368.2451123
VL  - 48
SP  - 51
EP  - 63
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880122979&doi=10.1145%2f2499368.2451123&partnerID=40&md5=5963c44ef24b0b8265b88d4b48e45b87
DB  - Scopus
KW  - Datacenters
KW  - Scheduling
KW  - Carbon footprint
KW  - Data centers
KW  - Batteries
KW  - Renewable energies
KW  - Renewable energy resources
KW  - Model based approach
KW  - Cost reduction
KW  - Research platforms
KW  - Renewable energy
KW  - Electrical grids
KW  - Outages
KW  - Small containers
KW  - Solar cells
KW  - Source of energy
KW  - Sources of energy
ER  - 

TY  - JOUR
TI  - Green information systems: Directives for the IS discipline
AU  - Vom Brocke, J.
AU  - Watson, R.T.
AU  - Dwyer, C.
AU  - Elliot, S.
AU  - Melville, N.
T2  - Communications of the Association for Information Systems
AB  - Green IS offers the promise for IS scholars to make a significant contribution to reducing greenhouse gas emissions and mitigating the effects of global climate change and other environmental problems. While significant achievements have been made in shaping Green IS as a subfield in the IS discipline, the emergence of Green IS is still by far too slow, given the magnitude of the problem. Against this background a panel was organized at ICIS 2012 in order to discuss future directives for the IS discipline. This article, co-authored by the panelists, reports on the major issues raised by this panel. First, the article gives an account of major achievements in the field of Green IS. Second, it presents five specific directives which we agree are important for the future of our discipline. © 2013 by the Association for Information Systems.
DA  - 2013///
PY  - 2013
DO  - 10.17705/1cais.03330
VL  - 33
IS  - 1
SP  - 509
EP  - 520
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892159927&doi=10.17705%2f1cais.03330&partnerID=40&md5=2005e85fdadc1fda6f90a563e1549d00
DB  - Scopus
KW  - Sustainable development
KW  - Greenhouse gases
KW  - Climate change
KW  - Sustainability
KW  - Gas emissions
KW  - Business transformation
KW  - Business transformations
KW  - Energy informatics
KW  - Environmental problems
KW  - Global climate changes
KW  - Green information systems
KW  - Green is
KW  - Green IS
KW  - Is disciplines
ER  - 

TY  - JOUR
TI  - Energy-efficient datacenters
AU  - Pedram, M.
T2  - IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems
AB  - Pervasive use of cloud computing and the resulting rise in the number of datacenters and hosting centers (that provide platform or software services to clients who do not have the means to set up and operate their own computing facilities) have brought forth many concerns, including the electrical energy cost, peak power dissipation, cooling, and carbon emission. With power consumption becoming an increasingly important issue for the operation and maintenance of the hosting centers, corporate and business owners are becoming increasingly concerned. Furthermore, provisioning resources in a cost-optimal manner so as to meet different performance criteria, such as throughput or response time, has become a critical challenge. The goal of this paper is to provide an introduction to resource provisioning and power or thermal management problems in datacenters, and to review strategies that maximize the datacenter energy efficiency subject to peak or total power consumption and thermal constraints, while meeting stipulated service level agreements in terms of task throughput and/or response time. © 2012 IEEE.
DA  - 2012///
PY  - 2012
DO  - 10.1109/TCAD.2012.2212898
VL  - 31
IS  - 10
SP  - 1465
EP  - 1484
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866609789&doi=10.1109%2fTCAD.2012.2212898&partnerID=40&md5=da8813937e1c7f70fcfef51e520f0804
DB  - Scopus
KW  - Energy efficiency
KW  - resource management
KW  - Resource management
KW  - green computing
KW  - Datacenter
KW  - Temperature control
KW  - Dynamic Power
KW  - dynamic power and thermal management
KW  - energy efficient design
KW  - Energy-efficient design
KW  - enterprise computing
KW  - Enterprise computing
ER  - 

TY  - JOUR
TI  - A microservice composition approach based on the choreography of BPMN fragments
AU  - Valderas, P.
AU  - Torres, V.
AU  - Pelechano, V.
T2  - Information and Software Technology
AB  - Context: Microservices must be composed to provide users with complex and elaborated functionalities. It seems that the decentralized nature of microservices makes a choreography style more appropriate to achieve such cooperation, where lighter solutions based on asynchronous events are generally used. However, a microservice composition based on choreography distributes the flow logic of the composition among microservices making further analysis and updating difficult, i.e. there is not a big picture of the composition that facilitates these tasks. Business Process Model and Notation (BPMN) is the OMG standard developed to represent Business Processes (BPs), being widely used to define the big picture of such compositions. However, BPMN is usually considered in orchestration-based solutions, and orchestration can be a drawback to achieve the decoupling pursued by a microservice architecture. Objective: Defining a microservice composition approach that allows us to create a composition in a BPMN model, which facilitates further analysis for taking engineering decisions, and execute them through an event-based choreography to have a high degree of decoupling and independence among microservices. Method: We followed a research methodology for information systems that consists of a 5-step process: awareness of the problem, suggestion, development, evaluation, and conclusion. Results: We presented a microservice composition approach based on the choreography of BPMN fragments. On the one hand, we propose to describe the big picture of the composition with a BPMN model, providing a valuable mechanism to analyse it when engineering decisions need to be taken. On the other hand, this model is split into fragments in order to be executed through an event-based choreography form, providing the high degree of decoupling among microservices demanded in this type of architecture. This composition approach is supported by a microservice architecture defined to achieve that both descriptions of a composition (big picture and split one) coexist. A realization of this architecture in Java/Spring technology is also presented. Conclusions: The evaluation that is done to our work allows us to conclude that the proposed approach for composing microservices is more efficient than solutions based on ad-hoc development. © 2020
DA  - 2020///
PY  - 2020
DO  - 10.1016/j.infsof.2020.106370
VL  - 127
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086996899&doi=10.1016%2fj.infsof.2020.106370&partnerID=40&md5=262618c8ab816ad2e63461161ea2c11c
DB  - Scopus
KW  - Microservices
KW  - Architecture
KW  - Business Process
KW  - AND splits
KW  - Asynchronous event
KW  - BPMN
KW  - Business process model and notation (BPMN)
KW  - Choreography
KW  - Composition
KW  - Engineering decisions
KW  - Event-based
KW  - Flow logic
KW  - Image analysis
KW  - Research methodologies
ER  - 

TY  - JOUR
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132727055&partnerID=40&md5=dca938c54289a58318ac660a9f5664ad
DB  - Scopus
ER  - 

TY  - CONF
TI  - Business process co-design for energy-aware adaptation
AU  - Cappiello, C.
AU  - Fugini, M.
AU  - Ferreira, A.M.
AU  - Plebani, P.
AU  - Vitali, M.
T2  - Proceedings - 2011 IEEE 7th International Conference on Intelligent Computer Communication and Processing, ICCP 2011
AB  - Green IT mainly focuses on techniques to extend the products longevity or to virtualise physical resources as well as the provision of energy efficient hardware infrastructures. Less attention has been paid on the applications that run on the machines and their impact on energy consumption. This paper proposes an approach for enabling an efficient use of energy driven by the design of energy-aware business processes. Energy-awareness is given by an enrichment of a typical Business Process conceptual model with annotations able to support the assessment of the energy consumption of the involved business tasks. This information is the basis for the energy-aware adaptation to enact specific strategies to adapt process execution in case energy consumption needs to be lowered or energy leakages have been identified. © 2011 IEEE.
DA  - 2011///
PY  - 2011
DO  - 10.1109/ICCP.2011.6047917
SP  - 463
EP  - 470
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755125694&doi=10.1109%2fICCP.2011.6047917&partnerID=40&md5=f8c79e0bf9ccbdb071ad01f6414bd3a1
DB  - Scopus
KW  - Energy efficiency
KW  - Resource management
KW  - Energy utilization
KW  - Energy efficient
KW  - Energy aware
KW  - Service oriented architecture (SOA)
KW  - Green IT
KW  - Information services
KW  - Context-Aware
KW  - Co-designs
KW  - Computer hardware
KW  - Wireless sensor networks
KW  - Physical resources
KW  - Business Process
KW  - Energy-awareness
KW  - Service Oriented
KW  - Adaptive and context-aware processes
KW  - Conceptual model
KW  - Efficient use of energy
KW  - Energy leakage
KW  - Green IT and energy-aware applications
KW  - Process execution
KW  - Resource management in business process execution
KW  - Service-oriented architectures for BPM
ER  - 

TY  - JOUR
TI  - Goal-oriented requirements engineering: an extended systematic mapping study
AU  - Horkoff, J.
AU  - Aydemir, F.B.
AU  - Cardoso, E.
AU  - Li, T.
AU  - Maté, A.
AU  - Paja, E.
AU  - Salnitri, M.
AU  - Piras, L.
AU  - Mylopoulos, J.
AU  - Giorgini, P.
T2  - Requirements Engineering
AB  - Over the last two decades, much attention has been paid to the area of goal-oriented requirements engineering (GORE), where goals are used as a useful conceptualization to elicit, model, and analyze requirements, capturing alternatives and conflicts. Goal modeling has been adapted and applied to many sub-topics within requirements engineering (RE) and beyond, such as agent orientation, aspect orientation, business intelligence, model-driven development, and security. Despite extensive efforts in this field, the RE community lacks a recent, general systematic literature review of the area. In this work, we present a systematic mapping study, covering the 246 top-cited GORE-related conference and journal papers, according to Scopus. Our literature map addresses several research questions: we classify the types of papers (e.g., proposals, formalizations, meta-studies), look at the presence of evaluation, the topics covered (e.g., security, agents, scenarios), frameworks used, venues, citations, author networks, and overall publication numbers. For most questions, we evaluate trends over time. Our findings show a proliferation of papers with new ideas and few citations, with a small number of authors and papers dominating citations; however, there is a slight rise in papers which build upon past work (implementations, integrations, and extensions). We see a rise in papers concerning adaptation/variability/evolution and a slight rise in case studies. Overall, interest in GORE has increased. We use our analysis results to make recommendations concerning future GORE research and make our data publicly available. © 2017, The Author(s).
DA  - 2019///
PY  - 2019
DO  - 10.1007/s00766-017-0280-z
VL  - 24
IS  - 2
SP  - 133
EP  - 160
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029538454&doi=10.1007%2fs00766-017-0280-z&partnerID=40&md5=1d72593d98289d844a0cb85d2815b55b
DB  - Scopus
KW  - Systematic mapping study
KW  - Systematic literature review
KW  - Mapping
KW  - Paper
KW  - Object oriented programming
KW  - Systematic mapping studies
KW  - Requirements engineering
KW  - Aspect orientation
KW  - Goal model
KW  - Goal modeling
KW  - Goal-oriented requirements engineering
KW  - GORE
KW  - Model driven development
KW  - Research questions
ER  - 

TY  - JOUR
TI  - Green versus brown: Comparing the employment impacts of energy efficiency, renewable energy, and fossil fuels using an input-output model
AU  - Garrett-Peltier, H.
T2  - Economic Modelling
AB  - Global carbon emissions have reached unsustainable levels, and transforming the energy sector by increasing efficiency and use of renewables is one of the primary strategies to reduce emissions. Policy makers need to understand both the environmental and economic impacts of fiscal and regulatory policies regarding the energy sector. Transitioning to lower-carbon energy will entail a contraction of the fossil fuel sector, along with a loss of jobs. An important question is whether clean energy will create more jobs than will be lost in fossil fuels. This article presents a method of using Input-Output (I-O) tables to create “synthetic” industries – namely clean energy industries that do not currently exist in I-O tables. This approach allows researchers to evaluate public and private spending in clean energy and compare it to the effects of spending on fossil fuels. Here we focus on employment impacts in the short-to-medium term, and leave aside the long-term comparison of operations and maintenance employment. We find that on average, 2.65 full-time-equivalent (FTE) jobs are created from $1 million spending in fossil fuels, while that same amount of spending would create 7.49 or 7.72 FTE jobs in renewables or energy efficiency. Thus each $1 million shifted from brown to green energy will create a net increase of 5 jobs. © 2016 Elsevier Ltd
DA  - 2017///
PY  - 2017
DO  - 10.1016/j.econmod.2016.11.012
VL  - 61
SP  - 439
EP  - 447
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007467164&doi=10.1016%2fj.econmod.2016.11.012&partnerID=40&md5=f04db1592f0a96b8c6becba5923b071f
DB  - Scopus
KW  - Energy efficiency
KW  - Renewable energy
KW  - Employment multipliers
KW  - Fiscal policy
KW  - Fossil Fuels
KW  - Input-output
ER  - 

TY  - JOUR
TI  - Augmenting processes with decision intelligence: Principles for integrated modelling
AU  - Hasić, F.
AU  - De Smedt, J.
AU  - Vanthienen, J.
T2  - Decision Support Systems
AB  - Until recently decisions were mostly modelled within the process. Such an approach was shown to impair the maintainability, scalability, and flexibility of both processes and decisions. Lately, literature is moving towards a separation of concerns between the process and decision model. Most notably, the introduction of the Decision Model and Notation (DMN) standard provides a suitable solution for filling the void of decision representation. This raises the question whether decisions and processes can easily be separated and consistently integrated. We introduce an integrated way of modelling the process, while providing a decision model which encompasses the process in its entirety, rather than focusing on local decision points only. Specifically, this paper contributes formal definitions for decision models and for the integration of processes and decisions. Additionally, inconsistencies between process and decision models are identified and we remedy those inconsistencies by establishing Five Principles for integrated Process and Decision Modelling (5PDM). The principles are subsequently illustrated and validated on a case of a Belgian accounting company. © 2017 Elsevier B.V.
DA  - 2018///
PY  - 2018
DO  - 10.1016/j.dss.2017.12.008
VL  - 107
SP  - 1
EP  - 12
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039774541&doi=10.1016%2fj.dss.2017.12.008&partnerID=40&md5=a53e3647e0f9625efad6fc58eade7a54
DB  - Scopus
KW  - Information systems
KW  - BPMN
KW  - Decision modelling
KW  - Decision support systems
KW  - DMN
KW  - Integrated modelling
KW  - Process modelling
KW  - Separation of concerns
ER  - 

TY  - JOUR
TI  - Understanding cloud-native applications after 10 years of cloud computing - A systematic mapping study
AU  - Kratzke, N.
AU  - Quint, P.-C.
T2  - Journal of Systems and Software
AB  - It is common sense that cloud-native applications (CNA) are intentionally designed for the cloud. Although this understanding can be broadly used it does not guide and explain what a cloud-native application exactly is. The term “cloud-native” was used quite frequently in birthday times of cloud computing (2006) which seems somehow obvious nowadays. But the term disappeared almost completely. Suddenly and in the last years the term is used again more and more frequently and shows increasing momentum. This paper summarizes the outcomes of a systematic mapping study analyzing research papers covering “cloud-native” topics, research questions and engineering methodologies. We summarize research focuses and trends dealing with cloud-native application engineering approaches. Furthermore, we provide a definition for the term “cloud-native application” which takes all findings, insights of analyzed publications and already existing and well-defined terminology into account. © 2017 Elsevier Inc.
DA  - 2017///
PY  - 2017
DO  - 10.1016/j.jss.2017.01.001
VL  - 126
SP  - 1
EP  - 16
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009186306&doi=10.1016%2fj.jss.2017.01.001&partnerID=40&md5=00833465689b49e90c3ae3861f0862ab
DB  - Scopus
KW  - Cloud computing
KW  - Microservice
KW  - Systematic mapping study
KW  - Mapping
KW  - Systematic mapping studies
KW  - Cloud-native application
KW  - CNA
KW  - Elastic platform
KW  - Pattern
KW  - Self service
KW  - Softwareization
ER  - 

TY  - JOUR
TI  - A survey on energy efficiency in information systems
AU  - Vitali, M.
AU  - Pernici, B.
T2  - International Journal of Cooperative Information Systems
AB  - Concerns about energy and sustainability are growing everyday involving a wide range of fields. Even Information Systems (ISs) are being influenced by the issue of reducing pollution and energy consumption and new fields are rising dealing with this topic. One of these fields is Green Information Technology (IT), which deals with energy efficiency with a focus on IT. Researchers have faced this problem according to several points of view. The purpose of this paper is to understand the trends and the future development of Green IT by analyzing the state-of-the-art and classifying existing approaches to understand which are the components that have an impact on energy efficiency in ISs and how this impact can be reduced. At first, we explore some guidelines that can help to understand the efficiency level of an organization and of an IS. Then, we discuss measurement and estimation of energy efficiency and identify which are the components that mainly contribute to energy waste and how it is possible to improve energy efficiency, both at the hardware and at the software level. © 2014 World Scientific Publishing Company.
DA  - 2014///
PY  - 2014
DO  - 10.1142/S0218843014500014
VL  - 23
IS  - 3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906266660&doi=10.1142%2fS0218843014500014&partnerID=40&md5=7c3bad7cd71ee7d53e7f9b1e8ba4cc2d
DB  - Scopus
KW  - Energy efficiency
KW  - Green IT
KW  - Green IS
KW  - adaptivity
KW  - assessment
KW  - Information Systems
ER  - 

TY  - CONF
TI  - Annotating executable DSLs with energy estimation formulas
AU  - Béziers La Fosse, T.
AU  - Tisi, M.
AU  - Mottu, J.-M.
AU  - Sunyé, G.
T2  - SLE 2020 - Proceedings of the 13th ACM SIGPLAN International Conference on Software Language Engineering, Co-located with SPLASH 2020
AB  - Reducing the energy consumption of a complex, especially cyber-physical, system is a cross-cutting concern through the system layers, and typically requires long feedback loops between experts in several engineering disciplines. Having an immediate automatic estimation of the global system consumption at design-time would significantly accelerate this process, but cross-layer tools are missing in several domains. Executable domain-specific modeling languages (xDSLs) can be used to design several layers of the system under development in an integrated view. By including the behavioral specification for software and physical components of the system, they are an effective source artifact for cross-layer energy estimation. In this paper we propose EEL, a language for annotating xDSL primitives with energy-related properties, i.e. how their execution would contribute to the energy consumption on a specific runtime platform. Given an xDSL, energy specialists create EEL models of that xDSL for each considered runtime platform. The models are used at design time, to predict the energy consumption of the real systems. This avoids the need of energetic analysis by deployment and measurement on all runtime platforms, that is slow and expensive. We augment an existing language workbench for xDSLs with an editor for EEL models and a component that computes energy-consumption estimations during model editing. The evaluation shows that EEL can be used to represent estimation models from literature, and provide useful predictions.  © 2020 ACM.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3426425.3426930
SP  - 22
EP  - 38
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097711159&doi=10.1145%2f3426425.3426930&partnerID=40&md5=81c27a72ed30adb5ec3fce4903105aa9
DB  - Scopus
KW  - cyber-physical systems
KW  - DSL
KW  - Energy utilization
KW  - Real time systems
KW  - Modeling languages
KW  - Automatic estimation
KW  - Behavioral specification
KW  - Computer programming languages
KW  - Cross-cutting concerns
KW  - Digital subscriber lines
KW  - Domain specific modeling languages
KW  - energy estimation
KW  - Energy specialists
KW  - Engineering disciplines
KW  - Language workbenches
KW  - Physical components
KW  - Specification languages
KW  - xDSL
ER  - 

TY  - JOUR
TI  - Energy efficient placement of workloads in composable data center networks
AU  - Ajibola, O.O.
AU  - El-Gorashi, T.
AU  - Elmirghani, J.
T2  - Journal of Lightwave Technology
AB  - This paper studies the energy efficiency of composable data center (DC) infrastructures over network topologies. Using a mixed integer linear programming (MILP) model, we compare the performance of disaggregation at rack-scale and pod-scale over selected electrical, optical and hybrid network topologies relative to a traditional DC. Relative to a pod-scale DC, the results show that physical disaggregation at rack-scale is sufficient for optimal efficiency when the optical network topology is adopted, and resource components are allocated in a suitable manner. The optical network topology also enables optimal energy efficiency in composable DCs. The paper also studies logical disaggregation of traditional DC servers over an optical network topology. Relative to physical disaggregation at rack-scale, logical disaggregation of server resources within each rack enables marginal fall in the total DC power consumption (TDPC) due to improved resource demands placement. Hence, an adaptable composable infrastructure that can support both in memory (access) latency sensitive and insensitive workloads is enabled. We also conduct a study of the adoption of micro-service architecture in both traditional and composable DCs. Our results show that increasing the modularity of workloads improves the energy efficiency in traditional DCs, but disproportionate utilization of DC resources persists. A combination of disaggregation and micro-services achieved up to 23% reduction in the TDPC of the traditional DC by enabling optimal resources utilization and energy efficiencies. Finally, we propose a heuristic for energy efficient placement of workloads in composable DCs which replicates the trends produced by the MILP model formulated in this paper.  © 1983-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/JLT.2021.3063325
VL  - 39
IS  - 10
SP  - 3037
EP  - 3063
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102297167&doi=10.1109%2fJLT.2021.3063325&partnerID=40&md5=450f3edf04ce174688cd5b761aa2e04c
DB  - Scopus
KW  - Energy efficiency
KW  - Topology
KW  - Green computing
KW  - Energy efficient
KW  - Network topology
KW  - Composable infrastructures
KW  - Integer programming
KW  - Resources utilizations
KW  - Resource demands
KW  - Data center networks
KW  - DC power consumption
KW  - Energy efficient data centers
KW  - Fiber optic networks
KW  - Micro-services
KW  - Milp
KW  - Mixed integer linear programming model
KW  - Optical networks
KW  - Optimal efficiency
KW  - Rack-scale data center
KW  - Software defined infrastructures
ER  - 

TY  - JOUR
TI  - Pattern-driven green adaptation of process-based applications and their runtime infrastructure
AU  - Nowak, A.
AU  - Binz, T.
AU  - Fehling, C.
AU  - Kopp, O.
AU  - Leymann, F.
AU  - Wagner, S.
T2  - Computing
AB  - Business processes are a key aspect of modern organization. In recent years, business process management and optimization has been applied to different cross-cutting concerns such as security, compliance, or Green IT, for example. Based on the ecological characteristics of a business process, proper environmentally sustainable adaptation strategies can be chosen to improve the total environmental impact of the business process. We use ecological sustainable adaptation strategies that are described as green business process patterns. The application of such a green business process pattern, however, affects the business process layer, the application component and the infrastructure layer. This implies that changes in the application infrastructure also need to be considered. Hence, we use best practices of cloud application architectures which are described as Cloud patterns. To guide developers through the adaptation process we propose a pattern-based approach in this work. We correlate Cloud patterns relevant for sustainable business processes to green business process patterns and organize them within a classification. To provide concrete implementation supportwe further annotate these Cloud patterns to application component models that are described with the topology and orchestration specification for cloud applications (TOSCA). Using these annotations, we describe a method that provides the means to optimize business processes based on green business process patterns through adapting the implementation of application components with concrete TOSCA implementation models. © Springer-Verlag 2012.
DA  - 2012///
PY  - 2012
DO  - 10.1007/s00607-012-0188-x
VL  - 94
IS  - 6
SP  - 463
EP  - 487
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864571412&doi=10.1007%2fs00607-012-0188-x&partnerID=40&md5=8636a305081456245beee386c74a5f77
DB  - Scopus
KW  - Information technology
KW  - Sustainable development
KW  - Optimization
KW  - Green IT
KW  - Business Process
KW  - Cloud pattern
KW  - Adaptation of applications
KW  - Ecological sustainable business processes
KW  - Ecology
KW  - Enterprise resource management
KW  - Green business process pattern
KW  - Sustainable business
KW  - TOSCA
ER  - 

TY  - JOUR
TI  - A Black-box Monitoring Approach to Measure Microservices Runtime Performance
AU  - Brondolin, R.
AU  - Santambrogio, M.D.
T2  - ACM Transactions on Architecture and Code Optimization
AB  - Microservices changed cloud computing by moving the applications' complexity from one monolithic executable to thousands of network interactions between small components. Given the increasing deployment sizes, the architectural exploitation challenges, and the impact on data-centers' power consumption, we need to efficiently track this complexity. Within this article, we propose a black-box monitoring approach to track microservices at scale, focusing on architectural metrics, power consumption, application performance, and network performance. The proposed approach is transparent w.r.t. the monitored applications, generates less overhead w.r.t. black-box approaches available in the state-of-the-art, and provides fine-grain accurate metrics. © 2020 ACM.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3418899
VL  - 17
IS  - 4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097221697&doi=10.1145%2f3418899&partnerID=40&md5=a35a5b1cfd13bccdec489a30d18ccd45
DB  - Scopus
KW  - cloud computing
KW  - docker
KW  - kubernetes
KW  - Microservices
KW  - network performance monitoring
KW  - performance monitoring
KW  - power attribution
KW  - Data centers
KW  - Green computing
KW  - Electric power utilization
KW  - State of the art
KW  - Complex networks
KW  - Application performance
KW  - Black box approach
KW  - Run-time performance
KW  - Monitoring approach
KW  - Network interaction
KW  - Small components
ER  - 

TY  - JOUR
TI  - Datazero: Datacenter with zero emission and robust management using renewable energy
AU  - Pierson, J.-M.
AU  - Baudic, G.
AU  - Caux, S.
AU  - Celik, B.
AU  - Da Costa, G.
AU  - Grange, L.
AU  - Haddad, M.
AU  - Lecuivre, J.
AU  - Nicod, J.-M.
AU  - Philippe, L.
AU  - Rehn-Sonigo, V.
AU  - Roche, R.
AU  - Rostirolla, G.
AU  - Sayah, A.
AU  - Stolf, P.
AU  - Thi, M.-T.
AU  - Varnier, C.
T2  - IEEE Access
AB  - As the need for cloud services has been growing steadily, the size and energy consumption of datacenters have increased significantly over the past years. Due to economic and environmental constraints, energy efficiency in datacenters and greenhouse emissions have become a major concern. Renewable energy is widely seen as a promising solution to supply datacenters using local energy, without greenhouse gas emissions. However, the intermittent power generation resulting from the use of renewable energy imposes a paradigm change in the way energy and computation activities are managed. On the one hand, service placement and scheduling may be used on the IT (information technologies) side to adapt to the available power. On the other hand, the storage units may be used to lessen power generation variations. Existing literature and actual deployment mainly design optimization algorithms including the entire system (from cloud service to electrical management, the latter often being neglected or simplified). Conversely to these approaches, we propose a solution where each side optimizes its own objectives, both interacting through a negotiation loop process to reach a common agreement. In this paper, we present DATAZERO, a project developing this idea to ensure high availability of IT services, avoiding unnecessary redundancies, under the constraints due to the intermittent nature of electrical and cloud services flows. © 2019 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.
DA  - 2019///
PY  - 2019
DO  - 10.1109/ACCESS.2019.2930368
VL  - 7
SP  - 103209
EP  - 103230
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076226061&doi=10.1109%2fACCESS.2019.2930368&partnerID=40&md5=7b284126f13bafd903b5e1137bd6f5dc
DB  - Scopus
KW  - Energy efficiency
KW  - Greenhouse gases
KW  - Green computing
KW  - Optimization
KW  - Energy utilization
KW  - Middleware
KW  - Web services
KW  - High availability
KW  - Renewable energies
KW  - Greenhouse emissions
KW  - Gas emissions
KW  - Service placements
KW  - Use of renewable energies
KW  - Negotiation
KW  - Cloud datacenters
KW  - Common agreement
KW  - Design optimization
KW  - Environmental constraints
KW  - Power models
ER  - 

TY  - JOUR
TI  - Cloud-Native Applications
AU  - Gannon, D.
AU  - Barga, R.
AU  - Sundaresan, N.
T2  - IEEE Cloud Computing
AB  - The term 'cloud-native' refers to a set of technologies and design patterns that have become the standard for building large-scale cloud applications. In this editorial we describe basic properties of successful cloud applications including dynamic scalability, extreme fault tolerance, seamless upgradeability and maintenance and security. To make it possible to build applications that meet these requirements we describe the microservice architecture and serverless computing foundation that are central to cloud-native design. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/MCC.2017.4250939
VL  - 4
IS  - 5
SP  - 16
EP  - 21
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038610210&doi=10.1109%2fMCC.2017.4250939&partnerID=40&md5=198ead132dc790dd17c30a4142e93127
DB  - Scopus
KW  - cloud computing
KW  - Cloud computing
KW  - microservices
KW  - serverless
KW  - distributed computing
KW  - Design Patterns
KW  - cloud-native
KW  - Distributed computer systems
KW  - Cloud applications
KW  - Fault tolerance
KW  - Upgradeability
ER  - 

TY  - JOUR
TI  - Extending BPMN 2.0 for intraoperative workflow modeling with IEEE 11073 SDC for description and orchestration of interoperable, networked medical devices
AU  - Neumann, J.
AU  - Franke, S.
AU  - Rockstroh, M.
AU  - Kasparick, M.
AU  - Neumuth, T.
T2  - International Journal of Computer Assisted Radiology and Surgery
AB  - Purpose: Surgical workflow management in integrated operating rooms (ORs) enables the implementation of novel computer-aided surgical assistance and new applications in process automation, situation awareness, and decision support. The context-sensitive configuration and orchestration of interoperable, networked medical devices is a prerequisite for an effective reduction in the surgeons’ workload, by providing the right service and right information at the right time. The information about the surgical situation must be described as surgical process models and distributed to the medical devices and IT systems in the OR. Available modeling languages are not capable of describing surgical processes for this application. Methods: In this work, the BPMNSIX modeling language for intraoperative processes is technically enhanced and implemented for workflow build-time and run-time. Therefore, particular attention is given to the integration of the recently published IEEE 11073 SDC standard family for a service-oriented architecture of networked medical devices. In addition, interaction patterns for context-aware configuration and device orchestration were presented. Results: The identified interaction patterns were implemented in BPMNSIX for an ophthalmologic use case. Therefore, the examples of the process-driven incorporation and control of device services could be demonstrated. Conclusion: The modeling of surgical procedures with BPMNSIX allows the implementation of context-sensitive surgical assistance functionalities and enables flexibility in terms of the orchestration of dynamically changing device ensembles and integration of unknown devices in the surgical workflow management. © 2019, CARS.
DA  - 2019///
PY  - 2019
DO  - 10.1007/s11548-019-01982-6
VL  - 14
IS  - 8
SP  - 1403
EP  - 1413
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065302899&doi=10.1007%2fs11548-019-01982-6&partnerID=40&md5=6db0fcc7709cbdbce8caa9f0e3925390
DB  - Scopus
KW  - workflow
KW  - automation
KW  - Automation
KW  - Workflow
KW  - Software
KW  - human
KW  - software
KW  - algorithm
KW  - Algorithms
KW  - Humans
KW  - Article
KW  - priority journal
KW  - commercial phenomena
KW  - anatomic model
KW  - Business Process Model and Notation
KW  - cataract extraction
KW  - clinical decision support system
KW  - computer language
KW  - computer simulation
KW  - Computer Simulation
KW  - computer system
KW  - Computer Systems
KW  - Decision Support Systems, Clinical
KW  - devices
KW  - IEEE 11073 SDC
KW  - Integrated OR
KW  - intraoperative period
KW  - medical decision making
KW  - Medical device interoperability
KW  - medical informatics
KW  - Medical Informatics
KW  - Models, Anatomic
KW  - operating room
KW  - Operating Rooms
KW  - ophthalmology
KW  - Ophthalmology
KW  - procedures
KW  - process model
KW  - Process modeling
KW  - Programming Languages
KW  - Surgical workflow
ER  - 

TY  - JOUR
TI  - Learning a goal-oriented model for energy efficient adaptive applications in data centers
AU  - Vitali, M.
AU  - Pernici, B.
AU  - O'Reilly, U.-M.
T2  - Information Sciences
AB  - This work has been motivated by the growing demand of energy coming from the IT sector. We propose a goal-oriented approach where the state of the system is assessed using a set of indicators. These indicators are evaluated against thresholds that are used as goals of our system. We propose a self-adaptive context-aware framework, where we learn both the relations existing between the indicators and the effect of the available actions over the indicators state. The system is also able to respond to changes in the environment, keeping these relations updated to the current situation. Results have shown that the proposed methodology is able to create a network of relations between indicators and to propose an effective set of repair actions to contrast suboptimal states of the data center. The proposed framework is an important tool for assisting the system administrator in the management of a data center oriented towards Energy Efficiency (EE), showing him the connections occurring between the sometimes contrasting goals of the system and suggesting the most likely successful repair action(s) to improve the system state, both in terms of EE and QoS. © 2015 Elsevier Inc. All rights reserved.
DA  - 2015///
PY  - 2015
DO  - 10.1016/j.ins.2015.01.023
VL  - 319
SP  - 152
EP  - 170
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930842086&doi=10.1016%2fj.ins.2015.01.023&partnerID=40&md5=eb7c0876a252bd23f2c30a9d88e9054e
DB  - Scopus
KW  - Energy efficiency
KW  - Data centers
KW  - Green computing
KW  - Energy efficient
KW  - Information management
KW  - Data center
KW  - System administrators
KW  - Current situation
KW  - Adaptive application
KW  - Goal oriented modeling
KW  - Goal-oriented
KW  - Goal-oriented adaptation
KW  - Goal-oriented approach
ER  - 

TY  - CONF
TI  - DEEP-mon: Dynamic and energy efficient power monitoring for container-based infrastructures
AU  - Brondolin, R.
AU  - Sardelli, T.
AU  - Santambrogio, M.D.
T2  - Proceedings - 2018 IEEE 32nd International Parallel and Distributed Processing Symposium Workshops, IPDPSW 2018
AB  - In the last few years energy efficiency of large scale infrastructures gained a lot of attention, as power consumption became one of the most impacting factors of the operative costs of a data-center and of its Total Cost of Ownership (TCO). Power consumption can be observed at different layers of the data-center, from the overall power grid, moving to each rack and arriving to each machine and system. Given the rise of application containers both in the cloud computing and High Performance Computing (HPC) scenarios, it becomes more and more important to measure power consumption also at the application level, where power-aware schedulers and orchestrators can optimize the execution of the workloads not only from a performance perspective, but also considering performance/power trade-offs. In this paper we propose DEEP-mon, a novel monitoring tool able to measure power consumption and attribute it for each thread and application container running in the system. Moreover, we show how the proposed approach has a negligible impact on the monitored system and on the running workloads, overcoming the limitations of the previous works in the field. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/IPDPSW.2018.00110
SP  - 676
EP  - 684
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052236003&doi=10.1109%2fIPDPSW.2018.00110&partnerID=40&md5=b3fc988b43b9d7d4cf6b652e09435256
DB  - Scopus
KW  - Energy efficiency
KW  - Containers
KW  - Power management
KW  - Application level
KW  - Green computing
KW  - Economic and social effects
KW  - Distributed computer systems
KW  - Monitoring
KW  - Electric power utilization
KW  - Electric power transmission networks
KW  - Total cost of ownership
KW  - High performance computing (HPC)
KW  - Power awareness
KW  - Application containers
KW  - Impacting factor
KW  - Large scale infrastructures
KW  - Monitored systems
KW  - Power attribution
ER  - 

TY  - CONF
TI  - Principles of green data mining
AU  - Schneider, J.
AU  - Basalla, M.
AU  - Seidel, S.
T2  - Proceedings of the Annual Hawaii International Conference on System Sciences
AB  - This paper develops a set of principles for green data mining, related to the key stages of business understanding, data understanding, data preparation, modeling, evaluation, and deployment. The principles are grounded in a review of the Cross Industry Standard Process for Data mining (CRISP-DM) model and relevant literature on data mining methods and Green IT. We describe how data scientists can contribute to designing environmentally friendly data mining processes, for instance, by using green energy, choosing between make-or-buy, exploiting approaches to data reduction based on business understanding or pure statistics, or choosing energy friendly models. © 2019 IEEE Computer Society. All rights reserved.
DA  - 2019///
PY  - 2019
VL  - 2019-January
SP  - 2065
EP  - 2074
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084268601&partnerID=40&md5=71404958600ace3dbea27590a8995f59
DB  - Scopus
KW  - Green energy
KW  - Data mining
KW  - Business understanding
KW  - Cross industry
KW  - Data mining methods
KW  - Data mining process
KW  - Data preparation
KW  - Data understanding
KW  - Green manufacturing
KW  - Make-or-buy
ER  - 

TY  - JOUR
TI  - A critical analysis of Power Usage Effectiveness and its use in communicating data center energy consumption
AU  - Yuventi, J.
AU  - Mehdizadeh, R.
T2  - Energy and Buildings
AB  - Data centers represent an increasingly popular construction project type, supported by the continued growth in internet-based services. These facilities can, however, consume large amounts of electricity and - especially if growth trends continue - put strain on utility grids and energy resources. Many metrics have been proposed to evaluate and communicate energy use in data centers. In many cases, the goal is that these metrics will be used to develop energy conscious behavior and perhaps data center energy rating systems or building codes to reduce average energy use. In this paper, we examine one of the more popular metrics, Power Usage Effectiveness (PUE), and discuss its shortcomings toward effectively communicating energy consumption. Our inference is that PUE is an instantaneous representation of electrical energy consumption that encourages operators to report the minimum observed values of PUE. Hence, PUE only conveys an understanding of the minimum possible energy use. Instead, we propose the use of energy-based metrics or average PUE over a significant time period - e.g., a year - to better understand the energy efficiency of a data center and to develop energy rating/ranking systems and energy codes. © 2013 Elsevier B.V. All rights reserved.
DA  - 2013///
PY  - 2013
DO  - 10.1016/j.enbuild.2013.04.015
VL  - 64
SP  - 90
EP  - 94
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878469495&doi=10.1016%2fj.enbuild.2013.04.015&partnerID=40&md5=35a45daec83f8b0f88268d82057748e9
DB  - Scopus
KW  - Energy efficiency
KW  - Data centers
KW  - Energy utilization
KW  - Energy resources
KW  - Power usage
KW  - Building codes
KW  - Building rating systems
KW  - Building ratings
KW  - Construction projects
KW  - Electrical energy consumption
KW  - Energy efficiency metrics
KW  - Internet-based services
KW  - Power Usage Effectiveness
KW  - Power Usage Effectiveness (PUE)
ER  - 

TY  - JOUR
T2  - IBM ACME Air
DA  - 2015///
PY  - 2015
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058264827&partnerID=40&md5=580dacad3f53e22722037e4aaf2f6241
DB  - Scopus
ER  - 

TY  - JOUR
T2  - CloudScale Consortium CloudStore
DA  - 2016///
PY  - 2016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058341351&partnerID=40&md5=9619448846ce21d13b7ef88287fbea43
DB  - Scopus
ER  - 

TY  - JOUR
T2  - RUBiS Users Manual
DA  - 2008///
PY  - 2008
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058324416&partnerID=40&md5=13ee76ddb2fef024539b9157faf9afec
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Validation of ultra-high dependability for software-based systems
AU  - Littlewood, B.
AU  - Strigini, L.
T2  - Predictably Dependable Computing Systems
DA  - 1995///
PY  - 1995
SP  - 473
EP  - 493
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0742267398&partnerID=40&md5=6485180a392bb9ed968304ee5f0b2fe1
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics): Preface
AU  - Rausch, A.
AU  - Reussner, R.
AU  - Mirandola, R.
AU  - Plášil, F.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
DA  - 2008///
PY  - 2008
VL  - 5153 LNCS
SP  - V
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-54249093386&partnerID=40&md5=a9001efba7a80b038512e38ec015b524
DB  - Scopus
ER  - 

TY  - CONF
TI  - Kieker: A framework for application performance monitoring and dynamic software analysis
AU  - Van Hoorn, A.
AU  - Waller, J.
AU  - Hasselbring, W.
T2  - ICPE'12 - Proceedings of the 3rd Joint WOSP/SIPEW International Conference on Performance Engineering
AB  - Kieker is an extensible framework for monitoring and analyzing the runtime behavior of concurrent or distributed software systems. It provides measurement probes for application performance monitoring and control-ow tracing. Analysis plugins extract and visualize architectural models, augmented by quantitative observations. Configurable readers and writers allow Kieker to be used for online and offine analysis. This paper reviews the Kieker framework focusing on its features, its provided extension points for custom components, as well the imposed monitoring overhead. Copyright 2012 ACM.
DA  - 2012///
PY  - 2012
DO  - 10.1145/2188286.2188326
SP  - 247
EP  - 248
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861019240&doi=10.1145%2f2188286.2188326&partnerID=40&md5=2c4581b4ee83dc7f5038d08a011b3f61
DB  - Scopus
KW  - Application performance
KW  - Engineering
KW  - Industrial engineering
KW  - Distributed software system
KW  - Architectural models
KW  - Dynamic softwares
KW  - Extensible framework
KW  - Plug-ins
KW  - Runtime behaviors
ER  - 

TY  - CONF
TI  - Run-time prediction of power consumption for component deployments
AU  - Kistowski, J.V.
AU  - Deffner, M.
AU  - Kounev, S.
T2  - Proceedings - 15th IEEE International Conference on Autonomic Computing, ICAC 2018
AB  - The Power consumption of servers in data centers depends greatly on the software running on each server and how it interacts with the hardware. Different deployments of distributed software components on heterogeneous servers can lead to significant differences in power consumption, depending on the server allocation and the current workload. As workloads and load intensity change, components may be re-deployed or exchanged in order to reduce the power consumption for the current load profile. The decision on which component to place on which server during run-time remains difficult as the power consumption that would result from such a placement remains unknown. Existing work on component deployment optimization at run-time focuses on maximizing performance or considers power in the context of static design time decisions. In this paper, we introduce a model to predict the power consumption of component placements at run-time based on the load and power profile collected for a running distributed application in a heterogeneous environment. In addition, we present a model that enables the use of our approach without dedicated power measurement devices, predicting power consumption based on load intensity and performance counters. We show that we can predict the power consumption of two different distributed web applications with a mean absolute percentage error of 2.21% and with an error of 1.04% when predicting a previously unobserved load level. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/ICAC.2018.00025
SP  - 151
EP  - 156
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058276849&doi=10.1109%2fICAC.2018.00025&partnerID=40&md5=771bcfad4c98f3502ecffb80de61a8f0
DB  - Scopus
KW  - Forecasting
KW  - Prediction
KW  - Microservice
KW  - Servers
KW  - Green computing
KW  - Energy
KW  - Power
KW  - Electric power utilization
KW  - Component
KW  - Regression
KW  - SERT
KW  - Server
KW  - Service
ER  - 

TY  - CONF
TI  - A methodology to predict the power consumption of servers in data centres
AU  - Basmadjian, R.
AU  - Ali, N.
AU  - Niedermeier, F.
AU  - De Meer, H.
AU  - Giuliani, G.
T2  - ACM International Conference Proceeding Series
AB  - Until recently, there have been relatively few studies exploring the power consumption of ICT resources in data centres. In this paper, we propose a methodology to capture the behaviour of most relevant energy-related ICT resources in data centres and present a generic model for them. This is achieved by decomposing the design process into four modelling phases. Furthermore, unlike the state-of-the-art approaches, we provide detailed power consumption models at server and storage levels. We evaluate our model for different types of servers and show that it suffers from an error rate of 2% in the best case, and less than 10% in the worst case. Copyright 2011 ACM.
DA  - 2012///
PY  - 2012
DO  - 10.1145/2318716.2318718
SP  - 1
EP  - 10
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864128556&doi=10.1145%2f2318716.2318718&partnerID=40&md5=9d473a68ac7040a8c3a3bc3050cbbafb
DB  - Scopus
KW  - Energy efficiency
KW  - Information technology
KW  - Models
KW  - Electric power utilization
KW  - Power consumption
KW  - State-of-the-art approach
KW  - Data centres
KW  - IT resources
KW  - Data centre
KW  - Design process
KW  - Error rate
KW  - Generic models
KW  - Modelling
KW  - Power consumption model
KW  - Storage level
ER  - 

TY  - CONF
TI  - PRESS: PRedictive Elastic reSource Scaling for cloud systems
AU  - Gong, Z.
AU  - Gu, X.
AU  - Wilkes, J.
T2  - Proceedings of the 2010 International Conference on Network and Service Management, CNSM 2010
AB  - Cloud systems require elastic resource allocation to minimize resource provisioning costs while meeting service level objectives (SLOs). In this paper, we present a novel PRedictive Elastic reSource Scaling (PRESS) scheme for cloud systems. PRESS unobtrusively extracts fine-grained dynamic patterns in application resource demands and adjust their resource allocations automatically. Our approach leverages light-weight signal processing and statistical learning algorithms to achieve online predictions of dynamic application resource requirements. We have implemented the PRESS system on Xen and tested it using RUBiS and an application load trace from Google. Our experiments show that we can achieve good resource prediction accuracy with less than 5% over-estimation error and near zero under-estimation error, and elastic resource scaling can both significantly reduce resource waste and SLO violations. © 2010 IEEE.
DA  - 2010///
PY  - 2010
DO  - 10.1109/CNSM.2010.5691343
SP  - 9
EP  - 16
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951632052&doi=10.1109%2fCNSM.2010.5691343&partnerID=40&md5=bdd6833a2056a110f4ba800f2b58b6e4
DB  - Scopus
KW  - Network management
KW  - Resource allocation
KW  - Learning algorithms
KW  - Signal processing
KW  - Resource provisioning
KW  - Resource demands
KW  - Cloud systems
KW  - Resource requirements
KW  - Dynamic applications
KW  - Service level objective
KW  - Estimation errors
KW  - Online prediction
KW  - Dynamic patterns
KW  - Light weight
KW  - Over-estimation
KW  - Presses (machine tools)
KW  - Resource prediction
KW  - Resource wastes
KW  - Statistical learning
ER  - 

TY  - JOUR
AU  - Software, P.
T2  - Spring PetClinic
DA  - 2016///
PY  - 2016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058298437&partnerID=40&md5=6653542145f2fba29a4d4871afa81245
DB  - Scopus
ER  - 

TY  - JOUR
DA  - 2011///
PY  - 2011
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058293065&partnerID=40&md5=3d815023a53064e536036ad7032e30a0
DB  - Scopus
ER  - 

TY  - JOUR
T2  - NET Foundation
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058298677&partnerID=40&md5=6f46dbf2d61e35f414ace6ad45a269ee
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Software Dependability in the Tandem GUARDIAN System
AU  - Lee, I.
AU  - Iyer, R.K.
T2  - IEEE Transactions on Software Engineering
AB  - Based on extensive field failure data for Tandem's GUARDIAN operating system, this paper discusses evaluation of the dependability of operational software. Software faults considered are major defects that result in processor failures and invoke backup processes to take over. The paper categorizes the underlying causes of software failures and evaluates the effectiveness of the process pair technique in tolerating software faults. A model to describe the impact of software faults on the reliability of an overall system is proposed. The model is used to evaluate the significance of key factors that determine software dependability and to identify areas for improvement An analysis of the data shows that about 77% of processor failures that are initially considered due to software are confirmed as software problems. The analysis shows that the use of process pairs to provide checkpointing and restart (originally intended for tolerating hardware faults) allows the system to tolerate about 75% of reported software faults that result in processor failures. The loose coupling between processors, which results in the backup execution (the processor state and the sequence of events) being different from the original execution, is a major reason for the measured software fault tolerance. Over two-thirds (72%) of measured software failures are recurrences of previously reported faults. Modeling, based on the data, shows that, in addition to reducing the number of software faults, software dependability can be enhanced by reducing the recurrence rate. © 1995 IEEE
DA  - 1995///
PY  - 1995
DO  - 10.1109/32.387474
VL  - 21
IS  - 5
SP  - 455
EP  - 467
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001631180&doi=10.1109%2f32.387474&partnerID=40&md5=acc04a2fe79b465f910a110066051352
DB  - Scopus
KW  - Measurement
KW  - software reliability
KW  - fault categorization
KW  - operational phase
KW  - recurrence
KW  - software fault tolerance
KW  - Tandem GUARDIAN System
ER  - 

TY  - JOUR
TI  - Facilitating performance predictions using software components
AU  - Happe, J.
AU  - Koziolek, H.
AU  - Reussner, R.
T2  - IEEE Software
AB  - Component-based software engineering (CBSE) poses challenges for predicting and evaluating software performance but also offers several advantages. Software performance engineering can benefit from CBSE ideas and concepts. The MediaStore, a fictional system, demonstrates how to achieve compositional reasoning about software performance. © 2011 IEEE.
DA  - 2011///
PY  - 2011
DO  - 10.1109/MS.2011.25
VL  - 28
IS  - 3
SP  - 27
EP  - 33
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955559323&doi=10.1109%2fMS.2011.25&partnerID=40&md5=eae75e318b7cd3fc0e6a289d17d2b90f
DB  - Scopus
KW  - Software architecture
KW  - Software performance
KW  - Performance prediction
KW  - Software component
KW  - component-based software architecture
KW  - Component-based software architecture
KW  - Component-based software engineering
KW  - Compositional reasoning
KW  - Evaluating software
KW  - quality assessment
KW  - Quality assessment
KW  - software performance engineering
KW  - Software performance engineerings
ER  - 

TY  - CONF
TI  - Modeling and prediction of Software-Defined Networks performance using Queueing Petri Nets
AU  - Rygielski, P.
AU  - Seliuchenko, M.
AU  - Kounev, S.
T2  - Proceedings of the 9th EAI International Conference on Simulation Tools and Techniques, SIMUTools 2016
AB  - Using various modeling and simulation approaches for predicting network performance requires extensive experience and involves a number of time consuming manual steps regarding each of the modeling formalisms. Descartes Network Infrastructure (DNI) is a data center network performance modeling approach that addresses this challenge by offering multiple performance models but requiring to use only a single modeling language. In this paper, we thoroughly extend DNI to support new networking paradigms like, among others, Software-Defined Networking (SDN) and Network-Function Virtualization (NFV). Additionally, we demonstrate how SDN-based networks can be modeled using DNI and how are they transformed later into Queueing Petri Nets (QPN) using a model-to-model transformation. In the analysis of the performance prediction accuracy, we show that automatically generated QPN models represent the performance of heterogeneous SDN hardware with maximal prediction accuracy error of 12%. Copyright © 2016 EAI.
DA  - 2016///
PY  - 2016
SP  - 66
EP  - 75
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052600005&partnerID=40&md5=7e441b4582d847a63a656e0c8e9b62b4
DB  - Scopus
KW  - Forecasting
KW  - Queueing networks
KW  - Petri nets
KW  - Software defined networking
KW  - Performance modeling
KW  - Data center networks
KW  - Modeling languages
KW  - Network function virtualization
KW  - Network performance
KW  - Software defined networking (SDN)
KW  - Performance prediction
KW  - Performance Model
KW  - Automatically generated
KW  - Meta model
KW  - Meta-modeling
KW  - Model to model transformation
KW  - Modeling and predictions
KW  - Software-defined networking
ER  - 

TY  - JOUR
T2  - Power and Performance Benchmark Methodology
DA  - 2012///
PY  - 2012
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043146402&partnerID=40&md5=da4aa23f60e16997ce20e59077c8ba9b
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Stateful component-based performance models
AU  - Happe, L.
AU  - Buhnova, B.
AU  - Reussner, R.
T2  - Software and Systems Modeling
AB  - The accuracy of performance-prediction models is crucial for widespread adoption of performance prediction in industry. One of the essential accuracy-influencing aspects of software systems is the dependence of system behaviour on a configuration, context or history related state of the system, typically reflected with a (persistent) system attribute. Even in the domain of component-based software engineering, the presence of state-reflecting attributes (the so-called internal states) is a natural ingredient of the systems, implying the existence of stateful services, stateful components and stateful systems as such. Currently, there is no consensus on the definition or method to include state-related information in component-based prediction models. Besides the task to identify and localise different types of stateful information across component-based software architecture, the issue is to balance the expressiveness and complexity of prediction models via an effective abstraction of state modelling. In this paper, we identify and classify stateful information in component-based software systems, study the performance impact of the individual state categories, and discuss the costs of their modelling in terms of the increased model size. The observations are formulated into a set of heuristics-guiding software engineers in state modelling. Finally, practical effect of state modelling on software performance is evaluated on a real-world case study, the SPECjms2007 Benchmark. The observed deviation of measurements and predictions was significantly decreased by more precise models of stateful dependencies. © 2013, Springer-Verlag Berlin Heidelberg.
DA  - 2014///
PY  - 2014
DO  - 10.1007/s10270-013-0336-6
VL  - 13
IS  - 4
SP  - 1319
EP  - 1343
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911008093&doi=10.1007%2fs10270-013-0336-6&partnerID=40&md5=a8be1980dd6f2f36e44fd4c67b2efc10
DB  - Scopus
KW  - Forecasting
KW  - Benchmarking
KW  - Classification (of information)
KW  - Software performance
KW  - Computer software
KW  - Predictive analytics
KW  - Performance prediction
KW  - Prediction accuracy
KW  - Component-based software architecture
KW  - Component-based software engineering
KW  - Component-based software systems
KW  - Performance prediction models
KW  - Stateful components
ER  - 

TY  - CONF
TI  - Energy Efficiency of Hierarchical Server Load Distribution Strategies
AU  - Von Kistowski, J.
AU  - Beckett, J.
AU  - Lange, K.-D.
AU  - Block, H.
AU  - Arnold, J.A.
AU  - Kounev, S.
T2  - Proceedings - IEEE Computer Society's Annual International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunications Systems, MASCOTS
AB  - Energy efficiency of servers has become a significant issue over the last years. Load distribution plays a crucial role in the improvement of energy efficiency as (un-)balancing strategies can be leveraged to distribute load over one or multiple systems in a way in which resources are utilized at high performance, yet low overall power consumption. This can be achieved on multiple levels, from load distribution on single CPU cores to machine level load balancing on distributed systems. With modern day server architectures providing load balancing opportunities at several layers, answering the question of optimal load distribution has become non-trivial. Work has to be distributed hierarchically in a fashion that enables maximum energy efficiency at each level. Current approaches balance load based on generalized assumptions about the energy efficiency of servers. These assumptions are based either on very machine-specific or highly generalized observations that may or may not hold true over a variety of systems and configurations. In this paper, we use a modified version of the SPEC SERT suite to measure the energy efficiency of a variety of hierarchical load distribution strategies on single and multi-node systems. We introduce a new strategy and evaluate energy efficiency for homogeneous and heterogeneous workloads over different hardware configurations. Our results show that the selection of a load distribution strategy depends heavily on workload, system utilization, as well as hardware. Used in conjunction with existing strategies, our new load distribution strategy can reduce a single system's power consumption by up to 10.7%. © 2015 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/MASCOTS.2015.11
VL  - 2015-November
SP  - 75
EP  - 84
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962213192&doi=10.1109%2fMASCOTS.2015.11&partnerID=40&md5=6f654663ce47bc05b76f98041aafe772
DB  - Scopus
KW  - Energy efficiency
KW  - Network management
KW  - Distributed systems
KW  - Software testing
KW  - Hardware
KW  - Servers
KW  - Program processors
KW  - Power demand
KW  - Benchmark testing
KW  - Electric power utilization
KW  - Computer hardware
KW  - Hierarchical systems
KW  - Distribution strategies
KW  - Reconfigurable hardware
KW  - Power demands
KW  - Electric load management
KW  - Hardware configurations
KW  - Heterogeneous workloads
KW  - Distribution strategy
KW  - Electric power plant loads
KW  - Load distribution strategies
KW  - Load management
KW  - Load testing
KW  - Optimal load distributions
ER  - 

TY  - JOUR
TI  - Performance-oriented DevOps: A research agenda
AU  - Brunnert, A.
T2  - Performance-oriented DevOps: A Research Agenda
DA  - 2015///
PY  - 2015
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968786582&partnerID=40&md5=d1fd18c1d6fc32df2dcd050e13cee27e
DB  - Scopus
ER  - 

TY  - JOUR
TI  - A unified approach to the performance analysis of caching systems
AU  - Garetto, M.
AU  - Leonardi, E.
AU  - Martina, V.
T2  - ACM Transactions on Modeling and Performance Evaluation of Computing Systems
AB  - We propose a unified methodology to analyze the performance of caches (both isolated and interconnected), by extending and generalizing a decoupling technique originally known as Che's approximation, which provides very accurate results at low computational cost. We consider several caching policies (including a very attractive one, called k-LRU), taking into account the effects of temporal locality. In the case of interconnected caches, our approach allows us to do better than the Poisson approximation commonly adopted in prior work. Our results, validated against simulations and trace-driven experiments, provide interesting insights into the performance of caching systems. © 2016 ACM 2376-3639/2016/05-ART12 $15.00
DA  - 2016///
PY  - 2016
DO  - 10.1145/2896380
VL  - 1
IS  - 3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074675553&doi=10.1145%2f2896380&partnerID=40&md5=5007b772db46dd9a7cfbb140cd8df404
DB  - Scopus
KW  - Hardware
KW  - Distributed computer systems
KW  - Computer science
KW  - Content delivery network
KW  - Performance analysis
KW  - Computer programming
KW  - Caching
KW  - Computer network performance evaluation
KW  - Content delivery networks
KW  - Decoupling technique
KW  - Information-centric networking
KW  - Information-centric networkings
KW  - Inter-connected caches
KW  - Poisson approximations
KW  - Trace driven experiments
ER  - 

TY  - JOUR
TI  - The Palladio component model for model-driven performance prediction
AU  - Becker, S.
AU  - Koziolek, H.
AU  - Reussner, R.
T2  - Journal of Systems and Software
AB  - One aim of component-based software engineering (CBSE) is to enable the prediction of extra-functional properties, such as performance and reliability, utilising a well-defined composition theory. Nowadays, such theories and their accompanying prediction methods are still in a maturation stage. Several factors influencing extra-functional properties need additional research to be understood. A special problem in CBSE stems from its specific development process: Software components should be specified and implemented independently from their later context to enable reuse. Thus, extra-functional properties of components need to be specified in a parametric way to take different influencing factors like the hardware platform or the usage profile into account. Our approach uses the Palladio component model (PCM) to specify component-based software architectures in a parametric way. This model offers direct support of the CBSE development process by dividing the model creation among the developer roles. This paper presents our model and a simulation tool based on it, which is capable of making performance predictions. Within a case study, we show that the resulting prediction accuracy is sufficient to support the evaluation of architectural design decisions. © 2008 Elsevier Inc. All rights reserved.
DA  - 2009///
PY  - 2009
DO  - 10.1016/j.jss.2008.03.066
VL  - 82
IS  - 1
SP  - 3
EP  - 22
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-55249125447&doi=10.1016%2fj.jss.2008.03.066&partnerID=40&md5=ce8cbacdbf56b0e251572a8f67d837d9
DB  - Scopus
KW  - Forecasting
KW  - Software architecture
KW  - Software engineering
KW  - Architectural design
KW  - Software reliability
KW  - Simulation tools
KW  - Computer software
KW  - Computer software reusability
KW  - Performance prediction
KW  - Prediction methods
KW  - Component-based software engineering
KW  - Case studies
KW  - Component models
KW  - Development processes
KW  - Functional properties
KW  - Hardware platforms
KW  - Influencing factors
KW  - Maturation stages
KW  - Model creations
KW  - Performance predictions
KW  - Prediction accuracies
KW  - Reliability theory
KW  - Software components
ER  - 

TY  - JOUR
TI  - Ready for rain? A view from SPEC research on the future of cloud metrics
AU  - Herbst, N.
T2  - Ready for Rain? A View from SPEC Research on the Future of Cloud Metrics
DA  - 2016///
PY  - 2016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019039933&partnerID=40&md5=ff7477cb445c6095456bcec8aa7db532
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Evaluating approaches to resource demand estimation
AU  - Spinner, S.
AU  - Casale, G.
AU  - Brosig, F.
AU  - Kounev, S.
T2  - Performance Evaluation
AB  - Abstract Resource demands are a key parameter of stochastic performance models that needs to be determined when performing a quantitative performance analysis of a system. However, the direct measurement of resource demands is not feasible in most realistic systems. Therefore, statistical approaches that estimate resource demands based on coarse-grained monitoring data (e.g., CPU utilization, and response times) have been proposed in the literature. These approaches have different assumptions and characteristics that need to be considered when estimating resource demands. This paper surveys the state-of-the-art in resource demand estimation and proposes a classification scheme for estimation approaches. Furthermore, it contains an experimental evaluation comparing the impact of different factors (monitoring window size, number of workload classes, load level, collinearity, and model mismatch) on the estimation accuracy of seven different approaches. The classification scheme and the experimental comparison helps performance engineers to select an approach to resource demand estimation that fulfills the requirements of a given analysis scenario. © 2015 Elsevier B.V.
DA  - 2015///
PY  - 2015
DO  - 10.1016/j.peva.2015.07.005
VL  - 92
SP  - 51
EP  - 71
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938851133&doi=10.1016%2fj.peva.2015.07.005&partnerID=40&md5=17fc45471db545e9b1a2005a5585d868
DB  - Scopus
KW  - Stochastic models
KW  - Stochastic systems
KW  - Experimental evaluation
KW  - Performance analysis
KW  - Performance modeling
KW  - Resource demands
KW  - Experimental comparison
KW  - Performance Model
KW  - Classification scheme
KW  - Estimation approaches
KW  - Quantitative performance analysis
KW  - Resource demand estimation
KW  - Workload characterization
ER  - 

TY  - CHAP
TI  - Online learning of run-time models for performance and resource management in data centers
AU  - Walter, J.
AU  - Di Marco, A.
AU  - Spinner, S.
AU  - Inverardi, P.
AU  - Kounev, S.
T2  - Self-Aware Computing Systems
AB  - In this chapter, we explain how to extract and learn run-timemodels that a system can use for self-aware performance and resource management in data centers. We abstract from concrete formalisms and identify extraction aspects relevant to performance models. We categorize the learning aspects into: (i) model structure, (ii) model parametrization (estimation and calibration of model parameters), and (iii) model adaptation options (change point detection and run-time reconfiguration). The chapter identifies alternative approaches for the respective model aspects. The type and granularity of each aspect depend on the characteristic of the concrete performance models. © Springer International Publishing AG 2017.
DA  - 2017///
PY  - 2017
SP  - 507
EP  - 528
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019438468&doi=10.1007%2f978-3-319-47474-8_17&partnerID=40&md5=e3565bd3acac9316b26add6c00bd1042
DB  - Scopus
ER  - 

TY  - CONF
TI  - Open versus closed: A cautionary tale
AU  - Schroeder, B.
AU  - Wierman, A.
AU  - Harchol-Balter, M.
T2  - 3rd Symposium on Networked Systems Design and Implementation, NSDI 2006
AB  - Workload generators may be classified as based on a closed system model, where new job arrivals are only triggered by job completions (followed by think time), or an open system model, where new jobs arrive independently of job completions. In general, system designers pay little attention to whether a workload generator is closed or open. Using a combination of implementation and simulation experiments, we illustrate that there is a vast difference in behavior between open and closed models in real-world settings. We synthesize these differences into eight simple guiding principles, which serve three purposes. First, the principles specify how scheduling policies are impacted by closed and open models, and explain the differences in user level performance. Second, the principles motivate the use of partly open system models, whose behavior we show to lie between that of closed and open models. Finally, the principles provide guidelines to system designers for determining which system model is most appropriate for a given workload. © NSDI 2006.All Rights Reserved.
DA  - 2006///
PY  - 2006
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093921502&partnerID=40&md5=4c23a91785ef30a3a5421955ebfe08b3
DB  - Scopus
KW  - Real world setting
KW  - Systems analysis
KW  - Cautionary tales
KW  - Guiding principles
KW  - Scheduling policies
KW  - System designers
KW  - System modeling
KW  - User-level performance
KW  - Workload generators
ER  - 

TY  - JOUR
T2  - Oracle and S. Microsystems JPetStore 2.0
DA  - 2005///
PY  - 2005
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058266828&partnerID=40&md5=0d31c3134612e88e7bf6368d6ede3fc6
DB  - Scopus
ER  - 

TY  - JOUR
AU  - Groenda, H.
AU  - Stier, C.
AU  - Krzywda, J.
AU  - Byrne, J.
AU  - Svorobej, S.
AU  - Castañ, G.G.
AU  - Papazachos, Z.
AU  - Sheridan, C.
AU  - Whigham, D.
AU  - Hauser, C.
T2  - Cactos Toolkit Version 2: Accompanying Document for Prototype Deliverable d5. 2.2
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058281345&partnerID=40&md5=098f0da0b04af4ae2cd7bdb3f8f51eb3
DB  - Scopus
ER  - 

TY  - CONF
TI  - An experimental performance evaluation of autoscaling policies for complex workflows
AU  - Ilyushkin, A.
AU  - Ali-Eldin, A.
AU  - Herbst, N.
AU  - Papadopoulos, A.V.
AU  - Ghit, B.
AU  - Epema, D.
AU  - Iosup, A.
T2  - ICPE 2017 - Proceedings of the 2017 ACM/SPEC International Conference on Performance Engineering
AB  - Simplifying the task of resource management and scheduling for customers, while still delivering complex Quality-of-Service (QoS), is key to cloud computing. Many autoscaling policies have been proposed in the past decade to decide on behalf of cloud customers when and how to provision resources to a cloud application utilizing cloud elasticity features. However, in prior work, when a new policy is proposed, it is seldom compared to the state-of-the-art, and is often compared only to static provisioning using a predefined QoS target. This reduces the ability of cloud customers and of cloud operators to choose and deploy an autoscaling policy. In our work, we conduct an experimental performance evaluation of autoscaling policies, using as application model workflows, a commonly used formalism for automating resource management for applications with well-defined yet complex structure. We present a detailed comparative study of general state-of-the-art autoscaling policies, along with two new workflow-specific policies. To understand the performance differences between the 7 policies, we conduct various forms of pairwise and group comparisons. We report both individual and aggregated metrics. Our results highlight the trade-offs between the suggested policies, and thus enable a better understanding of the current state-of-the-art. © 2017 ACM.
DA  - 2017///
PY  - 2017
DO  - 10.1145/3030207.3030214
SP  - 75
EP  - 86
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019018662&doi=10.1145%2f3030207.3030214&partnerID=40&md5=c8b601ce3d9af02185391daebeb015a3
DB  - Scopus
KW  - Resource management
KW  - Scheduling
KW  - Quality of service
KW  - Economic and social effects
KW  - Resource allocation
KW  - Natural resources management
KW  - Cloud applications
KW  - Sales
KW  - Comparative studies
KW  - Application modeling
KW  - Experimental performance evaluations
KW  - Cloud elasticities
KW  - Complex workflows
KW  - Resource management and scheduling
ER  - 

TY  - CONF
TI  - Performance and scalability of EJB applications
AU  - Cecchet, E.
AU  - Marguerite, J.
AU  - Zwaenepoel, W.
T2  - Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA
AB  - A study was conducted on several Enterprise JavabEJB implementations of the same e-commerce application, using different application implementation methods, container designs and communication layers. It was shown that stateless session beans with bean-managed persistence coupled with an efficient communication layer offer performance comparable to a servlets-only implementation. Entity beans impose a row-level access to the database resulting in a finer-grain access and significantly lower performance.
DA  - 2002///
PY  - 2002
SP  - 246
EP  - 261
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038486621&partnerID=40&md5=9cce9b3e09bdb0f5062d03931ee5fc81
DB  - Scopus
KW  - Performance
KW  - Scalability
KW  - Servers
KW  - Optimization
KW  - Semantics
KW  - Websites
KW  - Java programming language
KW  - Response time (computer systems)
KW  - Data communication systems
KW  - Interfaces (computer)
KW  - Program compilers
KW  - Communication optimization
KW  - EJB container design
KW  - Profiling
KW  - Run time
KW  - Software package Enterprise JavaBeans
ER  - 

TY  - CONF
TI  - Scalability Analysis of Cloud Software Services
AU  - Brataas, G.
AU  - Herbst, N.
AU  - Ivansek, S.
AU  - Polutnik, J.
T2  - Proceedings - 2017 IEEE International Conference on Autonomic Computing, ICAC 2017
AB  - Cloud computing theoretically offers its customers unlimited cloud resources. However, the scalability of software services is often limited by their underlying architecture. In contrast to current scalability analysis approaches, we make work parameters, quality thresholds, as well as the resource space explicit in a conceptually consistent set of equations. We propose two scalability metric functions based on these equations. The resource scalability metric function describes the relation between the capacity of the multi-tier cloud software service and its use of cloud resources, whereas the cost scalability metric function replaces cloud resources with cost. We validate using the Cloud-Store application. CloudStore follows the TPC-W specification, representing an online book store. We have experimented with 21 different public Amazon Web Service configurations and two private OpenStack configurations. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/ICAC.2017.34
SP  - 285
EP  - 292
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034448448&doi=10.1109%2fICAC.2017.34&partnerID=40&md5=f88fb2d56dd26ec9f8a3e0eab7b8fe4c
DB  - Scopus
KW  - cloud
KW  - scalability
KW  - Scalability
KW  - Costs
KW  - Quality control
KW  - Web services
KW  - Clouds
KW  - Measurements
KW  - cost
KW  - Amazon web services
KW  - Software services
KW  - measurement
KW  - metric
KW  - Multi-tier
KW  - Resource space
KW  - Scalability analysis
KW  - service
ER  - 

TY  - CONF
TI  - Optimal provision of multiple service types
AU  - Ezhilchelvan, P.
AU  - Mitrani, I.
T2  - Proceedings - 2016 IEEE 24th International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems, MASCOTS 2016
AB  - Services of different types are provided to paying customers on servers hired from a cloud. Different virtual machines can share a server, subject to one or more resource constraints. Incoming jobs whose resource requirements cannot be satisfied are lost. The objective is to maximize the long-term average profit per unit time. A single-server model is analyzed exactly and the results provide approximations for the system with n servers. The latter is also solved exactly when the servers are dedicated and when the VMs can migrate instantaneously. Numerical examples and comparisons with simulations are presented. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/MASCOTS.2016.16
SP  - 21
EP  - 29
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010424700&doi=10.1109%2fMASCOTS.2016.16&partnerID=40&md5=23e05b5d2327d8a2f1ea42ae17139f7f
DB  - Scopus
KW  - Network security
KW  - Virtual machine migrations
KW  - Service provisioning
KW  - Java programming language
KW  - Erlang model
KW  - Multiple job types
KW  - Revenue optimisation
KW  - Revenue Optimisation
KW  - Service Provisioning
KW  - Virtual Machine migration
ER  - 

TY  - JOUR
TI  - The uml-marte standardized profile
AU  - Gérard, S.
AU  - Selic, B.
T2  - IFAC Proceedings Volumes
DA  - 2008///
PY  - 2008
VL  - 41
IS  - 2
SP  - 6909
EP  - 6913
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047760462&partnerID=40&md5=8e3359d33ffe03cb55f58bb1a77d6492
DB  - Scopus
ER  - 

TY  - CONF
TI  - Dynamic scaling of web applications in a virtualized cloud computing environment
AU  - Chieu, T.C.
AU  - Mohindra, A.
AU  - Karve, A.A.
AU  - Segal, A.
T2  - Proceedings - IEEE International Conference on e-Business Engineering, ICEBE 2009; IEEE Int. Workshops - AiR 2009; SOAIC 2009; SOKMBI 2009; ASOC 2009
AB  - Scalability is critical to the success of many enterprises currently involved in doing business on the web and in providing information that may vary drastically from one time to another. Maintaining sufficient resources just to meet peak requirements can be costly. Cloud computing provides a powerful computing model that allows users to access resources on-demand. In this paper, we will describe a novel architecture for the dynamic scaling of web applications based on thresholds in a virtualized Cloud Computing environment. We will illustrate our scaling approach with a front-end load-balancer for routing and balancing user requests to web applications deployed on web servers installed in virtual machine instances. A dynamic scaling algorithm for automated provisioning of virtual machine resources based on threshold number of active sessions will be introduced. The on-demand capability of the Cloud to rapidly provision and dynamically allocate resources to users will be discussed. Our work has demonstrated the compelling benefits of the Cloud which is capable of handling sudden load surges, delivering IT resources on-demands to users, and maintaining higher resource utilization, thus reducing infrastructure and management costs. © 2009 IEEE.
DA  - 2009///
PY  - 2009
DO  - 10.1109/ICEBE.2009.45
SP  - 281
EP  - 286
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950987607&doi=10.1109%2fICEBE.2009.45&partnerID=40&md5=88acee7d2db24889fbf1c5a4f330a76b
DB  - Scopus
KW  - Cloud computing
KW  - Scalability
KW  - Virtualization
KW  - Balancing
KW  - Virtual machine
KW  - Virtual machines
KW  - Resource utilizations
KW  - Cost reduction
KW  - WEB application
KW  - Novel architecture
KW  - World Wide Web
KW  - Electronic commerce
KW  - Virtualizations
KW  - Technical presentations
KW  - Computing model
KW  - Management costs
KW  - IT resources
KW  - Dynamic scaling
KW  - On-Demand
KW  - Threshold numbers
KW  - Web servers
ER  - 

TY  - CONF
TI  - Slope one predictors for online rating-based collaborative filtering
AU  - Lemire, D.
AU  - Maclachlan, A.
T2  - Proceedings of the 2005 SIAM International Conference on Data Mining, SDM 2005
AB  - Rating-based collaborative filtering is the process of predicting how a user would rate a given item from other user ratings. We propose three related slope one schemes with predictors of the form f(x) = x + b, which precompute the average difference between the ratings of one item and another for users who rated both. Slope one algorithms are easy to implement, efficient to query, reasonably accurate, and they support both online queries and dynamic updates, which makes them good candidates for real-world systems. The basic SLOPE ONE scheme is suggested as a new reference scheme for collaborative filtering. By factoring in items that a user liked separately from items that a user disliked, we achieve results competitive with slower memory-based schemes over the standard benchmark EachMovie and Movielens data sets while better fulfilling the desiderata of CF applications. Copyright © by SIAM.
DA  - 2005///
PY  - 2005
DO  - 10.1137/1.9781611972757.43
SP  - 471
EP  - 475
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880129466&doi=10.1137%2f1.9781611972757.43&partnerID=40&md5=b0a0c19f430dd04a3350515903d3af97
DB  - Scopus
KW  - Benchmarking
KW  - Data mining
KW  - Online systems
KW  - Electronic commerce
KW  - Average difference
KW  - Collaborative filtering
KW  - Dynamic update
KW  - E-commerce
KW  - Knowledge discovery
KW  - Online ratings
KW  - Real-world system
KW  - Recommender
KW  - Slope one schemes
KW  - Slope ones
KW  - User rating
ER  - 

TY  - JOUR
TI  - Hybrid multi-attribute QoS optimization in component based software systems
AU  - Koziolek, A.
AU  - Ardagna, D.
AU  - Mirandola, R.
T2  - Journal of Systems and Software
AB  - Design decisions for complex, component-based systems impact multiple quality of service (QoS) properties. Often, means to improve one quality property deteriorate another one. In this scenario, selecting a good solution with respect to a single quality attribute can lead to unacceptable results with respect to the other quality attributes. A promising way to deal with this problem is to exploit multi-objective optimization where the objectives represent different quality attributes. The aim of these techniques is to devise a set of solutions, each of which assures an optimal trade-off between the conflicting qualities. Our previous work proposed a combined use of analytical optimization techniques and evolutionary algorithms to efficiently identify an optimal set of design alternatives with respect to performance and costs. This paper extends this approach to more QoS properties by providing analytical algorithms for availability-cost optimization and three-dimensional availability-performance-cost optimization. We demonstrate the use of this approach on a case study, showing that the analytical step provides a better-than-random starting population for the evolutionary optimization, which lead to a speed-up of 28% in the availability-cost case. © 2013 Elsevier Inc.
DA  - 2013///
PY  - 2013
DO  - 10.1016/j.jss.2013.03.081
VL  - 86
IS  - 10
SP  - 2542
EP  - 2558
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882598370&doi=10.1016%2fj.jss.2013.03.081&partnerID=40&md5=3816ffabd78ddfff2a0373d27f45ffe7
DB  - Scopus
KW  - Software engineering
KW  - Costs
KW  - Quality of service
KW  - Multiobjective optimization
KW  - Availability
KW  - Component-based software systems
KW  - Analytical algorithms
KW  - Analytical optimizations
KW  - Architecture optimization
KW  - Component based systems
KW  - Design alternatives
KW  - Evolutionary optimizations
KW  - Quality properties
KW  - Software architecture optimization
ER  - 

TY  - CONF
TI  - Using dynatrace monitoring data for generating performance models of java EE applications
AU  - Willnecker, F.
AU  - Brunnert, A.
AU  - Gottesheim, W.
AU  - Krcmar, H.
T2  - ICPE 2015 - Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering
AB  - Performance models assist capacity management and planning for large-scale enterprise applications by predicting their performance for different workloads and hardware environments. Manually creating these models often outweighs their benefits. Automatic performance model generators have been introduced to facilitate the model creation. These generators often use custom monitoring solutions to generate the required input data for the model creation. In contrast, standardized application performance management (APM) solutions are used in industry to control performance metrics for productive systems. This work presents the integration of industry standard APM solutions with a performance model generation framework. We apply the integration concepts using the APM solution Dynatrace and a performance model generation framework for Palladio Component Models (PCM). Copyright © 2015 ACM.
DA  - 2015///
PY  - 2015
DO  - 10.1145/2668930.2688061
SP  - 103
EP  - 104
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923923744&doi=10.1145%2f2668930.2688061&partnerID=40&md5=f4b0d47898ce9bae1d9207c824547980
DB  - Scopus
KW  - Performance evaluation
KW  - Java programming language
KW  - Enterprise applications
KW  - Capacity management
KW  - Application performance
KW  - Industry standards
KW  - Load testing
KW  - Application performance management
KW  - Control performance
KW  - Hardware environment
KW  - Productive systems
ER  - 

TY  - CONF
TI  - Modelling of input-parameter dependency for performance predictions of component-based embedded systems
AU  - Bondarev, E.
AU  - De With, P.
AU  - Chaudron, M.
AU  - Muskens, J.
T2  - Software Engineering and Advanced Applications, 2005. 31st EUROMICRO Conference
AB  - The guaranty of meeting the timing constraints during the design phase of real-time component-based embedded software has not been realized. To satisfy real-time requirements, we need to understand behaviour and resource usage of a system over time. In this paper we address both aspects in detail by observing the influence of input data on the system behaviour and performance. We extend an existing scenario simulation approach that features the modelling of input parameter dependencies and simulating the execution of the models. The approach enables specification of the dependencies in the component models, as well as initialisation of the parameters in the application scenario model. This gives a component-based application designer an explorative possibility of going through all possible execution scenarios with different parameter initialisations, and finding the worst-case scenarios where the predicted performance does not satisfy the requirements. The identification of these scenarios is important because it avoids system redesign at the later stage. In addition, the conditional behaviour and resource usage modelling with respect to the input data provide more accurate prediction. © 2005 IEEE.
DA  - 2005///
PY  - 2005
DO  - 10.1109/EUROMICRO.2005.40
VL  - 2005
SP  - 36
EP  - 43
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747428299&doi=10.1109%2fEUROMICRO.2005.40&partnerID=40&md5=47950250eb70ffc3fb295a2b6325a38f
DB  - Scopus
KW  - Software engineering
KW  - Resource allocation
KW  - Data reduction
KW  - Real time systems
KW  - Mathematical models
KW  - Embedded systems
KW  - Parameter estimation
KW  - Embedded software
KW  - Constraint theory
KW  - Resource usage modeling
KW  - Scenario simulation
KW  - Worst-case scenarios
ER  - 

TY  - CONF
TI  - Benchmark Requirements for Microservices Architecture Research
AU  - Aderaldo, C.M.
AU  - Mendonça, N.C.
AU  - Pahl, C.
AU  - Jamshidi, P.
T2  - Proceedings - 2017 IEEE/ACM 1st International Workshop on Establishing the Community-Wide Infrastructure for Architecture-Based Software Engineering, ECASE 2017
AB  - Microservices have recently emerged as a new architectural style in which distributed applications are broken up into small independently deployable services, each running in its own process and communicating via lightweight mechanisms. However, there is still a lack of repeatable empirical research on the design, development and evaluation of microservices applications. As a first step towards filling this gap, this paper proposes, discusses and illustrates the use of an initial set of requirements that may be useful in selecting a community-owned architecture benchmark to support repeatable microservices research. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/ECASE.2017.4
SP  - 8
EP  - 13
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027445401&doi=10.1109%2fECASE.2017.4&partnerID=40&md5=61d346a021eccc94d7914ec326403ca7
DB  - Scopus
KW  - Microservices
KW  - software architecture
KW  - Software architecture
KW  - Software engineering
KW  - Distributed applications
KW  - Architectural style
KW  - Engineering
KW  - Industrial engineering
KW  - Architecture research
KW  - Empirical research
KW  - research benchmark
KW  - Running-in
ER  - 

TY  - JOUR
AU  - Becker, S.
AU  - Bulej, L.
AU  - Bures, T.
AU  - Hnetynka, P.
AU  - Kapova, L.
AU  - Kofron, J.
AU  - Koziolek, H.
AU  - Kraft, J.
AU  - Mirandola, R.
AU  - Stammel, J.
AU  - Tamburelli, G.
AU  - Trifu, M.
T2  - Q-impress Consortium
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058269056&partnerID=40&md5=4145fd7925e14e222e2a5af08b21dd5a
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Performance comparison of middleware architectures for generating dynamic web content
AU  - Cecchet, E.
AU  - Chanda, A.
AU  - Elnikety, S.
AU  - Marguerite, J.
AU  - Zwaenepoel, W.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - On-line services are making increasing use of dynamically generated Web content. Serving dynamic content is more complex than serving static content. Besides a Web server, it typically involves a server-side application and a database to generate and store the dynamic content. A number of standard mechanisms have evolved to generate dynamic content. We evaluate three specific mechanisms in common use: PHP, Java servlets, and Enterprise Java Beans (EJB). These mechanisms represent three different architectures for generating dynamic content. PHP scripts are tied to the Web server and require writing explicit database queries. Java servlets execute in a different process from the Web server, allowing them to be located on a separate machine for better load balancing. The database queries are written explicitly, as in PHP, but in certain circumstances the Java synchronization primitives can be used to perform locking, reducing database lock contention and the amount of communication between servlets and the database. Enterprise Java Beans (EJB) provide several services and facilities. In particular, many of the database queries can be generated automatically. We measure the performance of these three architectures using two application benchmarks: an online bookstore and an auction site. These benchmarks represent common applications for dynamic content and stress different parts of a dynamic content Web server. The auction site stresses the server front-end, while the online bookstore stresses the server back-end. For all measurements, we use widely available open-source software (the Apache Web server, Tomcat servlet engine, JOnAS EJB server, and MySQL relational database). While Java servlets are less efficient than PHP, their ability to execute on a different machine from the Web server and their ability to perform synchronization leads to better performance when the front-end is the bottleneck or when there is database lock contention. EJB facilities and services come at the cost of lower performance than both PHP and Java servlets. © IFIP International Federation for Information Processing 2003.
DA  - 2003///
PY  - 2003
DO  - 10.1007/3-540-44892-6_13
VL  - 2672
SP  - 242
EP  - 261
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-35248840419&doi=10.1007%2f3-540-44892-6_13&partnerID=40&md5=d5f1d64aa8d6f739e43d7cae8d8c373a
DB  - Scopus
KW  - Balancing
KW  - Middleware
KW  - Web services
KW  - Benchmarking
KW  - Memory architecture
KW  - Open source software
KW  - Open systems
KW  - Dynamics
KW  - Websites
KW  - Java programming language
KW  - Electronic commerce
KW  - Performance comparison
KW  - Synchronization primitive
KW  - Query languages
KW  - Apache web server
KW  - Dynamic web content
KW  - Enterprise Java Beans
KW  - Middleware architecture
KW  - Online bookstore
KW  - Relational Database
ER  - 

TY  - JOUR
AU  - Consortium, O.
T2  - Rice University Bidding System
DA  - 2009///
PY  - 2009
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-75149156792&partnerID=40&md5=7e14c298222ac11b067cda89a1f40370
DB  - Scopus
ER  - 

TY  - CONF
TI  - Modeling of Parametric Dependencies for Performance Prediction of Component-Based Software Systems at Run-Time
AU  - Eismann, S.
AU  - Walter, J.
AU  - Von Kistowski, J.
AU  - Kounev, S.
T2  - Proceedings - 2018 IEEE 15th International Conference on Software Architecture, ICSA 2018
AB  - Model-based performance analysis can be leveraged to explore performance properties of software systems. To capture the behavior of varying workload mixes, configurations, and deployments of a software system requires formal modeling of the impact of configuration parameters and user input on the system behavior. Such influences are represented as parametric dependencies in software performance models. Existing modeling approaches focus on modeling parametric dependencies at design-time. This paper identifies runtime specific parametric dependency features, which are not supported by existing work. Therefore, this paper proposes a novel modeling methodology for parametric dependencies and a corresponding graph-based resolution algorithm. This algorithm enables the solution of models containing component instance-level dependencies, variables with multiple descriptions in parallel, and correlations modeled as parametric dependencies. We integrate our work into the Descartes Modeling Language (DML), allowing for accurate and efficient modeling and analysis of parametric dependencies. These performance predictions are valuable for various purposes such as capacity planning, bottleneck analysis, configuration optimization and proactive auto-scaling. Our evaluation analyzes a video store application. The prediction for varying language mixes and video sizes shows a mean error below 5% for utilization and below 10% for response time. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/ICSA.2018.00023
SP  - 135
EP  - 144
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051114862&doi=10.1109%2fICSA.2018.00023&partnerID=40&md5=cc9fd78b93c3d7ffdd0f6d73b483b543
DB  - Scopus
KW  - Performance
KW  - Forecasting
KW  - Software architecture
KW  - Runtime
KW  - Architecture
KW  - Computer software
KW  - Runtimes
KW  - Graphic methods
KW  - Modeling languages
KW  - Parameter estimation
KW  - Performance Model
KW  - Component based systems
KW  - Component-based-systems
KW  - Descartes
KW  - Descartes-Modeling-Language
KW  - Parametric dependencies
KW  - Parametric-dependencies
KW  - Performance-modeling
ER  - 

TY  - JOUR
TI  - Towards performance tooling interoperability: An open format for representing execution traces
AU  - Okanović, D.
AU  - Van Hoorn, A.
AU  - Heger, C.
AU  - Wert, A.
AU  - Siegl, S.
T2  - Proc. of the 13th European Workshop on Performance Engineering (EPEW)
DA  - 2016///
PY  - 2016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051676027&partnerID=40&md5=5553a9ed3194b10c3c70a3e30a7b5526
DB  - Scopus
ER  - 

TY  - CONF
TI  - An expandable extraction framework for architectural performance models
AU  - Walter, J.
AU  - Stier, C.
AU  - Koziolek, H.
AU  - Kounev, S.
T2  - ICPE 2017 - Companion of the 2017 ACM/SPEC International Conference on Performance Engineering
AB  - Providing users with Quality of Service (QoS) guarantees and the prevention of performance problems are challenging tasks for software systems. Architectural performance models can be applied to explore performance properties of a software system at design time and run time. At design time, architectural performance models support reasoning on effects of design decisions. At run time, they enable automatic reconfigurations by reasoning on the effects of changing user behavior. In this paper, we present a framework for the extraction of architectural performance models based on monitoring logfiles generalizing over the targeted architectural modeling language. Using the presented framework, the creation of a performance model extraction tool for a specific modeling formalism requires only the implementation of a key set of object creation routines specific to the formalism. Our framework integrates them with extraction techniques that apply to many architectural performance models, e.g., resource demand estimation techniques. This lowers the effort to implement performance model extraction tools tremendously through a high level of reuse. We evaluate our framework presenting builders for the Descartes Modeling Language (DML) and the Palladio Component Model (PCM). For the extracted models we compare simulation results with measurements receiving accurate results. © 2017 ACM.
DA  - 2017///
PY  - 2017
DO  - 10.1145/3053600.3053634
SP  - 165
EP  - 170
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019501371&doi=10.1145%2f3053600.3053634&partnerID=40&md5=0184ea21486209a6e8384d95ea97bc4b
DB  - Scopus
KW  - Architectural design
KW  - Quality of service
KW  - Computer software
KW  - Behavioral research
KW  - Extraction
KW  - Modeling languages
KW  - Network function virtualization
KW  - Performance Model
KW  - Quality of service (QoS) guarantees
KW  - Architectural modeling languages
KW  - Automatic reconfiguration
KW  - Extraction techniques
KW  - Modeling formalisms
KW  - Performance problems
KW  - Performance properties
ER  - 

TY  - JOUR
TI  - Energy-aware resource allocation heuristics for efficient management of data centers for Cloud computing
AU  - Beloglazov, A.
AU  - Abawajy, J.
AU  - Buyya, R.
T2  - Future Generation Computer Systems
AB  - Cloud computing offers utility-oriented IT services to users worldwide. Based on a pay-as-you-go model, it enables hosting of pervasive applications from consumer, scientific, and business domains. However, data centers hosting Cloud applications consume huge amounts of electrical energy, contributing to high operational costs and carbon footprints to the environment. Therefore, we need Green Cloud computing solutions that can not only minimize operational costs but also reduce the environmental impact. In this paper, we define an architectural framework and principles for energy-efficient Cloud computing. Based on this architecture, we present our vision, open research challenges, and resource provisioning and allocation algorithms for energy-efficient management of Cloud computing environments. The proposed energy-aware allocation heuristics provision data center resources to client applications in a way that improves energy efficiency of the data center, while delivering the negotiated Quality of Service (QoS). In particular, in this paper we conduct a survey of research in energy-efficient computing and propose: (a) architectural principles for energy-efficient management of Clouds; (b) energy-efficient resource allocation policies and scheduling algorithms considering QoS expectations and power usage characteristics of the devices; and (c) a number of open research challenges, addressing which can bring substantial benefits to both resource providers and consumers. We have validated our approach by conducting a performance evaluation study using the CloudSim toolkit. The results demonstrate that Cloud computing model has immense potential as it offers significant cost savings and demonstrates high potential for the improvement of energy efficiency under dynamic workload scenarios. © 2011 Elsevier B.V. All rights reserved.
DA  - 2012///
PY  - 2012
DO  - 10.1016/j.future.2011.04.017
VL  - 28
IS  - 5
SP  - 755
EP  - 768
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857370722&doi=10.1016%2fj.future.2011.04.017&partnerID=40&md5=404d8a6f96e7208ce49f7fb3ffc699cd
DB  - Scopus
KW  - Cloud computing
KW  - Energy efficiency
KW  - Resource management
KW  - Virtualization
KW  - Carbon footprint
KW  - Data centers
KW  - Performance evaluation
KW  - Quality of service
KW  - Resource allocation
KW  - Energy efficient
KW  - Energy aware
KW  - Computing environments
KW  - Green IT
KW  - Environmental impact
KW  - Research challenges
KW  - Resource provisioning
KW  - Computer systems
KW  - Dynamic consolidation
KW  - Resource providers
KW  - Research
KW  - Virtualizations
KW  - Allocation algorithm
KW  - Architectural frameworks
KW  - Architectural principles
KW  - Business domain
KW  - Client applications
KW  - Computing solutions
KW  - Cost saving
KW  - Electrical energy
KW  - Energy-efficient resource allocation
KW  - High potential
KW  - IT services
KW  - Operational costs
KW  - Pay-as-you-go
KW  - Pervasive applications
KW  - Power usage
ER  - 

TY  - JOUR
TI  - PEAS: A performance evaluation framework for auto-scaling strategies in cloud applications
AU  - Vittorio Papadopoulos, A.
AU  - Ali-Eldin, A.
AU  - Arzén, K.-E.
AU  - Tordsson, J.
AU  - Elmroth, E.
T2  - ACM Transactions on Modeling and Performance Evaluation of Computing Systems
AB  - Numerous auto-scaling strategies have been proposed in the past few years for improving various Quality of Service (QoS) indicators of cloud applications, for example, response time and throughput, by adapting the amount of resources assigned to the application to meet the workload demand. However, the evaluation of a proposed auto-scaler is usually achieved through experiments under specific conditions and seldom includes extensive testing to account for uncertainties in the workloads and unexpected behaviors of the system. These tests by no means can provide guarantees about the behavior of the system in general conditions. In this article, we present a Performance Evaluation framework for Auto-Scaling (PEAS) strategies in the presence of uncertainties. The evaluation is formulated as a chance constrained optimization problem, which is solved using scenario theory. The adoption of such a technique allows one to give probabilistic guarantees of the obtainable performance. Six different auto-scaling strategies have been selected from the literature for extensive test evaluation and compared using the proposed framework. We build a discrete event simulator and parameterize it based on real experiments. Using the simulator, each auto-scaler's performance is evaluated using 796 distinct real workload traces from projects hosted on the Wikimedia foundations' servers, and their performance is compared using PEAS. The evaluation is carried out using different performance metrics, highlighting the flexibility of the framework, while providing probabilistic bounds on the evaluation and the performance of the algorithms. Our results highlight the problem of generalizing the conclusions of the original published studies and show that based on the evaluation criteria, a controller can be shown to be better than other controllers. © 2016 ACM 2376-3639/2016/08-ART15 $15.00
DA  - 2016///
PY  - 2016
DO  - 10.1145/2930659
VL  - 1
IS  - 4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074674305&doi=10.1145%2f2930659&partnerID=40&md5=39dafecaa11d9622b5424d3fcb7da914
DB  - Scopus
KW  - Cloud computing
KW  - Auto-scaling
KW  - Elasticity
KW  - Performance evaluation
KW  - Quality of service
KW  - Constrained optimization
KW  - Computation theory
KW  - Performance evaluations
KW  - Chance constrained optimization problems
KW  - Discrete-event simulators
KW  - Frequency dividing circuits
KW  - Performance evaluation frameworks
KW  - Probabilistic bounds
KW  - Probabilistic guarantees
KW  - Randomized optimization
KW  - Randomized optimizations
ER  - 

TY  - JOUR
TI  - Using genetic search for reverse engineering of parametric behavior models for performance prediction
AU  - Krogmann, K.
AU  - Kuperberg, M.
AU  - Reussner, R.
T2  - IEEE Transactions on Software Engineering
AB  - In component-based software engineering, existing components are often reused in new applications. Correspondingly, the response time of an entire component-based application can be predicted from the execution durations of individual component services. These execution durations depend on the runtime behavior of a component which itself is influenced by three factors: the execution platform, the usage profile, and the component wiring. To cover all relevant combinations of these influencing factors, conventional prediction of response times requires repeated deployment and measurements of component services for all such combinations, incurring a substantial effort. This paper presents a novel comprehensive approach for reverse engineering and performance prediction of components. In it, genetic programming is utilized for reconstructing a behavior model from monitoring data, runtime bytecode counts, and static bytecode analysis. The resulting behavior model is parameterized over all three performance-influencing factors, which are specified separately. This results in significantly fewer measurements: The behavior model is reconstructed only once per component service, and one application-independent bytecode benchmark run is sufficient to characterize an execution platform. To predict the execution durations for a concrete platform, our approach combines the behavior model with platform-specific benchmarking results. We validate our approach by predicting the performance of a file sharing application. © 2006 IEEE.
DA  - 2010///
PY  - 2010
DO  - 10.1109/TSE.2010.69
VL  - 36
IS  - 6
SP  - 865
EP  - 877
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649781035&doi=10.1109%2fTSE.2010.69&partnerID=40&md5=90fc286a25c58fc1e393d8acdb79772d
DB  - Scopus
KW  - performance prediction
KW  - Forecasting
KW  - Software engineering
KW  - Benchmarking
KW  - Models
KW  - Response time
KW  - Runtimes
KW  - Behavioral research
KW  - Performance prediction
KW  - New applications
KW  - Individual components
KW  - Genetic programming
KW  - Runtime behaviors
KW  - Component-based software engineering
KW  - Behavior model
KW  - Bytecode analysis
KW  - bytecode benchmarking
KW  - Bytecodes
KW  - Component based applications
KW  - File sharing application
KW  - genetic programming
KW  - Genetic search
KW  - Influencing factor
KW  - Parameterized
KW  - reverse engineering
KW  - Reverse engineering
ER  - 

TY  - JOUR
AU  - Bastani, K.
T2  - Spring Cloud Example Project
DA  - 2015///
PY  - 2015
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058283557&partnerID=40&md5=94a0ded6fcd94e5b1a26bfdfd3e579c1
DB  - Scopus
ER  - 

TY  - CONF
TI  - LibReDE: A library for resource demand estimation
AU  - Spinner, S.
AU  - Casale, G.
AU  - Zhu, X.
AU  - Kounev, S.
T2  - ICPE 2014 - Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering
AB  - When creating a performance model, it is necessary to quantify the amount of resources consumed by an application serving individual requests. In distributed enterprise systems, these resource demands usually cannot be observed directly, their estimation is a major challenge. Different statistical approaches to resource demand estimation based on monitoring data have been proposed, e.g., using linear regression or Kalman filtering techniques. In this paper, we present LibReDE, a library of ready-to-use implementations of approaches to resource demand estimation that can be used for online and offline analysis. It is the first publicly available tool for this task and aims at supporting performance engineers during performance model construction. The library enables the quick comparison of the estimation accuracy of different approaches in a given context and thus helps to select an optimal one.
DA  - 2014///
PY  - 2014
DO  - 10.1145/2568088.2576093
SP  - 227
EP  - 228
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899689594&doi=10.1145%2f2568088.2576093&partnerID=40&md5=a02d48c5bd6b7f2bed864f10906054dc
DB  - Scopus
KW  - Resource demands
KW  - Engineering
KW  - Industrial engineering
KW  - Estimation
KW  - Performance Model
KW  - Enterprise system
KW  - Kalman filtering techniques
KW  - Online and offline analysis
KW  - Statistical approach
ER  - 

TY  - CONF
TI  - Comparing the accuracy of resource demand measurement and estimation techniques
AU  - Willnecker, F.
AU  - Dlugi, M.
AU  - Brunnert, A.
AU  - Spinner, S.
AU  - Kounev, S.
AU  - Gottesheim, W.
AU  - Krcmar, H.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Resource demands are a core aspect of performance models. They describe how an operation utilizes a resource and therefore influence the systems performance metrics: response time, resource utilization and throughput. Such demands can be determined by two extraction classes: direct measurement or demand estimation. Selecting the best suited technique depends on available tools, acceptable measurement overhead and the level of granularity necessary for the performance model. This work compares two direct measurement techniques and an adaptive estimation technique based on multiple statistical approaches to evaluate strengths and weaknesses of each technique. We conduct a series of experiments using the SPECjEnterprise2010 industry benchmark and an automatic performance model generator for architecture level performance models based on the Palladio Component Model. To compare the techniques we conduct two experiments with different levels of granularity on a standalone system, followed by one experiment using a distributed SPECjEnterprise2010 deployment combining both extraction classes for generating a full-stack performance model. © Springer International Publishing Switzerland 2015.
DA  - 2015///
PY  - 2015
DO  - 10.1007/978-3-319-23267-6_8
VL  - 9272
SP  - 115
EP  - 129
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944728289&doi=10.1007%2f978-3-319-23267-6_8&partnerID=40&md5=64bde25b287b84a76f9e8f3a6e723fad
DB  - Scopus
KW  - Benchmarking
KW  - Resource utilizations
KW  - Resource demands
KW  - Extraction
KW  - Performance Model
KW  - Statistical approach
KW  - Adaptive estimation techniques
KW  - Estimation techniques
KW  - Performance model generation
KW  - Resource demand estimations
KW  - Resource demand measurements
KW  - Specjenterprise2010
KW  - Systems performance
ER  - 

TY  - JOUR
TI  - Modeling and extracting load intensity profiles
AU  - Von Kistowski, J.
AU  - Herbst, N.
AU  - Kounev, S.
AU  - Groenda, H.
AU  - Stier, C.
AU  - Lehrig, S.
T2  - ACM Transactions on Autonomous and Adaptive Systems
AB  - Today's system developers and operators face the challenge of creating software systems that make efficient use of dynamically allocated resources under highly variable and dynamic load profiles, while at the same time delivering reliable performance. Autonomic controllers, for example, an advanced autoscaling mechanism in a cloud computing context, can benefit from an abstracted load model as knowledge to reconfigure on time and precisely. Existing workload characterization approaches have limited support to capture variations in the interarrival times of incoming work units over time (i.e., a variable load profile). For example, industrial and scientific benchmarks support constant or stepwise increasing load, or interarrival times defined by statistical distributions or recorded traces. These options show shortcomings either in representative character of load variation patterns or in abstraction and flexibility of their format. In this article, we present the Descartes Load Intensity Model (DLIM) approach addressing these issues. DLIM provides a modeling formalism for describing load intensity variations over time. A DLIM instance is a compact formal description of a load intensity trace. DLIM-based tools provide features for benchmarking, performance, and recorded load intensity trace analysis. As manually obtaining and maintaining DLIM instances becomes time consuming, we contribute three automated extraction methods and devised metrics for comparison and method selection. We discuss how these features are used to enhance systemmanagement approaches for adaptations during runtime, and how they are integrated into simulation contexts and enable benchmarking of elastic or adaptive behavior. We show that automatically extracted DLIM instances exhibit an average modeling error of 15.2% over 10 different real-world traces that cover between 2 weeks and 7 months. These results underline DLIM model expressiveness. In terms of accuracy and processing speed, our proposed extraction methods for the descriptive models are comparable to existing time series decomposition methods. Additionally, we illustrate DLIM applicability by outlining approaches of workload modeling in systems engineering that employ or rely on our proposed load intensity modeling formalism. © 2017 ACM.
DA  - 2017///
PY  - 2017
DO  - 10.1145/3019596
VL  - 11
IS  - 4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009291187&doi=10.1145%2f3019596&partnerID=40&md5=378f9006f5e59f185f2e223ec42f94cd
DB  - Scopus
KW  - Benchmarking
KW  - Extraction
KW  - Abstracting
KW  - Dynamic loads
KW  - Load intensity
KW  - Load intensity variation
KW  - Load profile
KW  - Load profiles
KW  - Metamodeling
KW  - Model extraction
KW  - Open workloads
KW  - Transformation
ER  - 

TY  - JOUR
TI  - Check before storing: What is the performance price of content integrity verification in LRU caching?
AU  - Bianchi, G.
AU  - Detti, A.
AU  - Caponi, A.
AU  - Blefari-Melazzi, N.
T2  - Computer Communication Review
AB  - In some network and application scenarios, it is useful to cache content in network nodes on the y, at line rate. Resilience of in-network caches can be improved by guaranteeing that all content therein stored is valid. Digital signatures could be indeed used to verify content integrity and provenance. However, their operation may be much slower than the line rate, thus limiting caching of cryptographically verified objects to a small subset of the forwarded ones. How this affects caching performance? To answer such a question, we devise a simple analytical approach which permits to assess performance of an LRU caching strategy storing a randomly sampled subset of requests. A key feature of our model is the ability to handle traffic beyond the traditional Independent Reference Model, thus permitting us to understand how performance vary in different temporal locality conditions. Results, also verified on real world traces, show that content integrity verification does not necessarily bring about a performance penalty; rather, in some specific (but practical) conditions, performance may even improve.
DA  - 2013///
PY  - 2013
DO  - 10.1145/2500098.2500106
VL  - 43
IS  - 3
SP  - 60
EP  - 67
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901503736&doi=10.1145%2f2500098.2500106&partnerID=40&md5=83fcf69ce2177a6977e112d5c6aadaee
DB  - Scopus
KW  - Authentication
KW  - Performance modeling
KW  - Application scenario
KW  - Performance penalties
KW  - Communication
KW  - Caching
KW  - Performance Model
KW  - Analytical approach
KW  - Content integrity verifications
KW  - Digital signatures
KW  - Electronic document identification systems
KW  - Independent reference models
KW  - Information centric networks
KW  - Information Centric Networks
ER  - 

TY  - JOUR
TI  - Model-based self-Aware performance and resource management using the descartes modeling language
AU  - Huber, N.
AU  - Brosig, F.
AU  - Spinner, S.
AU  - Kounev, S.
AU  - Bähr, M.
T2  - IEEE Transactions on Software Engineering
AB  - Modern IT systems have increasingly distributed and dynamic architectures providing flexibility to adapt to changes in the environment and thus enabling higher resource efficiency. However, these benefits come at the cost of higher system complexity and dynamics. Thus, engineering systems that manage their end-To-end application performance and resource efficiency in an autonomic manner is a challenge. In this article, we present a holistic model-based approach for self-Aware performance and resource management leveraging the Descartes Modeling Language (DML), an architecture-level modeling language for online performance and resource management. We propose a novel online performance prediction process that dynamically tailors the model solving depending on the requirements regarding accuracy and overhead. Using these prediction capabilities, we implement a generic modelbased control loop for proactive system adaptation. We evaluate our model-based approach in the context of two representative case studies showing that with the proposed methods, significant resource efficiency gains can be achieved while maintaining performance requirements. These results represent the first end-To-end validation of our approach, demonstrating its potential for self-Aware performance and resource management in the context of modern IT systems and infrastructures. © 2016 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/TSE.2016.2613863
VL  - 43
IS  - 5
SP  - 432
EP  - 452
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021706923&doi=10.1109%2fTSE.2016.2613863&partnerID=40&md5=6a2eed2925417c44cec41c3a25ea3c25
DB  - Scopus
KW  - Performance
KW  - Adaptation
KW  - Resource allocation
KW  - Natural resources management
KW  - Efficiency
KW  - Model-based OPC
KW  - Modeling languages
KW  - Autonomic
KW  - Model-based
KW  - Modeling language
KW  - Self-aware
KW  - Self-Aware
ER  - 

TY  - JOUR
TI  - Continuous monitoring of software services: Design and application of the kieker framework
AU  - Van Hoorn, A.
AU  - Rohr, M.
AU  - Hasselbring, W.
AU  - Waller, J.
AU  - Ehlers, J.
AU  - Frey, S.
AU  - Kieselhorst, D.
T2  - Continuous Monitoring of Software Services: Design and Application of the Kieker Framework
DA  - 2009///
PY  - 2009
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952129079&partnerID=40&md5=d55f0c7f2ad675b73aa55be6f4a70f72
DB  - Scopus
ER  - 

TY  - CONF
TI  - Performance specification of software components
AU  - Sitaraman, M.
AU  - Kulczycki, G.
AU  - Krone, J.
AU  - Ogden, W.F.
AU  - Reddy, A.L.N.
T2  - Proceedings of SSR'01 2001 Symposium on Software Reusability
AB  - Component-based software engineering is concerned with predictability in both functional and performance behavior, though most formal techniques have typically focused their attention on the former. Reasoning about the (functional or performance) behavior of a component-based system must be compositional in order to be scalable. Compositional performance reasoning demands that components include performance specifications, in addition to descriptions of functional behavior. Unfortunately, as explained in this paper, classical techniques and notations for performance analysis are either unsuitable or unnatural to capture performance behaviors of generic software components. They fail to work in the presence of parameterization and layering. The paper introduces elements of a compositional approach to performance analysis using a detailed example. It explains that performance specification problems are so basic that there are unresolved research issues to be tackled even for the simplest reusable components. These issues must be tackled by any practical proposal for sound performance reasoning. Only then will software developers be able to engineer new systems by Choosing and assembling components that best fit their performance (time and space) requirements.
DA  - 2001///
PY  - 2001
DO  - 10.1145/379377.375223
SP  - 3
EP  - 10
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035790616&doi=10.1145%2f379377.375223&partnerID=40&md5=c90a03984af9adef91250961f9bbd3c9
DB  - Scopus
KW  - Software engineering
KW  - Specifications
KW  - Data storage equipment
KW  - Data structures
KW  - Software components
KW  - Data Structures
KW  - Generic Objects
KW  - Storage Management
KW  - Time & Space Analysis
ER  - 

TY  - JOUR
TI  - Workload characterization study of the 1998 World Cup Web site
AU  - Arlitt, M.
AU  - Jin, T.
T2  - IEEE Network
AB  - This article presents a detailed workload characterization study of the 1998 World Cup Web site. Measurements from this site were collected over a three-month period. During this time the site received 1.35 billion requests, making this the largest Web workload analyzed to date. By examining this extremely busy site and through comparison with existing characterization studies, we are able to determine how Web server workloads are evolving. We find that improvements in the caching architecture of the World Wide Web are changing the workloads of Web servers, but major improvements to that architecture are still necessary. In particular, we uncover evidence that a better consistency mechanism is required for World Wide Web caches.
DA  - 2000///
PY  - 2000
DO  - 10.1109/65.844498
VL  - 14
IS  - 3
SP  - 30
EP  - 37
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033690948&doi=10.1109%2f65.844498&partnerID=40&md5=0f65b842deb7348874717cdb0ff42e17
DB  - Scopus
KW  - Telecommunication traffic
KW  - Client server computer systems
KW  - World Wide Web
KW  - Response time (computer systems)
KW  - Data communication systems
KW  - Computer systems programming
KW  - Congestion control (communication)
KW  - Web servers
ER  - 

TY  - JOUR
TI  - CloudStore — towards scalability, elasticity, and efficiency benchmarking and analysis in Cloud computing
AU  - Lehrig, S.
AU  - Sanders, R.
AU  - Brataas, G.
AU  - Cecowski, M.
AU  - Ivanšek, S.
AU  - Polutnik, J.
T2  - Future Generation Computer Systems
AB  - This paper describes CloudStore, an open source application that lends itself to analyzing key characteristics of Cloud computing platforms. Based on an earlier standard from transaction processing, it represents a simplified version of a typical e-commerce application–an electronic book store. We detail how a deployment on a popular public cloud offering can be instrumented to gain insight into system characteristics such as capacity, scalability, elasticity and efficiency. Based on our insights, we create a CloudStore performance model, allowing to accurately predict such properties already at design time. © 2017 Elsevier B.V.
DA  - 2018///
PY  - 2018
DO  - 10.1016/j.future.2017.04.018
VL  - 78
SP  - 115
EP  - 126
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018428867&doi=10.1016%2fj.future.2017.04.018&partnerID=40&md5=2497fbf19368ffe5ac5014e417b74cad
DB  - Scopus
KW  - Cloud computing
KW  - Performance
KW  - Elasticity
KW  - Scalability
KW  - Web services
KW  - Open systems
KW  - Efficiency
KW  - Measurements
KW  - Amazon web services
KW  - Cloud computing platforms
KW  - Amazon Web Services
KW  - E-Commerce applications
KW  - Capacity
KW  - Efficiency benchmarking
KW  - Open source application
KW  - System characteristics
KW  - TPC-W
ER  - 

TY  - JOUR
TI  - The social bookmark and publication management system bibsonomy
AU  - Benz, D.
AU  - Hotho, A.
AU  - Jäschke, R.
AU  - Krause, B.
AU  - Mitzlaff, F.
AU  - Schmitz, C.
AU  - Stumme, G.
T2  - VLDB Journal
AB  - Social resource sharing systems are central elements of the Web 2.0 and use the same kind of lightweight knowledge representation, called folksonomy. Their large user communities and ever-growing networks of user-generated content have made them an attractive object of investigation for researchers from different disciplines like Social Network Analysis, Data Mining, Information Retrieval or Knowledge Discovery. In this paper, we summarize and extend our work on different aspects of this branch of Web 2.0 research, demonstrated and evaluated within our own social bookmark and publication sharing system BibSonomy, which is currently among the three most popular systems of its kind. We structure this presentation along the different interaction phases of a user with our system, coupling the relevant research questions of each phase with the corresponding implementation issues. This approach reveals in a systematic fashion important aspects and results of the broad bandwidth of folksonomy research like capturing of emergent semantics, spam detection, ranking algorithms, analogies to search engine log data, personalized tag recommendations and information extraction techniques. We conclude that when integrating a real-life application like BibSonomy into research, certain constraints have to be considered; but in general, the tight interplay between our scientific work and the running system has made BibSonomy a valuable platform for demonstrating and evaluating Web 2.0 research. © 2010 Springer-Verlag.
DA  - 2010///
PY  - 2010
DO  - 10.1007/s00778-010-0208-4
VL  - 19
IS  - 6
SP  - 849
EP  - 875
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650521205&doi=10.1007%2fs00778-010-0208-4&partnerID=40&md5=445da64fbb3aacb6cb5a29187fdca7e6
DB  - Scopus
KW  - Web services
KW  - Semantics
KW  - Classification (of information)
KW  - Log data
KW  - Semantic Web
KW  - Data mining
KW  - Research
KW  - Knowledge representation
KW  - Running systems
KW  - Research questions
KW  - BibSonomy
KW  - Broad bandwidths
KW  - Central elements
KW  - Collaborative tagging
KW  - Electric network analysis
KW  - Folksonomies
KW  - Folksonomy
KW  - Information extraction techniques
KW  - Information retrieval
KW  - Knowledge Discovery
KW  - Management systems
KW  - Ranking algorithm
KW  - Real-life applications
KW  - Resource sharing systems
KW  - Search engines
KW  - Sharing systems
KW  - Social Network Analysis
KW  - Spam detection
KW  - Tag recommendations
KW  - User communities
KW  - User-generated content
KW  - Web 2.0
ER  - 

TY  - JOUR
T2  - Standard Performance Evaluation Corporation (SPEC) SPEC JEnterprise 2010 Design Document
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058336784&partnerID=40&md5=c011fe9c7b9ddbb71efc44f94d299c81
DB  - Scopus
ER  - 

TY  - JOUR
TI  - An experimental performance evaluation of autoscalers for complex workflows
AU  - Ilyushkin, A.
AU  - Ali-Eldin, A.
AU  - Herbst, N.
AU  - Bauer, A.
AU  - Papadopoulos, A.V.
AU  - Epema, D.
AU  - Iosup, A.
T2  - ACM Transactions on Modeling and Performance Evaluation of Computing Systems
DA  - 2018///
PY  - 2018
VL  - 3
IS  - 2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058277561&partnerID=40&md5=83e1b37d732f52cf8862eaa5accff449
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Weaveworks Inc Sock Shop: A Microservice Demo Application Accessed: 19.10.2017
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058315585&partnerID=40&md5=8bf7469d03ffcaae09cbc4e32c6bfba8
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Greencloud - The Green Cloud Simulator
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100766769&partnerID=40&md5=9e3da01ca853c62fca50013719ba2773
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Inside the linux 2.6 completely fair scheduler
AU  - Jones, M.T.
T2  - Inside the Linux 2.6 Completely Fair Scheduler
DA  - 2009///
PY  - 2009
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751519891&partnerID=40&md5=b2f473806b87bc5dd5b8983bd2360473
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Enhanced Intel SpeedStep Technology for the Intel Pentium M Processor
DA  - 2004///
PY  - 2004
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746279684&partnerID=40&md5=caf5513fb73ecdf14a9b669e30ddc5b4
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Samsung V-NAND Ssd 860 Evo
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100738579&partnerID=40&md5=6cfd312038f753ade131473f69c87c38
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Network Simulation
DA  - 2018///
PY  - 2018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100702819&partnerID=40&md5=d58e4d3fee94e93ff4f03b0d864e8f36
DB  - Scopus
ER  - 

TY  - CONF
TI  - iFogSim: A toolkit for modeling and simulation of resource management techniques in the Internet of Things, Edge and Fog computing environments
AU  - Gupta, H.
AU  - Vahid Dastjerdi, A.
AU  - Ghosh, S.K.
AU  - Buyya, R.
T2  - Software - Practice and Experience
AB  - Internet of Things (IoT) aims to bring every object (eg, smart cameras, wearable, environmental sensors, home appliances, and vehicles) online, hence generating massive volume of data that can overwhelm storage systems and data analytics applications. Cloud computing offers services at the infrastructure level that can scale to IoT storage and processing requirements. However, there are applications such as health monitoring and emergency response that require low latency, and delay that is caused by transferring data to the cloud and then back to the application can seriously impact their performances. To overcome this limitation, Fog computing paradigm has been proposed, where cloud services are extended to the edge of the network to decrease the latency and network congestion. To realize the full potential of Fog and IoT paradigms for real-time analytics, several challenges need to be addressed. The first and most critical problem is designing resource management techniques that determine which modules of analytics applications are pushed to each edge device to minimize the latency and maximize the throughput. To this end, we need an evaluation platform that enables the quantification of performance of resource management policies on an IoT or Fog computing infrastructure in a repeatable manner. In this paper we propose a simulator, called iFogSim, to model IoT and Fog environments and measure the impact of resource management techniques in latency, network congestion, energy consumption, and cost. We describe two case studies to demonstrate modeling of an IoT environment and comparison of resource management policies. Moreover, scalability of the simulation toolkit of RAM consumption and execution time is verified under different circumstances. Copyright © 2017 John Wiley & Sons, Ltd.
DA  - 2017///
PY  - 2017
DO  - 10.1002/spe.2509
VL  - 47
SP  - 1275
EP  - 1296
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021248358&doi=10.1002%2fspe.2509&partnerID=40&md5=f5351d8461158bc62bf3f14acd916f12
DB  - Scopus
KW  - Fog computing
KW  - Internet of Things (IoT)
KW  - Edge computing
KW  - Internet of things
KW  - Fog
KW  - Energy utilization
KW  - Resource allocation
KW  - Distributed computer systems
KW  - Natural resources management
KW  - Computing environments
KW  - Digital storage
KW  - Computing infrastructures
KW  - Internet of Things (IOT)
KW  - Resource management techniques
KW  - Random access storage
KW  - Model and simulation
KW  - Environmental management
KW  - Online systems
KW  - Resource management policy
KW  - Industrial management
KW  - Domestic appliances
KW  - Evaluation platforms
KW  - modeling and simulation
ER  - 

TY  - JOUR
T2  - Desktop Hdd Product Manual
DA  - 2016///
PY  - 2016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100813024&partnerID=40&md5=689610d04b90f0283edb78919307b376
DB  - Scopus
ER  - 

TY  - JOUR
T2  - IP Wan Emulator
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100762625&partnerID=40&md5=635556cb558ca45595414e6e45d05e35
DB  - Scopus
ER  - 

TY  - CONF
TI  - O-ICN Simulator (OICNSIM) an NS-3 based simulator for overlay information centric networking (O-ICN)
AU  - Agrawal, S.
AU  - Shailendra, S.
AU  - Panigrahi, B.
AU  - Rath, H.K.
AU  - Simha, A.
T2  - Proceedings of the Annual International Conference on Mobile Computing and Networking, MOBICOM
AB  - Information Centric Networking (ICN) is envisioned to advocate the efficient usage of the networking resources. Overlay ICN (O-ICN), a newly proposed architecture for ICN, is designed to be backward compatible and interoperable with the current Internet. O-ICN architecture can also be incrementally deployed without forcing the operators to upgrade the entire network at the same time. In this paper, we present the Overlay ICN simulator (OICNSIM) to simulate the O-ICN architecture. Additionally, we have studied the performance of OICNSIM for different ICN caching policies. © 2018 Association for Computing Machinery.
DA  - 2018///
PY  - 2018
DO  - 10.1145/3265997.3266000
SP  - 13
EP  - 15
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061480842&doi=10.1145%2f3265997.3266000&partnerID=40&md5=a638f492f9ebe12eb8aafe3cfdf2ce62
DB  - Scopus
KW  - Performance
KW  - Network architecture
KW  - Proposed architectures
KW  - 'current
KW  - Simulators
KW  - Information-centric networkings
KW  - Backward compatible
KW  - Caching policy
KW  - Forcings
KW  - Networking architecture
ER  - 

TY  - CONF
TI  - Kalimucho: Middleware for mobile applications
AU  - Da, K.
AU  - Dalmau, M.
AU  - Roose, P.
T2  - Proceedings of the ACM Symposium on Applied Computing
AB  - Developing ubiquitous applications is particularly complex. Beyond the dynamic aspect of such applications, the evolution of computing towards the multiplication of mobile access terminals is not making things easier. One solution to simplifying the development and use of such applications is to use software platforms dedicated to deployment and adaptation of applications and handling the heterogeneity of peripherals. They allow designers to focus on business aspects and facilitate reuse. The Kalimucho platform was designed and developed against this background. It executes and supervises applications based on software components. Copyright 2014 ACM.
DA  - 2014///
PY  - 2014
DO  - 10.1145/2554850.2554883
SP  - 413
EP  - 419
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905652787&doi=10.1145%2f2554850.2554883&partnerID=40&md5=c88b567dba93ee6384b57b3bbd3ba1ee
DB  - Scopus
KW  - Performance
KW  - Reliability
KW  - Application programs
KW  - Measurement
KW  - Middleware
KW  - Mobile applications
KW  - Design
KW  - Measurements
KW  - Software component
KW  - Query languages
KW  - Business aspects
KW  - Dynamic aspects
KW  - Languages
KW  - Mobile access
KW  - Software platforms
KW  - Ubiquitous application
ER  - 

TY  - CONF
TI  - A green analysis of mobile cloud computing applications
AU  - Zhan, K.
AU  - Lung, C.-H.
AU  - Srivastava, P.
T2  - Proceedings of the ACM Symposium on Applied Computing
AB  - With the widespread of rich mobile applications, the usage of mobile devices, especially smart phones and tablets, has become popular nowadays. However, battery in the mobile devices often limits continuous usages with its small size and capacity. Therefore, power consumption of mobile devices is a critical issue, not only for extending lifetime use of mobile devices, but also for creating a green IT which is a raising concern in academic and industrial communities. The goal of this paper is to investigate the power consumption of mobile devices and resource usages for various applications. To meet the goal, we have performed a number of experiments and detailed evaluations of resource usages and power consumption for various applications using a number of tools. In addition, we have measured performance metrics for applications either using a mobile device or running in the Amazon cloud. We examine the impact of various factors and provide insights on power consumption for different mobile applications. Copyright 2014 ACM.
DA  - 2014///
PY  - 2014
DO  - 10.1145/2554850.2555069
SP  - 357
EP  - 362
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905637579&doi=10.1145%2f2554850.2555069&partnerID=40&md5=a1b9a6bbb272621e55d54e65fd49f773
DB  - Scopus
KW  - Mobile computing
KW  - Green computing
KW  - Mobile cloud computing
KW  - CPU
KW  - Electric power utilization
KW  - Power consumption
KW  - Mobile applications
KW  - Resource usage
KW  - Performance metrics
KW  - Mobile devices
KW  - Amazon
KW  - Critical issues
KW  - Industrial communities
ER  - 

TY  - JOUR
T2  - Containercloudsim: An Environment for Modeling and Simulation of Containers in Cloud Data Centers
DA  - 2016///
PY  - 2016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100702586&partnerID=40&md5=ac0d531391dbc5a95cf32d5ba3db2283
DB  - Scopus
ER  - 

TY  - JOUR
AU  - Bongers, E.
AU  - Pouwelse, J.
T2  - A Survey of P2P Multidimensional Indexing Structures
DA  - 2015///
PY  - 2015
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009829721&partnerID=40&md5=2af19d476143121dc7585daaf730226d
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Design and evaluation of a scalable smart city software platform with large-scale simulations
AU  - de M. Del Esposte, A.
AU  - Santana, E.F.Z.
AU  - Kanashiro, L.
AU  - Costa, F.M.
AU  - Braghetto, K.R.
AU  - Lago, N.
AU  - Kon, F.
T2  - Future Generation Computer Systems
AB  - Smart Cities combine advances in Internet of Things, Big Data, Social Networks, and Cloud Computing technologies with the demand for cyber–physical applications in areas of public interest, such as Health, Public Safety, and Mobility. The end goal is to leverage the use of city resources to improve the quality of life of its citizens. Achieving this goal, however, requires advanced support for the development and operation of applications in a complex and dynamic environment. Middleware platforms can provide an integrated infrastructure that enables solutions for smart cities by combining heterogeneous city devices and providing unified, high-level facilities for the development of applications and services. Although several smart city platforms have been proposed in the literature, there are still open research and development challenges related to their scalability, maintainability, interoperability, and reuse in the context of different cities, to name a few. Moreover, available platforms lack extensive scientific validation, which hinders a comparative analysis of their applicability. Aiming to close this gap, we propose InterSCity, a microservices-based, open-source, smart city platform that enables the collaborative development of large-scale systems, applications, and services for the cities of the future, contributing to turn them into truly smart cyber–physical environments. In this paper, we present the architecture of the InterSCity platform, followed by a comprehensive set of experiments that evaluate its scalability. The experiments were conducted using a smart city simulator to generate realistic workloads used to assess the platform in extreme conditions. The experimental results demonstrate that the platform can scale horizontally to handle the highly dynamic demands of a large smart city while maintaining low response times. The experiments also show the effectiveness of the technique used to generate synthetic workloads. © 2018 Elsevier B.V.
DA  - 2019///
PY  - 2019
DO  - 10.1016/j.future.2018.10.026
VL  - 93
SP  - 427
EP  - 441
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056458360&doi=10.1016%2fj.future.2018.10.026&partnerID=40&md5=7d224d740a6105559dbb9c95abd0b280
DB  - Scopus
KW  - Microservices
KW  - Big data
KW  - Smart city
KW  - Scalability
KW  - Smart cities
KW  - Simulation
KW  - Distributed computer systems
KW  - Middleware
KW  - Open source software
KW  - Open systems
KW  - Dynamics
KW  - Open source
KW  - Open sources
KW  - Large scale systems
KW  - Cloud computing technologies
KW  - Collaborative development
KW  - Development and operations
KW  - Integrated infrastructure
KW  - Smart urban spaces
KW  - Social sciences computing
KW  - Urban spaces
ER  - 

TY  - JOUR
AU  - Blog, N.T.
T2  - Netflix Conductor: A Microservices Orchestrator
DA  - 2025///
PY  - 2025
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088595149&partnerID=40&md5=4e2096496f24a0abd19ff8a02142cc5f
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Power consumption of WLAN network elements
AU  - Chiaravalloti, S.
AU  - Idzikowski, F.
AU  - Budzisz, L.
T2  - Power Consumption of WLAN Network Elements
DA  - 2011///
PY  - 2011
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865796739&partnerID=40&md5=c02a2b7082d8f28c1c0b3914d64daff3
DB  - Scopus
ER  - 

TY  - CONF
TI  - Studying the energy consumption of data transfers in Clouds: The Ecofen approach
AU  - Cornea, B.F.
AU  - Orgerie, A.-C.
AU  - Lefèvre, L.
T2  - 2014 IEEE 3rd International Conference on Cloud Networking, CloudNet 2014
AB  - Energy consumption is one of the main limiting factors for designing large scale Clouds. Evaluating the energy consumption of Clouds networking architectures and providing multi-level views required by providers and users, is a challenging issue. In this paper, we show how to evaluate and understand network choices (protocols, topologies) in terms of contributions to the energy consumption of the global Cloud infrastructures. By applying the ECOFEN model (Energy Consumption mOdel For End-to-end Networks) and the corresponding simulation framework, we profile and analyze the energy consumption of data transfers in Clouds. © 2014 IEEE.
DA  - 2014///
PY  - 2014
DO  - 10.1109/CloudNet.2014.6968983
SP  - 143
EP  - 148
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925859008&doi=10.1109%2fCloudNet.2014.6968983&partnerID=40&md5=dcd337cdf783b075b4254c04e9ecb48b
DB  - Scopus
KW  - energy consumption
KW  - simulation
KW  - Green computing
KW  - Energy utilization
KW  - Data transfer
KW  - Energy consumption model
KW  - Simulation framework
KW  - Cloud data
KW  - Networking architecture
KW  - Cloud data transfers
KW  - End-to-end network
KW  - ethernet networks
KW  - Ethernet networks
KW  - Global clouds
ER  - 

TY  - JOUR
TI  - Simulating services-based systems hosted in networks with dynamic topology
AU  - Novotny, P.
AU  - Wolf, A.L.
T2  - Simulating Services-based Systems Hosted in Networks with Dynamic Topology
DA  - 2016///
PY  - 2016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047181173&partnerID=40&md5=bd1c57a545e32645f42742bd48b8d8ec
DB  - Scopus
ER  - 

TY  - CONF
TI  - SENS: A sensor, environment and network simulator
AU  - Sundresh, S.
AU  - Kim, W.
AU  - Agha, G.
T2  - Proceedings of the IEEE Annual Simulation Symposium
AB  - Recent advances in micro electro-mechanical systems and VLSI lithography have enabled the miniaturization of sensors and controllers. Such minitiarization facilitates the deployment of large-scale wireless sensor networks (WSNs). However, the considerable cost of deploying and maintaining large-scale WSNs for experimental purposes makes simulation useful in developing dependable and portable WSN applications. SENS is a customizable sensor network simulator for WSN applications, consisting of interchangeable and extensible components for applications, network communication, and the physical environment. Multiple component implementations in SENS offer varying degrees of realism. Users can assemble application-specific environments; such environments are modeled in SENS by their different signal propagation characteristics. The same source code that is executed on simulated sensor nodes in SENS may also be deployed on actual sensor nodes; this enables application portability. Furthermore, SENS provides diagnostic facilities such as power utilization analysis for development of dependable applications. We validate and demonstrate usability of these capabilities through analyzing two simple WSN services.
DA  - 2004///
PY  - 2004
SP  - 221
EP  - 228
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142724988&partnerID=40&md5=bc33435ebf398a4fabe49df46b9366d6
DB  - Scopus
KW  - Sensors
KW  - Signal processing
KW  - Computation theory
KW  - Wireless telecommunication systems
KW  - Microprocessor chips
KW  - VLSI circuits
KW  - Control equipment
KW  - Lithography
KW  - Microelectromechanical devices
KW  - Network communication
KW  - VLSI lithography
KW  - Wireless sensor networks (WSN)
ER  - 

TY  - CONF
TI  - DockerSim: Full-stack simulation of container-based Software-as-a-Service (SaaS) cloud deployments and environments
AU  - Nikdel, Z.
AU  - Gao, B.
AU  - Neville, S.W.
T2  - 2017 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing, PACRIM 2017 - Proceedings
AB  - Container-based Software-as-a-Service (SaaS) systems are rapidly emerging as a dominate cloud deployment paradigm, as supported via Docker, etc. This has opened new research avenues as lighter-weight containers replace heavierweight virtual machines (VMs) as atomic cloud deployment units. Advancing such research requires sufficiently rich, containeraware cloud simulation frameworks so as to explore issues such as optimal cloud orchestration and elastic service strategies, achieving assured application-layer quality of service (QoS), etc. This work presents DockerSim, an extension and augmentation of the iCanCloud OMNet++ cloud simulator to incorporate: i) a full container deployment and behavioral layer, ii) full packet-level network and protocol behaviors, iii) full multi-layer OS process scheduling behaviors, and iv) a generic queuing network approach to modeling application-layer SaaS deployments. To our knowledge DockerSim is the first cloud simulation platform fully incorporating capabilities (i)-(iv), as required to rigorous explore a wide range of timing sensitive container-based computing cloud and SaaS-deployment issues. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/PACRIM.2017.8121898
VL  - 2017-January
SP  - 1
EP  - 6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043471766&doi=10.1109%2fPACRIM.2017.8121898&partnerID=40&md5=85b8169e3568e6f3631063bdda732342
DB  - Scopus
KW  - Cloud computing
KW  - Cloud Computing
KW  - Cloud deployments
KW  - Containers
KW  - Simulation
KW  - Quality of service
KW  - Software as a service (SaaS)
KW  - Web services
KW  - Signal processing
KW  - Computer software
KW  - Application layers
KW  - Cloud simulation
KW  - Cloud simulation platforms
KW  - Container-as-a-Service(CaaS)
KW  - Docker-style container simulation
KW  - Full-stack cloud simulation
KW  - Model application
KW  - Process scheduling
KW  - Software-as-a-Service (SaaS)
ER  - 

TY  - JOUR
T2  - Cisco Packet Tracer
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091950863&partnerID=40&md5=13e8d067e8816e2ac2de13a00c3e8d38
DB  - Scopus
ER  - 

TY  - CONF
TI  - Scalability evaluation of an energy-aware resource management system for clusters of web servers
AU  - Kiertscher, S.
AU  - Schnor, B.
T2  - Proceedings of the 2015 International Symposium on Performance Evaluation of Computer and Telecommunication Systems, SPECTS 2015 - Part of SummerSim 2015 Multiconference
AB  - For green cluster computing resource management systems have to be energy-aware. CHERUB is such an energy-aware resource management system which works together with the Linux Virtual Server. Experiments in a small cluster setup with two nodes have shown the benefit of CHERUB. This paper presents necessary design changes to make CHERUB also work in big cluster setups. Our methodological approach is two-fold. First, we present unit measurements to evaluate the scaling of the re-implemented functions. Second, a cluster simulator is presented and validated which makes it possible to test CHERUB for backend clusters of arbitrary size. © 2015 Society for Modeling and Simulation International.
DA  - 2015///
PY  - 2015
DO  - 10.1109/SPECTS.2015.7285285
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992036358&doi=10.1109%2fSPECTS.2015.7285285&partnerID=40&md5=8fde33c6f96f037481f6dcf2f4c96100
DB  - Scopus
KW  - Energy efficiency
KW  - Scalability
KW  - Energy Efficiency
KW  - Power management
KW  - Simulation
KW  - Computer architecture
KW  - Resource allocation
KW  - Natural resources management
KW  - Green IT
KW  - Green-IT
KW  - Energy management systems
KW  - Cluster computing
KW  - Computer operating systems
KW  - Computing resource management
KW  - Energy awareness
KW  - Resource management systems
KW  - Cluster Computing
KW  - Energy Awareness
KW  - Integrated modeling
KW  - Integrated Modeling and Measurement
KW  - Methodological approach
KW  - Scalability evaluation
KW  - Scalability Studies
ER  - 

TY  - CONF
TI  - The Architecture of Kaligreen V2: A Middleware Aware of Hardware Opportunities to Save Energy
AU  - Valera, H.H.L.
AU  - Dalmau, M.
AU  - Roose, P.
AU  - Herzog, C.
T2  - 2019 6th International Conference on Internet of Things: Systems, Management and Security, IOTSMS 2019
AB  - Nowadays, energy saving in the use of information technologies is a very important issue both from the economic and sustainability point of view. Many scientists investigate methods to save energy at different application levels (cloud: i.e., architectures, grid: i.e., middlewares and frameworks and hardware management: i.e., operating systems) and many of them agree on the strategy of executing programs, processes or virtual machines only using the time and resources that are strictly necessary. For this, it is necessary to plan strategies for deployment and relocation of processes; but always taking into account hardware repercussions and the knowledge of the architecture and applications behavior. On the other hand, it has already been demonstrated that the use of microservices brings numerous advantages in availability and efficiency; but we do not find many jobs that exploit this technique on the energy level. In this article, we present the architecture of a middleware for distributed microservices-based applications, which allows any negotiation-based scheduling algorithm to duplicate or move microservices from one device to another in a non-centralized way for energy savings, taking into account the consumption characteristics of the microservices and the capabilities that the hardware components offer. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/IOTSMS48152.2019.8939237
SP  - 79
EP  - 86
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077969153&doi=10.1109%2fIOTSMS48152.2019.8939237&partnerID=40&md5=7b937f6435a19d89b288227377d059e7
DB  - Scopus
KW  - microservices
KW  - middleware
KW  - energy
KW  - Internet of things
KW  - Application programs
KW  - Application level
KW  - Program processors
KW  - Energy conservation
KW  - consumption
KW  - CPU
KW  - hard disk
KW  - Hard disk storage
KW  - Hardware components
KW  - Hardware management
KW  - Information use
KW  - Middleware
KW  - network
KW  - Network architecture
KW  - Networks (circuits)
KW  - Save energy
KW  - Scheduling algorithms
ER  - 

TY  - CONF
TI  - μqSim: Enabling Accurate and Scalable Simulation for Interactive Microservices
AU  - Zhang, Y.
AU  - Gan, Y.
AU  - Delimitrou, C.
T2  - Proceedings - 2019 IEEE International Symposium on Performance Analysis of Systems and Software, ISPASS 2019
AB  - Current cloud services are moving away from monolithic designs and towards graphs of many loosely-coupled, single-concerned microservices. Microservices have several advantages, including speeding up development and deployment, allowing specialization of the software infrastructure, and helping with debugging and error isolation. At the same time they introduce several hardware and software challenges. Given that most of the performance and efficiency implications of microservices happen at scales larger than what is available outside production deployments, studying such effects requires designing the right simulation infrastructures. We present j);qSim, a scalable and validated queueing network simulator designed specifically for interactive microser-vices. μqSim provides detailed intra- and inter-microservice models that allow it to faithfully reproduce the behavior of complex, many-tier applications. μqSim is also modular, allowing reuse of individual models across microservices and end-to-end applications. We have validated μqSim both against simple and more complex microservices graphs, and have shown that it accurately captures performance in terms of throughput and tail latency. Finally, we use μqSim to model the tail at scale effects of request fanout, and the performance impact of power management in latency -sensitive microservices. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/ISPASS.2019.00034
SP  - 212
EP  - 222
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065406209&doi=10.1109%2fISPASS.2019.00034&partnerID=40&md5=6d6d2dddbd199083af596f9b032e8407
DB  - Scopus
KW  - Program debugging
KW  - Complex networks
KW  - Loosely coupled
KW  - Hardware and software
KW  - Performance impact
KW  - Network simulators
KW  - End-to-end application
KW  - Individual models
KW  - Monolithic design
KW  - Software infrastructure
ER  - 

TY  - CONF
TI  - Adaptive power panel of cloud computing controlling cloud power consumption
AU  - Azmy, N.M.
AU  - El-Maddah, I.A.M.
AU  - Mohamed, H.K.
T2  - ACM International Conference Proceeding Series
AB  - Cloud computing had created a new era of network design, where end-users can get their required services without having to purchase expensive infrastructure or even to care about troubleshooting. Power consumption is a challenge facing the Cloud Providers to operate their Datacenters. One solution to overcome this is the Virtual Machine (VM) migration, which is a technique used to switch under-utilized hosts to sleep mode in order to save power, and to avoid over-utilized hosts from Service Level Agreement (SLA) violation. But still the problem is that the Cloud Service Provider apply a single policy on all nodes. Our proposed solution is an adaptive power panel where different policies can be applied based on both of the nature of the tasks running on hosts, and the Cloud Provider decision. © 2016 ACM.
DA  - 2016///
PY  - 2016
DO  - 10.1145/2944165.2944167
VL  - 28-29-May-2016
SP  - 9
EP  - 14
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985990751&doi=10.1145%2f2944165.2944167&partnerID=40&md5=c5b64478913d045dc22f54d9b7cff5fa
DB  - Scopus
KW  - Cloud computing
KW  - Placement
KW  - Software engineering
KW  - Green Computing
KW  - Green computing
KW  - Distributed computer systems
KW  - Virtual machines
KW  - Electric power utilization
KW  - Migration
KW  - Virtual Machine
KW  - Adaptive
KW  - Allocation
KW  - Cloudsim
KW  - Selection
ER  - 

TY  - CONF
TI  - Kali green: A distributed scheduler for energy saving
AU  - Lvarez-Valera, H.H.
AU  - Roose, P.
AU  - Dalmau, M.
AU  - Herzog, C.
AU  - Respicio, K.
T2  - Procedia Computer Science
AB  - A commonplace issue with portable technology is battery efficiency. While many industries are trying their best to improve battery life without sacrificing a products quality and efficiency, we believe that further can be done to improve battery consumption on ones mobile devicefrom tablets to smartphones to laptops to everything else. Many applications on these devices are based on a microservice architecture. In this article, we introduce a new algorithm KaliGreen that can maneuver the microservices within a network of devices in order to maximize the run-time of a microservice-based application; moreover, KaliGreen allows a 54% increase in the average run-time of an application by shifting microservices from 6 devices (as example) with low battery or inefficient processing ratios to devices in better conditions. To achieve this, KaliGreen utilizes KaliMucho middleware, which is able manipulate microservices in run-time. This algorithm provides a plausible solution to maximizing energy consumption within a network of devices. © 2018 The Authors. Published by Elsevier Ltd.
DA  - 2018///
PY  - 2018
DO  - 10.1016/j.procs.2018.10.172
VL  - 141
SP  - 223
EP  - 230
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058349351&doi=10.1016%2fj.procs.2018.10.172&partnerID=40&md5=762a84024c90ed200594143af3fc90ed
DB  - Scopus
KW  - Microservices
KW  - Smartphones
KW  - Green Computing
KW  - Green computing
KW  - Energy utilization
KW  - Energy conservation
KW  - mHealth
KW  - Middleware
KW  - Battery consumption
KW  - Battery efficiencies
KW  - Battery life
KW  - Distributed applicatioggns
KW  - Distributed schedulers
KW  - Electric batteries
KW  - Portable technologies
KW  - Products quality
ER  - 

TY  - JOUR
T2  - Debug Your App, Not Your Environment
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087273352&partnerID=40&md5=d1ea1829764f9ff729983d6da3150d70
DB  - Scopus
ER  - 

TY  - JOUR
AU  - Services, A.W.
T2  - Implementing Microservices on Aws
DA  - 2019///
PY  - 2019
VL  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100772318&partnerID=40&md5=c64d1506936741d57056a6ff7d1375dc
DB  - Scopus
ER  - 

TY  - CONF
TI  - ECOFEN: An end-to-end energy cost model and simulator for evaluating power consumption in large-scale networks
AU  - Orgerie, A.-C.
AU  - Lefèvre, L.
AU  - Guérin-Lassous, I.
AU  - Lopez Pacheco, D.M.
T2  - 2011 IEEE International Symposium on a World of Wireless, Mobile and Multimedia Networks, WoWMoM 2011 - Digital Proceedings
AB  - Wired networks are increasing in size and their power consumption is becoming a matter of concern. Evaluating the end-to-end electrical cost of new network architectures and protocols is difficult due to the lack of monitored realistic infrastructures. We propose an End-to-End energy Cost mOdel and simulator For Evaluating power consumption in large-scale Networks (ECOFEN) whose user's entries are the network topology and traffic. Based on configurable measurement of different network components (routers, switches, NICs, etc.), it provides the power consumption of the overall network including the end-hosts as well as the power consumption of each equipment over time. © 2011 IEEE.
DA  - 2011///
PY  - 2011
DO  - 10.1109/WoWMoM.2011.5986203
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052698432&doi=10.1109%2fWoWMoM.2011.5986203&partnerID=40&md5=59543a30d002fd0aae1815e186a1e3f4
DB  - Scopus
KW  - Energy efficiency
KW  - Costs
KW  - Topology
KW  - Network architecture
KW  - Network topology
KW  - Power consumption
KW  - Simulators
KW  - Energy cost
KW  - Network simulator
KW  - Wireless networks
KW  - Wired networks
KW  - Configurable
KW  - Electric network topology
KW  - Electrical costs
KW  - Energy cost model
KW  - Large-scale network
KW  - Switches
KW  - Time switches
ER  - 

TY  - JOUR
TI  - Automated test-based learning and verification of performance models for microservices systems
AU  - Camilli, M.
AU  - Janes, A.
AU  - Russo, B.
T2  - Journal of Systems and Software
AB  - Effective and automated verification techniques able to provide assurances of performance and scalability are highly demanded in the context of microservices systems. In this paper, we introduce a methodology that applies specification-driven load testing to learn the behavior of the target microservices system under multiple deployment configurations. Testing is driven by realistic workload conditions sampled in production. The sampling produces a formal description of the users’ behavior through a Discrete Time Markov Chain. This model drives multiple load testing sessions that query the system under test and feed a Bayesian inference process which incrementally refines the initial model to obtain a complete specification from run-time evidence as a Continuous Time Markov Chain. The complete specification is then used to conduct automated verification by using probabilistic model checking and to compute a configuration score that evaluates alternative deployment options. This paper introduces the methodology, its theoretical foundation, and the toolchain we developed to automate it. Our empirical evaluation shows its applicability, benefits, and costs on a representative microservices system benchmark. We show that the methodology detects performance issues, traces them back to system-level requirements, and, thanks to the configuration score, provides engineers with insights on deployment options. The comparison between our approach and a selected state-of-the-art baseline shows that we are able to reduce the cost up to 73% in terms of number of tests. The verification stage requires negligible execution time and memory consumption. We observed that the verification of 360 system-level requirements took ∼1 minute by consuming at most 34 KB. The computation of the score involved the verification of ∼7k (automatically generated) properties verified in ∼72 seconds using at most ∼50 KB. © 2022 The Author(s)
DA  - 2022///
PY  - 2022
DO  - 10.1016/j.jss.2022.111225
VL  - 187
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124274286&doi=10.1016%2fj.jss.2022.111225&partnerID=40&md5=5f4a1691c78038edaca0ea24e420f6f2
DB  - Scopus
KW  - Microservices
KW  - Automation
KW  - Microservice
KW  - Markov processes
KW  - Learning systems
KW  - Model checking
KW  - Continuous time systems
KW  - Performance Modeling
KW  - Specifications
KW  - Inference engines
KW  - Bayesian networks
KW  - Load testing
KW  - Automated test
KW  - Automated verification
KW  - Markov modeling
KW  - Markov models
KW  - Model learning
KW  - Performance testing
KW  - Statistical tests
KW  - System-level requirements
KW  - Test-based model learning
KW  - Verification techniques
ER  - 

TY  - CONF
TI  - Exploring alternative approaches to implement an elasticity policy
AU  - Ghanbari, H.
AU  - Simmons, B.
AU  - Litoiu, M.
AU  - Iszlai, G.
T2  - Proceedings - 2011 IEEE 4th International Conference on Cloud Computing, CLOUD 2011
AB  - An elasticity policy governs how and when resources (e.g., application server instances at the PaaS layer) are added to and/or removed from a cloud environment. The elasticity policy can be implemented as a conventional control loop or as a set of heuristic rules. In the control-theoretic approach, complex constructs such as tracking filters, estimators, regulators, and controllers are utilized. In the heuristic, rule-based approach, various alerts (e.g., events) are defined on instance metrics (e.g., CPU utilization), which are then aggregated at a global scale in order to make provisioning decisions for a given application tier. This work provides an overview of our experiences designing and working with both approaches to construct an autoscaler for simple applications. We enumerate different criteria such as design complexity, ease of comprehension, and maintenance upon which we form an informal comparison between the different methods. We conclude with a brief discussion of how these approaches can be used in the governance of resources to better meet a high-level goal over time. © 2011 IEEE.
DA  - 2011///
PY  - 2011
DO  - 10.1109/CLOUD.2011.101
SP  - 716
EP  - 723
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053169900&doi=10.1109%2fCLOUD.2011.101&partnerID=40&md5=7dde0a70b9837e8c4ea7bdde16f7f28e
DB  - Scopus
KW  - Cloud computing
KW  - Elasticity
KW  - CPU utilization
KW  - Alternative approach
KW  - Application Servers
KW  - Control-theoretic approach
KW  - Conventional control
KW  - Design complexity
KW  - Global scale
KW  - Heuristic rules
KW  - Rule-based approach
KW  - Tracking filter
ER  - 

TY  - JOUR
T2  - Container Resize Policies
DA  - 2024///
PY  - 2024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213702158&partnerID=40&md5=20b3c24101fab98ff919f53a67485f5f
DB  - Scopus
ER  - 

TY  - CONF
TI  - Automated verification techniques for probabilistic systems
AU  - Forejt, V.
AU  - Kwiatkowska, M.
AU  - Norman, G.
AU  - Parker, D.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - This tutorial provides an introduction to probabilistic model checking, a technique for automatically verifying quantitative properties of probabilistic systems. We focus on Markov decision processes (MDPs), which model both stochastic and nondeterministic behaviour. We describe methods to analyse a wide range of their properties, including specifications in the temporal logics PCTL and LTL, probabilistic safety properties and cost- or reward-based measures. We also discuss multi-objective probabilistic model checking, used to analyse trade-offs between several different quantitative properties. Applications of the techniques in this tutorial include performance and dependability analysis of networked systems, communication protocols and randomised distributed algorithms. Since such systems often comprise several components operating in parallel, we also cover techniques for compositional modelling and verification of multi-component probabilistic systems. Finally, we describe three large case studies which illustrate practical applications of the various methods discussed in the tutorial. © 2011 Springer-Verlag.
DA  - 2011///
PY  - 2011
DO  - 10.1007/978-3-642-21455-4_3
VL  - 6659 LNCS
SP  - 53
EP  - 113
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959282379&doi=10.1007%2f978-3-642-21455-4_3&partnerID=40&md5=ed0f3baeb4f73c313c751356e0b1db99
DB  - Scopus
KW  - Markov processes
KW  - Economic and social effects
KW  - Markov Decision Processes
KW  - Multi objective
KW  - Model checking
KW  - Probabilistic model checking
KW  - Stochastic models
KW  - Stochastic systems
KW  - Computer software
KW  - Networked systems
KW  - Design
KW  - Formal methods
KW  - Communication
KW  - Automated verification
KW  - Compositional modelling
KW  - Dependability analysis
KW  - Multicomponents
KW  - Probabilistic logics
KW  - Probabilistic safety
KW  - Probabilistic systems
KW  - Temporal logic
ER  - 

TY  - JOUR
TI  - State of the art on microservices autoscaling: An overview
AU  - Nunes, J.P.K.
AU  - Bianchi, T.
AU  - Iwasaki, A.Y.
AU  - Nakagawa, E.Y.
T2  - Anais do XLVIII Seminário Integrado de Software e Hardware
DA  - 2021///
PY  - 2021
SP  - 30
EP  - 38
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138084486&partnerID=40&md5=5222bb372ab1de4e761b5dbf3f612cd4
DB  - Scopus
ER  - 

TY  - JOUR
TI  - A review of microservices autoscaling with formal verification perspective
AU  - Jawaddi, S.N.A.
AU  - Johari, M.H.
AU  - Ismail, A.
T2  - Software - Practice and Experience
AB  - The process of scaling microservices is a challenging task, especially in maintaining optimum resource provisioning while respecting QoS constraints and SLA. Many research works have proposed autoscaling approaches for microservices, however, less likely concerned with the correctness guarantee of the proposed algorithms. Hence, it is significant to gather and summarize these approaches to foster future innovation. Meanwhile, a few reviews have been published concerning microservices from different aspects. Therefore, our review complements the existing by focusing on autoscaling with verification perspectives. This study highlights the recent contributions in three inter-related main topics that were published within the year 2017 to 2022, namely, microservice, verification, and autoscaling. Due to limited resources on verification for microservice autoscaling, we widen the perspective by considering the verification for autoscaling in cloud-based systems. Based on our findings, we found that the formal method is not a new thing in verifying the autoscaling policies in cloud-based systems, and one recent study that implements the formal method in the microservices area has been identified. Apart from the autoscaling techniques, we have also determined several factors that have been a concern in scaling the microservices as well as the relatable metrics. Meanwhile, from a verification perspective, we identified that probabilistic model checking is the common formal verification technique used to verify microservices and cloud autoscaling. Finally, we recommend open challenges from two perspectives which highlight the verification for existing microservice autoscaling and verification for ML-based microservice autoscaling. © 2022 John Wiley & Sons Ltd.
DA  - 2022///
PY  - 2022
DO  - 10.1002/spe.3135
VL  - 52
IS  - 11
SP  - 2476
EP  - 2495
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136984240&doi=10.1002%2fspe.3135&partnerID=40&md5=8f6e93508ba189b1ca0b43f0ca82724e
DB  - Scopus
KW  - microservice
KW  - scalability
KW  - autoscaling
KW  - Microservice
KW  - Quality of service
KW  - Autoscaling
KW  - Formal verification
KW  - Model checking
KW  - Probabilistic model checking
KW  - Probabilistic model-checking
KW  - Scalings
KW  - Cloud-based
KW  - Verification techniques
KW  - formal verification
KW  - Future innovations
KW  - probabilistic model checking
KW  - QoS constraints
ER  - 

TY  - JOUR
T2  - Cluster Autoscaling
DA  - 2023///
PY  - 2023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213699581&partnerID=40&md5=84fd93296046d3e3c47bac933e6c2bca
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Prioritized Fault Recovery Strategies for Multi-Access Edge Computing Using Probabilistic Model Checking
AU  - Ray, K.
AU  - Banerjee, A.
T2  - IEEE Transactions on Dependable and Secure Computing
AB  - The advent of Multi-Access Edge Computing (MEC) has enabled service providers to mitigate high network latencies often encountered in accessing cloud services by deploying containerized application instances on edge servers situated near end users. MEC servers are, however, susceptible to various types of failures such as communication link failures, hardware failures and so on. A fault recovery strategy determines which MEC servers to utilize to re-deploy application containers in the event of a failure. In this work, we propose a two-fold fault recovery strategy characterized by application priority. We propose a Formal Methods driven local recovery strategy for high-priority applications. We use Stochastic Multi-Player Games as a Formal Model to characterize the interactions between the different components in an MEC environment. We use objectives specified in Probabilistic Alternating-Time Temporal Logic with a Probabilistic Model Checker to derive recovery strategies considering all possible execution scenarios of the model. For lower priority applications, we resort to a global recovery strategy by designing a greedy heuristic considering each server's failure probability. We use benchmark datasets to validate our approach. Experimental results show an average 14% reduction in latency with our approach in comparison with other state-of-the-art methods. © 2004-2012 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/TDSC.2022.3143877
VL  - 20
IS  - 1
SP  - 797
EP  - 812
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123372496&doi=10.1109%2fTDSC.2022.3143877&partnerID=40&md5=ea219289ced67c337cef7753b35b9fdd
DB  - Scopus
KW  - fault-tolerance
KW  - Edge computing
KW  - Multi-access edge computing
KW  - Containers
KW  - Model checking
KW  - Probabilistic model checking
KW  - Probabilistic model-checking
KW  - Stochastic models
KW  - Stochastic systems
KW  - Fault tolerance
KW  - Real time systems
KW  - Online systems
KW  - Interactive computer systems
KW  - Real - Time system
KW  - Multiaccess
KW  - Computer games
KW  - Social networking (online)
KW  - Probabilistic logics
KW  - probabilistic model checking
KW  - Computer circuits
KW  - Game
ER  - 

TY  - CHAP
TI  - Model checking probabilistic systems
AU  - Baier, C.
AU  - De Alfaro, L.
AU  - Forejt, V.
AU  - Kwiatkowska, M.
T2  - Handbook of Model Checking
AB  - The model-checking approach was originally formulated for verifying qualitative properties of systems, for example safety and liveness (see Chap. 2), and subsequently extended to also handle quantitative features, such as real time (see Chap. 29), continuous flows (see Chap. 30), as well as stochastic phenomena, where system evolution is governed by a given probability distribution. Probabilistic model checking aims to establish the correctness of probabilistic system models against quantitative probabilistic specifications, such as those capable of expressing, for example, the probability of an unsafe event occurring, expected time to termination, or expected power consumption in the start-up phase. In this chapter, we present the foundations of probabilistic model checking, focusing on finite-state Markov decision processes as models and quantitative properties expressed in probabilistic temporal logic. Markov decision processes can be thought of as a probabilistic variant of labelled transition systems in the following sense: transitions are labelled with actions, which can be chosen nondeterministically, and successor states for the chosen action are specified by means of discrete probabilistic distributions, thus specifying the probability of transiting to each successor state. To reason about expectations, we additionally annotate Markov decision processes with quantitative costs, which are incurred upon taking the selected action from a given state. Quantitative properties are expressed as formulas of the probabilistic computation tree logic (PCTL) or using linear temporal logic (LTL). We summarise the main model-checking algorithms for both PCTL and LTL, and illustrate their working through examples. The chapter ends with a brief overview of extensions to more expressive models and temporal logics, existing probabilistic model-checking tool support, and main application domains. © Springer International Publishing AG, part of Springer Nature 2018. All rights reserved.
DA  - 2018///
PY  - 2018
SP  - 963
EP  - 999
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053872104&doi=10.1007%2f978-3-319-10575-8_28&partnerID=40&md5=be18cbef6bda96e335dc0ff836088398
DB  - Scopus
KW  - Markov processes
KW  - Markov Decision Processes
KW  - Model checking
KW  - Probabilistic model checking
KW  - Stochastic systems
KW  - Probability distributions
KW  - Probabilistic logics
KW  - Temporal logic
KW  - Computer circuits
KW  - Labelled transition systems
KW  - Model checking algorithm
KW  - Probabilistic computation tree logic (PCTL)
KW  - Probabilistic distribution
KW  - Probabilistic specifications
KW  - Probabilistic temporal logic
ER  - 

TY  - JOUR
TI  - A survey and taxonomy on energy efficient resource allocation techniques for cloud computing systems
AU  - Hameed, A.
AU  - Khoshkbarforoushha, A.
AU  - Ranjan, R.
AU  - Jayaraman, P.P.
AU  - Kolodziej, J.
AU  - Balaji, P.
AU  - Zeadally, S.
AU  - Malluhi, Q.M.
AU  - Tziritas, N.
AU  - Vishnu, A.
AU  - Khan, S.U.
AU  - Zomaya, A.
T2  - Computing
AB  - In a cloud computing paradigm, energy efficient allocation of different virtualized ICT resources (servers, storage disks, and networks, and the like) is a complex problem due to the presence of heterogeneous application (e.g., content delivery networks, MapReduce, web applications, and the like) workloads having contentious allocation requirements in terms of ICT resource capacities (e.g., network bandwidth, processing speed, response time, etc.). Several recent papers have tried to address the issue of improving energy efficiency in allocating cloud resources to applications with varying degree of success. However, to the best of our knowledge there is no published literature on this subject that clearly articulates the research problem and provides research taxonomy for succinct classification of existing techniques. Hence, the main aim of this paper is to identify open challenges associated with energy efficient resource allocation. In this regard, the study, first, outlines the problem and existing hardware and software-based techniques available for this purpose. Furthermore, available techniques already presented in the literature are summarized based on the energy-efficient research dimension taxonomy. The advantages and disadvantages of the existing techniques are comprehensively analyzed against the proposed research dimension taxonomy namely: resource adaption policy, objective function, allocation method, allocation operation, and interoperability. © 2014, Springer-Verlag Wien.
DA  - 2016///
PY  - 2016
DO  - 10.1007/s00607-014-0407-8
VL  - 98
IS  - 7
SP  - 751
EP  - 774
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901734176&doi=10.1007%2fs00607-014-0407-8&partnerID=40&md5=2210ab831c2623c652538ad760c0836d
DB  - Scopus
KW  - Cloud computing
KW  - Energy efficiency
KW  - Energy consumption
KW  - Power management
KW  - Energy utilization
KW  - Resource allocation
KW  - Distributed computer systems
KW  - Complex networks
KW  - Content delivery network
KW  - Taxonomies
KW  - Hardware and software
KW  - Network bandwidth
KW  - Energy-efficient resource allocation
KW  - Objective functions
KW  - Resource capacity
KW  - Allocation methods
KW  - Energy efficient resource allocation
KW  - Research problems
ER  - 

TY  - JOUR
TI  - Elasticity in Cloud Computing: State of the Art and Research Challenges
AU  - Al-Dhuraibi, Y.
AU  - Paraiso, F.
AU  - Djarallah, N.
AU  - Merle, P.
T2  - IEEE Transactions on Services Computing
AB  - Elasticity is a fundamental property in cloud computing that has recently witnessed major developments. This article reviews both classical and recent elasticity solutions and provides an overview of containerization, a new technological trend in lightweight virtualization. It also discusses major issues and research challenges related to elasticity in cloud computing. We comprehensively review and analyze the proposals developed in this field. We provide a taxonomy of elasticity mechanisms according to the identified works and key properties. Compared to other works in literature, this article presents a broader and detailed analysis of elasticity approaches and is considered as the first survey addressing the elasticity of containers. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/TSC.2017.2711009
VL  - 11
IS  - 2
SP  - 430
EP  - 447
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045377245&doi=10.1109%2fTSC.2017.2711009&partnerID=40&md5=1cc70c9e36479a0cd20b9b4c0d4d12bc
DB  - Scopus
KW  - cloud computing
KW  - Cloud computing
KW  - containers
KW  - scalability
KW  - Auto-scaling
KW  - auto-scaling
KW  - Containers
KW  - Elasticity
KW  - Scalability
KW  - State of the art
KW  - Research challenges
KW  - Fundamental properties
KW  - Elasticity approaches
KW  - Elasticity solutions
KW  - resource provision
KW  - Resource provisions
KW  - Technological trends
ER  - 

TY  - JOUR
TI  - Energy-efficient datacenters
AU  - Pedram, M.
T2  - IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems
AB  - Pervasive use of cloud computing and the resulting rise in the number of datacenters and hosting centers (that provide platform or software services to clients who do not have the means to set up and operate their own computing facilities) have brought forth many concerns, including the electrical energy cost, peak power dissipation, cooling, and carbon emission. With power consumption becoming an increasingly important issue for the operation and maintenance of the hosting centers, corporate and business owners are becoming increasingly concerned. Furthermore, provisioning resources in a cost-optimal manner so as to meet different performance criteria, such as throughput or response time, has become a critical challenge. The goal of this paper is to provide an introduction to resource provisioning and power or thermal management problems in datacenters, and to review strategies that maximize the datacenter energy efficiency subject to peak or total power consumption and thermal constraints, while meeting stipulated service level agreements in terms of task throughput and/or response time. © 2012 IEEE.
DA  - 2012///
PY  - 2012
DO  - 10.1109/TCAD.2012.2212898
VL  - 31
IS  - 10
SP  - 1465
EP  - 1484
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866609789&doi=10.1109%2fTCAD.2012.2212898&partnerID=40&md5=da8813937e1c7f70fcfef51e520f0804
DB  - Scopus
KW  - Energy efficiency
KW  - resource management
KW  - Resource management
KW  - green computing
KW  - Datacenter
KW  - Temperature control
KW  - Dynamic Power
KW  - dynamic power and thermal management
KW  - energy efficient design
KW  - Energy-efficient design
KW  - enterprise computing
KW  - Enterprise computing
ER  - 

TY  - JOUR
TI  - MONDEO-Tactics5G: Multistage botnet detection and tactics for 5G/6G networks
AU  - Sousa, B.
AU  - Dias, D.
AU  - Antunes, N.
AU  - Cámara, J.
AU  - Wagner, R.
AU  - Schmerl, B.
AU  - Garlan, D.
AU  - Fidalgo, P.
T2  - Computers and Security
AB  - Mobile malware is a malicious code specifically designed to target mobile devices to perform multiple types of fraud. The number of attacks reported each day is increasing constantly and is causing an impact not only at the end-user level but also at the network operator level. Malware like FluBot contributes to identity theft and data loss but also enables remote Command & Control (C2) operations, which can instrument infected devices to conduct Distributed Denial of Service (DDoS) attacks. Current mobile device-installed solutions are not effective, as the end user can ignore security warnings or install malicious software. This article designs and evaluates MONDEO-Tactics5G - a multistage botnet detection mechanism that does not require software installation on end-user devices, together with tactics for 5G network operators to manage infected devices. We conducted an evaluation that demonstrates high accuracy in detecting FluBot malware, and in the different adaptation strategies to reduce the risk of DDoS while minimising the impact on the clients' satisfaction by avoiding disrupting established sessions. © 2024 The Author(s)
DA  - 2024///
PY  - 2024
DO  - 10.1016/j.cose.2024.103768
VL  - 140
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186112209&doi=10.1016%2fj.cose.2024.103768&partnerID=40&md5=6209ad82884ec4e5065dcd3e85d3b392
DB  - Scopus
KW  - DDoS
KW  - Network security
KW  - Queueing networks
KW  - End-users
KW  - 5G mobile communication systems
KW  - Denial-of-service attack
KW  - Network operator
KW  - Malware
KW  - Botnet
KW  - Botnet detections
KW  - Botnets
KW  - Command control
KW  - Command control server
KW  - Command Control server
KW  - Distributed denial of service
KW  - Malwares
KW  - Mobile security
KW  - Multi-stages
KW  - Tactic
KW  - Tactics
ER  - 

TY  - CONF
TI  - PRISM 4.0: Verification of probabilistic real-time systems
AU  - Kwiatkowska, M.
AU  - Norman, G.
AU  - Parker, D.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - This paper describes a major new release of the PRISM probabilistic model checker, adding, in particular, quantitative verification of (priced) probabilistic timed automata. These model systems exhibiting probabilistic, nondeterministic and real-time characteristics. In many application domains, all three aspects are essential; this includes, for example, embedded controllers in automotive or avionic systems, wireless communication protocols such as Bluetooth or Zigbee, and randomised security protocols. PRISM, which is open-source, also contains several new components that are of independent use. These include: an extensible toolkit for building, verifying and refining abstractions of probabilistic models; an explicit-state probabilistic model checking library; a discrete-event simulation engine for statistical model checking; support for generation of optimal adversaries/strategies; and a benchmark suite. © 2011 Springer-Verlag.
DA  - 2011///
PY  - 2011
DO  - 10.1007/978-3-642-22110-1_47
VL  - 6806 LNCS
SP  - 585
EP  - 591
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960351824&doi=10.1007%2f978-3-642-22110-1_47&partnerID=40&md5=ffaf7f6b04e786a99356c0aafd8f2cff
DB  - Scopus
KW  - Network security
KW  - Model checking
KW  - Probabilistic model checking
KW  - Prisms
KW  - Benchmark suites
KW  - Open-source
KW  - Computer simulation
KW  - Wireless telecommunication systems
KW  - Real time systems
KW  - Application domains
KW  - Embedded systems
KW  - Communication
KW  - Avionic systems
KW  - Avionics
KW  - Computer aided analysis
KW  - Discrete events
KW  - Embedded controllers
KW  - Model system
KW  - New components
KW  - Probabilistic models
KW  - Probabilistic timed automata
KW  - Quantitative verification
KW  - Real time characteristics
KW  - Security protocols
KW  - Simulation engine
KW  - Statistical models
KW  - Wireless communication protocols
KW  - Zig-Bee
ER  - 

TY  - JOUR
AU  - Kinyae, N.
T2  - Kubernetes Resource & Performance Metrics Allocation.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213690814&partnerID=40&md5=1d40f4c6e03e882909850bf8815cbf5b
DB  - Scopus
ER  - 

TY  - CONF
TI  - Microservices for scalability: [Keynote talk abstract]
AU  - Hasselbring, W.
T2  - ICPE 2016 - Proceedings of the 7th ACM/SPEC International Conference on Performance Engineering
AB  - Microservice architectures provide small services that may be deployed and scaled independently of each other, and may employ different middleware stacks for their implementation. Microservice architectures emphasize transaction-less coordination between services, with explicit acceptance of eventual consistency. Polyglott persistence in this context means that the individual microservices may employ multiple data storage technologies. Microservice architectures are "cloud native" allowing for automated and rapid elasticity. Fault-tolerance mechanisms achieve that failures of individual mircroservices do not affect other services thanks to container isolation. Since services can fail at any time, it is important to be able to detect the failures quickly and, if possible, automatically restore services. Essential for success in such a setting is advanced monitoring. In this keynote, I discuss how mircoservices support scalability for both, runtime performance and development performance, via polyglott persistence, eventual consistency, loose coupling, open source frameworks, and continuous monitoring for elastic capacity management. © 2016 ACM.
DA  - 2016///
PY  - 2016
DO  - 10.1145/2851553.2858659
SP  - 133
EP  - 134
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009195980&doi=10.1145%2f2851553.2858659&partnerID=40&md5=5bc2faf13d6d79850273bddf0cd1e9bf
DB  - Scopus
KW  - Microservices
KW  - Scalability
KW  - Monitoring
KW  - Architecture
KW  - Middleware
KW  - Digital storage
KW  - Fault tolerance
KW  - Capacity management
KW  - Run-time performance
KW  - Continuous monitoring
KW  - Development performance
KW  - Eventual consistency
KW  - Fault tolerance mechanisms
KW  - Open source frameworks
ER  - 

TY  - CHAP
TI  - Probabilistic model checking: Advances and applications
AU  - Kwiatkowska, M.
AU  - Norman, G.
AU  - Parker, D.
T2  - Formal System Verification: State-of the-Art and Future Trends
DA  - 2017///
PY  - 2017
SP  - 73
EP  - 121
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033345325&doi=10.1007%2f978-3-319-57685-5_3&partnerID=40&md5=825693adb1cb692784f0fdd7f50ef565
DB  - Scopus
KW  - Model checking
KW  - Probabilistic model checking
ER  - 

TY  - CONF
TI  - An overview of cloud simulation enhancement using the monte-carlo method
AU  - Bertot, L.
AU  - Genaud, S.
AU  - Gossa, J.
T2  - Proceedings - 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, CCGRID 2018
AB  - In the cloud computing model, cloud providers invoice clients for resource consumption. Hence, tools helping the client to budget the cost of running their application are of pre-eminent importance. However, the opaque and multi-tenant nature of clouds, make job runtimes both variable and hard to predict. In this paper, we propose an improved simulation framework that takes into account this variability using the Monte-Carlo method. We consider the execution of batch jobs on an actual platform, scheduled using typical heuristics based on the user estimates of tasks' runtimes. We model the observed variability through simple distributions to use as inputs to the Monte-Carlo simulation. We show that, our method can capture over 90% of the empirical observations of total execution times. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/CCGRID.2018.00064
SP  - 386
EP  - 387
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050965525&doi=10.1109%2fCCGRID.2018.00064&partnerID=40&md5=db9e25470dbc29054d3da3856b158e9d
DB  - Scopus
KW  - Cloud computing
KW  - Cloud providers
KW  - Intelligent systems
KW  - Job shop scheduling
KW  - Cluster computing
KW  - Budget control
KW  - Grid computing
KW  - Computer simulation
KW  - Runtimes
KW  - Resource consumption
KW  - Simulation framework
KW  - Multi tenants
KW  - Cloud simulation
KW  - Batch jobs
KW  - Monte carlo methods
KW  - Monte Carlo methods
ER  - 

TY  - JOUR
TI  - Quantitative Verification for Monitoring Event-Streaming Systems
AU  - Su, G.
AU  - Liu, L.
AU  - Zhang, M.
AU  - Rosenblum, D.S.
T2  - IEEE Transactions on Software Engineering
AB  - High-performance data streaming technologies are increasingly adopted in IT companies to support the integration of heterogeneous and possibly distributed applications. Compared with the traditional message queuing middleware, a streaming platform enables the implementation of event-streaming systems (ESS) which include not only complex queues but also pipelines that transform and react to the streams of data. By analysing the centralised data streams, one can evaluate the Quality-of-Service for other systems and components that produce or consume those streams. We consider the exploitation of probabilistic model checking as a performance monitoring technique for ESS systems. Probabilistic model checking is a mature, powerful verification technique with successful application in performance analysis. However, an ESS system may contain quantitative parameters that are determined by event streams observed in a certain period of time. In this paper, we present a novel theoretical framework called QV4M (meaning 'quantitative verification for monitoring') for monitoring ESS systems, which is based on two recent methods of probabilistic model checking. QV4M assumes the parameters in a probabilistic system model as random variables and infers the statistical significance for the probabilistic model checking output. We also present an empirical evaluation of computational time and data cost for QV4M.  © 1976-2012 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/TSE.2020.2996033
VL  - 48
IS  - 2
SP  - 538
EP  - 550
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085761724&doi=10.1109%2fTSE.2020.2996033&partnerID=40&md5=8c9ece984385bccbae7ea1f83bc38e42
DB  - Scopus
KW  - Quality of service
KW  - Monitoring
KW  - Model checking
KW  - Probabilistic model checking
KW  - Quality control
KW  - Middleware
KW  - Data streams
KW  - Performance monitoring
KW  - Multimedia systems
KW  - Empirical evaluations
KW  - Verification techniques
KW  - Probabilistic systems
KW  - Quantitative verification
KW  - Discrete-time markov chain
KW  - Event stream
KW  - Parametric model checking
KW  - Quantitative parameters
KW  - Statistical confidence
KW  - Statistical inference
ER  - 

TY  - JOUR
TI  - Machine learning-based auto-scaling for containerized applications
AU  - Imdoukh, M.
AU  - Ahmad, I.
AU  - Alfailakawi, M.G.
T2  - Neural Computing and Applications
AB  - Containers are shaping the new era of cloud applications due to their key benefits such as lightweight, very quick to launch, consuming minimum resources to run an application which reduces cost, and can be easily and rapidly scaled up/down as per workload requirements. However, container-based cloud applications require sophisticated auto-scaling methods that automatically and in a timely manner provision and de-provision cloud resources without human intervention in response to dynamic fluctuations in workload. To address this challenge, in this paper, we propose a proactive machine learning-based approach to perform auto-scaling of Docker containers in response to dynamic workload changes at runtime. The proposed auto-scaler architecture follows the commonly abstracted four steps: monitor, analyze, plan, and execute the control loop. The monitor component continuously collects different types of data (HTTP request statistics, CPU, and memory utilization) that are needed during the analysis and planning phase to determine proper scaling actions. We employ in analysis phase a concise yet fast, adaptive, and accurate prediction model based on long short-term memory (LSTM) neural network to predict future HTTP workload to determine the number of containers needed to handle requests ahead of time to eliminate delays caused by starting or stopping running containers. Moreover, in the planning phase, the proposed gradually decreasing strategy avoids oscillations which happens when scaling operations are too frequent. Experimental results using realistic workload show that the prediction accuracy of LSTM model is as accurate as auto-regression integrated moving average model but offers 600 times prediction speedup. Moreover, as compared with artificial neural network model, LSTM model performs better in terms of auto-scaler metrics related to provisioning and elastic speedup. In addition, it was observed that when LSTM model is used, the predicted workload helped in using the minimum number of replicas to handle future workload. In the experiments, the use of GDS showed promising results in keeping the desired performance at reduced cost to handle cases with sudden workload increase/decrease. © 2019, Springer-Verlag London Ltd., part of Springer Nature.
DA  - 2020///
PY  - 2020
DO  - 10.1007/s00521-019-04507-z
VL  - 32
IS  - 13
SP  - 9745
EP  - 9760
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074476840&doi=10.1007%2fs00521-019-04507-z&partnerID=40&md5=86fd26d2f95eec826a8337d89a19ad52
DB  - Scopus
KW  - Forecasting
KW  - Prediction
KW  - Auto-scaling
KW  - Containers
KW  - Machine learning
KW  - Learning algorithms
KW  - Cloud applications
KW  - Containerization
KW  - Memory architecture
KW  - Neural networks
KW  - Long short-term memory
KW  - Cost reduction
KW  - Brain
KW  - HTTP
KW  - Human intervention
KW  - Prediction accuracy
KW  - Frequency dividing circuits
KW  - Accurate prediction
KW  - Artificial neural network modeling
KW  - Moving average model
KW  - Neural network
KW  - Proactive controller
ER  - 

TY  - CONF
TI  - A storm is coming: A modern probabilistic model checker
AU  - Dehnert, C.
AU  - Junges, S.
AU  - Katoen, J.-P.
AU  - Volk, M.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - We launch the new probabilistic model checker Storm. It features the analysis of discrete- and continuous-time variants of both Markov chains and MDPs. It supports the Prism and JANI modeling languages, probabilistic programs, dynamic fault trees and generalized stochastic Petri nets. It has a modular set-up in which solvers and symbolic engines can easily be exchanged. It offers a Python API for rapid prototyping by encapsulating Storm’s fast and scalable algorithms. Experiments on a variety of benchmarks show its competitive performance. © Springer International Publishing AG 2017
DA  - 2017///
PY  - 2017
DO  - 10.1007/978-3-319-63390-9_31
VL  - 10427 LNCS
SP  - 592
EP  - 600
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026765560&doi=10.1007%2f978-3-319-63390-9_31&partnerID=40&md5=bec40458d61e867317afa60deceea5f9
DB  - Scopus
KW  - Markov processes
KW  - Model checking
KW  - Benchmarking
KW  - Storms
KW  - Generalized Stochastic Petri nets
KW  - Petri nets
KW  - Stochastic systems
KW  - Continuous time systems
KW  - Software prototyping
KW  - Modeling languages
KW  - Competitive performance
KW  - Computer aided analysis
KW  - Computer simulation languages
KW  - Continuous-time
KW  - Dynamic fault trees
KW  - It supports
KW  - Probabilistic modeling
KW  - Probabilistic programs
KW  - Scalable algorithms
ER  - 

TY  - JOUR
TI  - Horizontal pod autoscaling in kubernetes for elastic container orchestration
AU  - Nguyen, T.-T.
AU  - Yeom, Y.-J.
AU  - Kim, T.
AU  - Park, D.-H.
AU  - Kim, S.
T2  - Sensors (Switzerland)
AB  - Kubernetes, an open-source container orchestration platform, enables high availability and scalability through diverse autoscaling mechanisms such as Horizontal Pod Autoscaler (HPA), Vertical Pod Autoscaler and Cluster Autoscaler. Amongst them, HPA helps provide seamless service by dynamically scaling up and down the number of resource units, called pods, without having to restart the whole system. Kubernetes monitors default Resource Metrics including CPU and memory usage of host machines and their pods. On the other hand, Custom Metrics, provided by external software such as Prometheus, are customizable to monitor a wide collection of metrics. In this paper, we investigate HPA through diverse experiments to provide critical knowledge on its operational behaviors. We also discuss the essential difference between Kubernetes Resource Metrics (KRM) and Prometheus Custom Metrics (PCM) and how they affect HPA’s performance. Lastly, we provide deeper insights and lessons on how to optimize the performance of HPA for researchers, developers, and system administrators working with Kubernetes in the future. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.
DA  - 2020///
PY  - 2020
DO  - 10.3390/s20164621
VL  - 20
IS  - 16
SP  - 1
EP  - 18
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089612460&doi=10.3390%2fs20164621&partnerID=40&md5=0bd64ccc5e149c37898e1b3e4c197267
DB  - Scopus
KW  - Cloud computing
KW  - Kubernetes
KW  - Container orchestration
KW  - Edge computing
KW  - Containers
KW  - Docker
KW  - Resource units
KW  - High availability
KW  - Open source software
KW  - System administrators
KW  - Open sources
KW  - Prometheus
KW  - Customizable
KW  - Custom metrics
KW  - Horizontal Pod Autoscaling (HPA)
KW  - Operational behavior
KW  - Resource metrics
KW  - Seamless services
ER  - 

TY  - JOUR
TI  - Dynamic Cloud Resource Allocation Considering Demand Uncertainty
AU  - Mireslami, S.
AU  - Rakai, L.
AU  - Wang, M.
AU  - Far, B.H.
T2  - IEEE Transactions on Cloud Computing
AB  - Cloud computing provisions scalable resources for high performance industrial applications. Cloud providers usually offer two types of usage plans: reserved and on-demand. Reserved plans offer cheaper resources for long-term contracts while on-demand plans are available for short or long periods but are more expensive. To satisfy incoming user demands with reasonable costs, cloud resources should be allocated efficiently. Most existing works focus on either cheaper solutions with reserved resources that may lead to under-provisioning or over-provisioning, or costly solutions with on-demand resources. Since inefficiency of allocating cloud resources can cause huge provisioning costs and fluctuation in cloud demand, resource allocation becomes a highly challenging problem. In this paper, we propose a hybrid method to allocate cloud resources according to the dynamic user demands. This method is developed as a two-phase algorithm that consists of reservation and dynamic provision phases. In this way, we minimize the total deployment cost by formulating each phase as an optimization problem while satisfying quality of service. Due to the uncertain nature of cloud demands, we develop a stochastic optimization approach by modeling user demands as random variables. Our algorithm is evaluated using different experiments and the results show its efficiency in dynamically allocating cloud resources.  © 2013 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/TCC.2019.2897304
VL  - 9
IS  - 3
SP  - 981
EP  - 994
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114504298&doi=10.1109%2fTCC.2019.2897304&partnerID=40&md5=0fc86aa256c3a391a288f4e9e4eddfbd
DB  - Scopus
KW  - quality of service
KW  - Cloud computing
KW  - Quality of service
KW  - Optimization
KW  - Resource allocation
KW  - Deployment costs
KW  - Optimization problems
KW  - Over provisioning
KW  - Its efficiencies
KW  - demand uncertainty
KW  - Demand uncertainty
KW  - Long-term contracts
KW  - pricing and resource allocation
KW  - Stochastic optimization approach
KW  - stochastic programming
KW  - Two phase algorithm
ER  - 

TY  - JOUR
TI  - Energy Consumption in Microservices Architectures: A Systematic Literature Review
AU  - Araujo, G.
AU  - Barbosa, V.
AU  - Lima, L.N.
AU  - Sabino, A.
AU  - Brito, C.
AU  - Fe, I.
AU  - Rego, P.
AU  - Choi, E.
AU  - Min, D.
AU  - Nguyen, T.A.
AU  - Silva, F.A.
T2  - IEEE Access
AB  - Cloud computing emerges as a paradigm that facilitates on-demand access to technological resources through the mechanism of service virtualization. This virtualization enables the partitioning of hardware resources among applications that are organized into distinct independent modules. The concept of microservice architecture takes advantage of virtualization capabilities to embrace a software architecture strategy focused on the development of applications as assemblies of several interdependent but loosely coupled modules. Nonetheless, the adoption of microservices architecture is accompanied by substantial energy demands to meet the desired standards of performance and availability. Existing research within the domain of microservices has explored various topics pertinent to energy consumption, including elasticity, reliability, performance, and availability. Yet, the diversity of challenges and solutions presents a complex landscape for identifying prevailing research trends and unaddressed gaps in the context of microservices. This study aims to methodically discern, evaluate, and juxtapose the existing research trends and voids concerning energy consumption within microservices. It elucidates a systematic review on the subject of energy consumption in microservices architectures, offering a compilation of references to facilitate more directed future investigations. The initial selection encompassed 3625 articles, which were subsequently narrowed down through three stages of refinement, resulting in 37 articles chosen for an exhaustive review. These selected studies were cataloged and analyzed based on various criteria, including metrics, evaluation methodologies, and architectural typologies, thus uncovering research gaps and emerging trends related to energy consumption in microservice architectures. Furthermore, this inquiry delineates significant research challenges and prospective directions, structured around the key metrics that underpin the reviewed studies: performance, elasticity, scalability, reliability, sustainability, and availability.  © 2013 IEEE.
DA  - 2024///
PY  - 2024
DO  - 10.1109/ACCESS.2024.3389064
VL  - 12
SP  - 186710
EP  - 186729
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190750888&doi=10.1109%2fACCESS.2024.3389064&partnerID=40&md5=4c9c1f346cd433aabcf7ef1bc610bb15
DB  - Scopus
KW  - cloud computing
KW  - Cloud computing
KW  - Microservices
KW  - energy consumption
KW  - container
KW  - Security
KW  - Microservice
KW  - Elasticity
KW  - Virtualization
KW  - Microservice architecture
KW  - Application programs
KW  - Computer architecture
KW  - Energy-consumption
KW  - Cloud-computing
KW  - Green computing
KW  - Energy utilization
KW  - Virtual reality
KW  - Virtualizations
KW  - Systematic mapping
KW  - Systematic
KW  - systematic mapping
ER  - 

TY  - JOUR
AU  - Kinyae, N.
T2  - Greedy Cloud Selection Deployment on Microservices. Kaggle.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213683134&partnerID=40&md5=c5263d02555422a85b15757a6032ea2f
DB  - Scopus
ER  - 

TY  - CONF
TI  - A Proactive Formal Approach For Microservice-based Applications Auto-Scaling
AU  - Merkouche, S.
AU  - Bouanaka, C.
T2  - CEUR Workshop Proceedings
AB  - Due to the emergence of cloud and containers, microservices has become widely adopted to develop large-scale applications, since deploying on the cloud provides an unlimited amount of resources to the developers. However, an uncontrolled usage of these resources leads to unnecessary costs or a non-performant system. Therefore, several researches have been carried out around an efficient resources auto-scaling, coming out with several policies. Most of the existing policies follow a reactive approach that relies on the current state of the system to adapt it. On the counterpart, proactive approaches are based on resource future usage estimation to adapt the system before it reaches a non-performant state, yet, complex and expensive methodologies are needed to ensure proactivity such as reinforcement learning. In this work, we propose a proactive approach of resource auto-scaling. We use the weak and strong dependencies concept to expect the future state of the system. To formally model the proposed approach, we combine high-level PNs and plausible PNs. The plausible PNs are suitable for decision-making, when several adaptation plans are available, they allow identifying a compromise plan when the auto-scaling concerns different qualities of the system. © 2020 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
DA  - 2022///
PY  - 2022
VL  - 3176
SP  - 15
EP  - 28
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136200457&partnerID=40&md5=0a07dffe2c418ec6bc3ef8ec750cceaa
DB  - Scopus
KW  - containers
KW  - Auto-scaling
KW  - auto-scaling
KW  - formal methods
KW  - Containers
KW  - Microservice architecture
KW  - Decision making
KW  - Microservice architectures
KW  - Reinforcement learning
KW  - Reinforcement learnings
KW  - Scalings
KW  - Petri nets
KW  - 'current
KW  - Large-scale applications
KW  - Decisions makings
KW  - Formal methods
KW  - Formal approach
KW  - Petri Nets
KW  - Pro-active approach
KW  - Proactivity
ER  - 

TY  - JOUR
TI  - Data Processing on Edge and Cloud: A Performability Evaluation and Sensitivity Analysis
AU  - Santos, L.
AU  - Cunha, B.
AU  - Fé, I.
AU  - Vieira, M.
AU  - Silva, F.A.
T2  - Journal of Network and Systems Management
AB  - Nowadays, the Internet of Things (IoT) allows monitoring and automation in diverse contexts, such as hospitals, homes, or even smart cities, just to name a few examples. IoT data processing may occur, at the edge of the network or in the cloud, but frequently the processing must be divided between the two layers. Aiming to guarantee that the IoT systems works efficiently, it is essential to evaluate the system even in initial design stages. However, evaluating hybrid systems composed by multiple layers is not an easy task as a myriad of parameters are involved in the process. Thus, this paper presents two SPN models (one base and extended one) that can represent an abstract distributed system composed of IoT, edge and cloud layers. The models are highly configurable to be used in diverse simulation scenarios. Besides a sensitivity analysis evidenced the most impacting components in the studied architecture and made it possible to optimize the base SPN model. Finally a case study explores multiple metrics of interest concurrently and works as a guide of the model utilization. Ultimately, the proposed approach can assist system designers to avoid unnecessary investment in original equipment. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
DA  - 2021///
PY  - 2021
DO  - 10.1007/s10922-021-09592-x
VL  - 29
IS  - 3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102179880&doi=10.1007%2fs10922-021-09592-x&partnerID=40&md5=823415fbdfc2da072c7ecb1abed6d62e
DB  - Scopus
KW  - Cloud computing
KW  - Internet of Things (IoT)
KW  - Edge computing
KW  - Distributed systems
KW  - Internet of things
KW  - Sensitivity analysis
KW  - Performability evaluation
KW  - Internet of thing (IOT)
KW  - Distributed database systems
KW  - Data handling
KW  - System designers
KW  - Cloud layers
KW  - Hybrid systems
KW  - Initial design
KW  - Multiple layers
KW  - SPN model
KW  - Stochastic petri nets
ER  - 

TY  - CONF
TI  - Quantitative Evaluation of Cloud Elasticity based on Fuzzy Analytic Hierarchy Process
AU  - Yang, B.
AU  - Zhang, F.
AU  - Khan, S.U.
T2  - Proceedings - 2022 IEEE Cloud Summit, Cloud Summit 2022
AB  - Elasticity is one of the most important cloud computing characteristics, which enables deployed applications to dynamically adapt to workload-changing demands by acquiring and releasing shared computing resources at runtime. However, the existing cloud elasticity metrics are either oversimplified or hard to use, thereby lacking a comprehensive evaluation mech-anism to properly compare the elastic feature among different cloud providers. To address this gap, we propose an assessment method for cloud elasticity based on fuzzy hierarchical analysis. We use a fuzzy hierarchical model to quantitatively assess the qualitative metrics with a unified standard model. We compare three public cloud providers (Ali Cloud, HUAWEI Cloud, Tencent Cloud) as case studies and measure their cloud elasticity based on the proposed model on a cluster. To verify the effectiveness of our method, we also measure three cloud platforms using auto scaling performance metrics proposed by SPEC Cloud Group. The results show that our proposed elasticity quantification method is feasible. © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/CloudSummit54781.2022.00022
SP  - 105
EP  - 112
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145434517&doi=10.1109%2fCloudSummit54781.2022.00022&partnerID=40&md5=d502bf4882fbadd357f1230becb31c05
DB  - Scopus
KW  - Cloud
KW  - Elasticity
KW  - Cloud providers
KW  - Cloud-computing
KW  - Hierarchical systems
KW  - Computing resource
KW  - Deployed applications
KW  - Quantitative evaluation
KW  - Cloud elasticities
KW  - Analytic hierarchy process
KW  - Fuzzy analytic hierarchy
KW  - Fuzzy analytic hierarchy process
KW  - Fuzzy analytic hierarchy process(FAHP)
KW  - Quantify
KW  - Resource
ER  - 

TY  - CONF
TI  - Dependable horizontal scaling based on probabilistic model checking
AU  - Naskos, A.
AU  - Stachtiari, E.
AU  - Gounaris, A.
AU  - Katsaros, P.
AU  - Tsoumakos, D.
AU  - Konstantinou, I.
AU  - Sioutas, S.
T2  - Proceedings - 2015 IEEE/ACM 15th International Symposium on Cluster, Cloud, and Grid Computing, CCGrid 2015
AB  - The focus of this work is the on-demand resource provisioning in cloud computing, which is commonly referredto as cloud elasticity. Although a lot of effort has been invested in developing systems and mechanisms that enable elasticity, the elasticity decision policies tend to be designed without quantifying or guaranteeing the quality of their operation. We present an approach towards the development of more formalized and dependable elasticity policies. We make two distinct contributions. First, we propose an extensible approach to enforcing elasticity through the dynamic instantiation and online quantitative verification of Markov Decision Processes(MDP) using probabilistic model checking. Second, various concrete elasticity models and elasticity policies are studied. We evaluate the decision policies using traces from a realNoSQL database cluster under constantly evolving externalload. We reason about the behaviour of different modelling and elasticity policy options and we show that our proposal can improve upon the state-of-the-art in significantly decreasing under-provisioning while avoiding over-provisioning. © 2015 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/CCGrid.2015.91
SP  - 31
EP  - 40
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941241984&doi=10.1109%2fCCGrid.2015.91&partnerID=40&md5=13dea36c1dc251271f7d864f4550dedf
DB  - Scopus
KW  - Elasticity
KW  - Markov processes
KW  - Markov Decision Processes
KW  - Model checking
KW  - Probabilistic model checking
KW  - Cluster computing
KW  - Prisms
KW  - Grid computing
KW  - Horizontal scaling
KW  - Autonomic Computing
KW  - Cloud elasticities
KW  - Quantitative verification
KW  - Autonomic computing
KW  - Cloud elasticity
KW  - Nosql database
KW  - NoSQL databases
KW  - On-demand resource provisioning
KW  - PRISM
ER  - 

TY  - JOUR
AU  - Kubernetes Documentation, J.
T2  - Horizontal Pod Autoscaling
DA  - 2023///
PY  - 2023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180090258&partnerID=40&md5=30e92de62008d000f7fa6d5ea8b21963
DB  - Scopus
ER  - 

TY  - CONF
TI  - Proactive self-adaptation under uncertainty: A probabilistic model checking approach
AU  - Moreno, G.A.
AU  - Cámara, J.
AU  - Garlan, D.
AU  - Schmerl, B.
T2  - 2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings
AB  - Self-adaptive systems tend to be reactive and myopic, adapting in response to changes without anticipating what the subsequent adaptation needs will be. Adapting reactively can result in inefficiencies due to the system performing a suboptimal sequence of adaptations. Furthermore, when adaptations have latency, and take some time to produce their effect, they have to be started with sufficient lead time so that they complete by the time their effect is needed. Proactive latency-aware adaptation addresses these issues by making adaptation decisions with a look-ahead horizon and taking adaptation latency into account. In this paper we present an approach for proactive latency-aware adaptation under uncertainty that uses probabilistic model checking for adaptation decisions. The key idea is to use a formal model of the adaptive system in which the adaptation decision is left underspecified through nondeterminism, and have the model checker resolve the nondeterministic choices so that the accumulated utility over the horizon is maximized. The adaptation decision is optimal over the horizon, and takes into account the inherent uncertainty of the environment predictions needed for looking ahead. Our results show that the decision based on a look-ahead horizon, and the factoring of both tactic latency and environment uncertainty, considerably improve the effectiveness of adaptation decisions. © 2015 ACM.
DA  - 2015///
PY  - 2015
DO  - 10.1145/2786805.2786853
SP  - 1
EP  - 12
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960425213&doi=10.1145%2f2786805.2786853&partnerID=40&md5=75e2f31c3d3472005b19c2cc8c26407a
DB  - Scopus
KW  - Software engineering
KW  - Model checking
KW  - Probabilistic model checking
KW  - Adaptive systems
KW  - Latency-aware
KW  - Self adaptation
KW  - Environment predictions
KW  - Environment uncertainty
KW  - Nondeterministic choice
KW  - Proactive
KW  - Self-adaptive system
KW  - Selfadaptation
ER  - 

TY  - JOUR
AU  - Burroughs, S.
T2  - Towards Predictive Runtime Modelling of Kubernetes Microservices
DA  - 2021///
PY  - 2021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122396157&partnerID=40&md5=26d3196cfb1e537f4e4a0905fd7eb01f
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Performance modelling and verification of cloud-based auto-scaling policies
AU  - Evangelidis, A.
AU  - Parker, D.
AU  - Bahsoon, R.
T2  - Future Generation Computer Systems
AB  - Auto-scaling, a key property of cloud computing, allows application owners to acquire and release resources on demand. However, the shared environment, along with the exponentially large configuration space of available parameters, makes the configuration of auto-scaling policies a challenging task. In particular, it is difficult to quantify, a priori, the impact of a policy on Quality of Service (QoS) provision. To address this problem, we propose a novel approach based on performance modelling and formal verification to produce performance guarantees on particular rule-based auto-scaling policies. We demonstrate the usefulness and efficiency of our techniques through a detailed validation process on two public cloud providers, Amazon EC2 and Microsoft Azure, targeting two cloud computing models, Infrastructure as a Service (IaaS) and Platform as a Service (PaaS), respectively. Our experimental results show that the modelling process along with the model itself can be very effective in providing the necessary formal reasoning to cloud application owners with respect to the configuration of their auto-scaling policies, and consequently helping them to specify an auto-scaling policy which could minimise QoS violations. © 2018 Elsevier B.V.
DA  - 2018///
PY  - 2018
DO  - 10.1016/j.future.2017.12.047
VL  - 87
SP  - 629
EP  - 638
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041570783&doi=10.1016%2fj.future.2017.12.047&partnerID=40&md5=9aed289bf5984ae13c67f66c71fc944f
DB  - Scopus
KW  - Auto-scaling
KW  - Markov processes
KW  - Quality of service
KW  - Windows operating system
KW  - Formal verification
KW  - Cloud applications
KW  - Infrastructure as a service (IaaS)
KW  - Platform as a Service (PaaS)
KW  - Performance guarantees
KW  - Performance modelling
KW  - Markov models
KW  - Configuration space
KW  - Markov model
KW  - Probabilistic verification
KW  - Validation process
ER  - 

TY  - CONF
TI  - Model-based analysis of microservice resiliency patterns
AU  - Mendonca, N.
AU  - Mendes Aderaldo, C.
AU  - Camara, J.
AU  - Garlan, D.
T2  - Proceedings - IEEE 17th International Conference on Software Architecture, ICSA 2020
AB  - Microservice application developers try to mitigate the impact of partial outages typically by implementing service-to-service interactions that use well-known resiliency patterns, such as Retry, Fail Fast, and Circuit Breaker. However, those resiliency patterns - as well as their available open-source implementations - are often documented informally, leaving it up to application developers to figure out when and how to use those patterns in the context of a particular microservice application. In this paper, we take a first step towards improving on this situation by introducing a model checking-based approach in which we use the PRISM probabilistic model checker to analyze the behavior of the Retry and Circuit Breaker resiliency patterns as continuous-time Markov chains (CTMC). This approach has enabled us to quantify the impact of applying each resiliency pattern on multiple quality attributes, as well as to determine how to best tune their parameters to deal with varying service availability conditions, in the context of a simple client-service interaction scenario. © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ICSA47634.2020.00019
SP  - 114
EP  - 124
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085929653&doi=10.1109%2fICSA47634.2020.00019&partnerID=40&md5=a1038668e9f8f97d2bdbdf8258f415fa
DB  - Scopus
KW  - Software architecture
KW  - Microservice architecture
KW  - Model checking
KW  - Probabilistic model checking
KW  - Electric circuit breakers
KW  - Continuous time Markov chain
KW  - Continuous time systems
KW  - Markov chains
KW  - Open source software
KW  - Service availability
KW  - Application developers
KW  - Service interaction
KW  - Probabilistic modeling
KW  - Model-based analysis
KW  - Multiple quality
KW  - Open source implementation
KW  - Resiliency patterns
ER  - 

TY  - JOUR
TI  - Formal Quality of Service assurances, ranking and verification of cloud deployment options with a probabilistic model checking method
AU  - Kochovski, P.
AU  - Drobintsev, P.D.
AU  - Stankovski, V.
T2  - Information and Software Technology
AB  - Context: Existing software workbenches allow for the deployment of cloud applications across a variety of Infrastructure-as-a-Service (IaaS) providers. The expected workload, Quality of Service (QoS) and Non-Functional Requirements (NFRs) must be considered before an appropriate infrastructure is selected. However, this decision-making process is complex and time-consuming. Moreover, the software engineer needs assurances that the selected infrastructure will lead to an adequate QoS of the application. Objective: The goal is to develop a new method for selection of an optimal cloud deployment option, that is, an infrastructure and configuration for deployment and to verify that all hard and as many soft QoS requirements as possible will be met at runtime. Method: A new Formal QoS Assurances Method (FoQoSAM), which relies on stochastic Markov models is introduced to facilitate an automated decision-making process. For a given workload, it uses QoS monitoring data and a user-related metric in order to automatically generate a probabilistic model. The probabilistic model takes the form of a finite automaton. It is further used to produce a rank list of cloud deployment options. As a result, any of the cloud deployment options can be verified by applying a probabilistic model checking approach. Results: Testing was performed by ranking deployment options for two cloud applications, File Upload and Video-conferencing. The FoQoSAM method was compared to a baseline Analytic Hierarchy Process (AHP). The results show that the first ranked cloud deployment options satisfy all hard and at least one of the soft requirements for both methods, however, the FoQoSAM method always satisfies at least an additional QoS requirement compared to the baseline AHP method. Conclusions: The proposed new FoQoSAM method is appropriate and can be used in decision-making when ranking and verifying cloud deployment options. Due to its practical utility it was integrated into the SWITCH workbench. © 2019 Elsevier B.V.
DA  - 2019///
PY  - 2019
DO  - 10.1016/j.infsof.2019.01.003
VL  - 109
SP  - 14
EP  - 25
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059819610&doi=10.1016%2fj.infsof.2019.01.003&partnerID=40&md5=347c52a1b9e5ada9ff8c921d0e26eae4
DB  - Scopus
KW  - Edge
KW  - Cloud
KW  - Decision-making
KW  - Markov processes
KW  - Fog
KW  - Software engineering
KW  - Decision making
KW  - Application programs
KW  - Quality of service
KW  - Model checking
KW  - Probabilistic model checking
KW  - Stochastic models
KW  - Stochastic systems
KW  - Infrastructure as a service (IaaS)
KW  - Hierarchical systems
KW  - Clouds
KW  - Non-functional requirements
KW  - Probabilistic modeling
KW  - Analytic hierarchy process
KW  - Analytic hierarchy process (ahp)
KW  - Automated decision making
KW  - Decision making process
KW  - Equivalence classes
KW  - Quality of service assurances
KW  - Video conferencing
ER  - 

TY  - JOUR
T2  - Kubernetes: Horizontal Pod Autoscaling Policies
DA  - 2024///
PY  - 2024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213679908&partnerID=40&md5=159a43ff5dfebf83ea1c995e1a4508f4
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Microservices: The journey so far and challenges ahead
AU  - Jamshidi, P.
AU  - Pahl, C.
AU  - Mendonca, N.C.
AU  - Lewis, J.
AU  - Tilkov, S.
T2  - IEEE Software
AB  - Microservices are an architectural approach emerging out of service-oriented architecture, emphasizing self-management and lightweightness as the means to improve software agility, scalability, and autonomy. This article examines microservice evolution from the technological and architectural perspectives and discusses key challenges facing future microservice developments. © 1984-2012 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/MS.2018.2141039
VL  - 35
IS  - 3
SP  - 24
EP  - 35
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046893906&doi=10.1109%2fMS.2018.2141039&partnerID=40&md5=b2913a1bc8e6dd75d8bc75e9f8feac8e
DB  - Scopus
KW  - microservices
KW  - Microservices
KW  - service-oriented architecture
KW  - model-driven development
KW  - software engineering
KW  - Software design
KW  - Software engineering
KW  - SOA
KW  - software development
KW  - Service oriented architecture (SOA)
KW  - Information services
KW  - legacy systems
KW  - Legacy systems
KW  - Architectural approach
KW  - Model driven development
KW  - Anti-patterns
KW  - architectural antipatterns
KW  - DDD
KW  - domain-driven design
KW  - Domain-driven designs
KW  - MDD
KW  - Self management
ER  - 

TY  - CONF
TI  - When Failure is (Not) an Option: Reliability Models for Microservices Architectures
AU  - Jagadeesan, L.J.
AU  - Mendiratta, V.B.
T2  - Proceedings - 2020 IEEE 31st International Symposium on Software Reliability Engineering Workshops, ISSREW 2020
AB  - Modern application development and deployment is rapidly evolving to microservices based architectures, in which thousands of microservices communicate with one another and can be independently scaled and updated. While these architectures enable flexibility of deployment and frequency of upgrades, the naive use of thousands of communicating and frequently updated microservices can significantly impact the reliability of applications. To address these challenges, service meshes are used to rapidly detect and respond to microservices failures without necessitating changes to the microservices themselves. However, there are inherent tradeoffs that service meshes must make with regards to how quickly they assume a microservice has failed and the subsequent impact on overall application reliability. We present in this paper a modeling framework for microservices and service mesh reliability that takes these tradeoffs into account. Index Terms-microservices, service mesh, sidecars, circuit breakers, reliability, availability, resilience, reliability models, probabilistic model checking, PRISM.  © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ISSREW51248.2020.00031
SP  - 19
EP  - 24
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099866678&doi=10.1109%2fISSREW51248.2020.00031&partnerID=40&md5=02036281c8d341d90b2350c3db5cf12e
DB  - Scopus
KW  - Reliability
KW  - Commerce
KW  - Model checking
KW  - Probabilistic model checking
KW  - Probabilistic model-checking
KW  - Application deployment
KW  - Availability
KW  - Electric circuit breakers
KW  - Reliability modelling
KW  - Application development
KW  - Application reliabilities
KW  - Circuit-breakers
KW  - Mesh generation
KW  - Microservice, service mesh, sidecar, circuit breaker, reliability, availability, resilience, reliability model, probabilistic model checking, PRISM
KW  - microservices, service mesh, sidecars, circuit breakers, reliability, availability, resilience, reliability models, probabilistic model checking, PRISM
KW  - Modelling framework
KW  - Modern applications
KW  - Prisms
KW  - Timing circuits
ER  - 

TY  - CONF
TI  - Testing the scalability of SaaS applications
AU  - Tsai, W.-T.
AU  - Huang, Y.
AU  - Shao, Q.
T2  - Proceedings - 2011 IEEE International Conference on Service-Oriented Computing and Applications, SOCA 2011
AB  - Cloud computing and SaaS (Software-as-a-Service) received significant attention recently. Testing SaaS applications is important because many mission-critical applications will be deployed on the cloud. However, to the best of our knowledge, testing framework designed specifically for SaaS applications is not developed. The issue of testing the scalability of SaaS applications remains untouched. This paper discusses the unique features and challenges in testing SaaS applications, and proposes scalability metrics that can be used to test the scalability of SaaS applications. © 2011 IEEE.
DA  - 2011///
PY  - 2011
DO  - 10.1109/SOCA.2011.6166245
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860008701&doi=10.1109%2fSOCA.2011.6166245&partnerID=40&md5=522b825395549d0d3ec73f5f8e3cc9c0
DB  - Scopus
KW  - Scalability
KW  - Mission critical applications
KW  - Software-as-a-Service
KW  - Testing framework
KW  - Unique features
ER  - 

TY  - JOUR
TI  - PRISM: Probabilistic model checking for performance and reliability analysis
AU  - Kwiatkowska, M.
AU  - Norman, G.
AU  - Parker, D.
T2  - ACM SIGMETRICS Performance Evaluation Review
DA  - 2009///
PY  - 2009
VL  - 36
IS  - 4
SP  - 40
EP  - 45
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349958924&partnerID=40&md5=d57648ded64df82b94f3ee2c21aa45d7
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Energy-aware resource allocation heuristics for efficient management of data centers for Cloud computing
AU  - Beloglazov, A.
AU  - Abawajy, J.
AU  - Buyya, R.
T2  - Future Generation Computer Systems
AB  - Cloud computing offers utility-oriented IT services to users worldwide. Based on a pay-as-you-go model, it enables hosting of pervasive applications from consumer, scientific, and business domains. However, data centers hosting Cloud applications consume huge amounts of electrical energy, contributing to high operational costs and carbon footprints to the environment. Therefore, we need Green Cloud computing solutions that can not only minimize operational costs but also reduce the environmental impact. In this paper, we define an architectural framework and principles for energy-efficient Cloud computing. Based on this architecture, we present our vision, open research challenges, and resource provisioning and allocation algorithms for energy-efficient management of Cloud computing environments. The proposed energy-aware allocation heuristics provision data center resources to client applications in a way that improves energy efficiency of the data center, while delivering the negotiated Quality of Service (QoS). In particular, in this paper we conduct a survey of research in energy-efficient computing and propose: (a) architectural principles for energy-efficient management of Clouds; (b) energy-efficient resource allocation policies and scheduling algorithms considering QoS expectations and power usage characteristics of the devices; and (c) a number of open research challenges, addressing which can bring substantial benefits to both resource providers and consumers. We have validated our approach by conducting a performance evaluation study using the CloudSim toolkit. The results demonstrate that Cloud computing model has immense potential as it offers significant cost savings and demonstrates high potential for the improvement of energy efficiency under dynamic workload scenarios. © 2011 Elsevier B.V. All rights reserved.
DA  - 2012///
PY  - 2012
DO  - 10.1016/j.future.2011.04.017
VL  - 28
IS  - 5
SP  - 755
EP  - 768
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857370722&doi=10.1016%2fj.future.2011.04.017&partnerID=40&md5=404d8a6f96e7208ce49f7fb3ffc699cd
DB  - Scopus
KW  - Cloud computing
KW  - Energy efficiency
KW  - Resource management
KW  - Virtualization
KW  - Carbon footprint
KW  - Data centers
KW  - Performance evaluation
KW  - Quality of service
KW  - Resource allocation
KW  - Energy efficient
KW  - Energy aware
KW  - Computing environments
KW  - Green IT
KW  - Environmental impact
KW  - Research challenges
KW  - Resource provisioning
KW  - Computer systems
KW  - Dynamic consolidation
KW  - Resource providers
KW  - Research
KW  - Virtualizations
KW  - Allocation algorithm
KW  - Architectural frameworks
KW  - Architectural principles
KW  - Business domain
KW  - Client applications
KW  - Computing solutions
KW  - Cost saving
KW  - Electrical energy
KW  - Energy-efficient resource allocation
KW  - High potential
KW  - IT services
KW  - Operational costs
KW  - Pay-as-you-go
KW  - Pervasive applications
KW  - Power usage
ER  - 

TY  - JOUR
TI  - Global sensitivity analysis: An introduction
AU  - Saltelli, A.
T2  - Sensitivity Analysis of Model Output
DA  - 2004///
PY  - 2004
SP  - 27
EP  - 43
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247402449&partnerID=40&md5=b5b0c412c2b7c40fe40d767ec3228357
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Energy aware resource allocation of cloud data center: review and open issues
AU  - Akhter, N.
AU  - Othman, M.
T2  - Cluster Computing
AB  - The demand for cloud computing is increasing dramatically due to the high computational requirements of business, social, web and scientific applications. Nowadays, applications and services are hosted on the cloud in order to reduce the costs of hardware, software and maintenance. To satisfy this high demand, the number of large-scale data centers has increased, which consumes a high volume of electrical power, has a negative impact on the environment, and comes with high operational costs. In this paper, we discuss many ongoing or implemented energy aware resource allocation techniques for cloud environments. We also present a comprehensive review on the different energy aware resource allocation and selection algorithms for virtual machines in the cloud. Finally, we come up with further research issues and challenges for future cloud environments. © 2016, Springer Science+Business Media New York.
DA  - 2016///
PY  - 2016
DO  - 10.1007/s10586-016-0579-4
VL  - 19
IS  - 3
SP  - 1163
EP  - 1182
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976634782&doi=10.1007%2fs10586-016-0579-4&partnerID=40&md5=6ffeaf3095e13dd3504670dc628c9cbf
DB  - Scopus
KW  - Cloud computing
KW  - Energy efficiency
KW  - Power management
KW  - Virtualization
KW  - Application programs
KW  - Resource allocation
KW  - Cloud data centers
KW  - Virtual machines
KW  - Electric power utilization
KW  - Power consumption
KW  - Java programming language
KW  - Resource allocation techniques
KW  - Scientific applications
KW  - Virtualizations
KW  - Computational requirements
KW  - Allocation of virtual machine
KW  - Impact on the environment
KW  - Selection algorithm
ER  - 

TY  - JOUR
T2  - Production-Grade Container Orchestration
DA  - 2024///
PY  - 2024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197847259&partnerID=40&md5=6b50c85c92a179d7478fa3ac80025dc7
DB  - Scopus
ER  - 

TY  - JOUR
TI  - HANSEL: Adaptive horizontal scaling of microservices using Bi-LSTM
AU  - Yan, M.
AU  - Liang, X.
AU  - Lu, Z.
AU  - Wu, J.
AU  - Zhang, W.
T2  - Applied Soft Computing
AB  - With the rapid development of 5G network, business scenarios such as intelligent service and new retail are becoming more and more popular. The demand for more flexible and scalable real-time data processing, in particular, the AI-related data processing has also increased in edge computing. Therefore, how to meet such business development has become a major challenge. Focusing on this requirement, microservice architecture, proposed and developed by some big cloud computing companies’ platform, such as Google Kubernetes platform, has gradually become a mainstream technology solution in edge computing. However, many microservices used in edge computing cannot achieve an even time distribution, which is random or sudden. Kubernetes built-in Horizontal POD Autoscaling (HPA) is unable to well handle the change of microservice load, which inevitably leads to the waste of system resources and affects the SLA of microservice. To solve this issue, this paper proposes a HANSEL system based on Kubernetes platform, which can optimize the horizontal elastic scaling policy of Kubernetes by accurately predicting the load of microservices based on the Bi-LSTM load prediction algorithm with attention mechanism. Furthermore, active elastic scaling is realized through reinforcement learning method, and we design a hybrid elastic scaling mechanism through combining reactive and active methods, so as to construct an elastic scaling system for automatic scheduling of working nodes. Our experimental results show that HANSEL system can improve the system resource utilization by about 20% when meeting the microservice SLA of edge computing. © 2021 Elsevier B.V.
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.asoc.2021.107216
VL  - 105
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102654500&doi=10.1016%2fj.asoc.2021.107216&partnerID=40&md5=6f43e967886f90ae8ff608ea83599d09
DB  - Scopus
KW  - Edge computing
KW  - Microservice
KW  - Elastic scaling
KW  - Learning systems
KW  - Reinforcement learning
KW  - Long short-term memory
KW  - 5G mobile communication systems
KW  - Data handling
KW  - System resource utilization
KW  - Intelligent Services
KW  - Real-time data processing
KW  - Attention mechanisms
KW  - Automatic scheduling
KW  - Business development
KW  - Horizontal wells
KW  - LSTM
KW  - Reinforcement learning method
KW  - Technology solutions
ER  - 

TY  - CONF
TI  - Stochastic game analysis and latency awareness for proactive self-adaptation
AU  - Cámara, J.
AU  - Moreno, G.A.
AU  - Garlan, D.
T2  - 9th International Symposium on Software Engineering for Adaptive and Self-Managing Systems, SEAMS 2014 - Proceedings
AB  - Although different approaches to decision-making in self-adaptive systems have shown their effectiveness in the past by factoring in predictions about the system and its environment (e.g., resource availability), no proposal considers the latency associated with the execution of tactics upon the target system. However, different adaptation tactics can take different amounts of time until their effects can be observed. In reactive adaptation, ignoring adaptation tactic latency can lead to suboptimal adaptation decisions (e.g., activating a server that takes more time to boot than the transient spike in trafic that triggered its activation). In proactive adaptation, taking adaptation latency into account is necessary to get the system into the desired state to deal with an upcoming situation. In this paper, we introduce a formal analysis technique based on model checking of stochas- tic multiplayer games (SMGs) that enables us to quantify the potential benefits of employing different types of algorithms for self-adaptation. In particular, we apply this tech- nique to show the potential benefit of considering adaptation tactic latency in proactive adaptation algorithms. Our results show that factoring in tactic latency in decision making improves the outcome of adaptation. We also present an algorithm to do proactive adaptation that considers tactic latency, and show that it achieves higher utility than an algorithm that under the assumption of no latency is optimal.
DA  - 2014///
PY  - 2014
DO  - 10.1145/2593929.2593933
SP  - 155
EP  - 164
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903726367&doi=10.1145%2f2593929.2593933&partnerID=40&md5=3032e7301a89e4652795b29fbe20f513
DB  - Scopus
KW  - Software engineering
KW  - Decision making
KW  - Model checking
KW  - Stochastic systems
KW  - Algorithms
KW  - Latency
KW  - Resource availability
KW  - Potential benefits
KW  - Self-adaptive system
KW  - Adaptation decisions
KW  - Chemical activation
KW  - Formal analysis
KW  - Multiplayer games
KW  - Proactive adaptation
KW  - Proactive adaptations
KW  - Stochastic multiplayer games
ER  - 

TY  - JOUR
TI  - Quantitative Analysis With the Probabilistic Model Checker PRISM
AU  - Kwiatkowska, M.
AU  - Norman, G.
AU  - Parker, D.
T2  - Electronic Notes in Theoretical Computer Science
AB  - Probabilistic model checking is a formal verification technique for establishing the correctness, performance and reliability of systems which exhibit stochastic behaviour. As in conventional verification, a precise mathematical model of a real-life system is constructed first, and, given formal specifications of one or more properties of this system, an analysis of these properties is performed. The exploration of the system model is exhaustive and involves a combination of graph-theoretic algorithms and numerical methods. In this paper, we give a brief overview of the probabilistic model checker PRISM (www.cs.bham.ac.uk/~dxp/prism) implemented at the University of Birmingham. PRISM supports a range of probabilistic models and specification languages based on temporal logic, and has been recently extended with costs and rewards. We describe our experience with using PRISM to analyse a number of case studies from a wide range of application domains. We demonstrate the usefulness of probabilistic model checking techniques in detecting flaws and unusual trends, focusing mainly on the quantitative analysis of a range of best, worst and average-case system characteristics. © 2006 Elsevier B.V. All rights reserved.
DA  - 2006///
PY  - 2006
DO  - 10.1016/j.entcs.2005.10.030
VL  - 153
IS  - 2 SPEC. ISS.
SP  - 5
EP  - 31
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646404908&doi=10.1016%2fj.entcs.2005.10.030&partnerID=40&md5=233a50c4369a035c621327726b4666af
DB  - Scopus
KW  - Performance
KW  - dependability
KW  - performability
KW  - Reliability
KW  - Markov processes
KW  - reliability
KW  - Probabilistic model checking
KW  - Algorithms
KW  - Performability
KW  - Real time systems
KW  - Mathematical models
KW  - Markov models
KW  - Probabilistic logics
KW  - Temporal logic
KW  - probabilistic model checking
KW  - Automatic verification
KW  - Numerical analysis
KW  - Stochastic programming
KW  - temporal logic
ER  - 

TY  - JOUR
TI  - A Review of Auto-scaling Techniques for Elastic Applications in Cloud Environments
AU  - Lorido-Botran, T.
AU  - Miguel-Alonso, J.
AU  - Lozano, J.A.
T2  - Journal of Grid Computing
AB  - Cloud computing environments allow customers to dynamically scale their applications. The key problem is how to lease the right amount of resources, on a pay-as-you-go basis. Application re-dimensioning can be implemented effortlessly, adapting the resources assigned to the application to the incoming user demand. However, the identification of the right amount of resources to lease in order to meet the required Service Level Agreement, while keeping the overall cost low, is not an easy task. Many techniques have been proposed for automating application scaling. We propose a classification of these techniques into five main categories: static threshold-based rules, control theory, reinforcement learning, queuing theory and time series analysis. Then we use this classification to carry out a literature review of proposals for auto-scaling in the cloud. © 2014, Springer Science+Business Media Dordrecht.
DA  - 2014///
PY  - 2014
DO  - 10.1007/s10723-014-9314-7
VL  - 12
IS  - 4
SP  - 559
EP  - 592
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912530861&doi=10.1007%2fs10723-014-9314-7&partnerID=40&md5=2910478cbfc08e55ee6c8ebb60478a04
DB  - Scopus
KW  - Cloud computing
KW  - Auto-scaling
KW  - Reinforcement learning
KW  - Queueing theory
KW  - Cloud computing environments
KW  - Cloud environments
KW  - Computation theory
KW  - Service Level Agreements
KW  - Time series analysis
KW  - Literature reviews
KW  - Service level agreement
KW  - Elastic applications
KW  - Re-dimensioning
KW  - Scalable applications
KW  - Static thresholds
ER  - 

TY  - JOUR
TI  - The datacenter as a computer: An introduction to the design of warehouse-scale machines, second edition
AU  - Barroso, L.A.
AU  - Clidaras, J.
AU  - Hölzle, U.
T2  - Synthesis Lectures on Computer Architecture
AB  - As computation continues to move into the cloud, the computing platform of interest no longer resembles a pizza box or a refrigerator, but a warehouse full of computers. These new large datacenters are quite different from traditional hosting facilities of earlier times and cannot be viewed simply as a collection of co-located servers. Large portions of the hardware and software resources in these facilities must work in concert to efficiently deliver good levels of Internet service performance, something that can only be achieved by a holistic approach to their design and deployment. In other words, we must treat the datacenter itself as one massive warehouse-scale computer (WSC). We describe the architecture of WSCs, the main factors influencing their design, operation, and cost structure, and the characteristics of their software base. We hope it will be useful to architects and programmers of today's WSCs, as well as those of future many-core platforms which may one day implement the equivalent of today's WSCs on a single board. Notes for the Second Edition After nearly four years of substantial academic and industrial developments in warehouse-scale computing, we are delighted to present our first major update to this lecture. The increased popularity of public clouds has made WSC software techniques relevant to a larger pool of programmers since our first edition. Therefore, we expanded Chapter 2 to reflect our better understanding of WSC software systems and the toolbox of software techniques for WSC programming. In Chapter 3, we added to our coverage of the evolving landscape of wimpy vs. brawny server trade-offs, and we now present an overview of WSC interconnects and storage systems that was promised but lacking in the original edition. Thanks largely to the help of our new co-author, Google Distinguished Engineer Jimmy Clidaras, the material on facility mechanical and power distribution design has been updated and greatly extended (see Chapters 4 and 5). Chapters 6 and 7 have also been revamped significantly. We hope this revised edition continues to meet the needs of educators and professionals in this area. © 2013 by Morgan & Claypool.
DA  - 2013///
PY  - 2013
DO  - 10.2200/S00516ED2V01Y201306CAC024
VL  - 24
SP  - 1
EP  - 156
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883331358&doi=10.2200%2fS00516ED2V01Y201306CAC024&partnerID=40&md5=23b188236b7201d7b8ae9aa8c056cdf8
DB  - Scopus
KW  - cloud computing
KW  - Cloud computing
KW  - energy efficiency
KW  - Energy efficiency
KW  - distributed systems
KW  - Distributed systems
KW  - Computer architecture
KW  - Data centers
KW  - Cluster computing
KW  - Internet
KW  - Internet services
KW  - Computer software
KW  - Fault tolerant computer systems
KW  - Warehouses
KW  - Design
KW  - data centers
KW  - cluster computing
KW  - Computer organization
KW  - computer organization and design
KW  - fault-tolerant computing
KW  - Fault-tolerant computing
ER  - 

TY  - JOUR
T2  - Resize CPU and Memory Resources Assigned to Containers
DA  - 2023///
PY  - 2023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213688281&partnerID=40&md5=72f733c94aa6c06bcfad147f8c408491
DB  - Scopus
ER  - 

TY  - JOUR
TI  - A meta reinforcement learning-based virtual machine placement algorithm in mobile edge computing
AU  - Xu, H.
AU  - Jian, C.
T2  - Cluster Computing
AB  - Mobile edge computing requires more and more high-performance servers, resulting in increased energy consumption. As an effective means to reduce energy consumption, virtual machine placement (VMP) has been widely studied. In the edge computing environment, as the number of terminal device requests continues to increase, the scale of VMP becomes larger and larger, and existing research algorithms may take a long time to converge. The reason is that as the number of VMs increases, the search space of the policy becomes larger and the agent needs to interact with the environment for a longer time to make the best decision. In addition, existing research methods only consider reducing energy consumption, rarely consider the response latency of virtual machines, and almost ignore the dynamic changes of the edge environment. To overcome these drawbacks, we propose a virtual machine placement algorithm based on meta-reinforcement learning, which consists of an inner and outer loop. The inner loop designs a deep reinforcement learning algorithm combined with the order exchange and migration mechanism to generate the best decision, and the outer loop provides meta-strategy parameters for the inner loop based on meta-learning to accelerate the convergence capability of the inner loop, thereby obtaining efficient virtual machine placement decisions quickly from a new environment. Through simulation experiments, we demonstrate that our approach effectively reduces the energy consumption of the edge server and the response latency of VMs at different problem sizes compared to the three baseline algorithms. At the same time, it quickly adapts to the new environment with only a small number of gradient updates. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.
DA  - 2024///
PY  - 2024
DO  - 10.1007/s10586-023-04030-w
VL  - 27
IS  - 2
SP  - 1883
EP  - 1896
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161367765&doi=10.1007%2fs10586-023-04030-w&partnerID=40&md5=fc8dfecc0133a12712abb775ba680264
DB  - Scopus
KW  - Mobile edge computing
KW  - Network security
KW  - Energy-consumption
KW  - Green computing
KW  - Energy utilization
KW  - Reinforcement learning
KW  - Learning algorithms
KW  - Reinforcement learnings
KW  - Deep learning
KW  - Deep reinforcement learning
KW  - Virtual machine
KW  - Placement algorithm
KW  - Virtual machine placement
KW  - Virtual machine placements
KW  - E-learning
KW  - Best decision
KW  - Inner loops
KW  - Meta reinforcement learning
KW  - Outer loop
ER  - 

TY  - JOUR
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201045593&partnerID=40&md5=8e8a223fd88601b90b7afe7c5a769e87
DB  - Scopus
ER  - 

TY  - JOUR
TI  - A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II
AU  - Deb, K.
AU  - Agrawal, S.
AU  - Pratap, A.
AU  - Meyarivan, T.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Multi-objective evolutionary algorithms which use non-dominated sorting and sharing have been mainly criticized for their (i) O(MN3) computational complexity (where M is the number of objectives and N is the population size), (ii) non-elitism approach, and (iii) the need for specifying a sharing parameter. In this paper, we suggest a non-dominated sorting based multi-objective evolutionary algorithm (we called it the Non-dominated Sorting GA-II or NSGA-II) which alleviates all the above three difficulties. Specifically, a fast non-dominated sorting approach with O(MN2) computational complexity is presented. Second, a selection operator is presented which creates a mating pool by combining the parent and child populations and selecting the best (with respect to fitness and spread) N solutions. Simulation results on five difficult test problems show that the proposed NSGA-II, in most problems, is able to find much better spread of solutions and better convergence near the true Pareto-optimal front compared to PAES and SPEA—two other elitist multi-objective EAs which pay special attention towards creating a diverse Pareto-optimal front. Because of NSGA-II's low computational requirements, elitist approach, and parameter-less sharing approach, NSGA-II should find increasing applications in the years to come. © Springer-Verlag Berlin Heidelberg 2000.
DA  - 2000///
PY  - 2000
DO  - 10.1007/3-540-45356-3_83
VL  - 1917
SP  - 849
EP  - 858
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947926042&doi=10.1007%2f3-540-45356-3_83&partnerID=40&md5=48ed55dda6b441840a2f3c3fa46a1371
DB  - Scopus
KW  - Optimization
KW  - Multi objective
KW  - Multiobjective optimization
KW  - Genetic algorithms
KW  - Algorithms
KW  - Computational complexity
KW  - Evolutionary algorithms
KW  - Problem solving
KW  - Pareto principle
KW  - Computational requirements
KW  - Composite structures
KW  - Elitist non-dominated sorting genetic algorithms
KW  - Multi objective evolutionary algorithms
KW  - Non-dominated Sorting
KW  - Pareto-optimal front
KW  - Population sizes
KW  - Population statistics
KW  - Selection operators
ER  - 

TY  - JOUR
TI  - A model-driven approach for continuous performance engineering in microservice-based systems
AU  - Cortellessa, V.
AU  - Di Pompeo, D.
AU  - Eramo, R.
AU  - Tucci, M.
T2  - Journal of Systems and Software
AB  - Microservices are quite widely impacting on the software industry in recent years. Rapid evolution and continuous deployment represent specific benefits of microservice-based systems, but they may have a significant impact on non-functional properties like performance. Despite the obvious relevance of this property, there is still a lack of systematic approaches that explicitly take into account performance issues in the lifecycle of microservice-based systems. In such a context of evolution and re-deployment, Model-Driven Engineering techniques can provide major support to various software engineering activities, and in particular they can allow managing the relationships between a running system and its architectural model. In this paper, we propose a model-driven integrated approach that exploits traceability relationships between the monitored data of a microservice-based running system and its architectural model to derive recommended refactoring actions that lead to performance improvement. The approach has been applied and validated on two microservice-based systems, in the domain of e-commerce and ticket reservation, respectively, whose architectural models have been designed in UML profiled with MARTE. © 2021 The Author(s)
DA  - 2022///
PY  - 2022
DO  - 10.1016/j.jss.2021.111084
VL  - 183
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117098236&doi=10.1016%2fj.jss.2021.111084&partnerID=40&md5=47e70199c168eed6e6f288b9b64e7894
DB  - Scopus
KW  - Microservices
KW  - Performance
KW  - Microservice
KW  - Software engineering
KW  - Model-driven Engineering
KW  - Life cycle
KW  - Performance engineering
KW  - Architectural modeling
KW  - Continuous deployment
KW  - Model driven approach
KW  - Model-driven engineering
KW  - Running systems
KW  - Software evolution
KW  - Software Evolution
KW  - Software refactoring
ER  - 

TY  - CONF
TI  - A practical guide for using statistical tests to assess randomized algorithms in software engineering
AU  - Arcuri, A.
AU  - Briand, L.
T2  - Proceedings - International Conference on Software Engineering
AB  - Randomized algorithms have been used to successfully address many different types of software engineering problems. This type of algorithms employ a degree of randomness as part of their logic. Randomized algorithms are useful for difficult problems where a precise solution cannot be derived in a deterministic way within reasonable time. However, randomized algorithms produce different results on every run when applied to the same problem instance. It is hence important to assess the effectiveness of randomized algorithms by collecting data from a large enough number of runs. The use of rigorous statistical tests is then essential to provide support to the conclusions derived by analyzing such data. In this paper, we provide a systematic review of the use of randomized algorithms in selected software engineering venues in 2009. Its goal is not to perform a complete survey but to get a representative snapshot of current practice in software engineering research. We show that randomized algorithms are used in a significant percentage of papers but that, in most cases, randomness is not properly accounted for. This casts doubts on the validity of most empirical results assessing randomized algorithms. There are numerous statistical tests, based on different assumptions, and it is not always clear when and how to use these tests. We hence provide practical guidelines to support empirical research on randomized algorithms in software engineering © 2011 ACM.
DA  - 2011///
PY  - 2011
DO  - 10.1145/1985793.1985795
SP  - 1
EP  - 10
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959871222&doi=10.1145%2f1985793.1985795&partnerID=40&md5=059ab141072a983e9289c18ede5d03e8
DB  - Scopus
KW  - survey
KW  - systematic review
KW  - Software engineering
KW  - Software testing
KW  - Algorithms
KW  - Surveys
KW  - Systematic Review
KW  - Random processes
KW  - Engineering
KW  - Parameter estimation
KW  - confidence interval
KW  - Effect size
KW  - Statistical tests
KW  - bonferroni adjustment
KW  - Bonferroni adjustment
KW  - Confidence interval
KW  - effect size
KW  - non-parametric test
KW  - Non-parametric test
KW  - parametric test
KW  - Parametric test
KW  - statistical difference
KW  - Statistical differences
ER  - 

TY  - JOUR
TI  - TOPSIS for MODM
AU  - Lai, Y.-J.
AU  - Liu, T.-Y.
AU  - Hwang, C.-L.
T2  - European Journal of Operational Research
AB  - In this study, we extend TOPSIS to solve a multiple objective decision making problem. The principle of compromise (of TOPSIS) for multiple criteria decision making is that the chosen solution should have the shortest distance from the positive ideal solution as well as the longest distance from the negative ideal solution. Thus, we reduce a k-dimensional objective space to a two-dimensional objective space by a first-order compromise procedure. We then use membership functions of fuzzy set theory to represent the satisfaction level for both criteria. We obtain a single-objective programming problem by using the max-min operator for the second-order compromise operation. To illustrate the procedure, the Bow River Valley water quality management problem is solved by use of TOPSIS. © 1994.
DA  - 1994///
PY  - 1994
DO  - 10.1016/0377-2217(94)90282-8
VL  - 76
IS  - 3
SP  - 486
EP  - 500
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028483495&doi=10.1016%2f0377-2217%2894%2990282-8&partnerID=40&md5=88a5244232099fb09b0a1fac28c87e43
DB  - Scopus
KW  - Mathematical programming
KW  - Vectors
KW  - Constraint theory
KW  - Compromise operation
KW  - Decision theory
KW  - Fuzzy sets
KW  - Mathematical operators
KW  - Membership functions
KW  - Multiple objective decision making
KW  - Negative ideal solution
KW  - Positive ideal solution
KW  - Project management
ER  - 

TY  - JOUR
T2  - UML Profile for Modeling and Analysis of Real-Time and Embedded Systems (MARTE)
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-47749093851&partnerID=40&md5=a64362f7427f163a983bba268aac625f
DB  - Scopus
ER  - 

TY  - JOUR
TI  - A new meta-heuristic approach for load aware-cost effective workflow scheduling
AU  - Ram, S.D.K.
AU  - Srivastava, S.
AU  - Mishra, K.K.
T2  - Concurrency and Computation: Practice and Experience
AB  - Workflow scheduling is an important way to manage the execution of a workflow. It introduces the concept of providing suitable resources to workflow tasks in order to finish workflow execution and meet the user's objectives. However, the problem becomes more complex when scheduling must balance two conflicting objectives, such as minimizing execution cost and maximizing load across all computing resources. A workflow has many interdependent tasks, and the cloud datacenter has many computing resources to execute the workflow. There can be an asymptotically infinite number of mappings of tasks-to-computing resources. Every mapping produces different execution costs with different workloads on computing resources. The main challenge for the researcher is to develop an intelligent scheduling algorithm to identify an optimal mapping that produces minimal execution cost with fair workload distribution on resources. We developed a novel meta-heuristic algorithm named Investment-Based Optimization (IBO) to identify an optimal mapping. The IBO algorithm was first tested on optimization benchmark functions and then simulated in CloudSim to see its performance for scheduling workflows. Finally, IBO was tested over Montage, Epigenomics, Sipht, and a sample workflow, and it was found that IBO reduces execution costs by 33%, 16%, 16.36%, and 20% with a fair workload distribution. © 2022 John Wiley & Sons, Ltd.
DA  - 2022///
PY  - 2022
DO  - 10.1002/cpe.7112
VL  - 34
IS  - 21
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130760464&doi=10.1002%2fcpe.7112&partnerID=40&md5=a77150a45168d7312481f660d9dbf35f
DB  - Scopus
KW  - cloud computing
KW  - workflow
KW  - Cost effectiveness
KW  - Scheduling
KW  - Optimisations
KW  - Heuristic algorithms
KW  - Cloud-computing
KW  - Optimization
KW  - meta-heuristic
KW  - Scheduling algorithms
KW  - Benchmarking
KW  - Metaheuristic
KW  - Execution costs
KW  - Heuristic methods
KW  - Workflow scheduling
KW  - Computing resource
KW  - Work-flows
KW  - Mapping
KW  - workflow scheduling
KW  - Cloudsim
KW  - CloudSim
KW  - Optimal mapping
KW  - Work-load distribution
ER  - 

TY  - JOUR
TI  - EnReal: An Energy-Aware Resource Allocation Method for Scientific Workflow Executions in Cloud Environment
AU  - Xu, X.
AU  - Dou, W.
AU  - Zhang, X.
AU  - Chen, J.
T2  - IEEE Transactions on Cloud Computing
AB  - Scientific workflows are often deployed across multiple cloud computing platforms due to their large-scale characteristic. This can be technically achieved by expanding a cloud platform. However, it is still a challenge to conduct scientific workflow executions in an energy-aware fashion across cloud platforms or even inside a cloud platform, since the cloud platform expansion will make the energy consumption a big concern. In this paper, we propose an Energy-aware Resource Allocation method, named EnReal, to address the above challenge. Basically, we leverage the dynamic deployment of virtual machines for scientific workflow executions. Specifically, an energy consumption model is presented for applications deployed across cloud computing platforms, and a corresponding energy-aware resource allocation algorithm is proposed for virtual machine scheduling to accomplish scientific workflow executions. Experimental evaluation demonstrates that the proposed method is both effective and efficient. © 2013 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/TCC.2015.2453966
VL  - 4
IS  - 2
SP  - 166
EP  - 179
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976588157&doi=10.1109%2fTCC.2015.2453966&partnerID=40&md5=08980cc191d10bca2dc09ef67c1074b4
DB  - Scopus
KW  - cloud computing
KW  - Cloud computing
KW  - resource allocation
KW  - Scientific workflows
KW  - Power management
KW  - Energy utilization
KW  - Resource allocation
KW  - Energy aware
KW  - Experimental evaluation
KW  - Energy consumption model
KW  - Java programming language
KW  - Resource allocation algorithms
KW  - Dynamic deployment
KW  - Cloud computing platforms
KW  - Energy-aware method
KW  - scientific workflow
KW  - Virtual machine scheduling
ER  - 

TY  - CONF
TI  - Search Budget in Multi-Objective Refactoring optimization: a Model-Based Empirical Study
AU  - Di Pompeo, D.
AU  - Tucci, M.
T2  - Proceedings - 48th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2022
AB  - Software model optimization is the task of automatically generate design alternatives, usually to improve quality aspects of software that are quantifiable, like performance and reliability. In this context, multi-objective optimization techniques have been applied to help the designer find suitable tradeoffs among several non-functional properties. In this process, design alternatives can be generated through automated model refactoring, and evaluated on non-functional models. Due to their complexity, this type of optimization tasks require considerable time and resources, often limiting their application in software engineering processes.In this paper, we investigate the effects of using a search budget, specifically a time limit, to the search for new solutions. We performed experiments to quantify the impact that a change in the search budget may have on the quality of solutions. Furthermore, we analyzed how different genetic algorithms (i.e., NSGh-II, SPEh2, and PESA2) perform when imposing different budgets. We experimented on two case studies of different size, complexity, and domain.We observed that imposing a search budget considerably deteriorates the quality of the generated solutions, but the specific algorithm we choose seems to play a crucial role. From our experiments, NSGh-II is the fastest algorithm, while PESA2 generates solutions with the highest quality. Differently, SPEh2 is the slowest algorithm, and produces the solutions with the lowest quality. © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/SEAA56994.2022.00070
SP  - 406
EP  - 413
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146884337&doi=10.1109%2fSEAA56994.2022.00070&partnerID=40&md5=1dcd8d5fb8b83d67b23ffca1be5a28c7
DB  - Scopus
KW  - performance
KW  - Performance
KW  - non-functional properties
KW  - Application programs
KW  - Optimisations
KW  - Multi objective
KW  - Multiobjective optimization
KW  - Genetic algorithms
KW  - model-driven engineering
KW  - Model-driven Engineering
KW  - refactoring
KW  - Refactorings
KW  - Software reliability
KW  - Budget control
KW  - Model-based OPC
KW  - Empirical studies
KW  - Design alternatives
KW  - multi-objective
KW  - Non functional properties
KW  - Software modeling
ER  - 

TY  - JOUR
AU  - Boehm, B.W.
T2  - Software Cost Estimation with COCOMO II
DA  - 2000///
PY  - 2000
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003567818&partnerID=40&md5=cd091e62c66f44e210bbfa7ffcb15652
DB  - Scopus
ER  - 

TY  - CONF
TI  - On parameter tuning in search based software engineering
AU  - Arcuri, A.
AU  - Fraser, G.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - When applying search-based software engineering (SBSE) techniques one is confronted with a multitude of different parameters that need to be chosen: Which population size for a genetic algorithm? Which selection mechanism to use? What settings to use for dozens of other parameters? This problem not only troubles users who want to apply SBSE tools in practice, but also researchers performing experimentation - how to compare algorithms that can have different parameter settings? To shed light on the problem of parameters, we performed the largest empirical analysis on parameter tuning in SBSE to date, collecting and statistically analysing data from more than a million experiments. As case study, we chose test data generation, one of the most popular problems in SBSE. Our data confirm that tuning does have a critical impact on algorithmic performance, and over-fitting of parameter tuning is a dire threat to external validity of empirical analyses in SBSE. Based on this large empirical evidence, we give guidelines on how to handle parameter tuning. © 2011 Springer-Verlag.
DA  - 2011///
PY  - 2011
DO  - 10.1007/978-3-642-23716-4_6
VL  - 6956 LNCS
SP  - 33
EP  - 47
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052895933&doi=10.1007%2f978-3-642-23716-4_6&partnerID=40&md5=3f92d4cb44dadf12e21ec2d3ebf7796e
DB  - Scopus
KW  - Software engineering
KW  - Search-based software engineering
KW  - Algorithms
KW  - Experiments
KW  - Object oriented
KW  - Engineering
KW  - Data communication systems
KW  - Parameter estimation
KW  - Population sizes
KW  - Population statistics
KW  - Empirical analysis
KW  - Empirical evidence
KW  - object-oriented
KW  - Overfitting
KW  - Parameter setting
KW  - Parameter-tuning
KW  - Search based software engineering
KW  - Selection mechanism
KW  - test data generation
KW  - Test data generation
KW  - unit testing
ER  - 

TY  - CONF
TI  - Hybrid semantics for PEPA
AU  - Bortolussi, L.
AU  - Galpin, V.
AU  - Hillston, J.
AU  - Tribastone, M.
T2  - Proceedings - 7th International Conference on the Quantitative Evaluation of Systems, QEST 2010
AB  - In order to circumvent the problem of statespace explosion of large-scale Markovian models, the stochastic process algebra PEPA has been given a fluid semantics based on ordinary differential equations, treating all entities as continuous. However, low numbers of instances and/or relatively slow dynamics may make such approximation too coarse for some parts of the system. To deal with such situations, we propose an hybrid semantics lying between these two extremes, treating parts of the system as discrete and stochastic and others as continuous and deterministic. The underlying mathematical object for the quantitative evaluation is a stochastic hybrid automaton. A case study of a client/server system with breakdowns and repairs is used to discuss the accuracy and the cost of this hybrid analysis. © 2010 IEEE.
DA  - 2010///
PY  - 2010
DO  - 10.1109/QEST.2010.31
SP  - 181
EP  - 190
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649461070&doi=10.1109%2fQEST.2010.31&partnerID=40&md5=0bbbdb7275b92bc953c909ce0eb5397e
DB  - Scopus
KW  - Markov processes
KW  - Stochastic models
KW  - Stochastic systems
KW  - Quantitative evaluation
KW  - Breakdowns and repairs
KW  - Client/server systems
KW  - Hybrid analysis
KW  - Hybrid automatons
KW  - Markovian model
KW  - Mathematical objects
KW  - Ordinary differential equations
KW  - Slow dynamics
KW  - State-space explosion
KW  - Stochastic process algebras
ER  - 

TY  - JOUR
TI  - Software Cost Estimation, Benchmarking, and Risk Assessment: The Software Decision-Makers’
AU  - Trendowicz, A.
T2  - Guide to Predictable Software Development.
DA  - 2013///
PY  - 2013
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169391889&partnerID=40&md5=afcb1c1fca4c430d45380796f1d8fe4d
DB  - Scopus
ER  - 

TY  - JOUR
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201046609&partnerID=40&md5=44bf75ed1c094157bdd91143600e899d
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Task Scheduling in Cloud Computing based on Meta-heuristics: Review, Taxonomy, Open Challenges, and Future Trends
AU  - Houssein, E.H.
AU  - Gad, A.G.
AU  - Wazery, Y.M.
AU  - Suganthan, P.N.
T2  - Swarm and Evolutionary Computation
AB  - Cloud computing is a recently looming-evoked paradigm, the aim of which is to provide on-demand, pay-as-you-go, internet-based access to shared computing resources (hardware and software) in a metered, self-service, dynamically scalable fashion. A related hot topic at the moment is task scheduling, which is well known for delivering critical cloud service performance. However, the dilemmas of resources being underutilized (underloaded) and overutilized (overloaded) may arise as a result of improper scheduling, which in turn leads to either wastage of cloud resources or degradation in service performance, respectively. Thus, the idea of incorporating meta-heuristic algorithms into task scheduling emerged in order to efficiently distribute complex and diverse incoming tasks (cloudlets) across available limited resources, within a reasonable time. Meta-heuristic techniques have proven very capable of solving scheduling problems, which is fulfilled herein from a cloud perspective by first providing a brief on traditional and heuristic scheduling methods before diving deeply into the most popular meta-heuristics for cloud task scheduling followed by a detailed systematic review featuring a novel taxonomy of those techniques, along with their advantages and limitations. More specifically, in this study, the basic concepts of cloud task scheduling are addressed smoothly, as well as diverse swarm, evolutionary, physical, emerging, and hybrid meta-heuristic scheduling techniques are categorized as per the nature of the scheduling problem (i.e., single- or multi-objective), the primary objective of scheduling, task-resource mapping scheme, and scheduling constraint. Armed with these methods, some of the most recent relevant literature are surveyed, and insights into the identification of existing challenges are presented, along with a trail to potential solutions. Furthermore, guidelines to future research directions drawn from recently emerging trends are outlined, which should definitely contribute to assisting current researchers and practitioners as well as pave the way for newbies excited about cloud task scheduling to pursue their own glory in the field. © 2021 Elsevier B.V.
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.swevo.2021.100841
VL  - 62
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100086183&doi=10.1016%2fj.swevo.2021.100841&partnerID=40&md5=8aab29bab18b62b064d7742b362fb40e
DB  - Scopus
KW  - Cloud computing
KW  - Scheduling
KW  - Optimisations
KW  - Heuristic algorithms
KW  - Quality of service
KW  - Cloud-computing
KW  - Optimization
KW  - Quality-of-service
KW  - Multitasking
KW  - Tasks scheduling
KW  - Scheduling algorithms
KW  - Metaheuristic
KW  - Systematic review
KW  - Systematic Review
KW  - Simulation tools
KW  - Taxonomies
KW  - Heuristic methods
KW  - Task scheduling
KW  - Future trends
KW  - Meta-heuristics
KW  - Open challenge
KW  - Open challenges
KW  - Quality of Service (QoS)
KW  - Simulation tool
ER  - 

TY  - JOUR
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201060421&partnerID=40&md5=927160d8f227e689dbd8726ac21cd41a
DB  - Scopus
ER  - 

TY  - CONF
TI  - Application interference analysis: Towards energy-efficient workload management on heterogeneous micro-server architectures
AU  - Hahnel, M.
AU  - Arega, F.M.
AU  - Dargie, W.
AU  - Khasanov, R.
AU  - Castrillon, J.
T2  - 2017 IEEE Conference on Computer Communications Workshops, INFOCOM WKSHPS 2017
AB  - The ever increasing demand for Internet traffic, storage and processing requires an ever increasing amount of hardware resources. In addition to this, infrastructure providers over-provision system architectures to serve users at peak times without performance delays. Over-provisioning leads to underutilization and thus to unnecessary power consumption. Therefore, there is a need for workload management strategies to map and schedule different services simultaneously in an energy-efficient manner without compromising performance, specially for heterogeneous micro-server architectures. This requires statistical models of how services interfere with each other, thereby affecting both performance and energy consumption. Indeed, the performance-energy behavior when mixing workloads is not well understood. This paper presents an interference analysis for heterogeneous workloads (i.e., CPU- and memory-intensive) on a big.LITTLE MPSoC architecture. We employ state-of-the-art tools to generate multiple single-application mappings and characterize the interference among two different services. We observed a performance degradation factor between 1.1 and 2.5. For some configurations, executing on different clusters resulted in reduced energy consumption with no performance penalty. This kind of detailed analysis give us first insights towards more general models for future workload management systems. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/INFCOMW.2017.8116415
SP  - 432
EP  - 437
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041308410&doi=10.1109%2fINFCOMW.2017.8116415&partnerID=40&md5=f93e550bb1af29e947fee389263c47aa
DB  - Scopus
KW  - Energy efficiency
KW  - Scheduling
KW  - Energy utilization
KW  - Performance degradation
KW  - Interference
KW  - Mapping
KW  - Energy efficient computing
KW  - Infrastructure providers
KW  - Performance penalties
KW  - Multiprocessing systems
KW  - System-on-chip
KW  - Heterogeneous workloads
KW  - Energy-efficient computing
KW  - Heterogeneous
KW  - Interference analysis
KW  - MPSoCs
KW  - Wave interference
ER  - 

TY  - JOUR
TI  - Many-objective optimization of non-functional attributes based on refactoring of software models
AU  - Cortellessa, V.
AU  - Di Pompeo, D.
AU  - Stoico, V.
AU  - Tucci, M.
T2  - Information and Software Technology
AB  - Context: Software quality estimation is a challenging and time-consuming activity, and models are crucial to face the complexity of such activity on modern software applications. In this context, software refactoring is a crucial activity within development life-cycles where requirements and functionalities rapidly evolve. Objective: One main challenge is that the improvement of distinctive quality attributes may require contrasting refactoring actions on software, as for trade-off between performance and reliability (or other non-functional attributes). In such cases, multi-objective optimization can provide the designer with a wider view on these trade-offs and, consequently, can lead to identify suitable refactoring actions that take into account independent or even competing objectives. Method: In this paper, we present an approach that exploits the NSGA-II as the genetic algorithm to search optimal Pareto frontiers for software refactoring while considering many objectives. We consider performance and reliability variations of a model alternative with respect to an initial model, the amount of performance antipatterns detected on the model alternative, and the architectural distance, which quantifies the effort to obtain a model alternative from the initial one. Results: We applied our approach on two case studies: a Train Ticket Booking Service, and CoCoME. We observed that our approach is able to improve performance (by up to 42%) while preserving or even improving the reliability (by up to 32%) of generated model alternatives. We also observed that there exists an order of preference of refactoring actions among model alternatives. Conclusion: Based on our analysis, we can state that performance antipatterns confirmed their ability to improve performance of a subject model in the context of many-objective optimization. In addition, the metric that we adopted for the architectural distance seems to be suitable for estimating the refactoring effort. © 2023 Elsevier B.V.
DA  - 2023///
PY  - 2023
DO  - 10.1016/j.infsof.2023.107159
VL  - 157
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147547137&doi=10.1016%2fj.infsof.2023.107159&partnerID=40&md5=1e209abdb3cc33fb8cb5dbab48c5e223
DB  - Scopus
KW  - Performance
KW  - Reliability
KW  - Software architecture
KW  - Application programs
KW  - Economic and social effects
KW  - Multiobjective optimization
KW  - Genetic algorithms
KW  - Model-driven Engineering
KW  - Refactorings
KW  - Search-based
KW  - Search-based software engineering
KW  - Multi-objectives optimization
KW  - Refactoring
KW  - Software reliability
KW  - Life cycle
KW  - Multi-objective optimization
KW  - Model-driven engineering
KW  - Software refactoring
KW  - Computer software selection and evaluation
KW  - Functional attribute
KW  - Many-objective optimizations
KW  - Non-functional
ER  - 

TY  - JOUR
TI  - Microservices and machine learning algorithms for adaptive green buildings
AU  - Rodríguez-Gracia, D.
AU  - Piedra-Fernández, J.A.
AU  - Iribarne, L.
AU  - Criado, J.
AU  - Ayala, R.
AU  - Alonso-Montesinos, J.
AU  - de las Mercedes, C.-U.M.
T2  - Sustainability (Switzerland)
AB  - In recent years, the use of services for Open Systems development has consolidated and strengthened. Advances in the Service Science and Engineering (SSE) community, promoted by the reinforcement ofWeb Services and SemanticWeb technologies and the presence of new Cloud computing techniques, such as the proliferation of microservices solutions, have allowed software architects to experiment and develop new ways of building open and adaptable computer systems at runtime. Home automation, intelligent buildings, robotics, graphical user interfaces are some of the social atmosphere environments suitable in which to apply certain innovative trends. This paper presents a schema for the adaptation of Dynamic Computer Systems (DCS) using interdisciplinary techniques on model-driven engineering, service engineering and soft computing. The proposal manages an orchestrated microservices schema for adapting component-based software architectural systems at runtime. This schema has been developed as a three-layer adaptive transformation process that is supported on a rule-based decision-making service implemented by means of Machine Learning (ML) algorithms. The experimental development was implemented in the Solar Energy Research Center (CIESOL) applying the proposed microservices schema for adapting home architectural atmosphere systems on Green Buildings. © 2019 by the authors.
DA  - 2019///
PY  - 2019
DO  - 10.3390/su11164320
VL  - 11
IS  - 16
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070712595&doi=10.3390%2fsu11164320&partnerID=40&md5=547ebabe27f2f4e288d0a8b65e03a6b6
DB  - Scopus
KW  - Microservices
KW  - machine learning
KW  - automation
KW  - Machine learning
KW  - software
KW  - Adaptive systems
KW  - algorithm
KW  - building
KW  - Smart building
KW  - World Wide Web
KW  - innovation
KW  - computer system
KW  - adaptive management
KW  - interdisciplinary approach
KW  - robotics
KW  - trend analysis
ER  - 

TY  - CONF
TI  - Towards design-time simulation support for energy-aware cloud application development
AU  - Ponsard, C.
AU  - De Landtsheer, R.
AU  - Ospina, G.
AU  - Deprez, J.-C.
T2  - CLOSER 2016 - Proceedings of the 6th International Conference on Cloud Computing and Services Science
AB  - Cloud application deployment is becoming increasingly popular for the removal of upfront hardware costs, the pay-per-use cost model and their ability to scale. However, deploying software on the Cloud carries both opportunities and threats regarding energy efficiency. In order to help Cloud application developers learn and reason about the energy consumption of their application on the server-side, we have developed a framework centred on a UML profile for relating energy goals, requirements and associated KPI metrics to application design and deployment elements. Our previous work has focused on the use of such a framework to carry out our run-time experiments in order to select the best approach. In this paper, we explore the feasibility of a complementary approach for providing support at design time based on finer grained deployment models, the specification of Cloud and energy adaptation policies and the use of a discrete event simulator for reasoning on key performance indicators such as energy but also overall performance, delay and costs. The goal is to support the Cloud developer in pre-selecting the best trade-off that can be further tuned at run-time. Copyright © 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.
DA  - 2016///
PY  - 2016
DO  - 10.5220/0005933503980404
VL  - 2
SP  - 398
EP  - 404
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979518755&doi=10.5220%2f0005933503980404&partnerID=40&md5=bcca5ad71ffaceec4c170c45ec152ac1
DB  - Scopus
KW  - Cloud computing
KW  - Energy efficiency
KW  - Self-adaptation
KW  - Cloud
KW  - Power management
KW  - Sustainable development
KW  - Green computing
KW  - Sustainability
KW  - Economic and social effects
KW  - Energy utilization
KW  - Cloud applications
KW  - Benchmarking
KW  - Key performance indicators
KW  - Clouds
KW  - Adaptation policies
KW  - Self adaptation
KW  - Application design
KW  - Deployment models
KW  - Discrete-event simulators
KW  - Discrete event simulation
KW  - Green-it
KW  - Hardware cost
ER  - 

TY  - CONF
TI  - Power provisioning for a warehouse-sized computer
AU  - Fan, X.
AU  - Weber, W.-D.
AU  - Barroso, L.A.
T2  - Proceedings - International Symposium on Computer Architecture
AB  - Large-scale Internet services require a computing infrastructure that can beappropriately described as a warehouse-sized computing system. The cost ofbuilding datacenter facilities capable of delivering a given power capacity tosuch a computer can rival the recurring energy consumption costs themselves.Therefore, there are strong economic incentives to operate facilities as closeas possible to maximum capacity, so that the non-recurring facility costs canbe best amortized. That is difficult to achieve in practice because ofuncertainties in equipment power ratings and because power consumption tends tovary significantly with the actual computing activity. Effective powerprovisioning strategies are needed to determine how much computing equipmentcan be safely and efficiently hosted within a given power budget. In this paper we present the aggregate power usage characteristics of largecollections of servers (up to 15 thousand) for different classes ofapplications over a period of approximately six months. Those observationsallow us to evaluate opportunities for maximizing the use of the deployed powercapacity of datacenters, and assess the risks of over-subscribing it. We findthat even in well-tuned applications there is a noticeable gap (7 - 16%)between achieved and theoretical aggregate peak power usage at the clusterlevel (thousands of servers). The gap grows to almost 40% in wholedatacenters. This headroom can be used to deploy additional compute equipmentwithin the same power budget with minimal risk of exceeding it. We use ourmodeling framework to estimate the potential of power management schemes toreduce peak power and energy usage. We find that the opportunities for powerand energy savings are significant, but greater at the cluster-level (thousandsof servers) than at the rack-level (tens). Finally we argue that systems needto be power efficient across the activity range, and not only at peakperformance levels. Copyright 2007 ACM.
DA  - 2007///
PY  - 2007
DO  - 10.1145/1250662.1250665
SP  - 13
EP  - 23
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-35348835964&doi=10.1145%2f1250662.1250665&partnerID=40&md5=0ba10bc53962f2321e9062dec64e3177
DB  - Scopus
KW  - Energy efficiency
KW  - Electric power utilization
KW  - Internet
KW  - Warehouses
KW  - Telecommunication services
KW  - Power modeling
KW  - Mathematical models
KW  - Power provisioning
ER  - 

TY  - CONF
TI  - Distributed qality-atribute optimization of sofware architectures
AU  - Rago, A.
AU  - Vidal, S.
AU  - Andres Diaz-Pace, J.
AU  - Frank, S.
AU  - Van Hoorn, A.
T2  - ACM International Conference Proceeding Series
AB  - A key challenge of software architecture design is how to satisfy quality-attribute requirements, which often confict with each other. This is usually a complex task, because there are several candidates for architectural solutions meeting the same requirements, and quality-attribute tradeofs of those solutions need to be considered by the architects. In this context, we present the SQuAT framework to assist architects in the exploration of design solutions and their tradeofs. This framework provides a modular approach for integrating quality-attribute analyzers and solvers, and also features a distributed search-based optimization. In this paper, we report on an experience using SQuAT with Palladio architectural models, which integrates third-party tools for performance and modifability, and shows the tradeofs among candidate solutions to the architect. Furthermore, we enhance the standard search schema of SQuAT with a distributed negotiation technique based on monotonic concession, in order to provide better tradeofs for the architect's decision making. © 2017 Association for Computing Machinery.
DA  - 2017///
PY  - 2017
DO  - 10.1145/3132498.3132509
VL  - Part F130660
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037813858&doi=10.1145%2f3132498.3132509&partnerID=40&md5=caeff8b75f9a1add3268059c459d1ad7
DB  - Scopus
KW  - Software architecture
KW  - Decision making
KW  - Commerce
KW  - Software architectures
KW  - Agents
KW  - Software agents
KW  - Computer software reusability
KW  - Quality attributes
KW  - Architectural models
KW  - Architectural solutions
KW  - Distributed negotiations
KW  - Distributed search
KW  - Modular approach
KW  - Software architecture design
KW  - Third-party tools
ER  - 

TY  - JOUR
TI  - Dominance statistics: Ordinal analyses to answer ordinal questions
AU  - Cliff, N.
T2  - Psychological Bulletin
AB  - Much behavioral research involves comparing the central tendencies of different groups, or of the same subjects under different conditions, and the usual analysis is some form of mean comparison. This article suggests that an ordinal statistic, d, is often more appropriate. d compares the number of times a score from one group or condition is higher than one from the other, compared with the reverse. Compared to mean comparisons, d is more robust and equally or more powerful; it is invariant under transformation: and it often conforms more closely to the experimenter's research hypothesis. It is suggested that inferences from d be based on sample estimates of its variance rather than on the more traditional assumption of identical distributions. The statistic is extended to simple repeated measures designs, and ways of extending its use to more complex designs are suggested.
DA  - 1993///
PY  - 1993
DO  - 10.1037/0033-2909.114.3.494
VL  - 114
IS  - 3
SP  - 494
EP  - 509
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-12044258480&doi=10.1037%2f0033-2909.114.3.494&partnerID=40&md5=2ca6ac7262d59edf80475edaf25c7f18
DB  - Scopus
ER  - 

TY  - CONF
TI  - A catalogue of green architectural tactics for the cloud
AU  - Procaccianti, G.
AU  - Lago, P.
AU  - Lewis, G.A.
T2  - Proceedings - 2014 IEEE 8th International Symposium on the Maintenance and Evolution of Service-Oriented and Cloud-Based Systems, MESOCA 2014
AB  - Energy efficiency is a primary concern for the ICTsector. In particular, the widespread adoption of cloud computing technologies has drawn attention to the massive energy consumption of data centers. Although hardware constantly improves with respect to energy efficiency, this should also be a main concern for software. In previous work we analyzed the literature and elicited a set of techniques for addressing energy efficiency in cloud-based software architectures. In this work we codified these techniques in the form of Green Architectural Tactics. These tactics will help architects extend their design reasoning towards energy efficiency and to apply reusable solutions for greener software. © 2014 IEEE.
DA  - 2014///
PY  - 2014
DO  - 10.1109/MESOCA.2014.12
SP  - 29
EP  - 36
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933047019&doi=10.1109%2fMESOCA.2014.12&partnerID=40&md5=fb46ca5da4f70055aa3a5818986d6d68
DB  - Scopus
KW  - Cloud computing
KW  - Energy efficiency
KW  - Software Architecture
KW  - Cloud Computing
KW  - Software architecture
KW  - Energy Efficiency
KW  - Computer architecture
KW  - Data centers
KW  - Green computing
KW  - Energy utilization
KW  - Cloud-based
KW  - Computer software reusability
KW  - Cloud computing technologies
KW  - Architectural Tactics
KW  - Design reasonings
ER  - 

TY  - JOUR
TI  - Energy-aware resource allocation heuristics for efficient management of data centers for Cloud computing
AU  - Beloglazov, A.
AU  - Abawajy, J.
AU  - Buyya, R.
T2  - Future Generation Computer Systems
AB  - Cloud computing offers utility-oriented IT services to users worldwide. Based on a pay-as-you-go model, it enables hosting of pervasive applications from consumer, scientific, and business domains. However, data centers hosting Cloud applications consume huge amounts of electrical energy, contributing to high operational costs and carbon footprints to the environment. Therefore, we need Green Cloud computing solutions that can not only minimize operational costs but also reduce the environmental impact. In this paper, we define an architectural framework and principles for energy-efficient Cloud computing. Based on this architecture, we present our vision, open research challenges, and resource provisioning and allocation algorithms for energy-efficient management of Cloud computing environments. The proposed energy-aware allocation heuristics provision data center resources to client applications in a way that improves energy efficiency of the data center, while delivering the negotiated Quality of Service (QoS). In particular, in this paper we conduct a survey of research in energy-efficient computing and propose: (a) architectural principles for energy-efficient management of Clouds; (b) energy-efficient resource allocation policies and scheduling algorithms considering QoS expectations and power usage characteristics of the devices; and (c) a number of open research challenges, addressing which can bring substantial benefits to both resource providers and consumers. We have validated our approach by conducting a performance evaluation study using the CloudSim toolkit. The results demonstrate that Cloud computing model has immense potential as it offers significant cost savings and demonstrates high potential for the improvement of energy efficiency under dynamic workload scenarios. © 2011 Elsevier B.V. All rights reserved.
DA  - 2012///
PY  - 2012
DO  - 10.1016/j.future.2011.04.017
VL  - 28
IS  - 5
SP  - 755
EP  - 768
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857370722&doi=10.1016%2fj.future.2011.04.017&partnerID=40&md5=404d8a6f96e7208ce49f7fb3ffc699cd
DB  - Scopus
KW  - Cloud computing
KW  - Energy efficiency
KW  - Resource management
KW  - Virtualization
KW  - Carbon footprint
KW  - Data centers
KW  - Performance evaluation
KW  - Quality of service
KW  - Resource allocation
KW  - Energy efficient
KW  - Energy aware
KW  - Computing environments
KW  - Green IT
KW  - Environmental impact
KW  - Research challenges
KW  - Resource provisioning
KW  - Computer systems
KW  - Dynamic consolidation
KW  - Resource providers
KW  - Research
KW  - Virtualizations
KW  - Allocation algorithm
KW  - Architectural frameworks
KW  - Architectural principles
KW  - Business domain
KW  - Client applications
KW  - Computing solutions
KW  - Cost saving
KW  - Electrical energy
KW  - Energy-efficient resource allocation
KW  - High potential
KW  - IT services
KW  - Operational costs
KW  - Pay-as-you-go
KW  - Pervasive applications
KW  - Power usage
ER  - 

TY  - JOUR
TI  - On a test of whether one of two random variables is stochastically larger than the other
AU  - Mann, H.B.
AU  - Whitney, D.R.
T2  - Ann Math Stat
DA  - 1947///
PY  - 1947
VL  - 18
IS  - 1
SP  - 50
EP  - 60
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002322469&partnerID=40&md5=f23859b7c1d2de39e8b4212b062c6aac
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Architecting cloud-enabled systems: a systematic survey of challenges and solutions
AU  - Chauhan, M.A.
AU  - Babar, M.A.
AU  - Benatallah, B.
T2  - Software - Practice and Experience
AB  - The literature on the challenges of and potential solutions to architecting cloud-based systems is rapidly growing but is scattered. It is important to systematically analyze and synthesize the existing research on architecting cloud-based software systems in order to build a cohesive body of knowledge of the reported challenges and solutions. We have systematically identified and reviewed 133 papers that report architecture-related challenges and solutions for cloud-based software systems. This paper reports the methodological details, findings, and implications of a systematic review that has enabled us to identify 44 unique categories of challenges and associated solutions for architecting cloud-based software systems. We assert that the identified challenges and solutions classified into the categories form a body of knowledge that can be leveraged for designing or evaluating software architectures for cloud-based systems. Our key conclusions are that a large number of primary studies focus on middleware services aimed at achieving scalability, performance, response time, and efficient resource optimization. Architecting cloud-based systems presents unique challenges as the systems to be designed range from pervasive embedded systems and enterprise applications to smart devices with Internet of Things. We also conclude that there is a huge potential of research on architecting cloud-based systems in areas related to green computing, energy efficient systems, mobile cloud computing, and Internet of Things. Copyright © 2016 John Wiley & Sons, Ltd. Copyright © 2016 John Wiley & Sons, Ltd.
DA  - 2017///
PY  - 2017
DO  - 10.1002/spe.2409
VL  - 47
IS  - 4
SP  - 599
EP  - 644
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969961838&doi=10.1002%2fspe.2409&partnerID=40&md5=444a5cf7af46567a0eb7e0dbaefe59b2
DB  - Scopus
KW  - cloud computing
KW  - Cloud computing
KW  - Energy efficiency
KW  - software architecture
KW  - systematic literature review
KW  - Software architecture
KW  - Internet of things
KW  - Knowledge management
KW  - Computer architecture
KW  - Distributed computer systems
KW  - Mobile cloud computing
KW  - Middleware
KW  - Systematic literature review
KW  - Systematic Review
KW  - Internet
KW  - Computer software
KW  - Enterprise applications
KW  - Embedded systems
KW  - Resource optimization
KW  - Body of knowledge
KW  - Energy efficient systems
KW  - Evaluating software architectures
KW  - Middleware services
ER  - 

TY  - CONF
TI  - PerOpteryx: Automated application of tactics in multi-objective software architecture optimization
AU  - Koziolek, A.
AU  - Koziolek, H.
AU  - Reussner, R.
T2  - CompArch'11 - Proceedings of the 2011 Federated Events on Component-Based Software Engineering and Software Architecture - QoSA+ISARCS'11
AB  - Designing software architectures that exhibit a good trade-off between multiple quality attributes is hard. Even with a given functional design, many degrees of freedom in the software architecture (e.g. component deployment or server configuration) span a large design space. In current practice, software architects try to find good solutions manually, which is time-consuming, can be error-prone and can lead to suboptimal designs. We propose an automated approach guided by architectural tactics to search the design space for good solutions. Our approach applies multi-objective evolutionary optimization to software architectures modelled with the Palladio Component Model. Software architects can then make well-informed trade-off decisions and choose the best architecture for their situation. To validate our approach, we applied it to the architecture models of two systems, a business reporting system and an industrial control system from ABB. The approach was able to find meaningful trade-offs leading to significant performance improvements or costs savings. The novel use of tactics decreased the time needed to find good solutions by up to 80%. © 2011 ACM.
DA  - 2011///
PY  - 2011
DO  - 10.1145/2000259.2000267
SP  - 33
EP  - 42
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960492145&doi=10.1145%2f2000259.2000267&partnerID=40&md5=54812b352d28340283f65291e844a43b
DB  - Scopus
KW  - performance
KW  - multi-objective optimization
KW  - software architecture
KW  - optimization
KW  - Software architecture
KW  - reliability
KW  - Software architects
KW  - Economic and social effects
KW  - Commerce
KW  - Multi objective
KW  - Multiobjective optimization
KW  - Software reliability
KW  - Automated approach
KW  - Error prones
KW  - Design
KW  - Architecture optimization
KW  - Evolutionary optimizations
KW  - Multiple quality
KW  - architectural tactics
KW  - Architecture models
KW  - Automated applications
KW  - Component deployment
KW  - Component model
KW  - costs
KW  - Design spaces
KW  - Functional design
KW  - Industrial control systems
KW  - Large designs
KW  - Performance improvements
KW  - Reporting systems
KW  - Sub-optimal designs
ER  - 

TY  - CONF
TI  - Multiclass multiservers with deferred operations in layered queueing networks, with software system applications
AU  - Franks, G.
AU  - Woodside, M.
T2  - Proceedings - IEEE Computer Society's Annual International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunications Systems, MASCOTS
AB  - Layered queueing networks describe the simultaneous-resource behaviour of servers that request lower-layer services and wait for them to complete. Layered software systems often follow this model, with messages to request service and receive the results. Their performance has been computed successfully using mean-value queueing approximations. Such systems also have multiservers (which model multi-threaded software processes), multiple classes of service, and what we call deferred operations or "second phases", which are executed after sending the reply message to the requester. In this paper, three established MVA approximations for multiclass multiservers were extended to include deferred service, and evaluated within the layered queueing context. Errors ranged from 1% up to about 15%. These servers were then used to model the Network File System, as implemented on Linux, to show that the method scales up and gives good accuracy on typical systems, with computation times of a few seconds to a few minutes. This is hundreds of times faster than simulation. © 2004 IEEE.
DA  - 2004///
PY  - 2004
SP  - 239
EP  - 245
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-16244417188&partnerID=40&md5=434310e665917ab5616e60cd53fca0e8
DB  - Scopus
KW  - Servers
KW  - Queueing networks
KW  - Computer software
KW  - World Wide Web
KW  - Telecommunication services
KW  - Computer operating systems
KW  - Approximation theory
KW  - Layered queueing networks
KW  - Layered software systems
KW  - Multiservers
KW  - Poisson equation
KW  - Queueing context
ER  - 

TY  - CONF
TI  - Benchmarking microservice systems for software engineering research
AU  - Zhou, X.
AU  - Peng, X.
AU  - Xie, T.
AU  - Sun, J.
AU  - Xu, C.
AU  - Ji, C.
AU  - Zhao, W.
T2  - Proceedings - International Conference on Software Engineering
AB  - Despite the prevalence and importance of microservices in industry, there exists limited research on microservices, partly due to lacking a benchmark system that reflects the characteristics of industrial microservice systems. To fill this gap, we conduct a review of literature and open source systems to identify the gap between existing benchmark systems and industrial microservice systems. Based on the results of the gap analysis, we then develop and release a medium-size benchmark system of microservice architecture. © 2018 Authors.
DA  - 2018///
PY  - 2018
DO  - 10.1145/3183440.3194991
SP  - 323
EP  - 324
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049690447&doi=10.1145%2f3183440.3194991&partnerID=40&md5=1f766836a80991cd78e34ddc73c8c69e
DB  - Scopus
KW  - Microservices
KW  - Microservice
KW  - Benchmark
KW  - Visualization
KW  - Benchmarking
KW  - Benchmark system
KW  - Open source software
KW  - Open systems
KW  - Industrial research
KW  - Tracing
KW  - Computer debugging
KW  - debugging
KW  - Failure diagnosis
KW  - Failure Diagnosis
KW  - Flow visualization
KW  - Gap analysis
KW  - Medium size
KW  - Open source system
KW  - Program diagnostics
ER  - 

TY  - JOUR
TI  - Green cloud computing: Balancing energy in processing, storage, and transport
AU  - Baliga, J.
AU  - Ayre, R.W.A.
AU  - Hinton, K.
AU  - Tucker, R.S.
T2  - Proceedings of the IEEE
AB  - Network-based cloud computing is rapidly expanding as an alternative to conventional office-based computing. As cloud computing becomes more widespread, the energy consumption of the network and computing resources that underpin the cloud will grow. This is happening at a time when there is increasing attention being paid to the need to manage energy consumption across the entire information and communications technology (ICT) sector. While data center energy use has received much attention recently, there has been less attention paid to the energy consumption of the transmission and switching networks that are key to connecting users to the cloud. In this paper, we present an analysis of energy consumption in cloud computing. The analysis considers both public and private clouds, and includes energy consumption in switching and transmission as well as data processing and data storage. We show that energy consumption in transport and switching can be a significant percentage of total energy consumption in cloud computing. Cloud computing can enable more energy-efficient use of computing power, especially when the computing tasks are of low intensity or infrequent. However, under some circumstances cloud computing can consume more energy than conventional computing where each user performs all computing on their own personal computer (PC). © 2010 IEEE.
DA  - 2011///
PY  - 2011
DO  - 10.1109/JPROC.2010.2060451
VL  - 99
IS  - 1
SP  - 149
EP  - 167
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019514248&doi=10.1109%2fJPROC.2010.2060451&partnerID=40&md5=299067989510e27cb90035020153aaeb
DB  - Scopus
KW  - Cloud computing
KW  - Energy efficiency
KW  - Energy consumption
KW  - Data centers
KW  - Green computing
KW  - Energy utilization
KW  - Computing power
KW  - Digital storage
KW  - Computing resource
KW  - Data handling
KW  - Balancing energy
KW  - Core networks
KW  - Personal computers
KW  - Total energy consumption
KW  - Energy efficient use
KW  - Information and communications technology
ER  - 

TY  - CONF
TI  - Comparing the accuracy of resource demand measurement and estimation techniques
AU  - Willnecker, F.
AU  - Dlugi, M.
AU  - Brunnert, A.
AU  - Spinner, S.
AU  - Kounev, S.
AU  - Gottesheim, W.
AU  - Krcmar, H.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Resource demands are a core aspect of performance models. They describe how an operation utilizes a resource and therefore influence the systems performance metrics: response time, resource utilization and throughput. Such demands can be determined by two extraction classes: direct measurement or demand estimation. Selecting the best suited technique depends on available tools, acceptable measurement overhead and the level of granularity necessary for the performance model. This work compares two direct measurement techniques and an adaptive estimation technique based on multiple statistical approaches to evaluate strengths and weaknesses of each technique. We conduct a series of experiments using the SPECjEnterprise2010 industry benchmark and an automatic performance model generator for architecture level performance models based on the Palladio Component Model. To compare the techniques we conduct two experiments with different levels of granularity on a standalone system, followed by one experiment using a distributed SPECjEnterprise2010 deployment combining both extraction classes for generating a full-stack performance model. © Springer International Publishing Switzerland 2015.
DA  - 2015///
PY  - 2015
DO  - 10.1007/978-3-319-23267-6_8
VL  - 9272
SP  - 115
EP  - 129
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944728289&doi=10.1007%2f978-3-319-23267-6_8&partnerID=40&md5=64bde25b287b84a76f9e8f3a6e723fad
DB  - Scopus
KW  - Benchmarking
KW  - Resource utilizations
KW  - Resource demands
KW  - Extraction
KW  - Performance Model
KW  - Statistical approach
KW  - Adaptive estimation techniques
KW  - Estimation techniques
KW  - Performance model generation
KW  - Resource demand estimations
KW  - Resource demand measurements
KW  - Specjenterprise2010
KW  - Systems performance
ER  - 

TY  - JOUR
TI  - Towards energy aware cloud computing application construction
AU  - Armstrong, D.
AU  - Djemame, K.
AU  - Kavanagh, R.
T2  - Journal of Cloud Computing
AB  - The energy consumption of cloud computing continues to be an area of significant concern as data center growth continues to increase. This paper reports on an energy efficient interoperable cloud architecture realised as a cloud toolbox that focuses on reducing the energy consumption of cloud applications holistically across all deployment models. The architecture supports energy efficiency at service construction, deployment and operation. We discuss our practical experience during implementation of an architectural component, the Virtual Machine Image Constructor (VMIC), required to facilitate construction of energy aware cloud applications. We carry out a performance evaluation of the component on a cloud testbed. The results show the performance of Virtual Machine construction, primarily limited by available I/O, to be adequate for agile, energy aware software development. We conclude that the implementation of the VMIC is feasible, incurs minimal performance overhead comparatively to the time taken by other aspects of the cloud application construction life-cycle, and make recommendations on enhancing its performance. © 2017, The Author(s).
DA  - 2017///
PY  - 2017
DO  - 10.1186/s13677-017-0083-2
VL  - 6
IS  - 1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021422050&doi=10.1186%2fs13677-017-0083-2&partnerID=40&md5=aeb11874807a0abce526302a62459200
DB  - Scopus
KW  - Cloud computing
KW  - Energy efficiency
KW  - Power management
KW  - Virtualization
KW  - Software design
KW  - Software engineering
KW  - Network security
KW  - Computer architecture
KW  - Performance evaluation
KW  - Energy utilization
KW  - Virtual machine
KW  - Cloud architectures
KW  - Architectural components
KW  - Cloud engineering
KW  - Cloud engineerings
KW  - Cloud interoperability
KW  - Construction life cycles
KW  - Machine construction
KW  - Practical experience
KW  - Service construction
ER  - 

TY  - JOUR
TI  - Appropriate statistics for ordinal level data: Should we really be using t-test and Cohen's d for evaluating group differences on the NSSE and other surveys?
AU  - Romano, J.
AU  - Kromrey, J.D.
AU  - Coraggio, J.
AU  - Skowronek, J.
T2  - Annual Meeting of the Florida Association of Institutional Research
DA  - 2006///
PY  - 2006
SP  - 1
EP  - 33
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960837892&partnerID=40&md5=97f4aa2643d0ecbcd14a9aad2f79763a
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Cloud computing resource scheduling and a survey of its evolutionary approaches
AU  - Zhan, Z.-H.
AU  - Liu, X.-F.
AU  - Gong, Y.-J.
AU  - Zhang, J.
AU  - Chung, H.S.-H.
AU  - Li, Y.
T2  - ACM Computing Surveys
AB  - A disruptive technology fundamentally transforming the way that computing services are delivered, cloud computing offers information and communication technology users a new dimension of convenience of resources, as services via the Internet. Because cloud provides a finite pool of virtualized on-demand resources, optimally scheduling them has become an essential and rewarding topic, where a trend of using Evolutionary Computation (EC) algorithms is emerging rapidly. Through analyzing the cloud computing architecture, this survey first presents taxonomy at two levels of scheduling cloud resources. It then paints a landscape of the scheduling problem and solutions. According to the taxonomy, a comprehensive survey of state-of-the-art approaches is presented systematically. Looking forward, challenges and potential future research directions are investigated and invited, including real-time scheduling, adaptive dynamic scheduling, large-scale scheduling, multiobjective scheduling, and distributed and parallel scheduling. At the dawn of Industry 4.0, cloud computing scheduling for cyber-physical integration with the presence of big data is also discussed. Research in this area is only in its infancy, but with the rapid fusion of information and data technology, more exciting and agenda-setting topics are likely to emerge on the horizon.
DA  - 2015///
PY  - 2015
DO  - 10.1145/2788397
VL  - 47
IS  - 4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939803867&doi=10.1145%2f2788397&partnerID=40&md5=51dd07826ce6c544fd26df28f90068de
DB  - Scopus
KW  - Cloud computing
KW  - Big data
KW  - Scheduling
KW  - Resource scheduling
KW  - Information and Communication Technologies
KW  - Computer architecture
KW  - Cloud-computing
KW  - Genetic algorithms
KW  - Genetic algorithm
KW  - Particle swarm optimization
KW  - Particle swarm optimization (PSO)
KW  - Resource-scheduling
KW  - Surveys
KW  - Computing resource
KW  - Ant colony optimization
KW  - Computing services
KW  - Particle swarm
KW  - Disruptive technology
KW  - Evolutionary approach
KW  - Evolutionary Computation
KW  - Swarm optimization
ER  - 

TY  - CONF
TI  - Comparison study of SPEA2+, SPEA2, and NSGA-II in diesel engine emissions and fuel economy problem
AU  - Hiroyasu, T.
AU  - Nakayama, S.
AU  - Miki, M.
T2  - 2005 IEEE Congress on Evolutionary Computation, IEEE CEC 2005. Proceedings
AB  - Recently, the technology that can control NOx and Soot values of diesel engines by changing the electronically controllable parameters has been developed. However, there is a trade-off relationship between fuel economy and NOx values. Therefore, the diesel engines that can change their characteristics with along to the driving environment should be emerged in the future. For designing these kinds of engines, the Pareto solutions that can express the trade-off between fuel economy and NOx values are needed. In that case, the derived non dominated solutions should have the diversity not only in the objective space but also in the design variable space. SPEA2+ is one of multi objective genetic algorithms and is developed based on SPEA2. The derived non dominated solutions by SPEA2+ have the diversity in both objective space and design variable space. In this study, the diesel engines that have high fuel economy and small amounts of NOx and Soot are designed by SEPA2+. The results are compared with those of SPEA2 and NSGA-II. From the discussions, it is found that the solutions of SPEA2+ have the diversity not only in the objective space but also in the design variable space. These characteristics are very suitable for designing diesel engines whose parameters are changing against the driving environment. © 2005 IEEE.
DA  - 2005///
PY  - 2005
VL  - 1
SP  - 236
EP  - 242
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-27144546968&partnerID=40&md5=3591cd640cb6ac4b4d69c85d76e5bd57
DB  - Scopus
KW  - Genetic algorithms
KW  - Gas emissions
KW  - Problem solving
KW  - Pareto principle
KW  - Multi-objective genetic algorithm
KW  - Diesel engines
KW  - Fuel economy
KW  - Pareto solution
KW  - Variable space
ER  - 

TY  - JOUR
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201055291&partnerID=40&md5=4dc631039e4fd76e74a0abe06f2bb29b
DB  - Scopus
ER  - 

TY  - CONF
TI  - Carving Sustainability into Architecture Knowledge Practice
AU  - Funke, M.
AU  - Lago, P.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - In the daily work of a software architect, knowledge is ubiquitous. In addition to technical expertise, architecture knowledge (AK) also requires practical experience in the representation, communication, and management of architectural decisions. However, there is a pressing need to also incorporate sustainability aspects, i.e., capturing decisions towards software systems that are environmentally, economically, and socially balanced in the long term. With this study, we aim to provide a review of AK concepts and their representation and communication from a practical point of view. Having this understanding, we explore where sustainability can be applied in daily practice and how we can address sustainability in architecture processes in the future. The paper presents an empirical study conducted in an industrial context encompassing a questionnaire survey with 32 participants and semi-structured interviews with 15 practitioners; both groups are from a major bank in the Netherlands. Based on the insights gained from combining our findings, we (i) provide a map of applied concepts for communicating and representing AK in a large enterprise, and (ii) discuss potential avenues for carving sustainability into current software architecture practice. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
DA  - 2023///
PY  - 2023
DO  - 10.1007/978-3-031-42592-9_4
VL  - 14212 LNCS
SP  - 54
EP  - 69
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172106147&doi=10.1007%2f978-3-031-42592-9_4&partnerID=40&md5=d235c63d9a95293e3bc79852916eb398
DB  - Scopus
KW  - Software architecture
KW  - Decision making
KW  - Sustainable development
KW  - Software architects
KW  - Empirical studies
KW  - Practical experience
KW  - Architectural decision
KW  - Industrial context
KW  - Pressung
KW  - Questionnaire surveys
KW  - Semi structured interviews
KW  - Software-systems
KW  - Technical expertise
ER  - 

TY  - JOUR
TI  - A review on genetic algorithm: past, present, and future
AU  - Katoch, S.
AU  - Chauhan, S.S.
AU  - Kumar, V.
T2  - Multimedia Tools and Applications
AB  - In this paper, the analysis of recent advances in genetic algorithms is discussed. The genetic algorithms of great interest in research community are selected for analysis. This review will help the new and demanding researchers to provide the wider vision of genetic algorithms. The well-known algorithms and their implementation are presented with their pros and cons. The genetic operators and their usages are discussed with the aim of facilitating new researchers. The different research domains involved in genetic algorithms are covered. The future research directions in the area of genetic operators, fitness function and hybrid algorithms are discussed. This structured review will be helpful for research and graduate teaching. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.
DA  - 2021///
PY  - 2021
DO  - 10.1007/s11042-020-10139-6
VL  - 80
IS  - 5
SP  - 8091
EP  - 8126
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094857903&doi=10.1007%2fs11042-020-10139-6&partnerID=40&md5=8c8ff133775d41c1500375a41f4b0ab5
DB  - Scopus
KW  - Optimization
KW  - Genetic algorithms
KW  - Genetic algorithm
KW  - Metaheuristic
KW  - Fitness functions
KW  - Research communities
KW  - Future research directions
KW  - Multimedia systems
KW  - Selection
KW  - Crossover
KW  - Evolution
KW  - Genetic operators
KW  - Graduate teachings
KW  - Hybrid algorithms
KW  - Mutation
KW  - Research domains
KW  - Structured review
ER  - 

TY  - CONF
TI  - A framework for quantified eco-efficiency analysis
AU  - Huppes, G.
AU  - Ishikawa, M.
T2  - Journal of Industrial Ecology
AB  - Eco-efficiency is an instrument for sustainability analysis, indicating an empirical relation in economic activities between environmental cost or value and environmental impact. This empirical relation can be matched against normative considerations as to how much environmental quality or improvement society would like to offer in exchange for economic welfare, or what the trade-off between the economy and the environment should be if society is to realize a certain level of environmental quality. Its relevance lies in the fact that relations between economy and environment are not self-evident, not at a micro level and not at the macro level resulting from micro-level decisions for society as a whole. Clarifying the why and what of eco-efficiency is a first step toward decision support on these two aspects of sustainability. With the main analytic framework established, filling in the actual economic and environmental relations requires further choices in modeling. Also, the integration of different environmental effects into a single score requires a clear definition of approach, because several partly overlapping methods exist. Some scaling problems accompany the specification of numerator and denominator, which need a solution and some standardization before eco-efficiency analysis can become more widely used. With a method established, the final decision is how to embed it in practical decision making. In getting the details of eco-efficiency better specified, its strengths, but also its weaknesses and limitations, need to be indicated more clearly. © 2005 by the Massachusetts Institute of Technology and Yale University.
DA  - 2005///
PY  - 2005
DO  - 10.1162/108819805775247882
VL  - 9
SP  - 25
EP  - 41
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646251728&doi=10.1162%2f108819805775247882&partnerID=40&md5=f2d17e16fa66a3e7b9f7443c70250a71
DB  - Scopus
KW  - sustainability
KW  - Cost effectiveness
KW  - Sustainable development
KW  - environmental impact
KW  - Economic and social effects
KW  - Environmental impact
KW  - Economics
KW  - Industrial engineering
KW  - Ecology
KW  - economic relations
KW  - Environmental cost-effectiveness
KW  - environmental economics
KW  - environmental effect
KW  - Environmental effect score
KW  - Environmental intensity
KW  - Environmental productivity
KW  - environmental quality
KW  - Industrial ecology
KW  - Productivity
KW  - trade-off
KW  - Win-win
ER  - 

TY  - CONF
TI  - Continuous integration of performance model
AU  - Mazkatli, M.
AU  - Koziolek, A.
T2  - ICPE 2018 - Companion of the 2018 ACM/SPEC International Conference on Performance Engineering
AB  - Applying model-based performance prediction requires that an up-to-date Performance Model (PM) is available throughout the development process. Creating such a model manually is an expensive process that is unsuitable for agile software development aiming to produce rapid releases in short cycles. Existing approaches automate the extraction of a PM based on reverse engineering and/or measurements techniques. However, these approaches require to monitor and analyse the whole application. Thus, they are too costly to be applied frequently, up to after each code change. Moreover, keeping potential manual changes of the PM is another challenge as long the PM is regenerated from scratch every time. To address these problems, this paper envisions an approach for efficient continuous integration of a parametrised performance model in an agile development process. Our work will combine static code analysis with adaptive, automatic, dynamic analysis covering updated parts of code to update the PM with parameters, like resource demands and branching probabilities. The benefit of our approach will be to automatically keep the PM up-to-date throughout the development process which enables the proactive identification of upcoming performance problems and provides a foundation for evaluating design alternatives at low costs. © 2018 Association for Computing Machinery.
DA  - 2018///
PY  - 2018
DO  - 10.1145/3185768.3186285
VL  - 2018-January
SP  - 153
EP  - 158
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052026661&doi=10.1145%2f3185768.3186285&partnerID=40&md5=d08ae752ef84d0c0cfc16ed5542e028e
DB  - Scopus
KW  - Software design
KW  - Continuous integrations
KW  - Performance engineering
KW  - Codes (symbols)
KW  - Performance prediction
KW  - Performance management
KW  - Performance problems
KW  - Reverse engineering
KW  - Agile software development
KW  - Branching probability
KW  - Continuous/incremental performance management
KW  - Incremental reverse engineering
KW  - Model-based performance engineering
KW  - Parametric performance
KW  - Parametric performance model
ER  - 

TY  - JOUR
TI  - Assessing ICT global emissions footprint: Trends to 2040 & recommendations
AU  - Belkhir, L.
AU  - Elmeligi, A.
T2  - Journal of Cleaner Production
AB  - In light of the concerted efforts to reduce global greenhouse gas emissions (GHGE) per the so-called Paris Agreement, the Information and Communication Industry (ICT) has received little attention as a significant contributor to GHGE and if anything is often highly praised for enabling efficiencies that help reduce other industry sectors footprint. In this paper, we aim at assessing the global carbon footprint of the overall ICT industry, including the contribution from the main consumer devices, the data centers and communication networks, and compare it with the to the total worldwide GHGE. We conduct a detailed and rigorous analysis of the ICT global carbon footprint, including both the production and the operational energy of ICT devices, as well as the operational energy for the supporting ICT infrastructure. We then compare this contribution to the global 2016-level GHGE. We have found that, if unchecked, ICT GHGE relative contribution could grow from roughly 1–1.6% in 2007 to exceed 14% of the 2016-level worldwide GHGE by 2040, accounting for more than half of the current relative contribution of the whole transportation sector. Our study also highlights the contribution of smart phones and shows that by 2020, the footprint of smart phones alone would surpass the individual contribution of desktops, laptops and displays. Finally, we offer some actionable recommendations on how to mitigate and curb the ICT explosive GHGE footprint, through a combination of renewable energy use, tax policies, managerial actions and alternative business models. © 2018 Elsevier Ltd
DA  - 2018///
PY  - 2018
DO  - 10.1016/j.jclepro.2017.12.239
VL  - 177
SP  - 448
EP  - 463
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041215618&doi=10.1016%2fj.jclepro.2017.12.239&partnerID=40&md5=c91dee0ffd32a303af9af58776c4c37e
DB  - Scopus
KW  - Smartphones
KW  - Carbon footprint
KW  - Greenhouse gases
KW  - Energy utilization
KW  - Information and communication
KW  - Gas emissions
KW  - ICT infrastructures
KW  - Managerial actions
KW  - Operational energy
KW  - Relative contribution
KW  - Renewable energy use
KW  - Rigorous analysis
KW  - Telephone sets
KW  - Transportation sector
ER  - 

TY  - JOUR
TI  - Assessing the suitability of the Greenhouse Gas Protocol for calculation of emissions from public cloud computing workloads
AU  - Mytton, D.
T2  - Journal of Cloud Computing
AB  - Efficiency improvements over the past decade have meant that data center energy usage has decoupled from the growth in IT workloads. Much of this efficiency improvement has been attributed to innovations made by “hyperscale” public cloud vendors, where a large proportion of new IT workloads are now being deployed. However, the move to the cloud is making it more difficult to assess the environmental impact of workloads deployed there. Although the large cloud vendors are amongst the largest purchasers of renewable electricity, customers do not have access to the data they need to complete emissions assessments under the Greenhouse Gas Protocol. Data such as Power Usage Effectiveness, emissions factors and equipment embodied energy are not available from public cloud vendors. This paper demonstrates how the Greenhouse Gas Protocol method of assessment of IT emissions does not work for public cloud environments and suggests how this can be tackled by the cloud vendors themselves. © 2020, The Author(s).
DA  - 2020///
PY  - 2020
DO  - 10.1186/s13677-020-00185-8
VL  - 9
IS  - 1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089199506&doi=10.1186%2fs13677-020-00185-8&partnerID=40&md5=df5dbc3b8be6be2c638643bf9ec5fc74
DB  - Scopus
KW  - Cloud computing
KW  - Data centers
KW  - Greenhouse gases
KW  - Environmental impact
KW  - Gas emissions
KW  - Efficiency
KW  - Public clouds
KW  - Efficiency improvement
KW  - Energy usage
KW  - Cloud environmental impact
KW  - Data center emissions
KW  - Embodied energy
KW  - Emissions factors
KW  - Greenhouse gas Protocol
KW  - Greenhouse gas protocols
KW  - IT emissions
KW  - Renewable electricity
ER  - 

TY  - CONF
TI  - Tulsa: a tool for transforming UML to layered queueing networks for performance analysis of data intensive applications
AU  - Li, C.
AU  - Altamimi, T.
AU  - Zargari, M.H.
AU  - Casale, G.
AU  - Petriu, D.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Motivated by the problem of detecting software performance anti-patterns in data-intensive applications (DIAs), we present a tool, Tulsa, for transforming software architecture models specified through UML into Layered Queueing Networks (LQNs), which are analytical performance models used to capture contention across multiple software layers. In particular, we generalize an existing transformation based on the Epsilon framework to generate LQNs from UML models annotated with the DICE profile, which extends UML to modelling DIAs based on technologies such as Apache Storm. © 2017, Springer International Publishing AG.
DA  - 2017///
PY  - 2017
DO  - 10.1007/978-3-319-66335-7_18
VL  - 10503 LNCS
SP  - 295
EP  - 299
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028647070&doi=10.1007%2f978-3-319-66335-7_18&partnerID=40&md5=a3ad50822e49b3a39a1b0a9d081ab16b
DB  - Scopus
KW  - Metadata
KW  - Software architecture
KW  - Application programs
KW  - Queueing networks
KW  - Software performance
KW  - Performance analysis
KW  - Network layers
KW  - Unified Modeling Language
KW  - Anti-patterns
KW  - Layered queueing networks
KW  - Analytical performance model
KW  - Data-intensive application
KW  - Software architecture model
KW  - Transformation based
ER  - 

TY  - JOUR
TI  - MORE: A multi-objective refactoring recommendation approach to introducing design patterns and fixing code smells
AU  - Ouni, A.
AU  - Kessentini, M.
AU  - Ó Cinnéide, M.
AU  - Sahraoui, H.
AU  - Deb, K.
AU  - Inoue, K.
T2  - Journal of Software: Evolution and Process
AB  - Refactoring is widely recognized as a crucial technique applied when evolving object-oriented software systems. If applied well, refactoring can improve different aspects of software quality including readability, maintainability, and extendibility. However, despite its importance and benefits, recent studies report that automated refactoring tools are underused much of the time by software developers. This paper introduces an automated approach for refactoring recommendation, called MORE, driven by 3 objectives: (1) to improve design quality (as defined by software quality metrics), (2) to fix code smells, and (3) to introduce design patterns. To this end, we adopt the recent nondominated sorting genetic algorithm, NSGA-III, to find the best trade-off between these 3 objectives. We evaluated the efficacy of our approach using a benchmark of 7 medium and large open-source systems, 7 commonly occurring code smells (god class, feature envy, data class, spaghetti code, shotgun surgery, lazy class, and long parameter list), and 4 common design pattern types (visitor, factory method, singleton, and strategy). Our approach is empirically evaluated through a quantitative and qualitative study to compare it against 3 different state-of-the art approaches, 2 popular multiobjective search algorithms, and random search. The statistical analysis of the results confirms the efficacy of our approach in improving the quality of the studied systems while successfully fixing 84% of code smells and introducing an average of 6 design patterns. In addition, the qualitative evaluation shows that most of the suggested refactorings (an average of 69%) are considered by developers to be relevant and meaningful. Copyright © 2017 John Wiley & Sons, Ltd.
DA  - 2017///
PY  - 2017
DO  - 10.1002/smr.1843
VL  - 29
IS  - 5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014656546&doi=10.1002%2fsmr.1843&partnerID=40&md5=9a7e0b9b3103399b1cc81a6c5fedfab1
DB  - Scopus
KW  - Design Patterns
KW  - Economic and social effects
KW  - Genetic algorithms
KW  - refactoring
KW  - Refactorings
KW  - search-based software engineering
KW  - Search-based software engineering
KW  - Quality control
KW  - Open source software
KW  - Open systems
KW  - Codes (symbols)
KW  - Object oriented programming
KW  - Computer software selection and evaluation
KW  - Code smell
KW  - code smells
KW  - design patterns
KW  - Odors
KW  - software quality
KW  - Software Quality
ER  - 

TY  - JOUR
TI  - Estimates of location based on rank tests
AU  - Hodges, J.L.
AU  - Lehmann, E.L.
T2  - Selected Works of EL Lehmann
DA  - 2012///
PY  - 2012
SP  - 287
EP  - 300
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042883472&partnerID=40&md5=ebabf115b3f394c5be08e435b4ae99df
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Enhanced Intel SpeedStep Technology for the Intel Pentium M Processor
DA  - 2004///
PY  - 2004
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746279684&partnerID=40&md5=caf5513fb73ecdf14a9b669e30ddc5b4
DB  - Scopus
ER  - 

TY  - CONF
TI  - The Architecture of Kaligreen V2: A Middleware Aware of Hardware Opportunities to Save Energy
AU  - Valera, H.H.L.
AU  - Dalmau, M.
AU  - Roose, P.
AU  - Herzog, C.
T2  - 2019 6th International Conference on Internet of Things: Systems, Management and Security, IOTSMS 2019
AB  - Nowadays, energy saving in the use of information technologies is a very important issue both from the economic and sustainability point of view. Many scientists investigate methods to save energy at different application levels (cloud: i.e., architectures, grid: i.e., middlewares and frameworks and hardware management: i.e., operating systems) and many of them agree on the strategy of executing programs, processes or virtual machines only using the time and resources that are strictly necessary. For this, it is necessary to plan strategies for deployment and relocation of processes; but always taking into account hardware repercussions and the knowledge of the architecture and applications behavior. On the other hand, it has already been demonstrated that the use of microservices brings numerous advantages in availability and efficiency; but we do not find many jobs that exploit this technique on the energy level. In this article, we present the architecture of a middleware for distributed microservices-based applications, which allows any negotiation-based scheduling algorithm to duplicate or move microservices from one device to another in a non-centralized way for energy savings, taking into account the consumption characteristics of the microservices and the capabilities that the hardware components offer. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/IOTSMS48152.2019.8939237
SP  - 79
EP  - 86
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077969153&doi=10.1109%2fIOTSMS48152.2019.8939237&partnerID=40&md5=7b937f6435a19d89b288227377d059e7
DB  - Scopus
KW  - microservices
KW  - middleware
KW  - energy
KW  - Internet of things
KW  - Application programs
KW  - Application level
KW  - Program processors
KW  - Energy conservation
KW  - consumption
KW  - CPU
KW  - hard disk
KW  - Hard disk storage
KW  - Hardware components
KW  - Hardware management
KW  - Information use
KW  - Middleware
KW  - network
KW  - Network architecture
KW  - Networks (circuits)
KW  - Save energy
KW  - Scheduling algorithms
ER  - 

TY  - JOUR
AU  - Bongers, E.
AU  - Pouwelse, J.
T2  - A Survey of P2P Multidimensional Indexing Structures
DA  - 2015///
PY  - 2015
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009829721&partnerID=40&md5=2af19d476143121dc7585daaf730226d
DB  - Scopus
ER  - 

TY  - CONF
TI  - Adaptive power panel of cloud computing controlling cloud power consumption
AU  - Azmy, N.M.
AU  - El-Maddah, I.A.M.
AU  - Mohamed, H.K.
T2  - ACM International Conference Proceeding Series
AB  - Cloud computing had created a new era of network design, where end-users can get their required services without having to purchase expensive infrastructure or even to care about troubleshooting. Power consumption is a challenge facing the Cloud Providers to operate their Datacenters. One solution to overcome this is the Virtual Machine (VM) migration, which is a technique used to switch under-utilized hosts to sleep mode in order to save power, and to avoid over-utilized hosts from Service Level Agreement (SLA) violation. But still the problem is that the Cloud Service Provider apply a single policy on all nodes. Our proposed solution is an adaptive power panel where different policies can be applied based on both of the nature of the tasks running on hosts, and the Cloud Provider decision. © 2016 ACM.
DA  - 2016///
PY  - 2016
DO  - 10.1145/2944165.2944167
VL  - 28-29-May-2016
SP  - 9
EP  - 14
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985990751&doi=10.1145%2f2944165.2944167&partnerID=40&md5=c5b64478913d045dc22f54d9b7cff5fa
DB  - Scopus
KW  - Cloud computing
KW  - Placement
KW  - Software engineering
KW  - Green Computing
KW  - Green computing
KW  - Distributed computer systems
KW  - Virtual machines
KW  - Electric power utilization
KW  - Migration
KW  - Virtual Machine
KW  - Adaptive
KW  - Allocation
KW  - Cloudsim
KW  - Selection
ER  - 

TY  - CONF
TI  - Kali green: A distributed scheduler for energy saving
AU  - Lvarez-Valera, H.H.
AU  - Roose, P.
AU  - Dalmau, M.
AU  - Herzog, C.
AU  - Respicio, K.
T2  - Procedia Computer Science
AB  - A commonplace issue with portable technology is battery efficiency. While many industries are trying their best to improve battery life without sacrificing a products quality and efficiency, we believe that further can be done to improve battery consumption on ones mobile devicefrom tablets to smartphones to laptops to everything else. Many applications on these devices are based on a microservice architecture. In this article, we introduce a new algorithm KaliGreen that can maneuver the microservices within a network of devices in order to maximize the run-time of a microservice-based application; moreover, KaliGreen allows a 54% increase in the average run-time of an application by shifting microservices from 6 devices (as example) with low battery or inefficient processing ratios to devices in better conditions. To achieve this, KaliGreen utilizes KaliMucho middleware, which is able manipulate microservices in run-time. This algorithm provides a plausible solution to maximizing energy consumption within a network of devices. © 2018 The Authors. Published by Elsevier Ltd.
DA  - 2018///
PY  - 2018
DO  - 10.1016/j.procs.2018.10.172
VL  - 141
SP  - 223
EP  - 230
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058349351&doi=10.1016%2fj.procs.2018.10.172&partnerID=40&md5=762a84024c90ed200594143af3fc90ed
DB  - Scopus
KW  - Microservices
KW  - Smartphones
KW  - Green Computing
KW  - Green computing
KW  - Energy utilization
KW  - Energy conservation
KW  - mHealth
KW  - Middleware
KW  - Battery consumption
KW  - Battery efficiencies
KW  - Battery life
KW  - Distributed applicatioggns
KW  - Distributed schedulers
KW  - Electric batteries
KW  - Portable technologies
KW  - Products quality
ER  - 

TY  - JOUR
DA  - 2018///
PY  - 2018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136077897&partnerID=40&md5=6f8b6fa55ae8d49e7d0f78e4572b6a90
DB  - Scopus
ER  - 

TY  - JOUR
AU  - Services, A.W.
T2  - Implementing Microservices on Aws
DA  - 2019///
PY  - 2019
VL  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136057210&partnerID=40&md5=de73f910ba756e078d87fc9e1f204cc3
DB  - Scopus
ER  - 

TY  - JOUR
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135986128&partnerID=40&md5=eebf7fe0a0d0c66b94426e2f44020f52
DB  - Scopus
ER  - 

TY  - JOUR
DA  - 2016///
PY  - 2016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136055906&partnerID=40&md5=2d8ee1e2e32a32ca49e573cdc786c5a6
DB  - Scopus
ER  - 

TY  - CONF
TI  - Kalimucho: Middleware for mobile applications
AU  - Da, K.
AU  - Dalmau, M.
AU  - Roose, P.
T2  - Proceedings of the ACM Symposium on Applied Computing
AB  - Developing ubiquitous applications is particularly complex. Beyond the dynamic aspect of such applications, the evolution of computing towards the multiplication of mobile access terminals is not making things easier. One solution to simplifying the development and use of such applications is to use software platforms dedicated to deployment and adaptation of applications and handling the heterogeneity of peripherals. They allow designers to focus on business aspects and facilitate reuse. The Kalimucho platform was designed and developed against this background. It executes and supervises applications based on software components. Copyright 2014 ACM.
DA  - 2014///
PY  - 2014
DO  - 10.1145/2554850.2554883
SP  - 413
EP  - 419
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905652787&doi=10.1145%2f2554850.2554883&partnerID=40&md5=c88b567dba93ee6384b57b3bbd3ba1ee
DB  - Scopus
KW  - Performance
KW  - Reliability
KW  - Application programs
KW  - Measurement
KW  - Middleware
KW  - Mobile applications
KW  - Design
KW  - Measurements
KW  - Software component
KW  - Query languages
KW  - Business aspects
KW  - Dynamic aspects
KW  - Languages
KW  - Mobile access
KW  - Software platforms
KW  - Ubiquitous application
ER  - 

TY  - CONF
TI  - An Open-Source Benchmark Suite for Microservices and Their Hardware-Software Implications for Cloud & Edge Systems
AU  - Gan, Y.
AU  - Zhang, Y.
AU  - Cheng, D.
AU  - Shetty, A.
AU  - Rathi, P.
AU  - Katarki, N.
AU  - Bruno, A.
AU  - Hu, J.
AU  - Ritchken, B.
AU  - Jackson, B.
AU  - Hu, K.
AU  - Pancholi, M.
AU  - He, Y.
AU  - Clancy, B.
AU  - Colen, C.
AU  - Wen, F.
AU  - Leung, C.
AU  - Wang, S.
AU  - Zaruvinsky, L.
AU  - Espinosa, M.
AU  - Lin, R.
AU  - Liu, Z.
AU  - Padilla, J.
AU  - Delimitrou, C.
T2  - International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS
AB  - Cloud services have recently started undergoing a major shift from monolithic applications, to graphs of hundreds of loosely-coupled microservices. Microservices fundamentally change a lot of assumptions current cloud systems are designed with, and present both opportunities and challenges when optimizing for quality of service (QoS) and utilization. In this paper we explore the implications microservices have across the cloud system stack. We first present Death- StarBench, a novel, open-source benchmark suite built with microservices that is representative of large end-to-end services, modular and extensible. DeathStarBench includes a social network, a media service, an e-commerce site, a banking system, and IoT applications for coordination control of UAV swarms. We then use DeathStarBench to study the architectural characteristics of microservices, their implications in networking and operating systems, their challenges with respect to cluster management, and their trade-offs in terms of application design and programming frameworks. Finally, we explore the tail at scale effects of microservices in real deployments with hundreds of users, and highlight the increased pressure they put on performance predictability. © 2019 Association for Computing Machinery.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3297858.3304013
SP  - 3
EP  - 18
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064688619&doi=10.1145%2f3297858.3304013&partnerID=40&md5=b9f662fba2a35d83eb40d32009cf7676
DB  - Scopus
KW  - cloud computing
KW  - microservices
KW  - datacenters
KW  - serverless
KW  - Serverless
KW  - Microservice
KW  - QoS
KW  - fpga
KW  - Quality of service
KW  - Cloud-computing
KW  - Datacenter
KW  - Economic and social effects
KW  - Quality-of-service
KW  - acceleration
KW  - Cluster computing
KW  - Open source software
KW  - Open systems
KW  - Benchmark suites
KW  - Cloud systems
KW  - cluster management
KW  - Cluster management
KW  - Field programmable gate arrays (FPGA)
KW  - Fpgum
KW  - Open-source
ER  - 

TY  - JOUR
TI  - Swayam: Distributed Autoscaling to Meet SLAs of Machine Learning Inference Services with Resource Efficiency
AU  - Gujarati, A.
AU  - Elnikety, S.
AU  - He, Y.
AU  - McKinley, K. S.
AU  - Brandenburg, B. B.
T2  - USENIX Middleware Conference
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095484895&partnerID=40&md5=220c1fa5fda624b6df2f84c7fcf6f4e0
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Provisioned Concurrency.
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119298797&partnerID=40&md5=dbaf19089c1c29634d0a5f1f317f1cba
DB  - Scopus
ER  - 

TY  - CONF
TI  - Agile cold starts for scalable serverless
AU  - Mohan, A.
AU  - Sane, H.
AU  - Doshi, K.
AU  - Edupuganti, S.
AU  - Nayak, N.
AU  - Sukhomlinov, V.
T2  - 11th USENIX Workshop on Hot Topics in Cloud Computing, HotCloud 2019, co-located with USENIX ATC 2019
AB  - The Serverless or Function-as-a-Service (FaaS) model capitalizes on lightweight execution by packaging code and dependencies together for just-in-time dispatch. Often a container environment has to be set up afresh– a condition called “cold start", and in such cases, performance suffers and overheads mount, both deteriorating rapidly under high concurrency. Caching and reusing previously employed containers ties up memory and risks information leakage. Latency for cold starts is frequently due to work and wait-times in setting up various dependencies – such as in initializing networking elements. This paper proposes a solution that pre-crafts such resources and then dynamically reassociates them with baseline containers. Applied to networking, this approach demonstrates an order of magnitude gain in cold starts, negligible memory consumption, and flat startup time under rising concurrency. © 2019 USENIX Association. All rights reserved.
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084160535&partnerID=40&md5=78d805902ca3dc9483d26084f74a76cf
DB  - Scopus
KW  - Cloud computing
KW  - Containers
KW  - Just in time
KW  - Startup time
KW  - Information leakage
KW  - Cold start
KW  - High concurrencies
KW  - Memory consumption
KW  - Wait time
ER  - 

TY  - CONF
TI  - Sequoia: Enabling quality-of-service in serverless computing
AU  - Tariq, A.
AU  - Pahl, A.
AU  - Nimmagadda, S.
AU  - Rozner, E.
AU  - Lanka, S.
T2  - SoCC 2020 - Proceedings of the 2020 ACM Symposium on Cloud Computing
AB  - Serverless computing is a rapidly growing paradigm that easily harnesses the power of the cloud. With serverless computing, developers simply provide an event-driven function to cloud providers, and the provider seamlessly scales function invocations to meet demands as event-triggers occur. As current and future serverless offerings support a wide variety of serverless applications, effective techniques to manage serverless workloads becomes an important issue. This work examines current management and scheduling practices in cloud providers, uncovering many issues including inflated application run times, function drops, inefficient allocations, and other undocumented and unexpected behavior. To fix these issues, a new quality-of-service function scheduling and allocation framework, called Sequoia, is designed. Sequoia allows developers or administrators to easily def ne how serverless functions and applications should be deployed, capped, prioritized, or altered based on easily configured, flexible policies. Results with controlled and realistic workloads show Sequoia seamlessly adapts to policies, eliminates mid-chain drops, reduces queuing times by up to 6.4X, enforces tight chain-level fairness, and improves run-time performance up to 25X.  © 2020 ACM.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3419111.3421306
SP  - 311
EP  - 327
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095439090&doi=10.1145%2f3419111.3421306&partnerID=40&md5=04b3111d11f1056b8d800b2feb8089c4
DB  - Scopus
KW  - Cloud computing
KW  - quality-of-service
KW  - serverless computing
KW  - Scheduling
KW  - Quality of service
KW  - Cloud providers
KW  - Run-time performance
KW  - Drops
KW  - Event-driven
KW  - measurement
KW  - Event trigger
ER  - 

TY  - CONF
TI  - PowerChief: Intelligent power allocation for multi-stage applications to improve responsiveness on power constrained CMP
AU  - Yang, H.
AU  - Chen, Q.
AU  - Riaz, M.
AU  - Luan, Z.
AU  - Tang, L.
AU  - Mars, J.
T2  - Proceedings - International Symposium on Computer Architecture
AB  - Modern user facing applications consist of multiple processing stages with a number of service instances in each stage. The latency profle of these multi-stage applications is intrinsically variable, making it challenging to provide satisfactory responsiveness. Given a limited power budget, improving the end-to-end latency requires intelligently boosting the bottleneck service across stages using multiple boosting techniques. However, prior work fail to acknowledge the multi-stage nature of user-facing applications and perform poorly in improving responsiveness on power constrained CMP, as they are unable to accurately identify bottleneck service and apply the boosting techniques adaptively. In this paper, we present PowerChief, a runtime framework that 1) provides joint design of service and query to monitor the latency statistics across service stages and accurately identifes the bottleneck service during runtime; 2) adaptively chooses the boosting technique to accelerate the bottleneck service with improved responsiveness; 3) dynamically reallocates the constrained power budget across service stages to accommodate the chosen boosting technique. Evaluated with real world multi-stage applications, PowerChief improves the average latency by 20.3× and 32.4× (99% tail latency by 13.3× and 19.4×) for Sirius and Natural Language Processing applications respectively compared to stage-agnostic power allocation. In addition, for the given QoS target, PowerChief reduces the power consumption of Sirius and Web Search applications by 23% and 33% respectively over prior work. © 2017 Association for Computing Machinery.
DA  - 2017///
PY  - 2017
DO  - 10.1145/3079856.3080224
VL  - Part F128643
SP  - 133
EP  - 146
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025619452&doi=10.1145%2f3079856.3080224&partnerID=40&md5=1b3762d3742b803bb5b2ea594b587a90
DB  - Scopus
KW  - Computer architecture
KW  - Quality of service
KW  - Budget control
KW  - Power allocations
KW  - End to end latencies
KW  - Facings
KW  - Intelligent Service Boosting
KW  - Intelligent Services
KW  - Multi stage
KW  - Multi-Stage Application
KW  - Multiple processing
KW  - Natural language processing systems
KW  - Number of services
KW  - Power Constrained CMP
KW  - Runtime frameworks
ER  - 

TY  - JOUR
T2  - Twitter Stream Traces.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119276526&partnerID=40&md5=203329d27a78f1756272a26527d66aba
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Wikipedia workload analysis for decentralized hosting
AU  - Urdaneta, G.
AU  - Pierre, G.
AU  - van Steen, M.
T2  - Computer Networks
AB  - We study an access trace containing a sample of Wikipedia's traffic over a 107-day period aiming to identify appropriate replication and distribution strategies in a fully decentralized hosting environment. We perform a global analysis of the whole trace, and a detailed analysis of the requests directed to the English edition of Wikipedia. In our study, we classify client requests and examine aspects such as the number of read and save operations, significant load variations and requests for nonexisting pages. We also review proposed decentralized wiki architectures and discuss how they would handle Wikipedia's workload. We conclude that decentralized architectures must focus on applying techniques to efficiently handle read operations while maintaining consistency and dealing with typical issues on decentralized systems such as churn, unbalanced loads and malicious participating nodes. © 2009 Elsevier B.V. All rights reserved.
DA  - 2009///
PY  - 2009
DO  - 10.1016/j.comnet.2009.02.019
VL  - 53
IS  - 11
SP  - 1830
EP  - 1845
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-67349226826&doi=10.1016%2fj.comnet.2009.02.019&partnerID=40&md5=b5fc02e0d8213494d9a714f5be814510
DB  - Scopus
KW  - P2P
KW  - Internet
KW  - Websites
KW  - Distribution strategies
KW  - Decentralized architecture
KW  - Decentralized hosting
KW  - Decentralized system
KW  - Global analysis
KW  - Unbalanced loads
KW  - Wikipedia
KW  - Workload analysis
ER  - 

TY  - CONF
TI  - Patterns for serverless functions (Function-as-a-Service): A multivocal literature review
AU  - Taibi, D.
AU  - El Ioini, N.
AU  - Pahl, C.
AU  - Niederkofler, J.R.S.
T2  - CLOSER 2020 - Proceedings of the 10th International Conference on Cloud Computing and Services Science
AB  - [Context] Serverless is a recent technology that enables companies to reduce the overhead for provisioning, scaling and in general managing the infrastructure. Companies are increasingly adopting Serverless, by migrating existing applications to this new paradigm. Different practitioners proposed patterns for composing and managing serverless functions. However, some of these patterns offer different solutions to solve the same problem, which makes it hard to select the most suitable solution for each problem. [Goal] In this work, we aim at supporting practitioners in understanding the different patterns, by classifying them and reporting possible benefits and issues. [Method]We adopted a multivocal literature review process, surveying peer-reviewed and grey literature and classifying patterns (common solutions to solve common problems), together with benefits and issues. [Results] Among 24 selected works, we identified 32 patterns that we classified as orchestration, aggregation, event-management, availability, communication, and authorization. [Conclusion] Practitioners proposed a list of fairly consistent patterns, even if a small number of patterns proposed different solutions to similar problems. Some patterns emerged to circumvent some serverless limitations, while others for some classical technical problems (e.g. publisher/subscriber). © Copyright 2020 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.
DA  - 2020///
PY  - 2020
SP  - 181
EP  - 192
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088373702&partnerID=40&md5=9c4430d886b446d7ae8f3ecc8b5c3fcc
DB  - Scopus
KW  - Cloud computing
KW  - Serverless
KW  - Cloud
KW  - Computer science
KW  - Grey literature
KW  - Literature reviews
KW  - Computer programming
KW  - Suitable solutions
KW  - Function as a Service
KW  - Event management
KW  - Publisher/subscriber
KW  - Serverless Functions
ER  - 

TY  - JOUR
T2  - Airbnb AWS Case Study.
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119272406&partnerID=40&md5=b2c15946f9d4ab9a841339b5693a0926
DB  - Scopus
ER  - 

TY  - CONF
TI  - Serverless in the wild: Characterizing and optimizing the serverless workload at a large cloud provider
AU  - Shahrad, M.
AU  - Fonseca, R.
AU  - Goiri, Í.
AU  - Chaudhry, G.
AU  - Batum, P.
AU  - Cooke, J.
AU  - Laureano, E.
AU  - Tresness, C.
AU  - Russinovich, M.
AU  - Bianchini, R.
T2  - Proceedings of the 2020 USENIX Annual Technical Conference, ATC 2020
AB  - Function as a Service (FaaS) has been gaining popularity as a way to deploy computations to serverless backends in the cloud. This paradigm shifts the complexity of allocating and provisioning resources to the cloud provider, which has to provide the illusion of always-available resources (i.e., fast function invocations without cold starts) at the lowest possible resource cost. Doing so requires the provider to deeply understand the characteristics of the FaaS workload. Unfortunately, there has been little to no public information on these characteristics. Thus, in this paper, we first characterize the entire production FaaS workload of Azure Functions. We show for example that most functions are invoked very infrequently, but there is an 8-order-of-magnitude range of invocation frequencies. Using observations from our characterization, we then propose a practical resource management policy that significantly reduces the number of function cold starts, while spending fewer resources than state-of-the-practice policies. Copyright © Proc. of the 2020 USENIX Annual Technical Conference, ATC 2020. All rights reserved.
DA  - 2020///
PY  - 2020
SP  - 205
EP  - 218
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091934381&partnerID=40&md5=16dd6343f7a2bb6ef374f99e1bb924da
DB  - Scopus
KW  - Cloud providers
KW  - Resource management policy
KW  - Cold start
KW  - Paradigm shifts
KW  - Public information
KW  - Resource costs
KW  - State of the practice
ER  - 

TY  - CONF
TI  - Prebaking functions to warm the serverless cold start
AU  - Silva, P.
AU  - Fireman, D.
AU  - Pereira, T.E.
T2  - Middleware 2020 - Proceedings of the 2020 21st International Middleware Conference
AB  - Function-as-service (FaaS) platforms promise a simpler programming model for cloud computing, in which the developers concentrate on writing its applications. In contrast, platform providers take care of resource management and administration. As FaaS users are billed based on the execution of the functions, platform providers have a natural incentive not to keep idle resources running at the platform's expense. However, this strategy may lead to the cold start issue, in which the execution of a function is delayed because there is no ready resource to host the execution. Cold starts can take hundreds of milliseconds to seconds and have been a prohibitive and painful disadvantage for some applications. This work describes and evaluates a technique to start functions, which restores snapshots from previously executed function processes. We developed a prototype of this technique based on the CRIU process checkpoint/restore Linux tool. We evaluate this prototype by running experiments that compare its start-up time against the standard Unix process creation/start-up procedure. We analyze the following three functions: i) a "do-nothing" function, ii) an Image Resizer function, and iii) a function that renders Markdown files. The results attained indicate that the technique can improve the start-up time of function replicas by 40% (in the worst case of a "do-nothing" function) and up to 71% for the Image Resizer one. Further analysis indicates that the runtime initialization is a key factor, and we confirmed it by performing a sensitivity analysis based on synthetically generated functions of different code sizes. These experiments demonstrate that it is critical to decide when to create a snapshot of a function. When one creates the snapshots of warm functions, the speed-up achieved by the prebaking technique is even higher: the speed-up increases from 127.45% to 403.96%, for a small, synthetic function; and for a bigger, synthetic function, this ratio increases from 121.07% to 1932.49%. © 2020 Association for Computing Machinery.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3423211.3425682
SP  - 1
EP  - 13
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098482026&doi=10.1145%2f3423211.3425682&partnerID=40&md5=a504f228433628dd70e0d3a74365fdba
DB  - Scopus
KW  - Resource management
KW  - Serverless
KW  - Cloud
KW  - Performance evaluation
KW  - Sensitivity analysis
KW  - Middleware
KW  - Startup time
KW  - Faas
KW  - Computer operating systems
KW  - Programming models
KW  - Image enhancement
KW  - Cold start
KW  - Generated function
KW  - ITS applications
KW  - Key factors
KW  - Process creation
ER  - 

TY  - JOUR
TI  - Prediction of user's web-browsing behavior: Application of markov model
AU  - Awad, M.A.
AU  - Khalil, I.
T2  - IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics
AB  - Web prediction is a classification problem in which we attempt to predict the next set of Web pages that a user may visit based on the knowledge of the previously visited pages. Predicting user's behavior while serving the Internet can be applied effectively in various critical applications. Such application has traditional tradeoffs between modeling complexity and prediction accuracy. In this paper, we analyze and study Markov model and all- K th Markov model in Web prediction. We propose a new modified Markov model to alleviate the issue of scalability in the number of paths. In addition, we present a new two-tier prediction framework that creates an example classifier EC, based on the training examples and the generated classifiers. We show that such framework can improve the prediction time without compromising prediction accuracy. We have used standard benchmark data sets to analyze, compare, and demonstrate the effectiveness of our techniques using variations of Markov models and association rule mining. Our experiments show the effectiveness of our modified Markov model in reducing the number of paths without compromising accuracy. Additionally, the results support our analysis conclusions that accuracy improves with higher orders of all- K th model. © 2012 IEEE.
DA  - 2012///
PY  - 2012
DO  - 10.1109/TSMCB.2012.2187441
VL  - 42
IS  - 4
SP  - 1131
EP  - 1142
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864147184&doi=10.1109%2fTSMCB.2012.2187441&partnerID=40&md5=bb189370cf038d274340441c686eaec4
DB  - Scopus
KW  - Forecasting
KW  - Markov processes
KW  - Behavioral research
KW  - Critical applications
KW  - Prediction accuracy
KW  - Markov model
KW  - All-K th Markov
KW  - association rule mining (ARM)
KW  - Benchmark data
KW  - Modeling complexity
KW  - N-gram
KW  - Training example
KW  - two-tier architecture
KW  - Two-tier architecture
KW  - Web prediction
ER  - 

TY  - CONF
TI  - Spock: Exploiting serverless functions for SLO and cost aware resource procurement in public cloud
AU  - Gunasekaran, J.R.
AU  - Thinakaran, P.
AU  - Kandemir, M.T.
AU  - Urgaonkar, B.
AU  - Kesidis, G.
AU  - Das, C.
T2  - IEEE International Conference on Cloud Computing, CLOUD
AB  - We are witnessing the emergence of elastic web services which are hosted in public cloud infrastructures. For reasons of cost-effectiveness, it is crucial for the elasticity of these web services to match the dynamically-evolving user demand. Traditional approaches employ clusters of virtual machines (VMs) to dynamically scale resources based on application demand. However, they still face challenges such as higher cost due to over-provisioning or incur service level objective (SLO) violations due to under-provisioning. Motivated by this observation, we propose Spock, a new scalable and elastic control system that exploits both VMs and serverless functions to reduce cost and ensure SLO for elastic web services. We show that under two different scaling policies, Spock reduces SLO violations of queries by up to 74\% when compared to VM-based resource procurement schemes. Further, Spock yields significant cost savings, by up to 33\% compared to traditional approaches which use only VMs. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/CLOUD.2019.00043
VL  - 2019-July
SP  - 199
EP  - 208
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069531622&doi=10.1109%2fCLOUD.2019.00043&partnerID=40&md5=db3951b69899a434967f97105d5a7d07
DB  - Scopus
KW  - Cloud computing
KW  - FaaS
KW  - Cost effectiveness
KW  - Serverless
KW  - Costs
KW  - Autoscaling
KW  - Web services
KW  - Websites
KW  - Cost-aware
KW  - Lambda
KW  - SLO
ER  - 

TY  - JOUR
T2  - Amazon States Language.
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119282135&partnerID=40&md5=10e725392a15d10448c1f8d29bb49d8c
DB  - Scopus
ER  - 

TY  - JOUR
TI  - On prediction using variable order Markov models
AU  - Begleiter, R.
AU  - El-Yaniv, R.
AU  - Yona, G.
T2  - Journal of Artificial Intelligence Research
AB  - This paper is concerned with algorithms for prediction of discrete sequences over a finite alphabet, using variable order Markov models. The class of such algorithms is large and in principle includes any lossless compression algorithm. We focus on six prominent prediction algorithms, including Context Tree Weighting (CTW), Prediction by Partial Match (PPM) and Probabilistic Suffix Trees (PSTs). We discuss the properties of these algorithms and compare their performance using real life sequences from three domains: proteins, English text and music pieces. The comparison is made with respect to prediction quality as measured by the average log-loss. We also compare classification algorithms based on these predictors with respect to a number of large protein classification tasks. Our results indicate that a "decomposed" CTW (a variant of the CTW algorithm) and PPM outperform all other algorithms in sequence prediction tasks. Somewhat surprisingly, a different algorithm, which is a modification of the Lempel-Ziv compression algorithm, significantly outperforms all algorithms on the protein classification problems. © 2004 AI Access Foundation. All rights reserved.
DA  - 2004///
PY  - 2004
DO  - 10.1613/jair.1491
VL  - 22
SP  - 385
EP  - 421
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-27344458404&doi=10.1613%2fjair.1491&partnerID=40&md5=d197c0fdad570b2edf2397794023188e
DB  - Scopus
KW  - Markov processes
KW  - Classification (of information)
KW  - Algorithms
KW  - Problem solving
KW  - Trees (mathematics)
KW  - Context Tree Weighting (CTW)
KW  - Data compression
KW  - Prediction by Partial Match (PPM)
KW  - Probabilistic Suffix Trees (PSTs)
KW  - Protein classification problems
KW  - Proteins
ER  - 

TY  - CONF
TI  - In search of a fast and efficient serverless DAG engine
AU  - Carver, B.
AU  - Zhang, J.
AU  - Wang, A.
AU  - Cheng, Y.
T2  - Proceedings of PDSW 2019: IEEE/ACM 4th International Parallel Data Systems Workshop - Held in conjunction with SC 2019: The International Conference for High Performance Computing, Networking, Storage and Analysis
AB  - Python-written data analytics applications can be modeled as and compiled into a directed acyclic graph (DAG) based workflow, where the nodes are fine-grained tasks and the edges are task dependencies.Such analytics workflow jobs are increasingly characterized by short, fine-grained tasks with large fan-outs. These characteristics make them well-suited for a new cloud computing model called serverless computing or Function-as-a-Service (FaaS), which has become prevalent in recent years. The auto-scaling property of serverless computing platforms accommodates short tasks and bursty workloads, while the pay-per-use billing model of serverless computing providers keeps the cost of short tasks low. In this paper, we thoroughly investigate the problem space of DAG scheduling in serverless computing. We identify and evaluate a set of techniques to make DAG schedulers serverless-aware. These techniques have been implemented in WUKONG, a serverless, DAG scheduler attuned to AWS Lambda. WUKONG provides decentralized scheduling through a combination of static and dynamic scheduling. We present the results of an empirical study in which WUKONG is applied to a range of microbenchmark and real-world DAG applications. Results demonstrate the efficacy of WUKONG in minimizing the performance overhead introduced by AWS Lambda-WUKONG achieves competitive performance compared to a serverful DAG scheduler, while improving the performance of real-world DAG jobs by as much as 4.1x at larger scale. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/PDSW49588.2019.00005
SP  - 1
EP  - 10
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078949158&doi=10.1109%2fPDSW49588.2019.00005&partnerID=40&md5=1161fc1d68b475b3cc851d5b17b805bc
DB  - Scopus
KW  - Scheduling
KW  - Digital storage
KW  - Empirical studies
KW  - Directed graphs
KW  - Directed acyclic graph (DAG)
KW  - Competitive performance
KW  - Data Analytics
KW  - Computing platform
KW  - Bursty workloads
KW  - Decentralized scheduling
KW  - Scaling properties
KW  - Static and dynamic scheduling
ER  - 

TY  - JOUR
T2  - Microsoft Azure Serverless Functions.
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119253679&partnerID=40&md5=5485c727227fed8133b9e069656d8d16
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Firecracker: Lightweight Virtualization for Serverless Applications
AU  - Brooker, M.
AU  - Florescu, A.
AU  - Popa, D.-M.
AU  - Neugebauer, R.
AU  - Agache, A.
AU  - Iordache, A.
AU  - Liguori, A.
AU  - Piwonka, P.
T2  - NSDI
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095497238&partnerID=40&md5=acdd7c262baa0e03669c6c09ec95784f
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Step functions
AU  - Prasad Buddha, J.
AU  - Beesetty, R.
T2  - The Definitive Guide to AWS Application Integration.
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119251379&partnerID=40&md5=a98c3777553fa985b616acf66c99b03d
DB  - Scopus
ER  - 

TY  - CONF
TI  - Lessons learned from the chameleon testbed
AU  - Keahey, K.
AU  - Anderson, J.
AU  - Zhen, Z.
AU  - Riteau, P.
AU  - Ruth, P.
AU  - Stanzione, D.
AU  - Cevik, M.
AU  - Colleran, J.
AU  - Gunawi, H.S.
AU  - Hammock, C.
AU  - Mambretti, J.
AU  - Barnes, A.
AU  - Halbach, F.
AU  - Rocha, A.
AU  - Stubbs, J.
T2  - Proceedings of the 2020 USENIX Annual Technical Conference, ATC 2020
AB  - The Chameleon testbed is a case study in adapting the cloud paradigm for computer science research. In this paper, we explain how this adaptation was achieved, evaluate it from the perspective of supporting the most experiments for the most users, and make a case that utilizing mainstream technology in research testbeds can increase efficiency without compromising on functionality. We also highlight the opportunity inherent in the shared digital artifacts generated by testbeds and give an overview of the efforts we've made to develop it to foster reproducibility. Copyright © Proc. of the 2020 USENIX Annual Technical Conference, ATC 2020. All rights reserved.
DA  - 2020///
PY  - 2020
SP  - 219
EP  - 233
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089064296&partnerID=40&md5=8eb1b29a826fb2b99aa478bc52588580
DB  - Scopus
KW  - Testbeds
KW  - Reproducibilities
KW  - Computer science research
KW  - Digital artifacts
ER  - 

TY  - CONF
TI  - SEUSS: Skip redundant paths to make serverless fast
AU  - Cadden, J.
AU  - Unger, T.
AU  - Awad, Y.
AU  - Dong, H.
AU  - Krieger, O.
AU  - Appavoo, J.
T2  - Proceedings of the 15th European Conference on Computer Systems, EuroSys 2020
AB  - This paper presents a system-level method for achieving the rapid deployment and high-density caching of serverless functions in a FaaS environment. For reduced start times, functions are deployed from unikernel snapshots, bypassing expensive initialization steps. To reduce the memory footprint of snapshots we apply page-level sharing across the entire software stack that is required to run a function. We demonstrate the effects of our techniques by replacing Linux on the compute node of a FaaS platform architecture. With our prototype OS, the deployment time of a function drops from 100s of milliseconds to under 10 ms. Platform throughput improves by 51x on workload composed entirely of new functions. We are able to cache over 50,000 function instances in memory as opposed to 3,000 using standard OS techniques. In combination, these improvements give the FaaS platform a new ability to handle large-scale bursts of requests. © 2020 ACM.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3342195.3392698
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087107959&doi=10.1145%2f3342195.3392698&partnerID=40&md5=7c393950590d0b7cd76c31891e4148d7
DB  - Scopus
KW  - Platform architecture
KW  - Cache memory
KW  - Computer operating systems
KW  - New functions
KW  - Deployment time
KW  - Initialization step
KW  - Memory footprint
KW  - Rapid deployments
KW  - Redundant paths
KW  - Software stacks
ER  - 

TY  - CONF
TI  - Kube-Knots: Resource Harvesting through Dynamic Container Orchestration in GPU-based Datacenters
AU  - Thinakaran, P.
AU  - Gunasekaran, J.R.
AU  - Sharma, B.
AU  - Kandemir, M.T.
AU  - Das, C.R.
T2  - Proceedings - IEEE International Conference on Cluster Computing, ICCC
AB  - Compute heterogeneity is increasingly gaining prominence in modern datacenters due to the addition of accelerators like GPUs and FPGAs. We observe that datacenter schedulers are agnostic of these emerging accelerators, especially their resource utilization footprints, and thus, not well equipped to dynamically provision them based on the application needs. We observe that the state-of-the-art datacenter schedulers fail to provide fine-grained resource guarantees for latency-sensitive tasks that are GPU-bound. Specifically for GPUs, this results in resource fragmentation and interference leading to poor utilization of allocated GPU resources. Furthermore, GPUs exhibit highly linear energy efficiency with respect to utilization and hence proactive management of these resources is essential to keep the operational costs low while ensuring the end-to-end Quality of Service (QoS) in case of user-facing queries.Towards addressing the GPU orchestration problem, we build Knots, a GPU-aware resource orchestration layer and integrate it with the Kubernetes container orchestrator to build Kube-Knots. Kube-Knots can dynamically harvest spare compute cycles through dynamic container orchestration enabling co-location of latency-critical and batch workloads together while improving the overall resource utilization. We design and evaluate two GPU-based scheduling techniques to schedule datacenter-scale workloads through Kube-Knots on a ten node GPU cluster. Our proposed Correlation Based Prediction (CBP) and Peak Prediction (PP) schemes together improves both average and 99th percentile cluster-wide GPU utilization by up to 80% in case of HPC workloads. In addition, CBP+PP improves the average job completion times (JCT) of deep learning workloads by up to 36% when compared to state-of-the-art schedulers. This leads to 33% cluster-wide energy savings on an average for three different workloads compared to state-of-the-art GPU-agnostic schedulers. Further, the proposed PP scheduler guarantees the end-to-end QoS for latency-critical queries by reducing QoS violations by up to 53% when compared to state-of-the-art GPU schedulers. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/CLUSTER.2019.8891040
VL  - 2019-September
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075268776&doi=10.1109%2fCLUSTER.2019.8891040&partnerID=40&md5=c15599158265a13428a55a593c69384d
DB  - Scopus
KW  - Energy efficiency
KW  - Containers
KW  - Scheduling
KW  - Computer architecture
KW  - Quality of service
KW  - Program processors
KW  - Deep learning
KW  - State of the art
KW  - Resource utilizations
KW  - Cluster computing
KW  - Scheduling techniques
KW  - Graphics processing unit
KW  - Proactive management
KW  - Batch workloads
KW  - End-to-end QoS
KW  - End-to-end quality of service
KW  - Job completion
ER  - 

TY  - JOUR
TI  - AWS Lambda
T2  - Serverless Functions.
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119282869&partnerID=40&md5=cd40c8bad8633c32da6d4d08288097d9
DB  - Scopus
ER  - 

TY  - CONF
TI  - KylinX: A dynamic library operating system for simplified and efficient cloud virtualization
AU  - Zhang, Y.
AU  - Crowcroft, J.
AU  - Li, D.
AU  - Zhang, C.
AU  - Li, H.
AU  - Wang, Y.
AU  - Yu, K.
AU  - Xiong, Y.
AU  - Chen, G.
T2  - Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018
AB  - Unikernel specializes a minimalistic LibOS and a target application into a standalone single-purpose virtual machine (VM) running on a hypervisor, which is referred to as (virtual) appliance. Compared to traditional VMs, Unikernel appliances have smaller memory footprint and lower overhead while guaranteeing the same level of isolation. On the downside, Unikernel strips off the process abstraction from its monolithic appliance and thus sacrifices flexibility, efficiency, and applicability. This paper examines whether there is a balance embracing the best of both Unikernel appliances (strong isolation) and processes (high flexibility/efficiency). We present KylinX, a dynamic library operating system for simplified and efficient cloud virtualization by providing the pVM (process-like VM) abstraction. A pVM takes the hypervisor as an OS and the Unikernel appliance as a process allowing both page-level and library-level dynamic mapping. At the page level, KylinX supports pVM fork plus a set of API for inter-pVM communication (IpC). At the library level, KylinX supports shared libraries to be linked to a Unikernel appliance at runtime. KylinX enforces mapping restrictions against potential threats. KylinX can fork a pVM in about 1.3 ms and link a library to a running pVM in a few ms, both comparable to process fork on Linux (about 1 ms). Latencies of KylinX IpCs are also comparable to that of UNIX IPCs. © Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018. All rights reserved.
DA  - 2020///
PY  - 2020
SP  - 173
EP  - 185
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077459733&partnerID=40&md5=ce16e0d3fd31f98b9e0dacc83a6f3a1a
DB  - Scopus
KW  - Virtualization
KW  - Virtual machine
KW  - Virtual reality
KW  - Runtimes
KW  - Mapping
KW  - Computer operating systems
KW  - Hypervisor
KW  - Abstracting
KW  - Memory footprint
KW  - Dynamic mapping
KW  - High flexibility
KW  - Potential threats
KW  - Shared libraries
KW  - Target application
ER  - 

TY  - JOUR
T2  - Expedia Case Study - Amazon AWS.
DA  - 2021///
PY  - 2021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119267908&partnerID=40&md5=18841b5e363b1e2e6f8ef11cb26a8cf9
DB  - Scopus
ER  - 

TY  - CONF
TI  - GrandSLAm: Guaranteeing SLAs for jobs in microservices execution frameworks
AU  - Kannan, R.S.
AU  - Subramanian, L.
AU  - Raju, A.
AU  - Ahn, J.
AU  - Mars, J.
AU  - Tang, L.
T2  - Proceedings of the 14th EuroSys Conference 2019
AB  - The microservice architecture has dramatically reduced user effort in adopting and maintaining servers by providing a catalog of functions as services that can be used as building blocks to construct applications. This has enabled datacenter operators to look at managing datacenter hosting microservices quite differently from traditional infrastructures. Such a paradigm shift calls for a need to rethink resource management strategies employed in such execution environments. We observe that the visibility enabled by a microservices execution framework can be exploited to achieve high throughput and resource utilization while still meeting Service Level Agreements, especially in multi-tenant execution scenarios. In this study, we present GrandSLAm, a microservice execution framework that improves utilization of datacenters hosting microservices. GrandSLAm estimates time of completion of requests propagating through individual microservice stages within an application. It then leverages this estimate to drive a runtime system that dynamically batches and reorders requests at each microservice in a manner where individual jobs meet their respective target latency while achieving high throughput. GrandSLAm significantly increases throughput by up to 3× compared to the our baseline, without violating SLAs for a wide range of real-world AI and ML applications. © 2019 Association for Computing Machinery.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3302424.3303958
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063902706&doi=10.1145%2f3302424.3303958&partnerID=40&md5=3421b52ef10bf2d471d82be441436ba2
DB  - Scopus
KW  - Machine Learning
KW  - Resource management
KW  - Microservice
KW  - Learning systems
KW  - Execution environments
KW  - Resource utilizations
KW  - Computer systems
KW  - Service Level Agreements
KW  - Building blockes
KW  - Execution framework
KW  - Execution scenario
KW  - Systems
ER  - 

TY  - JOUR
T2  - Azure Durable Functions
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091921373&partnerID=40&md5=429fb2a6809c7c1f4ec2d3f0368f5a27
DB  - Scopus
ER  - 

TY  - JOUR
TI  - SAND: Towards High-Performance Serverless Computing
AU  - Akkus, I.E.
AU  - Chen, R.
AU  - Rimac, I.
AU  - Stein, M.
AU  - Satzke, K.
AU  - Beck, A.
AU  - Aditya, P.
AU  - Hilt, V.
T2  - USENIX ATC
DA  - 2018///
PY  - 2018
SP  - 923
EP  - 935
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062596504&partnerID=40&md5=69803d1ae53e00a17efa1267107cd265
DB  - Scopus
ER  - 

TY  - CONF
TI  - The Ifs and Buts of Less is More: A Serverless Computing Reality Check
AU  - Kuhlenkamp, J.
AU  - Werner, S.
AU  - Tai, S.
T2  - Proceedings - 2020 IEEE International Conference on Cloud Engineering, IC2E 2020
AB  - Serverless computing defines a pay-as-you-go cloud execution model, where the unit of computation is a function that a cloud provider executes and auto-scales on behalf of a cloud consumer. Serverless suggests not (or less) caring about servers but focusing (more) on business logic expressed in functions. Server'less' may be 'more' when getting developer expectations and platform propositions right and when engineering solutions that take specific behavior and constraints of (current) Function-as-a-Service platforms into account. To this end, in this invited paper, we present a summary of findings and lessons learned from a series of research experiments conducted over the past two years. We argue that careful attention must be placed on the promises associated with the serverless model, provide a reality-check for five common assumptions, and suggest ways to mitigate unwanted effects. Our findings focus on application workload distribution and computational processing complexity, the specific auto-scaling mechanisms in place, the behavior and strategies implemented with operational tasks, the constraints and limitations existing when composing functions, and the costs of executing functions. © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/IC2E48712.2020.00023
SP  - 154
EP  - 161
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086010587&doi=10.1109%2fIC2E48712.2020.00023&partnerID=40&md5=017b80e2713eedd1b56287f83399c448
DB  - Scopus
KW  - serverless computing
KW  - function-as-a-service (FaaS)
KW  - Cloud providers
KW  - Engineering
KW  - Industrial engineering
KW  - Work-load distribution
KW  - Cloud consumers
KW  - Computational processing
KW  - Engineering solutions
KW  - FaaS experimentation
KW  - FaaS platforms
KW  - Operational tasks
KW  - Scaling mechanism
KW  - Service platforms
ER  - 

TY  - JOUR
T2  - Prometheus
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093068583&partnerID=40&md5=14a90c15a85c08ba691cf0d0d4f660b1
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Cloud programming simplified: A Berkeley view on serverless computing
AU  - Jonas, E.
T2  - Cloud Programming Simplified: A Berkeley View on Serverless Computing
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067433472&partnerID=40&md5=85287d7260661386b411bc1efadbde9b
DB  - Scopus
ER  - 

TY  - CONF
TI  - Cirrus: A Serverless Framework for End-To-end ML Workflows
AU  - Carreira, J.
AU  - Fonseca, P.
AU  - Tumanov, A.
AU  - Zhang, A.
AU  - Katz, R.
T2  - SoCC 2019 - Proceedings of the ACM Symposium on Cloud Computing
AB  - Machine learning (ML) workflows are extremely complex. The typical workflow consists of distinct stages of user interaction, such as preprocessing, training, and tuning, that are repeatedly executed by users but have heterogeneous computational requirements. This complexity makes it challenging for ML users to correctly provision and manage resources and, in practice, constitutes a significant burden that frequently causes over-provisioning and impairs user productivity. Serverless computing is a compelling model to address the resource management problem, in general, but there are numerous challenges to adopt it for existing ML frameworks due to significant restrictions on local resources. This work proposes Cirrus-An ML framework that automates the end-To-end management of datacenter resources for ML workflows by efficiently taking advantage of serverless infrastructures. Cirrus combines the simplicity of the serverless interface and the scalability of the serverless infrastructure (AWS Lambdas and S3) to minimize user effort. We show a design specialized for both serverless computation and iterative ML training is needed for robust and efficient ML training on serverless infrastructure. Our evaluation shows that Cirrus outperforms frameworks specialized along a single dimension: Cirrus is 100x faster than a general purpose serverless system [36] and 3.75x faster than specialized ML frameworks for traditional infrastructures [49]. © 2019 ACM.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3357223.3362711
SP  - 13
EP  - 24
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086244066&doi=10.1145%2f3357223.3362711&partnerID=40&md5=93d14acd78be426e6b2361507438ea14
DB  - Scopus
KW  - Cloud computing
KW  - Machine Learning
KW  - Serverless
KW  - Distributed Computing
KW  - Computer science
KW  - Computer programming
KW  - Over provisioning
KW  - Computational requirements
KW  - End-to-End management
KW  - Local resources
KW  - Resource management problems
KW  - Serverless systems
KW  - User interaction
KW  - User productivity
ER  - 

TY  - JOUR
T2  - AWS Lambda Cold Starts.
DA  - 2021///
PY  - 2021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119294537&partnerID=40&md5=5686c5a1d2e3e8964b3a6e6e2a16076f
DB  - Scopus
ER  - 

TY  - BOOK
TI  - Markov chains: From theory to implementation and experimentation
AU  - Gagniuc, P.A.
T2  - Markov Chains: From Theory to Implementation and Experimentation
AB  - A fascinating and instructive guide to Markov chains for experienced users and newcomers alike. This unique guide to Markov chains approaches the subject along the four convergent lines of mathematics, implementation, simulation, and experimentation. It introduces readers to the art of stochastic modeling, shows how to design computer implementations, and provides extensive worked examples with case studies. Markov Chains: From Theory to Implementation and Experimentation begins with a general introduction to the history of probability theory in which the author uses quantifiable examples to illustrate how probability theory arrived at the concept of discrete-time and the Markov model from experiments involving independent variables. An introduction to simple stochastic matrices and transition probabilities is followed by a simulation of a two-state Markov chain. The notion of steady state is explored in connection with the long-run distribution behavior of the Markov chain. Predictions based on Markov chains with more than two states are examined, followed by a discussion of the notion of absorbing Markov chains. Also covered in detail are topics relating to the average time spent in a state, various chain configurations, and n-state Markov chain simulations used for verifying experiments involving various diagram configurations. • Fascinating historical notes shed light on the key ideas that led to the development of the Markov model and its variants • Various configurations of Markov Chains and their limitations are explored at length • Numerous examples-from basic to complex-are presented in a comparative manner using a variety of color graphics • All algorithms presented can be analyzed in either Visual Basic, Java Script, or PHP • Designed to be useful to professional statisticians as well as readers without extensive knowledge of probability theory. Covering both the theory underlying the Markov model and an array of Markov chain implementations, within a common conceptual framework, Markov Chains: From Theory to Implementation and Experimentation is a stimulating introduction to and a valuable reference for those wishing to deepen their understanding of this extremely valuable statistical tool. © 2017 John Wiley & Sons, Inc. All rights reserved.
DA  - 2017///
PY  - 2017
SP  - 1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006461119&partnerID=40&md5=fcd942eb4ae1d63da5a68748376f5d30
DB  - Scopus
KW  - Statistics
KW  - Case-studies
KW  - Stochastic models
KW  - Markov chains
KW  - Stochastic-modeling
KW  - Random variables
KW  - Distribution functions
KW  - Markov modeling
KW  - Computer implementations
KW  - Discrete time
KW  - Higher order statistics
KW  - Independent variables
KW  - Markov chain approaches
KW  - Probability theory
KW  - Simple stochastic
KW  - Worked examples
ER  - 

TY  - JOUR
T2  - OpenFaaS.
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119284680&partnerID=40&md5=94302faef6f70fe60792580f1a2846a3
DB  - Scopus
ER  - 

TY  - CONF
TI  - Firm: An intelligent fine-grained resource management framework for SLO-Oriented microservices
AU  - Qiu, H.
AU  - Banerjee, S.S.
AU  - Jha, S.
AU  - Kalbarczyk, Z.T.
AU  - Iyer, R.K.
T2  - Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2020
AB  - User-facing latency-sensitive web services include numerous distributed, intercommunicating microservices that promise to simplify software development and operation. However, multiplexing of compute resources across microservices is still challenging in production because contention for shared resources can cause latency spikes that violate the service-level objectives (SLOs) of user requests. This paper presents FIRM, an intelligent fine-grained resource management framework for predictable sharing of resources across microservices to drive up overall utilization. FIRM leverages online telemetry data and machine-learning methods to adaptively (a) detect/localize microservices that cause SLO violations, (b) identify low-level resources in contention, and (c) take actions to mitigate SLO violations via dynamic reprovisioning. Experiments across four microservice benchmarks demonstrate that FIRM reduces SLO violations by up to 16× while reducing the overall requested CPU limit by up to 62%. Moreover, FIRM improves performance predictability by reducing tail latencies by up to 11×. © 2020 Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2020. All rights reserved.
DA  - 2020///
PY  - 2020
SP  - 805
EP  - 825
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096750460&partnerID=40&md5=b6f58ba9bf20eadf724e1ad34566cc29
DB  - Scopus
KW  - Software design
KW  - Learning systems
KW  - Resource allocation
KW  - Natural resources management
KW  - Web services
KW  - Digital storage
KW  - Service level objective
KW  - Systems analysis
KW  - Machine learning methods
KW  - Compute resources
KW  - Development and operations
KW  - Re-provisioning
KW  - Resource management framework
KW  - Shared resources
KW  - Telemetry data
ER  - 

TY  - CONF
TI  - Fifer: Tackling resource underutilization in the serverless era
AU  - Gunasekaran, J.R.
AU  - Thinakaran, P.
AU  - Nachiappan, N.C.
AU  - Kandemir, M.T.
AU  - Das, C.R.
T2  - Middleware 2020 - Proceedings of the 2020 21st International Middleware Conference
AB  - Datacenters are witnessing a rapid surge in the adoption of serverless functions for microservices-based applications. A vast majority of these microservices typically span less than a second, have strict SLO requirements, and are chained together as per the requirements of an application. The aforementioned characteristics introduce a new set of challenges, especially in terms of container provisioning and management, as the state-of-the-art resource management frameworks, employed in serverless platforms, tend to look at microservice-based applications similar to conventional monolithic applications. Hence, these frameworks suffer from microservice agnostic scheduling and colossal container over-provisioning, especially during workload fluctuations, thereby resulting in poor resource utilization. In this work, we quantify the above shortcomings using a variety of workloads on a multi-node cluster managed by the Kubernetes and Brigade serverless framework. To address them, we propose Fifer Ð an adaptive resource management framework to efficiently manage function-chains on serverless platforms. The key idea is to make Fifer (i) utilization conscious by efficiently bin packing jobs to fewer containers using function-aware container scaling and intelligent request batching, and (ii) at the same time, SLO-compliant by proactively spawning containers to avoid cold-starts, thus minimizing the overall response latency. Combining these benefits, Fifer improves container utilization and cluster-wide energy consumption by 4× and 31%, respectively, without compromising on SLO's, when compared to the state-of-the-art schedulers employed by serverless platforms. © 2020 Association for Computing Machinery.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3423211.3425683
SP  - 280
EP  - 295
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098501448&doi=10.1145%2f3423211.3425683&partnerID=40&md5=5b7c61544291abbc88128e4805963129
DB  - Scopus
KW  - Serverless
KW  - Scheduling
KW  - Data centers
KW  - Energy utilization
KW  - Resource allocation
KW  - Natural resources management
KW  - Middleware
KW  - State of the art
KW  - Resource utilizations
KW  - Over provisioning
KW  - Bin packing
KW  - Bins
KW  - Resource management framework
KW  - Adaptive Resource Management
KW  - Multi-nodes
KW  - Queuing
KW  - Resource-management
ER  - 

TY  - JOUR
T2  - Google Cloud Functions
DA  - 2018///
PY  - 2018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058988852&partnerID=40&md5=3754edb5c4abbd9a67b401f83d0d33ec
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Predicting WWW surfing using multiple evidence combination
AU  - Awad, M.
AU  - Khan, L.
AU  - Thuraisingham, B.
T2  - VLDB Journal
AB  - The improvement of many applications such as web search, latency reduction, and personalization/ recommendation systems depends on surfing prediction. Predicting user surfing paths involves tradeoffs between model complexity and predictive accuracy. In this paper, we combine two classification techniques, namely, the Markov model and Support Vector Machines (SVM), to resolve prediction using Dempster's rule. Such fusion overcomes the inability of the Markov model in predicting the unseen data as well as overcoming the problem of multiclassification in the case of SVM, especially when dealing with large number of classes. We apply feature extraction to increase the power of discrimination of SVM. In addition, during prediction we employ domain knowledge to reduce the number of classifiers for the improvement of accuracy and the reduction of prediction time. We demonstrate the effectiveness of our hybrid approach by comparing our results with widely used techniques, namely, SVM, the Markov model, and association rule mining. © 2006 Springer-Verlag.
DA  - 2008///
PY  - 2008
DO  - 10.1007/s00778-006-0014-1
VL  - 17
IS  - 3
SP  - 401
EP  - 417
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-43249090136&doi=10.1007%2fs00778-006-0014-1&partnerID=40&md5=af9bab42c29f9c96a245340d1460e98a
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Azure Functions Cold Starts.
DA  - 2021///
PY  - 2021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119286758&partnerID=40&md5=3512821f3bc6954e98ac682cfc3fd1aa
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Bin-Packing
AU  - Korte, B.
AU  - Vygen, J.
T2  - Combinatorial Optimization: Theory and Algorithms
DA  - 2018///
PY  - 2018
SP  - 489
EP  - 507
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047991612&partnerID=40&md5=3d0d49cd65ec37194d0d2edb77cc99df
DB  - Scopus
ER  - 

TY  - JOUR
TI  - SOCK: Rapid task provisioning with serverless-optimized containers
AU  - Oakes, E.
AU  - Yang, L.
AU  - Zhou, D.
AU  - Houck, K.
AU  - Harter, T.
AU  - Arpaci-Dusseau, A.
AU  - Arpaci-Dusseau, R.
T2  - USENIX ATC
DA  - 2018///
PY  - 2018
SP  - 57
EP  - 70
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062616330&partnerID=40&md5=9b225926df24cf1dbc5436bcf966f5ca
DB  - Scopus
ER  - 

TY  - CONF
TI  - Architectural implications of function-as-a-service computing
AU  - Shahrad, M.
AU  - Balkind, J.
AU  - Wentzlaff, D.
T2  - Proceedings of the Annual International Symposium on Microarchitecture, MICRO
AB  - Serverless computing is a rapidly growing cloud application model, popularized by Amazon's Lambda platform. Serverless cloud services provide fine-grained provisioning of resources, which scale automatically with user demand. Function-as-a-Service (FaaS) applications follow this serverless model, with the developer providing their application as a set of functions which are executed in response to a user- or system-generated event. Functions are designed to be short-lived and execute inside containers or virtual machines, introducing a range of system-level overheads. This paper studies the architectural implications of this emerging paradigm. Using the commercial-grade Apache OpenWhisk FaaS platform on real servers, this work investigates and identifies the architectural implications of FaaS serverless computing. The workloads, along with the way that FaaS inherently interleaves short functions from many tenants frustrates many of the locality-preserving architectural structures common in modern processors. In particular, we find that: FaaS containerization brings up to 20x slowdown compared to native execution, cold-start can be over 10x a short function's execution time, branch mispredictions per kilo-instruction are 20x higher for short functions, memory bandwidth increases by 6x due to the invocation pattern, and IPC decreases by as much as 35% due to inter-function interference. We open-source FaaSProfiler, the FaaS testing and profiling platform that we developed for this work. © 2019 Association for Computing Machinery.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3352460.3358296
SP  - 1063
EP  - 1075
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074454583&doi=10.1145%2f3352460.3358296&partnerID=40&md5=a1c5e1ae40346c4b02458962a3f50a50
DB  - Scopus
KW  - Serverless
KW  - Cloud
KW  - Containers
KW  - Function-as-a-service
KW  - Computer architecture
KW  - Architecture
KW  - Cloud applications
KW  - Clouds
KW  - Faas
KW  - Modern processors
KW  - Architectural structure
KW  - Branch mispredictions
KW  - Locality-preserving
KW  - Open whisk
ER  - 

TY  - JOUR
T2  - IBM-Composer.
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119298686&partnerID=40&md5=34666b790668eccc66f781770b3ee29c
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Peeking behind the curtains of serverless platforms
AU  - Wang, L.
AU  - Li, M.
AU  - Zhang, Y.
AU  - Ristenpart, T.
AU  - Swift, M.
T2  - ATC
DA  - 2018///
PY  - 2018
SP  - 133
EP  - 146
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059031873&partnerID=40&md5=caa34109dbbbd9ed3461168088924e09
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Kubernetes.
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119261837&partnerID=40&md5=992895f474fb0a5a71e423ec1cb3feaf
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Intel Power Gadget.
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119296266&partnerID=40&md5=c05731643c70baae9737c00730903f6b
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Hey HTTP Load Testing Tool.
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119274614&partnerID=40&md5=a42b90c12d54952d0cbea13b84823610
DB  - Scopus
ER  - 

TY  - JOUR
AU  - Singhvi, A.
AU  - Houck, K.
AU  - Balasubramanian, A.
AU  - Shaikh, M.D.
AU  - Venkataraman, S.
AU  - Akella, A.
T2  - Archipelago: A Scalable Low-Latency Serverless Platform
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086275868&partnerID=40&md5=74375a642a1e604dccda8238e9406f1a
DB  - Scopus
ER  - 

TY  - CONF
TI  - Xanadu: Mitigating cascading cold starts in serverless function chain deployments
AU  - Daw, N.
AU  - Bellur, U.
AU  - Kulkarni, P.
T2  - Middleware 2020 - Proceedings of the 2020 21st International Middleware Conference
AB  - Organization of tasks as workflows are an essential feature to expand the applicability of the serverless computing framework. Existing serverless platforms are either agnostic to function chains (workflows as a composition of functions) or rely on naive provisioning and management mechanisms of the serverless framework-an example is that they provision resources after the trigger to each function in a workflow arrives thereby forcing a setup latency for each function in the workflow. In this work, we focus on mitigating the cascading cold start problem- the latency overheads in triggering a sequence of serverless functions according to a workflow specification. We first establish the nature and extent of the cascading effects in cold start situations across multiple commercial server platforms and cloud providers. Towards mitigating these cascading overheads, we design and develop several optimizations, that are built into our tool Xanadu. Xanadu offers multiple instantiation options based on the desired runtime isolation requirements and supports function chaining with or without explicit workflow specifications. Xanadu's optimizations to address the cascading cold start problem are built on speculative and just-in-time provisioning of resources. Our evaluation of the Xanadu system reveals almost complete elimination of cascading cold starts at minimal cost overheads, outperforming the available state of the art platforms. For even relatively short workflows, Xanadu reduces platform overheads by almost 18x compared to Knative and 10x compared to Apache Openwhisk. © 2020 Association for Computing Machinery.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3423211.3425690
SP  - 356
EP  - 370
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098485885&doi=10.1145%2f3423211.3425690&partnerID=40&md5=8e4776490ec08205556e8edaac0873a7
DB  - Scopus
KW  - Middleware
KW  - State of the art
KW  - Specifications
KW  - Computing frameworks
KW  - Cascading effects
KW  - Cold start problems
KW  - Essential features
KW  - Just-in-time scheduling
KW  - Management mechanisms
KW  - Server platform
KW  - Serverless workflows
KW  - Speculative deployment
KW  - Workflow specification
ER  - 

TY  - CONF
TI  - Phoenix: A Constraint-Aware Scheduler for Heterogeneous Datacenters
AU  - Thinakaran, P.
AU  - Gunasekaran, J.R.
AU  - Sharma, B.
AU  - Kandemir, M.T.
AU  - Das, C.R.
T2  - Proceedings - International Conference on Distributed Computing Systems
AB  - Today's datacenters are increasingly becoming diverse with respect to both hardware and software architectures in order to support a myriad of applications. These applications are also heterogeneous in terms of job response times and resource requirements (eg., Number of Cores, GPUs, Network Speed) and they are expressed as task constraints. Constraints are used for ensuring task performance guarantees/Quality of Service(QoS) by enabling the application to express its specific resource requirements. While several schedulers have recently been proposed that aim to improve overall application and system performance, few of these schedulers consider resource constraints across tasks while making the scheduling decisions. Furthermore, latencycritical workloads and short-lived jobs that typically constitute about 90% of the total jobs in a datacenter have strict QoS requirements, which can be ensured by minimizing the tail latency through effective scheduling. In this paper, we propose Phoenix, a constraint-aware hybrid scheduler to address both these problems (constraint awareness and ensuring low tail latency) by minimizing the job response times at constrained workers. We use a novel Constraint Resource Vector (CRV) based scheduling, which in turn facilitates reordering of the jobs in a queue to minimize tail latency. We have used the publicly available Google traces to analyze their constraint characteristics and have embedded these constraints in Cloudera and Yahoo cluster traces for studying the impact of traces on system performance. Experiments with Google, Cloudera and Yahoo cluster traces across 15,000 worker node cluster shows that Phoenix improves the 99th percentile job response times on an average by 1.9× across all three traces when compared against a state-of-the-art hybrid scheduler. Further, in comparison to other distributed scheduler like Hawk, it improves the 90th and 99th percentile job response times by 4.5× and 5× respectively. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/ICDCS.2017.262
SP  - 977
EP  - 987
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027243807&doi=10.1109%2fICDCS.2017.262&partnerID=40&md5=ac206f4e869dd2cd31958088b9a24366
DB  - Scopus
KW  - Performance
KW  - Resource management
KW  - Scheduling
KW  - Application programs
KW  - Quality of service
KW  - Program processors
KW  - Distributed computer systems
KW  - Information management
KW  - Response time (computer systems)
KW  - Resource Management
KW  - Constraint-aware
KW  - Heterogeneous Data Center
KW  - Heterogeneous data centers
KW  - Hybrid
ER  - 

TY  - JOUR
T2  - Enhanced Intel SpeedStep Technology for the Intel Pentium M Processor
DA  - 2004///
PY  - 2004
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746279684&partnerID=40&md5=caf5513fb73ecdf14a9b669e30ddc5b4
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Samsung V-NAND Ssd 860 Evo
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100738579&partnerID=40&md5=6cfd312038f753ade131473f69c87c38
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Understanding Load Capacitance and Access Time
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104993806&partnerID=40&md5=24740d5b79e49ffef0b36ffda50b7d28
DB  - Scopus
ER  - 

TY  - CONF
TI  - A study of workload consolidation and power consumption on a multi-core processor
AU  - Seo, D.
T2  - Proceeding of the 2012 ACM Research in Applied Computation Symposium, RACS 2012
AB  - Workload consolidation based on virtualization techniques has been widely adopted in most Cloud Computing platforms to improve energy efficiency. However, workload consolidation comes with performance degradation due to the contention in shared resources, such as CPU cache, memory controller, etc. In this paper, we study the change of performance and power consumption with various mixes of workloads. We performed experiments with SPEC CPU2006 benchmark suite on an Intel's quad-core processor, and observed that there is noticeable relationship among characteristics of workloads, performance interference and power consumption. Copyright 2012 ACM.
DA  - 2012///
PY  - 2012
DO  - 10.1145/2401603.2401702
SP  - 457
EP  - 458
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871635925&doi=10.1145%2f2401603.2401702&partnerID=40&md5=d0166c177d5ddfb205890a2e5a06b2cc
DB  - Scopus
KW  - Energy efficiency
KW  - Performance degradation
KW  - Green IT
KW  - Benchmarking
KW  - Workload consolidation
KW  - Benchmark suites
KW  - Server consolidation
KW  - Virtualizations
KW  - Multi-core processor
KW  - Computing platform
KW  - Shared resources
KW  - Memory controller
KW  - Performance interference
ER  - 

TY  - CONF
TI  - DRACeo: A smart simulator to deploy energy saving methods in microservices based networks
AU  - Valera, H.H.A.
AU  - Dalmau, M.
AU  - Roose, P.
AU  - Larracoechea, J.
AU  - Herzog, C.
T2  - Proceedings of the Workshop on Enabling Technologies: Infrastructure for Collaborative Enterprises, WETICE
AB  - Nowadays, many researchers work to identify microservices-based application deployments and scheduling solutions to save energy without decreasing functional QoS. In this work, we present DRACeo: A simulator that allows facing this challenge in a simple and efficient way, enabling its users to focus uniquely on microservices deployment/scheduling algorithms and its hardware/software repercussions (load vs. energy consumption) without worrying about low-level network configurations or operating system issues. DRACeo is able to deploy and schedule (move, duplicate, start/stop) microservices and their dependencies on various devices with software and hardware heterogeneity (CPU, bandwidth, RAM, Battery, etc.), taking into account various scheduling heuristics algorithms: centralized vs non-centralized. To do this, DRACeo allows deploying custom network topologies based on client-server schemes or p2p distributions, where devices can (dis)appear, turn on/off obeying random circumstances or user strategies. Finally, the simulator performs relevant operations such as QoS definition, resource monitoring, calculation of energy saved and consumption tracking (at device and network level). We tested some ideas based on our previous work 'Kaligreen' to demonstrate the effectiveness of DRACeo. © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/WETICE49692.2020.00026
VL  - 2020-September
SP  - 94
EP  - 99
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100714002&doi=10.1109%2fWETICE49692.2020.00026&partnerID=40&md5=6dde8162ca929d1cfeb833956e56d257
DB  - Scopus
KW  - microservices
KW  - middleware
KW  - energy
KW  - Scheduling
KW  - Energy utilization
KW  - Energy conservation
KW  - consumption
KW  - CPU
KW  - hard disk
KW  - network
KW  - prototype
KW  - simulator
KW  - Network topology
KW  - Application deployment
KW  - Computer hardware
KW  - Energy saving methods
KW  - Hardware/software
KW  - Network configuration
KW  - Resource monitoring
KW  - Scheduling heuristics
KW  - Simulators
KW  - Software and hardwares
ER  - 

TY  - JOUR
T2  - Implementing Microservices on AWS
DA  - 2019///
PY  - 2019
VL  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104984254&partnerID=40&md5=e5ebc8c03310646cb19ef4f6136c7fbd
DB  - Scopus
ER  - 

TY  - CONF
TI  - SPEC CPU2017 – next-generation compute benchmark
AU  - Bucek, J.
AU  - Lange, K.-D.
AU  - Kistowski, J.V.
T2  - ICPE 2018 - Companion of the 2018 ACM/SPEC International Conference on Performance Engineering
AB  - Description of the new features of the SPEC CPU2017 industry standard benchmark and its metric calculations. © 2018 Association of Computing Machinery.
DA  - 2018///
PY  - 2018
DO  - 10.1145/3185768.3185771
VL  - 2018-January
SP  - 41
EP  - 42
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052022982&doi=10.1145%2f3185768.3185771&partnerID=40&md5=11d0c231ad2092a4e4df6e3067486f6b
DB  - Scopus
KW  - Performance
KW  - Program processors
KW  - Energy
KW  - CPU
KW  - Power
KW  - Engineering
KW  - Industrial engineering
KW  - Speed
KW  - Compiler
KW  - Rate
KW  - Rating
KW  - SPEC
ER  - 

TY  - CONF
TI  - A green analysis of mobile cloud computing applications
AU  - Zhan, K.
AU  - Lung, C.-H.
AU  - Srivastava, P.
T2  - Proceedings of the ACM Symposium on Applied Computing
AB  - With the widespread of rich mobile applications, the usage of mobile devices, especially smart phones and tablets, has become popular nowadays. However, battery in the mobile devices often limits continuous usages with its small size and capacity. Therefore, power consumption of mobile devices is a critical issue, not only for extending lifetime use of mobile devices, but also for creating a green IT which is a raising concern in academic and industrial communities. The goal of this paper is to investigate the power consumption of mobile devices and resource usages for various applications. To meet the goal, we have performed a number of experiments and detailed evaluations of resource usages and power consumption for various applications using a number of tools. In addition, we have measured performance metrics for applications either using a mobile device or running in the Amazon cloud. We examine the impact of various factors and provide insights on power consumption for different mobile applications. Copyright 2014 ACM.
DA  - 2014///
PY  - 2014
DO  - 10.1145/2554850.2555069
SP  - 357
EP  - 362
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905637579&doi=10.1145%2f2554850.2555069&partnerID=40&md5=a1b9a6bbb272621e55d54e65fd49f773
DB  - Scopus
KW  - Mobile computing
KW  - Green computing
KW  - Mobile cloud computing
KW  - CPU
KW  - Electric power utilization
KW  - Power consumption
KW  - Mobile applications
KW  - Resource usage
KW  - Performance metrics
KW  - Mobile devices
KW  - Amazon
KW  - Critical issues
KW  - Industrial communities
ER  - 

TY  - JOUR
TI  - Multidimensional Access Methods
AU  - Gaede, V.
AU  - Günther, O.
T2  - ACM Computing Surveys
AB  - Search operations in databases require special support at the physical level. This is true for conventional databases as well as spatial databases, where typical search operations include the point query (find all objects that contain a given search point) and the region query (find all objects that overlap a given search region). More than ten years of spatial database research have resulted in a great variety of multidimensional access methods to support such operations. We give an overview of that work. After a brief survey of spatial data management in general, we first present the class of point access methods, which are used to search sets of points in two or more dimensions. The second part of the paper is devoted to spatial access methods to handle extended objects, such as rectangles or polyhedra. We conclude with a discussion of theoretical and experimental results concerning the relative performance of various approaches.
DA  - 1998///
PY  - 1998
DO  - 10.1145/280277.280279
VL  - 30
IS  - 2
SP  - 170
EP  - 231
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032083561&doi=10.1145%2f280277.280279&partnerID=40&md5=cb89e29b44053485060d3f9c03a2890c
DB  - Scopus
KW  - Performance
KW  - Computer aided design
KW  - World Wide Web
KW  - Data structures
KW  - Database systems
KW  - Online searching
KW  - Query languages
KW  - Information retrieval
KW  - Constraint databases
KW  - Geographic information systems
KW  - H.2.2 [Database Management]: Physical Design - access methods
KW  - H.2.4 [Database Management]: Systems
KW  - H.2.8 [Database Management]: Database Applications - spatial databases and GIS
KW  - Indexing (of information)
KW  - Management information systems
KW  - Multidimensional access methods
KW  - Spatial data management
KW  - Spatial databases
KW  - Spatial variables control
ER  - 

TY  - JOUR
T2  - Cpu-frequency
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104949099&partnerID=40&md5=528703faff6ba19a8a777d50e5a4e6dc
DB  - Scopus
ER  - 

TY  - CONF
TI  - A microservice architecture for the Intranet of Things and energy in smart buildings
AU  - Bao, K.
AU  - Mauser, I.
AU  - Kochanneck, S.
AU  - Xu, H.
AU  - Schmeck, H.
T2  - Proceedings of the 1st International Workshop on Mashups of Things and APIs, MOTA 2016
AB  - This paper presents challenges and issues in smart buildings and the Internet of Things (IoT), which we identified in years of research in real buildings. To tackle these challenges, a decentralized service-oriented architecture based on a message-oriented middleware has been implemented for the domain of smart buildings. It uses a network-transparent IoT message bus and provides the means for composing applications from auxiliary services, which facilitate device abstraction, protocol adaption, modularity, and maintainability. We demonstrate the flexibility of our architecture by describing how three distinct applications - privacy-enhancing energy data visualization, automated building energy management, and a generic user interface - can be integrated and operated simultaneously in our real smart building laboratory. We compare the advantages of our architecture to conventional ones and provide a best-practice solution for the Intranet of Things and Energy in smart buildings. © 2016 ACM.
DA  - 2016///
PY  - 2016
DO  - 10.1145/3007203.3007215
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010375308&doi=10.1145%2f3007203.3007215&partnerID=40&md5=730b0a142140f2e954d073b1476e68bd
DB  - Scopus
KW  - Internet of Things
KW  - Internet of things
KW  - Visualization
KW  - Information management
KW  - Architecture
KW  - Smart energies
KW  - Middleware
KW  - Network architecture
KW  - Buildings
KW  - Service oriented architecture (SOA)
KW  - Information services
KW  - Energy management
KW  - Internet of thing (IOT)
KW  - Intelligent buildings
KW  - Smart building
KW  - User interfaces
KW  - Energy informatics
KW  - Flow visualization
KW  - Automated buildings
KW  - Auxiliary services
KW  - Data visualization
KW  - Device abstractions
KW  - Ge-neric user interfaces
KW  - Message oriented middleware
KW  - Smart energy
ER  - 

TY  - JOUR
AU  - Blog, N.T.
T2  - Netflix Conductor: A Microservices Orchestrator
DA  - 2025///
PY  - 2025
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088595149&partnerID=40&md5=4e2096496f24a0abd19ff8a02142cc5f
DB  - Scopus
ER  - 

TY  - CONF
TI  - MAAN: A multi-attribute addressable network for grid information services
AU  - Cai, M.
AU  - Frank, M.
AU  - Chen, J.
AU  - Szekely, P.
T2  - Proceedings - IEEE/ACM International Workshop on Grid Computing
AB  - Recent structured peer-to-peer (P2P) systems such as distributed hash tables (DHTs) offer scalable key-based lookup for distributed resources. However, they cannot be simply applied to grid information services because grid resources need to be registered and searched using multiple attributes. We propose a multiattribute addressable network (MAAN) which extends chord to support multiattribute and range queries. MAAN addresses range queries by mapping attribute values to the chord identifier space via uniform locality preserving hashing. It uses an iterative or single attribute dominated query routing algorithm to resolve multiattribute based queries. Each node in MAAN only has O(logN) neighbors for N nodes. The number of routing hops to resolve a multiattribute range query is O(logN+N×smin), where smin is the minimum range selectivity on all attributes. When smin=ε, it is logarithmic to the number of nodes, which is scalable to a large number of nodes and attributes. We also measured the performance of our MAAN implementation and the experimental results are consistent with our theoretical analysis. © 2003 IEEE.
DA  - 2003///
PY  - 2003
DO  - 10.1109/GRID.2003.1261714
VL  - 2003-January
SP  - 184
EP  - 191
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892599957&doi=10.1109%2fGRID.2003.1261714&partnerID=40&md5=3cf0588a11da0435e274cf5a9448e651
DB  - Scopus
KW  - Distributed computing
KW  - Routing
KW  - Distributed computer systems
KW  - Information services
KW  - Algorithms
KW  - Performance analysis
KW  - Grid computing
KW  - Iterative methods
KW  - Large scale systems
KW  - Query languages
KW  - Business
KW  - Distributed hash tables
KW  - Distributed resources
KW  - Grid information services
KW  - Industry
KW  - Iterative algorithm
KW  - Iterative algorithms
KW  - Large-scale systems
KW  - Maintainability
KW  - Maintenance engineering
KW  - Peer to peer computing
KW  - Peer to peer networks
KW  - Peer-to-peer computing
KW  - Query processing
KW  - Structured peer-to-peer
ER  - 

TY  - JOUR
TI  - Power consumption of WLAN network elements
AU  - Chiaravalloti, S.
AU  - Idzikowski, F.
AU  - Budzisz, L.
T2  - Power Consumption of WLAN Network Elements
DA  - 2011///
PY  - 2011
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865796739&partnerID=40&md5=c02a2b7082d8f28c1c0b3914d64daff3
DB  - Scopus
ER  - 

TY  - CONF
TI  - SD-Rtree: A scalable distributed Rtree
AU  - Du Mouza, C.
AU  - Litwin, W.
AU  - Rigaux, P.
T2  - Proceedings - International Conference on Data Engineering
AB  - We propose a scalable distributed data structure (SDDS) called SD-Rtree. We intend our structure for point and window queries over possibly large spatial dataseis distributed on clusters of interconnected servers. SD-Rtree generalizes the well-known Rtree structure. It uses a distributed balanced binary spatial tree that scales with insertions to potentially any number of storage servers through splits of the overloaded ones. A user/application manipulates the structure from a client node. The client addresses the tree through its image that the splits can make outdated. This may generate addressing errors, solved by the forwarding among the servers. Specific messages towards the clients incrementally correct the outdated images. © 2007 IEEE.
DA  - 2007///
PY  - 2007
DO  - 10.1109/ICDE.2007.367875
SP  - 296
EP  - 305
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548718163&doi=10.1109%2fICDE.2007.367875&partnerID=40&md5=6f091d3d94643d71d150f2c2769946c6
DB  - Scopus
KW  - Servers
KW  - Problem solving
KW  - Cluster analysis
KW  - Data structures
KW  - Query processing
KW  - Binary spatial tree
KW  - Binary trees
KW  - SD-Rtree
KW  - Window queries
ER  - 

TY  - JOUR
T2  - Creating Composite UI Based on Microservices
DA  - 2018///
PY  - 2018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105008681&partnerID=40&md5=fd80828b306b6fb971484df909431cd2
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Memory Benchmarks
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104986422&partnerID=40&md5=7612f2e2a692c5fd8ae99c4e2ba8c167
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Microservices: A definition of this new architectural term
AU  - Lewis, J.
AU  - Fowler, M.
T2  - Microservices: A Definition of This New Architectural Term
DA  - 2014///
PY  - 2014
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964234114&partnerID=40&md5=fd3de0bd7b34545e47485cf075757737
DB  - Scopus
ER  - 

TY  - CONF
TI  - Estimating Android applications' CPU energy usage via bytecode profiling
AU  - Hao, S.
AU  - Li, D.
AU  - Halfond, W.G.J.
AU  - Govindan, R.
T2  - 2012 1st International Workshop on Green and Sustainable Software, GREENS 2012 - Proceedings
AB  - Optimizing the energy efficiency of mobile applications can greatly increase user satisfaction. However, developers lack easily applied tools for estimating the energy consumption of their applications. This paper proposes a new approach, eCalc, that is lightweight in terms of its developer requirements and provides code-level estimates of energy consumption. The approach achieves this using estimation techniques based on program analysis of the mobile application. In evaluation, eCalc is able to estimate energy consumption within 9.5% of the ground truth for a set of mobile applications. Additionally, eCalc provides useful and meaningful feedback to the developer that helps to characterize energy consumption of the application. © 2012 IEEE.
DA  - 2012///
PY  - 2012
DO  - 10.1109/GREENS.2012.6224263
SP  - 1
EP  - 7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864260968&doi=10.1109%2fGREENS.2012.6224263&partnerID=40&md5=0396ef1cac05481c30d150b8a7f8f7a0
DB  - Scopus
KW  - Energy efficiency
KW  - Energy utilization
KW  - Mobile applications
KW  - Mobile telecommunication systems
KW  - Robots
KW  - Estimation
KW  - Program analysis
KW  - Energy usage
KW  - Energy estimation
KW  - Ground truth
KW  - User satisfaction
KW  - energy estimation
KW  - Bytecodes
KW  - Estimation techniques
KW  - Android apps
KW  - bytecode profiling
KW  - eCalc
ER  - 

TY  - JOUR
TI  - Kademlia: A peer-to-peer information system based on the XOR metric
AU  - Maymounkov, P.
AU  - Mazières, D.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - We describe a peer-to-peer distributed hash table with provable consistency and performance in a fault-prone environment. Our system routes queries and locates nodes using a novel XOR-based metric topology that simplifies the algorithm and facilitates our proof. The topology has the property that every message exchanged conveys or reinforces useful contact information. The system exploits this information to send parallel, asynchronous query messages that tolerate node failures without imposing timeout delays on users. © 2002 Springer-Verlag Berlin Heidelberg.
DA  - 2002///
PY  - 2002
DO  - 10.1007/3-540-45748-8_5
VL  - 2429
SP  - 53
EP  - 65
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947235017&doi=10.1007%2f3-540-45748-8_5&partnerID=40&md5=ce04a5a23535595758b18f88f1546304
DB  - Scopus
KW  - Performance
KW  - Topology
KW  - Property
KW  - Peer to peer networks
KW  - Distributed Hash Table
KW  - Fault-prone
KW  - Kademlia
KW  - Metric topology
KW  - Peer to peer
KW  - Peer-to-peer information
KW  - Query message
KW  - Route queries
KW  - Structured Query Language
ER  - 

TY  - JOUR
AU  - Krajewski, J.
AU  - Lozano, J.
AU  - Driver, J.
AU  - Escandon, E.
AU  - Kumar, S.
AU  - Malatini, S.
AU  - Lozano Hinojosa, J.
T2  - PASTRY: The Third Generation of Peer-to-peer Networks
DA  - 2006///
PY  - 2006
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104980207&partnerID=40&md5=a612dbb515e68c5dde35ca04bd676e6b
DB  - Scopus
ER  - 

TY  - CONF
TI  - Green architectural tactics for the cloud
AU  - Procaccianti, G.
AU  - Lago, P.
AU  - Lewis, G.A.
T2  - Proceedings - Working IEEE/IFIP Conference on Software Architecture 2014, WICSA 2014
AB  - Energy efficiency is a primary concern for the ICT sector. In particular, the widespread adoption of cloud computing technologies has drawn attention to the massive energy consumption of data centers. Although hardware constantly improves with respect to energy efficiency, this should also be a main concern for software. In previous work we analyzed the literature and elicited a set of techniques for addressing energy efficiency in cloud-based software architectures. In this work we codified these techniques in the form of Green Architectural Tactics. These tactics will help architects extend their design reasoning towards energy efficiency and to apply reusable solutions for greener software. © 2014 IEEE.
DA  - 2014///
PY  - 2014
DO  - 10.1109/WICSA.2014.30
SP  - 41
EP  - 44
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903736056&doi=10.1109%2fWICSA.2014.30&partnerID=40&md5=05d70982b487d6efcb5f7e23da19ca94
DB  - Scopus
KW  - Cloud computing
KW  - Energy efficiency
KW  - Software Architecture
KW  - Cloud Computing
KW  - Software architecture
KW  - Energy Efficiency
KW  - Data centers
KW  - Energy utilization
KW  - Cloud-based
KW  - Computer software reusability
KW  - Cloud computing technologies
KW  - Architectural Tactics
KW  - Design reasonings
KW  - Ict sectors
ER  - 

TY  - CONF
TI  - STACEE: Enhancing storage clouds using edge devices
AU  - Neumann, D.
AU  - Bodenstein, C.
AU  - Rana, O.F.
AU  - Krishnaswamy, R.
T2  - 8th International Conference on Autonomic Computing, ICAC 2011 Co-located Workshops - Proceedings of the 1st ACM/IEEE Workshop on Autonomic Computing in Economics, ACE'11
AB  - The explosion of user generated data along with the evolution of web 2.0 applications (e.g. social networks, blogs, podcasts, etc.) has resulted in a tremendous demand for storage. With cloud computing posing as a possible all-in-one solution, "storage clouds" focus on providing distributed storage capability. We discuss the creation of a storage cloud using edge devices, based on Peer-to-Peer resource provisioning. In this approach, mobile phones, PCs/Media Centers, Set-top-boxes, modems and networked storage devices can all contribute as storage within these storage clouds. Combining all end-user edge devices may result in a scalable, very flexible storage capability that keeps the data comparatively close to the user, increasing availability, while reducing latency. This work addresses the issue of Quality of Service (QoS)-aware scheduling in a P2P storage cloud, built with edge devices by designing an optimization scheme that minimizes energy from a system perspective and simultaneously maximizing user satisfaction from the individual user perspective. © 2011 ACM.
DA  - 2011///
PY  - 2011
DO  - 10.1145/1998561.1998567
SP  - 19
EP  - 26
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960117962&doi=10.1145%2f1998561.1998567&partnerID=40&md5=f0fcfad372abe313f6dd9ce55834e657
DB  - Scopus
KW  - Cloud computing
KW  - resource management
KW  - green it
KW  - Resource management
KW  - Social Networks
KW  - Quality of service
KW  - Optimization
KW  - Green IT
KW  - Distributed storage
KW  - Resource provisioning
KW  - Clouds
KW  - User interfaces
KW  - End users
KW  - Telecommunication equipment
KW  - User satisfaction
KW  - Peer to peer networks
KW  - Peer to peer
KW  - Flexible storage
KW  - Optimization scheme
KW  - peer-to-peer
KW  - Podcasts
KW  - Set top box
KW  - storage optimization
KW  - Virtual storage
KW  - Web 2.0 applications
ER  - 

TY  - JOUR
TI  - Searching in metric spaces
AU  - Chávez, E.
AU  - Navarro, G.
AU  - Baeza-Yates, R.
AU  - Marroquín, J.L.
T2  - ACM Computing Surveys
AB  - The problem of searching the elements of a set that are close to a given query element under some similarity criterion has a vast number of applications in many branches of computer science, from pattern recognition to textual and multimedia information retrieval. We are interested in the rather general case where the similarity criterion defines a metric space, instead of the more restricted case of a vector space. Many solutions have been proposed in different areas, in many cases without cross-knowledge. Because of this, the same ideas have been reconceived several times, and very different presentations have been given for the same approaches. We present some basic results that explain the intrinsic difficulty of the search problem. This includes a quantitative definition of the elusive concept of "intrinsic dimensionality." We also present a unified Categories and Subject Descriptors: F.2.2 [Analysis of algorithms and problem complexity]: Nonnumerical algorithms and problems-computations on discrete structures, geometrical problems and computations, sorting and searching; H.2.1 [Database management]: Physical design-access methods; H.3.1 [Information storage and retrieval]: Content analysis and indexing-indexing methods; H.3.2 [Information storage and retrieval]: Information storage-file organization; H.3.3 [Information storage and retrieval]: Information search and retrieval-clustering, search process; 1.5.1 [Pattern recognition]: Models-geometric; 1.5.3 [Pattern recognition]: Clustering General Terms: Algorithms. ©2001 ACM.
DA  - 2001///
PY  - 2001
DO  - 10.1145/502807.502808
VL  - 33
IS  - 3
SP  - 273
EP  - 321
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0345043999&doi=10.1145%2f502807.502808&partnerID=40&md5=4f5dc577f5c13d03370609b56123afae
DB  - Scopus
KW  - Algorithms
KW  - Computational complexity
KW  - Data storage equipment
KW  - Data mining
KW  - Curse of dimensionality
KW  - Vector spaces
KW  - Vectors
KW  - Information retrieval
KW  - Clustering search process
KW  - Computational geometry
KW  - Metric spaces
KW  - Nearest neighbors
KW  - Pattern matching
KW  - Similarity searching
ER  - 

TY  - JOUR
T2  - How to Overclock Your Unlocked Intel Core™ Processor
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104999899&partnerID=40&md5=c9ce7bb19797cdbcfd874a103b82e27d
DB  - Scopus
ER  - 

TY  - CONF
TI  - Scalability evaluation of an energy-aware resource management system for clusters of web servers
AU  - Kiertscher, S.
AU  - Schnor, B.
T2  - Proceedings of the 2015 International Symposium on Performance Evaluation of Computer and Telecommunication Systems, SPECTS 2015 - Part of SummerSim 2015 Multiconference
AB  - For green cluster computing resource management systems have to be energy-aware. CHERUB is such an energy-aware resource management system which works together with the Linux Virtual Server. Experiments in a small cluster setup with two nodes have shown the benefit of CHERUB. This paper presents necessary design changes to make CHERUB also work in big cluster setups. Our methodological approach is two-fold. First, we present unit measurements to evaluate the scaling of the re-implemented functions. Second, a cluster simulator is presented and validated which makes it possible to test CHERUB for backend clusters of arbitrary size. © 2015 Society for Modeling and Simulation International.
DA  - 2015///
PY  - 2015
DO  - 10.1109/SPECTS.2015.7285285
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992036358&doi=10.1109%2fSPECTS.2015.7285285&partnerID=40&md5=8fde33c6f96f037481f6dcf2f4c96100
DB  - Scopus
KW  - Energy efficiency
KW  - Scalability
KW  - Energy Efficiency
KW  - Power management
KW  - Simulation
KW  - Computer architecture
KW  - Resource allocation
KW  - Natural resources management
KW  - Green IT
KW  - Green-IT
KW  - Energy management systems
KW  - Cluster computing
KW  - Computer operating systems
KW  - Computing resource management
KW  - Energy awareness
KW  - Resource management systems
KW  - Cluster Computing
KW  - Energy Awareness
KW  - Integrated modeling
KW  - Integrated Modeling and Measurement
KW  - Methodological approach
KW  - Scalability evaluation
KW  - Scalability Studies
ER  - 

TY  - CONF
TI  - The Architecture of Kaligreen V2: A Middleware Aware of Hardware Opportunities to Save Energy
AU  - Valera, H.H.L.
AU  - Dalmau, M.
AU  - Roose, P.
AU  - Herzog, C.
T2  - 2019 6th International Conference on Internet of Things: Systems, Management and Security, IOTSMS 2019
AB  - Nowadays, energy saving in the use of information technologies is a very important issue both from the economic and sustainability point of view. Many scientists investigate methods to save energy at different application levels (cloud: i.e., architectures, grid: i.e., middlewares and frameworks and hardware management: i.e., operating systems) and many of them agree on the strategy of executing programs, processes or virtual machines only using the time and resources that are strictly necessary. For this, it is necessary to plan strategies for deployment and relocation of processes; but always taking into account hardware repercussions and the knowledge of the architecture and applications behavior. On the other hand, it has already been demonstrated that the use of microservices brings numerous advantages in availability and efficiency; but we do not find many jobs that exploit this technique on the energy level. In this article, we present the architecture of a middleware for distributed microservices-based applications, which allows any negotiation-based scheduling algorithm to duplicate or move microservices from one device to another in a non-centralized way for energy savings, taking into account the consumption characteristics of the microservices and the capabilities that the hardware components offer. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/IOTSMS48152.2019.8939237
SP  - 79
EP  - 86
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077969153&doi=10.1109%2fIOTSMS48152.2019.8939237&partnerID=40&md5=7b937f6435a19d89b288227377d059e7
DB  - Scopus
KW  - microservices
KW  - middleware
KW  - energy
KW  - Internet of things
KW  - Application programs
KW  - Application level
KW  - Program processors
KW  - Energy conservation
KW  - consumption
KW  - CPU
KW  - hard disk
KW  - Hard disk storage
KW  - Hardware components
KW  - Hardware management
KW  - Information use
KW  - Middleware
KW  - network
KW  - Network architecture
KW  - Networks (circuits)
KW  - Save energy
KW  - Scheduling algorithms
ER  - 

TY  - CONF
TI  - Kali green: A distributed scheduler for energy saving
AU  - Lvarez-Valera, H.H.
AU  - Roose, P.
AU  - Dalmau, M.
AU  - Herzog, C.
AU  - Respicio, K.
T2  - Procedia Computer Science
AB  - A commonplace issue with portable technology is battery efficiency. While many industries are trying their best to improve battery life without sacrificing a products quality and efficiency, we believe that further can be done to improve battery consumption on ones mobile devicefrom tablets to smartphones to laptops to everything else. Many applications on these devices are based on a microservice architecture. In this article, we introduce a new algorithm KaliGreen that can maneuver the microservices within a network of devices in order to maximize the run-time of a microservice-based application; moreover, KaliGreen allows a 54% increase in the average run-time of an application by shifting microservices from 6 devices (as example) with low battery or inefficient processing ratios to devices in better conditions. To achieve this, KaliGreen utilizes KaliMucho middleware, which is able manipulate microservices in run-time. This algorithm provides a plausible solution to maximizing energy consumption within a network of devices. © 2018 The Authors. Published by Elsevier Ltd.
DA  - 2018///
PY  - 2018
DO  - 10.1016/j.procs.2018.10.172
VL  - 141
SP  - 223
EP  - 230
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058349351&doi=10.1016%2fj.procs.2018.10.172&partnerID=40&md5=762a84024c90ed200594143af3fc90ed
DB  - Scopus
KW  - Microservices
KW  - Smartphones
KW  - Green Computing
KW  - Green computing
KW  - Energy utilization
KW  - Energy conservation
KW  - mHealth
KW  - Middleware
KW  - Battery consumption
KW  - Battery efficiencies
KW  - Battery life
KW  - Distributed applicatioggns
KW  - Distributed schedulers
KW  - Electric batteries
KW  - Portable technologies
KW  - Products quality
ER  - 

TY  - JOUR
TI  - A call for energy efficiency in data centers
AU  - Pawlish, M.
AU  - Varde, A.S.
AU  - Robila, S.A.
AU  - Ranganathan, A.
T2  - SIGMOD Record
AB  - In this paper, we explore a data center's performance with a call for energy efficiency through green computing. Some performance metrics we examine in data centers are server energy usage, Power Usage Effectiveness and utilization rate, i.e., the extent to which data center servers are being used. Recent literature indicates that utilization rates at many internal data centers are quite low, resulting in poor usage of resources such as energy and materials. Based on our study, we attribute these low utilization rates to not fully taking advantage of virtualization, and not retiring phantom (unused) servers. This paper describes our initiative corroborated with real data in a university setting. We suggest that future data centers will need to increase their utilization rates for better energy efficiency, and moving towards a cloud provider would help. However, we argue that neither a pure in-house data center or cloud model is the best solution. Instead we recommend, from a decision support perspective, a hybrid model in data center management to lower costs and increase services, while also providing greater energy efficiency.
DA  - 2014///
PY  - 2014
DO  - 10.1145/2627692.2627703
VL  - 43
IS  - 1
SP  - 45
EP  - 51
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901660119&doi=10.1145%2f2627692.2627703&partnerID=40&md5=41566c91c3f43607ba45eadfab151955
DB  - Scopus
KW  - Energy efficiency
KW  - Cloud
KW  - Energy Efficiency
KW  - Data centers
KW  - Green IT
KW  - Utilization rates
KW  - Clouds
KW  - Data center management
KW  - Performance metrics
KW  - Virtualizations
KW  - Decision support systems
KW  - Data Centers
KW  - Decision supports
KW  - University settings
KW  - Utilization Rates
ER  - 

TY  - CONF
TI  - ECOFEN: An end-to-end energy cost model and simulator for evaluating power consumption in large-scale networks
AU  - Orgerie, A.-C.
AU  - Lefèvre, L.
AU  - Guérin-Lassous, I.
AU  - Lopez Pacheco, D.M.
T2  - 2011 IEEE International Symposium on a World of Wireless, Mobile and Multimedia Networks, WoWMoM 2011 - Digital Proceedings
AB  - Wired networks are increasing in size and their power consumption is becoming a matter of concern. Evaluating the end-to-end electrical cost of new network architectures and protocols is difficult due to the lack of monitored realistic infrastructures. We propose an End-to-End energy Cost mOdel and simulator For Evaluating power consumption in large-scale Networks (ECOFEN) whose user's entries are the network topology and traffic. Based on configurable measurement of different network components (routers, switches, NICs, etc.), it provides the power consumption of the overall network including the end-hosts as well as the power consumption of each equipment over time. © 2011 IEEE.
DA  - 2011///
PY  - 2011
DO  - 10.1109/WoWMoM.2011.5986203
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052698432&doi=10.1109%2fWoWMoM.2011.5986203&partnerID=40&md5=59543a30d002fd0aae1815e186a1e3f4
DB  - Scopus
KW  - Energy efficiency
KW  - Costs
KW  - Topology
KW  - Network architecture
KW  - Network topology
KW  - Power consumption
KW  - Simulators
KW  - Energy cost
KW  - Network simulator
KW  - Wireless networks
KW  - Wired networks
KW  - Configurable
KW  - Electric network topology
KW  - Electrical costs
KW  - Energy cost model
KW  - Large-scale network
KW  - Switches
KW  - Time switches
ER  - 

TY  - CONF
TI  - An analysis of memory power consumption in database systems
AU  - Karyakin, A.
AU  - Salem, K.
T2  - Proceedings of the 13th International Workshop  on Data Management on New Hardware, DAMON 2017
AB  - The growing appetite for in-memory computing is increasing memory's share of total server power consumption. However, memory power consumption in database management systems is not well understood. This paper presents an empirical characterization of memory power consumption in database systems, for both analytical and transactional workloads. Our results indicate that memory power optimization will be effective only if it can reduce background power through more aggressive use of low power memory idle states. ©2017 ACM.
DA  - 2017///
PY  - 2017
DO  - 10.1145/3076113.3076117
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021733217&doi=10.1145%2f3076113.3076117&partnerID=40&md5=c604b7a1774dffc452900871b6efa76e
DB  - Scopus
KW  - Hardware
KW  - Information management
KW  - Electric power utilization
KW  - Computer hardware
KW  - Power Optimization
KW  - Database systems
KW  - Background power
KW  - Low-power memory
ER  - 

TY  - CONF
TI  - Modeling CPU energy consumption for energy efficient scheduling
AU  - Jaiantilal, A.
AU  - Jiang, Y.
AU  - Mishra, S.
T2  - 1st International Workshop on Green Computing Middleware 2010, GCM 2010
AB  - In the past few years, we have seen the rising popularity of multi-core systems, including the 4, 6, and 8-cores present in i7 processors from Intel, and the 8 and 12 cores present in Magny-Cours processors from AMD. There is a general trend that newer processors have more and more number of cores. A study [6] showed that, in data centers, the CPU load is around 10-50%; thus, when multi-core processors are used in data centers, many of the cores will be unused for a majority of the time. Such a scenario is also true for a casual desktop PC user. As an idle core still consumes energy, from the perspective of saving energy, it is important to ensure that the idle cores are put in the lowest energy state and unnecessary wakeups for these idle cores are avoided. This will ensure the lowest energy consumption for a given set of tasks. The precursor step towards developing an energy efficient CPU scheduler requires an understanding of the relation between the type of tasks and their corresponding power profile. In this paper, we show that the power profile of a task is dependent on the type of processor cycles executed by the task. We further develop a model that can predict the power consumption based on the processor cycles executed by the task. We conclude our paper showing initial results on how such a model can be used to schedule tasks and save energy. Copyright © 2010 ACM.
DA  - 2010///
PY  - 2010
DO  - 10.1145/1925013.1925015
SP  - 10
EP  - 15
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952339536&doi=10.1145%2f1925013.1925015&partnerID=40&md5=41f46a8723ae0d9bff6b8739ae223d9b
DB  - Scopus
KW  - Energy consumption
KW  - Scheduling
KW  - Data centers
KW  - Energy utilization
KW  - Energy-Efficient Scheduling
KW  - Energy efficient
KW  - Middleware
KW  - Save energy
KW  - Multi-core systems
KW  - Power Consumption
KW  - Multi-core processor
KW  - Microprocessor chips
KW  - Saving energy
KW  - General trends
KW  - Lowest energy state
KW  - Power profile
ER  - 

TY  - JOUR
TI  - A path generation scheme for real-time green internet of things
AU  - Lu, Y.-F.
AU  - Wu, J.
AU  - Kuo, C.-F.
T2  - SIGAPP Appl. Comput. Rev.
DA  - 2014///
PY  - 2014
VL  - 14
IS  - 2
SP  - 45
EP  - 58
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926450845&partnerID=40&md5=3c2e07eaf7deec34698397f5aa7597d8
DB  - Scopus
ER  - 

TY  - CONF
TI  - Deterministic 1-2 skip list in distributed system
AU  - Mandal, S.
AU  - Chakraborty, S.
AU  - Karmakar, S.
T2  - Proceedings of 2012 2nd IEEE International Conference on Parallel, Distributed and Grid Computing, PDGC 2012
AB  - Searching data efficiently in distributed applications like peer-to-peer system is a challenging task due to the random distribution of data among several participating nodes. Efficient data structures are designed and implemented to reduce the complexity of data searching in such an environment. In this paper a data structure called deterministic 1-2 skip list has been proposed as a solution for search problems in distributed environment. The data structure has three main operations viz. search, insert, and delete. The detailed description of the insertion, deletion and search operations are given in this paper. It is found that the message complexity of the insertion, deletion and search algorithm is O(log n) where n is the total number of nodes in the skip-list. © 2012 IEEE.
DA  - 2012///
PY  - 2012
DO  - 10.1109/PDGC.2012.6449835
SP  - 296
EP  - 301
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874425885&doi=10.1109%2fPDGC.2012.6449835&partnerID=40&md5=c6457017be28deab73b5fd55df515e08
DB  - Scopus
KW  - Distributed systems
KW  - Distributed applications
KW  - Grid computing
KW  - Random distribution
KW  - Data structures
KW  - Search engines
KW  - Data searching
KW  - Distributed environments
KW  - Efficient data structures
KW  - Message complexity
KW  - Peer-to-Peer system
KW  - Search Algorithms
KW  - Search operations
KW  - Search problem
KW  - Skip listes
ER  - 

TY  - CONF
TI  - One torus to rule them all: Multi-dimensional queries in P2P systems
AU  - Ganesan, P.
AU  - Yang, B.
AU  - Garcia-Molina, H.
T2  - ACM International Conference Proceeding Series
AB  - Peer-to-peer systems enable access to data spread over an extremely large number of machines. Most P2P systems support only simple lookup queries. However, many new applications, such as P2P photo sharing and massively multi-player games, would benefit greatly from support for multidimensional range queries. We show how such queries may be supported in a P2P system by adapting traditional spatial-database technologies with novel P2P routing networks and load-balancing algorithms. We show how to adapt two popular spatial-database solutions - kd-trees and space-filling curves - and experimentally compare their effectiveness.
DA  - 2004///
PY  - 2004
DO  - 10.1145/1017074.1017081
VL  - 67
SP  - 19
EP  - 24
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954435659&doi=10.1145%2f1017074.1017081&partnerID=40&md5=9ae45db4a31a58e8b4a18af759ab6c84
DB  - Scopus
KW  - Simple++
KW  - New applications
KW  - Trees (mathematics)
KW  - Peer to peer networks
KW  - Structured Query Language
KW  - Peer-to-Peer system
KW  - Lookups
KW  - Massively multiplayer games
KW  - Multi-dimensional queries
KW  - P2P system
KW  - Photo sharing
KW  - Spatial database
KW  - Systems support
KW  - Table lookup
ER  - 

TY  - JOUR
T2  - Desktop Hdd Product Manual
DA  - 2016///
PY  - 2016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100813024&partnerID=40&md5=689610d04b90f0283edb78919307b376
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Orquestacion de Contenedores Explicada
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104928973&partnerID=40&md5=6a764979e92e99a1bcea45f6966d2906
DB  - Scopus
ER  - 

TY  - CONF
TI  - Optimization of load balancing algorithm for green IT
AU  - Siddavatam, I.
AU  - Johri, E.
AU  - Patole, D.
T2  - International Conference and Workshop on Emerging Trends in Technology 2011, ICWET 2011 - Conference Proceedings
AB  - This paper is focused on calculating the processor utilization of various machines having different types of processor configurations employed in data centers by providing solutions to reduce the energy cost component of companies total IT budget. In this paper different parameters like CPU Utilization and Power Utilization on different machines installed in data centers have been analyzed. Also calculation of power required by different machines of different configurations in different modes of operations like active mode, hibernation mode and standby mode is been performed. Here an algorithm has been developed that will calculate the processor utilization by different machines according to the Load Distribution in a network and propose a strategy to utilize minimum possible energy by optimal resource utilization. Copyright © 2011 ACM.
DA  - 2011///
PY  - 2011
DO  - 10.1145/1980022.1980321
SP  - 1344
EP  - 1346
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958720139&doi=10.1145%2f1980022.1980321&partnerID=40&md5=b375d694610f5c51b7ce515e32c3cc5f
DB  - Scopus
KW  - Energy consumption
KW  - Information technology
KW  - Data centers
KW  - Optimization
KW  - Green IT
KW  - Algorithms
KW  - Electric power utilization
KW  - Wireless sensor networks
KW  - Resource utilizations
KW  - Energy cost
KW  - Load balancing algorithms
KW  - CPU utilization
KW  - Active mode
KW  - Different modes
KW  - IT budgets
KW  - Load distributions
KW  - Processor sharing
KW  - Processor sharing computer systems
KW  - Processor utilization
KW  - Standby mode
KW  - Standby power systems
ER  - 

TY  - CONF
TI  - Kalimucho: Middleware for mobile applications
AU  - Da, K.
AU  - Dalmau, M.
AU  - Roose, P.
T2  - Proceedings of the ACM Symposium on Applied Computing
AB  - Developing ubiquitous applications is particularly complex. Beyond the dynamic aspect of such applications, the evolution of computing towards the multiplication of mobile access terminals is not making things easier. One solution to simplifying the development and use of such applications is to use software platforms dedicated to deployment and adaptation of applications and handling the heterogeneity of peripherals. They allow designers to focus on business aspects and facilitate reuse. The Kalimucho platform was designed and developed against this background. It executes and supervises applications based on software components. Copyright 2014 ACM.
DA  - 2014///
PY  - 2014
DO  - 10.1145/2554850.2554883
SP  - 413
EP  - 419
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905652787&doi=10.1145%2f2554850.2554883&partnerID=40&md5=c88b567dba93ee6384b57b3bbd3ba1ee
DB  - Scopus
KW  - Performance
KW  - Reliability
KW  - Application programs
KW  - Measurement
KW  - Middleware
KW  - Mobile applications
KW  - Design
KW  - Measurements
KW  - Software component
KW  - Query languages
KW  - Business aspects
KW  - Dynamic aspects
KW  - Languages
KW  - Mobile access
KW  - Software platforms
KW  - Ubiquitous application
ER  - 

TY  - CONF
TI  - Exploration of CPU/GPU co-execution: From the perspective of performance, energy, and temperature
AU  - Kang, S.
AU  - Choi, H.J.
AU  - Kim, C.H.
AU  - Chung, S.W.
AU  - Kwon, D.
AU  - Na, J.C.
T2  - Proceedings of the 2011 ACM Research in Applied Computation Symposium, RACS 2011
AB  - In recent computing systems, CPUs have encountered the situations in which they cannot meet the increasing throughput demands. To overcome the limits of CPUs in processing heavy tasks, especially for computer graphics, GPUs have been widely used. Therefore, the performance of up-to-date computing systems can be maximized when the task scheduling between the CPU and the GPU is optimized. In this paper, we analyze the system in the perspective of performance, energy efficiency, and temperature according to the execution methods between the CPU and the GPU. Experimental results show that the GPU leads to better efficiency compared to the CPU when single application is executed. However, when two applications are executed, the GPU does not guarantee superior efficiency than the CPU depending on the application characteristics. © 2011 ACM.
DA  - 2011///
PY  - 2011
DO  - 10.1145/2103380.2103388
SP  - 38
EP  - 43
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863119763&doi=10.1145%2f2103380.2103388&partnerID=40&md5=47baf701aecd532d1795b9604448ed9c
DB  - Scopus
KW  - scheduling
KW  - Energy efficiency
KW  - GPU
KW  - High-performance computing
KW  - Scheduling
KW  - Program processors
KW  - CPU
KW  - Computing system
KW  - Task-scheduling
KW  - Computer graphics
KW  - Computer software selection and evaluation
KW  - CUDA
KW  - high-performance computing
ER  - 

TY  - JOUR
T2  - IntelR Core i5-8400 Processor
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104999405&partnerID=40&md5=1fd1276d941aa330413801d7bd0b2ddb
DB  - Scopus
ER  - 

TY  - CONF
TI  - The balance mechanism of power and performance in the virtualization
AU  - Chen, Q.
AU  - Li, J.
T2  - ACM International Conference Proceeding Series
AB  - Energy has become an important resource for computer system. In multicore systems, shared resource can lead to contention between tasks running on different cores, which also will give rise to poor power efficiency. The system with dynamic voltage and frequency scaling(DVFS) capability provide support for dynamic energy management to make a better energy efficiency. However, the coming of the virtualization technology make different in energy management. In this paper, we present the policies into a virtualization system, to improve the performance and energy efficiency by avoiding the resource contention of virtual CPU, which the application run on it, and use frequency scaling. Our evaluation based on Xen virtualization environment shows that our policies reduce the energy delay product considerably. © 2013 ACM.
DA  - 2013///
PY  - 2013
DO  - 10.1145/2556871.2556912
SP  - 189
EP  - 192
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899567061&doi=10.1145%2f2556871.2556912&partnerID=40&md5=97358bea132fd7f6d17117e96d2e7815
DB  - Scopus
KW  - Cloud computing
KW  - Energy efficiency
KW  - Resource contention
KW  - Scheduling
KW  - DVFS
KW  - Energy
KW  - Virtualization technologies
KW  - Virtual reality
KW  - Energy management
KW  - Computer systems
KW  - Dynamic voltage and frequency scaling
KW  - Multi-core systems
KW  - Dynamic energy managements
KW  - Energy delay product
KW  - Schedule
ER  - 

TY  - CONF
TI  - Cooperative wireless sensor networks for green internet of things
AU  - Yaacoub, E.
AU  - Kadri, A.
AU  - Abu-Dayya, A.
T2  - Q2SWinet'12 - Proceedings of the 8th ACM Symposium on QoS and Security for Wireless and Mobile Networks
AB  - Green internet of things are investigated by studying energy efficiency in wireless sensor networks. A cooperative multihop data transmission approach is presented and analyzed. Significant energy savings are achieved with the proposed approach compared to the non cooperative scenario, in addition to better delay results. Copyright 2012 ACM.
DA  - 2012///
PY  - 2012
DO  - 10.1145/2387218.2387235
SP  - 79
EP  - 80
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870372973&doi=10.1145%2f2387218.2387235&partnerID=40&md5=d83272830a260c4b0f1d7c18ad4ca982
DB  - Scopus
KW  - Energy efficiency
KW  - Internet of things
KW  - Sensor networks
KW  - Quality of service
KW  - Wireless sensor networks
KW  - Internet of Things (IOT)
KW  - Internet
KW  - Green internets
KW  - Data communication systems
KW  - Energy minimization
KW  - Green communications
KW  - Multihop
KW  - Non-cooperative
ER  - 

TY  - CONF
TI  - Middleware for managing provenance metadata
AU  - Malik, T.
AU  - Nistor, L.
AU  - Gehani, A.
T2  - Middleware'10 Posters and Demos Track, Middleware Posters'10
AB  - Current provenance collection systems typically gather metadata on remote hosts and submit it to a central server. We describe middleware for managing distributed provenance metadata, where each host maintains an authoritative local repository of the provenance metadata gathered on it. The approach provides several advantages - the system can scale to handle the large amounts of metadata generated when auditing occurs at fine granularity; users retain control over their provenance records; and the middleware transparently queries remote provenance stores to reconstruct distributed lineage. © 2010 ACM.
DA  - 2010///
PY  - 2010
DO  - 10.1145/1930028.1930033
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957978511&doi=10.1145%2f1930028.1930033&partnerID=40&md5=a129ebacb8a6a18c92f922d89481abb9
DB  - Scopus
KW  - Metadata
KW  - Middleware
KW  - Central servers
KW  - Collection systems
KW  - Fine granularity
KW  - Remote host
ER  - 

TY  - JOUR
TI  - Power management of datacenter workloads using per-core power gating
AU  - Leverich, J.
AU  - Monchiero, M.
AU  - Talwar, V.
AU  - Ranganathan, P.
AU  - Kozyrakis, C.
T2  - IEEE Computer Architecture Letters
AB  - While modern processors offer a wide spectrum of software-controlled power modes, most datacenters only rely on Dynamic Voltage and Frequency Scaling (DVFS, a.k.a. P-states) to achieve energy efficiency. This paper argues that, in the case of datacenter workloads, DVFS is not the only option for processor power management. We make the case for per-core power gating (PCPG) as an additional power management knob for multi-core processors. PCPG is the ability to cut the voltage supply to selected cores, thus reducing to almost zero the leakage power for the gated cores. Using a testbed based on a commercial 4-core chip and a set of real-world application traces from enterprise environments, we have evaluated the potential of PCPG. We show that PCPG can significantly reduce a processor's energy consumption (up to 40%) without significant performance overheads. When compared to DVFS, PCPG is highly effective saving up to 30% more energy than DVFS. When DVFS and PCPG operate together they can save up to almost 60%. © 2009 Published by the IEEE Computer Society.
DA  - 2010///
PY  - 2010
DO  - 10.1109/L-CA.2009.46
VL  - 8
IS  - 2
SP  - 48
EP  - 51
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-75449089335&doi=10.1109%2fL-CA.2009.46&partnerID=40&md5=304d051da79cd6d61a1c620c517769c7
DB  - Scopus
KW  - Energy efficiency
KW  - Energy consumption
KW  - Data centers
KW  - Energy management
KW  - Electric power measurement
KW  - Real-world application
KW  - Leakage power
KW  - Power managements
KW  - Modern processors
KW  - Multi-core processor
KW  - Energy-aware systems
KW  - Enterprise environment
KW  - Integration and modeling
KW  - ON dynamics
KW  - Power gatings
KW  - Power modes
KW  - Processor power
KW  - System architectures
KW  - Voltage supply
KW  - Wide spectrum
ER  - 

TY  - CONF
TI  - A Microservice Infrastructure for Distributed Communities of Practice
AU  - de Lange, P.
AU  - Göschlberger, B.
AU  - Farrell, T.
AU  - Klamma, R.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Non-formal learning in Communities of Practice (CoPs) makes up a significant portion of today’s knowledge gain. However, only little technological support is tailored specifically towards CoPs and their particular strengths and challenges. Even worse, CoPs often do not possess the resources to host or even develop a software ecosystem to support their activities. In this paper, we describe a distributed, microservice-based Web infrastructure for non-formal learning in CoPs. It mitigates the need for central infrastructures, coordination or facilitation and takes into account the constant change of these communities. As a real use case, we implement an inquiry-based learning application on-top of our infrastructure. Our evaluation results indicate the usefulness of this learning application, which shows promise for future work in the domain of community-hosted, microservice-based Web infrastructures for learning outside of formal settings. © 2018, Springer Nature Switzerland AG.
DA  - 2018///
PY  - 2018
DO  - 10.1007/978-3-319-98572-5_14
VL  - 11082 LNCS
SP  - 172
EP  - 186
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053214292&doi=10.1007%2f978-3-319-98572-5_14&partnerID=40&md5=2f36018faf5f3fa55203f0b2866ed8d4
DB  - Scopus
KW  - Microservices
KW  - Engineering education
KW  - Central infrastructure
KW  - Communities of Practice
KW  - Computer aided instruction
KW  - Distributed Communities of Practice
KW  - Inquiry-based learning
KW  - Learning infrastructures
KW  - Software ecosystems
KW  - Technological supports
ER  - 

TY  - CONF
TI  - Studying the energy consumption of data transfers in Clouds: The Ecofen approach
AU  - Cornea, B.F.
AU  - Orgerie, A.-C.
AU  - Lefèvre, L.
T2  - 2014 IEEE 3rd International Conference on Cloud Networking, CloudNet 2014
AB  - Energy consumption is one of the main limiting factors for designing large scale Clouds. Evaluating the energy consumption of Clouds networking architectures and providing multi-level views required by providers and users, is a challenging issue. In this paper, we show how to evaluate and understand network choices (protocols, topologies) in terms of contributions to the energy consumption of the global Cloud infrastructures. By applying the ECOFEN model (Energy Consumption mOdel For End-to-end Networks) and the corresponding simulation framework, we profile and analyze the energy consumption of data transfers in Clouds. © 2014 IEEE.
DA  - 2014///
PY  - 2014
DO  - 10.1109/CloudNet.2014.6968983
SP  - 143
EP  - 148
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925859008&doi=10.1109%2fCloudNet.2014.6968983&partnerID=40&md5=dcd337cdf783b075b4254c04e9ecb48b
DB  - Scopus
KW  - energy consumption
KW  - simulation
KW  - Green computing
KW  - Energy utilization
KW  - Data transfer
KW  - Energy consumption model
KW  - Simulation framework
KW  - Cloud data
KW  - Networking architecture
KW  - Cloud data transfers
KW  - End-to-end network
KW  - ethernet networks
KW  - Ethernet networks
KW  - Global clouds
ER  - 

TY  - JOUR
T2  - SSD Vs HDD Tested: What's the Difference and Which Is Better?
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105000819&partnerID=40&md5=5f84c97958557f877009954f0215fb8f
DB  - Scopus
ER  - 

TY  - JOUR
AU  - Stoica, I.
AU  - Morris, R.
AU  - Karger, D.
AU  - Kaashoek, M.
AU  - Balakrishnan, H.
T2  - Chord: A Scalable Peer-to-peer Lookup Service for Internet Applications
DA  - 2001///
PY  - 2001
VL  - 149-160
SP  - 149
EP  - 160
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952555588&partnerID=40&md5=2ee655b409f55c87e4c6862799cf2152
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Chord: A scalable peer-to-peer lookup protocol for Internet applications
AU  - Stoica, I.
AU  - Morris, R.
AU  - Liben-Nowell, D.
AU  - Karger, D.R.
AU  - Kaashoek, M.F.
AU  - Dabek, F.
AU  - Balakrishnan, H.
T2  - IEEE/ACM Transactions on Networking
AB  - A fundamental problem that confronts peer-to-peer applications is the efficient location of the node that stores a desired data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis and simulations show that Chord is scalable: Communication cost and the state maintained by each node scale logarithmically with the number of Chord nodes.
DA  - 2003///
PY  - 2003
DO  - 10.1109/TNET.2002.808407
VL  - 11
IS  - 1
SP  - 17
EP  - 32
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037298256&doi=10.1109%2fTNET.2002.808407&partnerID=40&md5=b789f15b0c5af2961f6a6532fa59419f
DB  - Scopus
KW  - Costs
KW  - Algorithms
KW  - Internet
KW  - Computer simulation
KW  - Data reduction
KW  - Iterative methods
KW  - Network protocols
KW  - Distributed scalable algorithms
KW  - Lookup protocols
KW  - Peer-to-peer networks
ER  - 

TY  - CHAP
TI  - Evaluating the RAM energy consumption at the stage of software development
AU  - Maevsky, D.A.
AU  - Maevskaya, E.J.
AU  - Stetsuyk, E.D.
T2  - Studies in Systems, Decision and Control
AB  - A method of absolute value estimation of the computer energy consumption in performing the programs is proposed in the chapter. The evaluation is made on the basis of the program source code and can help to choose the most optimal solution from the viewpoint of energy saving at the Software development stage. The method is based on the indication of energy consumption by the computer Random Access Memory (RAM) depending upon how intensive the RAM is used by Software. Selection of RAM due to the fact that it is a necessary equipment, and any computing device cannot function without it. Evaluation of energy consumed by RAM is made in using the two proposed mathematical models. The method for computer’s estimating power consumption based on assembler source code was made on the basis of the proposed models. This allows you to create a green software with the control of “green” degree on all stages of its development. © Springer International Publishing Switzerland 2017.
DA  - 2017///
PY  - 2017
VL  - 74
SP  - 101
EP  - 121
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028990231&doi=10.1007%2f978-3-319-44162-7_6&partnerID=40&md5=3c430799c139e365f2e8863f5f638572
DB  - Scopus
KW  - Energy consumption
KW  - Software engineering
KW  - Green software
KW  - Green computing
KW  - Energy-efficient software development
ER  - 

TY  - JOUR
T2  - Calculating Network Data Transfer Speeds
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105002315&partnerID=40&md5=a1a386cdd31dd70458fb6e0a1367c3e6
DB  - Scopus
ER  - 

TY  - CONF
TI  - Adaptive power panel of cloud computing controlling cloud power consumption
AU  - Azmy, N.M.
AU  - El-Maddah, I.A.M.
AU  - Mohamed, H.K.
T2  - ACM International Conference Proceeding Series
AB  - Cloud computing had created a new era of network design, where end-users can get their required services without having to purchase expensive infrastructure or even to care about troubleshooting. Power consumption is a challenge facing the Cloud Providers to operate their Datacenters. One solution to overcome this is the Virtual Machine (VM) migration, which is a technique used to switch under-utilized hosts to sleep mode in order to save power, and to avoid over-utilized hosts from Service Level Agreement (SLA) violation. But still the problem is that the Cloud Service Provider apply a single policy on all nodes. Our proposed solution is an adaptive power panel where different policies can be applied based on both of the nature of the tasks running on hosts, and the Cloud Provider decision. © 2016 ACM.
DA  - 2016///
PY  - 2016
DO  - 10.1145/2944165.2944167
VL  - 28-29-May-2016
SP  - 9
EP  - 14
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985990751&doi=10.1145%2f2944165.2944167&partnerID=40&md5=c5b64478913d045dc22f54d9b7cff5fa
DB  - Scopus
KW  - Cloud computing
KW  - Placement
KW  - Software engineering
KW  - Green Computing
KW  - Green computing
KW  - Distributed computer systems
KW  - Virtual machines
KW  - Electric power utilization
KW  - Migration
KW  - Virtual Machine
KW  - Adaptive
KW  - Allocation
KW  - Cloudsim
KW  - Selection
ER  - 

TY  - CONF
TI  - Poster abstract: Energy optimization framework in wireless sensor network
AU  - Esfahani, N.P.
AU  - Cerpa, A.E.
T2  - SenSys 2015 - Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems
AB  - We present a holistic architecture for energy management in sensor networks. Our architecture is based on a modeldriven approach which attempts to (a) establish functional relationships across different components of the software stack and the interrelated parameters based on empirical data, (b) use the maximum sensor value and time-synchronization errors acceptable by the users of the sensor network application as input to establish minimum quality of service requirements, and (c) optimize the parameter values of all the software modules within the node's application stack to minimize total energy consumption for each sensor node. We explore the trade-offs of the design space by using a non-trivial application that includes sensing, time synchronization and routing modules and show that when using our architecture, we can provide energy savings in the average of 37% to 76% while still maintaining quality of service both in terms of the expected sensing and time-synchronization errors. We further show that even when using modules that perform significantly better than others with default values (e.g. ORW ∗ CTP), we can still reduce overall energy consumption by properly adjusting the parameters of lowest performance modules and provide energy savings in the average of 30% to 43%.
DA  - 2015///
PY  - 2015
DO  - 10.1145/2809695.2817904
SP  - 441
EP  - 442
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962909447&doi=10.1145%2f2809695.2817904&partnerID=40&md5=5b17645e6efcdf2f1c9b7d9be1212204
DB  - Scopus
KW  - Application programs
KW  - Quality of service
KW  - Optimization
KW  - Economic and social effects
KW  - Energy utilization
KW  - Energy conservation
KW  - Energy
KW  - Network architecture
KW  - Synchronization
KW  - Wireless sensor networks
KW  - Sensor nodes
KW  - Embedded systems
KW  - C (programming language)
KW  - Model driven approach
KW  - Total energy consumption
KW  - Component
KW  - Functional relationship
KW  - Sensor network applications
KW  - Time synchronization
KW  - Time synchronization errors
ER  - 

TY  - CONF
TI  - Optimizing Energy Efficiency of Node.js Applications with CPU DVFS Awareness
AU  - Patrou, M.
AU  - Kent, K.B.
AU  - Siu, J.
AU  - Dawson, M.
T2  - 2022 IEEE 13th International Green and Sustainable Computing Conference, IGSC 2022
AB  - Node.js applications can incorporate CPU Dynamic Voltage and Frequency Scaling (DVFS) to adjust their energy consumption and runtime performance. Thus, we build a CPU frequency scaling policy that promotes 'green' and high-performing requests and enables customizations of their execution profile. Our technique requires a profiling step to classify the web requests based on the CPU frequency impact on their energy consumption and runtime performance and on their code syntax/paradigm. We also include the case of concurrent request execution in our model to select an appropriate CPU frequency. We enable priority-based requests to work along with this model for users to customize and formulate a policy based on their goals. Finally, we perform an energy-runtime analysis, which shows that our policy with the proposed configurations is an energy-efficient approach compared to the Linux scaling governors.  © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/IGSC55832.2022.9969367
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145440659&doi=10.1109%2fIGSC55832.2022.9969367&partnerID=40&md5=c768dcfc730f42b7b97d1f2ae74e6b74
DB  - Scopus
KW  - Energy efficiency
KW  - Energy-consumption
KW  - Energy utilization
KW  - Dynamic frequency scaling
KW  - Frequency-scaling
KW  - Voltage scaling
KW  - Dynamic voltage and frequency scaling
KW  - Computer operating systems
KW  - Concurrent requests
KW  - Customisation
KW  - Optimizing energy
KW  - Policy-based
KW  - Priority-based
KW  - Runtime performance
KW  - Web requests
ER  - 

TY  - CONF
TI  - Demeter: QoS-Aware CPU Scheduling to Reduce Power Consumption of Multiple Black-Box Workloads
AU  - Tang, W.
AU  - Ke, Y.
AU  - Fu, S.
AU  - Jiang, H.
AU  - Wu, J.
AU  - Peng, Q.
AU  - Gao, F.
T2  - SoCC 2022 - Proceedings of the 13th Symposium on Cloud Computing
AB  - Energy consumption in cloud data centers has become an increasingly important contributor to greenhouse gas emissions and operation costs. To reduce energy-related costs and improve environmental sustainability, most modern data centers consolidate Virtual Machine (VM) workloads belonging to different application classes, some being latency-critical (LC) and others being more tolerant to performance changes, known as best-effort (BE). However, in public cloud scenarios, the real classes of applications are often opaque to data center operators. The heterogeneous applications from different cloud tenants are usually consolidated onto the same hosts to improve energy efficiency, but it is not trivial to guarantee decent performance isolation among colocated workloads. We tackle the above challenges by introducing Demeter, a QoS-aware power management controller for heterogeneous black-box workloads in public clouds. Demeter is designed to work without offline profiling or prior knowledge about black-box workloads. Through the correlation analysis between network throughput and CPU resource utilization, Demeter automatically classifies black-box workloads as either LC or BE. By provisioning differentiated CPU management strategies (including dynamic core allocation and frequency scaling) to LC and BE workloads, Demeter achieves considerable power savings together with a minimum impact on the performance of all workloads. We discuss the design and implementation of Demeter in this work, and conduct extensive experimental evaluations to reveal its effectiveness. Our results show that Demeter not only meets the performance demand of all workloads, but also responds quickly to dynamic load changes in our cloud environment. In addition, Demeter saves an average of 10.6% power consumption than state of the art mechanisms.  © 2022 ACM.
DA  - 2022///
PY  - 2022
DO  - 10.1145/3542929.3563476
SP  - 31
EP  - 46
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143256886&doi=10.1145%2f3542929.3563476&partnerID=40&md5=86f69997bef090db61a3eb01f41c1bb7
DB  - Scopus
KW  - cloud computing
KW  - power management
KW  - quality of service
KW  - workload characterization
KW  - Energy efficiency
KW  - Performance
KW  - Sustainable development
KW  - Greenhouse gases
KW  - Quality of service
KW  - Cloud-computing
KW  - Datacenter
KW  - Green computing
KW  - Dynamic frequency scaling
KW  - Quality-of-service
KW  - Scheduling algorithms
KW  - Electric power utilization
KW  - Memory architecture
KW  - Gas emissions
KW  - Public clouds
KW  - Best-effort
KW  - Black boxes
KW  - QoS-aware
KW  - Workload characterization
KW  - Dynamic loads
KW  - Demeter
ER  - 

TY  - CONF
TI  - Fewer cores, more hertz: Leveraging high-frequency cores in the OS scheduler for improved application performance
AU  - Gouicem, R.
AU  - Carver, D.
AU  - Lozi, J.-P.
AU  - Sopena, J.
AU  - Lepers, B.
AU  - Zwaenepoel, W.
AU  - Palix, N.
AU  - Lawall, J.
AU  - Muller, G.
T2  - Proceedings of the 2020 USENIX Annual Technical Conference, ATC 2020
AB  - In modern server CPUs, individual cores can run at different frequencies, which allows for fine-grained control of the performance/energy tradeoff. Adjusting the frequency, however, incurs a high latency. We find that this can lead to a problem of frequency inversion, whereby the Linux scheduler places a newly active thread on an idle core that takes dozens to hundreds of milliseconds to reach a high frequency, just before another core already running at a high frequency becomes idle. In this paper, we first illustrate the significant performance overhead of repeated frequency inversion through a case study of scheduler behavior during the compilation of the Linux kernel on an 80-core Intel™ Xeon-based machine. Following this, we propose two strategies to reduce the likelihood of frequency inversion in the Linux scheduler. When benchmarked over 60 diverse applications on the Intel™ Xeon, the better performing strategy, Smove, improves performance by more than 5% (at most 56% with no energy overhead) for 23 applications, and worsens performance by more than 5% (at most 8%) for only 3 applications. On a 4-core AMD Ryzen we obtain performance improvements up to 56%. Copyright © Proc. of the 2020 USENIX Annual Technical Conference, ATC 2020. All rights reserved.
DA  - 2020///
PY  - 2020
SP  - 435
EP  - 448
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091917647&partnerID=40&md5=e4f1c220f6d833f03fd82d5696b87618
DB  - Scopus
KW  - Linux
KW  - Scheduling
KW  - Program processors
KW  - Application performance
KW  - Diverse applications
KW  - Different frequency
KW  - Energy overheads
KW  - Fine-grained control
KW  - High frequency HF
KW  - Linux schedulers
KW  - Repeated frequencies
ER  - 

TY  - CONF
TI  - DeepDive: Transparently identifying and managing performance interference in virtualized environments
AU  - Novaković, D.
AU  - Vasić, N.
AU  - Novaković, S.
AU  - Kostić, D.
AU  - Bianchini, R.
T2  - Proceedings of the 2013 USENIX Annual Technical Conference, USENIX ATC 2013
AB  - We describe the design and implementation of DeepDive, a system for transparently identifying and managing performance interference between virtual machines (VMs) co-located on the same physical machine in Infrastructure-as-a-Service cloud environments. DeepDive successfully addresses several important challenges, including the lack of performance information from applications, and the large overhead of detailed interference analysis. We first show that it is possible to use easily-obtainable, low-level metrics to clearly discern when interference is occurring and what resource is causing it. Next, using realistic workloads, we show that DeepDive quickly learns about interference across co-located VMs. Finally, we show DeepDive's ability to deal efficiently with interference when it is detected, by using a low-overhead approach to identifying a VM placement that alleviates interference. © USENIX Annual Technical Conference, USENIX ATC 2013. All rights reserved.
DA  - 2019///
PY  - 2019
SP  - 219
EP  - 230
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077182637&partnerID=40&md5=9e5bed2125480098d4735afef7666ac9
DB  - Scopus
KW  - Virtual reality
KW  - Virtualized environment
KW  - Infrastructure as a service (IaaS)
KW  - Design and implementations
KW  - Co-located
KW  - Interference analysis
KW  - Low overhead
KW  - VM placements
ER  - 

TY  - JOUR
TI  - Evaluation of CPU frequency transition latency
AU  - Mazouz, A.
AU  - Laurent, A.
AU  - Pradelle, B.
AU  - Jalby, W.
T2  - Computer Science - Research and Development
AB  - Dynamic Voltage and Frequency Scaling (DVFS) has appeared as one of the most important techniques to reduce energy consumption in computing systems. The main idea exploited by DVFS controllers is to reduce the CPU frequency in memory-bound phases, usually significantly reducing the energy consumption. However, depending on the CPU model, transitions between CPU frequencies may imply varying delays. Such delays are often optimistically ignored in DVFS controllers, whereas their knowledge could enhance the quality of frequency setting decisions. The current article presents an experimental study on the measurement of frequency transition latencies. The measurement methodology is presented accompanied with evaluations on three Intel machines, reflecting three distinct micro-architectures. In overall, we show for our experimental setup that, while changing CPU frequency upward leads to higher transition delays, changing it downward leads to smaller or similar transition delays across the set of available frequencies. © 2013 Springer-Verlag Berlin Heidelberg.
DA  - 2014///
PY  - 2014
DO  - 10.1007/s00450-013-0240-x
VL  - 29
IS  - 3-4
SP  - 187
EP  - 195
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904734533&doi=10.1007%2fs00450-013-0240-x&partnerID=40&md5=0752ca4094777b00087c3d2f87138cbd
DB  - Scopus
KW  - Computer architecture
KW  - Energy utilization
KW  - Dynamic frequency scaling
KW  - Voltage scaling
KW  - DVFS
KW  - Performances evaluation
KW  - Computing system
KW  - Reduce energy consumption
KW  - Dynamic voltage and frequency scaling
KW  - Frequency transition
KW  - Frequency transition latency
KW  - Memory bounds
KW  - Statistical performance
KW  - Statistical performance evaluation
KW  - Transition delays
ER  - 

TY  - JOUR
AU  - Mei, X.
AU  - Wang, Q.
AU  - Chu, X.
AU  - Liu, H.
AU  - Leung, Y.-W.
AU  - Li, Z.
T2  - Energy-aware Task Scheduling with Deadline Constraint in DVFSenabled Heterogeneous Clusters
DA  - 2021///
PY  - 2021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115721976&partnerID=40&md5=ade44e244f438a038493806d2c3bc8b9
DB  - Scopus
ER  - 

TY  - CONF
TI  - Zen: A next-generation high-performance ×86 core
AU  - Singh, T.
AU  - Rangarajan, S.
AU  - John, D.
AU  - Henrion, C.
AU  - Southard, S.
AU  - McIntyre, H.
AU  - Novak, A.
AU  - Kosonocky, S.
AU  - Jotwani, R.
AU  - Schaefer, A.
AU  - Chang, E.
AU  - Bell, J.
AU  - Co, M.
T2  - Digest of Technical Papers - IEEE International Solid-State Circuits Conference
AB  - Codenamed 'Zen', AMD's next-generation, high-performance ×86 core targets server, desktop, and mobile client applications. Utilizing Global Foundries' energy-efficient 14nm LPP FinFET process, the 44mm2 Zen core complex unit (CCX) has 1.4B transistors and contains a shared 8MB L3 cache and four cores (Fig. 3.2.7). The 7mm2 Zen core contains a dedicated 0.5MB L2 cache, 32KB L1 data cache, and 64KB L1 instruction cache. Each core has a digital low drop-out (LDO) voltage regulator and digital frequency synthesizer (DFS) to independently vary frequency and voltage across power states. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/ISSCC.2017.7870256
VL  - 60
SP  - 52
EP  - 53
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016245760&doi=10.1109%2fISSCC.2017.7870256&partnerID=40&md5=8bfadf9ab8cd6f2f84b95a8715830368
DB  - Scopus
KW  - Energy efficiency
KW  - Energy efficient
KW  - Cache memory
KW  - Mobile client
KW  - Voltage regulators
KW  - Power state
KW  - Core complex
KW  - Data caches
KW  - Digital frequency synthesizer
KW  - Instruction caches
KW  - Low drop outs
ER  - 

TY  - JOUR
TI  - Understanding the future of energy-performance trade-off via DVFS in HPC environments
AU  - Etinski, M.
AU  - Corbalan, J.
AU  - Labarta, J.
AU  - Valero, M.
T2  - Journal of Parallel and Distributed Computing
AB  - DVFS is a ubiquitous technique for CPU power management in modern computing systems. Reducing processor frequency/voltage leads to a decrease of CPU power consumption and an increase in the execution time. In this paper, we analyze which application/platform characteristics are necessary for a successful energy-performance trade-off of large scale parallel applications. We present a model that gives an upper bound on performance loss due to frequency scaling using the application parallel efficiency. The model was validated with performance measurements of large scale parallel applications. Then we track how application sensitivity to frequency scaling evolved over the last decade for different cluster generations. Finally, we study how cluster power consumption characteristics together with application sensitivity to frequency scaling determine the energy effectiveness of the DVFS technique. © 2012 Elsevier Inc. All rights reserved.
DA  - 2012///
PY  - 2012
DO  - 10.1016/j.jpdc.2012.01.006
VL  - 72
IS  - 4
SP  - 579
EP  - 590
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857792797&doi=10.1016%2fj.jpdc.2012.01.006&partnerID=40&md5=e3add416ae41fa32d7afa62b6b82d459
DB  - Scopus
KW  - Energy efficiency
KW  - Frequency-scaling
KW  - DVFS
KW  - Computing system
KW  - Energy management
KW  - Performance loss
KW  - High performance computing
KW  - Parallel application
KW  - Execution time
KW  - CPU power
KW  - Energy effectiveness
KW  - Parallel efficiency
KW  - Performance measurements
KW  - Upper Bound
ER  - 

TY  - CONF
TI  - Energy-efficient scheduling policy for collaborative execution in mobile cloud computing
AU  - Zhang, W.
AU  - Wen, Y.
AU  - Wu, D.O.
T2  - Proceedings - IEEE INFOCOM
AB  - In this paper, we investigate the scheduling policy for collaborative execution in mobile cloud computing. A mobile application is represented by a sequence of fine-grained tasks formulating a linear topology, and each of them is executed either on the mobile device or offloaded onto the cloud side for execution. The design objective is to minimize the energy consumed by the mobile device, while meeting a time deadline. We formulate this minimum-energy task scheduling problem as a constrained shortest path problem on a directed acyclic graph, and adapt the canonical 'LARAC' algorithm to solving this problem approximately. Numerical simulation suggests that a one-climb offloading policy is energy efficient for the Markovian stochastic channel, in which at most one migration from mobile device to the cloud is taken place for the collaborative task execution. Moreover, compared to standalone mobile execution and cloud execution, the optimal collaborative execution strategy can significantly save the energy consumed on the mobile device. © 2013 IEEE.
DA  - 2013///
PY  - 2013
DO  - 10.1109/INFCOM.2013.6566761
SP  - 190
EP  - 194
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883123478&doi=10.1109%2fINFCOM.2013.6566761&partnerID=40&md5=454691d41b512fabc758b416770a60f8
DB  - Scopus
KW  - Energy efficiency
KW  - Energy-Efficient Scheduling
KW  - Mobile cloud computing
KW  - mobile cloud computing
KW  - Computer applications
KW  - Mobile applications
KW  - Problem solving
KW  - Graph theory
KW  - Directed acyclic graph (DAG)
KW  - Mobile devices
KW  - Scheduling policies
KW  - collaborative execution
KW  - Collaborative execution
KW  - Collaborative tasks
KW  - Constrained shortest path
KW  - scheduling policy
KW  - Task scheduling problem
ER  - 

TY  - CONF
TI  - SLA-Aware Resource Scaling for Energy Efficiency
AU  - Alzahrani, E.J.
AU  - Tari, Z.
AU  - Zeephongsekul, P.
AU  - Lee, Y.C.
AU  - Alsadie, D.
AU  - Zomaya, A.Y.
T2  - Proceedings - 18th IEEE International Conference on High Performance Computing and Communications, 14th IEEE International Conference on Smart City and 2nd IEEE International Conference on Data Science and Systems, HPCC/SmartCity/DSS 2016
AB  - Cloud data centers (CDCs) with abundant resource capacities have prevailed in the past decade. However, these CDCs often struggle to efficiently deal with resource provisioning in terms of performance and energy efficiency. In this paper, we present Energy-Based Auto Scaling (EBAS) as a new resource auto-scaling approach - that takes into account Service Level Agreement (SLA) - for CDCs. EBAS proactively scales resources at the CPU core level in terms of both the number and frequency of cores. It incorporates the dynamic voltage and frequency scaling (DVFS) technique to dynamically adjust CPU frequencies. The proactive decisions on resource scaling are enabled primarily by the CPU usage prediction model and the workload consolidation model of EBAS. The experimental results show that EBAS can save energy on average by 14% compared with the Linux governor. In particular, EBAS contributes to enhancing DVFS by making it aware of SLA conditions, which leads to savings of computing power and in turn energy. © 2016 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/HPCC-SmartCity-DSS.2016.0123
SP  - 852
EP  - 859
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013638445&doi=10.1109%2fHPCC-SmartCity-DSS.2016.0123&partnerID=40&md5=42c81357f55b228e276a061a511a2352
DB  - Scopus
KW  - Energy efficiency
KW  - Smart city
KW  - Energy Efficiency
KW  - Dynamic frequency scaling
KW  - Voltage scaling
KW  - Cloud data centers
KW  - Workload consolidation
KW  - Dynamic voltage and frequency scaling
KW  - Computer operating systems
KW  - Abundant resources
KW  - Account services
KW  - Auto-Scaling
KW  - Cloud Data Centers
KW  - Docker Containers
KW  - Prediction model
KW  - Resource Provisioning
ER  - 

TY  - CONF
TI  - Work in progress: Towards a statistical worst-case energy consumption model
AU  - Khazen, M.W.E.
AU  - Amor, S.B.
AU  - Kougblenou, K.
AU  - Gogonel, A.
AU  - Cucu-Grosjean, L.
T2  - Proceedings of the IEEE Real-Time and Embedded Technology and Applications Symposium, RTAS
AB  - In this paper, we provide first results introducing the impact of both software and hardware events on the estimation of worst-case energy consumption of programs on embedded processors. We build a framework to better understand the representativeness of measurements with respect to both software and hardware events. We test this framework on execution times and energy consumption data for 5 existing benchmarks as a step towards a statistical worst-case energy consumption model. © 2023 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/RTAS58335.2023.00034
VL  - 2023-May
SP  - 333
EP  - 336
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164535166&doi=10.1109%2fRTAS58335.2023.00034&partnerID=40&md5=7a51a77564066ee8dbcdb472f842ac66
DB  - Scopus
KW  - Energy-consumption
KW  - Energy utilization
KW  - CPU
KW  - Software and hardwares
KW  - Energy consumption model
KW  - Measurement-based
KW  - Bad-case energy consumption
KW  - Measurement-based benchmark
KW  - measurement-based benchmarks
KW  - SDRAM
KW  - statistical estimator
KW  - Statistical estimators
KW  - WCET
KW  - worst-case energy consumption
ER  - 

TY  - CONF
TI  - DeepScaling: Microservices AutoScaling for Stable CPU Utilization in Large Scale Cloud Systems
AU  - Wang, Z.
AU  - Zhu, S.
AU  - Li, J.
AU  - Jiang, W.
AU  - Ramakrishnan, K.K.
AU  - Zheng, Y.
AU  - Yan, M.
AU  - Zhang, X.
AU  - Liu, A.X.
T2  - SoCC 2022 - Proceedings of the 13th Symposium on Cloud Computing
AB  - Cloud service providers conservatively provision excessive resources to ensure service level objectives (SLOs) are met. They often set lower CPU utilization targets to ensure service quality is not degraded, even when the workload varies significantly. Not only does this potentially waste resources, but it can also consume excessive power in large-scale cloud deployments. This paper aims to minimize resource costs while ensuring SLO requirements are met in a dynamically varying, large-scale production microservice environment. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization to a level that is maintained at a stable value to meet SLO constraints while using minimum resources. First, DeepScaling forecasts the workload for each service using a Spatio-temporal Graph Neural Network. Second, DeepScaling estimates the CPU utilization by mapping the workload intensity to an estimated CPU utilization with a Deep Neural Network, while taking into account multiple factors in the cloud environment (e.g., periodic tasks and traffic). Third, DeepScaling generates an autoscaling policy for each service based on an improved Deep Q Network (DQN). The adaptive autoscaling policy updates the target CPU utilization to be a maximum, stable value, while ensuring SLOs is not violated. We compare DeepScaling with state-of-the-art autoscaling approaches in the large-scale production cloud environment of the Ant Group. It shows that DeepScaling outperforms other approaches both in terms of maintaining stable service performance, and saving resources, by a significant margin. The deployment of DeepScaling in Ant Group's real production environment with 135 microservices saves the provisioning of over 30,000 CPU cores per day, on average.  © 2022 ACM.
DA  - 2022///
PY  - 2022
DO  - 10.1145/3542929.3563469
SP  - 16
EP  - 30
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143253714&doi=10.1145%2f3542929.3563469&partnerID=40&md5=988111d2dde96d83e5b770974715fc93
DB  - Scopus
KW  - Autoscaling
KW  - Deep neural networks
KW  - Cloud environments
KW  - Large-scales
KW  - Cloud service providers
KW  - Cloud systems
KW  - Graph neural networks
KW  - Service level objective
KW  - CPU utilization
KW  - Service Quality
KW  - Large scale productions
KW  - Waste resources
ER  - 

TY  - CONF
TI  - A voltage reduction technique for digital systems
AU  - Macken, Peter
AU  - Degrauwe, Marc
AU  - Van Paemel, Mark
AU  - Oguey, Henri
T2  - Digest of Technical Papers - IEEE International Solid-State Circuits Conference
AB  - A self-regulating on-chip voltage-reduction circuit that adjusts the internal supply voltage to the lowest value compatible with chip speed requirements is described. Besides enhancing reliability, this technique allows power savings. The technique is based on regulation of the supply voltage of an equivalent critical path, a small circuit with delay Vdd properties proportional to those of the actual critical path. The output of this equivalent critical path is compared with the output of a second identical equivalent critical path which is connected to the full supply voltage and serves as a reference. In a first-order approximation, the ratio of the delay of a critical path to the period of a ring oscillator is a constant that depends only on the number of gates, the dimensions of the transistors, and the load capacitances. This means that a ring oscillator can be used as an equivalent critical path for all digital circuits. Moreover, when the supply voltage of a ring oscillator (VCO) is changed, the frequency changes. The voltage regulator principle can be implemented with a phase-locked loop (PLL). By adjusting the VCO supply voltage, the PLL causes the VCO to oscillate at N × fin. If the dimensions of the VCO transistors and the division ratio N are such that the critical path functions correctly at the regulated voltage, it will always function correctly, as changing parameters, temperature, or frequency fin affect the VCO in the same way as the circuitry.
DA  - 1990///
PY  - 1990
DO  - 10.1109/isscc.1990.110213
SP  - 238
EP  - 239
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025450394&doi=10.1109%2fisscc.1990.110213&partnerID=40&md5=d65e292cf943991ec9743940617f47ba
DB  - Scopus
KW  - Chip Speed
KW  - Control, Electric variables
KW  - Digital Chips
KW  - Integrated Circuits, Digital--Design
KW  - Phase locked loops
KW  - Power Savings
KW  - Voltage Reduction Circuit
ER  - 

TY  - JOUR
TI  - Energy efficiency predictions for data centres in 2023
AU  - Law, M.
T2  - Energy Efficiency Predictions for Data Centres in 2023
DA  - 2022///
PY  - 2022
SP  - 1
EP  - 10
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207340176&partnerID=40&md5=fd2f209a56117b5a53940d9849b2fc76
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Energy-efficient DAG scheduling with DVFS for cloud data centers
AU  - Yang, W.
AU  - Zhao, M.
AU  - Li, J.
AU  - Zhang, X.
T2  - Journal of Supercomputing
AB  - With the growth of the cloud computing market, the number and scale of cloud data centers are expanding rapidly. While cloud data centers provide a large amount of computing power, generating tremendous energy consumption has become a fundamental issue in the financial and environmental fields. Improving quality of service and reducing energy costs are fundamental challenges for next-generation cloud data centers. Task scheduling in cloud data centers grows increasingly complex due to the heterogeneity of computing resources, intricate dependencies of jobs and rising expenses resulting from high energy consumption. Efficiently utilizing computing resources is crucial, so it is necessary to develop optimal strategies for job scheduling. This paper proposes a reinforcement learning-based task scheduler (E2DSched) for online scheduling of randomly arriving directed acyclic graph jobs in cloud data centers. E2DSched divides the scheduling process into three layers: task selection layer, server selection layer and frequency control layer. It achieves joint optimization of energy consumption and quality of service through three-layer cooperation. Finally, we compare E2DSched with various other algorithms, and the results show that E2DSched can provide excellent service with less energy consumption. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.
DA  - 2024///
PY  - 2024
DO  - 10.1007/s11227-024-06035-7
VL  - 80
IS  - 10
SP  - 14799
EP  - 14823
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188812348&doi=10.1007%2fs11227-024-06035-7&partnerID=40&md5=71a9884080839772edf0a241deaa3244
DB  - Scopus
KW  - Energy efficiency
KW  - Cloud data center
KW  - DAG scheduling
KW  - Quality of service
KW  - Energy-consumption
KW  - Green computing
KW  - Energy utilization
KW  - Reinforcement learning
KW  - Reinforcement learnings
KW  - Cloud data centers
KW  - Energy efficient
KW  - Quality-of-service
KW  - Scheduling algorithms
KW  - Computing power
KW  - Computing resource
KW  - Directed graphs
KW  - DAG job
KW  - Task schedule
KW  - Three-layer
ER  - 

TY  - JOUR
TI  - Foggy clouds and cloudy fogs: A real need for coordinated management of fog-to-cloud computing systems
AU  - Masip-Bruin, X.
AU  - Marín-Tordera, E.
AU  - Tashakor, G.
AU  - Jukan, A.
AU  - Ren, G.-J.
T2  - IEEE Wireless Communications
AB  - The recent advances in cloud services technology are fueling a plethora of information technology innovation, including networking, storage, and computing. Today, various flavors have evolved of IoT, cloud computing, and so-called fog computing, a concept referring to capabilities of edge devices and users' clients to compute, store, and exchange data among each other and with the cloud. Although the rapid pace of this evolution was not easily foreseeable, today each piece of it facilitates and enables the deployment of what we commonly refer to as a smart scenario, including smart cities, smart transportation, and smart homes. As most current cloud, fog, and network services run simultaneously in each scenario, we observe that we are at the dawn of what may be the next big step in the cloud computing and networking evolution, whereby services might be executed at the network edge, both in parallel and in a coordinated fashion, as well as supported by the unstoppable technology evolution. As edge devices become richer in functionality and smarter, embedding capacities such as storage or processing, as well as new functionalities, such as decision making, data collection, forwarding, and sharing, a real need is emerging for coordinated management of fog-to-cloud (F2C) computing systems. This article introduces a layered F2C architecture, its benefits and strengths, as well as the arising open and research challenges, making the case for the real need for their coordinated management. Our architecture, the illustrative use case presented, and a comparative performance analysis, albeit conceptual, all clearly show the way forward toward a new IoT scenario with a set of existing and unforeseen services provided on highly distributed and dynamic compute, storage, and networking resources, bringing together heterogeneous and commodity edge devices, emerging fogs, as well as conventional clouds. Introduction: The Scenario © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/MWC.2016.7721750
VL  - 23
IS  - 5
SP  - 120
EP  - 128
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012170641&doi=10.1109%2fMWC.2016.7721750&partnerID=40&md5=2679523a2a25e9e75cc2e84900cb7253
DB  - Scopus
KW  - Cloud computing
KW  - Automation
KW  - Smart city
KW  - Internet of things
KW  - Fog
KW  - Decision making
KW  - Distributed computer systems
KW  - Information management
KW  - Network architecture
KW  - Digital storage
KW  - Computing system
KW  - Research challenges
KW  - Intelligent buildings
KW  - Network services
KW  - Cloud computing and networkings
KW  - Comparative performance analysis
KW  - Embedding capacity
KW  - Information technology innovation
KW  - Technology evolution
ER  - 

TY  - JOUR
TI  - Complexity of the discrete time-cost tradeoff problem for project networks
AU  - De, P.
AU  - Dunne, E.J.
AU  - Ghosh, J.B.
AU  - Wells, C.E.
T2  - Operations Research
AB  - This note addresses the discrete version of the well-known time-cost tradeoff problem for project networks, which has been studied previously in the standard project management literature as well as in the related literature on Decision-CPM. All the algorithms proposed thus far for the solution of the general problem exhibit exponential worst-case complexity, with the notable exception of the pseudo-polynomial dynamic program due to Hindelang and Muth. We first demonstrate that this algorithm is flawed, and that when we correct it, it no longer remains pseudo-polynomial. Continuing on in the main result of the note, we show that this is not at all surprising, since the problem is strongly NP-hard. Finally, we discuss the complexities of various network structures and validate an old conjecture that certain structures are necessarily more difficult to solve.
DA  - 1997///
PY  - 1997
DO  - 10.1287/opre.45.2.302
VL  - 45
IS  - 2
SP  - 302
EP  - 306
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031102411&doi=10.1287%2fopre.45.2.302&partnerID=40&md5=a2bdd7f0964f2856908e1847bb2c220a
DB  - Scopus
KW  - Costs
KW  - Algorithms
KW  - Computational complexity
KW  - Problem solving
KW  - Dynamic programming
KW  - Computational methods
KW  - Project management
KW  - Critical path analysis
KW  - Operations research
KW  - Pseudo-polynomial dynamic program
KW  - Statistical methods
KW  - Time-cost tradeoff problem
ER  - 

TY  - JOUR
TI  - TACLeBench: A benchmark collection to support worst-case execution time research
AU  - Falk, H.
T2  - 16th International Workshop on Worst-Case Execution Time Analysis
DA  - 2016///
PY  - 2016
SP  - 1
EP  - 10
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031420216&partnerID=40&md5=b20a6495de47bcd1fe8bab13dd736cec
DB  - Scopus
ER  - 

TY  - CONF
TI  - CloudFreq: Elastic energy-efficient bag-of-tasks scheduling in DVFS-enabled clouds
AU  - Zhang, Y.
AU  - Wang, Y.
AU  - Hu, C.
T2  - Proceedings of the International Conference on Parallel and Distributed Systems - ICPADS
AB  - Energy consumption imposes a significant cost for data centers in providing cloud services. Many studies explore the opportunities to save power by energy-efficient task scheduling based on the technique of dynamic voltage and frequency scaling (DVFS). However, most of them assume that energy budgets and/or deadline constraints are known in advance. But these information can hardly be acquired in general computing environments, such as cloud computing, and job rejections caused by restricted constraints are intolerable to guarantee the service-level agreement (SLA). Moreover, previous works prefer to provide &ldquo;black-box&rdquo; algorithms with little consideration on adjustability, and cannot satisfy runtime requirements in performance and energy-saving. This paper proposes an elastic energy-efficient algorithm called CloudFreq for bag-of-tasks scheduling in DVFS-enabled clouds. Cloud-Freq enables a model of elastic, adjustable energy-efficient scheduling without any prior knowledge of constraints, and then eliminates job rejections accordingly. CloudFreq also provides an entry for operators to scale system performance at runtime. Experimental results demonstrate that the proposed algorithm can effectively perform energy-efficient scheduling without constraints, and has the capability of making an appropriate tradeoff to improve the weighted balance between schedule length and energy-saving. © 2015 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/ICPADS.2015.79
VL  - 2016-January
SP  - 585
EP  - 592
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964626489&doi=10.1109%2fICPADS.2015.79&partnerID=40&md5=49519800d25b87c7e9a1a3e09c29c6e4
DB  - Scopus
KW  - Cloud computing
KW  - Energy efficiency
KW  - Elasticity
KW  - Scheduling
KW  - Green computing
KW  - Energy utilization
KW  - Dynamic frequency scaling
KW  - Voltage scaling
KW  - Energy-Efficient Scheduling
KW  - Energy efficient
KW  - Computing environments
KW  - Budget control
KW  - Service Level Agreements
KW  - Dynamic voltage and frequency scaling
KW  - Bag of tasks
KW  - Bag-of-tasks
KW  - Deadline constraint
KW  - Elastic
KW  - Energy-efficient
ER  - 

TY  - JOUR
T2  - Linux Kernel Scaling Governors
DA  - 2023///
PY  - 2023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218193265&partnerID=40&md5=ecba8e61ffdc1477e3da33f622112e6c
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Reliable benchmarking: requirements and solutions
AU  - Beyer, D.
AU  - Löwe, S.
AU  - Wendler, P.
T2  - International Journal on Software Tools for Technology Transfer
AB  - Benchmarking is a widely used method in experimental computer science, in particular, for the comparative evaluation of tools and algorithms. As a consequence, a number of questions need to be answered in order to ensure proper benchmarking, resource measurement, and presentation of results, all of which is essential for researchers, tool developers, and users, as well as for tool competitions. We identify a set of requirements that are indispensable for reliable benchmarking and resource measurement of time and memory usage of automatic solvers, verifiers, and similar tools, and discuss limitations of existing methods and benchmarking tools. Fulfilling these requirements in a benchmarking framework can (on Linux systems) currently only be done by using the cgroup and namespace features of the kernel. We developed BenchExec, a ready-to-use, tool-independent, and open-source implementation of a benchmarking framework that fulfills all presented requirements, making reliable benchmarking and resource measurement easy. Our framework is able to work with a wide range of different tools, has proven its reliability and usefulness in the International Competition on Software Verification, and is used by several research groups worldwide to ensure reliable benchmarking. Finally, we present guidelines on how to present measurement results in a scientifically valid and comprehensible way. © 2017, Springer-Verlag GmbH Germany.
DA  - 2019///
PY  - 2019
DO  - 10.1007/s10009-017-0469-y
VL  - 21
IS  - 1
SP  - 1
EP  - 29
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032816422&doi=10.1007%2fs10009-017-0469-y&partnerID=40&md5=389f40c9f8dbaf6d6a725493a324edd7
DB  - Scopus
KW  - Container
KW  - Containers
KW  - Competition
KW  - Benchmarking
KW  - Software reliability
KW  - Open source software
KW  - Benchmarking tools
KW  - Computer operating systems
KW  - Open source implementation
KW  - Comparative evaluations
KW  - International competitions
KW  - Linux systems
KW  - Process control
KW  - Process isolation
KW  - Ready to use
KW  - Research groups
KW  - Resource measurement
KW  - Software verification
KW  - Verification
ER  - 

TY  - CONF
TI  - CPU Energy Meter: A Tool for Energy-Aware Algorithms Engineering
AU  - Beyer, D.
AU  - Wendler, P.
T2  - Lecture Notes in Computer Science
AB  - Verification algorithms are among the most resource-intensive computation tasks. Saving energy is important for our living environment and to save cost in data centers. Yet, researchers compare the efficiency of algorithms still in terms of consumption of CPU time (or even wall time). Perhaps one reason for this is that measuring energy consumption of computational processes is not as convenient as measuring the consumed time and there is no sufficient tool support. To close this gap, we contribute CPU Energy Meter, a small tool that takes care of reading the energy values that Intel CPUs track inside the chip. In order to make energy measurements as easy as possible, we integrated CPU Energy Meter into BenchExec, a benchmarking tool that is already used by many researchers and competitions in the domain of formal methods. As evidence for usefulness, we explored the energy consumption of some state-of-the-art verifiers and report some interesting insights, for example, that energy consumption is not necessarily correlated with CPU time. © 2020, The Author(s).
DA  - 2020///
PY  - 2020
DO  - 10.1007/978-3-030-45237-7_8
VL  - 12079 LNCS
SP  - 126
EP  - 133
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083986675&doi=10.1007%2f978-3-030-45237-7_8&partnerID=40&md5=a0a7196e4c0afc8fdd40e5f2086400ac
DB  - Scopus
KW  - Power management
KW  - Energy-consumption
KW  - Program processors
KW  - Energy utilization
KW  - Benchmarking
KW  - Electric power measurement
KW  - Energy Measurement
KW  - RAPL
KW  - Saving energy
KW  - Algorithm engineering
KW  - Benchexec
KW  - BenchExec
KW  - Computation tasks
KW  - CPU time
KW  - Energy aware algorithms
KW  - Energy meters
KW  - Verification algorithms
ER  - 

TY  - JOUR
TI  - Inside 6th-Generation Intel Core: New Microarchitecture Code-Named Skylake
AU  - Doweck, J.
AU  - Kao, W.-F.
AU  - Lu, A.K.-Y.
AU  - Mandelblat, J.
AU  - Rahatekar, A.
AU  - Rappoport, L.
AU  - Rotem, E.
AU  - Yasin, A.
AU  - Yoaz, A.
T2  - IEEE Micro
AB  - Skylake's core, processor graphics, and system on chip were designed to meet a demanding set of requirements for a wide range of power-performance points. Its coherent fabric was designed to provide high-memory bandwidth from multiple memory sources. Skylake's power management, which includes Intel Speed Shift technology, was designed to provide the largest dynamic power range among prior Intel processors. The Intel Architecture core delivers higher power efficiency, higher frequency, and a wider dynamic power range, supporting smaller form factors. Skylake's Gen9 graphics provides new features designed to maximize energy efficiency and bring the best visual experience for gaming and media. Skylake offers a rich performance monitoring unit that enhances software developers' ability to optimize their applications. © 1981-2012 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/MM.2017.38
VL  - 37
IS  - 2
SP  - 52
EP  - 62
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019228010&doi=10.1109%2fMM.2017.38&partnerID=40&md5=103537d56ce36287619a8b7670ce206f
DB  - Scopus
KW  - power management
KW  - Energy efficiency
KW  - GPU
KW  - performance monitoring
KW  - Power management
KW  - Application programs
KW  - Computer architecture
KW  - Energy management
KW  - Graphics processing unit
KW  - Performance monitoring
KW  - Micro architectures
KW  - System-on-chip
KW  - Performance measurements
KW  - eDRAM
KW  - Intel Speed Shift
KW  - microarchitecture
KW  - performance measurements
KW  - Skylake
KW  - Turbo
ER  - 

TY  - JOUR
T2  - ODROID XU3
DA  - 2023///
PY  - 2023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218190892&partnerID=40&md5=72e6500ad037419354549a5a9d1a8e95
DB  - Scopus
ER  - 

TY  - CONF
TI  - Internet of Things Services Orchestration Framework Based on Kubernetes and Edge Computing
AU  - Ermolenko, D.
AU  - Kilicheva, C.
AU  - Muthanna, A.
AU  - Khakimov, A.
T2  - Proceedings of the 2021 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering, ElConRus 2021
AB  - Presented work is fanalysis of how the microservices paradigm can be used to design and implement distributed edge services for Internet of Things (IoT) applications. Basically, IoT is a platform where integrated services are associated with the common network, thus all devices are able to gather and exchange data among each other. Typically, monolithic user mobility research services are developed for the unified ETSI MEC system reference architecture centers. ETSI MEC considers microservices as a tool for breaking monolithic applications into a set of loosely coupled distributed components. It is expected that this architecture will facilitate the dynamic adaptation during the application execution. However, increased modularity can also increase the burden on orchestration and system management. In MEC, user hardware is connected through gateways to microservices running on the edge host.There are three levels in each of the edge systems: 1) microservices perform a logical operation with components for motion track analysis, 2) movement foresight and 3) outcome visualization. The distributed service is realized with Docker containers and calculated on actual world adjustment with low capacity edge servers and real user mobility information. The results demonstrate the fact that the edge perspective of low latency may be encountered in this sort of implementation. The integration of a software creation technology with a standardized edge system supplies respectable basis for subsequent development. The paper considers the application of the boundary computing architecture and Kubernetes as an orchestration and management of network applications. © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/ElConRus51938.2021.9396553
SP  - 12
EP  - 17
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104714605&doi=10.1109%2fElConRus51938.2021.9396553&partnerID=40&md5=29c89717b94344a84352f846729a8419
DB  - Scopus
KW  - IoT
KW  - Kubernetes
KW  - Edge computing
KW  - Internet of things
KW  - Orchestration
KW  - Computer architecture
KW  - Application execution
KW  - Network architecture
KW  - Web services
KW  - Internet of Things (IOT)
KW  - Computing architecture
KW  - Clustering
KW  - Design and implements
KW  - Network applications
KW  - Distributed components
KW  - Reference architecture
KW  - User mobility information
ER  - 

TY  - CONF
TI  - Energy-Efficient Core Allocation and Deployment for Container-Based Virtualization
AU  - Lin, C.-C.
AU  - Chen, J.-J.
AU  - Liu, P.
AU  - Wu, J.-J.
T2  - Proceedings of the International Conference on Parallel and Distributed Systems - ICPADS
AB  - Infrastructure-as-a-Service (IaaS) is a popular form of cloud computing that provides virtualized computing resources. The current trend of IaaS is moving from virtual machine-based into container-based. In this paper, we study the energy-efficient resource allocation problem for container-based virtualization in a data center. Our goal is to minimize the energy consumption by determining 1) the number of cores allocated to a container, 2) the operating frequency of the container, and 3) the deployment of the container to server. Every container has to meet its service level agreement (SLA). We propose dynamic programming algorithms that can be used under different scenarios, depending on the affordable time complexity. The performance of the proposed algorithms is evaluated with energy consumption data collected from experiments. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/PADSW.2018.8644537
VL  - 2018-December
SP  - 93
EP  - 101
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063352888&doi=10.1109%2fPADSW.2018.8644537&partnerID=40&md5=f4ffdb4480d6d1d5f6e2f148ddb36059
DB  - Scopus
KW  - Energy efficiency
KW  - Container
KW  - Cloud Computing
KW  - Containers
KW  - Virtualization
KW  - Energy-consumption
KW  - Cloud-computing
KW  - Datacenter
KW  - Green computing
KW  - Energy utilization
KW  - Resource allocation
KW  - Energy efficient
KW  - 'current
KW  - Virtual reality
KW  - Infrastructure as a service (IaaS)
KW  - Computing resource
KW  - Resources allocation
KW  - Dynamic programming
KW  - Virtualizations
KW  - Energy-efficient resource allocation
KW  - Resource Allocation
KW  - Resource allocation problem
KW  - Dynamic Programming
KW  - Energy-Efficiency
ER  - 

TY  - JOUR
T2  - Nvidia jetson
DA  - 2023///
PY  - 2023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193760557&partnerID=40&md5=04b692e4e005441b08c2ca0a0216bf11
DB  - Scopus
ER  - 

TY  - CONF
TI  - Energy-Aware Workflow Scheduling Using Frequency Scaling
AU  - Pietri, I.
AU  - Sakellariou, R.
T2  - Proceedings of the International Conference on Parallel Processing Workshops
AB  - Dynamic Voltage and Frequency Scaling (DVFS) is a power management technique used to decrease the processor frequency and minimize power consumption in modern computing systems. This may lead to higher energy savings for large-scale computational problems, with scientific workflows comprising an important category of applications among these. However, as frequency scaling may result in increased execution time overall, idle time on the processors may also increase, to such a degree that any gains in power are annulled, this depends on the system and workflow characteristics. In this paper, we propose a scheduling algorithm that adopts frequency scaling to reduce overall energy consumption of scientific workflows given an allocation of tasks onto machines and a deadline to complete the execution. Based on the observation that using the lowest possible frequency may not necessarily be energy-efficient, the proposed algorithm works iteratively to scale the frequency further and distribute any slack time, only when overall energy consumption can be decreased. Synthetic data based on parameters of real scientific workflows are used in the evaluation. The results show that the proposed algorithm can achieve energy savings, sometimes at the expense of execution time to reduce the idle time of the processors and decrease overall energy consumption. © 2014 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/ICPPW.2014.26
VL  - 2015-May
SP  - 104
EP  - 113
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946568373&doi=10.1109%2fICPPW.2014.26&partnerID=40&md5=78127246419cc2193a6bbad390d1d0fb
DB  - Scopus
KW  - Energy efficiency
KW  - workflow
KW  - Scheduling
KW  - DAG scheduling
KW  - Green computing
KW  - Energy utilization
KW  - Dynamic frequency scaling
KW  - Frequency-scaling
KW  - Voltage scaling
KW  - DVFS
KW  - Scheduling algorithms
KW  - Energy-aware scheduling
KW  - Iterative methods
KW  - Parallel processing systems
KW  - energy-aware scheduling
KW  - frequency scaling
ER  - 

TY  - JOUR
T2  - CPU frequency trace low
DA  - 2023///
PY  - 2023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218215321&partnerID=40&md5=f06f5dcfa50f57dd2099aa68a11caa2a
DB  - Scopus
ER  - 

TY  - CONF
TI  - RAEF: Energy-efficient Resource Allocation through Energy Fungibility in Serverless
AU  - Jia, X.
AU  - Zhao, L.
T2  - Proceedings of the International Conference on Parallel and Distributed Systems - ICPADS
AB  - Datacenters' excessive energy consumption has become an increasingly significant pain point to cloud service providers. However, existing research usually tends to focus on the server power cap to increase application throughput and do not address the rapid growth in the energy consumption of datacenters. Although the serverless architecture makes it more difficult to improve datacenter energy efficiency due to its high density and high dynamic nature, we observe that the emerging serverless workloads bring new opportunities for energy reduction. First, the energy consumption of serverless workloads during their execution time can be divided into three stages with clear boundaries: startup, runtime and idle. As the three stages have quite different energy usage patterns, we are able to reduce overall energy consumption through orchestrating the energy consumption of the stages. Second, we observe the energy fungibility phenomenon, i.e., the different combinations of multidimensional resource allocations may lead to the same latency but different energy consumption. Exploiting the balanced combination of energy consumption and performance, we can reduce the overall energy consumption without violating the latency service level agreement (SLA). Based on this, we propose RAEF, a function-level resource allocator that proactively adjusts the resources of the functions for minimizing the energy consumption with SLA guarantees. Evaluation results demonstrate that RAEF reduces energy consumption by up to 21.2 % compared to the state-of-the-art technique while guaranteeing the SLA.  © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/ICPADS53394.2021.00060
VL  - 2021-December
SP  - 434
EP  - 441
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129871862&doi=10.1109%2fICPADS53394.2021.00060&partnerID=40&md5=1b3811b7386c38d53c0767e62f2d8bb2
DB  - Scopus
KW  - resource allocation
KW  - Energy efficiency
KW  - serverless
KW  - energy consumption
KW  - Serverless
KW  - Energy-consumption
KW  - Datacenter
KW  - Green computing
KW  - Energy utilization
KW  - Resource allocation
KW  - Servicelevel agreement (SLA)
KW  - Energy
KW  - Power
KW  - Cloud service providers
KW  - Resources allocation
KW  - Energy-efficient resource allocation
KW  - Excessive energy
KW  - system design
ER  - 

TY  - CHAP
TI  - Energy-Efficient and SLA-Based Resource Management in Cloud Data Centers
AU  - Sampaio, A.M.
AU  - Barbosa, J.G.
T2  - Advances in Computers
AB  - Nowadays, cloud data centers play an important role in modern Information Technology (IT) infrastructures, being progressively adopted in different scenarios. The proliferation of cloud has led companies and resource providers to build large warehouse-sized data centers, in an effort to respond to costumers demand for computing resources. Operating with powerful data centers requires a significant amount of electrical power, which translates into more heat to dissipate, possible thermal imbalances, and increased electricity bills. On the other hand, as data centers grow in size and in complexity, failure events become norms instead of exceptions. However, failures contribute to the energy waste as well, since preceding work of terminated tasks is lost. Therefore, today's cloud data centers are faced with the challenge of reducing operational costs through improved energy utilization while provisioning dependable service to customers. This chapter discusses the causes of power and energy consumption in data centers. The advantages brought by cloud computing on the management of data center resources are discussed, and the state of the art on schemes and strategies to improve power and energy efficiency of computing resources is reviewed. A practical case of energy-efficient and service-level agreement (SLA)-based management of resources, which analyzes and discusses the performance of three state-of-the-art scheduling algorithms to improve energy efficiency, is also included. This chapter concludes with a review of open challenges on strategies to improve power and energy efficiency in data centers. © 2016 Elsevier Inc.
DA  - 2016///
PY  - 2016
VL  - 100
SP  - 103
EP  - 159
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953272973&doi=10.1016%2fbs.adcom.2015.11.002&partnerID=40&md5=e9f4663c941c897974bec1ec541ae910
DB  - Scopus
KW  - Cloud computing
KW  - Virtualization
KW  - Interference
KW  - Power and energy efficiency
KW  - Proactive fault tolerance
KW  - Service-level agreement
ER  - 

TY  - CONF
TI  - Energy and Runtime Performance Optimization of Node.js Web Requests
AU  - Patrou, M.
AU  - Kent, K.B.
AU  - Siu, J.
AU  - Dawson, M.
T2  - Proceedings - 2021 IEEE International Conference on Cloud Engineering, IC2E 2021
AB  - The Node.js framework uses an event-driven model with a single-threaded event loop and provides asynchronous and non-blocking I/O operations. As with other programs, Node.js web applications take advantage of underlying resources, including CPUs, which can incorporate the dynamic voltage and frequency scaling (DVFS) technique. Using CPU DVFS, the applications can increase their runtime performance, at the expense of the system's energy consumption. Thus, software code that utilizes the CPU DVFS technique efficiently should lead to 'green' and high-performing applications with respect to the business logic. To this end, we build a CPU frequency scaling/energy aware system to enable CPU frequency control within Node.js applications and measure the energy consumption of specific tasks. We also build a benchmark suite to analyze the energy consumption and runtime performance of different requests based on the CPU frequency impact and collect information and patterns, as we scale the CPU frequency. The analysis aims to provide data and knowledge on the CPU frequency 'suitability' and impact in order to create a model for CPU frequency scaling on Node.js web applications and achieve an efficient and sustainable runtime performance.  © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/IC2E52221.2021.00021
SP  - 71
EP  - 82
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123217432&doi=10.1109%2fIC2E52221.2021.00021&partnerID=40&md5=bce669e715fcf15767034795a52f4062
DB  - Scopus
KW  - Application programs
KW  - green energy
KW  - Green energy
KW  - Energy-consumption
KW  - Green computing
KW  - Program processors
KW  - Energy utilization
KW  - Dynamic frequency scaling
KW  - Voltage scaling
KW  - Benchmarking
KW  - WEB application
KW  - Energy performance
KW  - Dynamic voltage and frequency scaling
KW  - Web applications
KW  - Runtime performance
KW  - CPU DVFS
KW  - CPU dynamic voltage and frequency scaling
KW  - efficiency
KW  - Js
KW  - Node.
KW  - Node.js
ER  - 

TY  - JOUR
TI  - Approximation algorithms for the discrete Time-Cost Tradeoff Problem
AU  - Skutella, M.
T2  - Mathematics of Operations Research
AB  - Due to its obvious practical relevance, the Time-Cost Tradeoff Problem has attracted the attention of many researchers over the last forty years. While the Linear Time-Cost Tradeoff Problem can be solved in polynomial time, its discrete variant is known to be NP-hard. We present the first approximation algorithms for the Discrete Time-Cost Tradeoff Problem. Specifically, given a fixed budget we consider the problem of finding a shortest schedule for a project. We give an approximanon algorithm with performance ratio 3/2 for the class of projects where all feasible durations of activities are either 0, 1, or 2. We extend our result by giving approximation algorithms with performance guarantee O( log I), where I is the ratio of the maximum duration of any activity to the minimum nonzero duration of any activity. Finally, we discuss bicriteria approximation algorithms which compute schedules for a given deadline or budget such that both project duration and cost are within a constant factor of the duration and cost of an optimum schedule for the given deadline or budget.
DA  - 1998///
PY  - 1998
DO  - 10.1287/moor.23.4.909
VL  - 23
IS  - 4
SP  - 909
EP  - 929
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032208139&doi=10.1287%2fmoor.23.4.909&partnerID=40&md5=60c1c941907d02476b4396113983e8ce
DB  - Scopus
KW  - Scheduling
KW  - Costs
KW  - Optimization
KW  - Bicriteria optimization
KW  - Algorithms
KW  - Computational complexity
KW  - Polynomials
KW  - Problem solving
KW  - Approximation theory
KW  - Approximation algorithm
KW  - Bicriteria approximation algorithms
KW  - Discrete time-cost tradeoffs
KW  - Time-cost tradeoff
ER  - 

TY  - JOUR
TI  - EneX: An Energy-Aware Execution Scheduler for Serverless Computing
AU  - Rastegar, S.H.
AU  - Shafiei, H.
AU  - Khonsari, A.
T2  - IEEE Transactions on Industrial Informatics
AB  - The emerging serverless computing paradigm has recently attracted huge attention from both academia and industry. It brings benefits, such as less operational complexity, high scalability and availability, and lower costs. Serverless applications are usually partitioned into several chains of functions. The serverless provider should schedule functions for execution per customers' requests considering their chained nature. Also, the existing scheduling mechanisms for serverless platforms pay little attention to the reduction of energy consumption during functions' execution. To fill this gap, we present an energy-aware execution scheduler for serverless service providers named EneX. To do so, we formulate the minimization of energy consumption for executing the incoming chains of functions with specified computational loads and deadlines. Due to the intractability of the problem, we introduce a linear programming reformulation based on which, we propose an online scheduler. Finally, our experiments demonstrate the significant improvement of EneX in terms of energy efficiency.  © 2005-2012 IEEE.
DA  - 2024///
PY  - 2024
DO  - 10.1109/TII.2023.3290985
VL  - 20
IS  - 2
SP  - 2342
EP  - 2353
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163532892&doi=10.1109%2fTII.2023.3290985&partnerID=40&md5=473905e516b7bc293804e36a17dd8bf0
DB  - Scopus
KW  - scheduling
KW  - Energy efficiency
KW  - energy consumption
KW  - serverless computing
KW  - Serverless computing
KW  - Cloud services
KW  - function-as-a-service (FaaS)
KW  - Power management
KW  - Scheduling
KW  - Function-as-a-service
KW  - Energy-consumption
KW  - Green computing
KW  - Energy utilization
KW  - Processor scheduling
KW  - Web services
KW  - Job shop scheduling
KW  - Delay
KW  - Chains
KW  - Linear programming
KW  - Job-Shop scheduling
KW  - Chain of function
KW  - Chain of functions
KW  - cloud services
ER  - 

TY  - JOUR
TI  - DVFaaS: Leveraging DVFS for FaaS Workflows
AU  - Tzenetopoulos, A.
AU  - Masouros, D.
AU  - Soudris, D.
AU  - Xydis, S.
T2  - IEEE Computer Architecture Letters
AB  - In this letter, we propose DVFaaS, a per-core DVFS framework that utilizes control systems theory to assign just-enough frequency for the purpose of addressing the QoS requirements on serverless workflows comprising unseen functions. DVFaaS exploits the intermittent nature of serverless workflows, which enables staged control on distinguishable functions, which jointly contribute to the end-to-end latency. Our results show that DVFaaS considerably outperforms related work, reducing power consumption by up to 22%, with 2x fewer QoS violations.  © 2002-2011 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/LCA.2023.3288089
VL  - 22
IS  - 2
SP  - 85
EP  - 88
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162865040&doi=10.1109%2fLCA.2023.3288089&partnerID=40&md5=7cedf4684268ea2184a1e90e6f079c40
DB  - Scopus
KW  - power management
KW  - Performance
KW  - distributed systems
KW  - Quality of service
KW  - Dynamic frequency scaling
KW  - Voltage scaling
KW  - Fine grained
KW  - Quality-of-service
KW  - Quality control
KW  - Computing power
KW  - Electric power utilization
KW  - Computation theory
KW  - Timing circuits
KW  - Work-flows
KW  - Power demands
KW  - Frequency measurements
KW  - QoS requirements
KW  - emerging technologies
KW  - Frequency control
KW  - Quality of Services
KW  - Time-frequency Analysis
KW  - Timing
ER  - 

TY  - JOUR
T2  - CPU frequency trace high
DA  - 2023///
PY  - 2023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218192387&partnerID=40&md5=75b760bb0876a97aab8e7c811d47e95e
DB  - Scopus
ER  - 

TY  - CONF
TI  - Learning-directed dynamic voltage and frequency scaling for computation time prediction
AU  - Chang, M.-F.
AU  - Liang, W.-Y.
T2  - Proc. 10th IEEE Int. Conf. on Trust, Security and Privacy in Computing and Communications, TrustCom 2011, 8th IEEE Int. Conf. on Embedded Software and Systems, ICESS 2011, 6th Int. Conf. on FCST 2011
AB  - Dynamic voltage and frequency scaling (DVFS) is an effective technique for reducing power consumption. A number of DVFS researches apply learning methods in an attempt to approach the DVFS prediction model instead of using complicated mathematical models. In this paper, we propose a lightweight learning-directed DVFS technique using Counter Propagation Networks (CPN) to identify the task behavior and predict the corresponding voltage/frequency setting precisely. An adjustable performance mechanism is also provided to users that have diverse performance requirement. The algorithm has been implemented on the Linux operating system and used a PXA270 development board. The results show that the learning-directed DVFS method could accurately predict the suitable frequency, given runtime statistics information of a running program. In this way, the user can easily control the energy consumption by specifying allowable performance loss factor. © 2011 IEEE.
DA  - 2011///
PY  - 2011
DO  - 10.1109/TrustCom.2011.140
SP  - 1023
EP  - 1029
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856157361&doi=10.1109%2fTrustCom.2011.140&partnerID=40&md5=34e71eebac01ca39db64af1adaa2a5dc
DB  - Scopus
KW  - neural network
KW  - Forecasting
KW  - Software design
KW  - Energy utilization
KW  - DVFS
KW  - Neural networks
KW  - Performance loss
KW  - Performance requirements
KW  - Runtimes
KW  - Dynamic voltage and frequency scaling
KW  - Mathematical models
KW  - Computer operating systems
KW  - Low Power
KW  - Embedded systems
KW  - Embedded software
KW  - Prediction model
KW  - Computation time
KW  - Counter propagation networks
KW  - CPN
KW  - Embedded System
KW  - Learning methods
KW  - LINUX- operating system
KW  - Low Power Software Design
KW  - Reducing power
ER  - 

TY  - CONF
TI  - Scheduling for reduced cpu energy
AU  - Weiser, M.
AU  - Welch, B.
AU  - Demers, A.
AU  - Shenker, S.
T2  - Proceedings of the 1st USENIX Conference on Operating Systems Design and Implementation, OSDI 1994
AB  - The energy usage of computer systems is becoming more important, especially for battery operated systems. Displays, disks, and cpus, in that order, use the most energy. Reducing the energy used by displays and disks has been studied elsewhere; this paper considers a new method for reducing the energy used by the cpu. We introduce a new metric for cpu energy performance, millions-of-instructions-per-joule (MIPJ). We examine a class of methods to reduce MIPJ that are characterized by dynamic control of system clock speed by the operating system scheduler. Reducing clock speed alone does not reduce MIPJ, since to do the same work the system must run longer. However, a number of methods are available for reducing energy with reduced clock-speed, such as reducing the voltage [Chandrakasan et al 1992] [Horowitz 1993] or using reversible [Younis and Knight 1993] or adiabatic logic [Athas et al 1994]. What are the right scheduling algorithms for taking advantage of reduced clock-speed, especially in the presence of applications demanding ever more instructions-per-second? We consider several methods for varying the clock speed dynamically under control of the operating system, and examine the performance of these methods against workstation traces. The primary result is that by adjusting the clock speed at a fine grain, substantial CPU energy can be saved with a limited impact on performance.
DA  - 1994///
PY  - 1994
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029600625&partnerID=40&md5=6346948850b6d57af72221896fafb9b5
DB  - Scopus
KW  - Scheduling
KW  - Scheduling algorithms
KW  - Energy performance
KW  - Systems analysis
KW  - Speed
KW  - Energy usage
KW  - Dynamic controls
KW  - Adiabatic logic
KW  - Battery-operated systems
KW  - Class of methods
KW  - Clocks
KW  - Instructions per seconds
KW  - Number of methods
ER  - 

TY  - CONF
TI  - Prediction-based power oversubscription in cloud platforms
AU  - Kumbhare, A.
AU  - Azimi, R.
AU  - Manousakis, I.
AU  - Bonde, A.
AU  - Frujeri, F.
AU  - Mahalingam, N.
AU  - Misra, P.A.
AU  - Javadi, S.A.
AU  - Schroeder, B.
AU  - Fontoura, M.
AU  - Bianchini, R.
T2  - 2021 USENIX Annual Technical Conference
AB  - Prior work has used power capping to shave rare power peaks and add more servers to a datacenter, thereby oversubscribing its resources and lowering capital costs. This works well when the workloads and their server placements are known. Unfortunately, these factors are unknown in public clouds, forcing providers to limit the oversubscription and thus the potential performance loss from power capping. In this paper, we argue that providers can use predictions of workload performance criticality and virtual machine (VM) resource utilization to increase oversubscription. This poses many challenges, such as identifying the performance-critical workloads from opaque VMs, creating support for criticalityaware power management, and increasing oversubscription while limiting the impact of capping. We address these challenges for the hardware and software of Microsoft Azure. The results show that we enable a 2× increase in oversubscription with minimum impact to critical workloads. We describe lessons from deploying our work in production. © 2021 USENIX Annual Technical Conference. All rights reserved.
DA  - 2021///
PY  - 2021
SP  - 65
EP  - 79
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111731471&partnerID=40&md5=404c2325683a9ccae00f90bd01ac22c0
DB  - Scopus
KW  - Windows operating system
KW  - Cloud platforms
KW  - Resource utilizations
KW  - Hardware and software
KW  - Performance loss
KW  - Public clouds
KW  - Power capping
KW  - Prediction-based
KW  - Server placements
ER  - 

TY  - CONF
TI  - SelfWatts: On-the-fly selection of performance events to optimize software-defined power meters
AU  - Fieni, G.
AU  - Rouvoy, R.
AU  - Seiturier, L.
T2  - Proceedings - 21st IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing, CCGrid 2021
AB  - Fine-grained power monitoring of software-defined infrastructures is unavoidable to maximize the power usage efficiency of data centers. However, the design of the underlying power models that estimate the power consumption of the monitored software components keeps being a long and fragile process that remains tightly coupled to the host machine and prevents a wider adoption by the industry beyond the rich literature on this topic.To overcome these limitations, this paper introduces SELFWATTS: a lightweight power monitoring system that explores and selects the relevant performance events to automatically optimize the power models to the underlying architecture. Unlike state-of-the-art techniques, SELFWATTS does not require any a priori training phase or specific hardware to configure the power models and can be deployed on a wide range of machines, including heterogeneous environments. © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/CCGrid51090.2021.00042
SP  - 324
EP  - 333
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114877010&doi=10.1109%2fCCGrid51090.2021.00042&partnerID=40&md5=573bb938fa5dda851d1986875a5c0bc7
DB  - Scopus
KW  - energy
KW  - Computer science
KW  - Cluster computing
KW  - Fine-grained power
KW  - State-of-the-art techniques
KW  - Computer programming
KW  - Heterogeneous environments
KW  - Software component
KW  - Tightly-coupled
KW  - Power monitoring
KW  - performance events
KW  - power meter
KW  - Specific hardware
KW  - Training phase
ER  - 

TY  - JOUR
TI  - GreenEdge: Joint Green Energy Scheduling and Dynamic Task Offloading in Multi-Tier Edge Computing Systems
AU  - Ma, H.
AU  - Huang, P.
AU  - Zhou, Z.
AU  - Zhang, X.
AU  - Chen, X.
T2  - IEEE Transactions on Vehicular Technology
AB  - As mobile edge computing (MEC) emerges as a paradigm to meet the ever-increasing computation demands from real-time Internet of Things (IoT) applications in 5 G era, the development trends of which are mainly divided into two, with one being MEC with advanced computing architectures, and the other being MEC with high efficiency for sustainable operations. We are committed to taking advantage of these two trends to explore a novel multi-tier edge computing scenario with hierarchical task offloading and green energy provisioning via leveraging the energy harvesting (EH) technique. Specifically, we focus on the key problem of joint task offloading and energy scheduling in such green multi-tier edge computing systems. We aim to minimize the task execution cost by jointly considering the system cost that covers latency, energy consumption, and cloud rental fees. By formulating the problem as a stochastic optimization problem, we invoke the Lyapunov technique to decompose the long-term optimization problem into a series of one-slot optimization problems which only use the current system information. To solve the one-slot optimization problem which is a mixed-integer linear problem (MILP) proved to be NP-hard, we first relax the integer variables into real ones to obtain the optimal fractional solutions. Considering the capacity of the physical resources of each edge server, we propose a resource-constrained randomized dependent rounding algorithm to properly round up or down the fractional variables to get a feasible yet near-optimal solution. We conduct rigorous theoretical analysis and extensive simulations to verify the superior performance of the proposed schemes.  © 1967-2012 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/TVT.2022.3147027
VL  - 71
IS  - 4
SP  - 4322
EP  - 4335
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124190337&doi=10.1109%2fTVT.2022.3147027&partnerID=40&md5=295d0ee2d641b0309259034423aa7ffa
DB  - Scopus
KW  - Mobile edge computing
KW  - Edge computing
KW  - Internet of things
KW  - Scheduling
KW  - Costs
KW  - Optimisations
KW  - Computer architecture
KW  - Green energy
KW  - Green products
KW  - Green computing
KW  - Energy utilization
KW  - Task analysis
KW  - task offloading
KW  - Multitasking
KW  - Scheduling algorithms
KW  - Integer programming
KW  - 5G mobile communication systems
KW  - Cost benefit analysis
KW  - Real time systems
KW  - Job analysis
KW  - Energy harvesting
KW  - Task offloading
KW  - Multi-tier
KW  - green energy scheduling
KW  - Green energy scheduling
KW  - Multi-tier edge computing
KW  - randomized dependent rounding
KW  - Randomized dependent rounding
ER  - 

TY  - CONF
TI  - System level analysis of fast, per-core DVFS using on-chip switching regulators
AU  - Kim, W.
AU  - Gupta, M.S.
AU  - Wei, G.-Y.
AU  - Brooks, D.
T2  - Proceedings - International Symposium on High-Performance Computer Architecture
AB  - Portable, embedded systems place ever-increasing demands on high-performance, low-power microprocessor design. Dynamic voltage and frequency scaling (DVFS) is a well-known technique to reduce energy in digital systems, but the effectiveness of DVFS is hampered by slow voltage transitions that occur on the order of tens of microseconds. In addition, the recent trend towards chip-multiprocessors (CMP) executing multi-threaded workloads with heterogeneous behavior motivates the need for per-core DVFS control mechanisms. Voltage regulators that are integrated onto the same chip as the microprocessor core provide the benefit of both nanosecond-scale voltage switching and per-core voltage control. We show that these characteristics provide significant energy-saving opportunities compared to traditional off-chip regulators. However, the implementation of on-chip regulators presents many challenges including regulator efficiency and output voltage transient characteristics, which are significantly impacted by the system-level application of the regulator. In this paper, we describe and model these costs, and perform a comprehensive analysis of a CMP system with on-chip integrated regulators. We conclude that on-chip regulators can significantly improve DVFS effectiveness and lead to overall system energy savings in a CMP, but architects must carefully account for overheads and costs when designing next-generation DVFS systems and algorithms. ©2008 IEEE.
DA  - 2008///
PY  - 2008
DO  - 10.1109/HPCA.2008.4658633
SP  - 123
EP  - 134
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-57749178620&doi=10.1109%2fHPCA.2008.4658633&partnerID=40&md5=832af2f38e87f730477a26c97dc68c55
DB  - Scopus
KW  - Computer architecture
KW  - Computers
KW  - Energy conservation
KW  - Electric potential
KW  - System levels
KW  - Cost benefit analysis
KW  - Comprehensive analysis
KW  - Nanotechnology
KW  - Embedded systems
KW  - Voltage control
KW  - Voltage regulators
KW  - Dynamic Voltage and Frequency Scaling
KW  - Microprocessor chips
KW  - And models
KW  - Control mechanisms
KW  - Digital systems
KW  - DVFS systems
KW  - Integrated circuits
KW  - Low powers
KW  - Microprocessor cores
KW  - Microprocessor designs
KW  - Multi-processors
KW  - Nano-seconds
KW  - On chips
KW  - Output voltages
KW  - Overall systems
KW  - Recent trends
KW  - Switching regulators
KW  - Voltage switching
KW  - Voltage transitions
ER  - 

TY  - JOUR
TI  - PAPSO: A power-aware VM placement technique based on particle swarm optimization
AU  - Ibrahim, A.
AU  - Noshy, M.
AU  - Ali, H.A.
AU  - Badawy, M.
T2  - IEEE Access
AB  - With the widespread usage of cloud computing to benefit from its services, cloud service providers have invested in constructing large scale data centers. Consequently, a tremendous increase in energy consumption has arisen in conjunction with its results, including a remarkable rise in costs of operating and cooling servers. Besides, increasing energy consumption has a significant impact on the environment due to emissions of carbon dioxide. Dynamic consolidation of Virtual Machines (VMs) into the minimal number of Physical Machines (PMs) is considered as one of the magic solutions to manage power consumption. The virtual machine placement problem is a critical issue for good VM consolidation. This paper proposes a Power-Aware technique depending on Particle Swarm Optimization (PAPSO) to determine the near-optimal placement for the migrated VMs. A discrete version of Particle Swarm Optimization (PSO) is adopted based on a decimal encoding to map the migrated VMs to the best appropriate PMs. Furthermore, an effective minimization fitness function is employed to reduce power consumption without violating the Service Level Agreement (SLA). Specifically, PAPSO consolidates the migrated VMs into the minimum number of PMs with a major constraint to decrease the number of overloaded hosts as much as possible. Therefore, the number of VM migrations can be reduced drastically by taking into consideration the main sources for VM migrations; overloaded hosts and underloaded ones. PAPSO is implemented in CloudSim and the experimental results on random workloads with different sizes of VMs and PMs show that PAPSO does not violate SLA and outperforms the Power-Aware Best Fit Decreasing algorithm (PABFD). It can reduce about 8.01%, 39.65%, 66.33%, and 11.87% on average in terms of consumed energy, number of VM migrations, number of host shutdowns and the combined metric Energy SLA Violation (ESV), respectively. © 2013 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ACCESS.2020.2990828
VL  - 8
SP  - 81747
EP  - 81764
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084951357&doi=10.1109%2fACCESS.2020.2990828&partnerID=40&md5=a4dc399316fb336a11ffc8cee200afa1
DB  - Scopus
KW  - Cloud computing
KW  - energy consumption
KW  - service level agreement
KW  - Power management
KW  - Network security
KW  - Green computing
KW  - Carbon dioxide
KW  - Energy utilization
KW  - Virtual machine
KW  - Particle swarm optimization (PSO)
KW  - Electric power utilization
KW  - Cloud service providers
KW  - Large scale data
KW  - Service Level Agreements
KW  - Dynamic consolidation
KW  - Virtual machine placements
KW  - Fitness functions
KW  - Impact on the environment
KW  - CloudSim
KW  - Best fit decreasing
KW  - dynamic VM consolidation
KW  - live VM migration
KW  - particle swarm optimization
KW  - virtual machine placement
ER  - 

TY  - CONF
TI  - Let's stay together: Towards traffic aware virtual machine placement in data centers
AU  - Li, X.
AU  - Wu, J.
AU  - Tang, S.
AU  - Lu, S.
T2  - Proceedings - IEEE INFOCOM
AB  - As tenants take networked virtual machines (VMs) as their requirements, effective placement of VMs is needed to reduce the network cost in cloud data centers. The cost is one of the major concerns for the cloud providers. In addition to the cost caused by network traffics (N-cost), the cost caused by the utilization of physical machines (PM-cost) is also non-negligible. In this paper, we focus on the optimized placement of VMs to minimize the cost, the combination of N-cost and PM-cost. We define N-cost by various functions, according to different communication models. We formulate the placement problem, and prove it to be NP-hard. We investigate the problem from two aspects. Firstly, we put a special emphasis on minimizing the N-cost with fixed PM-cost. For the case that tenants request the same amount of VMs, we present optimal algorithms under various definitions of N-cost. For the case that tenants require different numbers of VMs, we propose an approximation algorithm. Also, a greedy algorithm is implemented as the baseline to evaluate the performance. Secondly, we study the general case of the VM placement problem, in which both N-cost and PM-cost are taken into account. We present an effective binary-search-based algorithm to determine how many PMs should be used, which makes a tradeoff between PM-cost and N-cost. For all of the algorithms, we conduct theoretical analysis and extensive simulations to evaluate their performance and efficiency. © 2014 IEEE.
DA  - 2014///
PY  - 2014
DO  - 10.1109/INFOCOM.2014.6848123
SP  - 1842
EP  - 1850
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904410744&doi=10.1109%2fINFOCOM.2014.6848123&partnerID=40&md5=190bfa601d28767ded4e6c03542d56e4
DB  - Scopus
KW  - data center
KW  - Costs
KW  - Data centers
KW  - Clouds
KW  - Computer simulation
KW  - Virtual machine placements
KW  - Approximation algorithms
KW  - Bin packing
KW  - virtual machine placement
KW  - cost optimization
KW  - Cost optimization
KW  - subset-sum problem
KW  - Subset-sum problem
KW  - vector bin packing
ER  - 

TY  - CONF
TI  - Network aware resource allocation in distributed clouds
AU  - Alicherry, M.
AU  - Lakshman, T.V.
T2  - Proceedings - IEEE INFOCOM
AB  - We consider resource allocation algorithms for distributed cloud systems, which deploy cloud-computing resources that are geographically distributed over a large number of locations in a wide-area network. This distribution of cloud-computing resources over many locations in the network may be done for several reasons, such as to locate resources closer to users, to reduce bandwidth costs, to increase availability, etc. To get the maximum benefit from a distributed cloud system, we need efficient algorithms for resource allocation which minimize communication costs and latency. In this paper, we develop efficient resource allocation algorithms for use in distributed clouds. Our contributions are as follows: Assuming that users specify their resource needs, such as the number of virtual machines needed for a large computational task, we develop an efficient 2-approximation algorithm for the optimal selection of data centers in the distributed cloud. Our objective is to minimize the maximum distance, or latency, between the selected data centers. Next, we consider use of a similar algorithm to select, within each data center, the racks and servers where the requested virtual machines for the task will be located. Since the network inside a data center is structured and typically a tree, we make use of this structure to develop an optimal algorithm for rack and server selection. Finally, we develop a heuristic for partitioning the requested resources for the task amongst the chosen data centers and racks. We use simulations to evaluate the performance of our algorithms over example distributed cloud systems and find that our algorithms provide significant gains over other simpler allocation algorithms. © 2012 IEEE.
DA  - 2012///
PY  - 2012
DO  - 10.1109/INFCOM.2012.6195847
SP  - 963
EP  - 971
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861624273&doi=10.1109%2fINFCOM.2012.6195847&partnerID=40&md5=31970cf23ffaac4e2e4952ea8cd59047
DB  - Scopus
KW  - Data centers
KW  - Optimization
KW  - Efficient resource allocation
KW  - Resource allocation
KW  - Distributed computer systems
KW  - Virtual machines
KW  - Cloud systems
KW  - Computer simulation
KW  - Resource allocation algorithms
KW  - Approximation algorithms
KW  - Wide area networks
KW  - Optimal algorithm
KW  - Allocation algorithm
KW  - Telecommunication systems
KW  - Trees (mathematics)
KW  - Communication cost
KW  - Computational task
KW  - Maximum distance
KW  - Optimal selection
KW  - Server selection
ER  - 

TY  - JOUR
TI  - VMPlanner: Optimizing virtual machine placement and traffic flow routing to reduce network power costs in cloud data centers
AU  - Fang, W.
AU  - Liang, X.
AU  - Li, S.
AU  - Chiaraviglio, L.
AU  - Xiong, N.
T2  - Computer Networks
AB  - In recent years, the power costs of cloud data centers have become a practical concern and have attracted significant attention from both industry and academia. Most of the early works on data center energy efficiency have focused on the biggest power consumers (i.e., computer servers and cooling systems), yet without taking the networking part into consideration. However, recent studies have revealed that the network elements consume 10-20% of the total power in the data center, which poses a great challenge to effectively reducing network power cost without adversely affecting overall network performance. Based on the analysis on topology characteristics and traffic patterns of data centers, this paper presents a novel approach, called VMPlanner, for network power reduction in the virtualization-based data centers. The basic idea of VMPlanner is to optimize both virtual machine placement and traffic flow routing so as to turn off as many unneeded network elements as possible for power saving. We formulate the optimization problem, analyze its hardness, and solve it by designing VMPlanner as a stepwise optimization approach with three approximation algorithms. VMPlanner is implemented and evaluated in a simulated environment with traffic traces collected from a data center test-bed, and the experiment results illustrate the efficacy and efficiency of this approach.© 2012 Elsevier B.V. All rights reserved.
DA  - 2013///
PY  - 2013
DO  - 10.1016/j.comnet.2012.09.008
VL  - 57
IS  - 1
SP  - 179
EP  - 196
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873718586&doi=10.1016%2fj.comnet.2012.09.008&partnerID=40&md5=80f7be905016a1de95b7f47972923382
DB  - Scopus
KW  - Energy efficiency
KW  - Data centers
KW  - Optimization
KW  - Cloud data centers
KW  - Data center
KW  - Cost reduction
KW  - Computer simulation
KW  - Optimization problems
KW  - Virtual machine placements
KW  - Approximation algorithms
KW  - Network element
KW  - Traffic flow
KW  - Green networking
KW  - Network performance
KW  - Data center energy efficiencies
KW  - Power costs
KW  - VM placements
KW  - Computer servers
KW  - Green networkings
KW  - Network power
KW  - Power consumers
KW  - Power savings
KW  - Simulated environment
KW  - Stepwise optimization
KW  - Topology characteristics
KW  - Total power
KW  - Traffic pattern
KW  - Traffic traces
KW  - Turn offs
KW  - VM placement
ER  - 

TY  - CONF
TI  - Autonomic service placement in fog computing
AU  - Kayal, P.
AU  - Liebeherr, J.
T2  - 20th IEEE International Symposium on A World of Wireless, Mobile and Multimedia Networks, WoWMoM 2019
AB  - Fog computing recently emerged as novel distributed virtualized computing paradigm, where cloud services are extended to the edge of the network, thereby increasing network capacity and reducing latencies. In fog computing, applications are composed of building blocks, called microservices, that are mapped to edge computing and communication devices, referred to as fog nodes. A crucial component in fog computing are placement algorithms that assign microservices to fog nodes, since they determine the overall system performance in terms of energy consumption, communication costs, load balancing, and others. Placement strategies for virtual machines in cloud computing abound, but are generally centralized and therefore not well suited for decentralized fog systems. In this paper, we develop a fully distributed placement strategy that jointly optimizes energy consumption of fog nodes and communication costs of applications. We follow a Markov approximation approach for the design of a fully distributed autonomic service placement strategy without central coordination or global state information. Using numerical examples, we show that our placement algorithm finds solutions that are comparable to existing centralized solutions. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/WoWMoM.2019.8792989
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071463867&doi=10.1109%2fWoWMoM.2019.8792989&partnerID=40&md5=97b416bacc08e8335c5cb7d795e26846
DB  - Scopus
KW  - Fog computing
KW  - Fog
KW  - Green computing
KW  - Balancing
KW  - Energy utilization
KW  - Placement strategy
KW  - Computing paradigm
KW  - Placement algorithm
KW  - Service placements
KW  - Markov approximation
KW  - Communication cost
KW  - Communication device
KW  - Global state information
ER  - 

TY  - CONF
TI  - It's not easy being green
AU  - Gao, P.X.
AU  - Curtis, A.R.
AU  - Wong, B.
AU  - Keshav, S.
T2  - SIGCOMM'12 - Proceedings of the ACM SIGCOMM 2012 Conference Applications, Technologies, Architectures, and Protocols for Computer Communication
AB  - Large-scale Internet applications, such as content distribution networks, are deployed across multiple datacenters and consume massive amounts of electricity. To provide uniformly low access latencies, these datacenters are geographically distributed and the deployment size at each location reflects the regional demand for the application. Consequently, an application's environmental impact can vary significantly depending on the geographical distribution of end-users, as electricity cost and carbon footprint per watt is location specific. In this paper, we describe FORTE: Flow Optimization based framework for request-Routing and Traffic Engineering. FORTE dynamically controls the fraction of user traffic directed to each datacenter in response to changes in both request workload and carbon footprint. It allows an operator to navigate the three-way tradeoff between access latency, carbon footprint, and electricity costs and to determine an optimal datacenter upgrade plan in response to increases in traffic load. We use FORTE to show that carbon taxes or credits are impractical in incentivizing carbon output reduction by providers of large-scale Internet applications. However, they can reduce carbon emissions by 10% without increasing the mean latency nor the electricity bill. © 2012 ACM.
DA  - 2012///
PY  - 2012
DO  - 10.1145/2342356.2342398
SP  - 211
EP  - 222
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866480667&doi=10.1145%2f2342356.2342398&partnerID=40&md5=4538e816034604c9cf1a659eef1d291d
DB  - Scopus
KW  - energy
KW  - Computer architecture
KW  - green computing
KW  - Carbon footprint
KW  - Data centers
KW  - Carbon emissions
KW  - Optimization
KW  - Network architecture
KW  - Environmental impact
KW  - End-users
KW  - Internet
KW  - Communication
KW  - Access latency
KW  - Emission control
KW  - Electricity costs
KW  - Carbon taxes
KW  - Content distribution networks
KW  - Electricity
KW  - Electricity bill
KW  - Flow optimization
KW  - Internet application
KW  - Traffic Engineering
KW  - Traffic loads
KW  - User traffics
ER  - 

TY  - JOUR
AU  - Russell, S.
AU  - Norvig, P.
T2  - Artificial Intelligence: A Modern Approach
DA  - 2003///
PY  - 2003
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003584577&partnerID=40&md5=c41115534547c9393cf4dbff88045f7b
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Edge-CoCaCo: Toward Joint Optimization of Computation, Caching, and Communication on Edge Cloud
AU  - Chen, M.
AU  - Hao, Y.
AU  - Hu, L.
AU  - Hossain, M.S.
AU  - Ghoneim, A.
T2  - IEEE Wireless Communications
AB  - With the development of recent innovative applications (e.g., augmented reality, natural language processing, and various cognitive applications), more and more computation-intensive and rich-media tasks are delay-sensitive. Edge cloud computing is expected to be an effective solution to meet the demand for low latency. By the use of content offloading and/or computation offloading, users' quality of experience is improved with shorter delay. Compared to existing edge computing solutions, this article introduces a new concept of computing task caching and gives the optimal computing task caching policy. Furthermore, joint optimization of computation, caching, and communication on the edge cloud, dubbed Edge-CoCaCo, is proposed. Then we give the solution to that optimization problem. Finally, the simulation experimental results show that compared to the other schemes, Edge-CoCaCo has shorter delay. © 2002-2012 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/MWC.2018.1700308
VL  - 25
IS  - 3
SP  - 21
EP  - 27
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049616274&doi=10.1109%2fMWC.2018.1700308&partnerID=40&md5=f19ceafb34d8b8a0917f9fdcbb1def30
DB  - Scopus
KW  - Augmented reality
KW  - Quality of service
KW  - Quality of experience (QoE)
KW  - Optimization problems
KW  - Effective solution
KW  - Computation offloading
KW  - Computing solutions
KW  - Computation intensives
KW  - Joint optimization
KW  - Natural language processing systems
KW  - Optimal computing
ER  - 

TY  - CONF
TI  - Near-optimal virtual machine placement with product traffic pattern in data centers
AU  - You, K.
AU  - Tang, B.
AU  - Ding, F.
T2  - IEEE International Conference on Communications
AB  - This paper focuses on optimizing the virtual machine (VM) placement to reduce the bandwidth usage in modern virtualization based data centers. Existing solutions require all the traffic rates between any pair of VMs to be known beforehand, which may incur significant overhead or even be impossible. To reduce such overhead, we adopt the product traffic pattern model to characterize the traffic rates. In this model, each VM is associated with an activity level, and the normalized traffic rate between each pair of VMs is approximated by the product of their activity levels. Specifically, we consider the VM placement problem with product traffic in data centers that follow the Clique or VL2 architecture. We then present an optimal solution for the special case where all the physical machines (PMs) connect to the same switch. By extending the intrinsic idea of such solution, we further develop a simple yet efficient algorithm for the general case. We prove its optimality in a common case where the PMs are homogeneous, and also derive its approximation ratio in the general case. Finally, we demonstrate via simulations that the performance of our algorithm is near-optimal under practical settings. © 2013 IEEE.
DA  - 2013///
PY  - 2013
DO  - 10.1109/ICC.2013.6655130
SP  - 3705
EP  - 3709
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891368786&doi=10.1109%2fICC.2013.6655130&partnerID=40&md5=d2ea0a91dcd4a5b454d5bbe4d0a56a50
DB  - Scopus
KW  - Optimization
KW  - Virtual machines
KW  - Algorithms
KW  - Optimal solutions
KW  - Computer simulation
KW  - Virtual machine placements
KW  - Virtualizations
KW  - Approximation ratios
KW  - Traffic pattern
KW  - Activity levels
KW  - Bandwidth usage
ER  - 

TY  - CONF
TI  - Energy efficient allocation of virtual machines in cloud data centers
AU  - Beloglazov, A.
AU  - Buyya, R.
T2  - CCGrid 2010 - 10th IEEE/ACM International Conference on Cluster, Cloud, and Grid Computing
AB  - Rapid growth of the demand for computational power has led to the creation of large-scale data centers. They consume enormous amounts of electrical power resulting in high operational costs and carbon dioxide emissions. Moreover, modern Cloud computing environments have to provide high Quality of Service (QoS) for their customers resulting in the necessity to deal with power-performance trade-off. We propose an efficient resource management policy for virtualized Cloud data centers. The objective is to continuously consolidate VMs leveraging live migration and switch off idle nodes to minimize power consumption, while providing required Quality of Service. We present evaluation results showing that dynamic reallocation of VMs brings substantial energy savings, thus justifying further development of the proposed policy. © 2010 IEEE.
DA  - 2010///
PY  - 2010
DO  - 10.1109/ccgrid.2010.45
SP  - 577
EP  - 578
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954925596&doi=10.1109%2fccgrid.2010.45&partnerID=40&md5=1b74a6f3da49fc382b17010bea034f14
DB  - Scopus
KW  - Cloud computing
KW  - Energy efficiency
KW  - Resource management
KW  - Energy consumption
KW  - Virtualization
KW  - Quality of service
KW  - Carbon dioxide
KW  - Economic and social effects
KW  - Resource allocation
KW  - Energy conservation
KW  - Natural resources management
KW  - Quality control
KW  - Virtual machines
KW  - Green IT
KW  - Cluster computing
KW  - Grid computing
KW  - Computer simulation
KW  - Virtualizations
KW  - Global warming
KW  - Allocation of virtual machines
KW  - Live migration of virtual machines
ER  - 

TY  - JOUR
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106672919&partnerID=40&md5=9ef6859905dca20d1a6b38f0a19a2431
DB  - Scopus
ER  - 

TY  - CONF
TI  - Jellyfish: Networking data centers randomly
AU  - Singla, A.
AU  - Hong, C.-Y.
AU  - Popa, L.
AU  - Godfrey, P.B.
T2  - Proceedings of NSDI 2012: 9th USENIX Symposium on Networked Systems Design and Implementation
AB  - Industry experience indicates that the ability to incrementally expand data centers is essential. However, existing high-bandwidth network designs have rigid structure that interferes with incremental expansion. We present Jellyfish, a high-capacity network interconnect which, by adopting a random graph topology, yields itself naturally to incremental expansion. Somewhat surprisingly, Jellyfish is more cost-efficient than a fat-tree, supporting as many as 25% more servers at full capacity using the same equipment at the scale of a few thousand nodes, and this advantage improves with scale. Jellyfish also allows great flexibility in building networks with different degrees of oversubscription. However, Jellyfish's unstructured design brings new challenges in routing, physical layout, and wiring. We describe approaches to resolve these challenges, and our evaluation suggests that Jellyfish could be deployed in today's data centers. © 2012 by The USENIX Association. All Rights Reserved.
DA  - 2012///
PY  - 2012
SP  - 225
EP  - 238
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043480267&partnerID=40&md5=7dda5fa40c9c357fa6c6c9799ea3fbc5
DB  - Scopus
KW  - Topology
KW  - Data centers
KW  - Systems analysis
KW  - Expansion
KW  - High-bandwidth networks
KW  - Cost-efficient
KW  - High capacity networks
KW  - In-building network
KW  - Industry experience
KW  - Physical layout
KW  - Random graph topology
KW  - Rigid structures
ER  - 

TY  - CONF
TI  - Microservices-based IoT application placement within heterogeneous and resource constrained fog computing environments
AU  - Pallewatta, S.
AU  - Kostakos, V.
AU  - Buyya, R.
T2  - UCC 2019 - Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing
AB  - Fog computing paradigm has created innovation opportunities within Internet of Things (IoT) domain by extending cloud services to the edge of the network. Due to the distributed, heterogeneous and resource constrained nature of the Fog computing nodes, Fog applications need to be developed as a collection of interdependent, lightweight modules. Since this concept aligns with the goals of microservices architecture, efficient placement of microservices-based IoT applications within Fog environments has the potential to fully leverage capabilities of Fog devices. In this paper, we propose a decentralized microservices-based IoT application placement policy for heterogeneous and resource constrained Fog environments. The proposed policy utilizes the independently deployable and scalable nature of microservices to place them as close as possible to the data source to minimize latency and network usage. Moreover, it aims to handle service discovery and load balancing related challenges of the microservices architecture. We implement and evaluate our policy using iFogSim simulated Fog environment. Results of the simulations show around 85% improvement in latency and network usage for the proposed microservice placement policy when compared with Cloud-only placement approach and around 40% improvement over an alternative Fog application placement method known as Edge-ward placement policy. Moreover, the decentralized placement approach proposed in this paper demonstrates significant reduction in microservice placement delay over centralized placement. © 2019 Association for Computing Machinery.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3344341.3368800
SP  - 71
EP  - 81
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078454708&doi=10.1145%2f3344341.3368800&partnerID=40&md5=b99c5eccd2b311a7d66f0b4f73c80c63
DB  - Scopus
KW  - Fog computing
KW  - Internet of things
KW  - Fog
KW  - Microservices architecture
KW  - Computing environments
KW  - Network architecture
KW  - Application deployment
KW  - Computing paradigm
KW  - Internet of Things (IOT)
KW  - Service discovery
KW  - Internet of things (IoT)
KW  - Application placement
KW  - Application placements
KW  - IOT applications
KW  - Simulated fog environments
ER  - 

TY  - JOUR
TI  - Future Edge Cloud and Edge Computing for Internet of Things Applications
AU  - Pan, J.
AU  - McElhannon, J.
T2  - IEEE Internet of Things Journal
AB  - The Internet is evolving rapidly toward the future Internet of Things (IoT) which will potentially connect billions or even trillions of edge devices which could generate huge amount of data at a very high speed and some of the applications may require very low latency. The traditional cloud infrastructure will run into a series of difficulties due to centralized computation, storage, and networking in a small number of datacenters, and due to the relative long distance between the edge devices and the remote datacenters. To tackle this challenge, edge cloud and edge computing seem to be a promising possibility which provides resources closer to the resource-poor edge IoT devices and potentially can nurture a new IoT innovation ecosystem. Such prospect is enabled by a series of emerging technologies, including network function virtualization and software defined networking. In this survey paper, we investigate the key rationale, the state-of-the-art efforts, the key enabling technologies and research topics, and typical IoT applications benefiting from edge cloud. We aim to draw an overall picture of both ongoing research efforts and future possible research directions through comprehensive discussions. © 2014 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/JIOT.2017.2767608
VL  - 5
IS  - 1
SP  - 439
EP  - 449
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032725533&doi=10.1109%2fJIOT.2017.2767608&partnerID=40&md5=2db2b2f116b1799fb73a775280903e5d
DB  - Scopus
KW  - Cloud computing
KW  - survey
KW  - edge computing
KW  - Internet of Things (IoT)
KW  - Edge computing
KW  - Internet of things
KW  - Virtualization
KW  - Edge cloud
KW  - Distributed computer systems
KW  - Digital storage
KW  - Virtual reality
KW  - Software defined networking
KW  - Surveys
KW  - Edge clouds
KW  - Computational model
KW  - Network function virtualization
KW  - Transfer functions
KW  - network function virtualization (NFV)
KW  - Software defined networking (SDN)
KW  - software defined networking (SDN)
KW  - Surveying
KW  - HomeCloud
ER  - 

TY  - CONF
TI  - Minimizing the operational cost of data centers via geographical electricity price diversity
AU  - Xu, Z.
AU  - Liang, W.
T2  - IEEE International Conference on Cloud Computing, CLOUD
AB  - Data centers, serving as infrastructures for cloud services, are growing in both number and scale. However, they usually consume enormous amounts of electric power, which lead to high operational costs of cloud service providers. Reducing the operational cost of data centers thus has been recognized as a main challenge in cloud computing. In this paper we study the minimum operational cost problem of fair request rate allocations in a distributed cloud environment by incorporating the diversity of time-varying electricity prices in different regions, with an objective to fairly allocate requests to different data centers for processing while keeping the negotiated Service Level Agreements (SLAs) between request users and the cloud service provider to be met, where the data centers and web portals of a cloud service provider are geographically located in different regions. To this end, we first propose an optimization framework for the problem. We then devise a fast approximation algorithm with a provable approximation ratio by exploiting combinatorial properties of the problem. We finally evaluate the performance of the proposed algorithm through experimental simulation on real-life electricity price data sets. Experimental results demonstrate that the proposed algorithm is very promising, which not only outperforms other existing heuristics but also is highly scalable. © 2013 IEEE.
DA  - 2013///
PY  - 2013
DO  - 10.1109/CLOUD.2013.94
SP  - 99
EP  - 106
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897707284&doi=10.1109%2fCLOUD.2013.94&partnerID=40&md5=eae6c91f8b104a0d37916a4d1c768811
DB  - Scopus
KW  - Cloud computing
KW  - Costs
KW  - Distributed database systems
KW  - Cloud service providers
KW  - Service level agreement (SLAs)
KW  - Approximation algorithms
KW  - Optimization framework
KW  - Approximation ratios
KW  - Experimental simulations
KW  - Distributed clouds
KW  - Electricity
KW  - Combinatorial properties
KW  - Time-varying electricity prices
ER  - 

TY  - CONF
TI  - A scalable, commodity data center network architecture
AU  - Al-Fares, M.
AU  - Loukissas, A.
AU  - Vahdat, A.
T2  - Computer Communication Review
AB  - Today's data centers may contain tens of thousands of computers with significant aggregate bandwidth requirements. The network architecture typically consists of a tree of routing and switching elements with progressively more specialized and expensive equipment moving up the network hierarchy. Unfortunately, even when deploying the highest-end IP switches/routers, resulting topologies may only support 50% of the aggregate bandwidth available at the edge of the network, while still incurring tremendous cost. Non-uniform bandwidth among data center nodes complicates application design and limits overall system performance. In this paper, we show how to leverage largely commodity Ethernet switches to support the full aggregate bandwidth of clusters consisting of tens of thousands of elements. Similar to how clusters of commodity computers have largely replaced more specialized SMPs and MPPs, we argue that appropriately architected and interconnected commodity switches may deliver more performance at less cost than available from today's higher-end solutions. Our approach requires no modifications to the end host network interface, operating system, or applications; critically, it is fully backward compatible with Ethernet, IP, and TCP. Copyright 2008 ACM.
DA  - 2008///
PY  - 2008
DO  - 10.1145/1402946.1402967
VL  - 38
SP  - 63
EP  - 74
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-65249121271&doi=10.1145%2f1402946.1402967&partnerID=40&md5=79c74fac447868845b283d0dacd9469b
DB  - Scopus
KW  - Internet protocols
KW  - Costs
KW  - Topology
KW  - Network architecture
KW  - Operating systems
KW  - Bandwidth
KW  - Satellite communication systems
KW  - Ethernet
KW  - Backward compatible
KW  - Application designs
KW  - Bandwidth requirements
KW  - Commodity ethernets
KW  - Convolutional codes
KW  - Data center topology
KW  - Equal-cost routing
KW  - Network hierarchies
KW  - Network interfaces
KW  - Non-uniform
KW  - Shape memory effect
KW  - Switching elements
ER  - 

TY  - JOUR
TI  - Integrating Heuristic and Machine-Learning Methods for Efficient Virtual Machine Allocation in Data Centers
AU  - Pahlevan, A.
AU  - Qu, X.
AU  - Zapater, M.
AU  - Atienza, D.
T2  - IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems
AB  - Modern cloud data centers (DCs) need to tackle efficiently the increasing demand for computing resources and address the energy efficiency challenge. Therefore, it is essential to develop resource provisioning policies that are aware of virtual machine (VM) characteristics, such as CPU utilization and data communication, and applicable in dynamic scenarios. Traditional approaches fall short in terms of flexibility and applicability for large-scale DC scenarios. In this paper, we propose a heuristic- and a machine learning (ML)-based VM allocation method and compare them in terms of energy, quality of service (QoS), network traffic, migrations, and scalability for various DC scenarios. Then, we present a novel hyper-heuristic algorithm that exploits the benefits of both methods by dynamically finding the best algorithm, according to a user-defined metric. For optimality assessment, we formulate an integer linear programming (ILP)-based VM allocation method to minimize energy consumption and data communication, which obtains optimal results, but is impractical at runtime. Our results demonstrate that the ML approach provides up to 24% server-to-server network traffic improvement and reduces execution time by up to 480 × compared to conventional approaches, for large-scale scenarios. On the contrary, the heuristic outperforms the ML method in terms of energy and network traffic for reduced scenarios. We also show that the heuristic and ML approaches have up to 6% energy consumption overhead compared to ILP-based optimal solution. Our hyper-heuristic integrates the strengths of both the heuristic and the ML methods by selecting the best one during runtime. © 1982-2012 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/TCAD.2017.2760517
VL  - 37
IS  - 8
SP  - 1667
EP  - 1680
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031774157&doi=10.1109%2fTCAD.2017.2760517&partnerID=40&md5=ed06c75529660919029ceb200d260493
DB  - Scopus
KW  - Energy efficiency
KW  - Artificial intelligence
KW  - Resource management
KW  - Scalability
KW  - Integer Linear Programming
KW  - Learning systems
KW  - Heuristic algorithms
KW  - Network security
KW  - Quality of service
KW  - Servers
KW  - Green computing
KW  - Economic and social effects
KW  - Energy utilization
KW  - Distributed computer systems
KW  - Cloud data centers
KW  - Information management
KW  - Trade off
KW  - Virtual machine
KW  - machine learning (ML)
KW  - Integer programming
KW  - Heuristic methods
KW  - Correlation methods
KW  - E-learning
KW  - Inductive logic programming (ILP)
KW  - quality of service (QoS)
KW  - Convolutional codes
KW  - Cloud data centers (DCs)
KW  - Data-communication
KW  - energy-network traffic tradeoffs
KW  - greedy heuristic
KW  - Greedy heuristics
KW  - Heuristic programming
KW  - hyper heuristic
KW  - Hyperheuristic
KW  - integer linear programming (ILP)
KW  - scalability assessment
ER  - 

TY  - CONF
TI  - Carbon-aware load balancing for geo-distributed cloud services
AU  - Zhou, Z.
AU  - Liu, F.
AU  - Xu, Y.
AU  - Zou, R.
AU  - Xu, H.
AU  - Lui, J.C.S.
AU  - Jin, H.
T2  - Proceedings - IEEE Computer Society's Annual International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunications Systems, MASCOTS
AB  - Recently, data center carbon emission has become an emerging concern for the cloud service providers. Previous works are limited on cutting down the power consumption of the data centers to defuse such a concern. In this paper, we show how the spatial and temporal variabilities of the electricity carbon footprint can be fully exploited to further green the cloud running on top of geographically distributed data centers. We jointly consider the electricity cost, service level agreement (SLA) requirement, and emission reduction budget. To navigate such a three-way tradeoff, we take advantage of Lyapunov optimization techniques to design and analyze a carbon-aware control framework, which makes online decisions on geographical load balancing, capacity right-sizing, and server speed scaling. Results from rigorous mathematical analyses and real-world trace-driven empirical evaluation demonstrate its effectiveness in both minimizing electricity cost and reducing carbon emission. © 2013 IEEE.
DA  - 2013///
PY  - 2013
DO  - 10.1109/MASCOTS.2013.31
SP  - 232
EP  - 241
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894559511&doi=10.1109%2fMASCOTS.2013.31&partnerID=40&md5=279e1af21c2d4e31b5d44d917ec7ae6c
DB  - Scopus
ER  - 

TY  - JOUR
TI  - A priority, power and traffic-aware virtual machine placement of IoT applications in cloud data centers
AU  - Omer, S.
AU  - Azizi, S.
AU  - Shojafar, M.
AU  - Tafazolli, R.
T2  - Journal of Systems Architecture
AB  - Recent telecommunication paradigms, such as big data, Internet of Things (IoT), ubiquitous edge computing (UEC), and machine learning, are encountering with a tremendous number of complex applications that require different priorities and resource demands. These applications usually consist of a set of virtual machines (VMs) with some predefined traffic load between them. The efficiency of a cloud data center (CDC) as prominent component in UEC significantly depends on the efficiency of its VM placement algorithm applied. However, VM placement is an NP-hard problem and thus there exist practically no optimal solution for this problem. In this paper, motivated by this, we propose a priority, power and traffic-aware approach for efficiently solving the VM placement problem in a CDC. Our approach aims to jointly minimize power consumption, network consumption and resource wastage in a multi-dimensional and heterogeneous CDC. To evaluate the performance of the proposed method, we compared it to the state-of-the-art on a fat-tree topology under various experiments. Results demonstrate that the proposed method is capable of reducing the total network consumption up to 29%, the consumption of power up to 18%, and the wastage of resources up to 68%, compared to the second-best results. © 2021 Elsevier B.V.
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.sysarc.2021.101996
VL  - 115
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099224758&doi=10.1016%2fj.sysarc.2021.101996&partnerID=40&md5=b374e01d36d4eb1725d6c8a964d362c6
DB  - Scopus
KW  - Cloud computing
KW  - Internet of things
KW  - Network security
KW  - Cloud data centers
KW  - Virtual machine
KW  - Power consumption
KW  - Virtual reality
KW  - Optimal solutions
KW  - Internet of Things (IOT)
KW  - Complex applications
KW  - Resource demands
KW  - Efficiency
KW  - Virtual machine placements
KW  - IOT applications
KW  - NP-hard
KW  - Ubiquitous computing
KW  - Cloud data center (CDC)
KW  - Internet of Thing (IoT)
KW  - Multi dimensional
KW  - Priority-aware
KW  - Traffic-aware
KW  - Virtual machine placement (VMP)
ER  - 

TY  - JOUR
TI  - Sla-aware best fit decreasing techniques for workload consolidation in clouds
AU  - Mustafa, S.
AU  - Sattar, K.
AU  - Shuja, J.
AU  - Sarwar, S.
AU  - Maqsood, T.
AU  - Madani, S.A.
AU  - Guizani, S.
T2  - IEEE Access
AB  - Cloud computing emerged as one of the leading computational paradigms due to elastic resource provisioning and pay-as-you-go model. Large data centers are used by the service providers to host the various services. These data centers consume enormous energy, which leads to increase in operating costs and carbon footprints. Therefore, green cloud computing is a necessity, which not only reduces energy consumption, but also affects the environment positively. In order to reduce the energy consumption, workload consolidation approach is used that consolidates the tasks in minimum possible servers. However, workload consolidation may lead to service level agreement (SLA) violations due to non-availability of resources on the server. Therefore, workload consolidation techniques should consider the aforementioned problem. In this paper, we present two consolidation based energy-efficient techniques that reduce energy consumption along with resultant SLA violations. In addition to that, we also enhanced the existing Enhanced-Conscious Task Consolidation (ECTC) and Maximum Utilization (MaxUtil) techniques that attempt to reduce energy consumption and SLA violations. Experimental results show that the proposed techniques perform better than the selected heuristic based techniques in terms of energy, SLA, and migrations. © 2013 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/ACCESS.2019.2941145
VL  - 7
SP  - 135256
EP  - 135267
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077992627&doi=10.1109%2fACCESS.2019.2941145&partnerID=40&md5=57d27bf40881c3a3920cdfca6e8fc80f
DB  - Scopus
KW  - Cloud computing
KW  - Energy efficiency
KW  - Resource management
KW  - Carbon footprint
KW  - Green computing
KW  - Energy utilization
KW  - Workload consolidation
KW  - Service Level Agreements
KW  - Reduce energy consumption
KW  - Operating costs
KW  - Best fit decreasing
KW  - Computational paradigm
KW  - Energy-efficient techniques
KW  - SLA violation
ER  - 

TY  - CONF
TI  - Customer-aware resource overallocation to improve energy efficiency in realtime Cloud Computing data centers
AU  - Moreno, I.S.
AU  - Xu, J.
T2  - Proceedings - 2011 IEEE International Conference on Service-Oriented Computing and Applications, SOCA 2011
AB  - Energy efficiency is becoming a very important concern for Cloud Computing environments. These are normally composed of large and power consuming data centers to provide the required elasticity and scalability to their customers. In this context, many efforts have been developed to balance the loads at host level. However, determining how to maximize the resources utilization at Virtual Machine (VM) level still remains as a big challenge. This is mainly driven by very dynamic workload behaviors and a wide variety of customers' resource utilization patterns. This paper introduces a dynamic resource provisioning mechanism to overallocate the capacity of real-time Cloud data centers based on customer utilization patterns. Furthermore, its impact on the trade-off between energy efficiency and SLA fulfillment is analyzed. The main idea is to exploit the resource utilization patterns of each customer to decrease the waste produced by resource request overestimations. This creates the opportunity to allocate additional VMs in the same host incrementing its energy efficiency. Nevertheless, this also increases the risk of QoS affectations. The proposed model considers SLA deadlines, predictions based on historical data, and dynamic occupation to determine the amount of resources to overallocate for each host. In addition, a compensation mechanism to adjust resource allocation in cases of underestimation is also described. In order to evaluate the model, simulation experimentation was conducted. Results demonstrate meaningful improvements in energy-efficiency while SLA-deadlines are slightly impacted. However, they also point the importance of strongest compensation policies to reduce availability violations especially during peak utilization periods. © 2011 IEEE.
DA  - 2011///
PY  - 2011
DO  - 10.1109/SOCA.2011.6166239
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859959080&doi=10.1109%2fSOCA.2011.6166239&partnerID=40&md5=b89950e2fb0cdc6b863a714c157f2c03
DB  - Scopus
KW  - cloud computing
KW  - Cloud computing
KW  - Energy efficiency
KW  - energy-efficiency
KW  - Elasticity
KW  - green computing
KW  - Quality of service
KW  - Energy aware
KW  - Dynamics
KW  - Computer systems
KW  - Computer simulation
KW  - Sales
KW  - Customer satisfaction
KW  - customer-aware
KW  - energy-aware provisioning
KW  - overallocation
KW  - overbooking
KW  - real-time cloud computing
ER  - 

TY  - CONF
TI  - Modeling energy consumption in high-capacity routers and switches
AU  - Vishwanath Member, A.
AU  - Hinton, K.
AU  - Ayre, R.W.A.
AU  - Tucker, R.S.
T2  - IEEE Journal on Selected Areas in Communications
AB  - Routers and switches are major contributors to the energy consumption of modern networks. Today, many energy efficiency metrics for these high-capacity devices are coarse-grained, i.e., based upon a single energy per bit value given at peak load or averaged over several specific loads. In this paper, we develop a new power model and a vendor-agnostic methodology that permits quantifying the energy efficiency of Internet equipment at a more fundamental level, i.e., at the granularity of per-packet processing, and per-byte store and forward packet handling operations. We demonstrate the efficacy of the proposed technique by applying it to various types of routers and switches. We describe how our technique can be used to accurately estimate the network-wide energy footprint incurred when accessing different applications. We offer our method as a valuable framework against which the energy efficiency of current and future generation of load-proportional Internet equipment can be benchmarked. © 2014 IEEE.
DA  - 2014///
PY  - 2014
DO  - 10.1109/JSAC.2014.2335312
VL  - 32
SP  - 1524
EP  - 1532
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907518813&doi=10.1109%2fJSAC.2014.2335312&partnerID=40&md5=a3fb7906516c99cc244b3ebfa2605bdf
DB  - Scopus
KW  - energy efficiency
KW  - Energy efficiency
KW  - ethernet switches
KW  - Ethernet switches
KW  - High-capacity
KW  - IP routers
KW  - per-byte store and forward energy
KW  - per-packet processing energy
KW  - Store and forward
ER  - 

TY  - CONF
TI  - Green scheduling for cloud data centers using renewable resources
AU  - Gu, C.
AU  - Liu, C.
AU  - Zhang, J.
AU  - Huang, H.
AU  - Jia, X.
T2  - Proceedings - IEEE INFOCOM
AB  - Cloud data centers provide all kinds of service using hundreds of thousands of servers. This naturally leads to concerns about the effect on environment such as carbon emissions and global warming. Huge amounts of effort have been devoted to power-aware scheduling using renewable energy. However, the intermittent availability of the renewable energy brings us a new challenge: how to dynamically distribute the requests to the data centers that are powered by renewable energy, while minimizing carbon emissions under a fixed electricity budget. In this paper, we model our problem as a constraint optimization problem. The goal is to minimize the carbon emissions of the data centers by using renewable energy while satisfying: (1) the request processing time constraint; (2) the total electricity budget in each time slot; (3) the intermittent supply of the renewable resources; (4) the maximal number of servers in each data center. We solve the problem by ingeniously transforming it into an integer linear programming model, and calculate the decision variables using existed method. Experiments show that our scheduler can minimize carbon emissions using renewable resources, while satisfying the constraints mentioned above. © 2015 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/INFCOMW.2015.7179410
VL  - 2015-August
SP  - 354
EP  - 359
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943273501&doi=10.1109%2fINFCOMW.2015.7179410&partnerID=40&md5=b6293b303d3adf85046443386838d332
DB  - Scopus
KW  - cloud computing
KW  - Cloud computing
KW  - data center
KW  - Power management
KW  - Scheduling
KW  - Data centers
KW  - Optimization
KW  - Cloud data centers
KW  - Constrained optimization
KW  - Renewable energies
KW  - Budget control
KW  - Integer programming
KW  - Problem solving
KW  - Integer linear programming models
KW  - Emission control
KW  - renewable energy
KW  - Global warming
KW  - carbon emission reduction
KW  - Carbon emission reductions
KW  - Constraint optimization problems
KW  - green
KW  - Power-aware scheduling
KW  - schedule
ER  - 

TY  - JOUR
TI  - TRACTOR: Traffic-aware and power-efficient virtual machine placement in edge-cloud data centers using artificial bee colony optimization
AU  - Nabavi, S.S.
AU  - Gill, S.S.
AU  - Xu, M.
AU  - Masdari, M.
AU  - Garraghan, P.
T2  - International Journal of Communication Systems
AB  - Technology providers heavily exploit the usage of edge-cloud data centers (ECDCs) to meet user demand while the ECDCs are large energy consumers. Concerning the decrease of the energy expenditure of ECDCs, task placement is one of the most prominent solutions for effective allocation and consolidation of such tasks onto physical machine (PM). Such allocation must also consider additional optimizations beyond power and must include other objectives, including network-traffic effectiveness. In this study, we present a multi-objective virtual machine (VM) placement scheme (considering VMs as fog tasks) for ECDCs called TRACTOR, which utilizes an artificial bee colony optimization algorithm for power and network-aware assignment of VMs onto PMs. The proposed scheme aims to minimize the network traffic of the interacting VMs and the power dissipation of the data center's switches and PMs. To evaluate the proposed VM placement solution, the Virtual Layer 2 (VL2) and three-tier network topologies are modeled and integrated into the CloudSim toolkit to justify the effectiveness of the proposed solution in mitigating the network traffic and power consumption of the ECDC. Results indicate that our proposed method is able to reduce power energy consumption by 3.5% while decreasing network traffic and power by 15% and 30%, respectively, without affecting other QoS parameters. © 2021 John Wiley & Sons Ltd.
DA  - 2022///
PY  - 2022
DO  - 10.1002/dac.4747
VL  - 35
IS  - 1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100093763&doi=10.1002%2fdac.4747&partnerID=40&md5=44823c05fc64adacf3a2674e83ffbe98
DB  - Scopus
KW  - cloud computing
KW  - power consumption
KW  - Network security
KW  - Optimization
KW  - Energy utilization
KW  - Virtual machine
KW  - Network topology
KW  - Virtual machine placements
KW  - cloud data centers
KW  - network traffic
KW  - Power efficient
KW  - VM placement
KW  - artificial bee colony
KW  - Artificial bee colony optimization algorithms
KW  - Artificial bee colony optimizations
KW  - Energy expenditure
KW  - Placement scheme
KW  - Technology providers
KW  - Tractors (agricultural)
KW  - Tractors (truck)
ER  - 

TY  - JOUR
TI  - Fuzzy sets
AU  - Zadeh, L.A.
T2  - Information and Control
AB  - A fuzzy set is a class of objects with a continuum of grades of membership. Such a set is characterized by a membership (characteristic) function which assigns to each object a grade of membership ranging between zero and one. The notions of inclusion, union, intersection, complement, relation, convexity, etc., are extended to such sets, and various properties of these notions in the context of fuzzy sets are established. In particular, a separation theorem for convex fuzzy sets is proved without requiring that the fuzzy sets be disjoint. © 1965 Academic Press, Inc.
DA  - 1965///
PY  - 1965
DO  - 10.1016/S0019-9958(65)90241-X
VL  - 8
IS  - 3
SP  - 338
EP  - 353
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248666540&doi=10.1016%2fS0019-9958%2865%2990241-X&partnerID=40&md5=52dbe1d1e74f322d2c9f6532ac332a8c
DB  - Scopus
ER  - 

TY  - CONF
TI  - VM placement algorithms for hierarchical cloud infrastructure
AU  - Kabir, M.H.
AU  - Shoja, G.C.
AU  - Ganti, S.
T2  - Proceedings of the International Conference on Cloud Computing Technology and Science, CloudCom
AB  - A hierarchical cloud infrastructure involving cloud, cluster, and node controllers is the most preferred deployment architecture by the cloud providers since it helps them to achieve the desired scalability and geographical distribution. In this architecture, the cloud controller indirectly controls a large number of node controllers through a few cluster controllers. A Virtual Machine (VM) placement algorithm, therefore, should provide for intelligent cluster selection before node selection. To the best of our knowledge, there is no VM placement algorithm available that has addressed this issue. In this paper, we propose a new VM placement algorithm that intelligently selects the appropriate cluster, and then a node inside the cluster. We use multi-criteria-decision-analysis (MCDA) technique for this purpose. Simulation results have confirmed the superior performance of our algorithm. © 2014 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/CloudCom.2014.53
VL  - 2015-February
SP  - 656
EP  - 659
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937900728&doi=10.1109%2fCloudCom.2014.53&partnerID=40&md5=d67933241ad8a0f444537a61c79dca09
DB  - Scopus
KW  - Datacenter
KW  - Cloud platforms
KW  - Virtual machine
KW  - Data center
KW  - Cloud computing architecture
KW  - Cloud infrastructures
KW  - Placement algorithm
KW  - Virtual machine placements
KW  - Deployment architecture
KW  - VM placement
KW  - Cloud clusters
KW  - Cloud controller
KW  - Cluster controller
KW  - Hierarchical
KW  - Node controller
KW  - Node controllers
ER  - 

TY  - CONF
TI  - Application-Aware workload consolidation to minimize both energy consumption and network load in cloud environments
AU  - Tziritas, N.
AU  - Xu, C.-Z.
AU  - Loukopoulos, T.
AU  - Khan, S.U.
AU  - Yu, Z.
T2  - Proceedings of the International Conference on Parallel Processing
AB  - In this paper we tackle the problem of virtual machine (VM) placement onto physical servers to jointly optimize two objective functions. The first objective is to minimize the total energy spent within a cloud due to the servers that are commissioned to satisfy the computational demands of VMs. The second objective is to minimize the total network overhead incurred due to: (a) communicational dependencies between VMs, and (b) the VM migrations performed for the transition from an old assignment scheme to a new one. We study different methodologies for solving the aforementioned problem. The first approach is based on VM packing algorithms that optimize the above objective functions separately, reaching a single solution. The other approach is to tackle simultaneously the two optimization targets and define a set of non-dominating solutions. Performance evaluation using simulation experiments reveals interesting trade-offs between energy consumption and network load. © 2013 IEEE.
DA  - 2013///
PY  - 2013
DO  - 10.1109/ICPP.2013.54
SP  - 449
EP  - 457
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893266411&doi=10.1109%2fICPP.2013.54&partnerID=40&md5=bda7a99f28d013c2bdb7fbd431636ee3
DB  - Scopus
KW  - Cloud computing
KW  - Network security
KW  - Green computing
KW  - Economic and social effects
KW  - Energy utilization
KW  - Multiobjective optimization
KW  - Virtual machine
KW  - Workload consolidation
KW  - Performance evaluations
KW  - Virtual machine placement
KW  - Virtual machine placements
KW  - Pareto principle
KW  - Objective functions
KW  - Computational demands
KW  - Network load minimization
KW  - Network load minimizations
KW  - Packing algorithms
KW  - Pareto optimization
ER  - 

TY  - JOUR
TI  - Holistic Virtual Machine Scheduling in Cloud Datacenters towards Minimizing Total Energy
AU  - Li, X.
AU  - Garraghan, P.
AU  - Jiang, X.
AU  - Wu, Z.
AU  - Xu, J.
T2  - IEEE Transactions on Parallel and Distributed Systems
AB  - Energy consumed by Cloud datacenters has dramatically increased, driven by rapid uptake of applications and services globally provisioned through virtualization. By applying energy-aware virtual machine scheduling, Cloud providers are able to achieve enhanced energy efficiency and reduced operation cost. Energy consumption of datacenters consists of computing energy and cooling energy. However, due to the complexity of energy and thermal modeling of realistic Cloud datacenter operation, traditional approaches are unable to provide a comprehensive in-depth solution for virtual machine scheduling which encompasses both computing and cooling energy. This paper addresses this challenge by presenting an elaborate thermal model that analyzes the temperature distribution of airflow and server CPU. We propose GRANITE-a holistic virtual machine scheduling algorithm capable of minimizing total datacenter energy consumption. The algorithm is evaluated against other existing workload scheduling algorithms MaxUtil, TASA, IQR and Random using real Cloud workload characteristics extracted from Google datacenter tracelog. Results demonstrate that GRANITE consumes 4.3-43.6 percent less total energy in comparison to the state-of-the-art, and reduces the probability of critical temperature violation by 99.2 with 0.17 percent SLA violation rate as the performance penalty. © 1990-2012 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/TPDS.2017.2688445
VL  - 29
IS  - 6
SP  - 1317
EP  - 1331
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041005035&doi=10.1109%2fTPDS.2017.2688445&partnerID=40&md5=db168e86d80d672a81aa80fbff35123f
DB  - Scopus
KW  - Cloud computing
KW  - energy efficiency
KW  - Energy efficiency
KW  - Power management
KW  - Scheduling
KW  - virtual machine
KW  - Network security
KW  - Virtual machining
KW  - Servers
KW  - Datacenter
KW  - Green computing
KW  - Energy utilization
KW  - Processor scheduling
KW  - workload scheduling
KW  - Virtual machine
KW  - Scheduling algorithms
KW  - Cooling
KW  - Traditional approaches
KW  - Computational model
KW  - Data center operations
KW  - Virtual machine scheduling
KW  - datacenter modeling
KW  - Granite
KW  - Machinery
KW  - Thermal management (electronics)
KW  - Workload characteristics
ER  - 

TY  - CONF
TI  - Energy efficient resource management in virtualized cloud data centers
AU  - Beloglazov, A.
AU  - Buyya, R.
T2  - CCGrid 2010 - 10th IEEE/ACM International Conference on Cluster, Cloud, and Grid Computing
AB  - Rapid growth of the demand for computational power by scientific, business and web-applications has led to the creation of large-scale data centers consuming enormous amounts of electrical power. We propose an energy efficient resource management system for virtualized Cloud data centers that reduces operational costs and provides required Quality of Service (QoS). Energy savings are achieved by continuous consolidation of VMs according to current utilization of resources, virtual network topologies established between VMs and thermal state of computing nodes. We present first results of simulation-driven evaluation of heuristics for dynamic reallocation of VMs using live migration according to current requirements for CPU performance. The results show that the proposed technique brings substantial energy savings, while ensuring reliable QoS. This justifies further investigation and development of the proposed resource management system. © 2010 IEEE.
DA  - 2010///
PY  - 2010
DO  - 10.1109/CCGRID.2010.46
SP  - 826
EP  - 831
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954894902&doi=10.1109%2fCCGRID.2010.46&partnerID=40&md5=d50cafe5405862095a10731080802148
DB  - Scopus
KW  - Cloud computing
KW  - Energy efficiency
KW  - Resource management
KW  - Energy consumption
KW  - Virtualization
KW  - Quality of service
KW  - Resource allocation
KW  - Energy conservation
KW  - Natural resources management
KW  - Virtual machines
KW  - Green IT
KW  - Cluster computing
KW  - Cost reduction
KW  - Grid computing
KW  - Computer simulation
KW  - Virtualizations
KW  - Electric network topology
KW  - Allocation of virtual machines
KW  - Live migration of virtual machines
ER  - 

TY  - JOUR
TI  - Modeling the virtual machine allocation problem
AU  - Mann, Z.A.
T2  - Proceedings of the International Conference on Mathematical Methods, Mathematical Models, and Simulation in Science and Engineering
DA  - 2015///
PY  - 2015
SP  - 102
EP  - 106
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939793788&partnerID=40&md5=dd81ad675d04cd59c13886a4daebef0e
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Eurostat Electricity Price Statistics
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878872475&partnerID=40&md5=3bc157a761d10ef71040d1a994594319
DB  - Scopus
ER  - 

TY  - CONF
TI  - BFEPM: Best fit energy prediction modeling based on CPU utilization
AU  - Zhang, X.
AU  - Lu, J.
AU  - Qin, X.
T2  - Proceedings - 2013 IEEE 8th International Conference on Networking, Architecture and Storage, NAS 2013
AB  - Energy cost becomes a major part of data center operational cost. Computer system consume more power when it runs under high workload. Many past studies focused on how to predict power consumption by performance counters. Some models retrieve performance counters from chips. Some models query performance counters from OS. Most of these researches were verified on several machines and claimed their models were accurate under the test. We found different servers have different energy consumption characters even with same CPU. In this paper, we present BFEPM, a best fit energy prediction model. It choose best model based on the power consumption benchmark result. We illustrate how to use benchmark result to find a best fit model. Then we validate the viability and effectiveness of model on all published results. At last, we apply the best fit model on two different machines to estimate the real-time energy consumption. The results show our model can get better results than single model. © 2013 IEEE.
DA  - 2013///
PY  - 2013
DO  - 10.1109/NAS.2013.12
SP  - 41
EP  - 49
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893220699&doi=10.1109%2fNAS.2013.12&partnerID=40&md5=6d74344dc699b3a17fc82ad2a98acbb8
DB  - Scopus
KW  - Green computing
KW  - Energy utilization
KW  - Digital storage
KW  - Data center management
KW  - Performance counters
KW  - CPU utilization
KW  - Best-fit models
KW  - Energy consumption mode
KW  - Energy prediction
KW  - Query performance
KW  - Single models
ER  - 

TY  - JOUR
TI  - Energy and carbon-efficient placement of virtual machines in distributed cloud data centers
AU  - Beloglazov, A.
AU  - Buyya, R.
T2  - Euro-Par 2013 Parallel Processing
DA  - 2010///
PY  - 2010
SP  - 317
EP  - 328
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106686887&partnerID=40&md5=fe53b8d123d4f1ceb1f0a2428d160fe3
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Allocation of virtual machines in cloud data centers-a survey of problem models and optimization algorithms
AU  - Mann, Z.Á.
T2  - ACM Computing Surveys
AB  - Data centers in public, private, and hybrid cloud settings make it possible to provision virtual machines (VMs) with unprecedented flexibility. However, purchasing, operating, and maintaining the underlying physical resources incurs significant monetary costs and environmental impact. Therefore, cloud providers must optimize the use of physical resources by a careful allocation of VMs to hosts, continuously balancing between the conflicting requirements on performance and operational costs. In recent years, several algorithms have been proposed for this important optimization problem. Unfortunately, the proposed approaches are hardly comparable because of subtle differences in the used problem models. This article surveys the used problem formulations and optimization algorithms, highlighting their strengths and limitations, and pointing out areas that need further research. © 2015 ACM 0360-0300/2015/08-ART11 $15.00.
DA  - 2015///
PY  - 2015
DO  - 10.1145/2797211
VL  - 48
IS  - 1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939815146&doi=10.1145%2f2797211&partnerID=40&md5=c4ecc3d47a2edb7743e909e4ceef49fb
DB  - Scopus
KW  - Cloud computing
KW  - Network security
KW  - Cloud-computing
KW  - Datacenter
KW  - Green computing
KW  - Optimization
KW  - Cloud data centers
KW  - Virtual machine
KW  - Data center
KW  - Live migrations
KW  - Environmental impact
KW  - Surveys
KW  - Physical resources
KW  - Modeling and optimization
KW  - Virtual machine consolidations
KW  - Virtual machine placements
KW  - VM consolidation
KW  - Optimization algorithms
KW  - VM placement
KW  - Live migration
KW  - Problem models
ER  - 

TY  - JOUR
TI  - Optimizing service placement for microservice architecture in clouds
AU  - Hu, Y.
AU  - de Laat, C.
AU  - Zhao, Z.
T2  - Applied Sciences (Switzerland)
AB  - As microservice architecture is becoming more popular than ever, developers intend to transform traditional monolithic applications into service-based applications (composed by a number of services). To deploy a service-based application in clouds, besides the resource demands of each service, the traffic demands between collaborative services are crucial for the overall performance. Poor handling of the traffic demands can result in severe performance degradation, such as high response time and jitter. However, current cluster schedulers fail to place services at the best possible machine, since they only consider the resource constraints but ignore the traffic demands between services. To address this problem, we propose a new approach to optimize the placement of service-based applications in clouds. The approach first partitions the application into several parts while keeping overall traffic between different parts to a minimum and then carefully packs the different parts into machines with respect to their resource demands and traffic demands. We implement a prototype scheduler and evaluate it with extensive experiments on testbed clusters. The results show that our approach outperforms existing container cluster schedulers and representative heuristics, leading to much less overall inter-machine traffic. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.
DA  - 2019///
PY  - 2019
DO  - 10.3390/app9214663
VL  - 9
IS  - 21
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075245058&doi=10.3390%2fapp9214663&partnerID=40&md5=d1693884b287c76d75abd11a561f6b05
DB  - Scopus
KW  - Cloud computing
KW  - Resource management
KW  - Microservice architecture
KW  - service placement
KW  - Cluster scheduling
KW  - Network optimization
ER  - 

TY  - JOUR
TI  - An Application Placement Technique for Concurrent IoT Applications in Edge and Fog Computing Environments
AU  - Goudarzi, M.
AU  - Wu, H.
AU  - Palaniswami, M.
AU  - Buyya, R.
T2  - IEEE Transactions on Mobile Computing
AB  - Fog/Edge computing emerges as a novel computing paradigm that harnesses resources in the proximity of the Internet of Things (IoT) devices so that, alongside with the cloud servers, provide services in a timely manner. However, due to the ever-increasing growth of IoT devices with resource-hungry applications, fog/edge servers with limited resources cannot efficiently satisfy the requirements of the IoT applications. Therefore, the application placement in the fog/edge computing environment, in which several distributed fog/edge servers and centralized cloud servers are available, is a challenging issue. In this article, we propose a weighted cost model to minimize the execution time and energy consumption of IoT applications, in a computing environment with multiple IoT devices, multiple fog/edge servers and cloud servers. Besides, a new application placement technique based on the Memetic Algorithm is proposed to make batch application placement decision for concurrent IoT applications. Due to the heterogeneity of IoT applications, we also propose a lightweight pre-scheduling algorithm to maximize the number of parallel tasks for the concurrent execution. The performance results demonstrate that our technique significantly improves the weighted cost of IoT applications up to 65 percent in comparison to its counterparts.  © 2002-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/TMC.2020.2967041
VL  - 20
IS  - 4
SP  - 1298
EP  - 1311
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102372700&doi=10.1109%2fTMC.2020.2967041&partnerID=40&md5=e561893f3f533f283de0dc79a1949604
DB  - Scopus
KW  - Fog computing
KW  - application placement
KW  - edge computing
KW  - optimization
KW  - Internet of Things (IoT)
KW  - Internet of things
KW  - Fog
KW  - Green computing
KW  - Energy utilization
KW  - Computing environments
KW  - Computing paradigm
KW  - Internet of thing (IOT)
KW  - Application placements
KW  - IOT applications
KW  - New applications
KW  - application partitioning
KW  - Concurrent execution
KW  - Memetic algorithms
ER  - 

TY  - CONF
TI  - Energy consumption of ICT infrastructures: An operator's viewpoint
AU  - Gosselin, S.
AU  - Saliou, F.
AU  - Bourgart, F.
AU  - Rouzic, E.L.
AU  - Masson, S.L.
AU  - Gati, A.
T2  - European Conference and Exhibition on Optical Communication, ECEOC 2012
AB  - The digital revolution has resulted up to now in the need for increasingly powerful and energy-hungry infrastructures. This paper provides an operator's viewpoint on how to face this challenge, including some intermediate results of France Telecom Orange energy action plan and some expected technological and architectural evolutions of ICT infrastructures.© 2012 OSA.
DA  - 2012///
PY  - 2012
DO  - 10.1364/ECEOC.2012.We.1.G.4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882650992&doi=10.1364%2fECEOC.2012.We.1.G.4&partnerID=40&md5=ddd6c0d16e332bb473ff175861972d0b
DB  - Scopus
KW  - Energy utilization
KW  - ICT infrastructures
KW  - Action plan
KW  - Architectural evolution
KW  - Digital revolution
KW  - Exhibitions
KW  - France telecom
KW  - Intermediate results
KW  - Optical communication
ER  - 

TY  - CONF
TI  - Dynamic Virtual Machine Placement for Cloud Computing Environments
AU  - Zheng, X.
AU  - Cai, Y.
T2  - Proceedings of the International Conference on Parallel Processing Workshops
AB  - With the increasing application of large scale cloud computing platforms, how to place virtual machine (VM) requests into available computing servers to reduce energy consumption has become a hot research subject. However, the current VM placement approaches are still not effective for live migrations with dynamic characters. In this paper, we proposed a dynamic VM placement scheme for energy efficient resource allocation in a cloud platform. Our dynamic VM placement scheme supports VM requests scheduling and live migration to minimize the number of active nodes in order to save the overall energy in a virtualized data center. Specifically, the proposed VM placement scheme is built on a statistical mathematic framework, and it incorporates all the virtualization overheads in the dynamic migration process. In addition, our scheme considers other important factors in related to power consumption, and it is ready to be extended with more considerations on users demand. We conduct extensive evaluations based on HPC jobs in a simulated environment. The results prove the effectiveness of our scheme. © 2014 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/ICPPW.2014.28
VL  - 2015-May
SP  - 121
EP  - 128
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946541227&doi=10.1109%2fICPPW.2014.28&partnerID=40&md5=2f0a333a1961c53ecf7a2a11b6a337ff
DB  - Scopus
KW  - Cloud computing
KW  - energy efficiency
KW  - Energy efficiency
KW  - virtual machine
KW  - Network security
KW  - Green computing
KW  - Energy utilization
KW  - Cloud computing environments
KW  - Virtual machine
KW  - Reduce energy consumption
KW  - Dynamic consolidation
KW  - Virtualized data centers
KW  - Virtual machine placements
KW  - dynamic consolidation
KW  - Energy-efficient resource allocation
KW  - Cloud computing platforms
KW  - Simulated environment
ER  - 

TY  - CONF
TI  - A new approach to model energy consumption of servers in data centers
AU  - Warkozek, G.
AU  - Drayer, E.
AU  - Debusschere, V.
AU  - Bacha, S.
T2  - 2012 IEEE International Conference on Industrial Technology, ICIT 2012, Proceedings
AB  - Electricity consumption of data centers increases continuously. Beside of the IT industry which tries to reduce this consumption by improving efficiency of components in data centers, there are research solutions based on an optimized energy management of data centers by acting on the IT load placement, then on cooling, start-up and shut down. In this context, this paper focus on energetic modeling of servers in data centers. In the state of art, the IT load is usually presented as a whole unit by means of the percentage CPU, while in this work, the percentage CPU is separated in two parts. The first one is the percentage CPU due to server self applications (for example a virtual machine manager), while the second part is due to services turning on the server. This classification led to a new linear model which shows that electricity consumption of data centers can be modeled as accumulated layers depending on what kind of software is running on the servers. The model is developed and then validated with experimental measurements on actual server conduct with the help of industrial partners. This modeling presents the first step of further works aim to optimize the energy consumption of data centers by knowing the IT load that is held on its servers. © 2012 IEEE.
DA  - 2012///
PY  - 2012
DO  - 10.1109/ICIT.2012.6209940
SP  - 211
EP  - 216
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863895161&doi=10.1109%2fICIT.2012.6209940&partnerID=40&md5=02e0d3298ca5f771077b6991984e2100
DB  - Scopus
KW  - energy efficiency
KW  - Energy efficiency
KW  - modeling
KW  - data center
KW  - energy consumption
KW  - virtual machine
KW  - Modeling
KW  - Network security
KW  - Energy-consumption
KW  - Datacenter
KW  - Green computing
KW  - Virtual machine
KW  - Electric power utilization
KW  - New approaches
KW  - IT industry
KW  - Electricity-consumption
KW  - CPU usage
KW  - Improving efficiency
KW  - Is researches
KW  - Optimized energy managements
ER  - 

TY  - CONF
TI  - NACER: A network-aware cost-efficient resource allocation method for processing-intensive tasks in distributed clouds
AU  - Ahvar, E.
AU  - Ahvar, S.
AU  - Crespi, N.
AU  - Garcia-Alfaro, J.
AU  - Mann, Z.A.
T2  - Proceedings - 2015 IEEE 14th International Symposium on Network Computing and Applications, NCA 2015
AB  - In the distributed cloud paradigm, data centers are geographically dispersed and interconnected over a wide-area network. Due to the geographical distribution of data centers, communication networks play an important role in distributed clouds in terms of communication cost and QoS. Large-scale, processing-intensive tasks require the cooperation of many VMs, which may be distributed in more than one data center and should communicate with each other. In this setting, the number of data enters serving the given task and the network distance among those data centers have critical impact on the communication cost, traffic and even completion time of the task. In this paper, we present the NACER algorithm, a Network-Aware Cost-Efficient Resource allocation method for optimizing the placement of largemulti-VM tasks in distributed clouds. NACER builds on ideas of the A∗ search algorithm from Artificial Intelligence research in order to obtain better results than typical greedy heuristics. We present extensive simulation results to compare the performance of NACER with competing heuristics and show its effectiveness. © 2015 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/NCA.2015.37
SP  - 90
EP  - 97
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963542034&doi=10.1109%2fNCA.2015.37&partnerID=40&md5=6975b2bdca6a360ac93b06ff947e0d65
DB  - Scopus
KW  - Artificial intelligence
KW  - Costs
KW  - Resource allocation
KW  - Extensive simulations
KW  - Wide area networks
KW  - Geographical distribution
KW  - Distributed clouds
KW  - Search Algorithms
KW  - VM placements
KW  - Communication cost
KW  - VM placement
KW  - Cost-efficient
KW  - Greedy heuristics
KW  - Artificial intelligence research
KW  - Distributed cloud
ER  - 

TY  - CONF
TI  - Elastictree: Saving energy in data center networks
AU  - Heller, B.
AU  - Seetharaman, S.
AU  - Mahadevan, P.
AU  - Yiakoumis, Y.
AU  - Sharma, P.
AU  - Banerjee, S.
AU  - McKeown, N.
T2  - Proceedings of NSDI 2010: 7th USENIX Symposium on Networked Systems Design and Implementation
AB  - Networks are a shared resource connecting critical IT infrastructure, and the general practice is to always leave them on. Yet, meaningful energy savings can result from improving a network's ability to scale up and down, as traffic demands ebb and flow. We present ElasticTree, a network-wide power1 manager, which dynamically adjusts the set of active network elements - links and switches - to satisfy changing data center traffic loads. We first compare multiple strategies for finding minimum-power network subsets across a range of traffic patterns. We implement and analyze ElasticTree on a prototype testbed built with production OpenFlow switches from three network vendors. Further, we examine the trade-offs between energy efficiency, performance and robustness, with real traces from a production e-commerce website. Our results demonstrate that for data center workloads, ElasticTree can save up to 50% of network energy, while maintaining the ability to handle traffic surges. Our fast heuristic for computing network subsets enables ElasticTree to scale to data centers containing thousands of nodes. We finish by showing how a network admin might configure ElasticTree to satisfy their needs for performance and fault tolerance, while minimizing their network power bill. © Proceedings of NSDI 2010: 7th USENIX Symposium on Networked Systems Design and Implementation. All rights reserved.
DA  - 2010///
PY  - 2010
SP  - 249
EP  - 264
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885109587&partnerID=40&md5=9481fd77a3fc956f2b2428be0f9e780f
DB  - Scopus
KW  - Energy efficiency
KW  - Economic and social effects
KW  - Fault tolerance
KW  - IT infrastructures
KW  - Data center networks
KW  - Systems analysis
KW  - Network element
KW  - Shared resources
KW  - Traffic pattern
KW  - E-commerce websites
KW  - Multiple strategy
KW  - Openflow switches
ER  - 

TY  - CONF
TI  - CACEV: A cost and carbon emission-efficient virtual machine placement method for green distributed clouds
AU  - Ahvar, E.
AU  - Ahvar, S.
AU  - Mann, Z.A.
AU  - Crespi, N.
AU  - Garcia-Alfaro, J.
AU  - Glitho, R.
T2  - Proceedings - 2016 IEEE International Conference on Services Computing, SCC 2016
AB  - Distributed clouds have recently attracted many cloud providers and researchers as a topic of intensive interest. High energy costs and carbon emissions are two significant problems in distributed clouds. Due to the geographic distribution of data centers (DCs), there are a variety of resources, energy prices and carbon emission rates to consider in a distributed cloud, which makes the placement of virtual machines (VMs) for cost and carbon efficiency even more critical than in centralized clouds. Most previous work in this field investigated either optimizing cost without considering the amount of produced carbon or vice versa. This paper presents a cost and carbon emission-efficient VM placement method (CACEV) in distributed clouds. CACEV considers geographically varying energy prices and carbon emission rates as well as optimizing both network and server resources at the same time. By combining prediction-based A∗ algorithm with Fuzzy Sets technique, CACEV makes an intelligent decision to optimize cost and carbon emission for providers. Simulation results show the applicability and performance of CACEV. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/SCC.2016.43
SP  - 275
EP  - 282
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989840666&doi=10.1109%2fSCC.2016.43&partnerID=40&md5=79736e6b152742d02ad4c4fadd766ae4
DB  - Scopus
KW  - Cloud computing
KW  - Costs
KW  - Carbon emissions
KW  - Energy
KW  - Java programming language
KW  - Virtual machine placements
KW  - Geographical distribution
KW  - Data center (DCs)
KW  - Distributed clouds
KW  - VM placements
KW  - VM placement
KW  - Distributed cloud
KW  - Carbon efficiency
KW  - Carbon emission
KW  - Cost
KW  - Intelligent decisions
ER  - 

TY  - JOUR
TI  - An Energy Efficient Ant Colony System for Virtual Machine Placement in Cloud Computing
AU  - Liu, X.-F.
AU  - Zhan, Z.-H.
AU  - Deng, J.D.
AU  - Li, Y.
AU  - Gu, T.
AU  - Zhang, J.
T2  - IEEE Transactions on Evolutionary Computation
AB  - Virtual machine placement (VMP) and energy efficiency are significant topics in cloud computing research. In this paper, evolutionary computing is applied to VMP to minimize the number of active physical servers, so as to schedule underutilized servers to save energy. Inspired by the promising performance of the ant colony system (ACS) algorithm for combinatorial problems, an ACS-based approach is developed to achieve the VMP goal. Coupled with order exchange and migration (OEM) local search techniques, the resultant algorithm is termed an OEMACS. It effectively minimizes the number of active servers used for the assignment of virtual machines (VMs) from a global optimization perspective through a novel strategy for pheromone deposition which guides the artificial ants toward promising solutions that group candidate VMs together. The OEMACS is applied to a variety of VMP problems with differing VM sizes in cloud environments of homogenous and heterogeneous servers. The results show that the OEMACS generally outperforms conventional heuristic and other evolutionary-based approaches, especially on VMP with bottleneck resource characteristics, and offers significant savings of energy and more efficient use of different resources. © 1997-2012 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/TEVC.2016.2623803
VL  - 22
IS  - 1
SP  - 113
EP  - 128
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041323218&doi=10.1109%2fTEVC.2016.2623803&partnerID=40&md5=0bc23775671f54d3ee44222d97082fcd
DB  - Scopus
KW  - cloud computing
KW  - Cloud computing
KW  - Energy efficiency
KW  - Network security
KW  - Green computing
KW  - Optimization
KW  - Distributed computer systems
KW  - Virtual machine
KW  - Ant colony optimization
KW  - Virtual machine placements
KW  - Evolutionary algorithms
KW  - Ant colony system (ACS)
KW  - Ant colony system algorithms
KW  - Ant colony systems
KW  - Bottleneck resources
KW  - Combinatorial problem
KW  - Evolutionary computing
KW  - Global optimization
KW  - Heterogeneous servers
KW  - Local search techniques
KW  - Program compilers
KW  - virtual machine placement (VMP)
ER  - 

TY  - CONF
TI  - A traffic and resource-aware energy-saving mechanism in software defined networks
AU  - Rahnamay-Naeini, M.
AU  - Baidya, S.S.
AU  - Siavashi, E.
AU  - Ghani, N.
T2  - 2016 International Conference on Computing, Networking and Communications, ICNC 2016
AB  - Energy-saving mechanisms are crucial in reducing the environmental impact and energy costs of ICT network infrastructures. Integration of renewable energy resources to ICT infrastructures and dynamic energy pricing suggest that not only traffic demands and communication resources but also energy resources should be taken into account in designing energy-saving mechanisms for network infrastructures. With the introduction of Software Defined Networking (SDN) paradigm with a logically centralized controller architecture and programmability of network elements new opportunities have emerged for improving the energy efficiency of the networks. In this paper, by exploiting the SDN features and adopting the idea of powering down unnecessary links to save energy, we formulate an optimization problem for identifying the optimum set of active links that reduces the energy cost of the network while satisfying traffic demands and respecting communication and energy resources. To solve this optimization problem we introduce a computationally efficient heuristic algorithm. Using simulation results, we show that the solutions of the heuristic algorithm reduce the energy cost of the network up to 34% while satisfying the constraints of the problem. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/ICCNC.2016.7440553
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966570570&doi=10.1109%2fICCNC.2016.7440553&partnerID=40&md5=b2cd0db5729db83444365e8c21d2c7e5
DB  - Scopus
KW  - Energy efficiency
KW  - Energy Efficiency
KW  - Costs
KW  - Heuristic algorithms
KW  - Transportation
KW  - Optimization
KW  - Energy conservation
KW  - Energy resources
KW  - Environmental impact
KW  - Renewable energy resources
KW  - Software defined networking
KW  - Cost reduction
KW  - Computationally efficient
KW  - Problem solving
KW  - Software defined networking (SDN)
KW  - Communication resources
KW  - Centralized controllers
KW  - Constraint satisfaction problems
KW  - Energy Price
KW  - Energy prices
KW  - Energy saving mechanism
KW  - Heuristic Algorithm
KW  - Integration of renewable energies
KW  - Software Defined Networks
KW  - Traffic Demand
KW  - Traffic demands
ER  - 

TY  - JOUR
TI  - Priority, network and energy-aware placement of IoT-based application services in fog-cloud environments
AU  - Hassan, H.O.
AU  - Azizi, S.
AU  - Shojafar, M.
T2  - IET Communications
AB  - Fog computing is a decentralised model which can help cloud computing for providing high quality-of-service (QoS) for the Internet of Things (IoT) application services. Service placement problem (SPP) is the mapping of services among fog and cloud resources. It plays a vital role in response time and energy consumption in fog–cloud environments. However, providing an efficient solution to this problem is a challenging task due to difficulties such as different requirements of services, limited computing resources, different delay, and power consumption profile of devices in fog domain. Motivated by this, in this study, we propose an efficient policy, called MinRE, for SPP in fog–cloud systems. To provide both QoS for IoT services and energy efficiency for fog service providers, we classify services into two categories: critical services and normal ones. For critical services, we propose MinRes, which aims to minimise response time, and for normal ones, we propose MinEng, whose goal is reducing the energy consumption of fog environment. Our extensive simulation experiments show that our policy improves the energy consumption up to 18%, the percentage of deadline satisfied services up to 14% and the average response time up to 10% in comparison with the second-best results. © The Institution of Engineering and Technology 2020
DA  - 2020///
PY  - 2020
DO  - 10.1049/iet-com.2020.0007
VL  - 14
IS  - 13
SP  - 2117
EP  - 2129
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090241606&doi=10.1049%2fiet-com.2020.0007&partnerID=40&md5=b0ab2d79e10932812521254937d7946a
DB  - Scopus
KW  - Energy efficiency
KW  - Fog computing
KW  - Internet of things
KW  - Fog
KW  - Quality of service
KW  - Green computing
KW  - Energy utilization
KW  - Power management (telecommunication)
KW  - Cloud environments
KW  - Internet of thing (IOT)
KW  - Extensive simulations
KW  - Energy policy
KW  - Computing resource
KW  - Service placements
KW  - Critical service
KW  - Application services
KW  - Service provider
ER  - 

TY  - JOUR
TI  - A novel virtual machine deployment algorithm with energy efficiency in cloud computing
AU  - Zhou, Z.
AU  - Hu, Z.-G.
AU  - Song, T.
AU  - Yu, J.-Y.
T2  - Journal of Central South University
AB  - In order to improve the energy efficiency of large-scale data centers, a virtual machine (VM) deployment algorithm called three-threshold energy saving algorithm (TESA), which is based on the linear relation between the energy consumption and (processor) resource utilization, is proposed. In TESA, according to load, hosts in data centers are divided into four classes, that is, host with light load, host with proper load, host with middle load and host with heavy load. By defining TESA, VMs on lightly loaded host or VMs on heavily loaded host are migrated to another host with proper load; VMs on properly loaded host or VMs on middling loaded host are kept constant. Then, based on the TESA, five kinds of VM selection policies (minimization of migrations policy based on TESA (MIMT), maximization of migrations policy based on TESA (MAMT), highest potential growth policy based on TESA (HPGT), lowest potential growth policy based on TESA (LPGT) and random choice policy based on TESA (RCT)) are presented, and MIMT is chosen as the representative policy through experimental comparison. Finally, five research directions are put forward on future energy management. The results of simulation indicate that, as compared with single threshold (ST) algorithm and minimization of migrations (MM) algorithm, MIMT significantly improves the energy efficiency in data centers. © 2015, Central South University Press and Springer-Verlag Berlin Heidelberg.
DA  - 2015///
PY  - 2015
DO  - 10.1007/s11771-015-2608-5
VL  - 22
IS  - 3
SP  - 974
EP  - 983
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924804281&doi=10.1007%2fs11771-015-2608-5&partnerID=40&md5=525a3fe1e6068ff9427f5b215974925f
DB  - Scopus
KW  - cloud computing
KW  - Cloud computing
KW  - energy efficiency
KW  - Energy efficiency
KW  - Network security
KW  - Green computing
KW  - Energy utilization
KW  - Virtual machine
KW  - Resource utilizations
KW  - Energy management
KW  - Large scale data
KW  - Experimental comparison
KW  - energy management
KW  - Deployment algorithms
KW  - Potential growth
KW  - Selection policies
KW  - three-threshold
KW  - Threshold energy
KW  - virtual machine (VM) selection policy
ER  - 

TY  - CONF
TI  - Identifying and using energy-critical paths
AU  - Vasić, N.
AU  - Bhurat, P.
AU  - Novaković, D.
AU  - Canini, M.
AU  - Shekhar, S.
AU  - Kostić, D.
T2  - Proceedings of the 7th Conference on Emerging Networking EXperiments and Technologies, CoNEXT'11
AB  - The power consumption of the Internet and datacenter networks is already significant, and threatens to shortly hit the power delivery limits while the hardware is trying to sustain ever-increasing traffic requirements. Existing energy-reduction approaches in this domain advocate recomputing network configuration with each substantial change in demand. Unfortunately, computing the minimum network subset is computationally hard and does not scale. Thus, the network is forced to operate with diminished performance during the recomputation periods. In this paper, we propose REsPoNse, a framework which overcomes the optimality-scalability trade-off. The insight in REsPoNse is to identify a few energy-critical paths off-line, install them into network elements, and use a simple online element to redirect the traffic in a way that enables large parts of the network to enter a low-power state. We evaluate REsPoNse with real network data and demonstrate that it achieves the same energy savings as the existing approaches, with marginal impact on network scalability and application performance. © 2011 ACM.
DA  - 2011///
PY  - 2011
DO  - 10.1145/2079296.2079314
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889750111&doi=10.1145%2f2079296.2079314&partnerID=40&md5=10e1659fb4a623f3e7641e6fdf07de7a
DB  - Scopus
KW  - Scalability
KW  - Network configuration
KW  - Large parts
KW  - Application performance
KW  - Experiments
KW  - Low Power
KW  - Network element
KW  - Network scalability
KW  - Traffic requirements
KW  - Electric power transmission
KW  - Minimum networks
KW  - Online elements
KW  - Power delivery
KW  - Re-computing
KW  - Real networks
KW  - Recomputation
ER  - 

TY  - JOUR
TI  - Dynamic VM placement method for minimizing energy and carbon cost in geographically distributed cloud data centers
AU  - Khosravi, A.
AU  - Andrew, L.L.H.
AU  - Buyya, R.
T2  - IEEE Transactions on Sustainable Computing
AB  - Cloud data centers consume a large amount of energy that leads to a high carbon footprint. Taking into account a carbon tax imposed on the emitted carbon makes energy and carbon cost play a major role in data centers' operational costs. To address this challenge, we investigate parameters that have the biggest effect on energy and carbon footprint cost to propose more efficient VM placement approaches. We formulate the total energy cost as a function of the energy consumed by servers plus overhead energy, which is computed through power usage effectiveness (PUE) metric as a function of IT load and outside temperature. Furthermore, we consider that data center sites have access to renewable energy sources. This helps to reduce their reliance on 'brown' electricity delivered by off-site providers, which is typically drawn from polluting sources. We then propose multiple VM placement approaches to evaluate their performance and identify the parameters with the greatest impact on the total renewable and brown energy consumption, carbon footprint, and cost. The results show that the approach which considers dynamic PUE, renewable energy sources, and changes in the total energy consumption outperforms the others while still meeting cloud users' service level agreements. © 2016 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/TSUSC.2017.2709980
VL  - 2
IS  - 2
SP  - 183
EP  - 196
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051969893&doi=10.1109%2fTSUSC.2017.2709980&partnerID=40&md5=be7ebd313d414d378d5c862213c43d5c
DB  - Scopus
KW  - Cloud computing
KW  - energy consumption
KW  - green computing
KW  - Carbon footprint
KW  - Data centers
KW  - Green computing
KW  - Energy utilization
KW  - Cloud data centers
KW  - Renewable energy resources
KW  - Service Level Agreements
KW  - Minimizing energy
KW  - data centers
KW  - Renewable energy source
KW  - Emission control
KW  - Total energy consumption
KW  - Distributed clouds
KW  - VM placements
KW  - VM placement
KW  - Natural resources
ER  - 

TY  - JOUR
TI  - Power consumption estimation models for processors, virtual machines, and servers
AU  - Möbius, C.
AU  - Dargie, W.
AU  - Schill, A.
T2  - IEEE Transactions on Parallel and Distributed Systems
AB  - The power consumption of presently available Internet servers and data centers is not proportional to the work they accomplish. The scientific community is attempting to address this problem in a number of ways, for example, by employing dynamic voltage and frequency scaling, selectively switching off idle or underutilized servers, and employing energy-aware task scheduling. Central to these approaches is the accurate estimation of the power consumption of the various subsystems of a server, particularly, the processor. We distinguish between power consumption measurement techniques and power consumption estimation models. The techniques refer to the art of instrumenting a system to measure its actual power consumption whereas the estimation models deal with indirect evidences (such as information pertaining to CPU utilization or events captured by hardware performance counters) to reason about the power consumption of a system under consideration. The paper provides a comprehensive survey of existing or proposed approaches to estimate the power consumption of single-core as well as multicore processors, virtual machines, and an entire server. © 1990-2012 IEEE.
DA  - 2014///
PY  - 2014
DO  - 10.1109/TPDS.2013.183
VL  - 25
IS  - 6
SP  - 1600
EP  - 1614
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901020434&doi=10.1109%2fTPDS.2013.183&partnerID=40&md5=839dc24d2a3a2e2198abe57b81f81200
DB  - Scopus
KW  - energy-efficiency
KW  - Dynamic frequency scaling
KW  - Voltage scaling
KW  - Virtual machines
KW  - Computer simulation
KW  - Dynamic voltage and frequency scaling
KW  - Scientific community
KW  - Energy-aware task scheduling
KW  - Hardware performance counters
KW  - Power consumption model
KW  - Consumption measurement
KW  - power consumption estimation
KW  - Power consumption estimation
KW  - Power consumption models
KW  - processor's power consumption
KW  - server's power consumption
KW  - virtual machine's power consumption
ER  - 

TY  - JOUR
TI  - Hierarchical Approach for Efficient Workload Management in Geo-Distributed Data Centers
AU  - Forestiero, A.
AU  - Mastroianni, C.
AU  - Meo, M.
AU  - Papuzzo, G.
AU  - Sheikhalishahi, M.
T2  - IEEE Transactions on Green Communications and Networking
AB  - Geographically distributed data centers (DCs) offer promising business opportunities to both big companies that own several sites and multi-owner inter-cloud infrastructures. In these scenarios, workload management is a particularly challenging task, since the autonomy of single DCs should be preserved while global objectives, such as cost reduction and load balance, should be achieved. In this paper, a hierarchical approach for workload management in geographically distributed DCs is presented. The proposed solution is composed of two algorithms devoted to workload assignment and migration. Both algorithms are based on the computation of a simple function that represents the cost of running some workload in the different sites of the distributed DC. The framework requires a very limited exchange of state information among the sites and preserves the autonomy of single DCs and, at the same time, allows for an integrated management of heterogeneous platforms. Performance is analyzed for a specific infrastructure composed of four DCs, with two goals: 1) load balance and 2) energy cost reduction. Results show that the proposed approach smoothly adapts the workload distribution to variations of energy cost and load, while achieving the desired combination of management objectives. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/TGCN.2016.2603586
VL  - 1
IS  - 1
SP  - 97
EP  - 111
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044668066&doi=10.1109%2fTGCN.2016.2603586&partnerID=40&md5=144c392d1dd55a8491d0ee597ff1562d
DB  - Scopus
KW  - Cloud computing
KW  - load balancing
KW  - Resource allocation
KW  - Energy conservation
KW  - Vm migrations
KW  - Cost reduction
KW  - cost saving
KW  - energy saving
KW  - Cost saving
KW  - Business opportunities
KW  - Work-load distribution
KW  - Energy cost reduction
KW  - Geographical data
KW  - geographical data centers
KW  - Heterogeneous platforms
KW  - Integrated management
KW  - VM migrations
ER  - 

TY  - JOUR
TI  - CloudSim: A toolkit for modeling and simulation of cloud computing environments and evaluation of resource provisioning algorithms
AU  - Calheiros, R.N.
AU  - Ranjan, R.
AU  - Beloglazov, A.
AU  - De Rose, C.A.F.
AU  - Buyya, R.
T2  - Software - Practice and Experience
AB  - Cloud computing is a recent advancement wherein IT infrastructure and applications are provided as 'services' to end-users under a usage-based payment model. It can leverage virtualized services even on the fly based on requirements (workload patterns and QoS) varying with time. The application services hosted under Cloud computing model have complex provisioning, composition, configuration, and deployment requirements. Evaluating the performance of Cloud provisioning policies, application workload models, and resources performance models in a repeatable manner under varying system and user configurations and requirements is difficult to achieve. To overcome this challenge, we propose CloudSim: an extensible simulation toolkit that enables modeling and simulation of Cloud computing systems and application provisioning environments. The CloudSim toolkit supports both system and behavior modeling of Cloud system components such as data centers, virtual machines (VMs) and resource provisioning policies. It implements generic application provisioning techniques that can be extended with ease and limited effort. Currently, it supports modeling and simulation of Cloud computing environments consisting of both single and inter-networked clouds (federation of clouds). Moreover, it exposes custom interfaces for implementing policies and provisioning techniques for allocation of VMs under inter-networked Cloud computing scenarios. Several researchers from organizations, such as HP Labs in U.S.A., are using CloudSim in their investigation on Cloud resource provisioning and energy-efficient management of data center resources. The usefulness of CloudSim is demonstrated by a case study involving dynamic provisioning of application services in the hybrid federated clouds environment. The result of this case study proves that the federated Cloud computing model significantly improves the application QoS requirements under fluctuating resource and service demand patterns. Copyright © 2010 John Wiley & Sons, Ltd.
DA  - 2011///
PY  - 2011
DO  - 10.1002/spe.995
VL  - 41
IS  - 1
SP  - 23
EP  - 50
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650777991&doi=10.1002%2fspe.995&partnerID=40&md5=e5acc8ba81ccb3bce707222348c8d53b
DB  - Scopus
KW  - Cloud computing
KW  - resource management
KW  - performance evaluation
KW  - Resource management
KW  - Performance evaluation
KW  - Quality of service
KW  - Resource allocation
KW  - Distributed computer systems
KW  - Natural resources management
KW  - Application scheduling
KW  - Clouds
KW  - Computer simulation
KW  - Satellite communication systems
KW  - Research
KW  - application scheduling
KW  - Mathematical models
KW  - modelling and simulation
KW  - Modelling and simulations
ER  - 

TY  - CONF
TI  - Cost of virtual machine live migration in clouds: A performance evaluation
AU  - Voorsluys, W.
AU  - Broberg, J.
AU  - Venugopal, S.
AU  - Buyya, R.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Virtualization has become commonplace in modern data centers, often referred as "computing clouds". The capability of virtual machine live migration brings benefits such as improved performance, manageability and fault tolerance, while allowing workload movement with a short service downtime. However, service levels of applications are likely to be negatively affected during a live migration. For this reason, a better understanding of its effects on system performance is desirable. In this paper, we evaluate the effects of live migration of virtual machines on the performance of applications running inside Xen VMs. Results show that, in most cases, migration overhead is acceptable but cannot be disregarded, especially in systems where availability and responsiveness are governed by strict Service Level Agreements. Despite that, there is a high potential for live migration applicability in data centers serving modern Internet applications. Our results are based on a workload covering the domain of multi-tier Web 2.0 applications. © 2009 Springer-Verlag.
DA  - 2009///
PY  - 2009
DO  - 10.1007/978-3-642-10665-1_23
VL  - 5931 LNCS
SP  - 254
EP  - 265
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-71749095784&doi=10.1007%2f978-3-642-10665-1_23&partnerID=40&md5=ced1b0a8ca8421f33593bb8dd3698095
DB  - Scopus
KW  - Performance
KW  - Performance evaluation
KW  - Datacenter
KW  - Cloud platforms
KW  - Virtual machine
KW  - Virtual machines
KW  - Performances evaluation
KW  - Migration
KW  - Live migrations
KW  - Virtualizations
KW  - Xen
KW  - Systems performance
KW  - Computing clouds
KW  - Service levels
ER  - 

TY  - JOUR
TI  - CloudSim: A novel framework for modeling and simulation of cloud computing infrastructures and services
AU  - Calheiros, R.N.
AU  - Ranjan, R.
AU  - De Rose, C.A.F.
AU  - Buyya, R.
T2  - CloudSim: A Novel Framework for Modeling and Simulation of Cloud Computing Infrastructures and Services
DA  - 2009///
PY  - 2009
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-73849141413&partnerID=40&md5=14fe9d9f1b05364d29c089b256061c15
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Which is the best algorithm for virtual machine placement optimization?
AU  - Mann, Z.Á.
AU  - Szabó, M.
T2  - Concurrency and Computation: Practice and Experience
AB  - One of the key problems for Infrastructure-as-a-Service providers is finding the optimal allocation of virtual machines on the physical machines available in the provider's data center. Since the allocation has significant impact on operational costs as well as on the performance of the accommodated applications, several algorithms have been proposed for the virtual machine placement problem. So far, no objective comparison of the proposed algorithms has been provided; therefore, it is not known which one works best or what factors influence the performance of the algorithms. In this paper, we present an environment and methodology for such comparisons and compare 7 different algorithms using the proposed environment and methodology. Our results showcase differences of up to 66% between the effectiveness of different algorithms on the same real-world workload traces, thus underlining the importance of objectively comparing the performance of competing algorithms. Copyright © 2017 John Wiley & Sons, Ltd.
DA  - 2017///
PY  - 2017
DO  - 10.1002/cpe.4083
VL  - 29
IS  - 10
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016277509&doi=10.1002%2fcpe.4083&partnerID=40&md5=805b2f0a1d8771c5d33845276a07e882
DB  - Scopus
KW  - cloud computing
KW  - Cloud computing
KW  - data center
KW  - Network security
KW  - Data centers
KW  - Virtual machine
KW  - Infrastructure as a service (IaaS)
KW  - Virtual machine placements
KW  - VM consolidation
KW  - VM placements
KW  - VM placement
KW  - Competing algorithms
KW  - Infrastructure-as-a-Service
KW  - Optimal allocation
KW  - Real-world
KW  - virtual machines
ER  - 

TY  - JOUR
TI  - Delay-Aware Microservice Coordination in Mobile Edge Computing: A Reinforcement Learning Approach
AU  - Wang, S.
AU  - Guo, Y.
AU  - Zhang, N.
AU  - Yang, P.
AU  - Zhou, A.
AU  - Shen, X.
T2  - IEEE Transactions on Mobile Computing
AB  - As an emerging service architecture, microservice enables decomposition of a monolithic web service into a set of independent lightweight services which can be executed independently. With mobile edge computing, microservices can be further deployed in edge clouds dynamically, launched quickly, and migrated across edge clouds easily, providing better services for users in proximity. However, the user mobility can result in frequent switch of nearby edge clouds, which increases the service delay when users move away from their serving edge clouds. To address this issue, this article investigates microservice coordination among edge clouds to enable seamless and real-time responses to service requests from mobile users. The objective of this work is to devise the optimal microservice coordination scheme which can reduce the overall service delay with low costs. To this end, we first propose a dynamic programming-based offline microservice coordination algorithm, that can achieve the globally optimal performance. However, the offline algorithm heavily relies on the availability of the prior information such as computation request arrivals, time-varying channel conditions and edge cloud's computation capabilities required, which is hard to be obtained. Therefore, we reformulate the microservice coordination problem using Markov decision process framework and then propose a reinforcement learning-based online microservice coordination algorithm to learn the optimal strategy. Theoretical analysis proves that the offline algorithm can find the optimal solution while the online algorithm can achieve near-optimal performance. Furthermore, based on two real-world datasets, i.e., the Telecom's base station dataset and Taxi Track dataset from Shanghai, experiments are conducted. The experimental results demonstrate that the proposed online algorithm outperforms existing algorithms in terms of service delay and migration costs, and the achieved performance is close to the optimal performance obtained by the offline algorithm.  © 2002-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/TMC.2019.2957804
VL  - 20
IS  - 3
SP  - 939
EP  - 951
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100825426&doi=10.1109%2fTMC.2019.2957804&partnerID=40&md5=5d00c7389a242d200734018ecee0b0e6
DB  - Scopus
KW  - Mobile edge computing
KW  - mobile edge computing
KW  - Microservice
KW  - Markov processes
KW  - computation offloading
KW  - migration
KW  - Reinforcement learning
KW  - Service delays
KW  - Web services
KW  - Migration
KW  - On-line algorithms
KW  - Delay
KW  - Edge clouds
KW  - Optimal systems
KW  - Dynamic programming
KW  - delay
KW  - Optimal performance
KW  - coordination
KW  - Coordination
KW  - Coordination algorithms
KW  - Off-line algorithm
KW  - Taxicabs
ER  - 

TY  - CONF
TI  - Evaluation of time-series database on microservice architecture for health monitoring system
AU  - Simanjuntak, E.
AU  - Surantha, N.
AU  - Isa, S.M.
T2  - ISESD 2022 - 2022 International Symposium on Electronics and Smart Devices, Proceeding
AB  - Recently, the research on daily health monitoring using a wearable sensor has been continually evolving. In the future, when this system is actually implemented, a vast amount of data transmission will be conducted from multiple sensors to the cloud server. A system capable of transmitting, storing, processing, and visualizing massive volumes of data is necessary for health monitoring. This research aims to develop a health monitoring system that can accommodate a high amount of data with high performance. The proposed techniques include numerous modifications to system architecture and technology. In the system architecture, a multi-database is proposed to enhance the reliability of the system. As an enhancement to the technology aspect, a time series database and MQTT server are proposed. As the result, the proposed system achieves significantly better performance in terms of throughput, database-writing speed, and database-reading speed compared to the existing system.  © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ISESD56103.2022.9980618
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146216621&doi=10.1109%2fISESD56103.2022.9980618&partnerID=40&md5=74b1b68edd7e869cff161f1d99bbe56d
DB  - Scopus
KW  - Microservices
KW  - Performance
KW  - Microservice
KW  - Internet of things
KW  - Computer architecture
KW  - Cloud servers
KW  - Architecture
KW  - Time series
KW  - Health monitoring
KW  - Database systems
KW  - Data-transmission
KW  - Digital health
KW  - Digital Health
KW  - Health Monitoring
KW  - Health monitoring system
KW  - Internet-of-Things
KW  - Multiple sensors
KW  - Systems architecture
KW  - Time Series Database
KW  - Time-series database
KW  - Wearable sensors
ER  - 

TY  - JOUR
TI  - Open-source systems monitoring and alerting toolkit
AU  - Prometheus, M.
DA  - 2015///
PY  - 2015
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219497659&partnerID=40&md5=6bd26e1b0f55acc6cd4ca23aa22fb2f9
DB  - Scopus
ER  - 

TY  - JOUR
TI  - An in-memory database that persists on disk
AU  - Redis team, A.
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219501042&partnerID=40&md5=f1dbfeed68d9b97dd005d266b0ded871
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Dynamic Task Offloading and Resource Allocation for Mobile-Edge Computing in Dense Cloud RAN
AU  - Zhang, Q.
AU  - Gui, L.
AU  - Hou, F.
AU  - Chen, J.
AU  - Zhu, S.
AU  - Tian, F.
T2  - IEEE Internet of Things Journal
AB  - With the unprecedented development of smart mobile devices (SMDs), e.g., Internet-of-Things devices and smartphones, various computation-intensive applications are explosively increasing in ultradense networks (UDNs). Mobile-edge computing (MEC) has emerged as a key technology to alleviate the computation workloads of SMDs and decrease service latency for computation-intensive applications. With the benefits of network function virtualization, MEC can be integrated with the cloud radio access network (C-RAN) in UDNs for computation and communication cooperation. However, with stochastic computation task arrivals and time-varying channel states, it is challenging to offload computation tasks online with energy-efficient computation and radio resource management. In this article, we investigate the task offloading and resource allocation problem in MEC-enabled dense C-RAN, aiming at optimizing network energy efficiency. A stochastic mixed-integer nonlinear programming problem is formulated to jointly optimize the task offloading decision, elastic computation resource scheduling, and radio resource allocation. To tackle the problem, the Lyapunov optimization theory is introduced to decompose the original problem into four individual subproblems which are solved by convex decomposition methods and matching game. We theoretically analyze the tradeoff between energy efficiency and service delay. Extensive simulations evaluate the impacts of system parameters on both energy efficiency and service delay. The simulation results also validate the superiority of the proposed task offloading and resource allocation scheme in dense C-RAN. © 2014 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/JIOT.2020.2967502
VL  - 7
IS  - 4
SP  - 3282
EP  - 3299
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081559496&doi=10.1109%2fJIOT.2020.2967502&partnerID=40&md5=1253c1482d5dec255243e46247d5a6a8
DB  - Scopus
KW  - resource allocation
KW  - Energy efficiency
KW  - Edge computing
KW  - Internet protocols
KW  - Resource allocation
KW  - task offloading
KW  - Radio access networks
KW  - Stochastic systems
KW  - Computation theory
KW  - Integer programming
KW  - Nonlinear programming
KW  - Computation intensives
KW  - Resource allocation problem
KW  - Network function virtualization
KW  - Radio
KW  - mobile-edge computing (MEC)
KW  - Radio resource allocation
KW  - Cloud radio access network (C-RAN)
KW  - Lyapunov optimization
KW  - Mixed integer non-linear programming problems
KW  - Radio resource management
KW  - Resource allocation schemes
KW  - Site selection
KW  - Stochastic computations
KW  - Time-varying channel state
KW  - ultradense network (UDN)
ER  - 

TY  - JOUR
TI  - An empirical study of security practices for microservices systems
AU  - Rezaei Nasab, A.
AU  - Shahin, M.
AU  - Hoseyni Raviz, S.A.
AU  - Liang, P.
AU  - Mashmool, A.
AU  - Lenarduzzi, V.
T2  - Journal of Systems and Software
AB  - Despite the numerous benefits of microservices systems, security has been a critical issue in such systems. Several factors explain this difficulty, including a knowledge gap among microservices practitioners on properly securing a microservices system. To (partially) bridge this gap, we conducted an empirical study. We first manually analyzed 861 microservices security points, including 567 issues, 9 documents, and 3 wiki pages from 10 GitHub open-source microservices systems and 306 Stack Overflow posts concerning security in microservices systems. In this study, a microservices security point is referred to as “a GitHub issue, a Stack Overflow post, a document, or a wiki page that entails 5 or more microservices security paragraphs”. Our analysis led to a catalog of 28 microservices security practices. We then ran a survey with 74 microservices practitioners to evaluate the usefulness of these 28 practices. Our findings demonstrate that the survey respondents affirmed the usefulness of the 28 practices. We believe that the catalog of microservices security practices can serve as a valuable resource for microservices practitioners to more effectively address security issues in microservices systems. It can also inform the research community of the required or less explored areas to develop microservices-specific security practices and tools. © 2022 Elsevier Inc.
DA  - 2023///
PY  - 2023
DO  - 10.1016/j.jss.2022.111563
VL  - 198
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144630737&doi=10.1016%2fj.jss.2022.111563&partnerID=40&md5=fd7865e9b51a9f61b4d2a72af4e4dfb2
DB  - Scopus
KW  - Security
KW  - Microservice
KW  - Open systems
KW  - Security systems
KW  - Empirical studies
KW  - Critical issues
KW  - Empirical study
KW  - Practice
KW  - Practitioner
KW  - Practitioners
KW  - Security point
KW  - Security Practice
KW  - Stack overflow
KW  - System security
ER  - 

TY  - JOUR
TI  - Design, Modeling, and Implementation of Robust Migration of Stateful Edge Microservices
AU  - Calagna, A.
AU  - Yu, Y.
AU  - Giaccone, P.
AU  - Chiasserini, C.F.
T2  - IEEE Transactions on Network and Service Management
AB  - Stateful migration has emerged as the key solution to support latency-sensitive microservices at the edge while ensuring a satisfying experience for mobile users. In this paper, we address two relevant issues affecting stateful migration, namely, the migration of containerized microservices and that of the associated data connection. We do so by first introducing a novel network solution, based on OvS, that permits to preserve the established connection with mobile end users upon migrating a microservice. Then, using Podman and CRIU, we experimentally characterize the fundamental migration KPIs, i.e., migration duration and microservice downtime, and we devise an analytical model that, accounting for all the relevant real-world aspects of stateful migration, provides an accurate upper bound on such KPIs. We validate our model using real-world microservices, namely, MQTT Broker and Memcached, and show that it can predict KPIs values with an error that is up to 99.7% smaller than that yielded by the state of the art. Finally, we consider a UAV controller as relevant microservice use case and demonstrate how our model can be exploited to effectively configure the system parameters so that the required QoE level is met. © 2004-2012 IEEE.
DA  - 2024///
PY  - 2024
DO  - 10.1109/TNSM.2023.3331750
VL  - 21
IS  - 2
SP  - 1877
EP  - 1893
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177080673&doi=10.1109%2fTNSM.2023.3331750&partnerID=40&md5=8fa1775eedd8f19b5808eda7a56ca2ac
DB  - Scopus
KW  - modeling
KW  - microservices
KW  - Microservice
KW  - Containers
KW  - Microservice architecture
KW  - Modeling
KW  - Quality of service
KW  - Migration
KW  - Virtual reality
KW  - Quality of experience
KW  - Experimental analysis
KW  - Iterative methods
KW  - Network function virtualization
KW  - Transfer functions
KW  - network function virtualization
KW  - Real-world
KW  - Design implementation
KW  - Design models
KW  - experimental analysis
KW  - Image reconstruction
KW  - Socket
ER  - 

TY  - JOUR
TI  - UAV-Empowered Edge Computing Environment for Cyber-Threat Detection in Smart Vehicles
AU  - Garg, S.
AU  - Singh, A.
AU  - Batra, S.
AU  - Kumar, N.
AU  - Yang, L.T.
T2  - IEEE Network
AB  - Over the last few years, we have witnessed an exponential increase in the computing and storage capabilities of smart devices that has led to the popularity of an emerging technology called edge computing. Compared to the traditional cloud-computing- based infrastructure, computing and storage facilities are available near end users in edge computing. Moreover, with the widespread popularity of unmanned aerial vehicles (UAVs), huge amounts of information will be shared between edge devices and UAVs in the coming years. In this scenario, traffic surveillance using UAVs and edge computing devices is expected to become an integral part of the next generation intelligent transportation systems. However, surveillance in ITS requires uninterrupted data sharing, cooperative decision making, and stabilized network formation. Edge computing supports data processing and analysis closer to the deployed machines (i.e., the sources of the data). Instead of simply storing data and missing the opportunity to capitalize on it, edge devices can analyze data to gain insights before acting on them. Transferring data from the vehicle to the edge for real-time analysis can be facilitated by the use of UAVs, which can act as intermediate aerial nodes between the vehicles and edge nodes. However, as the communication between UAVs and edge devices is generally done using an open channel, there is a high risk of information leakage in this environment. Keeping our focus on all these issues, in this article, we propose a data-driven transportation optimization model where cyber-threat detection in smart vehicles is done using a probabilistic data structure (PDS)- based approach. A triple Bloom filter PDS- based scheduling technique for load balancing is initially used to host the real-time data coming from different vehicles, and then to distribute/collect the data to/from edges in a manner that minimizes the computational effort. The results obtained show that the proposed system requires comparatively less computational time and storage for load sharing, authentication, encryption, and decryption of data in the considered edge-computing-based smart transportation framework. © 1986-2012 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/MNET.2018.1700286
VL  - 32
IS  - 3
SP  - 42
EP  - 51
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048335823&doi=10.1109%2fMNET.2018.1700286&partnerID=40&md5=ac7585fdceaa29e9ae06b26c0362cd8b
DB  - Scopus
KW  - Edge computing
KW  - Decision making
KW  - Computing environments
KW  - Cryptography
KW  - Antennas
KW  - Unmanned aerial vehicles (UAV)
KW  - Intelligent systems
KW  - Intelligent vehicle highway systems
KW  - Digital storage
KW  - Authentication
KW  - Scheduling techniques
KW  - Emerging technologies
KW  - Data structures
KW  - Aircraft detection
KW  - Computational time and storage
KW  - Cooperative decision making
KW  - Data processing and analysis
KW  - Highway traffic control
KW  - Intelligent transportation systems
KW  - Transportation optimizations
ER  - 

TY  - JOUR
TI  - Migrating Enterprise Legacy Source Code to Microservices: On Multitenancy, Statefulness, and Data Consistency
AU  - Furda, A.
AU  - Fidge, C.
AU  - Zimmermann, O.
AU  - Kelly, W.
AU  - Barros, A.
T2  - IEEE Software
AB  - Microservice migration is a promising technique to incrementally modernize monolithic legacy enterprise applications and enable them to exploit the benefits of cloud-computing environments. This article elaborates on three challenges of microservice migration: Multitenancy, statefulness, and data consistency. The authors show how to identify each of these challenges in legacy code and explain refactoring and architectural pattern-based migration techniques relevant to microservice architectures. They explain how multitenancy enables microservices to be utilized by different organizations with distinctive requirements, why statefulness affects both the availability and reliability of a microservice system, and why data consistency challenges are encountered when migrating legacy code that operates on a centralized data repository to microservices operating on decentralized data repositories. They also explain the interdependencies between multitenancy, statefulness, and data consistency. © 1984-2012 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/MS.2017.440134612
VL  - 35
IS  - 3
SP  - 63
EP  - 72
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038819187&doi=10.1109%2fMS.2017.440134612&partnerID=40&md5=1dbc4fe57ed5beb49221566eebb5d3ae
DB  - Scopus
KW  - Cloud computing
KW  - microservices
KW  - software engineering
KW  - Scalability
KW  - Software engineering
KW  - software development
KW  - Distributed computer systems
KW  - refactoring
KW  - Refactorings
KW  - Architectural pattern
KW  - Logic gates
KW  - Web services
KW  - Service oriented architecture (SOA)
KW  - Information services
KW  - Computation theory
KW  - Services Architectures
KW  - Websites
KW  - Computer software
KW  - Emerging technologies
KW  - User interfaces
KW  - Services computing
KW  - Codes (symbols)
KW  - Object oriented programming
KW  - Communication
KW  - Computer systems organization
KW  - Database systems
KW  - Multi tenancies
KW  - Object recognition
KW  - Industry
KW  - architectural patterns
KW  - data consistency
KW  - general
KW  - multitenancy
KW  - Pattern recognition systems
KW  - principles of services
KW  - Stateful web services
KW  - statefulness
ER  - 

TY  - JOUR
TI  - CrownLabs - A Collaborative Environment to Deliver Remote Computing Laboratories
AU  - Iorio, M.
AU  - Palesandro, A.
AU  - Risso, F.
T2  - IEEE Access
AB  - The coronavirus pandemic hit the entire education sector hard. All students were sent home and lectures started to be delivered through video-conferencing systems. CrownLabs is an open-source project providing an answer to the problem of delivering remote computing laboratories. Simplicity is one of its main characteristics, requiring nothing but a simple web browser to interact with the system and being all heavyweight computations performed at the university premises. Cooperation and mentoring are also encouraged through parallel access to the same remote desktop. The entire system is built up using components from the Kubernetes ecosystem, to replicate a 'cloud grade' infrastructure, coupled with custom software implementing the core business logic. To this end, most of the complexity has been delegated to the infrastructure, to speed up the development process and reduce the maintenance burden. An extensive evaluation has been performed in both real and simulated scenarios to validate the overall performance: the results are encouraging, as well as the feedback from the early adopters of the system. © 2013 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ACCESS.2020.3007961
VL  - 8
SP  - 126428
EP  - 126442
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088692482&doi=10.1109%2fACCESS.2020.3007961&partnerID=40&md5=b597a2bbacc990d990c70da758580a02
DB  - Scopus
KW  - kubernetes
KW  - software architecture
KW  - Petroleum reservoir evaluation
KW  - Open source software
KW  - Open systems
KW  - Open source projects
KW  - Video conferencing
KW  - Collaborative environments
KW  - Collaborative software
KW  - computer science education
KW  - Core business
KW  - Development process
KW  - Education sectors
KW  - Entire system
KW  - laboratories
KW  - learning systems
KW  - platform virtualization
KW  - Remote computing
KW  - Telecommuting
KW  - Video conferencing system
ER  - 

TY  - JOUR
TI  - Energy-efficient task offloading and trajectory planning in UAV-enabled mobile edge computing networks
AU  - Li, B.
AU  - Liu, W.
AU  - Xie, W.
AU  - Li, X.
T2  - Computer Networks
AB  - In order to meet the double-sided challenges brought by the shortage of computation resources and energy of users, we investigate in this paper the optimization of energy efficiency (EE) in an unmanned aerial vehicle (UAV)-assisted wireless network, where UAV is functioned as a flying energy station and edge server to provide charging and computing services for ground users. We aim to maximize the average EE of the mobile edge computing network by the joint design of user transmit power, user computing frequency, UAV transmit power, bandwidth allocation, and UAV trajectory planning under strict energy and power constraints. In order to solve such challenging problem, we first elaborately construct a Markov decision process to model task offloading and resource allocation by learning from past experiences. Then, an average EE maximization method relying on deep reinforcement learning (DRL) is designed to efficiently adjust task offloading policy, where the policy of agent can be gradually improved by interacting with the environment and collecting the experience for learning. Finally, the EE-maximization proximal policy optimization (EE-PPO) algorithm is proposed to train the DRL agent and thereby solve this optimization problem. Numerical results are given to indicate that the proposed EE-PPO method has the properties of both fast convergence and well performance. © 2023 Elsevier B.V.
DA  - 2023///
PY  - 2023
DO  - 10.1016/j.comnet.2023.109940
VL  - 234
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165908841&doi=10.1016%2fj.comnet.2023.109940&partnerID=40&md5=8c6050f9d3493143ae238febe461928a
DB  - Scopus
KW  - Energy efficiency
KW  - Mobile edge computing
KW  - Markov processes
KW  - Reinforcement learning
KW  - Reinforcement learnings
KW  - Energy efficient
KW  - Deep learning
KW  - Deep reinforcement learning
KW  - Aerial vehicle
KW  - Antennas
KW  - Unmanned aerial vehicle
KW  - Unmanned aerial vehicles (UAV)
KW  - Computation offloading
KW  - Energy harvesting
KW  - Task offloading
KW  - Average energy
KW  - Efficiency maximization
KW  - Trajectory Planning
KW  - Transmit power
ER  - 

TY  - JOUR
TI  - Adaptive reverse task offloading in edge computing for AI processes
AU  - Amanatidis, P.
AU  - Karampatzakis, D.
AU  - Michailidis, G.
AU  - Lagkas, T.
AU  - Iosifidis, G.
T2  - Computer Networks
AB  - Nowadays, we witness the proliferation of edge IoT devices, ranging from smart cameras to autonomous vehicles, with increasing computing capabilities, used to implement AI-based services in users’ proximity, right at the edge. As these services are often computationally demanding, the popular paradigm of offloading their tasks to nearby cloud servers has gained much traction and been studied extensively. In this work, we propose a new paradigm that departs from the above typical edge computing offloading idea. Namely, we argue that it is possible to leverage these end nodes to assist larger nodes (e.g., cloudlets) in executing AI tasks. Indeed, as more and more end nodes are deployed, they create an abundance of idle computing capacity, which, when aggregated and exploited in a systematic fashion, can be proved beneficial. We introduce the idea of reverse offloading and study a scenario where a powerful node splits an AI task into a group of subtasks and assigns them to a set of nearby edge IoT nodes. The goal of each node is to minimize the overall execution time, which is constrained by the slowest subtask, while adhering to predetermined energy consumption and AI performance constraints. This is a challenging MINLP (Mixed Integer Non-Linear Problem) optimization problem that we tackle with a novel approach through our newly introduced EAI-ARO (Edge AI-Adaptive Reverse Offloading) algorithm. Furthermore, a demonstration of the efficacy of our reverse offloading proposal using an edge computing testbed and a representative AI service is performed. The findings suggest that our method optimizes the system's performance significantly when compared with a greedy and a baseline task offloading algorithm. © 2024
DA  - 2024///
PY  - 2024
DO  - 10.1016/j.comnet.2024.110844
VL  - 255
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206344272&doi=10.1016%2fj.comnet.2024.110844&partnerID=40&md5=c26274779dedc87d98de3faa0177f0e1
DB  - Scopus
KW  - Edge computing
KW  - Optimisations
KW  - Cloud servers
KW  - Optimization
KW  - Resource allocation
KW  - Computation offloading
KW  - Resources allocation
KW  - Task offloading
KW  - Mixed-integer linear programming
KW  - Computing capability
KW  - Smart cameras
KW  - AI process
KW  - AI processes
KW  - Autonomous Vehicles
KW  - Integer linear programming
KW  - Subtask
ER  - 

TY  - JOUR
TI  - A Task Offloading Method Based on User Satisfaction in C-RAN With Mobile Edge Computing
AU  - Yang, Q.
AU  - Chu, S.-C.
AU  - Hu, C.-C.
AU  - Kong, L.
AU  - Pan, J.-S.
T2  - IEEE Transactions on Mobile Computing
AB  - With the continuous development of the communication service industry, users pay more attention to the quality of network service. Previous studies on offloading problems, especially in the Cloud Radio Access Network (C-RAN) architecture with Mobile Edge Computing (MEC), are primarily focused on the economic perspective, with little consideration given to user-oriented satisfaction problems. To fill this gap, this article proposes a mathematical model for maximizing user satisfaction in the C-RAN architecture with multi-layer MEC. The problem is divided into two stages for solution. The first stage addresses the optimal connection problem between users and Remote Radio Heads (RRHs). The second stage then schedules user tasks reasonably based on the solution obtained in the first stage. The two-stage problems are all proved to be NP-Hard. Two efficient approximation algorithms, namely User-to-RRH Association Algorithm (URAA) and Maximum Satisfaction Algorithm (MSA), are proposed to solve the problems in different stages. This article proves and analyzes the theoretical performance of the two algorithms. Finally, the performance of the proposed algorithms is verified by simulation experiments. The experimental results demonstrate that the two proposed algorithms can achieve reasonable solutions to the problems, and the user satisfaction level can be maintained at a high level. © 2002-2012 IEEE.
DA  - 2024///
PY  - 2024
DO  - 10.1109/TMC.2023.3275580
VL  - 23
IS  - 4
SP  - 3452
EP  - 3465
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162912214&doi=10.1109%2fTMC.2023.3275580&partnerID=40&md5=0f8e674dc10f22de02c864cd587a02f4
DB  - Scopus
KW  - Mobile edge computing
KW  - mobile edge computing
KW  - Computer architecture
KW  - Task analysis
KW  - Mobile handsets
KW  - task offloading
KW  - Network architecture
KW  - Radio access networks
KW  - Continuous development
KW  - Approximation algorithms
KW  - Delay
KW  - Users' satisfactions
KW  - Job analysis
KW  - Task offloading
KW  - Approximation algorithm
KW  - cloud radio access network
KW  - Cloud radio access network
KW  - Remote radio heads
KW  - user statisfaction
KW  - User statisfaction
ER  - 

TY  - JOUR
TI  - Redis helm chart
AU  - Bitnami, M.
DA  - 2022///
PY  - 2022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219496574&partnerID=40&md5=69d5ed82e7eeb07c76a5afd52d4b0a5b
DB  - Scopus
ER  - 

TY  - CONF
TI  - Security in microservices architectures
AU  - Mateus-Coelho, N.
AU  - Cruz-Cunha, M.
AU  - Ferreira, L.G.
T2  - Procedia Computer Science
AB  - A Microservice is a small or even micro independent process that communicates, acts, and returns via messages through lightweight mechanisms like Thrift, HTTP or RESTAPI. Microservices Architecture is amateur evolution of the Monolithic Architecture. Observing it in afunctional way, it is correct to claim that it breaks down complex applications into a simpler abstraction. As this research demonstrates, Microservices Architecture is intrinsically connected as a symbiosis with container-based deployment, because these containers have no need for embedded operating systems and calls are made for OS resources, via an application programming interface. It is safe to claim that this technology is currently the focus of modern developers nowadays. Semantically speaking, Microservices functionally deconstruct larger applications into smaller, discrete services, and containers are viewed as a natural compute platform for this architecture [1]. A single service is and can be represented by multiple containers in a Microservices cluster, each single service is designed to provide a specific set of functions while services act to makeup the entire application. It's common in large application the decomposition into multiple arms of more than twenty services, although less can be commonly found as well [1]. The main objective of Microservices Architecture is to disassemble the core components of a given type of application [2]. This study could be done in multiple ways, all of them different because practically everybody has their specific way of looking at Microservices, but one aspect is the same cross mentalities, Security. So, the focus of this research is to expose the main security aspects of this specific architecture, in this cost-effective era. © 2021 The Authors. Published by Elsevier B.V.
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.procs.2021.01.320
VL  - 181
SP  - 1225
EP  - 1236
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105682499&doi=10.1016%2fj.procs.2021.01.320&partnerID=40&md5=51eebd544c9029f7af4b795b9df60018
DB  - Scopus
KW  - Cost effectiveness
KW  - Containers
KW  - Architecture
KW  - Application programming interfaces (API)
KW  - Core components
KW  - Complex applications
KW  - Monolithic architecture
KW  - Embedded systems
KW  - Simple++
KW  - Applications programming interfaces
KW  - Break down
KW  - Elsevier
KW  - Embedded operating systems
KW  - Multiple arms
KW  - Security aspects
ER  - 

TY  - JOUR
TI  - Perspectives on the CAP theorem
AU  - Gilbert, S.
AU  - Lynch, N.
T2  - Computer
DA  - 2012///
PY  - 2012
VL  - 45
IS  - 2
SP  - 30
EP  - 36
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869219414&partnerID=40&md5=fd5eb1ba6815c1913d7d006a510df35b
DB  - Scopus
ER  - 

TY  - CONF
TI  - The Cost of Stateless Network Functions in 5G
AU  - Kulkarni, U.
AU  - Sheoran, A.
AU  - Fahmy, S.
T2  - ANCS 2021 - Proceedings of the 2021 Symposium on Architectures for Networking and Communications Systems
AB  - The adoption of a cloud-native architecture in 5G networks has facilitated rapid deployment and update of cellular services. An important part of this architecture is the implementation of 5G network functions statelessly. However, statelessness and its associated serialization and de-serialization of data and database interaction significantly increase latency. In this work, we take the first steps towards quantifying the cost of statelessness in a cloud-native 5G system. We compare the cost of different state management paradigms, and propose a number of optimizations to reduce this cost. Our preliminary results indicate that sharing user state among 5G functions reduces the overall cost by on an average of 10% in experiments with 100 to 1000 simultaneous requests. Optimizations such as non-blocking calls and custom database APIs also reduce cost, albeit to a lower extent. We believe that the paradigms proposed in this paper can aid operators and software vendors as they design cloud-native 5G networks.  © 2021 Owner/Author.
DA  - 2021///
PY  - 2021
DO  - 10.1145/3493425.3502749
SP  - 73
EP  - 79
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124120008&doi=10.1145%2f3493425.3502749&partnerID=40&md5=883b8fad229c1558ac13eee9a88d0348
DB  - Scopus
KW  - 5G
KW  - Optimisations
KW  - Computer architecture
KW  - Queueing networks
KW  - Cost reduction
KW  - 5G mobile communication systems
KW  - Network functions
KW  - Network function virtualization
KW  - Transfer functions
KW  - Cellular network
KW  - Rapid deployments
KW  - Cellular networks
KW  - Cellular services
KW  - Cloud-native architecture
KW  - Cloud-native architectures
KW  - Data interactions
KW  - Database interactions
KW  - State management
KW  - Stateless network function
KW  - Stateless Network Functions
ER  - 

TY  - CONF
TI  - A Monitoring, Observability and Analytics Framework to Improve the Sustainability of B5G Technologies
AU  - Akbari, M.
AU  - Bolla, R.
AU  - Bruschi, R.
AU  - Davoli, F.
AU  - Lombardo, C.
AU  - Siccardi, B.
T2  - 2024 IEEE International Conference on Communications Workshops, ICC Workshops 2024
AB  - As mobile generations advance, blurring the lines between physical and digital worlds, the deployment of ultra-dense networks poses challenges in resource allocation and energy efficiency. This article revolves around the environmental sustainability of Fifth Generation (5G) and Beyond 5G (B5G) mobile networks as cloud-native technologies, highlighting the use phase. It aims to represent a monitoring, observability and analytical framework to devise a feature selection methodology within well-known monitoring tools, aligning with environmental sustainability objectives. Notable open-source monitoring applications (e.g., Kepler and Scaphandre) are employed to collect data on resource utilization and energy consumption for containers and physical servers. Dependence criteria indexes, Hilbert-Schmidt Independence Criterion (HSIC) and Pearson, identify interdependencies among timeseries from different tools. Addressing challenges, the paper resolves inconsistent dataset lengths, highlights the Pearson-HSIC trade-off, and underscores the need for alignment in feature selection with compatibility to resource utilization. Key findings include Scaphandre's underestimation of memory impact on power consumption, Kepler's limitations in host metrics. Finally, the results enable the development of an anomaly detection mechanism. © 2024 IEEE.
DA  - 2024///
PY  - 2024
DO  - 10.1109/ICCWorkshops59551.2024.10615948
SP  - 969
EP  - 975
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202432426&doi=10.1109%2fICCWorkshops59551.2024.10615948&partnerID=40&md5=34eec1c64c72d41eb7624a1924f8abd6
DB  - Scopus
KW  - anomaly detection
KW  - energy efficiency
KW  - Energy efficiency
KW  - Anomaly detection
KW  - B5G
KW  - monitoring
KW  - Resource allocation
KW  - Energy
KW  - Environmental sustainability
KW  - Resources utilizations
KW  - 5G mobile communication systems
KW  - Beyond 5g
KW  - Digital world
KW  - feature reduction
KW  - Features reductions
KW  - Features selection
KW  - Hilbert-schmidt independence criterions
KW  - observability
KW  - Physical world
ER  - 

TY  - JOUR
TI  - A Survey on the Convergence of Edge Computing and AI for UAVs: Opportunities and Challenges
AU  - McEnroe, P.
AU  - Wang, S.
AU  - Liyanage, M.
T2  - IEEE Internet of Things Journal
AB  - The latest 5G mobile networks have enabled many exciting Internet of Things (IoT) applications that employ unmanned aerial vehicles (UAVs/drones). The success of most UAV-based IoT applications is heavily dependent on artificial intelligence (AI) technologies, for instance, computer vision and path planning. These AI methods must process data and provide decisions while ensuring low latency and low energy consumption. However, the existing cloud-based AI paradigm finds it difficult to meet these strict UAV requirements. Edge AI, which runs AI on-device or on edge servers close to users, can be suitable for improving UAV-based IoT services. This article provides a comprehensive analysis of the impact of edge AI on key UAV technical aspects (i.e., autonomous navigation, formation control, power management, security and privacy, computer vision, and communication) and applications (i.e., delivery systems, civil infrastructure inspection, precision agriculture, search and rescue (SAR) operations, acting as aerial wireless base stations (BSs), and drone light shows). As guidance for researchers and practitioners, this article also explores UAV-based edge AI implementation challenges, lessons learned, and future research directions.  © 2014 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/JIOT.2022.3176400
VL  - 9
IS  - 17
SP  - 15435
EP  - 15459
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130441199&doi=10.1109%2fJIOT.2022.3176400&partnerID=40&md5=2dc9b4aa36de54eb5cb6dd4019b21d86
DB  - Scopus
KW  - Mobile edge computing
KW  - edge computing
KW  - Edge intelligence
KW  - Internet of Things (IoT)
KW  - Edge computing
KW  - Internet of things
KW  - MEC
KW  - Cloud-computing
KW  - Green computing
KW  - Energy utilization
KW  - Task analysis
KW  - Internet of thing
KW  - Antennas
KW  - Data privacy
KW  - 5G mobile communication systems
KW  - edge intelligence
KW  - Computer vision
KW  - Job analysis
KW  - Artificial intelligence technologies
KW  - Air navigation
KW  - Artificial intelligence (AI)
KW  - Artificial intelligence methods
KW  - Computer control systems
KW  - Drones
KW  - edge AI
KW  - Edge artificial intelligence
KW  - MEC.
KW  - Motion planning
KW  - unmanned aerial vehicle (UAV)
ER  - 

TY  - JOUR
AU  - O-RAN Alliance, U.
T2  - O-RAN WhitePaper - Building the Next Generation RAN
DA  - 2018///
PY  - 2018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124786808&partnerID=40&md5=473ae733b223d48caebfbd9cf4746259
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Databases in Edge and Fog Environments: A Survey
AU  - Ferreira, L.M.M.
AU  - Coelho, F.
AU  - Pereira, J.
T2  - ACM Computing Surveys
AB  - While a significant number of databases are deployed in cloud environments, pushing part or all data storage and querying planes closer to their sources (i.e., to the edge) can provide advantages in latency, connectivity, privacy, energy, and scalability. This article dissects the advantages provided by databases in edge and fog environments by surveying application domains and discussing the key drivers for pushing database systems to the edge. At the same time, it also identifies the main challenges faced by developers in this new environment and analyzes the mechanisms employed to deal with them. By providing an overview of the current state of edge and fog databases, this survey provides valuable insights into future research directions. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
DA  - 2024///
PY  - 2024
DO  - 10.1145/3666001
VL  - 56
IS  - 11
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199680091&doi=10.1145%2f3666001&partnerID=40&md5=3cb4befc9a3c21a53a367a046f6d6a0d
DB  - Scopus
KW  - Fog computing
KW  - fog computing
KW  - Edge computing
KW  - Fog
KW  - Energy
KW  - Cloud environments
KW  - Digital storage
KW  - 'current
KW  - Future research directions
KW  - Database systems
KW  - Query processing
KW  - Applications domains
KW  - Data querying
KW  - Data storage
ER  - 

TY  - CONF
TI  - Kepler: A Framework to Calculate the Energy Consumption of Containerized Applications
AU  - Amaral, M.
AU  - Chen, H.
AU  - Chiba, T.
AU  - Nakazawa, R.
AU  - Choochotkaew, S.
AU  - Lee, E.K.
AU  - Eilam, T.
T2  - IEEE International Conference on Cloud Computing, CLOUD
AB  - Energy accounting is crucial in data centers for optimizing power provisioning, capping, and tuning. This paper introduces the Kepler framework, which estimates power consumption at the process, container, and Kubernetes pod levels. Kepler offers a set of power models applicable to various architectures and metrics. In this study, we propose a generic power model that utilizes hardware counters (HC) and realtime system power metrics (e.g., running average power limit (RAPL)) as independent variables in a regression model. Unlike previous approaches that rely on aggregate power consumption, our methodology measures individual process power consumption to train the power model. We provide step-by-step instructions to measure process power consumption in a controlled environment, considering the activation constant and load-dependent dynamic power consumption in different executions. By following the Greenhouse Gas (GHG) Protocol, our approach ensures the fair distribution of constant power among the user's processes. The results demonstrate significantly improved accuracy with a mean squared error (MSE) as low as 0.010 for the proposed method, compared with an MSE of 0.16 for a simple ratio approach and 0.92 when training the model using aggregated workload power. © 2023 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/CLOUD60044.2023.00017
VL  - 2023-July
SP  - 69
EP  - 71
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174254307&doi=10.1109%2fCLOUD60044.2023.00017&partnerID=40&md5=ad9611593dafbcf59292d9d1201e3478
DB  - Scopus
KW  - kubernetes
KW  - Kubernetes
KW  - Containers
KW  - Sustainable development
KW  - Greenhouse gases
KW  - Energy-consumption
KW  - Green computing
KW  - Sustainability
KW  - Power
KW  - Electric power utilization
KW  - Regression analysis
KW  - Average power limit
KW  - Power modeling
KW  - RAPL
KW  - eBPF
KW  - Energy accounting
KW  - container power modeling
KW  - Container power modeling
KW  - EBPF
KW  - Electric power distribution
KW  - energy accounting
KW  - Mean square error
KW  - Mean squared error
KW  - Running average power limit
ER  - 

TY  - JOUR
TI  - Intelligence and Learning in O-RAN for Data-Driven NextG Cellular Networks
AU  - Bonati, L.
AU  - D'Oro, S.
AU  - Polese, M.
AU  - Basagni, S.
AU  - Melodia, T.
T2  - IEEE Communications Magazine
AB  - Next generation (NextG) cellular networks will be natively cloud-based and built on programmable, virtualized, and disaggregated architectures. The separation of control functions from the hardware fabric and the introduction of standardized control interfaces will enable the definition of custom closed-control loops, which will ultimately enable embedded intelligence and real-time analytics, thus effectively realizing the vision of autonomous and self-optimizing networks. This article explores the disaggregated network architecture proposed by the O-RAN Alliance as a key enabler of NextG networks. Within this architectural context, we discuss the potential, the challenges, and the limitations of data-driven optimization approaches to network control over different timescales. We also present the first large-scale integration of O-RAN-compliant software components with an open source full-stack softwarized cellular network. Experiments conducted on Colosseum, the world's largest wireless network emulator, demonstrate closed-loop integration of real-time analytics and control through deep reinforcement learning agents. We also show the feasibility of radio access network (RAN) control through xApps running on the near-real-time RAN intelligent controller to optimize the scheduling policies of coexisting network slices, leveraging the O-RAN open interfaces to collect data at the edge of the network.  © 1979-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/MCOM.101.2001120
VL  - 59
IS  - 10
SP  - 21
EP  - 27
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119879211&doi=10.1109%2fMCOM.101.2001120&partnerID=40&md5=0f6ae2132eb84a71b604dec46ee77b2d
DB  - Scopus
KW  - Reinforcement learning
KW  - Deep learning
KW  - Network architecture
KW  - Radio access networks
KW  - Cloud-based
KW  - Open source software
KW  - Open systems
KW  - Mobile telecommunication systems
KW  - Wireless networks
KW  - Real-time analytics
KW  - Closed control loop
KW  - Closed loop control systems
KW  - Control functions
KW  - Control interfaces
KW  - Data driven
KW  - Embedded intelligence
KW  - Network-control
KW  - Next generation cellular networks
ER  - 

TY  - JOUR
AU  - Etcd team, A.
T2  - etcd: A distributed, reliable key-value store for the most critical data of a distributed system
DA  - 2013///
PY  - 2013
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219502077&partnerID=40&md5=c407b9ff6cc6b305736f8461fa6e5978
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Task offloading strategies for mobile edge computing: A survey
AU  - Dong, S.
AU  - Tang, J.
AU  - Abbas, K.
AU  - Hou, R.
AU  - Kamruzzaman, J.
AU  - Rutkowski, L.
AU  - Buyya, R.
T2  - Computer Networks
AB  - With the wide adoption of 5G technology and the rapid development of 6G technology, a variety of new applications have emerged. A multitude of compute-intensive and time-sensitive applications deployed on terminal equipment have placed increased demands on Internet delay and bandwidth. Mobile Edge Computing (MEC) can effectively mitigate the issues of long transmission times, high energy consumption, and data insecurity. Task offloading, as a key technology within MEC, has become a prominent research focus in this field. This paper presents a comprehensive review of the current research progress in MEC task offloading. Firstly, it introduces the fundamental concepts, application scenarios, and related technologies of MEC. Secondly, it categorizes offloading decisions into five aspects: reducing delay, minimizing energy consumption, balancing energy consumption and delay, enabling high-computing offloading, and addressing different application scenarios. It then critically analyzes and compares existing research efforts in these areas. © 2024 Elsevier B.V.
DA  - 2024///
PY  - 2024
DO  - 10.1016/j.comnet.2024.110791
VL  - 254
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204212918&doi=10.1016%2fj.comnet.2024.110791&partnerID=40&md5=5417d9fd58a03a25f50fc0da0856d2db
DB  - Scopus
KW  - Edge computing
KW  - MEC
KW  - Application scenario
KW  - Computation offloading
KW  - Task offloading
KW  - New applications
KW  - Internet delay
KW  - Offloading decision
KW  - Task offloading algorithm
KW  - Terminal equipment
KW  - Time sensitive applications
ER  - 

TY  - JOUR
TI  - Mobile edge computing for V2X architectures and applications: A survey
AU  - Bréhon–Grataloup, L.
AU  - Kacimi, R.
AU  - Beylot, A.-L.
T2  - Computer Networks
AB  - In mobile environments, with the help of larger bandwidths and cloud computing solutions, any task can be offloaded from a mobile user equipment to be handled remotely. However, even though this process is accelerated with every cellular generation, with 5G being no exception, offloading to a faraway centralized cloud implies non-negligible delay. To tackle this issue concerning delay-sensitive applications, mobile edge computing, now denominated as multi-access edge computing (MEC), was brought to light. With cloud resources brought closer to the edge of the network, MEC greatly reduces task offloading delay, thereby striving to satisfy the constraints of real-time applications. As highly demanding mobile applications, vehicular networks are a target to be addressed in terms of performance, especially communication and computation delay. In this article, we establish the specificities of MEC when applied to the Internet of Vehicles (IoV), and survey recent papers studying implementations of MEC relevant to real-time vehicular considerations. We categorize these latest V2X architectures so as to unveil the mechanisms behind their improved performance: network availability and coverage, reliability and loss of network connectivity, large data handling and task offloading. This survey not only provides an initial understanding of the state-of-the-art advancements in the field of MEC-enabled vehicular networks, but also raises open issues and challenges that need to be addressed before enjoying the full benefits of this paradigm. © 2022
DA  - 2022///
PY  - 2022
DO  - 10.1016/j.comnet.2022.108797
VL  - 206
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124200551&doi=10.1016%2fj.comnet.2022.108797&partnerID=40&md5=e563c8b630a53cbc5ba4d308d3c444c1
DB  - Scopus
KW  - Mobile edge computing
KW  - Edge computing
KW  - Multi-access edge computing
KW  - Computer architecture
KW  - Internet of Vehicles
KW  - Quality of service
KW  - Delay-sensitive applications
KW  - Quality-of-service
KW  - Network architecture
KW  - Surveys
KW  - 5G mobile communication systems
KW  - Data handling
KW  - Multiaccess
KW  - Cellular network
KW  - Task offloading
KW  - Cellular networks
KW  - Cache management
KW  - IEEE-802.11p
KW  - Internet of vehicle
KW  - Vehicle to Everything
KW  - Vehicular edge computing
ER  - 

TY  - CONF
TI  - Advantages of Redis in-memory database to efficiently search for healthcare medical supplies using geospatial data
AU  - Muradova, G.
AU  - Hematyar, M.
AU  - Jamalova, J.
T2  - 16th IEEE International Conference on Application of Information and Communication Technologies, AICT 2022 - Proceedings
AB  - According to diagnostic criteria, a patient can find clinics he needs depending on the symptoms of disease. The paper shows an effective solution that allows rapid access to information about clinic using Redis in memory database technology. In this paper using Redis help us to collect a wide array of geospatial capabilities finding the best way and build out this type of functionality. Redis has the capability to store by Redis intelligent optimized systems in their native format, and update and serve them with minimal computing infrastructure needed to implement these algorithms at scale. Also, our approach is to study the effect of the databases on system's working speed, comparing Redis and MS SQL. © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/AICT55583.2022.10013544
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147541038&doi=10.1109%2fAICT55583.2022.10013544&partnerID=40&md5=5a5b566a61d1aa2b9df544fc5db07cef
DB  - Scopus
KW  - Metadata
KW  - Big data
KW  - Healthcare
KW  - Health care
KW  - Digital transformation
KW  - Diagnosis
KW  - Geo-spatial
KW  - Geo-spatial data
KW  - Geolocation provider
KW  - geolocation providers
KW  - Geolocations
KW  - geospatial capabilities
KW  - Geospatial capability
KW  - geospatial data
KW  - Health care providers
KW  - healthcare
KW  - healthcare providers
KW  - MS SQL
KW  - public health digital transformation
KW  - Public health digital transformation
KW  - Redis
ER  - 

TY  - JOUR
TI  - Smart Traffic Monitoring System Using Computer Vision and Edge Computing
AU  - Liu, G.
AU  - Shi, H.
AU  - Kiani, A.
AU  - Khreishah, A.
AU  - Lee, J.
AU  - Ansari, N.
AU  - Liu, C.
AU  - Yousef, M.M.
T2  - IEEE Transactions on Intelligent Transportation Systems
AB  - Traffic management systems capture tremendous video data and leverage advances in video processing to detect and monitor traffic incidents. The collected data are traditionally forwarded to the traffic management center (TMC) for in-depth analysis and may thus exacerbate the network paths to the TMC. To alleviate such bottlenecks, we propose to utilize edge computing by equipping edge nodes that are close to cameras with computing resources (e.g., cloudlets). A cloudlet, with limited computing resources as compared to TMC, provides limited video processing capabilities. In this paper, we focus on two common traffic monitoring tasks, congestion detection, and speed detection, and propose a two-tier edge computing based model that takes into account of both the limited computing capability in cloudlets and the unstable network condition to the TMC. Our solution utilizes two algorithms for each task, one implemented at the edge and the other one at the TMC, which are designed with the consideration of different computing resources. While the TMC provides strong computation power, the video quality it receives depends on the underlying network conditions. On the other hand, the edge processes very high-quality video but with limited computing resources. Our model captures this trade-off. We evaluate the performance of the proposed two-tier model as well as the traffic monitoring algorithms via test-bed experiments under different weather as well as network conditions and show that our proposed hybrid edge-cloud solution outperforms both the cloud-only and edge-only solutions.  © 2000-2011 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/TITS.2021.3109481
VL  - 23
IS  - 8
SP  - 12027
EP  - 12038
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115724008&doi=10.1109%2fTITS.2021.3109481&partnerID=40&md5=d6b9b67665d8d89beb37aa75b55099da
DB  - Scopus
KW  - Edge computing
KW  - Cloud-computing
KW  - Economic and social effects
KW  - Monitoring
KW  - Information management
KW  - Image edge detection
KW  - Traffic congestion
KW  - Computational modelling
KW  - Edge detection
KW  - Traffic monitoring
KW  - Cameras
KW  - Video analytics
KW  - Video signal processing
KW  - edge-computing
KW  - Incident detection
KW  - incidents detection
KW  - Traffic management centers
KW  - video analytic
KW  - Video analytic.
ER  - 

TY  - JOUR
TI  - Artificial Intelligence Defined 5G Radio Access Networks
AU  - Yao, M.
AU  - Sohul, M.
AU  - Marojevic, V.
AU  - Reed, J.H.
T2  - IEEE Communications Magazine
AB  - Massive multiple-input multiple-output antenna systems, millimeter-wave communications, and ultra-dense networks have been widely perceived as the three key enablers that facilitate the development and deployment of 5G systems. This article discusses the intelligent agent that combines sensing, learning, and optimizing to facilitate these enablers. We present a flexible, rapidly deployable, and cross-layer artificial intelligence (AI)-based framework to enable the imminent and future demands on 5G and beyond. We present example AI-enabled 5G use cases that accommodate important 5G-specific capabilities and discuss the value of AI for enabling network evolution. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/MCOM.2019.1800629
VL  - 57
IS  - 3
SP  - 14
EP  - 20
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063005440&doi=10.1109%2fMCOM.2019.1800629&partnerID=40&md5=3fac456bc85a6cdb23d7fd82c553f6a5
DB  - Scopus
KW  - Artificial intelligence
KW  - Antennas
KW  - Radio access networks
KW  - 5G mobile communication systems
KW  - Cross layer
KW  - Dense network
KW  - G-system
KW  - Millimeter waves
KW  - Millimeter-wave communication
KW  - MIMO systems
KW  - Multiple-input-multiple-output antenna systems
KW  - Network evolution
ER  - 

TY  - JOUR
TI  - Cost and Latency Optimized Edge Computing Platform
AU  - Pelle, I.
AU  - Szalay, M.
AU  - Czentye, J.
AU  - Sonkoly, B.
AU  - Toka, L.
T2  - Electronics (Switzerland)
AB  - Latency-critical applications, e.g., automated and assisted driving services, can now be deployed in fog or edge computing environments, offloading energy-consuming tasks from end devices. Besides the proximity, though, the edge computing platform must provide the necessary operation techniques in order to avoid added delays by all means. In this paper, we propose an integrated edge platform that comprises orchestration methods with such objectives, in terms of handling the deployment of both functions and data. We show how the integration of the function orchestration solution with the adaptive data placement of a distributed key–value store can lead to decreased end-to-end latency even when the mobility of end devices creates a dynamic set of requirements. Along with the necessary monitoring features, the proposed edge platform is capable of serving the nomad users of novel applications with low latency requirements. We showcase this capability in several scenarios, in which we articulate the end-to-end latency performance of our platform by comparing delay measurements with the benchmark of a Redis-based setup lacking the adaptive nature of data orchestration. Our results prove that the stringent delay requisites necessitate the close integration that we present in this paper: functions and data must be orchestrated in sync in order to fully exploit the potential that the proximity of edge resources enables. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.
DA  - 2022///
PY  - 2022
DO  - 10.3390/electronics11040561
VL  - 11
IS  - 4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124334818&doi=10.3390%2felectronics11040561&partnerID=40&md5=a8accb2ef73c69e075450ee7183f3107
DB  - Scopus
KW  - FaaS
KW  - Edge computing
KW  - Serverless
KW  - Function-as-a-Service
KW  - Lambda
KW  - Redis
KW  - Cloud native
KW  - Data locality
KW  - Distributed data store
KW  - Greengrass
ER  - 

TY  - JOUR
TI  - The role of microservice approach in edge computing: Opportunities, challenges, and research directions
AU  - Hossain, M.D.
AU  - Sultana, T.
AU  - Akhter, S.
AU  - Hossain, M.I.
AU  - Thu, N.T.
AU  - Huynh, L.N.T.
AU  - Lee, G.-W.
AU  - Huh, E.-N.
T2  - ICT Express
AB  - Edge computing has emerged as a promising computing paradigm that enables real-time data processing and analysis closer to the data source and boosts decision-making applications in a safe manner. On the other hand, the microservice is a new type of architecture that can be dynamically deployed, migrating across edge clouds on demand. Therefore, the combination of these two technologies can provide numerous benefits, including improved performance, reduced latency, and better resource utilization. In this paper, we present a thorough analysis of state-of-the-art research on the use of microservices in edge computing environments. We take into consideration several distinct microservice research directions, including coordination, orchestration, repositories, scheduling, autoscaling, deployment, resource management, and different security issues. Furthermore, we explore the potential applications of microservices in edge computing across various domains. Finally, the unsolved research issues and future directions of emerging trends in this area are also discussed. © 2023 The Author(s)
DA  - 2023///
PY  - 2023
DO  - 10.1016/j.icte.2023.06.006
VL  - 9
IS  - 6
SP  - 1162
EP  - 1182
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165079284&doi=10.1016%2fj.icte.2023.06.006&partnerID=40&md5=417afeddf065684b7a62d0e0cb8ddd2e
DB  - Scopus
KW  - Microservices
KW  - AI
KW  - Edge computing
KW  - Microservice security
KW  - Monolithic architectures
ER  - 

TY  - CONF
TI  - A distributed database system for event-based microservices
AU  - Laigner, R.
AU  - Zhou, Y.
AU  - Salles, M.A.V.
T2  - DEBS 2021 - Proceedings of the 15th ACM International Conference on Distributed and Event-Based Systems
AB  - Microservice architectures are an emerging industrial approach to build large scale and event-based systems. In this architectural style, an application is functionally partitioned into several small and autonomous building blocks, so-called microservices, communicating and exchanging data with each other via events. By pursuing a model where fault isolation is enforced at microservice level, each microservice manages their own database, thus database systems are not shared across microservices. Developers end up encoding substantial data management logic in the application-tier and encountering a series of challenges on enforcing data integrity and maintaining data consistency across microservices. In this vision paper, we argue that there is a need to rethink how database systems can better support microservices and relieve the burden of handling complex data management tasks faced by programmers. We envision the design and research opportunities for a novel distributed database management system targeted at event-driven microservices.  © 2021 ACM.
DA  - 2021///
PY  - 2021
DO  - 10.1145/3465480.3466919
SP  - 25
EP  - 30
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110335342&doi=10.1145%2f3465480.3466919&partnerID=40&md5=b0f58c3069cd82adc159026594a6dca7
DB  - Scopus
KW  - microservices
KW  - event-driven architecture
KW  - Software architecture
KW  - Information management
KW  - Data consistency
KW  - Architectural style
KW  - Distributed database systems
KW  - Data handling
KW  - Building blockes
KW  - Research opportunities
KW  - database system
KW  - Distributed database
KW  - Event-based system
KW  - Fault isolation
KW  - Management tasks
ER  - 

TY  - JOUR
TI  - AI-Enabled Secure Microservices in Edge Computing: Opportunities and Challenges
AU  - Al-Doghman, F.
AU  - Moustafa, N.
AU  - Khalil, I.
AU  - Sohrabi, N.
AU  - Tari, Z.
AU  - Zomaya, A.Y.
T2  - IEEE Transactions on Services Computing
AB  - The paradigm of edge computing has formed an innovative scope within the domain of the Internet of Things (IoT) through expanding the services of the cloud to the network edge to design distributed architectures and securely enhance decision-making applications. Due to the heterogeneous, distributed and resource-constrained essence of edge Computing, edge applications are required to be developed as a set of lightweight and interdependent modules. As this concept aligns with the objectives of microservice architecture, effective implementation of microservices-based edge applications within IoT networks has the prospective of fully leveraging edge nodes capabilities. Deploying microservices at IoT edge faces plenty of challenges associated with security and privacy. Advances in Artificial Intelligence (AI) (especially Machine Learning), and the easy access to resources with powerful computing providing opportunities for deriving precise models and developing different intelligent applications at the edge of network. In this study, an extensive survey is presented for securing edge computing-based AI Microservices to elucidate the challenges of IoT management and enable secure decision-making systems at the edge. We present recent research studies on edge AI and microservices orchestration and highlight key requirements as well as challenges of securing Microservices at IoT edge. We also propose a Microservices-based edge computing framework that provides secure edge AI algorithms as Microservices utilizing the containerization technology to offer automated and secure AI-based applications at the network edge.  © 2008-2012 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/TSC.2022.3155447
VL  - 16
IS  - 2
SP  - 1485
EP  - 1504
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125753164&doi=10.1109%2fTSC.2022.3155447&partnerID=40&md5=6596edb9d44bb1d36a36dcf2e8419632
DB  - Scopus
KW  - microservices
KW  - Security
KW  - Edge computing
KW  - Microservice
KW  - Internet of things
KW  - Microservice architecture
KW  - Decision making
KW  - Computer architecture
KW  - Network edges
KW  - Data privacy
KW  - Computational modelling
KW  - edge AI
KW  - Edge AI
KW  - edge privacy
KW  - Edge privacy
KW  - edge security
KW  - Edge security
ER  - 

TY  - CONF
TI  - In search of an understandable consensus algorithm
AU  - Ongaro, D.
AU  - Ousterhout, J.
T2  - Proceedings of the 2014 USENIX Annual Technical Conference, USENIX ATC 2014
AB  - Raft is a consensus algorithm for managing a replicated log. It produces a result equivalent to (multi-)Paxos, and it is as efficient as Paxos, but its structure is different from Paxos; this makes Raft more understandable than Paxos and also provides a better foundation for building practical systems. In order to enhance understandability, Raft separates the key elements of consensus, such as leader election, log replication, and safety, and it enforces a stronger degree of coherency to reduce the number of states that must be considered. Results from a user study demonstrate that Raft is easier for students to learn than Paxos. Raft also includes a new mechanism for changing the cluster membership, which uses overlapping majorities to guarantee safety. © Proceedings of the 2014 USENIX Annual Technical Conference, USENIX ATC 2014. All rights reserved.
DA  - 2014///
PY  - 2014
SP  - 305
EP  - 319
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077435090&partnerID=40&md5=662997594757a26b9feaf99dc66d27cc
DB  - Scopus
KW  - Consensus algorithms
KW  - Cluster memberships
KW  - Key elements
KW  - Leader election
KW  - Learn+
KW  - New mechanisms
KW  - Number of state
KW  - Practical systems
KW  - Understandability
KW  - User study
ER  - 

TY  - JOUR
TI  - Robust Edge Computing in UAV Systems via Scalable Computing and Cooperative Computing
AU  - Liu, Z.
AU  - Zhan, C.
AU  - Cui, Y.
AU  - Wu, C.
AU  - Hu, H.
T2  - IEEE Wireless Communications
AB  - Unmanned aerial vehicle (UAV) systems are of increasing interest to academia and industry due to their mobility, flexibility, and maneuverability, and are an effective alternative to various uses such as surveillance and mobile edge computing. However, due to their limited computational and communications resources, it is difficult to serve all computation tasks simultaneously. This article tackles this problem by first proposing a scalable aerial computing solution, which is applicable for computation tasks of multiple quality levels, corresponding to different computation workloads and computation results of distinct performance. It opens up the possibility to maximally improve the overall computing performance with limited computational and communications resources. To meet the demands for timely video analysis that exceed the computing power of a UAV, we propose an aerial video streaming enabled cooperative computing solution, namely, UAVideo, which streams videos from a UAV to ground servers. As a complement to scalable aerial computing, UAVideo minimizes the video streaming time under the constraints on UAV trajectory, video features, and communications resources. Simulation results reveal the substantial advantages of the proposed solutions. Furthermore, we highlight relevant directions for future research. © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/MWC.121.2100041
VL  - 28
IS  - 5
SP  - 36
EP  - 42
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119973828&doi=10.1109%2fMWC.121.2100041&partnerID=40&md5=dfeca8191bfae4e720b61a790149bc80
DB  - Scopus
KW  - Mobile edge computing
KW  - Edge computing
KW  - Antennas
KW  - Unmanned aerial vehicles (UAV)
KW  - Computational resources
KW  - Computing solutions
KW  - Video streaming
KW  - Video-streaming
KW  - Communication resources
KW  - Computation tasks
KW  - Cooperative computing
KW  - Maneuverability
KW  - Manoeuvrability
KW  - Scalable computing
KW  - Unmanned aerial vehicle systems
ER  - 

TY  - JOUR
TI  - Idle power matters: Kepler metrics for public cloud energy efficiency
AU  - Cloud Native Computing Foundation (CNCF): Environmental Sustainability, M.
T2  - Idle Power Matters: Kepler Metrics for Public Cloud Energy Efficiency
DA  - 2024///
PY  - 2024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219494057&partnerID=40&md5=becc77ee82462f276e1a5fcb068948f3
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Etcd helm chart
AU  - Bitnami, M.
DA  - 2022///
PY  - 2022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219497964&partnerID=40&md5=6383d0e217e6ea5d343224aa70d0e2e7
DB  - Scopus
ER  - 

TY  - CONF
TI  - An NVMe-oF Distributed Storage Design Based on Etcd
AU  - Wang, Y.
AU  - Gong, W.
AU  - Fang, H.
T2  - 2022 4th International Conference on Frontiers Technology of Information and Computer, ICFTIC 2022
AB  - The emerging NVMe-oF (NVMe over Fabrics) protocol provides access to an SSD on another node over the network using NVMe commands, providing performance close to that of an SSD using the NVMe protocol locally. To further improve performance, the SPDK (Storage Performance Development Kit) proposes an NVMe-oF implementation in user mode. Compared to the native NVMe-oF I/O stack provided by the Linux kernel, the SPDK's user mode I/O stack can significantly reduce I/O latency, however, the NVMe-oF implementation in the SPDK only serves as a demonstration and does not implement cluster management and failover features. In this paper, we propose a design of multi-node NVMe-oF distributed storage based on Etcd technology, which enables bulk deployment and management of clusters with the help of Etcd's node discovery and management functions. It also utilizes Etcd's K-V database to persist configuration information for failover and recovery.  © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ICFTIC57696.2022.10075110
SP  - 701
EP  - 705
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152198599&doi=10.1109%2fICFTIC57696.2022.10075110&partnerID=40&md5=5aa37d53683192bb89aa1d490fb8839e
DB  - Scopus
KW  - Performance
KW  - Distributed storage
KW  - Computer operating systems
KW  - Distributed storage system
KW  - Distributed Storage System
KW  - Etcd
KW  - Failover
KW  - NVMe
KW  - NVMe over fabric
KW  - NVMe over Fabrics
KW  - SPDK
KW  - Storage performance
KW  - Storage performance development kit
KW  - User mode
ER  - 

TY  - JOUR
TI  - Kubernetes-based efficient power level exporter
AU  - Kepler, M.
T2  - Kubernetes-based Efficient Power Level Exporter
DA  - 2025///
PY  - 2025
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219494370&partnerID=40&md5=422414ac7344b6bb3607bb2ea9e194ba
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Impact of power consumption in containerized clouds: A comprehensive analysis of open-source power measurement tools
AU  - Centofanti, C.
AU  - Santos, J.
AU  - Gudepu, V.
AU  - Kondepu, K.
T2  - Computer Networks
AB  - Recently, container-based solutions have become de facto compute units of modern cloud-native applications. However, the exponential growth in data traffic and the power consumption of these technologies to handle high data traffic alarm the strong need for energy evaluation approaches in containerized clouds. Furthermore, the proliferation of highly distributed edge clouds raises additional concerns regarding the power consumption of future cloud architectures. This article presents a detailed overview of methods and techniques for monitoring power consumption within popular cloud platforms. The study offers an in-depth evaluation of these approaches, demonstrating variations in measured power consumption based on the chosen technique. A well-known container orchestration platform named Kubernetes (K8s) has been applied in our extensive measurements. This work argues that energy-efficient container clouds will play a vital role in building a more sustainable and eco-friendly digital infrastructure by optimizing power consumption and reducing carbon footprint, paving the way for a greener future. The paper also discusses open challenges and future research directions on energy sustainability, leading to the conclusion, offering lessons learned and prospects on potential solutions to foster sustainable practices within the container ecosystem. © 2024 The Authors
DA  - 2024///
PY  - 2024
DO  - 10.1016/j.comnet.2024.110371
VL  - 245
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189754287&doi=10.1016%2fj.comnet.2024.110371&partnerID=40&md5=2931318c8c09c272b303cf029696b0e1
DB  - Scopus
KW  - Cloud computing
KW  - Energy efficiency
KW  - Kubernetes
KW  - Containers
KW  - Sustainable development
KW  - Carbon footprint
KW  - Cloud-computing
KW  - Green computing
KW  - Sustainability
KW  - Service orchestration
KW  - Power
KW  - Computing power
KW  - Electric power utilization
KW  - Power consumption
KW  - Exponential growth
KW  - Open-source
KW  - Comprehensive analysis
KW  - Data traffic
KW  - Measurement tools
KW  - Traffic alarms
ER  - 

TY  - JOUR
TI  - PandORA: Automated Design and Comprehensive Evaluation of Deep Reinforcement Learning Agents for Open RAN
AU  - Tsampazi, M.
AU  - D'oro, S.
AU  - Polese, M.
AU  - Bonati, L.
AU  - Poitau, G.
AU  - Healy, M.
AU  - Alavirad, M.
AU  - Melodia, T.
T2  - IEEE Transactions on Mobile Computing
AB  - The highly heterogeneous ecosystem of Next Generation (NextG) wireless communication systems calls for novel networking paradigms where functionalities and operations can be dynamically and optimally reconfigured in real time to adapt to changing traffic conditions and satisfy stringent and diverse Quality of Service (QoS) demands. Open Radio Access Network (RAN) technologies, and specifically those being standardized by the O-RAN Alliance, make it possible to integrate network intelligence into the once monolithic RAN via intelligent applications, namely, xApps and rApps. These applications enable flexible control of the network resources and functionalities, network management, and orchestration through data-driven intelligent control loops. Recent work has showed how Deep Reinforcement Learning (DRL) is effective in dynamically controlling O-RAN systems. However, how to design these solutions in a way that manages heterogeneous optimization goals and prevents unfair resource allocation is still an open challenge, with the logic within DRL agents often considered as a opaque system. In this paper, we introduce PandORA, a framework to automatically design and train DRL agents for Open RAN applications, package them as xApps and evaluate them in the Colosseum wireless network emulator. We benchmark 23 xApps that embed DRL agents trained using different architectures, reward design, action spaces, and decision-making timescales, and with the ability to hierarchically control different network parameters. We test these agents on the Colosseum testbed under diverse traffic and channel conditions, in static and mobile setups. Our experimental results indicate how suitable fine-tuning of the RAN control timers, as well as proper selection of reward designs and DRL architectures can boost network performance according to the network conditions and demand. Notably, finer decision-making granularities can improve Massive Machine-Type Communications (mMTC)'s performance by ∼56% and even increase Enhanced Mobile Broadband (eMBB) Throughput by ∼99%.  © 2024 IEEE.
DA  - 2025///
PY  - 2025
DO  - 10.1109/TMC.2024.3505781
VL  - 24
IS  - 4
SP  - 3223
EP  - 3240
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000760440&doi=10.1109%2fTMC.2024.3505781&partnerID=40&md5=79c59f629c2ec55412eb556469f1b6bd
DB  - Scopus
KW  - resource allocation
KW  - open RAN
KW  - Decision making
KW  - Resource allocation
KW  - Reinforcement learning
KW  - Reinforcement learnings
KW  - Deep reinforcement learning
KW  - Information management
KW  - Benchmarking
KW  - Radio access networks
KW  - Resources allocation
KW  - Decisions makings
KW  - Network performance
KW  - network intelligence
KW  - Network intelligence
KW  - O-radio access network
KW  - O-RAN
KW  - Open radio access network
KW  - Reinforcement learning agent
KW  - Traffic conditions
ER  - 

TY  - JOUR
TI  - Programmable and Customized Intelligence for Traffic Steering in 5G Networks Using Open RAN Architectures
AU  - Lacava, A.
AU  - Polese, M.
AU  - Sivaraj, R.
AU  - Soundrarajan, R.
AU  - Bhati, B.S.
AU  - Singh, T.
AU  - Zugno, T.
AU  - Cuomo, F.
AU  - Melodia, T.
T2  - IEEE Transactions on Mobile Computing
AB  - 5G and beyond mobile networks will support heterogeneous use cases at an unprecedented scale, thus demanding automated control and optimization of network functionalities customized to the needs of individual users. Such fine-grained control of the Radio Access Network (RAN) is not possible with the current cellular architecture. To fill this gap, the Open RAN paradigm and its specification introduce an 'open' architecture with abstractions that enable closed-loop control and provide data-driven, and intelligent optimization of the RAN at the user-level. This is obtained through custom RAN control applications (i.e., xApps) deployed on near-real-time RAN Intelligent Controller (near-RT RIC) at the edge of the network. Despite these premises, as of today the research community lacks a sandbox to build data-driven xApps, and create large-scale datasets for effective Artificial Intelligence (AI) training. In this paper, we address this by introducing ns-O-RAN, a software framework that integrates a real-world, production-grade near-RT RIC with a 3GPP-based simulated environment on ns-3, enabling at the same time the development of xApps, automated large-scale data collection and testing of Deep Reinforcement Learning (DRL)-driven control policies for the optimization at the user-level. In addition, we propose the first user-specific O-RAN Traffic Steering (TS) intelligent handover framework. It uses Random Ensemble Mixture (REM), a Conservative QQ-learning (CQL) algorithm, combined with a state-of-the-art Convolutional Neural Network (CNN) architecture, to optimally assign a serving base station to each user in the network. Our TS xApp, trained with more than 40 million data points collected by ns-O-RAN, runs on the near-RT RIC and controls the ns-O-RAN base stations. We evaluate the performance on a large-scale deployment with up to 126 users with 8 base stations, showing that the xApp-based handover improves throughput and spectral efficiency by an average of 50% over traditional handover heuristics, with less mobility overhead.  © 2002-2012 IEEE.
DA  - 2024///
PY  - 2024
DO  - 10.1109/TMC.2023.3266642
VL  - 23
IS  - 4
SP  - 2882
EP  - 2897
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153511847&doi=10.1109%2fTMC.2023.3266642&partnerID=40&md5=5d594173b6733da1ad1c45dd13b15975
DB  - Scopus
KW  - Optimisations
KW  - Software testing
KW  - Computer architecture
KW  - Optimization
KW  - Reinforcement learning
KW  - Reinforcement learnings
KW  - Deep reinforcement learning
KW  - Network architecture
KW  - Deep neural networks
KW  - Radio access networks
KW  - Convolutional neural network
KW  - 5G mobile communication systems
KW  - Computer programming
KW  - Convolution
KW  - deep reinforcement learning
KW  - traffic steering
KW  - Ns-3
KW  - Base stations
KW  - network intelligence
KW  - Network intelligence
KW  - O-radio access network
KW  - O-RAN
KW  - Hand over
KW  - ns-3
KW  - Traffic steering
ER  - 

TY  - JOUR
TI  - Impact of etcd deployment on Kubernetes, Istio, and application performance
AU  - Larsson, L.
AU  - Tärneberg, W.
AU  - Klein, C.
AU  - Elmroth, E.
AU  - Kihl, M.
T2  - Software - Practice and Experience
AB  - This experience article describes lessons learned as we conducted experiments in a Kubernetes-based environment, the most notable of which was that the performance of both the Kubernetes control plane and the deployed application depends strongly and in unexpected ways on the performance of the etcd database. The article contains (a) detailed descriptions of how networking with and without Istio works in Kubernetes, based on the Flannel Container Networking Interface (CNI) provider in VXLAN mode with IP Virtual Server (IPVS)-backed Kubernetes Services, (b) a comprehensive discussion about how to conduct load and performance testing using a closed-loop workload generator, and (c) an open source experiment framework useful for executing experiments in a shared cloud environment and exploring the resulting data. It also shows that statistical analysis may reveal the data resulting from such experiments to be misleading even when careful preparations are made, and that nondeterministic behavior stemming from etcd can affect both the platform as a whole and the deployed application. Finally, it is demonstrated that using high-performance backing storage for etcd can reduce the occurrence of such nondeterministic behaviors by a statistically significant (P <.05) margin. The implication of this experience article is that systems researchers studying the performance of applications deployed on Kubernetes cannot simply consider their specific application to be under test. Instead, the particularities of the underlying Kubernetes and cloud platform must be taken into account, in particular because their performance can impact that of etcd. © 2020 The Authors. Software: Practice and Experience published by John Wiley & Sons Ltd.
DA  - 2020///
PY  - 2020
DO  - 10.1002/spe.2885
VL  - 50
IS  - 10
SP  - 1986
EP  - 2007
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089109897&doi=10.1002%2fspe.2885&partnerID=40&md5=b3aa69908375961bf4ec1a6872c4c810
DB  - Scopus
KW  - cloud computing
KW  - performance
KW  - Kubernetes
KW  - distributed systems
KW  - Cloud platforms
KW  - Cloud environments
KW  - Digital storage
KW  - Application performance
KW  - Nondeterministic behavior
KW  - C (programming language)
KW  - Deployed applications
KW  - Workload generators
KW  - Performance testing
KW  - etcd
KW  - Virtual servers
ER  - 

TY  - JOUR
TI  - Mobility Aware and Dynamic Migration of MEC Services for the Internet of Vehicles
AU  - Labriji, I.
AU  - Meneghello, F.
AU  - Cecchinato, D.
AU  - Sesia, S.
AU  - Perraud, E.
AU  - Strinati, E.C.
AU  - Rossi, M.
T2  - IEEE Transactions on Network and Service Management
AB  - Vehicles are becoming connected entities, and with the advent of online gaming, on demand streaming and assisted driving services, are expected to turn into data hubs with abundant computing needs. In this article, we show the value of estimating vehicular mobility as 5G users move across radio cells, and of using such estimates in combination with an online algorithm that assesses when and where the computing services (virtual machines, VM) that are run on the mobile edge nodes are to be migrated to ensure service continuity at the vehicles. This problem is tackled via a Lyapunov-based approach, which is here solved in closed form, leading to a low-complexity and distributed algorithm, whose performance is numerically assessed in a real-life scenario, featuring thousands of vehicles and densely deployed 5G base stations. Our numerical results demonstrate a reduction of more than 50% in the energy expenditure with respect to previous strategies (full migration). Also, our scheme self-adapts to meet any given risk target, which is posed as an optimization constraint and represents the probability that the computing service is interrupted during a handover. Through it, we can effectively control the trade-off between seamless computation and energy consumption when migrating VMs.  © 2004-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/TNSM.2021.3052808
VL  - 18
IS  - 1
SP  - 570
EP  - 584
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099731183&doi=10.1109%2fTNSM.2021.3052808&partnerID=40&md5=4683d23177e7154bebb9d8916d9356e5
DB  - Scopus
KW  - 5G
KW  - Economic and social effects
KW  - Energy utilization
KW  - Web services
KW  - convolutional neural network
KW  - Computational complexity
KW  - On-line algorithms
KW  - Computing services
KW  - Numerical results
KW  - 5G mobile communication systems
KW  - Vehicle to vehicle communications
KW  - Vehicles
KW  - Energy expenditure
KW  - Lyapunov optimization
KW  - Dynamic migration
KW  - Electronic assessment
KW  - Internet of Vehicles (IoV)
KW  - Markov chain
KW  - mobility estimation
KW  - multi-access edge computing (MEC)
KW  - On-demand streaming
KW  - recurrent neural network
KW  - Service continuity
KW  - service migration
KW  - Vehicular mobilities
KW  - virtual machine (VM)
ER  - 

TY  - JOUR
TI  - Understanding O-RAN: Architecture, Interfaces, Algorithms, Security, and Research Challenges
AU  - Polese, M.
AU  - Bonati, L.
AU  - D'Oro, S.
AU  - Basagni, S.
AU  - Melodia, T.
T2  - IEEE Communications Surveys and Tutorials
AB  - The Open Radio Access Network (RAN) and its embodiment through the O-RAN Alliance specifications are poised to revolutionize the telecom ecosystem. O-RAN promotes virtualized RANs where disaggregated components are connected via open interfaces and optimized by intelligent controllers. The result is a new paradigm for the RAN design, deployment, and operations: O-RAN networks can be built with multi-vendor, interoperable components, and can be programmatically optimized through a centralized abstraction layer and data-driven closed-loop control. Therefore, understanding O-RAN, its architecture, its interfaces, and workflows is key for researchers and practitioners in the wireless community. In this article, we present the first detailed tutorial on O-RAN. We also discuss the main research challenges and review early research results. We provide a deep dive of the O-RAN specifications, describing its architecture, design principles, and the O-RAN interfaces. We then describe how the O-RAN RAN Intelligent Controllers (RICs) can be used to effectively control and manage 3GPP-defined RANs. Based on this, we discuss innovations and challenges of O-RAN networks, including the Artificial Intelligence (AI) and Machine Learning (ML) workflows that the architecture and interfaces enable, security, and standardization issues. Finally, we review experimental research platforms that can be used to design and test O-RAN networks, along with recent research results, and we outline future directions for O-RAN development.  © 1998-2012 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/COMST.2023.3239220
VL  - 25
IS  - 2
SP  - 1376
EP  - 1411
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147300222&doi=10.1109%2fCOMST.2023.3239220&partnerID=40&md5=38e8ed5d2706f9c03ead9a8ba022db81
DB  - Scopus
KW  - Security
KW  - Artificial intelligence
KW  - 5G
KW  - Learning systems
KW  - Optimisations
KW  - Network security
KW  - Computer architecture
KW  - Controllers
KW  - Network architecture
KW  - 6G
KW  - Radio access networks
KW  - Research challenges
KW  - Specifications
KW  - Work-flows
KW  - ITS architecture
KW  - Abstracting
KW  - O-RAN
KW  - 3GPP
KW  - cellular
KW  - Intelligent control
KW  - Intelligent controllers
KW  - Open RAN
KW  - Precoding
KW  - Radiofrequencies
ER  - 

TY  - CONF
TI  - Intelligent O-RAN for Beyond 5G and 6G Wireless Networks
AU  - Niknam, S.
AU  - Roy, A.
AU  - Dhillon, H.S.
AU  - Singh, S.
AU  - Banerji, R.
AU  - Reed, J.H.
AU  - Saxena, N.
AU  - Yoon, S.
T2  - 2022 IEEE GLOBECOM Workshops, GC Wkshps 2022 - Proceedings
AB  - Building on the principles of openness and intelligence, there has been a concerted global effort from the operators towards enhancing the radio access network (RAN) architecture. The objective is to build an operator-defined RAN architecture (and associated interfaces) on open hardware that provides intelligent radio control for beyond fifth generation (5G) as well as future sixth generation (6G) wireless networks. Specifically, the open-radio access network (O-RAN) alliance has been formed by merging xRAN forum and C-RAN alliance to formally define the requirements that would help achieve this objective. Owing to the importance of O-RAN in the current wireless landscape, this article provides an introduction to the concepts, principles, and requirements of the Open RAN as specified by the O-RAN alliance. In order to illustrate the role of intelligence in O-RAN, we propose an intelligent radio resource management scheme to handle traffic congestion and demonstrate its efficacy on a real-world dataset obtained from a large operator. A high-level architecture of this deployment scenario that is compliant with the O-RAN requirements is also discussed. The article concludes with key technical challenges and open problems for future research and development.  © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/GCWkshps56602.2022.10008676
SP  - 215
EP  - 220
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146878758&doi=10.1109%2fGCWkshps56602.2022.10008676&partnerID=40&md5=e770877fa824071a74f4feb6bccd0507
DB  - Scopus
KW  - machine learning
KW  - Machine learning
KW  - Resource allocation
KW  - Natural resources management
KW  - Network architecture
KW  - 6G
KW  - Radio access networks
KW  - Traffic congestion
KW  - 'current
KW  - 5G mobile communication systems
KW  - 6g
KW  - Machine-learning
KW  - Large dataset
KW  - Wireless networks
KW  - Radio
KW  - beyond 5G
KW  - Beyond 5g
KW  - Open radio access network
KW  - Intelligent controllers
KW  - Open RAN
KW  - intelligent controller
KW  - Open hardware
KW  - Radio control
KW  - radio resource management
KW  - Radio resources managements
ER  - 

TY  - CONF
TI  - Data management in microservices: State of the practice, challenges, and research directions
AU  - Laigner, R.
AU  - Zhou, Y.
AU  - Salles, M.A.V.
AU  - Liu, Y.
AU  - Kalinowski, M.
T2  - Proceedings of the VLDB Endowment
AB  - Microservices have become a popular architectural style for datadriven applications, given their ability to functionally decompose an application into small and autonomous services to achieve scalability, strong isolation, and specialization of database systems to the workloads and data formats of each service. Despite the accelerating industrial adoption of this architectural style, an investigation of the state of the practice and challenges practitioners face regarding data management in microservices is lacking. To bridge this gap, we conducted a systematic literature review of representative articles reporting the adoption of microservices, we analyzed a set of popular open-source microservice applications, and we conducted an online survey to cross-validate the findings of the previous steps with the perceptions and experiences of over 120 experienced practitioners and researchers. Through this process, we were able to categorize the state of practice of data management in microservices and observe several foundational challenges that cannot be solved by software engineering practices alone, but rather require system-level support to alleviate the burden imposed on practitioners. We discuss the shortcomings of state-of-the-art database systems regarding microservices and we conclude by devising a set of features for microservice-oriented database systems. © 2021, VLDB Endowment. All rights reserved.
DA  - 2021///
PY  - 2021
DO  - 10.14778/3484224.3484232
VL  - 14
SP  - 3348
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117280834&doi=10.14778%2f3484224.3484232&partnerID=40&md5=1a6c2a7b1d33c549a29a7a412282c765
DB  - Scopus
KW  - Information management
KW  - Architecture
KW  - Open source software
KW  - Open systems
KW  - Systematic literature review
KW  - Architectural style
KW  - Open-source
KW  - Industrial adoption
KW  - State of the practice
KW  - Database systems
KW  - Data-driven applications
KW  - Online surveys
KW  - Software engineering practices
KW  - Specialisation
KW  - State of practise
ER  - 

TY  - JOUR
TI  - Pace
AU  - Calagna, A.
AU  - Ravera, S.
AU  - Chiasserini, C.F.
DA  - 2024///
PY  - 2024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219496315&partnerID=40&md5=1ca856cba705114264654f47f97d7fc5
DB  - Scopus
ER  - 

TY  - CONF
TI  - Towards scalable and reliable in-memory storage system: A case study with Redis
AU  - Chen, S.
AU  - Tang, X.
AU  - Wang, H.
AU  - Zhao, H.
AU  - Guo, M.
T2  - Proceedings - 15th IEEE International Conference on Trust, Security and Privacy in Computing and Communications, 10th IEEE International Conference on Big Data Science and Engineering and 14th IEEE International Symposium on Parallel and Distributed Processing with Applications, IEEE TrustCom/BigDataSE/ISPA 2016
AB  - In recent years, in-memory key-value storage systems have become more and more popular in solving real-time and interactive tasks. Compared with disks, memories have much higher throughput and lower latency which enables them to process data requests with much higher performance. However, since memories have much smaller capacity than disks, how to expand the capacity of in-memory storage system while maintain its high performance become a crucial problem. At the same time, since data in memories are non-persistent, the data may be lost when the system is down. In this paper, we make a case study with Redis, which is one popular in-memory key-value storage system. We find that although the latest release of Redis support clustering so that data can be stored in distributed nodes to support a larger storage capacity, its performance is limited by its decentralized design that clients usually need two connections to get their request served. To make the system more scalable, we propose a Clientside Key-to-Node Caching method that can help direct request to the right service node. Experimental results show that by applying this technique, it can significantly improve the system's performance by near 2 times. We also find that although Redis supports data replication on slave nodes to ensure data safety, it still gets a chance of losing a part of the data due to a weak consistency between master and slave nodes that its defective order of data replication and request reply may lead to losing data without notifying the client. To make it more reliable, we propose a Master-slave Semi Synchronization method which utilizes TCP protocol to ensure the order of data replication and request reply so that when a client receives an 'OK' message, the corresponding data must have been replicated. With a significant improvement in data reliability, its performance overhead is limited within 5%. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/TrustCom.2016.0255
SP  - 1660
EP  - 1667
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015242440&doi=10.1109%2fTrustCom.2016.0255&partnerID=40&md5=2f7f887b84514110b83c2dbe5bb6131e
DB  - Scopus
KW  - Reliability
KW  - Big data
KW  - Scalability
KW  - Data privacy
KW  - Digital storage
KW  - Data storage equipment
KW  - Real time systems
KW  - Decentralized design
KW  - Distributed nodes
KW  - In-memory
KW  - Key values
KW  - Key-to-node caching
KW  - Key-value
KW  - Node caching
KW  - Semi synchronization
KW  - Storage capacity
KW  - Storage system
KW  - Storage systems
KW  - Synchronization method
KW  - System's performance
ER  - 

TY  - CONF
TI  - Renewable and cooling aware workload management for sustainable data centers
AU  - Liu, Z.
AU  - Chen, Y.
AU  - Bash, C.
AU  - Wierman, A.
AU  - Gmach, D.
AU  - Wang, Z.
AU  - Marwah, M.
AU  - Hyser, C.
T2  - Performance Evaluation Review
AB  - Recently, the demand for data center computing has surged, increasing the total energy footprint of data centers worldwide. Data centers typically comprise three subsystems: IT equipment provides services to customers; power infrastructure supports the IT and cooling equipment; and the cooling infrastructure removes heat generated by these subsystems. This work presents a novel approach to model the energy flows in a data center and optimize its operation. Traditionally, supply-side constraints such as energy or cooling availability were treated independently from IT workload management. This work reduces electricity cost and environmental impact using a holistic approach that integrates renewable supply, dynamic pricing, and cooling supply including chiller and outside air cooling, with IT workload planning to improve the overall sustainability of data center operations. Specifically, we first predict renewable energy as well as IT demand. Then we use these predictions to generate an IT workload management plan that schedules IT workload and allocates IT resources within a data center according to time varying power supply and cooling efficiency. We have implemented and evaluated our approach using traces from real data centers and production systems. The results demonstrate that our approach can reduce both the recurring power costs and the use of non-renewable energy by as much as 60% compared to existing techniques, while still meeting the Service Level Agreements. © 2012 ACM.
DA  - 2012///
PY  - 2012
DO  - 10.1145/2254756.2254779
VL  - 40
SP  - 175
EP  - 186
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864700984&doi=10.1145%2f2254756.2254779&partnerID=40&md5=dfdf71c33e8d6ec1bb243fe5c0840045
DB  - Scopus
KW  - scheduling
KW  - Scheduling
KW  - Information technology
KW  - Data centers
KW  - Optimization
KW  - Information management
KW  - Cooling
KW  - Total energy
KW  - Renewable energies
KW  - Power infrastructures
KW  - Cost reduction
KW  - Computer systems
KW  - Cooling equipment
KW  - Service Level Agreements
KW  - Non-renewable energy
KW  - Time varying
KW  - Air cooling
KW  - Cooling efficiency
KW  - cooling optimization
KW  - Cooling optimization
KW  - Data center operations
KW  - demand shaping
KW  - Dynamic pricing
KW  - Electricity costs
KW  - Energy flow
KW  - Holistic approach
KW  - IT equipment
KW  - IT resources
KW  - Power costs
KW  - Power supply
KW  - Production system
KW  - renewable energy
KW  - sustainable data center
KW  - Workload management
KW  - Workload planning
ER  - 

TY  - JOUR
AU  - Schmiedmayer, P.
T2  - Designing evolvable web services
DA  - 2022///
PY  - 2022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165971953&partnerID=40&md5=9eaa92bf220714c56e1b17b448fc0ec5
DB  - Scopus
ER  - 

TY  - CONF
TI  - DRACeo: A smart simulator to deploy energy saving methods in microservices based networks
AU  - Valera, H.H.A.
AU  - Dalmau, M.
AU  - Roose, P.
AU  - Larracoechea, J.
AU  - Herzog, C.
T2  - Proceedings of the Workshop on Enabling Technologies: Infrastructure for Collaborative Enterprises, WETICE
AB  - Nowadays, many researchers work to identify microservices-based application deployments and scheduling solutions to save energy without decreasing functional QoS. In this work, we present DRACeo: A simulator that allows facing this challenge in a simple and efficient way, enabling its users to focus uniquely on microservices deployment/scheduling algorithms and its hardware/software repercussions (load vs. energy consumption) without worrying about low-level network configurations or operating system issues. DRACeo is able to deploy and schedule (move, duplicate, start/stop) microservices and their dependencies on various devices with software and hardware heterogeneity (CPU, bandwidth, RAM, Battery, etc.), taking into account various scheduling heuristics algorithms: centralized vs non-centralized. To do this, DRACeo allows deploying custom network topologies based on client-server schemes or p2p distributions, where devices can (dis)appear, turn on/off obeying random circumstances or user strategies. Finally, the simulator performs relevant operations such as QoS definition, resource monitoring, calculation of energy saved and consumption tracking (at device and network level). We tested some ideas based on our previous work 'Kaligreen' to demonstrate the effectiveness of DRACeo. © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/WETICE49692.2020.00026
VL  - 2020-September
SP  - 94
EP  - 99
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100714002&doi=10.1109%2fWETICE49692.2020.00026&partnerID=40&md5=6dde8162ca929d1cfeb833956e56d257
DB  - Scopus
KW  - microservices
KW  - middleware
KW  - energy
KW  - Scheduling
KW  - Energy utilization
KW  - Energy conservation
KW  - consumption
KW  - CPU
KW  - hard disk
KW  - network
KW  - prototype
KW  - simulator
KW  - Network topology
KW  - Application deployment
KW  - Computer hardware
KW  - Energy saving methods
KW  - Hardware/software
KW  - Network configuration
KW  - Resource monitoring
KW  - Scheduling heuristics
KW  - Simulators
KW  - Software and hardwares
ER  - 

TY  - CONF
TI  - Architectural Tactics to Optimize Software for Energy Efficiency in the Public Cloud
AU  - Vos, S.
AU  - Lago, P.
AU  - Verdecchia, R.
AU  - Heitlager, I.
T2  - Proceedings - 2022 International Conference on ICT for Sustainability, ICT4S 2022
AB  - A promise of cloud computing is the reduction of energy footprint enabled by economies of scale. Unfortunately, little research is available on how cloud consumers can reduce their energy footprint when running software in the public cloud. Moreover, cloud consumers do not have full access to information regarding their cloud infrastructure usage, which is required to understand the impact of design decisions on energy usage. The purpose of our study is to support cloud consumers in developing energy-efficient workloads in the public cloud. To achieve our goal, we collaborated with a large cloud solution provider to discover an initial set of reusable architectural tactics for software energy efficiency. Starting from interviews with 17 practitioners, we reviewed and selected available tactics to improve the energy efficiency of individual workloads in the public cloud, and synthetized the identified tactics in a reusable model. In addition, we conducted a case study to assess the impact of utilizing a tactic, which was selected following a prioritization provided by the practitioners. Our results demonstrate the possibility to architect cloud workloads for energy efficiency through reasoning and estimation of resource optimization. However, the process is not (yet) straightforward due to the current lack of transparency of cloud providers.  © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ICT4S55073.2022.00019
SP  - 77
EP  - 87
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136183770&doi=10.1109%2fICT4S55073.2022.00019&partnerID=40&md5=5176c4ed51d498c58321e5243a1e2818
DB  - Scopus
KW  - Energy efficiency
KW  - Software Architecture
KW  - Edge computing
KW  - Edge Computing
KW  - Energy Efficiency
KW  - Cloud-computing
KW  - Green computing
KW  - Energy utilization
KW  - Energy
KW  - Design decisions
KW  - Cloud infrastructures
KW  - Public clouds
KW  - % reductions
KW  - Economics
KW  - Computer software reusability
KW  - Economy of scale
KW  - Tactic
KW  - Tactics
KW  - Cloud consumers
KW  - Public Cloud
ER  - 

TY  - JOUR
TI  - Data center maturity model
AU  - Singh, H.
T2  - Techn. Ber. the Green Grid
DA  - 2011///
PY  - 2011
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179520942&partnerID=40&md5=c99ad8196ae6196dcf003166faa3e979
DB  - Scopus
ER  - 

TY  - CONF
TI  - Towards Greener Applications: Enabling Sustainable-aware Cloud Native Applications Design
AU  - Vitali, M.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - Data centers energy demand is increasing. While a great deal of effort has been made to reduce the amount of CO2 generated by large cloud providers, too little has been done from the application perspective. We claim that application developers can impact the environmental footprint by enhancing the application design with additional features. Following the proposed Sustainable Application Design Process (SADP), the application design is enriched with information that can be leveraged by cloud providers to manage application execution in an energy-aware manner. This exploratory work aims to emphasize the awareness on the sustainability of applications by proposing a methodology for its evaluation. To this end, we first suggest possible actions to enrich the application design towards sustainability, and finally describe how this additional information can be leveraged in the application workflow. We discuss the feasibility of our methodology by referring to existing tools and technologies capable of supporting the design features proposed in a production environment. © 2022, Springer Nature Switzerland AG.
DA  - 2022///
PY  - 2022
DO  - 10.1007/978-3-031-07472-1_6
VL  - 13295 LNCS
SP  - 93
EP  - 108
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132711955&doi=10.1007%2f978-3-031-07472-1_6&partnerID=40&md5=2232e16fe7c76fece1a4f0b3bd31320c
DB  - Scopus
KW  - Energy efficiency
KW  - Power management
KW  - Cloud-native
KW  - Energy-efficiency
KW  - Sustainability-awareness
KW  - Sustainable applications
KW  - Workflow design
KW  - Sustainable development
KW  - Cloud providers
KW  - Datacenter
KW  - Green computing
KW  - Application developers
KW  - Application design
KW  - Design
KW  - Energy demands
KW  - Environmental footprints
KW  - Sustainable application
KW  - Workflow designs
ER  - 

TY  - JOUR
TI  - Reducing the energy consumption of large-scale computing systems through combined shutdown policies with multiple constraints
AU  - Benoit, A.
AU  - Lefèvre, L.
AU  - Orgerie, A.-C.
AU  - Raïs, I.
T2  - International Journal of High Performance Computing Applications
AB  - Large-scale distributed systems (high-performance computing centers, networks, data centers) are expected to consume huge amounts of energy. In order to address this issue, shutdown policies constitute an appealing approach able to dynamically adapt the resource set to the actual workload. However, multiple constraints have to be taken into account for such policies to be applied on real infrastructures: the time and energy cost of switching on and off, the power and energy consumption bounds caused by the electricity grid or the cooling system, and the availability of renewable energy. In this article, we propose models translating these various constraints into different shutdown policies that can be combined for a multiconstraint purpose. Our models and their combinations are validated through simulations on a real workload trace. © 2017, © The Author(s) 2017.
DA  - 2018///
PY  - 2018
DO  - 10.1177/1094342017714530
VL  - 32
IS  - 1
SP  - 176
EP  - 188
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039869052&doi=10.1177%2f1094342017714530&partnerID=40&md5=16390b43f86e710b523417082387d1fb
DB  - Scopus
KW  - simulation
KW  - Green computing
KW  - Energy utilization
KW  - Distributed computer systems
KW  - Energy policy
KW  - High performance computing
KW  - Energy model
KW  - Multiple constraint
KW  - Large-scale distributed system
KW  - energy models
KW  - Large-scale computing systems
KW  - Large-scale distributed systems
KW  - Power and energy consumption
KW  - shutdown policies
KW  - Shutdown policies
ER  - 

TY  - JOUR
TI  - Green IT: A matter of business and information systems engineering?
AU  - Loos, P.
AU  - Nebel, W.
AU  - Gómez, J.M.
AU  - Hasan, H.
AU  - Watson, R.T.
AU  - Brocke, J.V.
AU  - Seidel, S.
AU  - Recker, J.
T2  - Business and Information Systems Engineering
DA  - 2011///
PY  - 2011
DO  - 10.1007/s12599-011-0165-5
VL  - 3
IS  - 4
SP  - 245
EP  - 252
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856547763&doi=10.1007%2fs12599-011-0165-5&partnerID=40&md5=a05fcbd7691c30e7c1bfc3e67ed799de
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Google Carbon Footprint Console
DA  - 2023///
PY  - 2023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179518463&partnerID=40&md5=c0a3d900fe077c6ae827ed647de70aea
DB  - Scopus
ER  - 

TY  - JOUR
AU  - Fowler, M.
T2  - Domain-Specific Languages
DA  - 2010///
PY  - 2010
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-51749119047&partnerID=40&md5=8e2e2e0563da2f07ae37e513101b03aa
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Big Data, Big Waste? A Reflection on the Environmental Sustainability of Big Data Initiatives
AU  - Lucivero, F.
T2  - Science and Engineering Ethics
AB  - This paper addresses a problem that has so far been neglected by scholars investigating the ethics of Big Data and policy makers: that is the ethical implications of Big Data initiatives’ environmental impact. Building on literature in environmental studies, cultural studies and Science and Technology Studies, the article draws attention to the physical presence of data, the material configuration of digital service, and the space occupied by data. It then explains how this material and situated character of data raises questions concerning the ethics of the increasingly fashionable Big Data discourses. It argues that attention should be paid to (1) the vocabulary currently used when discussing the governance of data initiatives; (2) the internal tension between current data initiatives and environmental policies; (3) issues of fair distribution. The article explains how taking into account these aspects would allow for a more responsible behaviour in the context of data storage and production. © 2019, The Author(s).
DA  - 2020///
PY  - 2020
DO  - 10.1007/s11948-019-00171-7
VL  - 26
IS  - 2
SP  - 1009
EP  - 1030
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077193813&doi=10.1007%2fs11948-019-00171-7&partnerID=40&md5=3c4562e342b30f8c1641e0966ddb6eb2
DB  - Scopus
KW  - Big Data
KW  - Sustainability
KW  - Technology
KW  - Environmental impacts
KW  - Materiality
KW  - Responsibility
KW  - technology
ER  - 

TY  - CONF
TI  - Apodini: An Internal Domain Specific Language to Design Web Services
AU  - Schmiedmayer, P.
T2  - Middleware 2020 Doctoral Symposium - Proceedings of the 2020 21st International Middleware Conference Doctoral Symposium, Part of Middleware 2020
AB  - Web services use protocols and middleware mechanisms to offer interfaces that enable the interoperability of heterogeneous components. Interface definition languages are used to describe service interfaces derived from system requirements and from system architecture. System designers faces two challenges: To replace an existing middleware and simultaneously supporting multiple types of middleware. This paper describes Apodini, an internal domain specific language to deal with these challenges. Apodini enables system designers to express the nonfunctional requirements and functionality of web service interfaces in a single domain specific language. Apodini is independent of any specific middleware, protocol, or interface definition language to enable extensible system design and implementation.  © 2020 ACM.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3429351.3431751
SP  - 47
EP  - 49
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099723133&doi=10.1145%2f3429351.3431751&partnerID=40&md5=1f3ba530a34e3e339f49f66c31cfa18d
DB  - Scopus
KW  - middleware
KW  - Interoperability
KW  - Domain specific languages
KW  - Middleware
KW  - Web services
KW  - Design and implementations
KW  - Heterogeneous component
KW  - Websites
KW  - Systems analysis
KW  - Non-functional requirements
KW  - System architectures
KW  - domain specific language
KW  - interface definition language
KW  - Interface definition languages
KW  - Middleware mechanisms
KW  - Problem oriented languages
KW  - Swift
KW  - web service
KW  - Web service interface
ER  - 

TY  - CONF
TI  - Power-aware cloud brownout: Response time and power consumption control
AU  - Papadopoulos, A.V.
AU  - Krzywda, J.
AU  - Elmroth, E.
AU  - Maggio, M.
T2  - 2017 IEEE 56th Annual Conference on Decision and Control, CDC 2017
AB  - Cloud computing infrastructures are powering most of the web hosting services that we use at all times. A recent failure in the Amazon cloud infrastructure made many of the website that we use on a hourly basis unavailable1. This illustrates the importance of cloud applications being able to absorb peaks in workload, and at the same time to tune their power requirements to the power and energy capacity offered by the data center infrastructure. In this paper we combine an established technique for response time control-brownout-with power capping. We use cascaded control to take into account both the need for predictability in the response times (the inner loop), and the power cap (the outer loop). We execute tests on real machines to determine power usage and response times models and extend an existing simulator. We then evaluate the cascaded controller approach with a variety of workloads and both open-and closed-loop client models. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/CDC.2017.8264049
VL  - 2018-January
SP  - 2686
EP  - 2691
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046161925&doi=10.1109%2fCDC.2017.8264049&partnerID=40&md5=e264f54eb60fff423fa1b049bfe670de
DB  - Scopus
KW  - Power management
KW  - Datacenter
KW  - Green computing
KW  - Cloud applications
KW  - Computing power
KW  - Electric power utilization
KW  - Cloud infrastructures
KW  - Power requirement
KW  - Cloud computing infrastructures
KW  - Power-aware
KW  - Power capacity
KW  - Energy-capacity
KW  - Power consumption controls
KW  - Web hosting services
ER  - 

TY  - JOUR
T2  - Cloud Native Computing Foundation Annual Survey 2021
DA  - 2022///
PY  - 2022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179510856&partnerID=40&md5=d2ecf1170484f16db0c23042bda8476e
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Aws Well-Architected Framework
DA  - 2023///
PY  - 2023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179510899&partnerID=40&md5=4d8aa54da955fe409930a0f7344daee4
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Minimizing Energy Cost for Green Data Center by Exploring Heterogeneous Energy Resource
AU  - Hu, X.
AU  - Li, P.
AU  - Sun, Y.
T2  - Journal of Modern Power Systems and Clean Energy
AB  - With the deteriorating effects resulting from global warming in many areas, geographically distributed data centers contribute greatly to carbon emissions, because the major energy supply is fossil fuels. Considering this issue, many geographically distributed data centers are attempting to use clean energy as their energy supply, such as fuel cells and renewable energy sources. However, not all workloads can be powered by a single power sources, since different workloads exhibit different characteristics. In this paper, we propose a fine-grained heterogeneous power distribution model with an objective of minimizing the total energy costs and the sum of the energy gap generated by the geographically distributed data centers powered by multiple types of energy resources. In order to achieve these two goals, we design a two-stage online algorithm to leverage the power supply of each energy source. In each time slot, we also consider a chance-constraint problem and use the Bernstein approximation to solve the problem. Finally, simulation results based on real-world traces illustrate that the proposed algorithm can achieve satisfactory performance.  © 2021 State Grid Electric Power Research Institute.
DA  - 2021///
PY  - 2021
DO  - 10.35833/MPCE.2019.000052
VL  - 9
IS  - 1
SP  - 148
EP  - 159
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100023467&doi=10.35833%2fMPCE.2019.000052&partnerID=40&md5=381fb359c26bf2289314afb380e5b16d
DB  - Scopus
KW  - Carbon emissions
KW  - Green computing
KW  - Data center
KW  - Renewable energy resources
KW  - On-line algorithms
KW  - Minimizing energy
KW  - Green data centers
KW  - Fossil fuels
KW  - energy management
KW  - Renewable energy source
KW  - Global warming
KW  - Bernstein approximation
KW  - Chance constraint
KW  - Distributed data
KW  - Fossil fuel power plants
KW  - Fuel cells
KW  - heterogeneous energy resources
KW  - power distribution algorithm
KW  - Power distribution modeling
ER  - 

TY  - CONF
TI  - Expressing the Adaptation Intent as a Sustainability Goal
AU  - Gerostathopoulos, I.
AU  - Raibulet, C.
AU  - Lago, P.
T2  - Proceedings - International Conference on Software Engineering
AB  - Adaptation and sustainability are two key challenges leading the development of software-systems nowadays. Adaptation denotes the capacity of a system to cope with variations and uncertainties at runtime in order to continue providing its functionalities with certain quality levels, notwithstanding change. But how can adaptation and its intent be expressed at design time so that to analyze its possible impact at runtime over a long period of time? To answer this question we look at adaptation from the sustainability point of view. Sustainability denotes the capacity of a system to both endure and preserve its function over time. We propose an approach which uses decision maps to make sustainability-driven decisions for adaptation in a systematic way. The proposed approach is illustrated through two self-adaptive exemplars as illustrative cases.  © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ICSE-NIER55298.2022.9793525
SP  - 36
EP  - 40
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131966980&doi=10.1109%2fICSE-NIER55298.2022.9793525&partnerID=40&md5=9f2acbb436eb02dbdea7dcb872db56ab
DB  - Scopus
KW  - Self-adaptive systems
KW  - Sustainable development
KW  - Adaptive systems
KW  - Runtimes
KW  - Self-adaptive system
KW  - Software-systems
KW  - adaptation intent
KW  - Adaptation intent
KW  - Decision maps
KW  - Design time
KW  - Quality levels
KW  - sustainability goal
KW  - Sustainability goal
KW  - Uncertainty
ER  - 

TY  - JOUR
TI  - Energy usage reports: Environmental awareness as part of algorithmic accountability
AU  - Lottick, K.
AU  - Susai, S.
AU  - Friedler, S.A.
AU  - Wilson, J.P.
T2  - Energy usage reports: Environmental awareness as part of algorithmic accountability
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102644245&partnerID=40&md5=2893d7fe594fc83783af1041ab63593a
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Energy-efficient datacenters
AU  - Pedram, M.
T2  - IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems
AB  - Pervasive use of cloud computing and the resulting rise in the number of datacenters and hosting centers (that provide platform or software services to clients who do not have the means to set up and operate their own computing facilities) have brought forth many concerns, including the electrical energy cost, peak power dissipation, cooling, and carbon emission. With power consumption becoming an increasingly important issue for the operation and maintenance of the hosting centers, corporate and business owners are becoming increasingly concerned. Furthermore, provisioning resources in a cost-optimal manner so as to meet different performance criteria, such as throughput or response time, has become a critical challenge. The goal of this paper is to provide an introduction to resource provisioning and power or thermal management problems in datacenters, and to review strategies that maximize the datacenter energy efficiency subject to peak or total power consumption and thermal constraints, while meeting stipulated service level agreements in terms of task throughput and/or response time. © 2012 IEEE.
DA  - 2012///
PY  - 2012
DO  - 10.1109/TCAD.2012.2212898
VL  - 31
IS  - 10
SP  - 1465
EP  - 1484
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866609789&doi=10.1109%2fTCAD.2012.2212898&partnerID=40&md5=da8813937e1c7f70fcfef51e520f0804
DB  - Scopus
KW  - Energy efficiency
KW  - resource management
KW  - Resource management
KW  - green computing
KW  - Datacenter
KW  - Temperature control
KW  - Dynamic Power
KW  - dynamic power and thermal management
KW  - energy efficient design
KW  - Energy-efficient design
KW  - enterprise computing
KW  - Enterprise computing
ER  - 

TY  - JOUR
TI  - Green information systems: Directives for the IS discipline
AU  - Vom Brocke, J.
AU  - Watson, R.T.
AU  - Dwyer, C.
AU  - Elliot, S.
AU  - Melville, N.
T2  - Communications of the Association for Information Systems
AB  - Green IS offers the promise for IS scholars to make a significant contribution to reducing greenhouse gas emissions and mitigating the effects of global climate change and other environmental problems. While significant achievements have been made in shaping Green IS as a subfield in the IS discipline, the emergence of Green IS is still by far too slow, given the magnitude of the problem. Against this background a panel was organized at ICIS 2012 in order to discuss future directives for the IS discipline. This article, co-authored by the panelists, reports on the major issues raised by this panel. First, the article gives an account of major achievements in the field of Green IS. Second, it presents five specific directives which we agree are important for the future of our discipline. © 2013 by the Association for Information Systems.
DA  - 2013///
PY  - 2013
DO  - 10.17705/1cais.03330
VL  - 33
IS  - 1
SP  - 509
EP  - 520
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892159927&doi=10.17705%2f1cais.03330&partnerID=40&md5=2005e85fdadc1fda6f90a563e1549d00
DB  - Scopus
KW  - Sustainable development
KW  - Greenhouse gases
KW  - Climate change
KW  - Sustainability
KW  - Gas emissions
KW  - Business transformation
KW  - Business transformations
KW  - Energy informatics
KW  - Environmental problems
KW  - Global climate changes
KW  - Green information systems
KW  - Green is
KW  - Green IS
KW  - Is disciplines
ER  - 

TY  - JOUR
TI  - A microservice composition approach based on the choreography of BPMN fragments
AU  - Valderas, P.
AU  - Torres, V.
AU  - Pelechano, V.
T2  - Information and Software Technology
AB  - Context: Microservices must be composed to provide users with complex and elaborated functionalities. It seems that the decentralized nature of microservices makes a choreography style more appropriate to achieve such cooperation, where lighter solutions based on asynchronous events are generally used. However, a microservice composition based on choreography distributes the flow logic of the composition among microservices making further analysis and updating difficult, i.e. there is not a big picture of the composition that facilitates these tasks. Business Process Model and Notation (BPMN) is the OMG standard developed to represent Business Processes (BPs), being widely used to define the big picture of such compositions. However, BPMN is usually considered in orchestration-based solutions, and orchestration can be a drawback to achieve the decoupling pursued by a microservice architecture. Objective: Defining a microservice composition approach that allows us to create a composition in a BPMN model, which facilitates further analysis for taking engineering decisions, and execute them through an event-based choreography to have a high degree of decoupling and independence among microservices. Method: We followed a research methodology for information systems that consists of a 5-step process: awareness of the problem, suggestion, development, evaluation, and conclusion. Results: We presented a microservice composition approach based on the choreography of BPMN fragments. On the one hand, we propose to describe the big picture of the composition with a BPMN model, providing a valuable mechanism to analyse it when engineering decisions need to be taken. On the other hand, this model is split into fragments in order to be executed through an event-based choreography form, providing the high degree of decoupling among microservices demanded in this type of architecture. This composition approach is supported by a microservice architecture defined to achieve that both descriptions of a composition (big picture and split one) coexist. A realization of this architecture in Java/Spring technology is also presented. Conclusions: The evaluation that is done to our work allows us to conclude that the proposed approach for composing microservices is more efficient than solutions based on ad-hoc development. © 2020
DA  - 2020///
PY  - 2020
DO  - 10.1016/j.infsof.2020.106370
VL  - 127
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086996899&doi=10.1016%2fj.infsof.2020.106370&partnerID=40&md5=262618c8ab816ad2e63461161ea2c11c
DB  - Scopus
KW  - Microservices
KW  - Architecture
KW  - Business Process
KW  - AND splits
KW  - Asynchronous event
KW  - BPMN
KW  - Business process model and notation (BPMN)
KW  - Choreography
KW  - Composition
KW  - Engineering decisions
KW  - Event-based
KW  - Flow logic
KW  - Image analysis
KW  - Research methodologies
ER  - 

TY  - CONF
TI  - Business process co-design for energy-aware adaptation
AU  - Cappiello, C.
AU  - Fugini, M.
AU  - Ferreira, A.M.
AU  - Plebani, P.
AU  - Vitali, M.
T2  - Proceedings - 2011 IEEE 7th International Conference on Intelligent Computer Communication and Processing, ICCP 2011
AB  - Green IT mainly focuses on techniques to extend the products longevity or to virtualise physical resources as well as the provision of energy efficient hardware infrastructures. Less attention has been paid on the applications that run on the machines and their impact on energy consumption. This paper proposes an approach for enabling an efficient use of energy driven by the design of energy-aware business processes. Energy-awareness is given by an enrichment of a typical Business Process conceptual model with annotations able to support the assessment of the energy consumption of the involved business tasks. This information is the basis for the energy-aware adaptation to enact specific strategies to adapt process execution in case energy consumption needs to be lowered or energy leakages have been identified. © 2011 IEEE.
DA  - 2011///
PY  - 2011
DO  - 10.1109/ICCP.2011.6047917
SP  - 463
EP  - 470
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755125694&doi=10.1109%2fICCP.2011.6047917&partnerID=40&md5=f8c79e0bf9ccbdb071ad01f6414bd3a1
DB  - Scopus
KW  - Energy efficiency
KW  - Resource management
KW  - Energy utilization
KW  - Energy efficient
KW  - Energy aware
KW  - Service oriented architecture (SOA)
KW  - Green IT
KW  - Information services
KW  - Context-Aware
KW  - Co-designs
KW  - Computer hardware
KW  - Wireless sensor networks
KW  - Physical resources
KW  - Business Process
KW  - Energy-awareness
KW  - Service Oriented
KW  - Adaptive and context-aware processes
KW  - Conceptual model
KW  - Efficient use of energy
KW  - Energy leakage
KW  - Green IT and energy-aware applications
KW  - Process execution
KW  - Resource management in business process execution
KW  - Service-oriented architectures for BPM
ER  - 

TY  - JOUR
TI  - Green versus brown: Comparing the employment impacts of energy efficiency, renewable energy, and fossil fuels using an input-output model
AU  - Garrett-Peltier, H.
T2  - Economic Modelling
AB  - Global carbon emissions have reached unsustainable levels, and transforming the energy sector by increasing efficiency and use of renewables is one of the primary strategies to reduce emissions. Policy makers need to understand both the environmental and economic impacts of fiscal and regulatory policies regarding the energy sector. Transitioning to lower-carbon energy will entail a contraction of the fossil fuel sector, along with a loss of jobs. An important question is whether clean energy will create more jobs than will be lost in fossil fuels. This article presents a method of using Input-Output (I-O) tables to create “synthetic” industries – namely clean energy industries that do not currently exist in I-O tables. This approach allows researchers to evaluate public and private spending in clean energy and compare it to the effects of spending on fossil fuels. Here we focus on employment impacts in the short-to-medium term, and leave aside the long-term comparison of operations and maintenance employment. We find that on average, 2.65 full-time-equivalent (FTE) jobs are created from $1 million spending in fossil fuels, while that same amount of spending would create 7.49 or 7.72 FTE jobs in renewables or energy efficiency. Thus each $1 million shifted from brown to green energy will create a net increase of 5 jobs. © 2016 Elsevier Ltd
DA  - 2017///
PY  - 2017
DO  - 10.1016/j.econmod.2016.11.012
VL  - 61
SP  - 439
EP  - 447
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007467164&doi=10.1016%2fj.econmod.2016.11.012&partnerID=40&md5=f04db1592f0a96b8c6becba5923b071f
DB  - Scopus
KW  - Energy efficiency
KW  - Renewable energy
KW  - Employment multipliers
KW  - Fiscal policy
KW  - Fossil Fuels
KW  - Input-output
ER  - 

TY  - JOUR
TI  - Power Modeling for Effective Datacenter Planning and Compute Management
AU  - Radovanovic, A.
AU  - Chen, B.
AU  - Talukdar, S.
AU  - Roy, B.
AU  - Duarte, A.
AU  - Shahbazi, M.
T2  - IEEE Transactions on Smart Grid
AB  - Over the past decade, there has been a global growth in datacenter capacity, power consumption and the associated costs. Accurate mapping of datacenter resource usage (CPU, RAM, etc.) and hardware configurations (servers, accelerators, etc.) to its power consumption is necessary for efficient long-term infrastructure planning and real-time compute load management. This paper presents two types of statistical power models that relate CPU usage of Google's Power Distribution Units (PDUs, commonly referred to as power domains) to their power consumption. The models are deployed in production and are used for cost- and carbon-aware load management, power provisioning and infrastructure rightsizing. They are simple, interpretable and exhibit uniformly high prediction accuracy in modeling power domains with large diversity of hardware configurations and workload types across Google fleet. A multi-year validation of the deployed models demonstrate that they can predict power with less than 5% Mean Absolute Percent Error (MAPE) for more than 95% diverse PDUs across Google fleet. This performance matches the best reported accuracies coming from studies that focus on specific workload types, hardware platforms and, typically, more complex statistical models. © 2010-2012 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/TSG.2021.3125275
VL  - 13
IS  - 2
SP  - 1611
EP  - 1621
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118668503&doi=10.1109%2fTSG.2021.3125275&partnerID=40&md5=5fad320ed8e10cd39463a0ee20804373
DB  - Scopus
KW  - Energy efficiency
KW  - Hardware
KW  - Datacenter
KW  - Green computing
KW  - Load modeling
KW  - Predictive models
KW  - Electric power utilization
KW  - Computer hardware
KW  - Computational modelling
KW  - Power modeling
KW  - Power demands
KW  - Electric load management
KW  - Electric power plant loads
KW  - datacenter power efficiency
KW  - Datacenter power efficiency.
KW  - Datacenter power modeling
KW  - Power-efficiency
KW  - Statistical power
KW  - Statistical power model
KW  - statistical power models
ER  - 

TY  - CONF
TI  - Annotating executable DSLs with energy estimation formulas
AU  - Béziers La Fosse, T.
AU  - Tisi, M.
AU  - Mottu, J.-M.
AU  - Sunyé, G.
T2  - SLE 2020 - Proceedings of the 13th ACM SIGPLAN International Conference on Software Language Engineering, Co-located with SPLASH 2020
AB  - Reducing the energy consumption of a complex, especially cyber-physical, system is a cross-cutting concern through the system layers, and typically requires long feedback loops between experts in several engineering disciplines. Having an immediate automatic estimation of the global system consumption at design-time would significantly accelerate this process, but cross-layer tools are missing in several domains. Executable domain-specific modeling languages (xDSLs) can be used to design several layers of the system under development in an integrated view. By including the behavioral specification for software and physical components of the system, they are an effective source artifact for cross-layer energy estimation. In this paper we propose EEL, a language for annotating xDSL primitives with energy-related properties, i.e. how their execution would contribute to the energy consumption on a specific runtime platform. Given an xDSL, energy specialists create EEL models of that xDSL for each considered runtime platform. The models are used at design time, to predict the energy consumption of the real systems. This avoids the need of energetic analysis by deployment and measurement on all runtime platforms, that is slow and expensive. We augment an existing language workbench for xDSLs with an editor for EEL models and a component that computes energy-consumption estimations during model editing. The evaluation shows that EEL can be used to represent estimation models from literature, and provide useful predictions.  © 2020 ACM.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3426425.3426930
SP  - 22
EP  - 38
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097711159&doi=10.1145%2f3426425.3426930&partnerID=40&md5=81c27a72ed30adb5ec3fce4903105aa9
DB  - Scopus
KW  - cyber-physical systems
KW  - DSL
KW  - Energy utilization
KW  - Real time systems
KW  - Modeling languages
KW  - Automatic estimation
KW  - Behavioral specification
KW  - Computer programming languages
KW  - Cross-cutting concerns
KW  - Digital subscriber lines
KW  - Domain specific modeling languages
KW  - energy estimation
KW  - Energy specialists
KW  - Engineering disciplines
KW  - Language workbenches
KW  - Physical components
KW  - Specification languages
KW  - xDSL
ER  - 

TY  - JOUR
TI  - Energy efficient placement of workloads in composable data center networks
AU  - Ajibola, O.O.
AU  - El-Gorashi, T.
AU  - Elmirghani, J.
T2  - Journal of Lightwave Technology
AB  - This paper studies the energy efficiency of composable data center (DC) infrastructures over network topologies. Using a mixed integer linear programming (MILP) model, we compare the performance of disaggregation at rack-scale and pod-scale over selected electrical, optical and hybrid network topologies relative to a traditional DC. Relative to a pod-scale DC, the results show that physical disaggregation at rack-scale is sufficient for optimal efficiency when the optical network topology is adopted, and resource components are allocated in a suitable manner. The optical network topology also enables optimal energy efficiency in composable DCs. The paper also studies logical disaggregation of traditional DC servers over an optical network topology. Relative to physical disaggregation at rack-scale, logical disaggregation of server resources within each rack enables marginal fall in the total DC power consumption (TDPC) due to improved resource demands placement. Hence, an adaptable composable infrastructure that can support both in memory (access) latency sensitive and insensitive workloads is enabled. We also conduct a study of the adoption of micro-service architecture in both traditional and composable DCs. Our results show that increasing the modularity of workloads improves the energy efficiency in traditional DCs, but disproportionate utilization of DC resources persists. A combination of disaggregation and micro-services achieved up to 23% reduction in the TDPC of the traditional DC by enabling optimal resources utilization and energy efficiencies. Finally, we propose a heuristic for energy efficient placement of workloads in composable DCs which replicates the trends produced by the MILP model formulated in this paper.  © 1983-2012 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/JLT.2021.3063325
VL  - 39
IS  - 10
SP  - 3037
EP  - 3063
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102297167&doi=10.1109%2fJLT.2021.3063325&partnerID=40&md5=450f3edf04ce174688cd5b761aa2e04c
DB  - Scopus
KW  - Energy efficiency
KW  - Topology
KW  - Green computing
KW  - Energy efficient
KW  - Network topology
KW  - Composable infrastructures
KW  - Integer programming
KW  - Resources utilizations
KW  - Resource demands
KW  - Data center networks
KW  - DC power consumption
KW  - Energy efficient data centers
KW  - Fiber optic networks
KW  - Micro-services
KW  - Milp
KW  - Mixed integer linear programming model
KW  - Optical networks
KW  - Optimal efficiency
KW  - Rack-scale data center
KW  - Software defined infrastructures
ER  - 

TY  - JOUR
TI  - Pattern-driven green adaptation of process-based applications and their runtime infrastructure
AU  - Nowak, A.
AU  - Binz, T.
AU  - Fehling, C.
AU  - Kopp, O.
AU  - Leymann, F.
AU  - Wagner, S.
T2  - Computing
AB  - Business processes are a key aspect of modern organization. In recent years, business process management and optimization has been applied to different cross-cutting concerns such as security, compliance, or Green IT, for example. Based on the ecological characteristics of a business process, proper environmentally sustainable adaptation strategies can be chosen to improve the total environmental impact of the business process. We use ecological sustainable adaptation strategies that are described as green business process patterns. The application of such a green business process pattern, however, affects the business process layer, the application component and the infrastructure layer. This implies that changes in the application infrastructure also need to be considered. Hence, we use best practices of cloud application architectures which are described as Cloud patterns. To guide developers through the adaptation process we propose a pattern-based approach in this work. We correlate Cloud patterns relevant for sustainable business processes to green business process patterns and organize them within a classification. To provide concrete implementation supportwe further annotate these Cloud patterns to application component models that are described with the topology and orchestration specification for cloud applications (TOSCA). Using these annotations, we describe a method that provides the means to optimize business processes based on green business process patterns through adapting the implementation of application components with concrete TOSCA implementation models. © Springer-Verlag 2012.
DA  - 2012///
PY  - 2012
DO  - 10.1007/s00607-012-0188-x
VL  - 94
IS  - 6
SP  - 463
EP  - 487
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864571412&doi=10.1007%2fs00607-012-0188-x&partnerID=40&md5=8636a305081456245beee386c74a5f77
DB  - Scopus
KW  - Information technology
KW  - Sustainable development
KW  - Optimization
KW  - Green IT
KW  - Business Process
KW  - Cloud pattern
KW  - Adaptation of applications
KW  - Ecological sustainable business processes
KW  - Ecology
KW  - Enterprise resource management
KW  - Green business process pattern
KW  - Sustainable business
KW  - TOSCA
ER  - 

TY  - JOUR
TI  - A taxonomy and future directions for sustainable cloud computing: 360 degree view
AU  - Gill, S.S.
AU  - Buyya, R.
T2  - ACM Computing Surveys
AB  - The cloud-computing paradigm offers on-demand services over the Internet and supports a wide variety of applications. With the recent growth of Internet of Things (IoT)–based applications, the use of cloud services is increasing exponentially. The next generation of cloud computing must be energy efficient and sustainable to fulfill end-user requirements, which are changing dynamically. Presently, cloud providers are facing challenges to ensure the energy efficiency and sustainability of their services. The use of a large number of cloud datacenters increases cost as well as carbon footprints, which further affects the sustainability of cloud services. In this article, we propose a comprehensive taxonomy of sustainable cloud computing. The taxonomy is used to investigate the existing techniques for sustainability that need careful attention and investigation as proposed by several academic and industry groups. The current research on sustainable cloud computing is organized into several categories: application design, sustainability metrics, capacity planning, energy management, virtualization, thermal-aware scheduling, cooling management, renewable energy, and waste heat utilization. The existing techniques have been compared and categorized based on common characteristics and properties. A conceptual model for sustainable cloud computing has been presented along with a discussion on future research directions. © 2018 Association for Computing Machinery.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3241038
VL  - 51
IS  - 5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060525188&doi=10.1145%2f3241038&partnerID=40&md5=d3f40eea5104992aa1dda08cc14c583b
DB  - Scopus
KW  - Cloud computing
KW  - Energy efficiency
KW  - Cloud datacenter
KW  - Internet of things
KW  - Scheduling
KW  - Virtualization
KW  - Sustainable development
KW  - Carbon footprint
KW  - Quality of service
KW  - Cloud-computing
KW  - Datacenter
KW  - Green computing
KW  - Sustainability
KW  - Quality-of-service
KW  - Web services
KW  - Renewable energies
KW  - Energy management
KW  - Distributed database systems
KW  - Capacity planning
KW  - Application design
KW  - Virtualizations
KW  - Waste management
KW  - Renewable energy
KW  - Cloud datacenters
KW  - And waste heat utilization
KW  - Cooling management
KW  - Holistic management
KW  - Sustainable cloud computing
KW  - Sustainable cloud datacenter
KW  - Sustainable cloud datacenters
KW  - Sustainable metric
KW  - Sustainable metrics
KW  - Thermal-aware scheduling
ER  - 

TY  - JOUR
T2  - Business Process Model and Notation (BPMN)
DA  - 2009///
PY  - 2009
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957994724&partnerID=40&md5=9f5862f053a504beb84804c136c403e9
DB  - Scopus
ER  - 

TY  - JOUR
AU  - Knight, W.
T2  - AI Can Do Great Things-if It Doesn't Burn the Planet
DA  - 2020///
PY  - 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109274656&partnerID=40&md5=48623e1a31d1ab6b8ccc435d9e1709b5
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Datazero: Datacenter with zero emission and robust management using renewable energy
AU  - Pierson, J.-M.
AU  - Baudic, G.
AU  - Caux, S.
AU  - Celik, B.
AU  - Da Costa, G.
AU  - Grange, L.
AU  - Haddad, M.
AU  - Lecuivre, J.
AU  - Nicod, J.-M.
AU  - Philippe, L.
AU  - Rehn-Sonigo, V.
AU  - Roche, R.
AU  - Rostirolla, G.
AU  - Sayah, A.
AU  - Stolf, P.
AU  - Thi, M.-T.
AU  - Varnier, C.
T2  - IEEE Access
AB  - As the need for cloud services has been growing steadily, the size and energy consumption of datacenters have increased significantly over the past years. Due to economic and environmental constraints, energy efficiency in datacenters and greenhouse emissions have become a major concern. Renewable energy is widely seen as a promising solution to supply datacenters using local energy, without greenhouse gas emissions. However, the intermittent power generation resulting from the use of renewable energy imposes a paradigm change in the way energy and computation activities are managed. On the one hand, service placement and scheduling may be used on the IT (information technologies) side to adapt to the available power. On the other hand, the storage units may be used to lessen power generation variations. Existing literature and actual deployment mainly design optimization algorithms including the entire system (from cloud service to electrical management, the latter often being neglected or simplified). Conversely to these approaches, we propose a solution where each side optimizes its own objectives, both interacting through a negotiation loop process to reach a common agreement. In this paper, we present DATAZERO, a project developing this idea to ensure high availability of IT services, avoiding unnecessary redundancies, under the constraints due to the intermittent nature of electrical and cloud services flows. © 2019 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.
DA  - 2019///
PY  - 2019
DO  - 10.1109/ACCESS.2019.2930368
VL  - 7
SP  - 103209
EP  - 103230
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076226061&doi=10.1109%2fACCESS.2019.2930368&partnerID=40&md5=7b284126f13bafd903b5e1137bd6f5dc
DB  - Scopus
KW  - Energy efficiency
KW  - Greenhouse gases
KW  - Green computing
KW  - Optimization
KW  - Energy utilization
KW  - Middleware
KW  - Web services
KW  - High availability
KW  - Renewable energies
KW  - Greenhouse emissions
KW  - Gas emissions
KW  - Service placements
KW  - Use of renewable energies
KW  - Negotiation
KW  - Cloud datacenters
KW  - Common agreement
KW  - Design optimization
KW  - Environmental constraints
KW  - Power models
ER  - 

TY  - CONF
TI  - Parasol and greenswitch: Managing datacenters powered by renewable energy
AU  - Goiri, Í.
AU  - Katsak, W.
AU  - Le, K.
AU  - Nguyen, T.D.
AU  - Bianchini, R.
T2  - International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS
AB  - Several companies have recently announced plans to build "green" datacenters, i.e. datacenters partially or completely powered by renewable energy. These datacenters will either generate their own renewable energy or draw it directly from an existing nearby plant. Besides reducing carbon footprints, renewable energy can potentially reduce energy costs, reduce peak power costs, or both. However, certain renewable fuels are intermittent, which requires approaches for tackling the energy supply variability. One approach is to use batteries and/or the electrical grid as a backup for the renewable energy. It may also be possible to adapt the workload to match the renewable energy supply. For highest benefits, green datacenter operators must intelligently manage their workloads and the sources of energy at their disposal. In this paper, we first discuss the tradeoffs involved in building green datacenters today and in the future. Second, we present Parasol, a prototype green datacenter that we have built as a research platform. Parasol comprises a small container, a set of solar panels, a battery bank, and a grid-tie. Third, we describe GreenSwitch, our model-based approach for dynamically scheduling the workload and selecting the source of energy to use. Our real experiments with Parasol, GreenSwitch, and MapReduce workloads demonstrate that intelligent workload and energy source management can produce significant cost reductions. Our results also isolate the cost implications of peak power management, storing energy on the grid, and the ability to delay the MapReduce jobs. Finally, our results demonstrate that careful workload and energy source management can minimize the negative impact of electrical grid outages. Categories and Subject Descriptors C.m [Computer Systems Organization]: Miscellaneous; D.4.1 [Operating Systems]: Process Management-Scheduling. Copyright © 2013 ACM.
DA  - 2013///
PY  - 2013
DO  - 10.1145/2451116.2451123
SP  - 51
EP  - 63
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875681902&doi=10.1145%2f2451116.2451123&partnerID=40&md5=45dbcae6059c770dd9afad66d3c8ff26
DB  - Scopus
KW  - Datacenters
KW  - Scheduling
KW  - Datacenter
KW  - Batteries
KW  - Energy supplies
KW  - Renewable energies
KW  - Battery
KW  - Renewable energy
KW  - Electrical grids
KW  - Sources of energy
KW  - Energy source
KW  - Map-reduce
KW  - Peak power
KW  - Source management
ER  - 

TY  - JOUR
T2  - Cloud Carbon Footprint Tool
DA  - 2023///
PY  - 2023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179503064&partnerID=40&md5=8bbe00d3d506a9496db6d11fd78d98d7
DB  - Scopus
ER  - 

TY  - JOUR
TI  - On revisiting energy and performance in microservices applications: A cloud elasticity-driven approach
AU  - Fontana de Nardin, I.
AU  - da Rosa Righi, R.
AU  - Lima Lopes, T.R.
AU  - André da Costa, C.
AU  - Yeom, H.Y.
AU  - Köstler, H.
T2  - Parallel Computing
AB  - Monolithic applications are a subject that includes several knowledge areas. Sometimes it can be a challenge to optimize CPU or IO requirements because it is not trivial to recognize the problem itself and improve it. There are many approaches to resolve this situation, where a trending one is the microservices. As a variant of the service-oriented architecture, microservices is a technique that arranges an application as a collection of loosely coupled services. This decomposition enables better software management in cloud-based environments since we can replicate each part individually using cloud elasticity to avoid execution bottlenecks. Also, since elasticity mitigates resource overprovisioning, it favors better energy consumption: the cloud owner can redistribute finite available resources among different tenants, and users can pay less to use the infrastructure. However, elasticity tuning is not trivial and depends on several factors, such as user experience, application architecture, and parameter modeling. Today, we observe a lack of initiatives in the literature that address both performance and energy perspectives to support the execution of microservices applications in the cloud. Concerning this context, this article introduces Elergy as a lightweight proactive elasticity model that provides resource reorganization for a cloud-based microservices application. Its differential approach appears in improving energy consumption by periodically handling the most appropriate amount of resources to execute an application while maintaining or yet improving the performance of CPU-bound applications. Elergy performs these functions proactively, in such a way of preventing future problems related to either resource under- or overprovisioning. The results showed energy consumption reduction and a competitive cost (application time x consumed resources) when comparing Elergy with a non-elastic scenario. Elergy obtained savings from 1.93% to 27.92% for energy consumption. © 2021 Elsevier B.V.
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.parco.2021.102858
VL  - 108
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117368062&doi=10.1016%2fj.parco.2021.102858&partnerID=40&md5=cd2fc1322467a84cd684103e180284ac
DB  - Scopus
KW  - Cloud computing
KW  - Microservices
KW  - Performance
KW  - Microservice
KW  - Elasticity
KW  - Energy-consumption
KW  - Cloud-computing
KW  - Green computing
KW  - Energy utilization
KW  - Energy
KW  - Service oriented architecture (SOA)
KW  - Information services
KW  - Cloud-based
KW  - Over provisioning
KW  - Monolithics
KW  - Cloud elasticities
KW  - Knowledge areas
ER  - 

TY  - JOUR
TI  - Recalibrating global data center energy-use estimates: Growth in energy use has slowed owing to efficiency gains that smart policies can help maintain in the near term
AU  - Masanet, E.
AU  - Shehabi, A.
AU  - Lei, N.
AU  - Smith, S.
AU  - Koomey, J.
T2  - Science
DA  - 2020///
PY  - 2020
DO  - 10.1126/science.aba3758
VL  - 367
IS  - 6481
SP  - 984
EP  - 986
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080913294&doi=10.1126%2fscience.aba3758&partnerID=40&md5=e714910a4e298ff7a30d72190beae6c0
DB  - Scopus
KW  - cloud computing
KW  - energy efficiency
KW  - energy
KW  - data processing
KW  - electricity
KW  - priority journal
KW  - policy
KW  - power supply
KW  - information technology
KW  - calibration
KW  - client server application
KW  - data assimilation
KW  - energy use
KW  - funding
KW  - information storage
KW  - internet protocol
KW  - investment
KW  - policy making
KW  - power usage effectiveness
KW  - Review
ER  - 

TY  - CONF
TI  - Game-based negotiation between power demand and supply in green datacenters
AU  - Thi, M.-T.
AU  - Pierson, J.-M.
AU  - Da Costa, G.
T2  - Proceedings - 2020 IEEE International Symposium on Parallel and Distributed Processing with Applications, 2020 IEEE International Conference on Big Data and Cloud Computing, 2020 IEEE International Symposium on Social Computing and Networking and 2020 IEEE International Conference on Sustainable Computing and Communications, ISPA-BDCloud-SocialCom-SustainCom 2020
AB  - The power consumption of datacenters is growing rapidly and becoming a major concern. For reducing carbon footprint and increasing energy efficiency, a promising solution is to locally supply datacenters with renewable energies. However, a challenging problem in building such green datacenter is the coordinating between the power demand and the intermittent power supply. To address this problem, we propose to model the green datacenter as two subsystems, namely, Information Technology (IT) subsystem which consumes energy, and electrical subsystem which supplies energy. Then we aim to find an efficient compromise between the power supply and power demand, taking into account the constraints of both subsystems. Based on buyer-supplier game, we introduce a negotiation approach, in which the two subsystems are modeled as the energy buyer and energy supplier. A negotiation algorithm is proposed, allowing the two subsystems to negotiate and reach an efficient trade-off, while respecting their own utility/monetary gain. The algorithm is evaluated in our middleware of renewable energies-powered datacenter. The experimental results show that the proposed algorithm allows the negotiation process to reach stable points. This algorithm also obtains significant improvement in the datacenter's utility and quality of service (QoS), compared to the algorithms in which joint IT-energy management is not considered. © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ISPA-BDCloud-SocialCom-SustainCom51426.2020.00112
SP  - 690
EP  - 697
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108028950&doi=10.1109%2fISPA-BDCloud-SocialCom-SustainCom51426.2020.00112&partnerID=40&md5=3b71c813352ce106a7569fb763e4acbc
DB  - Scopus
KW  - Cloud computing
KW  - Energy efficiency
KW  - Game theory
KW  - Big data
KW  - Carbon footprint
KW  - Quality of service
KW  - Green computing
KW  - Economic and social effects
KW  - Commerce
KW  - Middleware
KW  - Electric power utilization
KW  - Renewable energies
KW  - Power demands
KW  - Social networking (online)
KW  - Power supply
KW  - Electric power systems
KW  - Electrical subsystems
KW  - Energy suppliers
KW  - Green Datacenter
KW  - Negotiation
KW  - Negotiation algorithm
KW  - Negotiation process
KW  - Stable points
ER  - 

TY  - JOUR
TI  - A novel energy-aware resource management technique using joint VM and container consolidation approach for green computing in cloud data centers
AU  - Gholipour, N.
AU  - Arianyan, E.
AU  - Buyya, R.
T2  - Simulation Modelling Practice and Theory
AB  - Cloud computing is being rapidly adopted for managing IT services as a notable solution due to diverse beneficiaries such as automatically optimized resource management as well as modern service delivery models. The container as a service has been recently introduced by cloud providers as a new service apart from traditional cloud services. Containers enable applications to run and deploy on isolated virtual space, and the operating system kernel is shared among them. Also, containerization has some attributes such as scalability, highly portable properties, and lightweight, for those reasons, it is applied for running isolated applications. Reducing energy consumption, as well as their CO2 emissions, are great deals for cloud providers. In this direction, consolidation is recommended as a vital energy-aware approach in cloud data centers. Previously, independent virtual machine migration or container migration was proposed in the literature for green computing in cloud data centers. However, this paper proposes a new cloud resource management procedure based on a multi-criteria decision-making method that takes advantage of a joint virtual machine and container migration approach concurrently. The results of simulations using ContainerCloudsim simulator validates the applicability of the proposed approach which shows notable reductions in energy consumption, SLA violation, and number of migrations in comparison with the state-of-the-art algorithms. © 2020 Elsevier B.V.
DA  - 2020///
PY  - 2020
DO  - 10.1016/j.simpat.2020.102127
VL  - 104
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086469770&doi=10.1016%2fj.simpat.2020.102127&partnerID=40&md5=f6c07590171b1d1c04d2407d84f70c54
DB  - Scopus
KW  - Cloud computing
KW  - Resource management
KW  - Containers
KW  - Energy consumption
KW  - Power management
KW  - Decision making
KW  - Network security
KW  - Computer aided software engineering
KW  - Datacenter
KW  - Green computing
KW  - Energy utilization
KW  - Reducing energy consumption
KW  - Resource allocation
KW  - Natural resources management
KW  - Information management
KW  - Virtual machine
KW  - Consolidation
KW  - Containerization
KW  - Virtual machine migrations
KW  - Resource management techniques
KW  - State-of-the-art algorithms
KW  - Energy aware approaches
KW  - Multi-criteria decision making methods
KW  - Operating system kernel
ER  - 

TY  - JOUR
AU  - Souza, A.
AU  - Bashir, N.
T2  - Ecovisor: A Virtual Energy System for Carbon-efficient Applications
DA  - 2022///
PY  - 2022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179523438&partnerID=40&md5=413ec0637a84a68c003b35671169313d
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Azure Emissions Impact Dashboard
DA  - 2023///
PY  - 2023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179519572&partnerID=40&md5=c16d417e84300f715a1bd9d1615a8f6e
DB  - Scopus
ER  - 

TY  - JOUR
AU  - Bruegge, B.
AU  - Dutoit, A.H.
T2  - Object-Oriented Software Engineering Using UML, Patterns, and Java
DA  - 2004///
PY  - 2004
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003984986&partnerID=40&md5=c2fd5ef3e7d01eea418ca6aaa0e50808
DB  - Scopus
ER  - 

TY  - JOUR
T2  - AWS customer carbon footprint tool
DA  - 2023///
PY  - 2023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179510119&partnerID=40&md5=74c5854553d60b000a0b3a740acaba9e
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Enabling rank-based distribution of microservices among containers for green cloud computing environment
AU  - Saboor, A.
AU  - Mahmood, A.K.
AU  - Omar, A.H.
AU  - Hassan, M.F.
AU  - Shah, S.N.M.
AU  - Ahmadian, A.
T2  - Peer-to-Peer Networking and Applications
AB  - Microservices architecture is a functional software design methodology that promises the redefinition of the architectural style that aims to create a single application as a suite of tiny, loosely coupled services or components, each performing its own tasks and interacting with each other. The cloud services widely shifted from monoliths to microservices and gained the popularity for use in scalable cloud application. The usage of microservices involved intensive network communication to call number of interdependent microservices running inside the cloud nodes. It provides flexibility in the delivery of service but also increases energy usage and poor service efficiency which results in increased carbon emissions. To solve these issues, the prevailing technologies were designed for single unit monolithic cloud applications, and not tailored for the chain oriented service delivery. This study addresses the dynamic provisioning of containers and respective microservices in cloud computing environment by building rank-based profiles and using those profiles for allocation of web application’s microservices along with containers to the cloud data centers. The MicroRanker service is proposed to rank all of the participating microservices and distribute them across different nodes even before the execution of the cloud services. Further, the MicroRanker service is utilized to dynamically update the container placement due to continuous DevOps actions. The proposed solution was tested using custom built simulation environment. The achieved results showed that the distribution of containers along with respective microservices in accordance with MicroRanker service resulted in less energy consumption (i.e. between 81.6 kWh-87.7 kWh compared to 88.9 kWh-95.7 kWh) and significantly lowered the emission of carbon (i.e. between 5.92 kg-33.31 kg compared to 17.2 kg-47.35 kg) due to higher utilization of renewable energy. The use of rank-based microservices distribution also decreased response time (i.e. between 29 ms-142 ms compared to 106 ms-217 ms) due to the availability of the container along with microservice within the same data center region. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
DA  - 2022///
PY  - 2022
DO  - 10.1007/s12083-021-01218-y
VL  - 15
IS  - 1
SP  - 77
EP  - 91
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112841408&doi=10.1007%2fs12083-021-01218-y&partnerID=40&md5=ee57caac17e75a71c6e3bdd06f31114e
DB  - Scopus
KW  - Cloud computing
KW  - Microservices
KW  - Containers
KW  - Software design
KW  - Application programs
KW  - Carbon
KW  - Green computing
KW  - Optimization
KW  - Energy utilization
KW  - Cloud computing environments
KW  - Cloud data centers
KW  - Web services
KW  - Renewable energies
KW  - Architectural style
KW  - Network communications
KW  - Dynamic provisioning
KW  - High performance computing
KW  - Ranking
KW  - Simulation environment
KW  - Software design methodologies
ER  - 

TY  - JOUR
TI  - How green is my cloud?
AU  - Swan, G.
T2  - Cio
DA  - 2011///
PY  - 2011
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179515540&partnerID=40&md5=1d61b66316e92c6a9b04ba544ec57eb2
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Understanding cloud-native applications after 10 years of cloud computing - A systematic mapping study
AU  - Kratzke, N.
AU  - Quint, P.-C.
T2  - Journal of Systems and Software
AB  - It is common sense that cloud-native applications (CNA) are intentionally designed for the cloud. Although this understanding can be broadly used it does not guide and explain what a cloud-native application exactly is. The term “cloud-native” was used quite frequently in birthday times of cloud computing (2006) which seems somehow obvious nowadays. But the term disappeared almost completely. Suddenly and in the last years the term is used again more and more frequently and shows increasing momentum. This paper summarizes the outcomes of a systematic mapping study analyzing research papers covering “cloud-native” topics, research questions and engineering methodologies. We summarize research focuses and trends dealing with cloud-native application engineering approaches. Furthermore, we provide a definition for the term “cloud-native application” which takes all findings, insights of analyzed publications and already existing and well-defined terminology into account. © 2017 Elsevier Inc.
DA  - 2017///
PY  - 2017
DO  - 10.1016/j.jss.2017.01.001
VL  - 126
SP  - 1
EP  - 16
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009186306&doi=10.1016%2fj.jss.2017.01.001&partnerID=40&md5=00833465689b49e90c3ae3861f0862ab
DB  - Scopus
KW  - Cloud computing
KW  - Microservice
KW  - Systematic mapping study
KW  - Mapping
KW  - Systematic mapping studies
KW  - Cloud-native application
KW  - CNA
KW  - Elastic platform
KW  - Pattern
KW  - Self service
KW  - Softwareization
ER  - 

TY  - JOUR
TI  - Carbon-aware computing for datacenters
AU  - Radovanovic, A.
AU  - Koningstein, R.
AU  - Schneider, I.
AU  - Chen, B.
AU  - Duarte, A.
AU  - Roy, B.
AU  - Xiao, D.
AU  - Haridasan, M.
AU  - Hung, P.
AU  - Care, N.
T2  - Carbon-aware computing for datacenters
DA  - 2021///
PY  - 2021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115135925&partnerID=40&md5=9021d768febdcbc0a73844b7efd23ce9
DB  - Scopus
ER  - 

TY  - JOUR
TI  - A Cost and Energy Efficient Task Scheduling Technique to Offload Microservices Based Applications in Mobile Cloud Computing
AU  - Ali, A.
AU  - Iqbal, M.M.
T2  - IEEE Access
AB  - The number of smartphone users and mobile devices has increased significantly. The Mobile Cloud Applications based on cloud computing have also been increased. The mobile apps can be used in Augmented Reality, E-Transportation, 2D/3-D Games, E-Healthcare, and Education. The modern cloud-based frameworks provide such services on Virtual Machines. The existing frameworks worked well, but these suffered the problems such as overhead, resource utilization, lengthy boot-time, and cost of running Mobile Applications. This study addresses these problems by proposing a Dynamic Decision-Based Task Scheduling Technique for Microservice-based Mobile Cloud Computing Applications (MSCMCC). The MSCMCC runs delay-sensitive applications and mobility with less cost than existing approaches. The study focused on Task Scheduling problems on heterogeneous Mobile Cloud servers. We further propose Task Scheduling and Microservices based Computational Offloading (TSMCO) framework to solve the Task Scheduling in steps, such as Resource Matching, Task Sequencing, and Task Scheduling. Furthermore, the experimental results elaborate that the proposed MSCMCC and TSMCO enhance the Mobile Server Utilization. The proposed system effectively minimizes the cost of healthcare applications by 25%, augmented reality by 23%, E-Transport tasks by 21%, and 3-D games tasks by 19%, the average boot-time of microservices applications by 17%, resource utilization by 36%, and tasks arrival time by 16%.  © 2013 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ACCESS.2022.3170918
VL  - 10
SP  - 46633
EP  - 46651
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129650903&doi=10.1109%2fACCESS.2022.3170918&partnerID=40&md5=bbe4705e536aabb149df2254dbbbe6be
DB  - Scopus
KW  - Cloud computing
KW  - task scheduling
KW  - Energy efficiency
KW  - microservices
KW  - Augmented reality
KW  - Microservice
KW  - Microservice architecture
KW  - Costs
KW  - Cloud-computing
KW  - Delay-sensitive applications
KW  - Task analysis
KW  - Mobile cloud computing
KW  - Processor scheduling
KW  - mobile cloud computing
KW  - Mobile handsets
KW  - task offloading
KW  - task sequencing
KW  - Multitasking
KW  - Tasks scheduling
KW  - mHealth
KW  - Scheduling algorithms
KW  - Cost benefit analysis
KW  - Job analysis
KW  - Task offloading
KW  - Task sequencing
ER  - 

TY  - JOUR
TI  - 2018 Best Practice Guidelines for the EU Code of Conduct on Data Centre Energy Efficiency
AU  - Acton, M.
AU  - Bertoldi, P.
AU  - Booth, J.
AU  - Newcombe, L.
AU  - Rouyer, A.
AU  - Tozer, R.
T2  - Best Practice Guidelines for the EU Code of Conduct on Data Centre Energy Efficiency
DA  - 2018///
PY  - 2018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060207119&partnerID=40&md5=fa589630dad8c227e70c8ecff6fd0320
DB  - Scopus
ER  - 

TY  - CHAP
TI  - Software Sustainability in the Age of Everything as a Service
AU  - Andrikopoulos, V.
AU  - Lago, P.
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
AB  - The need for acknowledging and managing sustainability as an essential quality of software systems has been steadily increasing over the past few years, in part as a reaction to the implications of “software eating the world”. Especially the widespread adoption of the Everything as a Service (*aaS) model of delivering software and (virtualized) hardware through cloud computing has put two sustainability dimensions upfront and center. On the one hand, services must be sustainable on a technical level by ensuring continuity of operations for both providers and consumers despite, or even better, while taking into account their evolution. On the other hand, the prosuming of services must also be financially sustainable for the involved stakeholders. In this work, we discuss the need for a software architecting approach that encompasses in a holistic manner the other two dimensions of software sustainability as well, namely the social and environmental aspects of services. We highlight relevant works and identify key challenges still to be addressed in the context of software systems operating across different models for cloud delivery and deployment. We then present our vision for an architecting framework that allows system stakeholders to work in tandem towards improving a set of sustainability indicators specifically tailored for the *aaS model. © 2021, Springer Nature Switzerland AG.
DA  - 2021///
PY  - 2021
VL  - 12521 LNCS
SP  - 35
EP  - 47
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104134021&doi=10.1007%2f978-3-030-73203-5_3&partnerID=40&md5=9dbb5e649567d314bea19d4024dee983
DB  - Scopus
KW  - Cloud computing
KW  - Software architecture
KW  - Sustainable development
KW  - Software sustainability
KW  - Software systems
KW  - Computer software
KW  - *aaS
KW  - Architecting frameworks
KW  - Quality of softwares
KW  - Social and environmental
KW  - Software architecting
KW  - Sustainability dimensions
KW  - Sustainability indicators
KW  - Technical levels
KW  - Vision
ER  - 

TY  - JOUR
TI  - A Black-box Monitoring Approach to Measure Microservices Runtime Performance
AU  - Brondolin, R.
AU  - Santambrogio, M.D.
T2  - ACM Transactions on Architecture and Code Optimization
AB  - Microservices changed cloud computing by moving the applications' complexity from one monolithic executable to thousands of network interactions between small components. Given the increasing deployment sizes, the architectural exploitation challenges, and the impact on data-centers' power consumption, we need to efficiently track this complexity. Within this article, we propose a black-box monitoring approach to track microservices at scale, focusing on architectural metrics, power consumption, application performance, and network performance. The proposed approach is transparent w.r.t. the monitored applications, generates less overhead w.r.t. black-box approaches available in the state-of-the-art, and provides fine-grain accurate metrics. © 2020 ACM.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3418899
VL  - 17
IS  - 4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097221697&doi=10.1145%2f3418899&partnerID=40&md5=a35a5b1cfd13bccdec489a30d18ccd45
DB  - Scopus
KW  - cloud computing
KW  - docker
KW  - kubernetes
KW  - Microservices
KW  - network performance monitoring
KW  - performance monitoring
KW  - power attribution
KW  - Data centers
KW  - Green computing
KW  - Electric power utilization
KW  - State of the art
KW  - Complex networks
KW  - Application performance
KW  - Black box approach
KW  - Run-time performance
KW  - Monitoring approach
KW  - Network interaction
KW  - Small components
ER  - 

TY  - JOUR
TI  - Cloud-Native Applications
AU  - Gannon, D.
AU  - Barga, R.
AU  - Sundaresan, N.
T2  - IEEE Cloud Computing
AB  - The term 'cloud-native' refers to a set of technologies and design patterns that have become the standard for building large-scale cloud applications. In this editorial we describe basic properties of successful cloud applications including dynamic scalability, extreme fault tolerance, seamless upgradeability and maintenance and security. To make it possible to build applications that meet these requirements we describe the microservice architecture and serverless computing foundation that are central to cloud-native design. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/MCC.2017.4250939
VL  - 4
IS  - 5
SP  - 16
EP  - 21
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038610210&doi=10.1109%2fMCC.2017.4250939&partnerID=40&md5=198ead132dc790dd17c30a4142e93127
DB  - Scopus
KW  - cloud computing
KW  - Cloud computing
KW  - microservices
KW  - serverless
KW  - distributed computing
KW  - Design Patterns
KW  - cloud-native
KW  - Distributed computer systems
KW  - Cloud applications
KW  - Fault tolerance
KW  - Upgradeability
ER  - 

TY  - CONF
TI  - Principles of green data mining
AU  - Schneider, J.
AU  - Basalla, M.
AU  - Seidel, S.
T2  - Proceedings of the Annual Hawaii International Conference on System Sciences
AB  - This paper develops a set of principles for green data mining, related to the key stages of business understanding, data understanding, data preparation, modeling, evaluation, and deployment. The principles are grounded in a review of the Cross Industry Standard Process for Data mining (CRISP-DM) model and relevant literature on data mining methods and Green IT. We describe how data scientists can contribute to designing environmentally friendly data mining processes, for instance, by using green energy, choosing between make-or-buy, exploiting approaches to data reduction based on business understanding or pure statistics, or choosing energy friendly models. © 2019 IEEE Computer Society. All rights reserved.
DA  - 2019///
PY  - 2019
VL  - 2019-January
SP  - 2065
EP  - 2074
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084268601&partnerID=40&md5=71404958600ace3dbea27590a8995f59
DB  - Scopus
KW  - Green energy
KW  - Data mining
KW  - Business understanding
KW  - Cross industry
KW  - Data mining methods
KW  - Data mining process
KW  - Data preparation
KW  - Data understanding
KW  - Green manufacturing
KW  - Make-or-buy
ER  - 

TY  - JOUR
T2  - Introducing the Website Carbon Rating System
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218919009&partnerID=40&md5=98207cc5196ae939820fcd87be59505a
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Carbon footprint reporting-customer carbon footprint tool-amazon web services
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163845871&partnerID=40&md5=c5fd5048a46cf53bdd56b301ce89de0b
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Carbon Footprint — Reporting Methodology - Carbon Footprint Documentation
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199329343&partnerID=40&md5=4c611d4d5a6baef095ba9cb6ac12cb67
DB  - Scopus
ER  - 

TY  - CONF
TI  - GreenAdvisor: A tool for analyzing the impact of software evolution on energy consumption
AU  - Aggarwal, K.
AU  - Hindle, A.
AU  - Stroulia, E.
T2  - 2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings
AB  - Change-impact analysis, namely 'identifying the potential consequences of a change' is an important and well studied problem in software evolution. Any change may potentially affect an application's behaviour, performance, and energy consumption profile. Our previous work demonstrated that changes to the system-call profile of an application correlated with changes to the application's energy-consumption profile. This paper evaluates and describes GreenAdvisor, a first of its kind tool that systematically records and analyzes an application's system calls to predict whether the energy-consumption profile of an application has changed. The GreenAdvisor tool was distributed to numerous software teams, whose members were surveyed about their experience using GreenAdvisor while developing Android applications to examine the energy-consumption impact of selected commits from the teams' projects. GreenAdvisor was evaluated against commits of these teams' projects. The two studies confirm the usefulness of our tool in assisting developers analyze and understand the energy-consumption profile changes of a new version. Based on our study findings, we constructed an improved prediction model to forecast the direction of the change, when a change in the energy-consumption profile is anticipated. This work can potentially be extremely useful to developers who currently have no similar tools. © 2015 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/ICSM.2015.7332477
SP  - 311
EP  - 320
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961670071&doi=10.1109%2fICSM.2015.7332477&partnerID=40&md5=7934db344282dbc065f2e634177812ce
DB  - Scopus
KW  - energy efficiency
KW  - Energy efficiency
KW  - Forecasting
KW  - Application programs
KW  - Computer aided software engineering
KW  - Energy utilization
KW  - Computer software
KW  - Intrusion detection
KW  - Embedded systems
KW  - System calls
KW  - Software Evolution
KW  - Computer software maintenance
KW  - Android applications
KW  - Prediction model
KW  - application software
KW  - Change impact analysis
KW  - Software energy consumption
KW  - Software teams
KW  - software tools
ER  - 

TY  - JOUR
T2  - Greenframe-cli
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218915307&partnerID=40&md5=7dafb2a2ac942376edb27e9a0265ee4a
DB  - Scopus
ER  - 

TY  - CONF
TI  - Carat: Collaborative energy diagnosis for mobile devices
AU  - Oliner, A.J.
AU  - Iyer, A.P.
AU  - Stoica, I.
AU  - Lagerspetz, E.
AU  - Tarkoma, S.
T2  - SenSys 2013 - Proceedings of the 11th ACM Conference on Embedded Networked Sensor Systems
AB  - We aim to detect and diagnose energy anomalies, abnormally heavy battery use. This paper describes a collaborative black-box method, and an implementation called Carat, for diagnosing anomalies on mobile devices. A client app sends intermittent, coarse-grained measurements to a server, which correlates higher expected energy use with client properties like the running apps, device model, and operating system. The analysis quantifies the error and confidence associated with a diagnosis, suggests actions the user could take to improve battery life, and projects the amount of improvement. During a deployment to a community of more than 500,000 devices, Carat diagnosed thousands of energy anomalies in the wild. Carat detected all synthetically injected anomalies, produced no known instances of false positives, projected the battery impact of anomalies with 95% accuracy, and, on average, increased a user's battery life by 11% after 10 days (compared with 1.9% for the control group).
DA  - 2013///
PY  - 2013
DO  - 10.1145/2517351.2517354
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905676511&doi=10.1145%2f2517351.2517354&partnerID=40&md5=1834b8bf85324e3eaf7669a17e06a53f
DB  - Scopus
KW  - Analytics
KW  - Energy
KW  - Electric batteries
KW  - Mobile
KW  - Embedded systems
KW  - Diagnosis
KW  - Mobile devices
KW  - Battery
KW  - Collaborative
ER  - 

TY  - CONF
TI  - PISCO: A smart simulator to deploy energy saving methods in microservices based networks
AU  - Humberto Alvarez Valera, H.
AU  - Dalmau, M.
AU  - Roose, P.
AU  - Larracoechea, J.
AU  - Herzog, C.
T2  - 2022 18th International Conference on Intelligent Environments, IE 2022 - Proceedings
AB  - Nowadays, many researchers work to identify microservices-based application deployments and scheduling solutions to save energy without decreasing functional QoS. In this work, we present PISCO: A simulator that allows facing this challenge in a simple and efficient way, enabling its users to focus uniquely on microservices deployment/scheduling algorithms and its hardware/software repercussions (load vs. energy consumption) without worrying about low-level network configurations or operating system issues. PISCO is able to deploy and schedule (move, duplicate, start/stop) microservices and their dependencies on various devices with software and hardware heterogeneity (CPU, bandwidth, RAM, Battery, etc.), taking into account various scheduling heuristics algorithms: centralized vs non-centralized. To do this, PISCO allows deploying custom network topologies based on client-server schemes or p2p distributions, where devices can (dis)appear, turn on/off obeying random circumstances or user strategies.Finally, the simulator performs relevant operations such as QoS definition, resource monitoring, calculation of energy saved and consumption tracking (at device and network level). We tested some ideas based on our previous work 'Kaligreen' to demonstrate the effectiveness of PISCO.  © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/IE54923.2022.9826775
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136004987&doi=10.1109%2fIE54923.2022.9826775&partnerID=40&md5=d4709e1da2c1de3afc149006c04bedf6
DB  - Scopus
KW  - microservices
KW  - middleware
KW  - energy
KW  - Microservice
KW  - Scheduling
KW  - Network
KW  - Energy utilization
KW  - Energy conservation
KW  - Application scheduling
KW  - Energy
KW  - consumption
KW  - CPU
KW  - hard disk
KW  - Middleware
KW  - network
KW  - Scheduling algorithms
KW  - prototype
KW  - simulator
KW  - Centralised
KW  - Computer hardware
KW  - Energy saving methods
KW  - Simulators
KW  - Peer to peer networks
KW  - Consumption
KW  - Hard disc
KW  - Prototype
ER  - 

TY  - CONF
TI  - PowerJoular and JoularJX: Multi-Platform Software Power Monitoring Tools
AU  - Noureddine, A.
T2  - 2022 18th International Conference on Intelligent Environments, IE 2022 - Proceedings
AB  - Monitoring the power consumption of applications and source code is an important step in writing green software. In this paper, we propose PowerJoular and JoularJX, our software power monitoring tools. We aim to help software developers in understanding and analyzing the power consumption of their programs, and help system administrators and automated tools in monitoring the power consumption of large numbers of heterogeneous devices.  © 2022 IEEE.
DA  - 2022///
PY  - 2022
DO  - 10.1109/IE54923.2022.9826760
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136040656&doi=10.1109%2fIE54923.2022.9826760&partnerID=40&md5=5b3a65ad6dd9d140eb3a7edfaf97c36f
DB  - Scopus
KW  - Application programs
KW  - Measurement
KW  - Electric power utilization
KW  - System administrators
KW  - Software developer
KW  - Monitoring tools
KW  - Energy analysis
KW  - Power Consumption
KW  - Source codes
KW  - Application codes
KW  - Power monitoring
KW  - Energy Analysis
KW  - Help systems
KW  - Multi-platform
KW  - Power Monitoring
KW  - Program systems
ER  - 

TY  - JOUR
TI  - A Proposal of Behavior-Based Consumption Profiles for Green Software Design
AU  - Larracoechea, J.A.
AU  - Ilarri, S.
AU  - Roose, P.
T2  - Applied Sciences (Switzerland)
AB  - Despite the increase in the efficiency of energy consumption in information and communication technology, software execution and its constraints are responsible for how energy is consumed in hardware hosts. Consequently, researchers have promoted the development of sustainable software with new development methods and tools to lessen its hardware demands. However, the approaches developed so far lack cohesiveness along the stages of the software development life cycle (SDLC) and exist outside of a holistic method for green software development (GSD). In addition, there is a severe lack of approaches that target the analysis and design stages of the SDLC, leaving software architects and designers unsupported. In this article, we introduce our behavior-based consumption profile (BBCP) external Domain-Specific Language (DSL), aimed at assisting software architects and designers in modeling the behavior of software. The models generated with our external DSL contain multiple sets of properties that characterize features of the software’s behavior. In contrast to other modeling languages, our BBCP emphasizes how time and probability are involved in software execution and its evolution over time, helping its users to gather an expectation of software usage and hardware consumption from the initial stages of software development. To illustrate the feasibility and benefits of our proposal, we conclude with an analysis of the model of a software service created using the BBCP, which is simulated using Insight Maker to obtain an estimation of hardware consumption and later translated to energy consumption. © 2024 by the authors.
DA  - 2024///
PY  - 2024
DO  - 10.3390/app14177456
VL  - 14
IS  - 17
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203852902&doi=10.3390%2fapp14177456&partnerID=40&md5=764572cb721393b46c5d6d5cbbf1be5c
DB  - Scopus
KW  - software engineering
KW  - Software design
KW  - Green software
KW  - behavior with software
KW  - Behavior with software
KW  - Behavior-based
KW  - Energy-consumption
KW  - Green development
KW  - green software
KW  - Program translators
KW  - SOA
KW  - Software architects
KW  - software behavior
KW  - Software behavior
KW  - Software development life-cycle
KW  - Software execution
KW  - software profiling
KW  - Software profiling
ER  - 

TY  - JOUR
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218978256&partnerID=40&md5=b0a71f39575d3a14932bf1bb0c1f0cab
DB  - Scopus
ER  - 

TY  - CONF
TI  - Towards Services Profiling for Energy Management in Service-oriented Architectures
AU  - Larracoechea, J.A.
AU  - Roose, P.
AU  - Ilarri, S.
AU  - Cardinale, Y.
AU  - Laborie, S.
AU  - González, M.J.
T2  - International Conference on Web Information Systems and Technologies, WEBIST - Proceedings
AB  - Even though hardware architects have managed to incrementally mitigate energy consumption in information and communication technology devices, it will always be a requisite for software execution. This has motivated researchers to develop a limited amount of methodologies that promote green software development and its philosophy, with new assessment methods for calculating the energetic costs of software development and software execution. In spite of this, they have been acknowledged and adopted with limited success, as they try to address highly-volatile variables (like human behavior) and environments with specific hardware/software platforms and language-centric solutions. This has created a conflict between theory and practice where, otherwise, a generic and adaptive approach could manage the discord. In this paper, we present a brief review of available selected research in relation to services' requirements definition and profiling for energy management, as well as the limitations and advantages of existing proposals in relation to green software development. Furthermore, we present our progress towards a series of properties to define services' requirements and their resource consumption behavior. Our final goal is to create a proper approach for energy management from the analysis and design phases of the Software Development Life Cycle using Service- Oriented Architectures as a platform for our work. © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.
DA  - 2021///
PY  - 2021
VL  - 2021-October
SP  - 209
EP  - 216
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146200750&partnerID=40&md5=95366b38f0d161158badb32d4e8a0817
DB  - Scopus
KW  - Software design
KW  - Information and Communication Technologies
KW  - Green software
KW  - Energy-consumption
KW  - Software execution
KW  - Green computing
KW  - Energy utilization
KW  - Information management
KW  - Service oriented architecture (SOA)
KW  - Information services
KW  - Service requirements
KW  - ICT
KW  - Computation theory
KW  - Energy management
KW  - Life cycle
KW  - Behavioral research
KW  - Energy Management
KW  - Mobile-computing
KW  - Energetic costs
KW  - Green Software
KW  - Human environment
KW  - Mobile Computing
KW  - Service-oriented Architecture
KW  - Soa (serviceoriented architecture)
ER  - 

TY  - JOUR
T2  - PowerJoular
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218973261&partnerID=40&md5=1bf964a5f4f416faec8ff38cdba4f43c
DB  - Scopus
ER  - 

TY  - JOUR
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218919452&partnerID=40&md5=e34b3ece610bb510ecc5288285293f3d
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Visualize - CodeCarbon documentation
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199360300&partnerID=40&md5=80d1416c8c15151a5d097d19a4224563
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Calculating My Carbon Footprint - Microsoft Sustainability
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199386338&partnerID=40&md5=baada82c0562424570efa26d3379de99
DB  - Scopus
ER  - 

TY  - CONF
TI  - GreenMiner: A hardware based mining software repositories software energy consumption framework
AU  - Hindle, A.
AU  - Wilson, A.
AU  - Rasmussen, K.
AU  - Barlow, E.J.
AU  - Campbell, J.C.
AU  - Romansky, S.
T2  - 11th Working Conference on Mining Software Repositories, MSR 2014 - Proceedings
AB  - Green Mining is a field of MSR that studies software energy consumption and relies on software performance data. Unfortunately there is a severe lack of publicly available software power use performance data. This means that green mining researchers must generate this data themselves by writing tests, building multiple revisions of a product, and then running these tests multiple times (10+) for each software revision while measuring power use. Then, they must aggregate these measurements to estimate the energy consumed by the tests for each software revision. This is time consuming and is made more difficult by the constraints of mobile devices and their OSes. In this paper we propose, implement, and demonstrate Green Miner: the first dedicated hardware mining software repositories testbed. The Green Miner physically measures the energy consumption of mobile devices (Android phones) and automates the testing of applications, and the reporting of measurements back to developers and researchers. The Green Miner has already produced valuable results for commercial Android application developers, and has been shown to replicate other power studies' results. Copyright is held by the author/owner(s). Publication rights licensed to ACM.
DA  - 2014///
PY  - 2014
DO  - 10.1145/2597073.2597097
SP  - 12
EP  - 21
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938780683&doi=10.1145%2f2597073.2597097&partnerID=40&md5=c9e2899691445bb728bdb85a9db1a8dd
DB  - Scopus
KW  - Android
KW  - Software testing
KW  - Energy utilization
KW  - Software performance
KW  - Dedicated hardware
KW  - Android applications
KW  - Software energy consumption
KW  - Android (operating system)
KW  - Miners
KW  - Mining software repositories
KW  - Software change
KW  - Software revisions
ER  - 

TY  - JOUR
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218974675&partnerID=40&md5=3b3225e5da3c52a2135fca7e19eda144
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Behavior-Based Consumption Profiles for the Approximation of the Energy Consumption of Services
AU  - Larracoechea, J.
AU  - Roose, P.
AU  - Ilarri, S.
AU  - Cardinale, Y.
AU  - Laborie, S.
AU  - Vara, O.
T2  - Proceedings of the International Conference on Information Systems Development (ISD)
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203860904&partnerID=40&md5=2e7376a66b60abe8388dda1642faa0d9
DB  - Scopus
ER  - 

TY  - JOUR
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218931022&partnerID=40&md5=3cf04f0a58d5edf9f3903830ea018e2d
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Methodology - Cloud Carbon Footprint
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199349781&partnerID=40&md5=44706c59696e4284af4dc56a76c4b2d0
DB  - Scopus
ER  - 

TY  - CONF
TI  - Awakening awareness on energy consumption in software engineering
AU  - Jagroep, E.
AU  - Broekman, J.
AU  - Van Der Werf, J.M.E.M.
AU  - Brinkkemper, S.
AU  - Lago, P.
AU  - Blom, L.
AU  - Van Vliet, R.
T2  - Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Society Track, ICSE-SEIS 2017
AB  - Software producing organizations have the ability to address the energy impact of their ICT solutions during the development process. However, while industry is convinced of the energy impact of hardware, the role of software has mostly been acknowledged by researchers in software engineering. Strengthened by the limited practical knowledge to reduce the energy consumption, organizations have less control over the energy impact of their products and lose the contribution of software towards energy related strategies. Consequently, industry risks not being able to meet customer requirements or even fulfillcorporate sustainability goals. In this paper we perform an exploratory case study on how to create and maintain awareness on an energy consumption perspective for software among stakeholders involved with the development of software products. During the study, we followed the development process of two commercial software products and provided direct feedback to the stakeholders on the effects of their development efforts, specifically concerning energy consumption and performance, using an energy dashboard. Multiple awareness measurements allowed us to keep track of changes over time on specific aspects affecting software development. Our results show that, despite a mixed sentiment towards the dashboard, changed awareness has triggered discussion on the energy consumption of software. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/ICSE-SEIS.2017.10
SP  - 76
EP  - 85
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025819050&doi=10.1109%2fICSE-SEIS.2017.10&partnerID=40&md5=331c876afe2f7c9e9fe8285c0525c478
DB  - Scopus
KW  - Software design
KW  - Software engineering
KW  - Energy utilization
KW  - Software products
KW  - Development process
KW  - Software energy consumption
KW  - Awareness
KW  - Awareness measurement
KW  - Commercial software products
KW  - Customer requirements
KW  - Energy consumption perspective
KW  - Exploratory case studies
ER  - 

TY  - JOUR
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218940533&partnerID=40&md5=3b5d0748ff8c9ef0857c4cf03268ab0f
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Mlco2/codecarbon
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218932193&partnerID=40&md5=4a1183dfb641a7fc4acba8c2eb6c91aa
DB  - Scopus
ER  - 

TY  - JOUR
T2  - Overview | Cloud Carbon Footprint
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218923535&partnerID=40&md5=2f85ad18d1ed42455434d7e2c8e14fab
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Electricity Consumption by ICT: Facts, trends, and measurements
AU  - Gelenbe, E.
T2  - Ubiquity
DA  - 2023///
PY  - 2023
VL  - 2023
IS  - August
SP  - 1
EP  - 15
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197041776&partnerID=40&md5=7348dc3b4c10032368d4ce69c820cc01
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Towards an Integrated Full-Stack Green Software Development Methodology
AU  - Roose, P.
AU  - Sergio, I.
AU  - Larracoechea, J.A.
AU  - Cardinale, Y.
AU  - Laborie, S.
T2  - Proceedings of the 29th International Conference on Information Systems Development
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203850899&partnerID=40&md5=36c6f68557686700d736a2330696cd51
DB  - Scopus
ER  - 

TY  - JOUR
TI  - What Do Programmers Know about Software Energy Consumption?
AU  - Pang, C.
AU  - Hindle, A.
AU  - Adams, B.
AU  - Hassan, A.E.
T2  - IEEE Software
AB  - Traditionally, programmers received a range of training on programming languages and methodologies, but they rarely receive training on software energy consumption. Yet, the popularity of mobile devices and cloud computing requires increased awareness of software energy consumption. On mobile devices, battery life often limits computation. Under the demands of cloud computing, datacenters struggle to reduce energy consumption through virtualization and datacenter-infrastructure-management systems. Efficient software energy consumption is increasingly becoming an important nonfunctional requirement for programmers. However, are programmers knowledgeable enough about software energy consumption? Do they base their implementation decision on popular beliefs? Researchers surveyed more than 100 programmers regarding their knowledge of software energy consumption. They found that the programmers had limited knowledge of energy efficiency, lacked knowledge of the best practices to reduce software energy consumption, and were often unsure about how software consumes energy. These results highlight the need for better training and education on energy consumption and efficiency. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/MS.2015.83
VL  - 33
IS  - 3
SP  - 83
EP  - 89
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968902060&doi=10.1109%2fMS.2015.83&partnerID=40&md5=783696f1bd63fb00affeec5dfd446470
DB  - Scopus
KW  - Cloud computing
KW  - energy efficiency
KW  - Energy efficiency
KW  - Energy utilization
KW  - Distributed computer systems
KW  - Best practices
KW  - Reduce energy consumption
KW  - Virtualizations
KW  - Power usage
KW  - Non-functional requirements
KW  - Mobile devices
KW  - Software energy consumption
KW  - Infrastructure management system
KW  - power usage
KW  - software energy consumption
KW  - software power consumption
KW  - Training and education
ER  - 

TY  - JOUR
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218969261&partnerID=40&md5=57894daa490d623cac1c7abe9ed9c6eb
DB  - Scopus
ER  - 

TY  - CONF
TI  - SEEDS: A software engineer's energy-optimization decision support framework
AU  - Manotas, I.
AU  - Pollock, L.
AU  - Clause, J.
T2  - Proceedings - International Conference on Software Engineering
AB  - Reducing the energy usage of software is becoming more important in many environments, in particular, battery-powered mobile devices, embedded systems and data centers. Recent empirical studies indicate that software engineers can support the goal of reducing energy usage by making design and implementation decisions in ways that take into consideration how such decisions impact the energy usage of an application. However, the large number of possible choices and the lack of feedback and information available to software engineers necessitates some form of automated decision-making support. This paper describes the first known automated support for systematically optimizing the energy usage of applications by making code-level changes. It is effective at reducing energy usage while freeing developers from needing to deal with the low-level, tedious tasks of applying changes and monitoring the resulting impacts to the energy usage of their application. We present a general framework, SEEDS, as well as an instantiation of the framework that automatically optimizes Java applications by selecting the most energy-efficient library implementations for Java's Collections API. Our empirical evaluation of the framework and instantiation show that it is possible to improve the energy usage of an application in a fully automated manner for a reasonable cost. © 2014 ACM.
DA  - 2014///
PY  - 2014
DO  - 10.1145/2568225.2568297
SP  - 503
EP  - 514
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994139877&doi=10.1145%2f2568225.2568297&partnerID=40&md5=21c1add538c0945f0fe450fe640a2dea
DB  - Scopus
KW  - Energy efficiency
KW  - Automation
KW  - Software engineering
KW  - Decision making
KW  - Application programs
KW  - Design and implementations
KW  - Java programming language
KW  - Energy optimization
KW  - Embedded systems
KW  - Mobile devices
KW  - Energy usage
KW  - Empirical evaluations
KW  - Software optimization
KW  - Decision support systems
KW  - Automated decision making
KW  - analysis framework
KW  - Analysis frameworks
KW  - Decision support framework
KW  - Engineers
KW  - software optimization
ER  - 

TY  - CONF
TI  - Kepler: A Framework to Calculate the Energy Consumption of Containerized Applications
AU  - Amaral, M.
AU  - Chen, H.
AU  - Chiba, T.
AU  - Nakazawa, R.
AU  - Choochotkaew, S.
AU  - Lee, E.K.
AU  - Eilam, T.
T2  - IEEE International Conference on Cloud Computing, CLOUD
AB  - Energy accounting is crucial in data centers for optimizing power provisioning, capping, and tuning. This paper introduces the Kepler framework, which estimates power consumption at the process, container, and Kubernetes pod levels. Kepler offers a set of power models applicable to various architectures and metrics. In this study, we propose a generic power model that utilizes hardware counters (HC) and realtime system power metrics (e.g., running average power limit (RAPL)) as independent variables in a regression model. Unlike previous approaches that rely on aggregate power consumption, our methodology measures individual process power consumption to train the power model. We provide step-by-step instructions to measure process power consumption in a controlled environment, considering the activation constant and load-dependent dynamic power consumption in different executions. By following the Greenhouse Gas (GHG) Protocol, our approach ensures the fair distribution of constant power among the user's processes. The results demonstrate significantly improved accuracy with a mean squared error (MSE) as low as 0.010 for the proposed method, compared with an MSE of 0.16 for a simple ratio approach and 0.92 when training the model using aggregated workload power. © 2023 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/CLOUD60044.2023.00017
VL  - 2023-July
SP  - 69
EP  - 71
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174254307&doi=10.1109%2fCLOUD60044.2023.00017&partnerID=40&md5=ad9611593dafbcf59292d9d1201e3478
DB  - Scopus
KW  - kubernetes
KW  - Kubernetes
KW  - Containers
KW  - Sustainable development
KW  - Greenhouse gases
KW  - Energy-consumption
KW  - Green computing
KW  - Sustainability
KW  - Power
KW  - Electric power utilization
KW  - Regression analysis
KW  - Average power limit
KW  - Power modeling
KW  - RAPL
KW  - eBPF
KW  - Energy accounting
KW  - container power modeling
KW  - Container power modeling
KW  - EBPF
KW  - Electric power distribution
KW  - energy accounting
KW  - Mean square error
KW  - Mean squared error
KW  - Running average power limit
ER  - 

TY  - JOUR
AU  - Acar, H.
T2  - Software Development Methodology in a Green IT Environment
DA  - 2017///
PY  - 2017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087679908&partnerID=40&md5=9c871a65a452a417239820c89f8861b9
DB  - Scopus
ER  - 

TY  - JOUR
T2  - The green web foundation/green-cost-explorer
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218957202&partnerID=40&md5=1fd19e3bd3dee5dcd36e755ab0affe33
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Automated test-based learning and verification of performance models for microservices systems
AU  - Camilli, M.
AU  - Janes, A.
AU  - Russo, B.
T2  - Journal of Systems and Software
AB  - Effective and automated verification techniques able to provide assurances of performance and scalability are highly demanded in the context of microservices systems. In this paper, we introduce a methodology that applies specification-driven load testing to learn the behavior of the target microservices system under multiple deployment configurations. Testing is driven by realistic workload conditions sampled in production. The sampling produces a formal description of the users’ behavior through a Discrete Time Markov Chain. This model drives multiple load testing sessions that query the system under test and feed a Bayesian inference process which incrementally refines the initial model to obtain a complete specification from run-time evidence as a Continuous Time Markov Chain. The complete specification is then used to conduct automated verification by using probabilistic model checking and to compute a configuration score that evaluates alternative deployment options. This paper introduces the methodology, its theoretical foundation, and the toolchain we developed to automate it. Our empirical evaluation shows its applicability, benefits, and costs on a representative microservices system benchmark. We show that the methodology detects performance issues, traces them back to system-level requirements, and, thanks to the configuration score, provides engineers with insights on deployment options. The comparison between our approach and a selected state-of-the-art baseline shows that we are able to reduce the cost up to 73% in terms of number of tests. The verification stage requires negligible execution time and memory consumption. We observed that the verification of 360 system-level requirements took ∼1 minute by consuming at most 34 KB. The computation of the score involved the verification of ∼7k (automatically generated) properties verified in ∼72 seconds using at most ∼50 KB. © 2022 The Author(s)
DA  - 2022///
PY  - 2022
DO  - 10.1016/j.jss.2022.111225
VL  - 187
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124274286&doi=10.1016%2fj.jss.2022.111225&partnerID=40&md5=5f4a1691c78038edaca0ea24e420f6f2
DB  - Scopus
KW  - Microservices
KW  - Automation
KW  - Microservice
KW  - Markov processes
KW  - Learning systems
KW  - Model checking
KW  - Continuous time systems
KW  - Performance Modeling
KW  - Specifications
KW  - Inference engines
KW  - Bayesian networks
KW  - Load testing
KW  - Automated test
KW  - Automated verification
KW  - Markov modeling
KW  - Markov models
KW  - Model learning
KW  - Performance testing
KW  - Statistical tests
KW  - System-level requirements
KW  - Test-based model learning
KW  - Verification techniques
ER  - 

TY  - CONF
TI  - Microservices: A performance tester's dream or nightmare?
AU  - Eismann, S.
AU  - Bezemer, C.-P.
AU  - Shang, W.
AU  - Okanović, D.
AU  - Van Hoorn, A.
T2  - ICPE 2020 - Proceedings of the ACM/SPEC International Conference on Performance Engineering
AB  - In recent years, there has been a shift in software development towards microservice-based architectures, which consist of small services that focus on one particular functionality. Many companies are migrating their applications to such architectures to reap the benefits of microservices, such as increased flexibility, scalability and a smaller granularity of the offered functionality by a service. On the one hand, the benefits of microservices for functional testing are often praised, as the focus on one functionality and their smaller granularity allow for more targeted and more convenient testing. On the other hand, using microservices has their consequences (both positive and negative) on other types of testing, such as performance testing. Performance testing is traditionally done by establishing the baseline performance of a software version, which is then used to compare the performance testing results of later software versions. However, as we show in this paper, establishing such a baseline performance is challenging in microservice applications. In this paper, we discuss the benefits and challenges of microservices from a performance tester's point of view. Through a series of experiments on the TeaStore application, we demonstrate how microservices affect the performance testing process, and we demonstrate that it is not straightforward to achieve reliable performance testing results for a microservice application. © 2020 ACM.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3358960.3379124
SP  - 138
EP  - 149
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085919253&doi=10.1145%2f3358960.3379124&partnerID=40&md5=455a9f26d3dbf08578fe068636b995e1
DB  - Scopus
KW  - Microservices
KW  - DevOps
KW  - Performance
KW  - Software design
KW  - Software testing
KW  - Performance testing
KW  - Base-line performance
KW  - Functional testing
KW  - Increased flexibility
KW  - Regression testing
KW  - Reliable performance
KW  - Software versions
ER  - 

TY  - CONF
TI  - Workload characterization for microservices
AU  - Ueda, T.
AU  - Nakaike, T.
AU  - Ohara, M.
T2  - Proceedings of the 2016 IEEE International Symposium on Workload Characterization, IISWC 2016
AB  - The microservice architecture is a new framework to construct a Web service as a collection of small services that communicate with each other. It is becoming increasingly popular because it can accelerate agile software development, deployment, and operation practices. As a result, cloud service providers are expected to host an increasing number of microservices that can generate significant resource pressure on the cloud infrastructure. We want to understand the characteristics of microservice workloads to design an infrastructure optimized for microservices. In this paper, we used Acme Air, an open-source benchmark for Web services, and analyzed the behavior of two versions of the benchmark, microservice and monolithic, for two widely used language runtimes, Node.js and Java. We observed a significant overhead due to the microservice architecture; the performance of the microservice version can be 79.2% lower than the monolithic version on the same hardware configuration. On Node.js, the microservice version consumed 4.22 times more time in the libraries of Node.js than the monolithic version to process one user request. On Java, the microservice version also consumed more time in the application server than the monolithic version. We explain these performance differences from both hardware and software perspectives. We discuss the network virtualization in Docker, an infrastructure for microservices that has nonnegligible impact on performance. These findings give clues to develop optimization techniques in a language runtime and hardware for microservice workloads. © 2016 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/IISWC.2016.7581269
SP  - 85
EP  - 94
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994792149&doi=10.1109%2fIISWC.2016.7581269&partnerID=40&md5=4d5176224c8f26bb71dad3e667e46e66
DB  - Scopus
KW  - Microservices
KW  - Container
KW  - Containers
KW  - Microservice architecture
KW  - Docker
KW  - Software design
KW  - Software engineering
KW  - Hardware
KW  - Network architecture
KW  - Web services
KW  - Java
KW  - Computer hardware
KW  - Open source software
KW  - Websites
KW  - Java programming language
KW  - Characterization
KW  - Reconfigurable hardware
KW  - Node.js
KW  - WebSphere Liberty
ER  - 

TY  - JOUR
TI  - A view of cloud computing
AU  - Armbrust, M.
AU  - Fox, A.
AU  - Griffith, R.
AU  - Joseph, A.D.
AU  - Katz, R.
AU  - Konwinski, A.
AU  - Lee, G.
AU  - Patterson, D.
AU  - Rabkin, A.
AU  - Stoica, I.
AU  - Zaharia, M.
T2  - Communications of the ACM
AB  - CLOUD COMPUTING, the long-held dream of computing as a utility, has the potential to transform a large part of the IT industry, making software even more attractive as a service and shaping the way IT hardware is designed and purchased. Developers with innovative ideas for new Internet services no longer require the large capital outlays in hardware to deploy their service or the human expense to operate it. They need not be concerned about overprovisioning for a service whose popularity does not meet their predictions, thus wasting costly resources, or underprovisioning for one that becomes wildly popular, thus missing potential customers and revenue. Moreover, companies with large batch-oriented tasks can get results as quickly as their programs can scale, since using 1,000 servers for one hour costs no more than using one server for 1,000. © 2010 ACM.
DA  - 2010///
PY  - 2010
DO  - 10.1145/1721654.1721672
VL  - 53
IS  - 4
SP  - 50
EP  - 58
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950347409&doi=10.1145%2f1721654.1721672&partnerID=40&md5=6164f58679f057d5164d1f8f31d3f125
DB  - Scopus
KW  - Cloud computing
KW  - Servers
KW  - Capital outlay
KW  - Innovative ideas
KW  - Internet
KW  - Internet services
KW  - IT industry
KW  - Large parts
KW  - Potential customers
ER  - 

TY  - JOUR
TI  - Software architecture: Perspectives on an emerging discipline
AU  - Shaw, M.
AU  - Garlan, D.
T2  - Pearson Us Imports & PHIPEs
DA  - 1996///
PY  - 1996
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893250361&partnerID=40&md5=f9d571027f167ace2e5ddb70ec2cb7e5
DB  - Scopus
ER  - 

TY  - JOUR
TI  - From Monolithic to Microservices: An Experience Report from the Banking Domain
AU  - Bucchiarone, A.
AU  - Dragoni, N.
AU  - Dustdar, S.
AU  - Larsen, S.T.
AU  - Mazzara, M.
T2  - IEEE Software
AB  - Microservices have seen their popularity blossoming with an explosion of concrete applications in real-life software. Several companies are currently involved in a major refactoring of their back-end systems in order to improve scalability. This article presents an experience report of a real-world case study, from the banking domain, in order to demonstrate how scalability is positively affected by reimplementing a monolithic architecture into microservices. The case study is based on the FX Core system for converting from one currency to another. FX Core is a mission-critical system of Danske Bank, the largest bank in Denmark and one of the leading financial institutions in Northern Europe. © 1984-2012 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/MS.2018.2141026
VL  - 35
IS  - 3
SP  - 50
EP  - 55
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046855472&doi=10.1109%2fMS.2018.2141026&partnerID=40&md5=d07fd2c213749afdf2f2fc900fa8adc4
DB  - Scopus
KW  - microservices
KW  - Microservices
KW  - scalability
KW  - software architecture
KW  - software engineering
KW  - Software architecture
KW  - Scalability
KW  - Software design
KW  - Software engineering
KW  - Application programs
KW  - software development
KW  - Financial institution
KW  - Mission critical systems
KW  - Monolithic architecture
KW  - Concrete applications
KW  - Danske Bank
KW  - Experience report
KW  - FX Core
ER  - 

TY  - CONF
TI  - Migrating Monolithic Mobile Application to Microservice Architecture: An Experiment Report
AU  - Fan, C.-Y.
AU  - Ma, S.-P.
T2  - Proceedings - 2017 IEEE 6th International Conference on AI and Mobile Services, AIMS 2017
AB  - The microservice architecture (MSA) is an emerging cloud software system, which provides fine-grained, self-contained service components (microservices) used in the construction of complex software systems. DevOps techniques are commonly used to automate the process of development and operation through continuous integration and continuous deployment. Monitoring software systems created by DevOps, makes it possible for MSA to obtain the feedback necessary to improve the system quickly and easily. Nonetheless, systematic, SDLC-driven methods (SDLC: software development life cycle) are lacking to facilitate the migration of software systems from a traditional monolithic architecture to MSA. Therefore, this paper proposes a migration process based on SDLC, including all of the methods and tools required during design, development, and implementation. The mobile application, EasyLearn, was used as an illustrative example to demonstrate the efficacy of the proposed migration process. We believe that this paper could provide valuable references for other development teams seeking to facilitate the migration of existing applications to MSA. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/AIMS.2017.23
SP  - 109
EP  - 112
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032264331&doi=10.1109%2fAIMS.2017.23&partnerID=40&md5=ab113a2950bd047d43bece5f8a51f8d4
DB  - Scopus
KW  - Microservice
KW  - Microservice architecture
KW  - Software design
KW  - Software engineering
KW  - Mobile computing
KW  - Migration
KW  - Continuous integrations
KW  - Mobile applications
KW  - Life cycle
KW  - Computer software
KW  - Mobile telecommunication systems
KW  - Monolithic architecture
KW  - Development and operations
KW  - Complex software systems
KW  - Software development life cycle
ER  - 

TY  - JOUR
TI  - Mass produced software components
AU  - McIlroy, M.D.
T2  - Software Engineering
DA  - 1968///
PY  - 1968
SP  - 138
EP  - 155
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002099505&partnerID=40&md5=d4e1e673f968392ab038c58c3fd8096e
DB  - Scopus
ER  - 

TY  - JOUR
AU  - Newman, S.
T2  - Monolith to Microservices: Evolutionary Patterns to Transform Your Monolith
DA  - 2019///
PY  - 2019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085744999&partnerID=40&md5=dc75f6331fe52756b87a41af7426af17
DB  - Scopus
ER  - 

TY  - CONF
TI  - Extraction of Microservices from Monolithic Software Architectures
AU  - Mazlami, G.
AU  - Cito, J.
AU  - Leitner, P.
T2  - Proceedings - 2017 IEEE 24th International Conference on Web Services, ICWS 2017
AB  - Driven by developments such as mobile computing, cloud computing infrastructure, DevOps and elastic computing, the microservice architectural style has emerged as a new alternative to the monolithic style for designing large software systems. Monolithic legacy applications in industry undergo a migration to microservice-oriented architectures. A key challenge in this context is the extraction of microservices from existing monolithic code bases. While informal migration patterns and techniques exist, there is a lack of formal models and automated support tools in that area. This paper tackles that challenge by presenting a formal microservice extraction model to allow algorithmic recommendation of microservice candidates in a refactoring and migration scenario. The formal model is implemented in a web-based prototype. A performance evaluation demonstrates that the presented approach provides adequate performance. The recommendation quality is evaluated quantitatively by custom microservice-specific metrics. The results show that the produced microservice candidates lower the average development team size down to half of the original size or lower. Furthermore, the size of recommended microservice conforms with microservice sizing reported by empirical surveys and the domain-specific redundancy among different microservices is kept at a low rate. © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/ICWS.2017.61
SP  - 524
EP  - 531
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032372980&doi=10.1109%2fICWS.2017.61&partnerID=40&md5=76deccb11538edbd7866455f17f937cf
DB  - Scopus
KW  - microservices
KW  - Distributed computer systems
KW  - Quality control
KW  - Web services
KW  - Architectural style
KW  - Legacy applications
KW  - Websites
KW  - Graphic methods
KW  - Extraction
KW  - Formal methods
KW  - Cloud computing infrastructures
KW  - Migration patterns
KW  - Large software systems
KW  - coupling
KW  - Couplings
KW  - extraction
KW  - graph-based clustering
KW  - Graph-based clustering
KW  - Web-based prototype
ER  - 

TY  - JOUR
TI  - Circuit breakers, discovery, and API gateways in microservices
AU  - Montesi, F.
AU  - Weber, J.
T2  - Circuit Breakers, Discovery, and API Gateways in Microservices
DA  - 2016///
PY  - 2016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020536128&partnerID=40&md5=27b5165dbed31f0c460fe5d1d6ee6e64
DB  - Scopus
ER  - 

TY  - JOUR
TI  - A Survey on Load Testing of Large-Scale Software Systems
AU  - Jiang, Z.M.
AU  - Hassan, A.E.
T2  - IEEE Transactions on Software Engineering
AB  - Many large-scale software systems must service thousands or millions of concurrent requests. These systems must be load tested to ensure that they can function correctly under load (i.e., the rate of the incoming requests). In this paper, we survey the state of load testing research and practice. We compare and contrast current techniques that are used in the three phases of a load test: (1) designing a proper load, (2) executing a load test, and (3) analyzing the results of a load test. This survey will be useful for load testing practitioners and software engineering researchers with interest in the load testing of large-scale software systems. © 2015 IEEE.
DA  - 2015///
PY  - 2015
DO  - 10.1109/TSE.2015.2445340
VL  - 41
IS  - 11
SP  - 1091
EP  - 1118
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961770650&doi=10.1109%2fTSE.2015.2445340&partnerID=40&md5=9f8caec0055ae5f398b9c5177af0751b
DB  - Scopus
KW  - survey
KW  - Software engineering
KW  - Software testing
KW  - Surveys
KW  - Computer software
KW  - Surveying
KW  - Load testing
KW  - Computer software selection and evaluation
KW  - software quality
KW  - Software Quality
KW  - Concurrent requests
KW  - large-scale software systems
KW  - Large-scale software systems
KW  - load testing
KW  - Three phasis
KW  - Under loads
ER  - 

TY  - JOUR
TI  - Defining the application performance index
AU  - Sevcik, P.
T2  - Business Communications Review
DA  - 2005///
PY  - 2005
VL  - 20
SP  - 8
EP  - 10
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-74549130451&partnerID=40&md5=94525950f83a09dc3c014e666b40b896
DB  - Scopus
ER  - 

TY  - JOUR
AU  - Beyer, B.
AU  - Murphy, N.R.
AU  - Rensin, D.K.
AU  - Kawahara, K.
AU  - Thorne, S.
T2  - The Site Reliability Workbook: Practical Ways to Implement SRE
DA  - 2018///
PY  - 2018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059833122&partnerID=40&md5=1c1f88f43b69e1feae072b440ff93275
DB  - Scopus
ER  - 

TY  - JOUR
AU  - Finnigan, K.
T2  - Enterprise Java Microservices
DA  - 2018///
PY  - 2018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080616922&partnerID=40&md5=822f83c2c0a97c265329a42c1325de60
DB  - Scopus
ER  - 

TY  - CONF
TI  - Sagas
AU  - Garcia-Molina, H.
AU  - Salem, K.
T2  - Proceedings of the ACM SIGMOD International Conference on Management of Data
AB  - Long lived transactions (LLTs) hold on to database resources for relatively long periods of time, significantly delaying the termination of shorter and more common transactions. To alleviate these problems we propose the notion of a saga. A LLT is a saga if it can be written as a sequence of transactions that can be interleaved with other transactions. The database management system guarantees that either all the transactions in a saga are successfully completed or compensating transactions are run to amend a partial execution. Both the concept of saga and its implementation are relatively simple, but they have the potential to improve performance significantly. We analyze the various implementation issues related to sagas, including how they can be run on an existing system that does not directly support them. We also discuss techniques for database and LLT design that make it feasible to break up LLTs into sagas. © 1987 ACM.
DA  - 1987///
PY  - 1987
DO  - 10.1145/38713.38742
SP  - 249
EP  - 259
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899882908&doi=10.1145%2f38713.38742&partnerID=40&md5=0cc78c81d986553a91ca13780f9c899b
DB  - Scopus
KW  - Simple++
KW  - Database systems
KW  - Existing systems
KW  - Break-up
KW  - Improve performance
KW  - Partial executions
KW  - Transaction design
ER  - 

TY  - JOUR
TI  - Microservices patterns: With examples in Java
AU  - Richardson, C.
T2  - Simon and Schuster
DA  - 2018///
PY  - 2018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122634128&partnerID=40&md5=5f491e2324dd6b42da804b1c24efc666
DB  - Scopus
ER  - 

TY  - JOUR
TI  - Scalability Assessment of Microservice Architecture Deployment Configurations: A Domain-based Approach Leveraging Operational Profiles and Load Tests
AU  - Avritzer, A.
AU  - Ferme, V.
AU  - Janes, A.
AU  - Russo, B.
AU  - Hoorn, A.V.
AU  - Schulz, H.
AU  - Menasché, D.
AU  - Rufino, V.
T2  - Journal of Systems and Software
AB  - Microservices have emerged as an architectural style for developing distributed applications. Assessing the performance of architecture deployment configurations — e.g., with respect to deployment alternatives — is challenging and must be aligned with the system usage in the production environment. In this paper, we introduce an approach for using operational profiles to generate load tests to automatically assess scalability pass/fail criteria of microservice configuration alternatives. The approach provides a Domain-based metric for each alternative that can, for instance, be applied to make informed decisions about the selection of alternatives and to conduct production monitoring regarding performance-related system properties, e.g., anomaly detection. We have evaluated our approach using extensive experiments in a large bare metal host environment and a virtualized environment. First, the data presented in this paper supports the need to carefully evaluate the impact of increasing the level of computing resources on performance. Specifically, for the experiments presented in this paper, we observed that the evaluated Domain-based metric is a non-increasing function of the number of CPU resources for one of the environments under study. In a subsequent series of experiments, we investigate the application of the approach to assess the impact of security attacks on the performance of architecture deployment configurations. © 2020
DA  - 2020///
PY  - 2020
DO  - 10.1016/j.jss.2020.110564
VL  - 165
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080916968&doi=10.1016%2fj.jss.2020.110564&partnerID=40&md5=c88d1e94be06f1c2496ff9e6c0f0d2c0
DB  - Scopus
KW  - Anomaly detection
KW  - Scalability
KW  - Architecture
KW  - Distributed applications
KW  - Virtual reality
KW  - Virtualized environment
KW  - Architectural style
KW  - Computing resource
KW  - Production environments
KW  - Load testing
KW  - Increasing functions
KW  - Operational profile
KW  - Production monitoring
ER  - 

TY  - CONF
TI  - Characteristics of scalability and their impact on performance
AU  - Bondi, A.B.
T2  - Proceedings Second International Workshop on Software and Performance WOSP 2000
AB  - Scalability is a desirable attribute of a network, system, or process. Poor scalability can result in poor system performance, necessitating the reengineering or duplication of systems. While scalability is valued, its characteristics and the characteristics that undermine it are usually only apparent from the context. Here, we attempt to define different aspects of scalability, such as structural scalability and load scalability. Structural scalability is the ability of a system to expand in a chosen dimension without major modifications to its architecture. Load scalability is the ability of a system to perform gracefully as the offered traffic increases. It is argued that systems with poor load scalability may exhibit it because they repeatedly engage in wasteful activity, because they are encumbered with poor scheduling algorithms, because they cannot fully take advantage of parallelism, or because they are algorithmically inefficient. We qualitatively illustrate these concepts with classical examples from the literature of operating systems and local area networks, as well as an example of our own. Some of these are accompanied by rudimentary delay analysis.
DA  - 2000///
PY  - 2000
DO  - 10.1145/350391.350432
SP  - 195
EP  - 203
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034592897&doi=10.1145%2f350391.350432&partnerID=40&md5=711bb72b0448ee75a205d3ad50fa4fe5
DB  - Scopus
KW  - Performance
KW  - Algorithms
KW  - Computer networks
KW  - Computer operating systems
KW  - Parallel processing systems
KW  - Data structures
KW  - Load scalability
KW  - Structural and space scalability
ER  - 

TY  - JOUR
AU  - Lewis, J.
AU  - Fowler, M.
T2  - Richardson Maturity Model
DA  - 2014///
PY  - 2014
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210251216&partnerID=40&md5=979f8116dbbb317bdf356751c3348e58
DB  - Scopus
ER  - 

TY  - JOUR
AU  - Newman, S.
T2  - Building Microservices
DA  - 2015///
PY  - 2015
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950338538&partnerID=40&md5=aec25db8f81564a4ab82f370c5e620cc
DB  - Scopus
ER  - 

TY  - CONF
TI  - Performance modeling for cloud microservice applications
AU  - Jindal, A.
AU  - Podolskiy, V.
AU  - Gerndt, M.
T2  - ICPE 2019 - Proceedings of the 2019 ACM/SPEC International Conference on Performance Engineering
AB  - Microservices enable a fine-grained control over the cloud applications that they constitute and thus became widely-used in the industry. Each microservice implements its own functionality and communicates with other microservices through language- and platform-agnostic API. The resources usage of microservices varies depending on the implemented functionality and the workload. Continuously increasing load or a sudden load spike may yield a violation of a service level objective (SLO). To characterize the behavior of a microservice application which is appropriate for the user, we define a MicroService Capacity (MSC) as a maximal rate of requests that can be served without violating SLO. The paper addresses the challenge of identifying MSC individually for each microservice. Finding individual capacities of microservices ensures the flexibility of the capacity planning for an application. This challenge is addressed by sandboxing a microservice and building its performance model. This approach was implemented in a tool Terminus. The tool estimates the capacity of a microservice on different deployment configurations by conducting a limited set of load tests followed by fitting an appropriate regression model to the acquired performance data. The evaluation of the microservice performance models on microservices of four different applications shown relatively accurate predictions with mean absolute percentage error (MAPE) less than 10%. The results of the proposed performance modeling for individual microservices are deemed as a major input for the microservice application performance modeling. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.
DA  - 2019///
PY  - 2019
DO  - 10.1145/3297663.3310309
SP  - 25
EP  - 32
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064819069&doi=10.1145%2f3297663.3310309&partnerID=40&md5=bd1cc7d512919b877b7240a6ffc95011
DB  - Scopus
KW  - Kubernetes
KW  - Regression analysis
KW  - Performance modeling
KW  - Service level objective
KW  - Application performance
KW  - Performance Model
KW  - Load testing
KW  - Accurate prediction
KW  - Fine-grained control
KW  - Mean absolute percentage error
KW  - Microservice capacity
ER  - 

TY  - CONF
TI  - Monolith to Microservice Candidates using Business Functionality Inference
AU  - Agarwal, S.
AU  - Sinha, R.
AU  - Sridhara, G.
AU  - Das, P.
AU  - Desai, U.
AU  - Tamilselvam, S.
AU  - Singhee, A.
AU  - Nakamuro, H.
T2  - Proceedings - 2021 IEEE International Conference on Web Services, ICWS 2021
AB  - In this paper, we propose a novel approach for monolith decomposition, that maps the implementation structure of a monolith application to a functional structure that in turn can be mapped to business functionality. First, we infer the classes in the monolith application that are distinctively representative of the business functionality in the application domain. This is done using formal concept analysis on statically determined code flow structures in a completely automated manner. Then, we apply a clustering technique, guided by the inferred representatives, on the classes belonging to the monolith to group them into different types of partitions, mainly: 1) functional groups representing microservice candidates, 2) a utility class group, and 3) a group of classes that require significant refactoring to enable a clean microservice architecture. This results in microservice candidates that are naturally aligned with the different business functions exposed by the application. A detailed evaluation on four publicly available applications show that our approach is able to determine better quality microservice candidates when compared to other existing state of the art techniques. We also conclusively show that clustering quality metrics like modularity are not reliable indicators of microservice candidate goodness. © 2021 IEEE.
DA  - 2021///
PY  - 2021
DO  - 10.1109/ICWS53863.2021.00104
SP  - 758
EP  - 763
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123207360&doi=10.1109%2fICWS53863.2021.00104&partnerID=40&md5=57a101ce3da88b221b96c8e85427fb50
DB  - Scopus
KW  - Quality control
KW  - Business function inference
KW  - Business Function Inference
KW  - Business functionality
KW  - Business Functionality
KW  - Business functions
KW  - Concept lattice
KW  - Concept Lattices
KW  - Domain entities
KW  - Domain Entity
KW  - Entrypoint specification
KW  - EntryPoint Specification
KW  - Formal concept analysis
KW  - Function inference
KW  - Information analysis
KW  - Microservice recommendation
KW  - Mono2micro
KW  - Mono2Micro
KW  - Monolith decomposition
KW  - Monolith Decomposition
KW  - Seed expansion
KW  - Seed Expansion
ER  - 

TY  - CONF
TI  - ShellOnYou: Learning by Doing Unix Command Line
AU  - Berry, V.
AU  - Castelltort, A.
AU  - Pelissier, C.
AU  - Rousseau, M.
AU  - Tibermacine, C.
T2  - Annual Conference on Innovation and Technology in Computer Science Education, ITiCSE
AB  - We present ShellOnYou, a new Computer Science education tool, and analyze its use with four successive student cohorts. Developed to help instructors manage numerous students, this web application offers auto-graded exercises to acquire practical knowledge of Unix-like operating systems from the command line. For each answer, and almost instantly, students receive a score and detailed feedback. This reactive and iterative process encourages students to resubmit answers and progressively expand their procedural knowledge. The tool can also deliver individualized statements, thereby allowing students to improve their skills by combining personal research and peer learning. As an online tool, ShellOnYou affords students access flexibility, and also easily fits in distance learning programs. We found it particularly useful when teaching students with heterogeneous Unix backgrounds. The tool is available on request. We placed four successive student cohorts in a learning situation involving this tool, and asked them to fill a survey at the end of the learning period. We combine qualitative and quantitative methods to analyze their answers to the survey. We attempt to characterize their acquisition of procedural knowledge and the building of group dynamics. Several dimensions emerge: the benefits of using the tool in a learning situation, the learning process iteration as a catalyst for renewed commitment, the tool's entertaining format and its scoring system as a motivation for regular studying, and the inherent customization of the learning pace.  © 2022 ACM.
DA  - 2022///
PY  - 2022
DO  - 10.1145/3502718.3524753
VL  - 1
SP  - 379
EP  - 385
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134432493&doi=10.1145%2f3502718.3524753&partnerID=40&md5=5cdc5a9a2e8f84c9d90822bab80f9d4e
DB  - Scopus
KW  - Learning systems
KW  - Surveys
KW  - Iterative methods
KW  - Engineering education
KW  - Command line
KW  - autograder
KW  - Autograder
KW  - automated feedback
KW  - Automated feedback
KW  - Education computing
KW  - learning by doing
KW  - Learning situation
KW  - Learning-by-doing
KW  - practical knowledge
KW  - Practical knowledge
KW  - Procedural knowledge
KW  - Student perceptions
KW  - Students
KW  - students' perception
KW  - UNIX command
KW  - unix command line interface
KW  - Unix command line interface
ER  - 

TY  - JOUR
TI  - JavaSpaces: Principles, patterns, and practice
AU  - Freeman, E.
AU  - Hupfer, S.
AU  - Arnold, K.
T2  - JavaSpaces Principles, Patterns, and Practice
DA  - 1999///
PY  - 1999
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003400120&partnerID=40&md5=e8ca496fcaf57ed2ed7b6db66250191c
DB  - Scopus
ER  - 

TY  - CONF
TI  - From monolith to microservices: Lessons learned on an industrial migration to a web oriented architecture
AU  - Gouigoux, J.-P.
AU  - Tamzalit, D.
T2  - Proceedings - 2017 IEEE International Conference on Software Architecture Workshops, ICSAW 2017: Side Track Proceedings
AB  - MGDIS SA is a software editing company that underwent a major strategic and technical change during the past three years, investing 17 300 man. Days rewriting its core business software from monolithic architecture to a Web Oriented Architecture using microservices. The paper presents technical lessons learned during and from this migration by addressing three crucial questions for a successful context-adapted migration towards a Web Oriented Architecture: how to determine (i) the most suitable granularity of micro-services, (ii) the most appropriate deployment and (iii) the most efficient orchestration? © 2017 IEEE.
DA  - 2017///
PY  - 2017
DO  - 10.1109/ICSAW.2017.35
SP  - 62
EP  - 65
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025592012&doi=10.1109%2fICSAW.2017.35&partnerID=40&md5=5f601d6922482948f79fa4c45f45d76d
DB  - Scopus
KW  - Microservices
KW  - Software architecture
KW  - Migration
KW  - Monolithic architecture
KW  - Core business
KW  - Technical change
KW  - Web Oriented Architecture
ER  - 

TY  - JOUR
AU  - Bass, L.
AU  - Clements, P.
AU  - Kazman, R.
T2  - Software Architecture in Practice
DA  - 2003///
PY  - 2003
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0004025223&partnerID=40&md5=5033f9948ca9ba9361258afb3c821863
DB  - Scopus
ER  - 

TY  - CONF
TI  - Equivalence-enhanced microservice workflow orchestration to efficiently increase reliability
AU  - Song, Z.
AU  - Tilevich, E.
T2  - Proceedings - 2019 IEEE International Conference on Web Services, ICWS 2019 - Part of the 2019 IEEE World Congress on Services
AB  - The applicability of the microservice architecture has extended beyond traditional web services, making steady inroads into the domains of IoT and edge computing. Due to dissimilar contexts in different execution environments and inherent mobility, edge and IoT applications suffer from low execution reliability. Replication, traditionally used to increase service reliability and scalability, is inapplicable in these resource-scarce environments. Alternately, programmers can orchestrate the parallel or sequential execution of equivalent microservices-microservices that provide the same functionality by different means. Unfortunately, the resulting orchestrations rely on parallelization, synchronization, and failure handing, all tedious and error-prone to implement. Although automated orchestration shifts the burden of generating workflows from the programmer to the compiler, existing programming models lack both syntactic and semantic support for equivalence. In this paper, we enhance compiler-generated execution orchestration with equivalence to efficiently increase reliability. We introduce a dataflow-based domain-specific language, whose dataflow specifications include the implicit declarations of equivalent microservices and their execution patterns. To automatically generate reliable workflows and execute them efficiently, we introduce new equivalence workflow constructs. Our evaluation results indicate that our solution can effectively and efficiently increase the reliability of microservice-based applications. © 2019 IEEE.
DA  - 2019///
PY  - 2019
DO  - 10.1109/ICWS.2019.00076
SP  - 426
EP  - 433
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072786259&doi=10.1109%2fICWS.2019.00076&partnerID=40&md5=279088611b89191dec2f2432e661d111
DB  - Scopus
KW  - Reliability
KW  - Microservice
KW  - Internet of things
KW  - Quality of service
KW  - Domain specific languages
KW  - Web services
KW  - Semantics
KW  - Execution environments
KW  - Service reliability
KW  - Websites
KW  - Data flow analysis
KW  - Program compilers
KW  - Problem oriented languages
KW  - Dataflow specifications
KW  - Functional equivalence
KW  - Sequential execution
KW  - Workflow orchestration
ER  - 

TY  - CONF
TI  - On the Maintenance of a Scientific Application based on Microservices: An Experience Report
AU  - Tizzei, L.P.
AU  - Azevedo, L.
AU  - Soares, E.
AU  - Thiago, R.
AU  - Costa, R.
T2  - Proceedings - 2020 IEEE 13th International Conference on Web Services, ICWS 2020
AB  - Microservices Architecture has been adopted by several companies to develop applications and replace monolithic ones. Several works point out that this approach supports the design of maintainable software systems. However, none of them presents a quantitative empirical study on the extent of the maintenance support in a real-world application. This work assesses how Microservices Architecture supports software maintenance through an empirical quantitative study of a scientific application built from scratch. We collected data from January 2016 (the beginning of the project) to December 2019, and analyzed 19 microservices, 34 repositories, and 15,408 commits. Then, we present the lessons learned during the project that allowed reaching the assessment results. Our findings may assist practitioners in making architectural decisions and pointing out research opportunities for academics.  © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ICWS49710.2020.00021
SP  - 102
EP  - 109
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099276336&doi=10.1109%2fICWS49710.2020.00021&partnerID=40&md5=192c6da76cc1563b84c48016e06e588e
DB  - Scopus
KW  - microservices
KW  - software architecture
KW  - Application programs
KW  - Web services
KW  - Software systems
KW  - Maintenance
KW  - Websites
KW  - software evolution
KW  - Scientific applications
KW  - Research opportunities
KW  - Architectural decision
KW  - Experience report
KW  - Maintenance supports
KW  - Quantitative empirical studies
KW  - Quantitative study
KW  - scientific software
KW  - software maintenance
ER  - 

TY  - CONF
TI  - Performance evaluation of microservices architectures using containers
AU  - Amaral, M.
AU  - Polo, J.
AU  - Carrera, D.
AU  - Mohomed, I.
AU  - Unuvar, M.
AU  - Steinder, M.
T2  - Proceedings - 2015 IEEE 14th International Symposium on Network Computing and Applications, NCA 2015
AB  - Micro services architecture has started a new trend for application development for a number of reasons: (1) to reduce complexity by using tiny services, (2) to scale, remove and deploy parts of the system easily, (3) to improve flexibility to use different frameworks and tools, (4) to increase the overall scalability, and (5) to improve the resilience of the system. Containers have empowered the usage of micro services architectures by being lightweight, providing fast start-up times, and having a low overhead. Containers can be used to develop applications based on monolithic architectures where the whole system runs inside a single container or inside a micro services architecture where one or few processes run inside the containers. Two models can be used to implement a micro services architecture using containers: master-slave, or nested-container. The goal of this work is to compare the performance of CPU and network running benchmarks in the two aforementioned models of micro services architecture hence provide a benchmark analysis guidance for system designers. © 2015 IEEE.
DA  - 2016///
PY  - 2016
DO  - 10.1109/NCA.2015.49
SP  - 27
EP  - 34
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963600252&doi=10.1109%2fNCA.2015.49&partnerID=40&md5=6c7edd04e0d0c3f878d5a3fd4d126855
DB  - Scopus
KW  - Microservices
KW  - Containers
KW  - Computer architecture
KW  - Network architecture
KW  - Benchmarking
KW  - Complex networks
KW  - Application development
KW  - Fast start-up
KW  - Monolithic architecture
KW  - System designers
KW  - Benchmark analysis
KW  - Networking
KW  - Performance Evaluation
ER  - 

TY  - CONF
TI  - Monolith Development History for Microservices Identification: a Comparative Analysis
AU  - Lourenco, J.
AU  - Silva, A.R.
T2  - Proceedings - 2023 IEEE International Conference on Web Services, ICWS 2023
AB  - Recent research has proposed different approaches on the automated identification of candidate microservices on monolith systems, which vary on the monolith representation, similarity criteria, and quality metrics used. On the other hand, they are generally limited in the number of codebases and decompositions evaluated, and few comparisons between approaches exist. Considering the emerging trend in software engineering in techniques based on the analysis of codebases' evolution, we compare a representation based on the monolith code structure, in particular the sequences of accesses to domain entities, with representations based on the monolith development history (file changes and changes authorship). From the analysis on a total of 468k decompositions of 28 codebases, using five quality metrics that evaluate modularity, minimization of the number of transactions per functionality, and reduction of teams and communication, we conclude that the changes authorship representation of codebases with many authors achieves comparable or better results than the sequences of accesses representation of codebases with few authors with respect to minimization of the number of transactions per functionality and the reduction of teams. © 2023 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/ICWS60048.2023.00019
SP  - 50
EP  - 56
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173810444&doi=10.1109%2fICWS60048.2023.00019&partnerID=40&md5=36c5ba01812a0afa8826638be566112a
DB  - Scopus
KW  - Microservices
KW  - Microservice
KW  - Software engineering
KW  - Quality control
KW  - % reductions
KW  - Minimisation
KW  - Architecture migration
KW  - Architecture Migration
KW  - Comparative analyzes
KW  - Development history
KW  - Microservice identification
KW  - Microservices Identification
KW  - Monolith
KW  - Quality metrices
KW  - Repository mining
KW  - Repository Mining
ER  - 

TY  - CONF
TI  - Formal Analysis of Load Balancing in Microservices with Scenario Calculus
AU  - Zhu, H.
AU  - Wang, H.
AU  - Bayley, I.
T2  - IEEE International Conference on Cloud Computing, CLOUD
AB  - Load balancing plays a crucial role in realising the benefits of microservices, especially to achieve elastic scalability and performance optimisation. However, it is different from load balancing for virtual machines, because workloads on microservices are harder to predict and the number of services in the systems is large. In this paper, we formalise load balance as an emergent property of the microservices ecosystem, and employ scenario calculus to formally analyse the impact of scheduling on service capability and scalability. We discovered that elastic round robin scheduling is highly scalable but the service capability is limited by the slowest microservice instance. In contrast, shortest waiting queue scheduling is less scalable, but the service capability is higher. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/CLOUD.2018.00133
VL  - 2018-July
SP  - 908
EP  - 911
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057467049&doi=10.1109%2fCLOUD.2018.00133&partnerID=40&md5=8eb2fad15017aa7eaf34f5d6e8ac532b
DB  - Scopus
KW  - Cloud computing
KW  - Microservices
KW  - Scalability
KW  - Scheduling
KW  - Balancing
KW  - Load balance
KW  - Scheduling algorithms
KW  - Scheduling policies
KW  - Calculations
KW  - Scenario calculus
KW  - Service capability
ER  - 

TY  - CONF
TI  - EvolutionSim: An Extensible Simulation Toolkit for Microservice System Evolution
AU  - Wang, T.
AU  - He, X.
AU  - Shi, H.
AU  - Wang, Z.
T2  - Proceedings - 2023 IEEE International Conference on Web Services, ICWS 2023
AB  - Recently, microservices architecture has become the mainstream software development and deployment architecture for most enterprises, with the advantages of continuous delivery/deployment. However, the inability of microservices systems to meet changing user requirements and other external factors can lead to a degradation of quality of service (QoS), necessitating microservice evolution to ensure QoS stability by adjusting the deployment structure and configuration of microservices through evolutionary means such as redeployment. In order to study different evolutionary problems, simulation of microservice evolution is critical because the flexibility and diversity of the simulation environment can provide a more prosperous experimental environment for the researchers involved compared single physical experiments and numerical experiments. However, existing simulators have limitations when it comes to simulating microservice evolution. They either lack the capability to support the simulation of microservice evolution or only provide simulation for specific evolutionary means, which limits their extensibility. To address this issue, this paper designs and implements a simulation toolkit EvolutionSim for MSS evolution based on discrete event. It aimes to simulate the running and evolution process of MSSs with the support of several evolutionary means. In addition, Experiments were carried out to compare the simulated results with the actual results, and the distribution was found to be similar by ADF test, which indicates that EvolutionSim can provide valid results. Moreover, three experimental scenarios were conducted to simulate three mainstream evolutionary means, and the results indicate that EvolutionSim can accurately simulate the effects of different evolutionary means on MSS. © 2023 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/ICWS60048.2023.00018
SP  - 43
EP  - 49
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173848072&doi=10.1109%2fICWS60048.2023.00018&partnerID=40&md5=70f4834f2a76ac9f9fe9be1ae10f98d2
DB  - Scopus
KW  - cloud computing
KW  - simulation
KW  - Microservice system
KW  - Simulation
KW  - evolution
KW  - Software design
KW  - Quality of service
KW  - Cloud-computing
KW  - Quality-of-service
KW  - Satellites
KW  - Deployment architecture
KW  - Evolution
KW  - microservice modeling
KW  - Microservice modeling
KW  - microservice system
KW  - Simulation toolkits
KW  - System evolution
KW  - User requirements
ER  - 

TY  - JOUR
TI  - Understanding and addressing quality attributes of microservices architecture: A Systematic literature review
AU  - Li, S.
AU  - Zhang, H.
AU  - Jia, Z.
AU  - Zhong, C.
AU  - Zhang, C.
AU  - Shan, Z.
AU  - Shen, J.
AU  - Babar, M.A.
T2  - Information and Software Technology
AB  - Context: As a rapidly adopted architectural style in software engineering, Microservices Architecture (MSA) advocates implementing small-scale and independently distributed services, rather than binding all functions into one monolith. Although many initiatives have contributed to the quality improvement of microservices-based systems, there is still a lack of a systematic understanding of the Quality Attributes (QAs) associated with MSA. Objective: This study aims to investigate the evidence-based state-of-the-art of QAs of microservices-based systems. Method: We carried out a Systematic Literature Review (SLR) to identify and synthesize the relevant studies that report evidence related to QAs of MSA. Results: Based on the data extracted from the 72 selected primary studies, we portray an overview of the six identified QAs most concerned in MSA, scalability, performance, availability, monitorability, security, and testability. We identify 19 tactics that architecturally address the critical QAs in MSA, including two tactics for scalability, four for performance, four for availability, four for monitorability, three for security, and two for testability. Conclusion: This SLR concludes that for MSA-based systems: 1) Although scalability is the commonly acknowledged benefit of MSA, it is still an indispensable concern among the identified QAs, especially when trading-off with other QAs, e.g., performance. Apart from the six identified QAs in this study, other QAs for MSA like maintainability need more attention for effective improvement and evaluation in the future. 3) Practitioners need to carefully make the decision of migrating to MSA based on the return on investment, since this architectural style additionally cause some pains in practice. © 2020
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.infsof.2020.106449
VL  - 131
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097144108&doi=10.1016%2fj.infsof.2020.106449&partnerID=40&md5=12a7fa386b4fdce447ce3bfda55806de
DB  - Scopus
KW  - Microservices
KW  - Scalability
KW  - Software engineering
KW  - Architecture
KW  - State of the art
KW  - Systematic literature review
KW  - Architectural style
KW  - Systematic literature review (SLR)
KW  - Distributed service
KW  - Quality attributes
KW  - Monolith
KW  - Evidence-based
KW  - Quality improvement
ER  - 

TY  - JOUR
TI  - A Survey on Microservices Architecture: Principles, Patterns and Migration Challenges
AU  - Velepucha, V.
AU  - Flores, P.
T2  - IEEE Access
AB  - Microservices architecture is a new trend embraced by many organizations as a way to modernize their legacy applications. However, although there is work related to the migration process, there is a gap in the body of knowledge related to the principles they should adopt when implementing a microservices architecture. This work presents a comprehensive survey, gathering literature that explores the fundamental principles underlying the object-oriented approach and how these concepts are related to monolithic and microservices architectures. In addition, our research encompasses both monolithic architectures and microservices, along with an investigation into the design patterns and principles utilized within microservices. Our contribution is present a list of patterns used in microservices architecture, the comparation between the principles expounded by the experts in the decomposition of microservices architectures, Martin Fowler and Sam Neuman, and the forerunner of the Principle of Information Hiding, David Parnas, who discusses modularization as a mechanism to improve flexibility and understanding of a system. Additionally, we expose the advantages and disadvantages of monolithic and microservices architectures obtained from the literature review carried out in summary form, which can help as a reference for researchers from academia and industry and finally reveal the trends of microservices architectures today. © 2013 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/ACCESS.2023.3305687
VL  - 11
SP  - 88339
EP  - 88358
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168269805&doi=10.1109%2fACCESS.2023.3305687&partnerID=40&md5=6a49ba1cdabc871eb5aa5bb54da30852
DB  - Scopus
KW  - Microservices
KW  - patterns
KW  - Microservice
KW  - Software architecture
KW  - Microservice architecture
KW  - migration
KW  - Software
KW  - Migration
KW  - Complexity theory
KW  - Modular construction
KW  - Behavioral research
KW  - Monolithics
KW  - Object oriented programming
KW  - Decomposition
KW  - Pattern
KW  - Behavioral science
KW  - decomposition
KW  - monolithic
KW  - Objectoriented programming (OOP)
KW  - Principle
KW  - principles
ER  - 

TY  - CONF
TI  - Testing microservices architecture-based applications: A systematic mapping study
AU  - Waseem, M.
AU  - Liang, P.
AU  - Marquez, G.
AU  - Salle, A.D.
T2  - Proceedings - Asia-Pacific Software Engineering Conference, APSEC
AB  - Microservices is an architectural style that provides several benefits to develop applications as small, independent, and modular services. Building Microservices Architecture (MSA)-based applications is immensely supported by using software testing fundamentals. With the increasing interest in the development of MSA-based applications, it is important to systematically identify, analyze, and classify the publication trends, research themes, approaches, tools, and challenges in the context of testing MSA-based applications. The search yielded 2, 481 articles, and 33 articles were finally selected as the primary studies with snowballing. The key findings are that (i) 5 research themes characterize testing approaches in MSA-based applications; (ii) integration and unit testing are the most popular testing approaches; and (iii) addressing the challenges in automated and inter-communication testing is gaining the interest of the community. Additionally, it emerges that there is a lack of dedicated tools to support testing for MSA-based applications, and the reasons and solutions behind the challenges in testing MSA-based applications need to be further explored.  © 2020 IEEE.
DA  - 2020///
PY  - 2020
DO  - 10.1109/APSEC51365.2020.00020
VL  - 2020-December
SP  - 119
EP  - 128
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102372417&doi=10.1109%2fAPSEC51365.2020.00020&partnerID=40&md5=073007603c5ccc92e3b41f66c95a5980
DB  - Scopus
KW  - Microservices
KW  - Application programs
KW  - Testing
KW  - Architectural style
KW  - Systematic mapping studies
KW  - Architecture-based
KW  - Dedicated tools
KW  - Integration testing
KW  - Inter-communication
KW  - Microservices Architecture based Application
KW  - Support testing
KW  - Systematic Mapping Study
KW  - Unit testing
ER  - 

TY  - JOUR
TI  - On the criteria to be used in decomposing systems into modules
AU  - Parnas, D.L.
T2  - Communications of the ACM
AB  - This paper discusses modularization as a mechanism for improving the flexibility and comprehensibility of a system while allowing the shortening of its development time. The effectiveness of a “modularization” is dependent upon the criteria used in dividing the system into modules. A system design problem is presented and both a conventional and unconventional decomposition are described. It is shown that the unconventional decompositions have distinct advantages for the goals outlined. The criteria used in arriving at the decompositions are discussed. The unconventional decomposition, if implemented with the conventional assumption that a module consists of one or more subroutines, will be less efficient in most cases. An alternative approach to implementation which does not have this effect is sketched. © 1972, ACM. All rights reserved.
DA  - 1972///
PY  - 1972
DO  - 10.1145/361598.361623
VL  - 15
IS  - 12
SP  - 1053
EP  - 1058
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0015482049&doi=10.1145%2f361598.361623&partnerID=40&md5=5ad203b48ffeba294b79df1fd0cbb8be
DB  - Scopus
KW  - software engineering
KW  - modularity
KW  - software
KW  - software design
KW  - COMPUTER PROGRAMMING
KW  - KWIC index
KW  - modules
ER  - 

TY  - CONF
TI  - A Comparative Review of Microservices and Monolithic Architectures
AU  - Al-Debagy, O.
AU  - Martinek, P.
T2  - 18th IEEE International Symposium on Computational Intelligence and Informatics, CINTI 2018 - Proceedings
AB  - Microservices' architecture is getting attention in the academic community and the industry, and mostly is compared with monolithic architecture. Plenty of the results of these research papers contradict each other regarding the performance of these architectures. Therefore, these two architectures are compared in this paper, and some specific configurations of microservices' applications are evaluated as well in the term of service discovery. Monolithic architecture in concurrency testing showed better performance in throughput by 6% when compared to microservices architecture. The load testing scenario did not present significant difference between the two architectures. Furthermore, a third test comparing microservices applications built with different service discovery technologies such as Consul and Eureka showed that applications with Consul presented better results in terms of throughput. © 2018 IEEE.
DA  - 2018///
PY  - 2018
DO  - 10.1109/CINTI.2018.8928192
SP  - 149
EP  - 154
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077783518&doi=10.1109%2fCINTI.2018.8928192&partnerID=40&md5=ee6c0f01b783ae6cb6b72fef1b67a020
DB  - Scopus
KW  - Artificial intelligence
KW  - Microservices Architecture
KW  - Architecture
KW  - Service discovery
KW  - Monolithic architecture
KW  - Research papers
KW  - Load testing
KW  - Performance Evaluation
KW  - Academic community
KW  - Different services
KW  - Monolithic Architecture
ER  - 

TY  - JOUR
TI  - Software reliability modeling based on ISO/IEC SQuaRE
AU  - Febrero, F.
AU  - Calero, C.
AU  - Moraga, M.Á.
T2  - Information and Software Technology
AB  - Context: The increasing dependence of our society on software driven systems has led Software Reliability to become a key factor as well as making it a highly active research area with hundreds of works being published every year. It would, however, appear that this activity is much more reduced as regards how to apply representative international standards on Product Quality to industrial environments, with just a few works on Standard Based software reliability modeling (SB-SRM). This is surprising given the relevance of such International Standards in industry. Objective: To identify and analyze the existing works on the modeling of Software Reliability based on International Standards as the starting point for a reliability assessment proposal based on ISO/IEC-25000 "Software Product Quality Requirements and Evaluation" (SQuaRE) series. Method: The work methodology is based on the guidelines provided in Evidence Based Software Engineering for Systematic Literature Reviews (SLR). Results: A total of 1820 works were obtained as a result of the SLR search, more than 800 primary studies were selected after data filtering. After scrutiny, over thirty of those were thoroughly analyze, the results obtained show a very limited application of SB-SRM particularly to industrial environment. Conclusion: Our analysis point to the complexity of the proposed models together with the difficulties involved in applying them to the management of engineering activities as a root cause to be considered for such limited application. The various stakeholder needs are also a point of paramount importance that should be better covered if the industrial applicability of the proposed models is to be increased. © 2015 Elsevier B.V. All rights reserved.
DA  - 2016///
PY  - 2016
DO  - 10.1016/j.infsof.2015.09.006
VL  - 70
SP  - 18
EP  - 29
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949431570&doi=10.1016%2fj.infsof.2015.09.006&partnerID=40&md5=58b53227f2f7b77494e50288eb98c3ad
DB  - Scopus
KW  - Reliability
KW  - Software quality
KW  - Software engineering
KW  - Quality control
KW  - Software reliability
KW  - Systematic literature review (SLR)
KW  - Standards
KW  - Computer software selection and evaluation
KW  - Software Quality
KW  - Evidence Based Software Engineering
KW  - Industrial environments
KW  - International standard
KW  - International standards
KW  - Software product quality
KW  - Software reliability modeling
KW  - Software reliability models
KW  - SQuaRE
ER  - 

TY  - CONF
TI  - Differences in performance, scalability, and cost of using microservice and monolithic architecture
AU  - Okrój, S.
AU  - Jatkiewicz, P.
T2  - Proceedings of the ACM Symposium on Applied Computing
AB  - A microservices-based architecture is a set of small components that communicate with each other using a programming language-independent API [1]. It has been gaining popularity for more than a decade. One of its advantages is greater agility in software development and following modern, agile software development practices [2]. The article presents an experimental study. Two applications with the same business logic and different architecture were developed. Both applications were tested using prepared test cases on the local computer of one of the authors and the Microsoft Azure platform. The results were collected and compared using the JMeter tool. In almost all cases, the monolithic architecture proved to be more efficient. The comparable performance of both architectures occurred when queries were handled by the business logic layer for a relatively long time.  © 2023 Owner/Author(s).
DA  - 2023///
PY  - 2023
DO  - 10.1145/3555776.3578725
SP  - 1038
EP  - 1041
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162891826&doi=10.1145%2f3555776.3578725&partnerID=40&md5=f9cca2c591f4f4c8e8befc72561fb71d
DB  - Scopus
KW  - cloud computing
KW  - Cloud computing
KW  - microservices
KW  - performance
KW  - Performance
KW  - scalability
KW  - software architecture
KW  - benchmarking
KW  - Microservice
KW  - Scalability
KW  - Software design
KW  - Cloud-computing
KW  - Windows operating system
KW  - Application programming interfaces (API)
KW  - Benchmarking
KW  - Computation theory
KW  - Monolithic architecture
KW  - Performance costs
KW  - Small components
KW  - Computer circuits
KW  - Monolith
KW  - azure
KW  - Azure
KW  - Language independents
KW  - monolith
KW  - Performance scalability
ER  - 

TY  - BOOK
TI  - Experimentation in software engineering
AU  - Wohlin, C.
AU  - Runeson, P.
AU  - Höst, M.
AU  - Ohlsson, M.C.
AU  - Regnell, B.
AU  - Wesslén, A.
T2  - Experimentation in Software Engineering
AB  - Like other sciences and engineering disciplines, software engineering requires a cycle of model building, experimentation, and learning. Experiments are valuable tools for all software engineers who are involved in evaluating and choosing between different methods, techniques, languages and tools. The purpose of Experimentation in Software Engineering is to introduce students, teachers, researchers, and practitioners to empirical studies in software engineering, using controlled experiments. The introduction to experimentation is provided through a process perspective, and the focus is on the steps that we have to go through to perform an experiment. The book is divided into three parts. The first part provides a background of theories and methods used in experimentation. Part II then devotes one chapter to each of the five experiment steps: scoping, planning, execution, analysis, and result presentation. Part III completes the presentation with two examples. Assignments and statistical material are provided in appendixes. Overall the book provides indispensable information regarding empirical studies in particular for experiments, but also for case studies, systematic literature reviews, and surveys. It is a revision of the authors' book, which was published in 2000. In addition, substantial new material, e.g. concerning systematic literature reviews and case study research, is introduced. The book is self-contained and it is suitable as a course book in undergraduate or graduate studies where the need for empirical studies in software engineering is stressed. Exercises and assignments are included to combine the more theoretical material with practical aspects. Researchers will also benefit from the book, learning more about how to conduct empirical studies, and likewise practitioners may use it as a "cookbook" when evaluating new methods or techniques before implementing them in their organization. © Springer-Verlag Berlin Heidelberg 2012. All rights are reserved.
DA  - 2012///
PY  - 2012
VL  - 9783642290442
SP  - 1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949178783&doi=10.1007%2f978-3-642-29044-2&partnerID=40&md5=22cfcf448329dc928057c3f13bb130a2
DB  - Scopus
KW  - Education
KW  - Software engineering
KW  - Case-studies
KW  - Systematic literature review
KW  - Engineering education
KW  - Empirical studies
KW  - Controlled experiment
KW  - Engineering disciplines
KW  - Case study research
KW  - Graduate studies
KW  - Reviews
KW  - Scoping
KW  - Teaching
ER  - 

TY  - CONF
TI  - Validity of the single processor approach to achieving large scale computing capabilities
AU  - Amdahl, G.M.
T2  - AFIPS Conference Proceedings - 1967 Spring Joint Computer Conference, AFIPS 1967
AB  - For over a decade prophets have voiced the contention that the organization of a single computer has reached its limits and that truly significant advances can be made only by interconnection of a multiplicity of computers in such a manner as to permit cooperative solution. Variously the proper direction has been pointed out as general purpose computers with a generalized interconnection of memories, or as specialized computers with geometrically related memory interconnections and controlled by one or more instruction streams. © 1967 ACM.
DA  - 1967///
PY  - 1967
DO  - 10.1145/1465482.1465560
SP  - 483
EP  - 485
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060036181&doi=10.1145%2f1465482.1465560&partnerID=40&md5=9ed463e7a30c37b9801df9f3dca544f6
DB  - Scopus
KW  - Computers
KW  - Computer science
KW  - Large-scale computing
KW  - General purpose computers
KW  - Instruction streams
KW  - Single computer
KW  - Single processors
ER  - 

