"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"CLPGTVR5","journalArticle","2025","Liu, Guozhi; Lin, Weiwei; Zhang, Haotong; Lin, Jianpeng; Peng, Shaoliang; Li, Keqin","Public Datasets for Cloud Computing: A Comprehensive Survey","ACM Comput. Surv.","","0360-0300","10.1145/3719003","https://doi.org/10.1145/3719003","Publicly available datasets are vital to researchers because they permit the testing of new algorithms under a variety of conditions and ensure the verifiability and reproducibility of scientific experiments. In cloud computing research, there is a particular dependence on obtaining load traces and network traces from real cloud computing clusters, which are used for designing energy efficiency prediction, workload analysis, and anomaly detection solutions. To address the current lack of a comprehensive overview and thorough analysis of cloud computing datasets and to gain insight into their current status and future trends, in this article, we provide a comprehensive survey of existing publicly cloud computing datasets. First, we utilize a systematic mapping approach to analyze 968 scientific papers from 6 scientific databases, resulting in the retrieval of 42 datasets related to cloud computing. Second, we categorize these datasets based on 11 characteristics to assist researchers in quickly finding datasets suitable for their specific needs. Third, we provide detailed descriptions of each dataset to assist researchers in gaining a clearer understanding of their characteristics. Fourth, we select 12 mainstream datasets and conduct a comprehensive analysis and comparison of their characteristics. Finally, we discuss the weaknesses of existing datasets, identify challenges, provide recommendations for long-term dataset maintenance and updates, and outline directions for the future creation of new cloud computing datasets. Related resources are available at .","2025-03","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","8","57","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","anomaly detection; Cloud computing; datasets; energy efficiency; resource allocation; security; task scheduling; workload","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IGK9Y42J","journalArticle","2022","Muralidhar, Rajeev; Borovica-Gajic, Renata; Buyya, Rajkumar","Energy Efficient Computing Systems: Architectures, Abstractions and Modeling to Techniques and Standards","ACM Comput. Surv.","","0360-0300","10.1145/3511094","https://doi.org/10.1145/3511094","Computing systems have undergone a tremendous change in the last few decades with several inflexion points. While Moore’s law guided the semiconductor industry to cram more and more transistors and logic into the same volume, the limits of instruction-level parallelism (ILP) and the end of Dennard’s scaling drove the industry towards multi-core chips. More recently, we have entered the era of domain-specific architectures (DSA) and chips for new workloads like artificial intelligence (AI) and machine learning (ML). These trends continue, arguably with other limits, along with challenges imposed by tighter integration, extreme form factors and increasingly diverse workloads, making systems more complex to architect, design, implement and optimize from an energy efficiency perspective. Energy efficiency has now become a first order design parameter and constraint across the entire spectrum of computing devices.Many research surveys have gone into different aspects of energy efficiency techniques implemented in hardware and microarchitecture across devices, servers, HPC/cloud, data center systems along with improved software, algorithms, frameworks, and modeling energy/thermals. Somewhat in parallel, the semiconductor industry has developed techniques and standards around specification, modeling/simulation, benchmarking and verification of complex chips; these areas have not been addressed in detail by previous research surveys. This survey aims to bring these domains holistically together, present the latest in each of these areas, highlight potential gaps and challenges, and discuss opportunities for the next generation of energy efficient systems. The survey is composed of a systematic categorization of key aspects of building energy efficient systems - (1) specification - the ability to precisely specify the power intent, attributes or properties at different layers (2) modeling and simulation of the entire system or subsystem (hardware or software or both) so as to be able to experiment with possible options and perform what-if analysis, (3) techniques used for implementing energy efficiency at different levels of the stack, (4) verification techniques used to provide guarantees that the functionality of complex designs are preserved, and (5) energy efficiency benchmarks, standards and consortiums that aim to standardize different aspects of energy efficiency, including cross-layer optimizations.","2022-09","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","11s","54","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","dynamic power management; Energy efficiency; low power; low power optimizations; modeling; platform-level power management; specification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PKWEARY4","journalArticle","2020","Brondolin, Rolando; Santambrogio, Marco D.","A Black-box Monitoring Approach to Measure Microservices Runtime Performance","ACM Trans. Archit. Code Optim.","","1544-3566","10.1145/3418899","https://doi.org/10.1145/3418899","Microservices changed cloud computing by moving the applications’ complexity from one monolithic executable to thousands of network interactions between small components. Given the increasing deployment sizes, the architectural exploitation challenges, and the impact on data-centers’ power consumption, we need to efficiently track this complexity. Within this article, we propose a black-box monitoring approach to track microservices at scale, focusing on architectural metrics, power consumption, application performance, and network performance. The proposed approach is transparent w.r.t. the monitored applications, generates less overhead w.r.t. black-box approaches available in the state-of-the-art, and provides fine-grain accurate metrics.","2020-11","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","4","17","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","cloud computing; docker; kubernetes; Microservices; network performance monitoring; performance monitoring; power attribution","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QNSXPFTY","journalArticle","2025","Yang, Yihong; Zhou, Zhangbing; Shu, Lei; Zhou, Feng; Gaaloul, Walid; Khan, Arif Ali","Web 3.0-Enabled Microservice Re-Scheduling for Heterogenous Resources Co-Optimization in Metaverse-Integrated Edge Networks","ACM Trans. Auton. Adapt. Syst.","","1556-4665","10.1145/3715700","https://doi.org/10.1145/3715700","The Web 3.0 and metaverse can empower intelligent application of Connected Autonomous Vehicles (CAVs). The adoption of edge computing can contribute to the low latency interaction between CAVs and the metaverse. Microservices are widely deployed on edge networks and the cloud nowadays. User's requests from CAVs are typically fulfilled through the composition of microservices, which may be hosted by contiguous edge nodes. Requests may differ on their required resources at runtime. Consequently, when requests are continuously injected into edge networks, the usage of heterogenous resources, including CPU, memory, and network bandwidth, may not be the same, or differ significantly, on certain edge nodes. This happens especially when burst requests are injected into the network to be satisfied concurrently. Therefore, the usage of heterogenous resources provided by edge nodes should be co-optimized through re-scheduling microservices. To address this challenge, this paper proposes a Web 3.0-enabled Microservice Re-Scheduling approach (called MRS), which is a migration-based mechanism integrating a placement strategy. Specifically, we formulate the microservice re-scheduling task as a multi-objective and multi-constraint optimization problem, which can be solved through a penalty signal-integrated framework and an improved pointer network. Extensive experiments are conducted on two real-world datasets. Evaluation results show that our MRS performs better than the counterparts with improvements of at least 7.7%, 2.4% and 2.2% in terms of network throughput, latency and energy consumption.","2025-01","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Connected Autonomous Vehicles (CAVs); Heterogenous Resources Co-Optimization; Metaverse; Microservice Re-Scheduling; Web 3.0","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IYCRJS2M","journalArticle","2020","Lin, Weiwei; Shi, Fang; Wu, Wentai; Li, Keqin; Wu, Guangxin; Mohammed, Al-Alas","A Taxonomy and Survey of Power Models and Power Modeling for Cloud Servers","ACM Comput. Surv.","","0360-0300","10.1145/3406208","https://doi.org/10.1145/3406208","Due to the increasing demand of cloud resources, the ever-increasing number and scale of cloud data centers make their massive power consumption a prominent issue today. Evidence reveals that the behaviors of cloud servers make the major impact on data centers’ power consumption. Although extensive research can be found in this context, a systematic review of the models and modeling methods for the entire hierarchy (from underlying hardware components to the upper-layer applications) of the cloud server is still missing, which is supposed to cover the relevant studies on physical and virtual cloud server instances, server components, and cloud applications. In this article, we summarize a broad range of relevant studies from three perspectives: power data acquisition, power models, and power modeling methods for cloud servers (including bare-metal, virtual machine (VM), and container instances). We present a comprehensive taxonomy on the collection methods of server-level power data, the existing mainstream power models at multiple levels from hardware to software and application, and commonly used methods for modeling power consumption including classical regression analysis and emerging methods like reinforcement learning. Throughout the work, we introduce a variety of models and methods, illustrating their implementation, usability, and applicability while discussing the limitations of existing approaches and possible ways of improvement. Apart from reviewing existing studies on server power models and modeling methods, we further figure out several open challenges and possible research directions, such as the study on modeling the power consumption of lightweight virtual units like unikernel and the necessity of further explorations toward empowering server power estimation/prediction with machine learning. As power monitoring is drawing increasing attention from cloud service providers (CSPs), this survey provides useful guidelines on server power modeling and can be inspiring for further research on energy-efficient data centers.","2020-09","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","5","53","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","data center; Cloud server; power consumption; power model; power modeling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MS3Q4GC8","journalArticle","2024","Antoniou, Georgia; Bartolini, Davide; Volos, Haris; Kleanthous, Marios; Wang, Zhe; Kalaitzidis, Kleovoulos; Rollet, Tom; Li, Ziwei; Mutlu, Onur; Sazeides, Yiannakis; Haj Yahya, Jawad","Agile C-states: A Core C-state Architecture for Latency Critical Applications Optimizing both Transition and Cold-Start Latency","ACM Trans. Archit. Code Optim.","","1544-3566","10.1145/3674734","https://doi.org/10.1145/3674734","Latency-critical applications running in modern datacenters exhibit irregular request arrival patterns and are implemented using multiple services with strict latency requirements (30–250μs). These characteristics render existing energy-saving idle CPU sleep states ineffective due to the performance overhead caused by the state’s transition latency. Besides the state transition latency, another important contributor to the performance overhead of sleep states is the cold-start latency, or in other words, the time required to warm up the microarchitectural state (e.g., cache contents, branch predictor metadata) that is flushed or discarded when transitioning to a lower-power state. Both the transition latency and cold-start latency can be particularly detrimental to the performance of latency critical applications with short execution times. While prior work focuses on mitigating the effects of transition and cold-start latency by optimizing request scheduling, in this work we propose a redesign of the core C-state architecture for latency-critical applications. In particular, we introduce C6Awarm, a new Agile core C-state that drastically reduces the performance overhead caused by idle sleep state transition latency and cold-start latency while maintaining significant energy savings. C6Awarm achieves its goals by (1) implementing medium-grained power gating, (2) preserving the microarchitectural state of the core, and (3) keeping the clock generator and PLL active and locked. Our analysis for a set of microservices based on an Intel Skylake server shows that C6Awarm manages to reduce the energy consumption by up to 70% with limited performance degradation (at most 2%).","2024-11","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","4","21","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","microservices; datacenters; Power management; C-states; idle states","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RW25QVPW","journalArticle","2024","Wang, Ziliang; Zhu, Shiyi; Li, Jianguo; Jiang, Wei; Ramakrishnan, K. K.; Yan, Meng; Zhang, Xiaohong; Liu, Alex X.","DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems","IEEE/ACM Trans. Netw.","","1063-6692","10.1109/TNET.2024.3400953","https://doi.org/10.1109/TNET.2024.3400953","Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.","2024-05","2025-03-18 09:07:49","2025-03-18 09:07:49","","3961–3976","","5","32","","","","","","","","","","","","","","","","","Publisher: IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GSHIS2V5","journalArticle","2024","Soussi, Wissem; Gür, Gürkan; Stiller, Burkhard","Democratizing Container Live Migration for Enhanced Future Networks - A Survey","ACM Comput. Surv.","","0360-0300","10.1145/3704436","https://doi.org/10.1145/3704436","Emerging cloud-centric networks span from edge clouds to large-scale datacenters with shared infrastructure among multiple tenants and applications with high availability, isolation, fault tolerance, security, and energy efficiency demands. Live migration (LiMi) plays an increasingly critical role in these environments by enabling seamless application mobility covering the edge-to-cloud continuum and maintaining these requirements. This paper presents a comprehensive survey of recent advancements that democratize LiMi, making it more applicable to a broader range of scenarios and network environments both for virtual machines (VMs) and containers, and analyzes LiMi’s technical underpinnings and optimization techniques. It also delves into the issue of connections handover, presenting a taxonomy to categorize methods of traffic redirection synthesized from the existing literature. Finally, it identifies technical challenges and paves the way for future research directions in this key technology.","2024-12","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","4","57","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","edge-to-cloud continuum; Moving Target Defense (MTD); Service live migration; traffic session handover","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LPQPYIHY","journalArticle","2023","Pallewatta, Samodha; Kostakos, Vassilis; Buyya, Rajkumar","Placement of Microservices-based IoT Applications in Fog Computing: A Taxonomy and Future Directions","ACM Comput. Surv.","","0360-0300","10.1145/3592598","https://doi.org/10.1145/3592598","The Fog computing paradigm utilises distributed, heterogeneous and resource-constrained devices at the edge of the network for efficient deployment of latency-critical and bandwidth-hungry IoT application services. Moreover, MicroService Architecture (MSA) is increasingly adopted to keep up with the rapid development and deployment needs of fast-evolving IoT applications. Due to the fine-grained modularity of the microservices and their independently deployable and scalable nature, MSA exhibits great potential in harnessing Fog and Cloud resources, thus giving rise to novel paradigms like Osmotic computing. The loosely coupled nature of the microservices, aided by the container orchestrators and service mesh technologies, enables the dynamic composition of distributed and scalable microservices to achieve diverse performance requirements of the IoT applications using distributed Fog resources. To this end, efficient placement of microservice plays a vital role, and scalable placement algorithms are required to utilise the said characteristics of the MSA while overcoming novel challenges introduced by the architecture. Thus, we present a comprehensive taxonomy of recent literature on microservices-based IoT applications placement within Fog computing environments. Furthermore, we organise multiple taxonomies to capture the main aspects of the placement problem, analyse and classify related works, identify research gaps within each category, and discuss future research directions.","2023-07","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","14s","55","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Fog computing; application placement; Internet of Things; microservice architecture; Osmotic computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E4Z2Q4SF","journalArticle","2024","Ghammam, Anwar; Khalsi, Rania; Kessentini, Marouane; Hassan, Foyzul","Efficient Management of Containers for Software Defined Vehicles","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3672461","https://doi.org/10.1145/3672461","Containerization technology, such as Docker, is gaining in popularity in newly established software-defined vehicle architectures (SDVA). However, executing those containers can quickly become computationally expensive in constrained environments, given the limited CPU, memory, and energy resources in the Electric Control Units (ECU) of SDVA. Consequently, the efficient management of these containers is crucial for enabling the on-demand usage of the applications in the vehicle based on the available resources while considering several constraints and priorities, including failure tolerance, security, safety, and comfort. In this article, we propose a dynamic software container management approach for constrained environments such as embedded devices/ECUs in SDVA within smart cars. To address the conflicting objectives and constraints within the vehicle, we design a novel search-based approach based on multi-objective optimization. This approach facilitates the allocation, movement, or suspension of containers between ECUs in the cluster. Collaborating with our industry partner, Ford Motor Company, we evaluate our approach using different real-world software-defined scenarios. These scenarios involve using heterogeneous clusters of ECU devices in vehicles based on real-world software containers and use-case studies from the automotive industry. The experimental results demonstrate that our scheduler outperforms existing scheduling algorithms, including the default Docker scheduler -Spread- commonly used in automotive applications. Our proposed scheduler exhibits superior performance in terms of energy and resource cost efficiency. Specifically, it achieves a 35% reduction in energy consumption in power-saving mode compared to the scheduler employed by Ford Motor Company. Additionally, our scheduler effectively distributes workload among the ECUs in the cluster, minimizing resource usage, and dynamically adjusts to the real-time requirements and constraints of the car environment. This work will serve as a fundamental building block in the automotive industry to efficiently manage software containers in smart vehicles, considering constraints and priorities in the real world.","2024-11","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","8","33","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","docker; Dynamic scheduling; management of software containers; many-objective optimization; software defined vehicles","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q3JSXAKA","journalArticle","2024","Min, Dongmoon; Byun, Ilkwon; Lee, Gyu-Hyeon; Kim, Jangwoo","CoolDC: A Cost-Effective Immersion-Cooled Datacenter with Workload-Aware Temperature Scaling","ACM Trans. Archit. Code Optim.","","1544-3566","10.1145/3664925","https://doi.org/10.1145/3664925","For datacenter architects, it is the most important goal to minimize the datacenter’s total cost of ownership for the target performance (i.e., TCO/performance). As the major component of a datacenter is a server farm, the most effective way of reducing TCO/performance is to improve the server’s performance and power efficiency. To achieve the goal, we claim that it is highly promising to reduce each server’s temperature to its most cost-effective point (or temperature scaling).In this article, we propose CoolDC, a novel and immediately applicable low-temperature cooling method to minimize the datacenter’s TCO. The key idea is to find and apply the most cost-effective sub-freezing temperature to target servers and workloads. For that purpose, we first apply the immersion cooling method to the entire servers to maintain a stable low temperature with little extra cooling and maintenance costs. Second, we define the TCO-optimal temperature for datacenter operation (e.g., 248K 273K (-25℃ 0℃)) by carefully estimating all the costs and benefits at low temperatures. Finally, we propose CoolDC, our immersion-cooling datacenter architecture to run every workload at its own TCO-optimal temperature. By incorporating our low-temperature workload-aware temperature scaling, CoolDC achieves 12.7% and 13.4% lower TCO/performance than the conventional air-cooled and immersion-cooled datacenters, respectively, without any modification to existing computers.","2024-09","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","3","21","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Datacenter architecture; low-temperature computing; temperature scaling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"75S7W858","journalArticle","2024","Gheibi, Omid; Weyns, Danny","Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive Systems Using Lifelong Self-Adaptation","ACM Trans. Auton. Adapt. Syst.","","1556-4665","10.1145/3636428","https://doi.org/10.1145/3636428","Recently, machine learning (ML) has become a popular approach to support self-adaptation. ML has been used to deal with several problems in self-adaptation, such as maintaining an up-to-date runtime model under uncertainty and scalable decision-making. Yet, exploiting ML comes with inherent challenges. In this article, we focus on a particularly important challenge for learning-based self-adaptive systems: drift in adaptation spaces. With adaptation space, we refer to the set of adaptation options a self-adaptive system can select from to adapt at a given time based on the estimated quality properties of the adaptation options. A drift of adaptation spaces originates from uncertainties, affecting the quality properties of the adaptation options. Such drift may imply that the quality of the system may deteriorate, eventually, no adaptation option may satisfy the initial set of adaptation goals, or adaptation options may emerge that allow enhancing the adaptation goals. In ML, such a shift corresponds to a novel class appearance, a type of concept drift in target data that common ML techniques have problems dealing with. To tackle this problem, we present a novel approach to self-adaptation that enhances learning-based self-adaptive systems with a lifelong ML layer. We refer to this approach as lifelong self-adaptation. The lifelong ML layer tracks the system and its environment, associates this knowledge with the current learning tasks, identifies new tasks based on differences, and updates the learning models of the self-adaptive system accordingly. A human stakeholder may be involved to support the learning process and adjust the learning and goal models. We present a general architecture for lifelong self-adaptation and apply it to the case of drift of adaptation spaces that affects the decision-making in self-adaptation. We validate the approach for a series of scenarios with a drift of adaptation spaces using the DeltaIoT exemplar.","2024-02","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","1","19","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","concept drift; lifelong self-adaptation; machine-learning; novel class appearance; Self-adaptation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EJKZSKK5","journalArticle","2022","Shi, Yimin; Duan, Haihan; Yang, Lei; Cai, Wei","An Energy-efficient and Privacy-aware Decomposition Framework for Edge-assisted Federated Learning","ACM Trans. Sen. Netw.","","1550-4859","10.1145/3522741","https://doi.org/10.1145/3522741","Deep Learning (DL) is an essential technology for modern intelligent sensor network and interactive multimedia applications, having problems with user data privacy when training on a central cloud. While Federated Learning (FL) motivates to preserve user privacy, it also causes new problems of lower user terminal usability and training efficiency, which caused substantial energy consumption. This article proposes a novel energy-efficient and privacy-aware decomposition framework to improve user-side FL efficiency under pre-defined privacy requirements with the assistance of Mobile Edge Computing (MEC) and Software Decomposition. It takes the propagation of each neural layer as the migrating unit and considers the tradeoff relationship between privacy and efficiency. We also propose an online scheduling algorithm to optimize the framework’s training performance. Furthermore, we summarize eight privacy-sensitive information classes on which existing privacy attacks base and design configurable privacy preservation mechanisms for each class. Simulations and experiments prove the effectiveness of our framework and algorithm in FL efficiency improvement and the effects of different privacy constraints on the overall training efficiency.","2022-11","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","4","18","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","distributed computing; federated learning; Mobile edge computing; software decomposition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6ZDLZLZC","journalArticle","2022","Carrión, Carmen","Kubernetes Scheduling: Taxonomy, Ongoing Issues and Challenges","ACM Comput. Surv.","","0360-0300","10.1145/3539606","https://doi.org/10.1145/3539606","Continuous integration enables the development of microservices-based applications using container virtualization technology. Container orchestration systems such as Kubernetes, which has become the de facto standard, simplify the deployment of container-based applications. However, developing efficient and well-defined orchestration systems is a challenge. This article focuses specifically on the scheduler, a key orchestrator task that assigns physical resources to containers. Scheduling approaches are designed based on different Quality of Service (QoS) parameters to provide limited response time, efficient energy consumption, better resource utilization, and other things. This article aims to establish insight knowledge into Kubernetes scheduling, find the main gaps, and thus guide future research in the area. Therefore, we conduct a study of empirical research on Kubernetes scheduling techniques and present a new taxonomy for Kubernetes scheduling. The challenges, future direction, and research opportunities are also discussed.","2022-12","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","7","55","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","scheduling; containers; Kubernetes; orchestration; survey","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BSAXVIK5","journalArticle","2023","Tournaire, Thomas; Castel-Taleb, Hind; Hyon, Emmanuel","Efficient Computation of Optimal Thresholds in Cloud Auto-scaling Systems","ACM Trans. Model. Perform. Eval. Comput. Syst.","","2376-3639","10.1145/3603532","https://doi.org/10.1145/3603532","We consider a horizontal and dynamic auto-scaling technique in a cloud system where virtual machines hosted on a physical node are turned on and off to minimise energy consumption while meeting performance requirements. Finding cloud management policies that adapt the system to the load is not straightforward, and we consider here that virtual machines are turned on and off depending on queue load thresholds. We want to compute the optimal threshold values that minimize consumption costs and penalty costs (when performance requirements are not met). To solve this problem, we propose several optimisation methods, based on two different mathematical approaches. The first one is based on queueing theory and uses local search heuristics coupled with the stationary distributions of Markov chains. The second approach tackles the problem using Markov Decision Process (MDP) in which we assume that the policy is of a special multi-threshold type called hysteresis. We improve the heuristics of the former approach with the aggregation of Markov chains and queues approximation techniques. We assess the benefit of threshold-aware algorithms for solving MDPs. Then we carry out theoretical analyzes of the two approaches. We also compare them numerically and we show that all of the presented MDP algorithms strongly outperform the local search heuristics. Finally, we propose a cost model for a real scenario of a cloud system to apply our optimisation algorithms and to show their practical relevance. The major scientific contribution of the article is a set of fast (almost in real time) load-based threshold computation methods that can be used by a cloud provider to optimize its financial costs.","2023-07","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","4","8","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Markov processes; cloud system; Decision processes; heuristics; hysteresis queues","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PYDUZ6RM","journalArticle","2020","Pham, Minh; Hoang, Doan B.; Chaczko, Zenon","Congestion-Aware and Energy-Aware Virtual Network Embedding","IEEE/ACM Trans. Netw.","","1063-6692","10.1109/TNET.2019.2958367","https://doi.org/10.1109/TNET.2019.2958367","Network virtualization is an inherent component of future internet architectures. Network resources are virtualized from the underlying substrate and elastically provisioned and offered to customers on-demand. Optimal allocation of network resources in terms of utilization, quality of service, and energy consumption has been a challenge. Existing solutions consider congestion control in a single-objective virtual network embedding (VNE) problem. This paper defines a multiple-objective VNE problem called the congestion-aware, energy-aware VNE (CEVNE). The aim is to seek a solution that saves cost, saves energy and avoids network congestion simultaneously. CEVNE modelling techniques and solution approaches apply both the weighting method and the constraint method to search for pareto-optimal solutions that produce the best compromised solutions for all three objectives. Solving VNE problem is, however, NP-hard. A heuristic solution is proposed involving a two-stage coordinated CEVNE. The node-mapping algorithm searches for the sub-optimal solutions for three objectives. The link mapping process is an SDN-based heuristic algorithm that deploys a path service and a resource monitoring application on an SDN controller. The solution is realized using SDN, Segment Routing, and open network operating system platform (ONOS) technologies. The energy minimization is implemented with a registry that keeps track of active nodes and sets inactive nodes to sleep mode. The evaluation results showed that the multiple-objective CEVNE approach is feasible and achieves its goals of optimizing the resource allocation, improving the runtime, saving the energy consumption and controlling the network congestion.","2020-02","2025-03-18 09:07:49","2025-03-18 09:07:49","","210–223","","1","28","","","","","","","","","","","","","","","","","Publisher: IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FXFDC4IY","journalArticle","2024","Malhotra, Ruchika; Bansal, Anjali; Kessentini, Marouane","A Systematic Literature Review on Maintenance of Software Containers","ACM Comput. Surv.","","0360-0300","10.1145/3645092","https://doi.org/10.1145/3645092","Nowadays, cloud computing is gaining tremendous attention to deliver information via the internet. Virtualization plays a major role in cloud computing as it deploys multiple virtual machines on the same physical machine and thus results in improving resource utilization. Hypervisor-based virtualization and containerization are two commonly used approaches in operating system virtualization. In this article, we provide a systematic literature review on various phases in maintenance of containers including container image detection, container scheduling, container security measures, and performance evaluation of containers. We have selected 145 primary studies out of which 24% of studies are related to container performance evaluation, 42% of studies are related to container scheduling techniques, 22% of studies are related to container security measures, and 12% of studies are related to container image detection process. A few studies are related to container image detection process and evaluation of container security measures. Resource utilization is the most considered performance objective in almost all container scheduling techniques. We conclude that there is a need to introduce new tagging approaches, smell detection approaches, and also new approaches to detect and resolve threat issues in containers so that we can maintain the security of containers.","2024-04","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","8","56","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","kubernetes; orchestration; performance evaluation; Containers; Docker; scheduling techniques; vulnerability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H69GU8AM","journalArticle","2023","Queiroz, Rui; Cruz, Tiago; Mendes, Jérôme; Sousa, Pedro; Simões, Paulo","Container-based Virtualization for Real-time Industrial Systems—A Systematic Review","ACM Comput. Surv.","","0360-0300","10.1145/3617591","https://doi.org/10.1145/3617591","Industrial Automation and Control systems have matured into a stable infrastructure model that has been kept fundamentally unchanged, using discrete embedded systems (such as Programmable Logic Controllers) to implement the first line of sensorization, actuation, and process control and stations and servers providing monitoring, supervision, logging/database and data-sharing capabilities, among others. More recently, with the emergence of the Industry 4.0 paradigm and the need for more flexibility, there has been a steady trend towards virtualizing some of the automation station/server components, first by using virtual machines and, more recently, by using container technology. This trend is pushing for better support for real-time requirements on enabling virtualization technologies such as virtual machines and containers. This article provides a systematic review on the use of container virtualization in real-time environments such as cyber-physical systems, assessing how existing and emerging technologies can fulfill the associated requirements. Starting by reviewing fundamental concepts related to container technology and real-time requirements, it goes on to present the methodology and results of a systematic study of 37 selected papers covering aspects related to the enforcement of real-time constrains within container hosts and the expected task latency on such environments, as well as an overview of container platforms and orchestration mechanisms for RT systems.","2023-10","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","3","56","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","industrial automation control systems; latency; Real-time containers; virtualization of cyber-physical systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5J42EHYT","journalArticle","2024","Karmakar, Prasenjit; Pradhan, Swadhin; Chakraborty, Sandip","Exploring Indoor Air Quality Dynamics in Developing Nations: A Perspective from India","ACM J. Comput. Sustain. Soc.","","","10.1145/3685694","https://doi.org/10.1145/3685694","Indoor air pollution is a major issue in developing countries such as India and Bangladesh, exacerbated by factors such as traditional cooking methods, insufficient ventilation, and cramped living conditions, all of which elevate the risk of health issues such as lung infections and cardiovascular diseases. With the World Health Organization associating around 3.2 million annual deaths globally to household air pollution, the gravity of the problem is clear. Yet, extensive empirical studies exploring these unique patterns and indoor pollution’s extent are missing. To fill this gap, we carried out a 6-months long field study involving over 30 households, uncovering the complexity of indoor air pollution in developing countries, such as the longer lingering time of volatile organic compounds (VOCs) in the air or the significant influence of air circulation on the spatiotemporal distribution of pollutants. We introduced an innovative Internet of Things (IoT) air quality sensing platform, the Distributed Air QuaLiTy MONitor (DALTON), explicitly designed to meet the needs of these nations, considering factors such as cost, sensor type, accuracy, network connectivity, power, and usability. As a result of a multi-device deployment, the platform identifies pollution hot spots in low- and middle-income households in developing nations. It identifies best practices to minimize daily indoor pollution exposure. Our extensive qualitative survey estimates an overall system usability score of 2.04, indicating an efficient system for air quality monitoring.","2024-09","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","3","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","best practices; Indoor pollution; pollution dynamics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VWC34HVQ","journalArticle","2018","Azimi, Reza; Fox, Tyler; Gonzalez, Wendy; Reda, Sherief","Scale-Out vs Scale-Up: A Study of ARM-based SoCs on Server-Class Workloads","ACM Trans. Model. Perform. Eval. Comput. Syst.","","2376-3639","10.1145/3232162","https://doi.org/10.1145/3232162","ARM 64-bit processing has generated enthusiasm to develop ARM-based servers that are targeted for both data centers and supercomputers. In addition to the server-class components and hardware advancements, the ARM software environment has grown substantially over the past decade. Major development ecosystems and libraries have been ported and optimized to run on ARM, making ARM suitable for server-class workloads. There are two trends in available ARM SoCs: mobile-class ARM SoCs that rely on the heterogeneous integration of a mix of CPU cores, GPGPU streaming multiprocessors (SMs), and other accelerators, and the server-class SoCs that instead rely on integrating a larger number of CPU cores with no GPGPU support and a number of IO accelerators. For scaling the number of processing cores, there are two different paradigms: mobile-class SoCs that use scale-out architecture in the form of a cluster of simpler systems connected over a network, and server-class ARM SoCs that use the scale-up solution and leverage symmetric multiprocessing to pack a large number of cores on the chip. In this article, we present ScaleSoC cluster, which is a scale-out solution based on mobile class ARM SoCs. ScaleSoC leverages fast network connectivity and GPGPU acceleration to improve performance and energy efficiency compared to previous ARM scale-out clusters. We consider a wide range of modern server-class parallel workloads to study both scaling paradigms, including latency-sensitive transactional workloads, MPI-based CPU and GPGPU-accelerated scientific applications, and emerging artificial intelligence workloads. We study the performance and energy efficiency of ScaleSoC compared to server-class ARM SoCs and discrete GPGPUs in depth. We quantify the network overhead on the performance of ScaleSoC and show that packing a large number of ARM cores on a single chip does not necessarily guarantee better performance, due to the fact that shared resources, such as last-level cache, become performance bottlenecks. We characterize the GPGPU accelerated workloads and demonstrate that for applications that can leverage the better CPU-GPGPU balance of the ScaleSoC cluster, performance and energy efficiency improve compared to discrete GPGPUs.","2018-08","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","4","3","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","ARM computing; GPGPU acceleration; scale-out clusters","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"98C3TDRF","journalArticle","2024","Tian, Hao; Xu, Xiaolong; Wu, Hongyue; Zhao, Qingzhan; Dai, Jianguo; Khan, Maqbool","Cost-Efficient Deep Neural Network Placement in Edge Intelligence-Enabled Internet of Things","ACM Trans. Sen. Netw.","","1550-4859","10.1145/3685930","https://doi.org/10.1145/3685930","Edge intelligence (EI) integrates edge computing and artificial intelligence empowering service providers to deploy deep neural networks (DNNs) on edge servers in proximity to users to provision intelligent applications (e.g., autonomous driving) for ubiquitous Internet of Things (IoT) in smart cities, which facilitates the quality of experience (QoE) of users and improves the processing and energy efficiency. However, considering DNN is typically computational-intensive and resource-hungry, conventional placement approaches ignore the influence of multi-dimensional resource requirements (processor, memory, etc.), which may degrade the real-time performance. Moreover, with the increasing scale of geo-distributed edge servers, centralized decision-making is still challenging to find the optimal strategies effectively. To overcome these shortcomings, in this paper we propose a game theoretic DNN placement approach in EI-enabled IoT. First, a DNN placement optimization problem is formulated to maximize system benefits, which is proven to be (mathcal Nmathcal P ) -hard and model the original problem as an exact potential game (EPG). Moreover, an EPG-based DNN model placement algorithm, named EPOL, is designed for edge servers to make sub-optimal strategies independently and theoretical analysis is possessed to guarantee the performance of EPOL. Finally, real-world dataset based experimental results corroborate the superiority and effectiveness of EPOL.","2024-08","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Internet of Things; DNN placement; Edge intelligence; Game theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SI936ELX","journalArticle","2023","Viola, Roberto; Martín, Ángel; Zorrilla, Mikel; Montalbán, Jon; Angueira, Pablo; Muntean, Gabriel-Miro","A Survey on Virtual Network Functions for Media Streaming: Solutions and Future Challenges","ACM Comput. Surv.","","0360-0300","10.1145/3567826","https://doi.org/10.1145/3567826","Media services must ensure an enhanced user’s perceived quality during content playback to attract and retain audiences, especially while the streams are distributed remotely via networks. Thus, media streaming services rely heavily on good and predictable network performance when delivered to a large number of people. Furthermore, as the quality of media content gets high, the network performance demands are also increasing, and meeting them is challenging. Network functions devoted to improving media streaming services become essential to cope with the high dynamics of network performance and user mobility. Furthermore, new networking paradigms and architectures under the 5G networks umbrella are bringing new possibilities to deploy smart network functions, which monitor the media streaming services through live and objective metrics and boost them in real-time. This survey overviews the state-of-the-art technologies and solutions proposed to apply new network functions for enhancing media streaming.","2023-02","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","11","55","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Media streaming; network functions; network virtualization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TPEJNNAY","journalArticle","2024","Lumpp, Francesco; Panato, Marco; Bombieri, Nicola; Fummi, Franco","A Design Flow Based on Docker and Kubernetes for ROS-based Robotic Software Applications","ACM Trans. Embed. Comput. Syst.","","1539-9087","10.1145/3594539","https://doi.org/10.1145/3594539","Human-centered robotic applications are becoming pervasive in the context of robotics and smart manufacturing, and such a pervasiveness is even more expected with the shift to Industry 5.0. The always increasing level of autonomy of modern robotic platforms requires the integration of software applications from different domains to implement artificial intelligence, cognition, and human-robot/robot-robot interaction. Developing and (re)configuring such a multi-domain software to meet functional constraints is a challenging task. Even more challenging is customizing the software to satisfy non-functional requirements such as real-time, reliability, and energy efficiency. In this context, the concept of Edge-Cloud continuum is gaining consensus as a solution to address functional and non-functional constraints in a seamless way. Containerization and orchestration are becoming a standard practice, as they allow for better information flow among different network levels as well as increased modularity in the use of multi-domain software components. Nevertheless, the adoption of such a practice along the design flow, from simulation to the deployment of complex robotic applications by addressing the de facto development standards (e.g., ROS - Robotic Operating System) is still an open problem. We present a design methodology based on Docker and Kubernetes that enables containerization and orchestration of ROS-based robotic SW applications for heterogeneous and hierarchical HW architectures. The methodology aims at (i) integrating and verifying multi-domain components since early in the design flow, (ii) mapping software tasks to containers to minimize the performance and memory footprint overhead, (iii) clustering containers to efficiently distribute load across the edge-cloud architecture by minimizing resource utilization, and (iv) enabling multi-domain verification of functional and non-functional constraints before deployment. The article presents the results obtained with a real case of study, in which the design methodology has been applied to program the mission of a Robotnik RB-Kairos mobile robot in an industrial agile production chain. We have obtained reduced load on the robot’s HW with minimal performance and network overhead, thanks to the optimized distributed system.","2024-08","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","5","23","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Kubernetes; Docker; ROS; Edge-Cloud computing; K3S; robotic applications","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PDVHC6C8","journalArticle","2023","Jeyaraj, Rathinaraja; Balasubramaniam, Anandkumar; M.A., Ajay Kumara; Guizani, Nadra; Paul, Anand","Resource Management in Cloud and Cloud-influenced Technologies for Internet of Things Applications","ACM Comput. Surv.","","0360-0300","10.1145/3571729","https://doi.org/10.1145/3571729","The trend of adopting Internet of Things (IoT) in healthcare, smart cities, Industry 4.0, and so on is increasing by means of cloud computing, which provides on-demand storage and computation facilities over the Internet. To meet specific requirements of IoT applications, the cloud has also shifted its service offering platform to its next-generation models, such as fog, mist, and dew computing. As a result, the cloud and IoT have become part and parcel of smart applications that play significant roles in improving the quality of human life. In addition to the inherent advantages of advanced cloud models, to improve the performance of IoT applications further, it is essential to understand how the resources in the cloud and cloud-influenced platforms are managed to support various phases in the end-to-end IoT deployment. Considering this importance, in this article, we provide a brief description, a systematic review, and possible research directions on every aspect of resource management tasks, such as workload modeling, resource provisioning, workload scheduling, resource allocation, load balancing, energy management, and resource heterogeneity in such advanced platforms, from a cloud perspective. The primary objective of this article is to help early researchers gain insight into the underlying concepts of resource management tasks in the cloud for IoT applications.","2023-03","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","12","55","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Cloud computing; resource allocation; Internet of Things; edge computing; dew computing; fog computing; load balancing; mist computing; resource heterogeneity; resource provisioning; resource scheduling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P2UBNC4U","journalArticle","2024","Zhao, Laiping; Cui, Yushuai; Yang, Yanan; Zhou, Xiaobo; Qiu, Tie; Li, Keqiu; Bao, Yungang","Component-distinguishable Co-location and Resource Reclamation for High-throughput Computing","ACM Trans. Comput. Syst.","","0734-2071","10.1145/3630006","https://doi.org/10.1145/3630006","Cloud service providers improve resource utilization by co-locating latency-critical (LC) workloads with best-effort batch (BE) jobs in datacenters. However, they usually treat multi-component LCs as monolithic applications and treat BEs as “second-class citizens” when allocating resources to them. Neglecting the inconsistent interference tolerance abilities of LC components and the inconsistent preemption loss of BE workloads can result in missed co-location opportunities for higher throughput.We present Rhythm, a co-location controller that deploys workloads and reclaims resources rhythmically for maximizing the system throughput while guaranteeing LC service’s tail latency requirement. The key idea is to differentiate the BE throughput launched with each LC component, that is, components with higher interference tolerance can be deployed together with more BE jobs. It also assigns different reclamation priority values to BEs by evaluating their preemption losses into a multi-level reclamation queue. We implement and evaluate Rhythm using workloads in the form of containerized processes and microservices. Experimental results show that it can improve the system throughput by 47.3%, CPU utilization by 38.6%, and memory bandwidth utilization by 45.4% while guaranteeing the tail latency requirement.","2024-02","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","1–2","42","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","co-locating; Datacenters; resource utilization; tail latency","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JKBPSQBZ","journalArticle","2023","Shen, Zhishu; Jin, Jiong; Tan, Cheng; Tagami, Atsushi; Wang, Shangguang; Li, Qing; Zheng, Qiushi; Yuan, Jingling","A Survey of Next-generation Computing Technologies in Space-air-ground Integrated Networks","ACM Comput. Surv.","","0360-0300","10.1145/3606018","https://doi.org/10.1145/3606018","Space-air-ground integrated networks (SAGINs) are key elements for facilitating high-speed seamless connectivity to the devices/users in infrastructure-less environments, where the traditional terrestrial networks are critically infeasible or uneconomical to be fully deployed. This article comprehensively surveys the advanced computing technologies that support the utilization of SAGINs for infrastructure-less environments. The advanced computing technologies refer to the emerging computing techniques, tools, and the processes that can be utilized to support SAGINs in handling the increasing computing tasks. The main contents include: (1) background of SAGINs, (2) typical use cases of SAGINs in infrastructure-less environments, (3) advanced computing technologies to assist SAGINs to meet the requirements of various services in infrastructure-less environments, (4) the related practical initiatives, and (5) the open research challenges and the future research directions.","2023-08","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","1","56","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Internet of Things; computing and networking convergence; infrastructure-less environments; Space-air-ground integrated networks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AL66GDWY","journalArticle","2023","Zhang, Wenzhao; Gao, Yi; Dong, Wei","Providing Realtime Support for Containerized Edge Services","ACM Trans. Internet Technol.","","1533-5399","10.1145/3617123","https://doi.org/10.1145/3617123","Containers have emerged as a popular technology for edge computing platforms. Although there are varieties of container orchestration frameworks, e.g., Kubernetes to provide high-reliable services for cloud infrastructure, providing real-time support at the containerized edge systems (CESs) remains a challenge. In this paper, we propose EdgeMan, a holistic edge service management framework for CESs, which consists of (1) a model-assisted event-driven lightweight online scheduling algorithm to provide request-level execution plans; (2) a bottleneck-metric-aware progressive resource allocation mechanism to improve resource efficiency. We then build a testbed that installed three containerized services with different latency sensitivities for concrete evaluation. Additionally, we adopt real-world data traces from Alibaba and Twitter for large-scale emulations. Extensive experiments demonstrate that the deadline miss ratio of time-sensitive services run with EdgeMan&nbsp;is reduced by 85.9%&nbsp;on average compared with that of existing methods in both industry and academia.","2023-11","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","4","23","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","cloud-native; containerized edge service; Realtime support","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ATQDMNB5","journalArticle","2022","Sobhy, Dalia; Minku, Leandro; Bahsoon, Rami; Kazman, Rick","Continuous and Proactive Software Architecture Evaluation: An IoT Case","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3492762","https://doi.org/10.1145/3492762","Design-time evaluation is essential to build the initial software architecture to be deployed. However, experts’ assumptions made at design-time are unlikely to remain true indefinitely in systems that are characterized by scale, hyperconnectivity, dynamism, and uncertainty in operations (e.g. IoT). Therefore, experts’ design-time decisions can be challenged at run-time. A continuous architecture evaluation that systematically assesses and intertwines design-time and run-time decisions is thus necessary. This paper proposes the first proactive approach to continuous architecture evaluation of the system leveraging the support of simulation. The approach evaluates software architectures by not only tracking their performance over time, but also forecasting their likely future performance through machine learning of simulated instances of the architecture. This enables architects to make cost-effective informed decisions on potential changes to the architecture. We perform an IoT case study to show how machine learning on simulated instances of architecture can fundamentally guide the continuous evaluation process and influence the outcome of architecture decisions. A series of experiments is conducted to demonstrate the applicability and effectiveness of the approach. We also provide the architect with recommendations on how to best benefit from the approach through choice of learners and input parameters, grounded on experimentation and evidence.","2022-03","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","3","31","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","IoT; Continuous evaluation; software architecture evaluation; time series forecasting","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7UBZ4X8K","journalArticle","2025","Casadei, Roberto; Aguzzi, Gianluca; Audrito, Giorgio; Damiani, Ferruccio; Pianini, Danilo; Scarso, Giordano; Torta, Gianluca; Viroli, Mirko","Software Engineering for Collective Cyber-Physical Ecosystems","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3712004","https://doi.org/10.1145/3712004","Today's distributed and pervasive computing addresses large-scale cyber-physical ecosystems, characterised by dense and large networks of devices capable of computation, communication and interaction with the environment and people. While most research focuses on treating these systems as “composites” (i.e., heterogeneous functional complexes), recent developments in fields such as self-organising systems and swarm robotics have opened up a complementary perspective: treating systems as “collectives” (i.e., uniform, collaborative, and self-organising groups of entities). This article explores the motivations, state of the art, and implications of this “collective computing paradigm” in software engineering. In particular, it discusses its peculiar challenges, implied by characteristics like distribution, situatedness, large scale, and cooperative nature. These challenges outline significant directions for future research in software engineering, touching on aspects such as macro-programming, collective intelligence, self-adaptive middleware, learning/synthesis of collective behaviour, human involvement, safety and security in collective cyber-physical ecosystems.","2025-01","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","edge-cloud continuum; collective adaptive systems; cyber-physical ecosystems; distributed artificial intelligence; macro-programming; multi-agent systems; swarm intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KMHYHR2E","journalArticle","2022","Costa, Breno; Bachiega, Joao; de Carvalho, Leonardo Rebouças; Araujo, Aleteia P. F.","Orchestration in Fog Computing: A Comprehensive Survey","ACM Comput. Surv.","","0360-0300","10.1145/3486221","https://doi.org/10.1145/3486221","Fog computing is a paradigm that brings computational resources and services to the network edge in the vicinity of user devices, lowering latency and connecting with cloud computing resources. Unlike cloud computing, fog resources are based on constrained and heterogeneous nodes whose connectivity can be unstable. In this complex scenario, there is a need to define and implement orchestration processes to ensure that applications and services can be provided, considering the settled agreements. Although some publications have dealt with orchestration in fog computing, there are still some diverse definitions and functional intersection with other areas, such as resource management and monitoring. This article presents a systematic review of the literature with focus on orchestration in fog computing. A generic architecture of fog orchestration is presented, created from the consolidation of the analyzed proposals, bringing to light the essential functionalities addressed in the literature. This work also highlights the main challenges and open research questions.","2022-01","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","2","55","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","resource management; Fog computing; orchestration; monitoring","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YBFIYDXU","journalArticle","2024","Zamanzadeh Darban, Zahra; Webb, Geoffrey I.; Pan, Shirui; Aggarwal, Charu; Salehi, Mahsa","Deep Learning for Time Series Anomaly Detection: A Survey","ACM Comput. Surv.","","0360-0300","10.1145/3691338","https://doi.org/10.1145/3691338","Time series anomaly detection is important for a wide range of research fields and applications, including financial markets, economics, earth sciences, manufacturing, and healthcare. The presence of anomalies can indicate novel or unexpected events, such as production faults, system defects, and heart palpitations, and is therefore of particular interest. The large size and complexity of patterns in time series data have led researchers to develop specialised deep learning models for detecting anomalous patterns. This survey provides a structured and comprehensive overview of state-of-the-art deep learning for time series anomaly detection. It provides a taxonomy based on anomaly detection strategies and deep learning models. Aside from describing the basic anomaly detection techniques in each category, their advantages and limitations are also discussed. Furthermore, this study includes examples of deep anomaly detection in time series across various application domains in recent years. Finally, it summarises open issues in research and challenges faced while adopting deep anomaly detection models to time series data.","2024-10","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","1","57","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Anomaly detection; deep learning; multivariate time series; outlier detection; time series; univariate time series","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IYUASXD7","journalArticle","2022","Zhong, Zhiheng; Xu, Minxian; Rodriguez, Maria Alejandra; Xu, Chengzhong; Buyya, Rajkumar","Machine Learning-based Orchestration of Containers: A Taxonomy and Future Directions","ACM Comput. Surv.","","0360-0300","10.1145/3510415","https://doi.org/10.1145/3510415","Containerization is a lightweight application virtualization technology, providing high environmental consistency, operating system distribution portability, and resource isolation. Existing mainstream cloud service providers have prevalently adopted container technologies in their distributed system infrastructures for automated application management. To handle the automation of deployment, maintenance, autoscaling, and networking of containerized applications, container orchestration is proposed as an essential research problem. However, the highly dynamic and diverse feature of cloud workloads and environments considerably raises the complexity of orchestration mechanisms. Machine learning algorithms are accordingly employed by container orchestration systems for behavior modeling and prediction of multi-dimensional performance metrics. Such insights could further improve the quality of resource provisioning decisions in response to the changing workloads under complex environments. In this article, we present a comprehensive literature review of existing machine learning-based container orchestration approaches. Detailed taxonomies are proposed to classify the current researches by their common features. Moreover, the evolution of machine learning-based container orchestration technologies from the year 2016 to 2021 has been designed based on objectives and metrics. A comparative analysis of the reviewed techniques is conducted according to the proposed taxonomies, with emphasis on their key characteristics. Finally, various open research challenges and potential future directions are highlighted.","2022-09","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","10s","54","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","cloud computing; machine learning; resource provisioning; Container orchestration; systematic review","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8H4ANLC2","journalArticle","2022","Li, Huan; Lu, Hua; Jensen, Christian S.; Tang, Bo; Cheema, Muhammad Aamir","Spatial Data Quality in the Internet of Things: Management, Exploitation, and Prospects","ACM Comput. Surv.","","0360-0300","10.1145/3498338","https://doi.org/10.1145/3498338","With the continued deployment of the Internet of Things (IoT), increasing volumes of devices are being deployed that emit massive spatially referenced data. Due in part to the dynamic, decentralized, and heterogeneous architecture of the IoT, the varying and often low quality of spatial IoT data (SID) presents challenges to applications built on top of this data. This survey aims to provide unique insight to practitioners who intend to develop IoT-enabled applications and to researchers who wish to conduct research that relates to data quality in the IoT setting. The survey offers an inventory analysis of major data quality dimensions in SID and covers significant data characteristics and associated quality considerations. The survey summarizes data quality related technologies from both task and technique perspectives. Organizing the technologies from the task perspective, it covers recent progress in SID quality management, encompassing location refinement, uncertainty elimination, outlier removal, fault correction, data integration, and data reduction; and it covers low-quality SID exploitation, encompassing querying, analysis, and decision-making techniques. Finally, the survey covers emerging trends and open issues concerning the quality of SID.","2022-02","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","3","55","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Internet of Things; geo-sensory data; location refinement; quality management; spatial computing; spatial queries; spatiotemporal data cleaning; spatiotemporal dependencies","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UHEW4ALQ","journalArticle","2021","Siqueira, Frank; Davis, Joseph G.","Service Computing for Industry 4.0: State of the Art, Challenges, and Research Opportunities","ACM Comput. Surv.","","0360-0300","10.1145/3478680","https://doi.org/10.1145/3478680","Recent advances in the large-scale adoption of information and communication technologies in manufacturing processes, known as Industry 4.0 or Smart Manufacturing, provide us a window into how the manufacturing sector will evolve in the coming decades. As a result of these initiatives, manufacturing firms have started to integrate a series of emerging technologies into their processes that will change the way products are designed, manufactured, and consumed. This article provides a comprehensive review of how service-oriented computing is being employed to develop the required software infrastructure for Industry 4.0 and identifies the major challenges and research opportunities that ensue. Particular attention is paid to the microservices architecture, which is increasingly recognized as offering a promising approach for developing innovative industrial applications. This literature review is based on the current state of the art on service computing for Industry 4.0 as described in a large corpus of recently published research papers, which helped us to identify and explore a series of challenges and opportunities for the development of this emerging technology frontier, with the goal of facilitating its widespread adoption.","2021-10","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","9","54","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","digital twins; microservices; industry 4.0; industrial IoT; Service-oriented architecture","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RWRBHL3K","journalArticle","2020","Mahmud, Redowan; Ramamohanarao, Kotagiri; Buyya, Rajkumar","Application Management in Fog Computing Environments: A Taxonomy, Review and Future Directions","ACM Comput. Surv.","","0360-0300","10.1145/3403955","https://doi.org/10.1145/3403955","The Internet of Things (IoT) paradigm is being rapidly adopted for the creation of smart environments in various domains. The IoT-enabled cyber-physical systems associated with smart city, healthcare, Industry 4.0 and Agtech handle a huge volume of data and require data processing services from different types of applications in real time. The Cloud-centric execution of IoT applications barely meets such requirements as the Cloud datacentres reside at a multi-hop distance from the IoT devices. Fog computing, an extension of Cloud at the edge network, can execute these applications closer to data sources. Thus, Fog computing can improve application service delivery time and resist network congestion. However, the Fog nodes are highly distributed and heterogeneous, and most of them are constrained in resources and spatial sharing. Therefore, efficient management of applications is necessary to fully exploit the capabilities of Fog nodes. In this work, we investigate the existing application management strategies in Fog computing and review them in terms of architecture, placement and maintenance. Additionally, we propose a comprehensive taxonomy and highlight the research gaps in Fog-based application management. We also discuss a perspective model and provide future research directions for further improvement of application management in Fog computing.","2020-07","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","4","53","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Fog computing; application placement; Internet of Things; application architecture; application maintenance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MWXREV23","journalArticle","2024","Ghosh, Ushasi; Chiejina, Azuka; Stephenson, Nathan; Shah, Vijay K; Shakkottai, Srinivas; Bharadia, Dinesh","SPARC: Spatio-Temporal Adaptive Resource Control for Multi-site Spectrum Management in NextG Cellular Networks","Proc. ACM Netw.","","","10.1145/3696405","https://doi.org/10.1145/3696405","This work presents SPARC (Spatio-Temporal Adaptive Resource Control), a novel approach for multi-site spectrum management in NextG cellular networks. SPARC addresses the challenge of limited licensed spectrum in dynamic environments. We leverage the O-RAN architecture to develop a multi-timescale RAN Intelligent Controller (RIC) framework, featuring an xApp for near-real-time interference detection and localization, and a xApp for real-time intelligent resource allocation. By utilizing base stations as spectrum sensors, SPARC enables efficient and fine-grained dynamic resource allocation across multiple sites, enhancing signal-to-noise ratio (SNR) by up to 7dB, spectral efficiency by up to 15%, and overall system throughput by up to 20%. Comprehensive evaluations, including emulations and over-the-air experiments, demonstrate the significant performance gains achieved through SPARC, showcasing it as a promising solution for optimizing resource efficiency and network performance in NextG cellular networks.","2024-11","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","CoNEXT4","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","interference detection; ran intelligent control; traffic","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SHUIQ25A","journalArticle","2024","Golpayegani, Fateneh; Chen, Nanxi; Afraz, Nima; Gyamfi, Eric; Malekjafarian, Abdollah; Schäfer, Dominik; Krupitzer, Christian","Adaptation in Edge Computing: A Review on Design Principles and Research Challenges","ACM Trans. Auton. Adapt. Syst.","","1556-4665","10.1145/3664200","https://doi.org/10.1145/3664200","Edge computing places the computational services and resources closer to the user proximity, to reduce latency, and ensure the quality of service and experience. Low latency, context awareness and mobility support are the major contributors to edge-enabled smart systems. Such systems require handling new situations and change on the fly and ensuring the quality of service while only having access to constrained computation and communication resources and operating in mobile, dynamic and ever-changing environments. Hence, adaptation and self-organisation are crucial for such systems to maintain their performance, and operability while accommodating new changes in their environment.This article reviews the current literature in the field of adaptive edge computing systems. We use a widely accepted taxonomy, which describes the important aspects of adaptive behaviour implementation in computing systems. This taxonomy discusses aspects such as adaptation reasons, the various levels an adaptation strategy can be implemented, the time of reaction to a change, categories of adaptation technique and control of the adaptive behaviour. In this article, we discuss how these aspects are addressed in the literature and identify the open research challenges and future direction in adaptive edge computing systems.The results of our analysis show that most of the identified approaches target adaptation at the application level, and only a few focus on middleware, communication infrastructure and context. Adaptations that are required to address the changes in the context, changes caused by users or in the system itself are also less explored. Furthermore, most of the literature has opted for reactive adaptation, although proactive adaptation is essential to maintain the edge computing systems’ performance and interoperability by anticipating the required adaptations on the fly. Additionally, most approaches apply a centralised adaptation control, which does not perfectly fit the mostly decentralised/distributed edge computing settings.","2024-09","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","3","19","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","edge computing; Adaptation; edge-enabled; MAPE-loop","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5JDWHDL7","journalArticle","2022","Jamil, Bushra; Ijaz, Humaira; Shojafar, Mohammad; Munir, Kashif; Buyya, Rajkumar","Resource Allocation and Task Scheduling in Fog Computing and Internet of Everything Environments: A Taxonomy, Review, and Future Directions","ACM Comput. Surv.","","0360-0300","10.1145/3513002","https://doi.org/10.1145/3513002","The Internet of Everything paradigm is being rapidly adopted in developing applications for different domains like smart agriculture, smart city, big data streaming, and so on. These IoE applications are leveraging cloud computing resources for execution. Fog computing, which emerged as an extension of cloud computing, supports mobility, heterogeneity, geographical distribution, context awareness, and services such as storage, processing, networking, and analytics on nearby fog nodes. The resource-limited, heterogeneous, dynamic, and uncertain fog environment makes task scheduling a great challenge that needs to be investigated. The article is motivated by this consideration and presents a systematic, comprehensive, and detailed comparative study by discussing the merits and demerits of different scheduling algorithms, focused optimization metrics, and evaluation tools in the fog computing and IoE environment. The goal of this survey article is fivefold. First, we review the fog computing and IoE paradigms. Second, we delineate the optimization metric engaged with fog computing and IoE environment. Third, we review, classify, and compare existing scheduling algorithms dealing with fog computing and IoE environment paradigms by leveraging some examples. Fourth, we rationalize the scheduling algorithms and point out the lesson learned from the survey. Fifth, we discuss the open issues and future research directions to improve scheduling in fog computing and the IoE environment.","2022-09","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","11s","54","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Cloud computing; resource allocation; task scheduling; fog computing; Internet of Everything (IoE); Internet of Things (IoT)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6LGVGPU9","journalArticle","2021","Varghese, Blesson; Wang, Nan; Bermbach, David; Hong, Cheol-Ho; Lara, Eyal De; Shi, Weisong; Stewart, Christopher","A Survey on Edge Performance Benchmarking","ACM Comput. Surv.","","0360-0300","10.1145/3444692","https://doi.org/10.1145/3444692","Edge computing is the next Internet frontier that will leverage computing resources located near users, sensors, and data stores to provide more responsive services. Therefore, it is envisioned that a large-scale, geographically dispersed, and resource-rich distributed system will emerge and play a key role in the future Internet. However, given the loosely coupled nature of such complex systems, their operational conditions are expected to change significantly over time. In this context, the performance characteristics of such systems will need to be captured rapidly, which is referred to as performance benchmarking, for application deployment, resource orchestration, and adaptive decision-making. Edge performance benchmarking is a nascent research avenue that has started gaining momentum over the past five years. This article first reviews articles published over the past three decades to trace the history of performance benchmarking from tightly coupled to loosely coupled systems. It then systematically classifies previous research to identify the system under test, techniques analyzed, and benchmark runtime in edge performance benchmarking.","2021-04","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","3","54","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","benchmark runtime; Edge computing; edge performance benchmarking; system under test; techniques analyzed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2NSWRB8L","journalArticle","2025","Syed, Naeem; Anwar, Adnan; Baig, Zubair; Zeadally, Sherali","Artificial Intelligence as a Service (AIaaS) for Cloud, Fog and the Edge: State-of-the-Art Practices","ACM Comput. Surv.","","0360-0300","10.1145/3712016","https://doi.org/10.1145/3712016","Artificial Intelligence (AI) fosters enormous business opportunities that build and utilize private AI models. Implementing AI models at scale and ensuring cost-effective production of AI-based technologies through entirely in-house capabilities is a challenge. The success of the Infrastructure as a Service (IaaS) and Software as a Service (SaaS) Cloud Computing models can be leveraged to facilitate a cost-effective and scalable AI service paradigm, namely, ‘AI as a Service.’ We summarize current state-of-the-art solutions for AI-as-a-Service (AIaaS), and we discuss its prospects for growth and opportunities to advance the concept. To this end, we perform a thorough review of recent research on AI and various deployment strategies for emerging domains considering both technical as well as survey articles. Next, we identify various characteristics and capabilities that need to be met before an AIaaS model can be successfully designed and deployed. Based on this we present a general framework of an AIaaS architecture that integrates the required aaS characteristics with the capabilities of AI. We also compare various approaches for offering AIaaS to end users. Finally, we illustrate several real-world use cases for AIaaS models, followed by a discussion of some of the challenges that must be addressed to enable AIaaS adoption.","2025-01","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","IoT; AIaaS; Artificial intelligence; Artificial intelligence as a Service; Cloud Computing; Edge","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LP479RTF","journalArticle","2018","Buyya, Rajkumar; Srirama, Satish Narayana; Casale, Giuliano; Calheiros, Rodrigo; Simmhan, Yogesh; Varghese, Blesson; Gelenbe, Erol; Javadi, Bahman; Vaquero, Luis Miguel; Netto, Marco A. S.; Toosi, Adel Nadjaran; Rodriguez, Maria Alejandra; Llorente, Ignacio M.; Vimercati, Sabrina De Capitani Di; Samarati, Pierangela; Milojicic, Dejan; Varela, Carlos; Bahsoon, Rami; Assuncao, Marcos Dias De; Rana, Omer; Zhou, Wanlei; Jin, Hai; Gentzsch, Wolfgang; Zomaya, Albert Y.; Shen, Haiying","A Manifesto for Future Generation Cloud Computing: Research Directions for the Next Decade","ACM Comput. Surv.","","0360-0300","10.1145/3241737","https://doi.org/10.1145/3241737","The Cloud computing paradigm has revolutionised the computer science horizon during the past decade and has enabled the emergence of computing as the fifth utility. It has captured significant attention of academia, industries, and government bodies. Now, it has emerged as the backbone of modern economy by offering subscription-based services anytime, anywhere following a pay-as-you-go model. This has instigated (1) shorter establishment times for start-ups, (2) creation of scalable global enterprise applications, (3) better cost-to-value associativity for scientific and high-performance computing applications, and (4) different invocation/execution models for pervasive and ubiquitous applications. The recent technological developments and paradigms such as serverless computing, software-defined networking, Internet of Things, and processing at network edge are creating new opportunities for Cloud computing. However, they are also posing several new challenges and creating the need for new approaches and research strategies, as well as the re-evaluation of the models that were developed to address issues such as scalability, elasticity, reliability, security, sustainability, and application models. The proposed manifesto addresses them by identifying the major open challenges in Cloud computing, emerging trends, and impact areas. It then offers research directions for the next decade, thus helping in the realisation of Future Generation Cloud Computing.","2018-11","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","5","51","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Cloud computing; Fog computing; serverless computing; sustainability; scalability; application development; Cloud economics; data management; InterCloud","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4X74ZQH3","journalArticle","2023","Barrios, Carlos; Kumar, Mohan","Service Caching and Computation Reuse Strategies at the Edge: A Survey","ACM Comput. Surv.","","0360-0300","10.1145/3609504","https://doi.org/10.1145/3609504","With the proliferation of connected devices including smartphones, novel network connectivity and management methods are needed to meet user Quality of Experience (QoE) and computational demands of contemporary applications. Service caching and computation reuse techniques are being employed to alleviate challenges due to scalability, interoperability, and mobility, as well as to reduce application latency by enabling caching at the edge. This survey provides a taxonomy for service caching and computation reuse and describes the current state of the research and its challenges. This is the first survey that provides a comprehensive analysis and suggests future research directions on this topic.","2023-09","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","2","56","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","computation deduplication; computation reuse; Service caching; task caching","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U2QXH93Z","journalArticle","2023","Sun, Danfeng; Hu, Junjie; Wu, Huifeng; Wu, Jia; Yang, Jian; Sheng, Quan Z.; Dustdar, Schahram","A Comprehensive Survey on Collaborative Data-access Enablers in the IIoT","ACM Comput. Surv.","","0360-0300","10.1145/3612918","https://doi.org/10.1145/3612918","The scope of the Industrial Internet of Things (IIoT) has stretched beyond manufacturing to include energy, healthcare, transportation, and all that tomorrow’s smart cities will entail. The realm of IIoT includes smart sensors, actuators, programmable logic controllers, distributed control systems (DCS), embedded devices, supervisory control, and data acquisition systems—all produced by manufacturers for different purposes and with different data structures and formats; designed according to different standards and made to follow different protocols. In this sea of incompatibility, how can we flexibly acquire these heterogeneous data, and how can we uniformly structure them to suit thousands of different applications? In this article, we survey the four pillars of information science that enable collaborative data access in an IIoT—standardization, data acquisition, data fusion, and scalable architecture—to provide an up-to-date audit of current research in the field. Here, standardization in IIoT relies on standards and technologies to make things communicative; data acquisition attempts to transparently collect data through plug-and-play architectures, reconfigurable schemes, or hardware expansion; data fusion refers to the techniques and strategies for overcoming heterogeneity in data formats and sources; and scalable architecture provides basic techniques to support heterogeneous requirements. The article also concludes with an overview of the frontier researches and emerging technologies for supporting or challenging data access from the aspects of 5G, machine learning, blockchain, and semantic web.","2023-09","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","2","56","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VFX4ITDF","journalArticle","2020","Liu, Xunyun; Buyya, Rajkumar","Resource Management and Scheduling in Distributed Stream Processing Systems: A Taxonomy, Review, and Future Directions","ACM Comput. Surv.","","0360-0300","10.1145/3355399","https://doi.org/10.1145/3355399","Stream processing is an emerging paradigm to handle data streams upon arrival, powering latency-critical application such as fraud detection, algorithmic trading, and health surveillance. Though there are a variety of Distributed Stream Processing Systems (DSPSs) that facilitate the development of streaming applications, resource management and task scheduling is not automatically handled by the DSPS middleware and requires a laborious process to tune toward specific deployment targets. As the advent of cloud computing has supported renting resources on-demand, it is of great interest to review the research progress of hosting streaming systems in clouds under certain Service Level Agreements (SLA) and cost constraints. In this article, we introduce the hierarchical structure of streaming systems, define the scope of the resource management problem, and present a comprehensive taxonomy in this context covering critical research topics such as resource provisioning, operator parallelisation, and task scheduling. The literature is then reviewed following the taxonomy structure, facilitating a deeper understanding of the research landscape through classification and comparison of existing works. Finally, we discuss the open issues and future research directions toward realising an automatic, SLA-aware resource management framework.","2020-05","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","3","53","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","task scheduling; distributed stream processing systems; Resource management; stream processing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MYBKB2NS","journalArticle","2022","Da Silva, Thiago Pereira; Batista, Thais; Lopes, Frederico; Neto, Aluizio Rocha; Delicato, Flávia C.; Pires, Paulo F.; Da Rocha, Atslands R.","Fog Computing Platforms for Smart City Applications: A Survey","ACM Trans. Internet Technol.","","1533-5399","10.1145/3488585","https://doi.org/10.1145/3488585","Emerging IoT applications with stringent requirements on latency and data processing have posed many challenges to cloud-centric platforms for Smart Cities. Recently, Fog Computing has been advocated as a promising approach to support such new applications and handle the increasing volume of IoT data and devices. The Fog Computing paradigm is characterized by a horizontal system-level architecture where devices close to end-users and IoT devices are used for processing, storage, and networking functions. Fog Computing platforms aim to facilitate the development of applications and systems for Smart Cities by providing services and abstractions designed to integrate data from IoT devices and various information systems deployed in the city. Despite the potential of the Fog Computing paradigm, the literature still lacks a broad, comprehensive overview of what has been investigated on the use of such paradigm in platforms for Smart Cities and open issues to be addressed in future research and development. In this paper, a systematic mapping study was performed and we present a comprehensive understanding of the use of the Fog Computing paradigm in Smart Cities platforms, providing an overview of the current state of research on this topic, and identifying important gaps in the existing approaches and promising research directions.","2022-12","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","4","22","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Fog computing; edge computing; smart cities","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6QHM7ILX","journalArticle","2022","Neha, Benazir; Panda, Sanjaya Kumar; Sahu, Pradip Kumar; Sahoo, Kshira Sagar; Gandomi, Amir H.","A Systematic Review on Osmotic Computing","ACM Trans. Internet Things","","","10.1145/3488247","https://doi.org/10.1145/3488247","Osmotic computing in association with related computing paradigms (cloud, fog, and edge) emerges as a promising solution for handling bulk of security-critical as well as latency-sensitive data generated by the digital devices. It is a growing research domain that studies deployment, migration, and optimization of applications in the form of microservices across cloud/edge infrastructure. It presents dynamically tailored microservices in technology-centric environments by exploiting edge and cloud platforms. Osmotic computing promotes digital transformation and furnishes benefits to transportation, smart cities, education, and healthcare. In this article, we present a comprehensive analysis of osmotic computing through a systematic literature review approach. To ensure high-quality review, we conduct an advanced search on numerous digital libraries to extracting related studies. The advanced search strategy identifies 99 studies, from which 29 relevant studies are selected for a thorough review. We present a summary of applications in osmotic computing build on their key features. On the basis of the observations, we outline the research challenges for the applications in this research field. Finally, we discuss the security issues resolved and unresolved in osmotic computing.","2022-02","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","2","3","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Osmotic computing; edge computing; fog computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KVEZRE34","journalArticle","2024","Pal, Asmita; Desai, Keerthana; Chatterjee, Rahul; San Miguel, Joshua","Camouflage: Utility-Aware Obfuscation for Accurate Simulation of Sensitive Program Traces","ACM Trans. Archit. Code Optim.","","1544-3566","10.1145/3650110","https://doi.org/10.1145/3650110","Trace-based simulation is a widely used methodology for system design exploration. It relies on realistic traces that represent a range of behaviors necessary to be evaluated, containing a lot of information about the application, its inputs and the underlying system on which it was generated. Consequently, generating traces from real-world executions risks leakage of sensitive information. To prevent this, traces can be obfuscated before release. However, this can undermine their ideal utility, i.e., how realistically a program behavior was captured. To address this, we propose Camouflage, a novel obfuscation framework, designed with awareness of the necessary architectural properties required to preserve trace utility, while ensuring secrecy of the inputs used to generate the trace. Focusing on memory access traces, our extensive evaluation on various benchmarks shows that camouflaged traces preserve the performance measurements of the original execution, with an average τ correlation of 0.66. We model input secrecy as an input indistinguishability problem and show that the average security loss is 7.8%, which is better than traces generated from the state-of-the-art.","2024-05","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","2","21","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Performance characterization; Privacy of traces; Synthetic trace generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LLTIWD8B","journalArticle","2020","Do, Jaeyoung; Ferreira, Victor C.; Bobarshad, Hossein; Torabzadehkashi, Mahdi; Rezaei, Siavash; Heydarigorji, Ali; Souza, Diego; Goldstein, Brunno F.; Santiago, Leandro; Kim, Min Soo; Lima, Priscila M. V.; França, Felipe M. G.; Alves, Vladimir","Cost-effective, Energy-efficient, and Scalable Storage Computing for Large-scale AI Applications","ACM Trans. Storage","","1553-3077","10.1145/3415580","https://doi.org/10.1145/3415580","The growing volume of data produced continuously in the Cloud and at the Edge poses significant challenges for large-scale AI applications to extract and learn useful information from the data in a timely and efficient way. The goal of this article is to explore the use of computational storage to address such challenges by distributed near-data processing. We describe Newport, a high-performance and energy-efficient computational storage developed for realizing the full potential of in-storage processing. To the best of our knowledge, Newport is the first commodity SSD that can be configured to run a server-like operating system, greatly minimizing the effort for creating and maintaining applications running inside the storage. We analyze the benefits of using Newport by running complex AI applications such as image similarity search and object tracking on a large visual dataset. The results demonstrate that data-intensive AI workloads can be efficiently parallelized and offloaded, even to a small set of Newport drives with significant performance gains and energy savings. In addition, we introduce a comprehensive taxonomy of existing computational storage solutions together with a realistic cost analysis for high-volume production, giving a good big picture of the economic feasibility of the computational storage technology.","2020-10","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","4","16","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Computational storage; in-storage processing; neural network; object tracking; similarity search; solid-state drive","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JEHXYZIC","journalArticle","2021","Avasalcai, Cosmin; Tsigkanos, Christos; Dustdar, Schahram","Adaptive Management of Volatile Edge Systems at Runtime With Satisfiability","ACM Trans. Internet Technol.","","1533-5399","10.1145/3470658","https://doi.org/10.1145/3470658","Edge computing offers the possibility of deploying applications at the edge of the network. To take advantage of available devices’ distributed resources, applications often are structured as microservices, often having stringent requirements of low latency and high availability. However, a decentralized edge system that the application may be intended for is characterized by high volatility, due to devices making up the system being unreliable or leaving the network unexpectedly. This makes application deployment and assurance that it will continue to operate under volatility challenging. We propose an adaptive framework capable of deploying and efficiently maintaining a microservice-based application at runtime, by tackling two intertwined problems: (i) finding a microservice placement across device hosts and (ii) deriving invocation paths that serve it. Our objective is to maintain correct functionality by satisfying given requirements in terms of end-to-end latency and availability, in a volatile edge environment. We evaluate our solution quantitatively by considering performance and failure recovery.","2021-09","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","1","22","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","edge computing; Resource management; distributed systems; adaptive systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"87E5PXYI","journalArticle","2023","Lin, Weiwei; Xiong, Chennian; Wu, Wentai; Shi, Fang; Li, Keqin; Xu, Minxian","Performance Interference of Virtual Machines: A Survey","ACM Comput. Surv.","","0360-0300","10.1145/3573009","https://doi.org/10.1145/3573009","The rapid development of cloud computing with virtualization technology has benefited both academia and industry. For any cloud data center at scale, one of the primary challenges is how to effectively orchestrate a large number of virtual machines (VMs) in a performance-aware and cost-effective manner. A key problem here is that the performance interference between VMs can significantly undermine the efficiency of cloud data centers, leading to performance degradation and additional operation cost. To address this issue, extensive studies have been conducted to investigate the problem from different aspects. In this survey, we make a comprehensive investigation into the causes of VM interference and provide an in-depth review of existing research and solutions in the literature. We first categorize existing studies on interference models according to their modeling objectives, metrics used, and modeling methods. Then we revisit interference-aware strategies for scheduling optimization as well as co-optimization-based approaches. Finally, the survey identifies open challenges with respect to VM interference in data centers and discusses possible research directions to provide insights for future research in the area.","2023-03","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","12","55","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Cloud data center; measuring and modeling; scheduling optimization; VM performance interference","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RU88JDLT","journalArticle","2025","Anderson, Patrick; Aranas, Erika; Assaf, Youssef; Behrendt, Raphael; Black, Richard; Caballero, Marco; Cameron, Pashmina; Canakci, Burcu; Chatzieleftheriou, Andromachi; Clarke, Rebekah; Clegg, James; Cletheroe, Daniel; Cooper, Bridgette; De Carvalho, Thales; Deegan, Tim; Donnelly, Austin; Drevinskas, Rokas; Gaunt, Alexander; Gkantsidis, Christos; Gomez Diaz, Ariel; Haller, Istvan; Hong, Freddie; Ilieva, Teodora; Joshi, Shashidhar; Joyce, Russell; Kunkel, William; Lara, David; Legtchenko, Sergey; Liu, Fanglin; Magalhaes, Bruno; Marzoev, Alana; McNett, Marvin; Mohan, Jayashree; Myrah, Michael; Nguyen, Truong; Nowozin, Sebastian; Ogus, Aaron; Overweg, Hiske; Rowstron, Antony; Sah, Maneesh; Sakakura, Masaaki; Scholtz, Peter; Schreiner, Nina; Sella, Omer; Smith, Adam; Stefanovici, Ioan; Sweeney, David; Thomsen, Benn; Verkes, Govert; Wainman, Phil; Westcott, Jonathan; Weston, Luke; Whittaker, Charles; Wilke Berenguer, Pablo; Williams, Hugh; Winkler, Thomas; Winzeck, Stefan","Project Silica: Towards Sustainable Cloud Archival Storage in Glass","ACM Trans. Storage","","1553-3077","10.1145/3708996","https://doi.org/10.1145/3708996","Sustainable and cost-effective long-term storage remains an unsolved problem. The most widely used storage technologies today are magnetic (hard disk drives and tape). They use media that degrades over time and has a limited lifetime, which leads to inefficient, wasteful, and costly solutions for long-lived data. This article presents Silica: the first cloud storage system for archival data underpinned by quartz glass, an extremely resilient media that allows data to be left in situ indefinitely. The hardware and software of Silica have been co-designed and co-optimized from the media up to the service level with sustainability as a primary objective. The design follows a cloud-first, data-driven methodology underpinned by principles derived from analyzing the archival workload of a large public cloud service. Silica can support a wide range of archival storage workloads and ushers in a new era of sustainable, cost-effective storage.","2025-02","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","1","21","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","data center; sustainability; archival; cloud storage; cold storage; disaggregation; glass; Storage","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QI7UAHLH","journalArticle","2019","Duc, Thang Le; Leiva, Rafael García; Casari, Paolo; Östberg, Per-Olov","Machine Learning Methods for Reliable Resource Provisioning in Edge-Cloud Computing: A Survey","ACM Comput. Surv.","","0360-0300","10.1145/3341145","https://doi.org/10.1145/3341145","Large-scale software systems are currently designed as distributed entities and deployed in cloud data centers. To overcome the limitations inherent to this type of deployment, applications are increasingly being supplemented with components instantiated closer to the edges of networks—a paradigm known as edge computing. The problem of how to efficiently orchestrate combined edge-cloud applications is, however, incompletely understood, and a wide range of techniques for resource and application management are currently in use.This article investigates the problem of reliable resource provisioning in joint edge-cloud environments, and surveys technologies, mechanisms, and methods that can be used to improve the reliability of distributed applications in diverse and heterogeneous network environments. Due to the complexity of the problem, special emphasis is placed on solutions to the characterization, management, and control of complex distributed applications using machine learning approaches. The survey is structured around a decomposition of the reliable resource provisioning problem into three categories of techniques: workload characterization and prediction, component placement and system consolidation, and application elasticity and remediation. Survey results are presented along with a problem-oriented discussion of the state-of-the-art. A summary of identified challenges and an outline of future research directions are presented to conclude the article.","2019-09","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","5","52","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","cloud computing; placement; machine learning; edge computing; optimization; autoscaling; consolidation; distributed systems; Reliability; remediation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KRL7VXZS","journalArticle","2025","Murillo, Juan M.; Garcia-Alonso, Jose; Moguel, Enrique; Barzen, Johanna; Leymann, Frank; Ali, Shaukat; Yue, Tao; Arcaini, Paolo; Pérez-Castillo, Ricardo; García Rodríguez de Guzmán, Ignacio; Piattini, Mario; Ruiz-Cortés, Antonio; Brogi, Antonio; Zhao, Jianjun; Miranskyy, Andriy; Wimmer, Manuel","Quantum Software Engineering: Roadmap and Challenges Ahead","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3712002","https://doi.org/10.1145/3712002","As quantum computers advance, the complexity of the software they can execute increases as well. To ensure this software is efficient, maintainable, reusable, and cost-effective —key qualities of any industry-grade software— mature software engineering practices must be applied throughout its design, development, and operation. However, the significant differences between classical and quantum software make it challenging to directly apply classical software engineering methods to quantum systems. This challenge has led to the emergence of Quantum Software Engineering as a distinct field within the broader software engineering landscape. In this work, a group of active researchers analyse in depth the current state of quantum software engineering research. From this analysis, the key areas of quantum software engineering are identified and explored in order to determine the most relevant open challenges that should be addressed in the next years. These challenges help identify necessary breakthroughs and future research directions for advancing Quantum Software Engineering.","2025-01","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","open challenges; QSE; Quantum Computing; Quantum Software Engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"83XLMJJR","journalArticle","2025","Bayram, Firas; Ahmed, Bestoun S.","Towards Trustworthy Machine Learning in Production: An Overview of the Robustness in MLOps Approach","ACM Comput. Surv.","","0360-0300","10.1145/3708497","https://doi.org/10.1145/3708497","Artificial intelligence (AI), and especially its sub-field of Machine Learning (ML), are impacting the daily lives of everyone with their ubiquitous applications. In recent years, AI researchers and practitioners have introduced principles and guidelines to build systems that make reliable and trustworthy decisions. From a practical perspective, conventional ML systems process historical data to extract the features that are consequently used to train ML models that perform the desired task. However, in practice, a fundamental challenge arises when the system needs to be operationalized and deployed to evolve and operate in real-life environments continuously. To address this challenge, Machine Learning Operations (MLOps) have emerged as a potential recipe for standardizing ML solutions in deployment. Although MLOps demonstrated great success in streamlining ML processes, thoroughly defining the specifications of robust MLOps approaches remains of great interest to researchers and practitioners. In this paper, we provide a comprehensive overview of the trustworthiness property of MLOps systems. Specifically, we highlight technical practices to achieve robust MLOps systems. In addition, we survey the existing research approaches that address the robustness aspects of ML systems in production. We also review the tools and software available to build MLOps systems and summarize their support to handle the robustness aspects. Finally, we present the open challenges and propose possible future directions and opportunities within this emerging field. The aim of this paper is to provide researchers and practitioners working on practical AI applications with a comprehensive view to adopt robust ML solutions in production environments.","2025-01","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","5","57","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","machine learning; Artificial intelligence; DataOps; MLOps systems; model performance; ModelOps; robustness; Trustworthy AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G4J54N7G","journalArticle","2024","Gilman, Ekaterina; Bugiotti, Francesca; Khalid, Ahmed; Mehmood, Hassan; Kostakos, Panos; Tuovinen, Lauri; Ylipulli, Johanna; Su, Xiang; Ferreira, Denzil","Addressing Data Challenges to Drive the Transformation of Smart Cities","ACM Trans. Intell. Syst. Technol.","","2157-6904","10.1145/3663482","https://doi.org/10.1145/3663482","Cities serve as vital hubs of economic activity and knowledge generation and dissemination. As such, cities bear a significant responsibility to uphold environmental protection measures while promoting the welfare and living comfort of their residents. There are diverse views on the development of smart cities, from integrating Information and Communication Technologies into urban environments for better operational decisions to supporting sustainability, wealth, and comfort of people. However, for all these cases, data are the key ingredient and enabler for the vision and realization of smart cities. This article explores the challenges associated with smart city data. We start with gaining an understanding of the concept of a smart city, how to measure that the city is a smart one, and what architectures and platforms exist to develop one. Afterwards, we research the challenges associated with the data of the cities, including availability, heterogeneity, management, analysis, privacy, and security. Finally, we discuss ethical issues. This article aims to serve as a “one-stop shop” covering data-related issues of smart cities with references for diving deeper into particular topics of interest.","2024-11","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","5","15","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","machine learning; smart city; Big data; data analysis; urban computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N29GR4KF","journalArticle","2018","Andersen, Michael P.; Kolb, John; Chen, Kaifei; Fierro, Gabe; Culler, David E.; Katz, Randy","Democratizing Authority in the Built Environment","ACM Trans. Sen. Netw.","","1550-4859","10.1145/3199665","https://doi.org/10.1145/3199665","Operating systems and applications in the built environment have relied upon central authorization and management mechanisms that restrict their scalability, especially with respect to administrative overhead. We propose a new set of primitives encompassing syndication, security, and service execution that unifies the management of applications and services across the built environment, while enabling participants to individually delegate privilege across multiple administrative domains with no loss of security or manageability. We show how to leverage a decentralized authorization syndication platform to extend the design of building operating systems beyond the single administrative domain of a building. The authorization system leveraged is based on blockchain smart contracts to permit decentralized and democratized delegation of authorization without central trust. Upon this, a publish/subscribe syndication tier and a containerized service execution environment are constructed. Combined, these mechanisms solve problems of delegation, federation, device protection and service execution that arise throughout the built environment. We leverage a high-fidelity city-scale emulation to verify the scalability of the authorization tier, and briefly describe a prototypical democratized operating system for the built environment using this foundation.This is an extension of work presented in Ref.&nbsp;[3].","2018-12","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","3–4","14","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","microservices; Built environment; federation; syndication","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"76BCY46Z","journalArticle","2024","Nickel, Matthias; Göhringer, Diana","A Survey on Architectures, Hardware Acceleration and Challenges for In-Network Computing","ACM Trans. Reconfigurable Technol. Syst.","","1936-7406","10.1145/3699514","https://doi.org/10.1145/3699514","By moving data and computation away from the end user to more powerful servers in the cloud or to cloudlets at the edge, end user devices only need to compute locally for small amounts of data and when low latency is required. However, with the advent of 6G and Internet-of-Everything, the demand for more powerful networks continues to grow. The introduction of Software-Defined Networking and Network Function Virtualization has allowed us to rethink networks and use them for more than just routing data to servers. In addition, the use of more powerful network devices is bringing new life to the concept of active networks in the form of in-network computing. In-Network Computing provides the ability to move applications into the network and process data on programmable network devices as they are transmitted. In this work, we provide an overview of in-network computing and its enabling technologies. We take a look at the programmability and different hardware architectures for SmartNICs and switches, focusing primarily on accelerators such as FPGAs. We discuss the state of the art and challenges in this area, and look at CGRAs, a class of hardware accelerators that have not been widely discussed in this context.","2024-12","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","1","18","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","CGRA; FPGA; In-Network Computing; SoC; Software-Defined Networking","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EAI35HRC","journalArticle","2019","Ren, Ju; Zhang, Deyu; He, Shiwen; Zhang, Yaoxue; Li, Tao","A Survey on End-Edge-Cloud Orchestrated Network Computing Paradigms: Transparent Computing, Mobile Edge Computing, Fog Computing, and Cloudlet","ACM Comput. Surv.","","0360-0300","10.1145/3362031","https://doi.org/10.1145/3362031","Sending data to the cloud for analysis was a prominent trend during the past decades, driving cloud computing as a dominant computing paradigm. However, the dramatically increasing number of devices and data traffic in the Internet-of-Things (IoT) era are posing significant burdens on the capacity-limited Internet and uncontrollable service delay. It becomes difficult to meet the delay-sensitive and context-aware service requirements of IoT applications by using cloud computing alone. Facing these challenges, computing paradigms are shifting from the centralized cloud computing to distributed edge computing. Several new computing paradigms, including Transparent Computing, Mobile Edge Computing, Fog Computing, and Cloudlet, have emerged to leverage the distributed resources at network edge to provide timely and context-aware services. By integrating end devices, edge servers, and cloud, they form a hierarchical IoT architecture, i.e., End-Edge-Cloud orchestrated architecture to improve the performance of IoT systems. This article presents a comprehensive survey of these emerging computing paradigms from the perspective of end-edge-cloud orchestration. Specifically, we first introduce and compare the architectures and characteristics of different computing paradigms. Then, a comprehensive survey is presented to discuss state-of-the-art research in terms of computation offloading, caching, security, and privacy. Finally, some potential research directions are envisioned for fostering continuous research efforts.","2019-10","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","6","52","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","fog computing; cloudlet; End-edge-cloud orchestration; mobile edge computing; network computing; transparent computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K9AZR7LH","journalArticle","2023","Li, Jing; Guo, Song; Liang, Weifa; Wang, Jianping; Chen, Quan; Xu, Zichuan; Xu, Wenzheng","AoI-Aware User Service Satisfaction Enhancement in Digital Twin-Empowered Edge Computing","IEEE/ACM Trans. Netw.","","1063-6692","10.1109/TNET.2023.3324704","https://doi.org/10.1109/TNET.2023.3324704","The emerging digital twin technique enhances the network management efficiency and provides comprehensive insights on network performance, through mapping physical objects to their digital twins. The user satisfaction on digital twin-enabled service relies on the freshness of digital twin data, which is measured by the Age of Information (AoI). Due to long service delays, the use of the remote cloud for delay-sensitive service provisioning faces serious challenges. Mobile Edge Computing (MEC), as an ideal paradigm for delay-sensitive services, is able to realize real-time data communication between physical objects and their digital twins at the network edge. However, the mobility of physical objects and dynamics of user query arrivals make seamless service provisioning in MEC become challenging. In this paper, we investigate dynamic digital twin placements for improving user service satisfaction in MEC environments, by introducing a novel metric to measure user service satisfaction based on the AoI concept and formulating two user service satisfaction enhancement problems: the static and dynamic utility maximization problems under static and dynamic digital twin placement schemes. To this end, we first formulate an Integer Linear Programming (ILP) solution to the static utility maximization problem when the problem size is small; otherwise, we propose a performance-guaranteed approximation algorithm. We then propose an online algorithm with a provable competitive ratio for the dynamic utility maximization problem, by considering dynamic user query services. Finally, we evaluate the performance of the proposed algorithms via simulations. Simulation results demonstrate that the proposed algorithms outperform the comparison baseline algorithms, improving the algorithm performance by at least 10.7%, compared to the baseline algorithms.","2023-10","2025-03-18 09:07:49","2025-03-18 09:07:49","","1677–1690","","2","32","","","","","","","","","","","","","","","","","Publisher: IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3R3CMLIG","journalArticle","2024","Bellavista, Paolo; Bicocchi, Nicola; Fogli, Mattia; Giannelli, Carlo; Mamei, Marco; Picone, Marco","An Entanglement-Aware Middleware for Digital Twins","ACM Trans. Internet Things","","","10.1145/3699520","https://doi.org/10.1145/3699520","The development of the Digital Twin (DT) approach is tilting research from initial approaches that aim at promoting early adoption to sophisticated attempts to develop, deploy, and maintain applications based on DTs. In this context, we propose a highly dynamic and distributed ecosystem where containerized DTs co-evolve with an orchestration middleware. DTs provide digitalized representations of the targeted physical systems, while the orchestration middleware monitors and re-configures the deployed DTs in light of application constraints, available resources, and the quality of cyber-physical entanglement. First, we lay out the reference scenario. Then, we discuss the limitations of current approaches and identify a set of requirements that shape both DTs and the orchestration middleware. Subsequently, we describe a blueprint architecture that meets those requirements. Finally, we report empirical evidence on both the feasibility and the effectiveness of a proof-of-concept implementation of the proposed ecosystem.","2024-11","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","4","5","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","digital twins; middleware; Cyber-physical systems; entanglement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HESR4GAZ","journalArticle","2022","Paleyes, Andrei; Urma, Raoul-Gabriel; Lawrence, Neil D.","Challenges in Deploying Machine Learning: A Survey of Case Studies","ACM Comput. Surv.","","0360-0300","10.1145/3533378","https://doi.org/10.1145/3533378","In recent years, machine learning has transitioned from a field of academic research interest to a field capable of solving real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries, and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. By mapping found challenges to the steps of the machine learning deployment workflow, we show that practitioners face issues at each stage of the deployment process. The goal of this article is to lay out a research agenda to explore approaches addressing these challenges.","2022-12","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","6","55","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Machine learning applications; sofware deployment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TI8FBBZA","journalArticle","2024","Rac, Samuel; Brorsson, Mats","Cost-aware Service Placement and Scheduling in the Edge-Cloud Continuum","ACM Trans. Archit. Code Optim.","","1544-3566","10.1145/3640823","https://doi.org/10.1145/3640823","The edge to data center computing continuum is the aggregation of computing resources located anywhere between the network edge (e.g., close to 5G antennas), and servers in traditional data centers. Kubernetes is the de facto standard for the orchestration of services in data center environments, where it is very efficient. It, however, fails to give the same performance when including edge resources. At the edge, resources are more limited, and networking conditions are changing over time.In this article, we present a methodology that lowers the costs of running applications in the edge-to-cloud computing continuum. This methodology can adapt to changing environments, e.g., moving end-users. We are also monitoring some Key Performance Indicators of the applications to ensure that cost optimizations do not negatively impact their Quality of Service. In addition, to ensure that performances are optimal even when users are moving, we introduce a background process that periodically checks if a better location is available for the service and, if so, moves the service. To demonstrate the performance of our scheduling approach, we evaluate it using a vehicle cooperative perception use case, a representative 5G application. With this use case, we can demonstrate that our scheduling approach can robustly lower the cost in different scenarios, while other approaches that are already available fail in either being adaptive to changing environments or will have poor cost-effectiveness in some scenarios.","2024-03","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","2","21","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Cloud computing; resource allocation; scheduling; Kubernetes; edge computing; 5G; container orchestration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SA7YCYB5","journalArticle","2023","Hamissi, Asma; Dhraief, Amine","A Survey on the Unmanned Aircraft System Traffic Management","ACM Comput. Surv.","","0360-0300","10.1145/3617992","https://doi.org/10.1145/3617992","The Unmanned Aircraft System Traffic Management (UTM) system is a set of services offering an automated management of the airspace and thus providing safe and secure Unmanned Aerial Vehicle (UAV) flights in both controlled and uncontrolled airspace. Controlled airspace refers to the portion of the airspace that is under the authority of Air Traffic Control (ATC) and where separation services are offered, while uncontrolled airspace refers to the portion of airspace where aircraft are not regulated by ATC. This article is a comprehensive survey of the existing UTMs development efforts with a focus on the different UTMs architectures, the provided services, the used communication technologies and the decision-making process within UTMs. We firstly review the different UTM architecture and propose a novel UTM taxonomy based on high-level qualitative criteria. Secondly, we detail the services provided by UTMs with an emphasis on the used technologies in the identification, the surveillance, the monitoring, and the deconfliction services. Effective decision-making is crucial, particularly in emergency scenarios such as Air-to-Ground (A2G) communication loss, battery or motor malfunction, or encountering aerial obstacles, among other potential hazards. Despite its significance, the UTM decision-making process is not enough considered in the literature and especially in UTM surveys. We analyze and compare in this article both the centralized and decentralized UTM decision-making. Centralized decision-making is not conducted in real-time and primarily relies on Air-to-Ground (A2G) communication. In the decentralized case, the decision-making process primarily relies on communication and collaboration among UAVs with varying degrees of autonomy. We show in this paper that centralized decision-making may encounter issues with packet loss and imperfect data, which can negatively impact the quality of decision-making. We also highlight that the decentralized decision-making may also face challenges related to security and scalability, which can hinder its effectiveness. Finally, evaluating the performance of UTMs on a real environment raises several challenges and the simulation is a cost-effective alternative. Hence, we provide a summary of the existing UTMs simulators and discuss their main features.","2023-10","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","3","56","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","performance; architecture; services; UAV; communication; decision-making; simulators; UTM","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X6MLN4RV","journalArticle","2023","Ma, Xueyan","The Application Research of Distributed Interface Coordination Based on Deep Reinforcement Learning in English MOOC Platform","ACM Trans. Asian Low-Resour. Lang. Inf. Process.","","2375-4699","10.1145/3614428","https://doi.org/10.1145/3614428","Developing information technology and other business management firms is not the same as technological advancement. The only way we can declare that complete technical progress is in place and operating well in the nation is if the education system has been built. The educational system is set up to instruct pupils via wireless technologies and online classes. Advancement in online classes has resulted in the Massive Open Online Courses (MOOC) Platform. The advantage of the MOOC platform is that it is a platform that provides free online courses for anyone to register and continue the course at their own pace. On this MOOC platform, implementing Deep Reinforcement Learning (DRL) aids in choosing the course and learning through an intelligent mechanism. The transmission between students and teachers will significantly rise because of this teaching strategy through the Distributed Interface Coordination (DIC) mechanism where the resources are distributed in the wireless networking environment. Additionally, the registered students should possess privileges to access the resources. This combination of DIC with DRL can be termed as DIC-DRL model for the proposed system. The data was collected using a low packet rate and outdated wireless connection technologies. The suggested model has a 97.67 percent accuracy rate in comparison with the existing model's support vector machine (SVM) mechanism.","2023-10","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Deep Reinforcement Learning (DRL); DIC-DRL model; Distributed Interface Coordination (DIC); English Learning; intelligent mechanism; Massive Open Online Courses (MOOC) Platform; Support Vector Machine (SVM); Wireless networking environment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3DRRY8ZV","journalArticle","2022","Korala, Harindu; Georgakopoulos, Dimitrios; Jayaraman, Prem Prakash; Yavari, Ali","A Survey of Techniques for Fulfilling the Time-Bound Requirements of Time-Sensitive IoT Applications","ACM Comput. Surv.","","0360-0300","10.1145/3510411","https://doi.org/10.1145/3510411","This article surveys existing techniques for meeting the time-bound requirements of time-sensitive applications in the Internet of Things (IoT). To provide the foundation for identifying and classifying relevant techniques, we present three sample time-sensitive IoT applications and their time-bound requirements, describe the main computation and network resources in IoT that can be used to process such applications, and identify the main challenges in meeting their time-bound requirements. Based on these, the article presents a comprehensive literature review of existing techniques and tools that can help meet application-specific time-bound requirements in IoT. The article also includes a gap analysis in existing research outcomes and proposes research directions for bridging the remaining research gaps in supporting time-sensitive IoT applications.","2022-09","2025-03-18 09:07:49","2025-03-18 09:07:49","","","","11s","54","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Internet of Things (IoT); time sensitive; time-bound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B7XT8G2L","journalArticle","2019","Merlino, Giovanni; Dautov, Rustem; Distefano, Salvatore; Bruneo, Dario","Enabling Workload Engineering in Edge, Fog, and Cloud Computing through OpenStack-based Middleware","ACM Trans. Internet Technol.","","1533-5399","10.1145/3309705","https://doi.org/10.1145/3309705","To enable and support smart environments, a recent ICT trend promotes pushing computation from the remote Cloud as close to data sources as possible, resulting in the emergence of the Fog and Edge computing paradigms. Together with Cloud computing, they represent a stacked architecture, in which raw datasets are first pre-processed locally at the Edge and then vertically offloaded to the Fog and/or the Cloud. However, as hardware is becoming increasingly powerful, Edge devices are seen as candidates for offering data processing capabilities, able to pool and share computing resources to achieve better performance at a lower network latency—a pattern that can be also applied to Fog nodes. In these circumstances, it is important to enable efficient, intelligent, and balanced allocation of resources, as well as their further orchestration, in an elastic and transparent manner. To address such a requirement, this article proposes an OpenStack-based middleware platform through which resource containers at the Edge, Fog, and Cloud levels can be discovered, combined, and provisioned to end users and applications, thereby facilitating and orchestrating offloading processes. As demonstrated through a proof of concept on an intelligent surveillance system, by converging the Edge, Fog, and Cloud, the proposed architecture has the potential to enable faster data processing, as compared to processing at the Edge, Fog, or Cloud levels separately. This also allows architects to combine different offloading patterns in a flexible and fine-grained manner, thus providing new workload engineering patterns. Measurements demonstrated the effectiveness of such patterns, even outperforming edge clusters.","2019-04","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","2","19","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","cloud computing; edge; IoT; Fog; big data; offloading; smart environment; Stack4Things","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IILSCIAT","journalArticle","2018","Liu, Xuanzhe; Yu, Meihua; Ma, Yun; Huang, Gang; Mei, Hong; Liu, Yunxin","i-Jacob: An Internetware-Oriented Approach to Optimizing Computation-Intensive Mobile Web Browsing","ACM Trans. Internet Technol.","","1533-5399","10.1145/3093899","https://doi.org/10.1145/3093899","Web browsing is always a key requirement of Internet users. Current mobile Web apps can contain computation-intensive JavaScript logics and thus affect browsing performance. Learning from our over-decade research and development experiences of the Internetware paradigm, we present the novel and generic i-Jacob approach to improving the performance of mobile Web browsing with effective JavaScript-code offloading. Our approach proposes a programming abstraction to make mobile Web situational and adaptive to contexts, by specifying the computation-intensive and “ offloadable ” code, and develops a platform-independent lightweight runtime spanning the mobile devices and the cloud. We demonstrate the efficiency of i-Jacob with some typical computation-intensive tasks over various combinations of hardware, operating systems, browsers, and network connections. The improvements can reach up to 49× speed-up in response time and 90% saving in energy.","2018-03","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","2","18","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","performance; energy; JavaScript offloading; Mobile Web","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZJJHYLHZ","journalArticle","2023","Lubbers, Mart; Koopman, Pieter; Ramsingh, Adrian; Singer, Jeremy; Trinder, Phil","Could Tierless Languages Reduce IoT Development Grief?","ACM Trans. Internet Things","","","10.1145/3572901","https://doi.org/10.1145/3572901","Internet of Things (IoT) software is notoriously complex, conventionally comprising multiple tiers. Traditionally an IoT developer must use multiple programming languages and ensure that the components interoperate correctly. A novel alternative is to use a single tierless language with a compiler that generates the code for each component and ensures their correct interoperation.We report a systematic comparative evaluation of two tierless language technologies for IoT stacks: one for resource-rich sensor nodes (Clean with iTask) and one for resource-constrained sensor nodes (Clean with iTask and mTask). The evaluation is based on four implementations of a typical smart campus application: two tierless and two Python-based tiered.(1) We show that tierless languages have the potential to significantly reduce the development effort for IoT systems, requiring 70% less code than the tiered implementations. Careful analysis attributes this code reduction to reduced interoperation (e.g., two embedded domain-specific languages and one paradigm versus seven languages and two paradigms), automatically generated distributed communication, and powerful IoT programming abstractions. (2) We show that tierless languages have the potential to significantly improve the reliability of IoT systems, describing how Clean iTask/mTask maintains type safety, provides higher-order failure management, and simplifies maintainability. (3) We report the first comparison of a tierless IoT codebase for resource-rich sensor nodes with one for resource-constrained sensor nodes. The comparison shows that they have similar code size (within 7%), and functional structure. (4) We present the first comparison of two tierless IoT languages, one for resource-rich sensor nodes and the other for resource-constrained sensor nodes.","2023-02","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","1","4","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","IoT stacks; Tierless languages","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DR75H26L","journalArticle","2022","Mampage, Anupama; Karunasekera, Shanika; Buyya, Rajkumar","A Holistic View on Resource Management in Serverless Computing Environments: Taxonomy and Future Directions","ACM Comput. Surv.","","0360-0300","10.1145/3510412","https://doi.org/10.1145/3510412","Serverless computing has emerged as an attractive deployment option for cloud applications in recent times. The unique features of this computing model include rapid auto-scaling, strong isolation, fine-grained billing options, and access to a massive service ecosystem, which autonomously handles resource management decisions. This model is increasingly being explored for deployments in geographically distributed edge and fog computing networks as well, due to these characteristics. Effective management of computing resources has always gained a lot of attention among researchers. The need to automate the entire process of resource provisioning, allocation, scheduling, monitoring, and scaling has resulted in the need for specialized focus on resource management under the serverless model. In this article, we identify the major aspects covering the broader concept of resource management in serverless environments and propose a taxonomy of elements that influence these aspects, encompassing characteristics of system design, workload attributes, and stakeholder expectations. We take a holistic view on serverless environments deployed across edge, fog, and cloud computing networks. We also analyse existing works discussing aspects of serverless resource management using this taxonomy. This article further identifies gaps in literature and highlights future research directions for improving capabilities of this computing model.","2022-09","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","11s","54","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","workload characterization; resource management; resource scheduling; application modelling; performance prediction; resource scaling; Serverless computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"23KXYZBU","journalArticle","2022","Wang, Minhu; Xu, Mingwei; Wu, Jianping","Understanding I/O Direct Cache Access Performance for End Host Networking","Proc. ACM Meas. Anal. Comput. Syst.","","","10.1145/3508042","https://doi.org/10.1145/3508042","Direct Cache Access (DCA) enables a network interface card (NIC) to load and store data directly on the processor cache, as conventional Direct Memory Access (DMA) is no longer suitable as the bridge between NIC and CPU in the era of 100 Gigabit Ethernet. As numerous I/O devices and cores compete for scarce cache resources, making the most of DCA for networking applications with varied objectives and constraints is a challenge, especially given the increasing complexity of modern cache hardware and I/O stacks. In this paper, we reverse engineer details of one commercial implementation of DCA, Intel's Data Direct I/O (DDIO), to explicate the importance of hardware-level investigation into DCA. Based on the learned knowledge of DCA and network I/O stacks, we (1) develop an analytical framework to predict the effectiveness of DCA (i.e., its hit rate) under certain hardware specifications, system configurations, and application properties; (2) measure penalties of the ineffective use of DCA (i.e., its miss penalty) to characterize its benefits; and (3) show that our reverse engineering, measurement, and model contribute to a deeper understanding of DCA, which in turn helps diagnose, optimize, and design end-host networking.","2022-02","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","1","6","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","direct cache access; end host networking","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QTPWJAEG","journalArticle","2022","Shafiei, Hossein; Khonsari, Ahmad; Mousavi, Payam","Serverless Computing: A Survey of Opportunities, Challenges, and Applications","ACM Comput. Surv.","","0360-0300","10.1145/3510611","https://doi.org/10.1145/3510611","The emerging serverless computing paradigm has attracted attention from both academia and industry. This paradigm brings benefits such as less operational complexity, a pay-as-you-go pricing model, and an auto-scaling feature. The paradigm opens up new opportunities and challenges for cloud application developers. In this article, we present a comprehensive overview of the past development as well as the recent advances in research areas related to serverless computing. First, we survey serverless applications introduced in the literature. We categorize applications in eight domains and separately discuss the objectives and the viability of the serverless paradigm along with challenges in each of those domains. We then classify those challenges into nine topics and survey the proposed solutions. Finally, we present the areas that need further attention from the research community and identify open problems.","2022-11","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","11s","54","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","serverless computing; Cloud services; function-as-a-service (FaaS)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8H4EWH56","journalArticle","2025","Jarkas, Omar; Ko, Ryan; Dong, Naipeng; Mahmud, Redowan","A Container Security Survey: Exploits, Attacks, and Defenses","ACM Comput. Surv.","","0360-0300","10.1145/3715001","https://doi.org/10.1145/3715001","Containerization significantly boosts cloud computing efficiency by reducing resource consumption, enhancing scalability, and simplifying orchestration. Yet, these same features introduce notable security vulnerabilities due to the shared Linux kernel and reduced isolation compared to traditional virtual machines (VMs). This architecture, while resource-efficient, increases susceptibility to software vulnerabilities, exposing containers to potential breaches; a single kernel vulnerability could compromise all containers on the same host. Existing academic research on container security is often theoretical and lacks empirical data on the nature of attacks, exploits, and vulnerabilities. Studies that do look at vulnerabilities often focus on specific types. This lack of detailed data and breadth hampers the development of effective mitigation strategies and restricts insights into the inherent weaknesses of containers. To address these gaps, our study introduces a novel taxonomy integrating academic knowledge with industry insights and real-world vulnerabilities, creating a comprehensive and actionable framework for container security. We analyzed over 200 container-related vulnerabilities, categorizing them into 47 exploit types across 11 distinct attack vectors. This taxonomy not only advances theoretical understanding but also facilitates the identification of vulnerabilities and the implementation of effective mitigation strategies in containerized environments. Our approach enhances the resilience of these environments by mapping vulnerabilities to their corresponding exploits and mitigation strategies, especially in complex, multi-tenant cloud settings. By providing actionable insights, our taxonomy helps practitioners enhance container security. Our findings have identified critical areas for further investigation, thereby laying a comprehensive foundation for future research and improving container security in cloud environments.","2025-02","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","7","57","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","cloud computing; confidential computing; Containerization security; hardware-based security; vulnerabilities","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GT85EXPX","journalArticle","2019","Puliafito, Carlo; Mingozzi, Enzo; Longo, Francesco; Puliafito, Antonio; Rana, Omer","Fog Computing for the Internet of Things: A Survey","ACM Trans. Internet Technol.","","1533-5399","10.1145/3301443","https://doi.org/10.1145/3301443","Research in the Internet of Things (IoT) conceives a world where everyday objects are connected to the Internet and exchange, store, process, and collect data from the surrounding environment. IoT devices are becoming essential for supporting the delivery of data to enable electronic services, but they are not sufficient in most cases to host application services directly due to their intrinsic resource constraints. Fog Computing (FC) can be a suitable paradigm to overcome these limitations, as it can coexist and cooperate with centralized Cloud systems and extends the latter toward the network edge. In this way, it is possible to distribute resources and services of computing, storage, and networking along the Cloud-to-Things continuum. As such, FC brings all the benefits of Cloud Computing (CC) closer to end (user) devices. This article presents a survey on the employment of FC to support IoT devices and services. The principles and literature characterizing FC are described, highlighting six IoT application domains that may benefit from the use of this paradigm. The extension of Cloud systems towards the network edge also creates new challenges and can have an impact on existing approaches employed in Cloud-based deployments. Research directions being adopted by the community are highlighted, with an indication of which of these are likely to have the greatest impact. An overview of existing FC software and hardware platforms for the IoT is also provided, along with the standardisation efforts in this area initiated by the OpenFog Consortium (OFC).","2019-04","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","2","19","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","cloud computing; Fog computing; internet of things; topological proximity","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HW42IPF5","journalArticle","2021","Sobhy, Dalia; Bahsoon, Rami; Minku, Leandro; Kazman, Rick","Evaluation of Software Architectures under Uncertainty: A Systematic Literature Review","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3464305","https://doi.org/10.1145/3464305","Context: Evaluating software architectures in uncertain environments raises new challenges, which require continuous approaches. We define continuous evaluation as multiple evaluations of the software architecture that begins at the early stages of the development and is periodically and repeatedly performed throughout the lifetime of the software system. Numerous approaches have been developed for continuous evaluation; to handle dynamics and uncertainties at run-time, over the past years, these approaches are still very few, limited, and lack maturity. Objective: This review surveys efforts on architecture evaluation and provides a unified terminology and perspective on the subject. Method: We conducted a systematic literature review to identify and analyse architecture evaluation approaches for uncertainty including continuous and non-continuous, covering work published between 1990–2020. We examined each approach and provided a classification framework for this field. We present an analysis of the results and provide insights regarding open challenges. Major results and conclusions: The survey reveals that most of the existing architecture evaluation approaches typically lack an explicit linkage between design-time and run-time. Additionally, there is a general lack of systematic approaches on how continuous architecture evaluation can be realised or conducted. To remedy this lack, we present a set of necessary requirements for continuous evaluation and describe some examples.","2021-08","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","4","30","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Continuous software architecture evaluation; design-time software architecture evaluation; run-time software architecture evaluation; uncertainty","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SVUYPYSL","journalArticle","2024","Tang, Li; Hu, Haibo; Gabbouj, Moncef; Ye, Qingqing; Xiang, Yang; Li, Jin; Li, Lang","A Survey on Securing Image-Centric Edge Intelligence","ACM Trans. Multimedia Comput. Commun. Appl.","","1551-6857","10.1145/3700792","https://doi.org/10.1145/3700792","Facing enormous data generated at the network edge, Edge Intelligence (EI) emerges as the fusion of Edge Computing and Artificial Intelligence, revolutionizing edge data processing and intelligent decision-making. Nonetheless, this emergent mode presents a complex array of security challenges, particularly prominent in image-centric applications due to the sheer volume of visual data and its direct connection to user privacy. These challenges include safeguarding model/image privacy and ensuring model integrity against various security threats, such as model poisoning. Essentially, those threats originate from data attacks, suggesting data protection as a promising solution. Although data protection measures are well-established in other domains, image-centric EI necessitates focused research. This survey examines the security issues inherent to image-centric EI and outlines the protection efforts, providing a comprehensive overview of the landscape. We begin by introducing EI, detailing its operational mechanics and associated security issues. We then explore the technologies facilitating security enhancement (e.g., differential privacy) and edge intelligence (e.g., compact networks and distributed learning frameworks). Next, we categorize security strategies by their application in data preparation, training, and inference, with a focus on image-based contexts. Despite these efforts on security, our investigation identifies research gaps. We also outline promising research directions to bridge these gaps, bolstering security frameworks in image-centric EI applications.","2024-12","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HQFY5GPA","journalArticle","2021","Gheibi, Omid; Weyns, Danny; Quin, Federico","Applying Machine Learning in Self-adaptive Systems: A Systematic Literature Review","ACM Trans. Auton. Adapt. Syst.","","1556-4665","10.1145/3469440","https://doi.org/10.1145/3469440","Recently, we have been witnessing a rapid increase in the use of machine learning techniques in self-adaptive systems. Machine learning has been used for a variety of reasons, ranging from learning a model of the environment of a system during operation to filtering large sets of possible configurations before analyzing them. While a body of work on the use of machine learning in self-adaptive systems exists, there is currently no systematic overview of this area. Such an overview is important for researchers to understand the state of the art and direct future research efforts. This article reports the results of a systematic literature review that aims at providing such an overview. We focus on self-adaptive systems that are based on a traditional Monitor-Analyze-Plan-Execute (MAPE)-based feedback loop. The research questions are centered on the problems that motivate the use of machine learning in self-adaptive systems, the key engineering aspects of learning in self-adaptation, and open challenges in this area. The search resulted in 6,709 papers, of which 109 were retained for data collection. Analysis of the collected data shows that machine learning is mostly used for updating adaptation rules and policies to improve system qualities, and managing resources to better balance qualities and resources. These problems are primarily solved using supervised and interactive learning with classification, regression, and reinforcement learning as the dominant methods. Surprisingly, unsupervised learning that naturally fits automation is only applied in a small number of studies. Key open challenges in this area include the performance of learning, managing the effects of learning, and dealing with more complex types of goals. From the insights derived from this systematic literature review, we outline an initial design process for applying machine learning in self-adaptive systems that are based on MAPE feedback loops.","2021-08","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","3","15","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Self-adaptation; feedback loops; MAPE-K","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CT5GWD8B","journalArticle","2020","Welsh, Thomas; Benkhelifa, Elhadj","On Resilience in Cloud Computing: A Survey of Techniques across the Cloud Domain","ACM Comput. Surv.","","0360-0300","10.1145/3388922","https://doi.org/10.1145/3388922","Cloud infrastructures are highly favoured as a computing delivery model worldwide, creating a strong societal dependence. It is therefore vital to enhance their resilience, providing persistent service delivery under a variety of conditions. Cloud environments are highly complex and continuously evolving. Additionally, the plethora of use-cases ensures requirements for persistent service delivery vary. As a contribution to knowledge, this work surveys resilience techniques for cloud environments. We apply a novel perspective using a layered model of traditional and emerging cloud paradigms. Works are then classified according to the Resilinets model. For each layer, the most common techniques with limitations are derived including an actor’s strength in influencing resilience in the cloud with each technique. We conclude with some future challenges to the field of resilient cloud computing.","2020-05","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","3","53","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","cloud; edge; survey; fog; Resilience","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EFGHSAZT","journalArticle","2023","Weyns, Danny; Gerostathopoulos, Ilias; Abbas, Nadeem; Andersson, Jesper; Biffl, Stefan; Brada, Premek; Bures, Tomas; Di Salle, Amleto; Galster, Matthias; Lago, Patricia; Lewis, Grace; Litoiu, Marin; Musil, Angelika; Musil, Juergen; Patros, Panos; Pelliccione, Patrizio","Self-Adaptation in Industry: A Survey","ACM Trans. Auton. Adapt. Syst.","","1556-4665","10.1145/3589227","https://doi.org/10.1145/3589227","Computing systems form the backbone of many areas in our society, from manufacturing to traffic control, healthcare, and financial systems. When software plays a vital role in the design, construction, and operation, these systems are referred to as software-intensive systems. Self-adaptation equips a software-intensive system with a feedback loop that either automates tasks that otherwise need to be performed by human operators or deals with uncertain conditions. Such feedback loops have found their way to a variety of practical applications; typical examples are an elastic cloud to adapt computing resources and automated server management to respond quickly to business needs. To gain insight into the motivations for applying self-adaptation in practice, the problems solved using self-adaptation and how these problems are solved, and the difficulties and risks that industry faces in adopting self-adaptation, we performed a large-scale survey. We received 184 valid responses from practitioners spread over 21 countries. Based on the analysis of the survey data, we provide an empirically grounded overview the of state of the practice in the application of self-adaptation. From that, we derive insights for researchers to check their current research with industrial needs, and for practitioners to compare their current practice in applying self-adaptation. These insights also provide opportunities for applying self-adaptation in practice and pave the way for future industry-research collaborations.","2023-05","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","2","18","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Self-adaptation; survey; industry","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2BR6K7VB","journalArticle","2024","Ye, Peigen; Wang, Wenfeng; Mi, Bing; Chen, Kongyang","EdgeStreaming: Secure Computation Intelligence in Distributed Edge Networks for Streaming Analytics","ACM Trans. Multimedia Comput. Commun. Appl.","","1551-6857","10.1145/3686161","https://doi.org/10.1145/3686161","In modern information systems, real-time streaming data are generated in various vertical application scenarios, such as industrial security cameras, household intelligent devices, mobile robots and among others. However, these low-end devices can hardly provide real-time and accurate data analysis functionalities due to their limited on-board performances. Traditional centralized server computing also suffers from its prolonged transmission latency, resulting in huge response time. To deal with this problem, this paper presents a novel distributed computation intelligent system with nearby edge devices, abbreviated as EdgeStreaming, to facilitate rapid and accurate analysis of streaming data. Firstly, we thoroughly explore the available edge devices surrounding the terminal to generate an internally interconnected edge network. This edge network real-time perceives and updates the internal resource status of each edge device, such as computational and storage resources. Dynamic allocation of external computational or storage demands can be made based on the current load of individual edge devices. Consequently, the streaming data perceived by external terminal devices can be transmitted in real-time to any edge gateway. The edge network employs a well-designed task scheduling strategy to partition and allocate streaming data processing demands to one or multiple edge devices. Additionally, it customizes computational requirements judiciously, for instance, by utilizing model compression to expedite computation speed. We deployed an edge network comprising multiple Raspberry Pis, NVIDIA Jetson Nano, and Jetson NVIDIA TX2 devices, successfully achieving real-time analysis and detection of video streaming data. We believe our work provides new technological support for the real-time processing of streaming data.","2024-08","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Blockchain; Distributed Computing; Edge Computing; Streaming Data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HYF8JXXA","journalArticle","2024","Barriga, Arturo; Barriga, José A.; Pérez-Toledano, Miguel A.; Clemente, Pedro J.","Model-Driven Development Towards Distributed Intelligent Systems","ACM Trans. Internet Technol.","","1533-5399","10.1145/3687472","https://doi.org/10.1145/3687472","A Distributed Intelligent System (DIS) encompasses a set of intelligent subsystems and components that collaborate to perform tasks and solve problems. Given the advancements of paradigms such as the Internet of Things, along with the advancements of technologies such as Machine Learning and Digital Twins, DISs are on the rise. These systems are increasingly integrating components that perform intelligent functions, and these intelligent functions are increasingly heterogeneous and varied. Moreover, there is no standardized framework to help researchers and practitioners adequately address DISs. As a result, the complexity, interoperability issues, and development time and costs of these systems are growing. However, Model-Driven Development (MDD) can help to address these challenges by providing a Domain-Specific Language (DSL) for developing DISs. In this work, a DSL for the design, validation, generation, and deployment of DISs is proposed. Firstly, the proposed DSL captures in a metamodel the key and high-level abstract concepts of the distinct DISs documented in the literature. Then, it allows modeling of DISs conforming to this metamodel. Subsequently, the DSL enables formal validation of the modeled systems. Lastly, it allows the generation and deployment of all DISs into production. Therefore, the work undertaken in this communication provides a methodological, formal, and standardized approach to defining and developing DISs from a high level of abstraction. This work allows users to address DISs by facilitating agility, minimizing manual tasks, and reducing the number of defects introduced in their development. To illustrate the applicability of the proposed DSL, a real case study of an agricultural digital twin is presented.","2024-11","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","4","24","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Internet of Things; model-driven development; digital twin; Distributed intelligent system; model-to- text transformation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YABVPZQX","journalArticle","2020","Li, Yuhao; Sun, Dan; Lee, Benjamin C.","Dynamic Colocation Policies with Reinforcement Learning","ACM Trans. Archit. Code Optim.","","1544-3566","10.1145/3375714","https://doi.org/10.1145/3375714","We draw on reinforcement learning frameworks to design and implement an adaptive controller for managing resource contention. During runtime, the controller observes the dynamic system conditions and optimizes control policies that satisfy latency targets yet improve server utilization. We evaluate a physical prototype that guarantees 95th percentile latencies for a search engine and improves server utilization by up to 70%, compared to exclusively reserving servers for interactive services, for varied batch workloads in machine learning.","2020-03","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","1","17","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","machine learning; adaptive control; Resource contention","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BGKSW3W3","journalArticle","2022","Li, Zijun; Guo, Linsong; Cheng, Jiagan; Chen, Quan; He, Bingsheng; Guo, Minyi","The Serverless Computing Survey: A Technical Primer for Design Architecture","ACM Comput. Surv.","","0360-0300","10.1145/3508360","https://doi.org/10.1145/3508360","The development of cloud infrastructures inspires the emergence of cloud-native computing. As the most promising architecture for deploying microservices, serverless computing has recently attracted more and more attention in both industry and academia. Due to its inherent scalability and flexibility, serverless computing becomes attractive and more pervasive for ever-growing Internet services. Despite the momentum in the cloud-native community, the existing challenges and compromises still wait for more advanced research and solutions to further explore the potential of the serverless computing model. As a contribution to this knowledge, this article surveys and elaborates the research domains in the serverless context by decoupling the architecture into four stack layers: Virtualization, Encapsule, System Orchestration, and System Coordination. Inspired by the security model, we highlight the key implications and limitations of these works in each layer, and make suggestions for potential challenges to the field of future serverless computing.","2022-09","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","10s","54","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","FaaS; Serverless computing; architecture design; Lambda paradigm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XD3FYS7A","journalArticle","2022","Berenberg, Anna; Calder, Brad","Deployment Archetypes for Cloud Applications","ACM Comput. Surv.","","0360-0300","10.1145/3498336","https://doi.org/10.1145/3498336","This is a survey article that explores six Cloud-based deployment archetypes for Cloud applications and the tradeoffs between them to achieve high availability, low end-user latency, and acceptable costs. These are (1) Zonal, (2) Regional, (3) Multi-regional, (4) Global, (5) Hybrid, and (6) Multi-cloud deployment archetypes. The goal is to classify cloud applications into a set of deployment archetypes and deployment models that tradeoff their needs around availability, latency, and geographical constraints with a focus on serving applications. This enables application owners to better examine the tradeoffs of each deployment model and what is needed for achieving the availability and latency goals for their application.","2022-02","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","3","55","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","cloud architecture; cloud archetypes; cloud availability; Cloud deployments","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JUMG5KXQ","journalArticle","2020","Hilman, Muhammad H.; Rodriguez, Maria A.; Buyya, Rajkumar","Multiple Workflows Scheduling in Multi-tenant Distributed Systems: A Taxonomy and Future Directions","ACM Comput. Surv.","","0360-0300","10.1145/3368036","https://doi.org/10.1145/3368036","Workflows are an application model that enables the automated execution of multiple interdependent and interconnected tasks. They are widely used by the scientific community to manage the distributed execution and dataflow of complex simulations and experiments. As the popularity of scientific workflows continue to rise, and their computational requirements continue to increase, the emergence and adoption of multi-tenant computing platforms that offer the execution of these workflows as a service becomes widespread. This article discusses the scheduling and resource provisioning problems particular to this type of platform. It presents a detailed taxonomy and a comprehensive survey of the current literature and identifies future directions to foster research in the field of multiple workflow scheduling in multi-tenant distributed computing systems.","2020-02","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","1","53","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","multi-tenant platforms; multiple workflows scheduling; Scientific workflows","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NH7IK39K","journalArticle","2019","Masson, Charles; Rim, Jee E.; Lee, Homin K.","DDSketch: a fast and fully-mergeable quantile sketch with relative-error guarantees","Proc. VLDB Endow.","","2150-8097","10.14778/3352063.3352135","https://doi.org/10.14778/3352063.3352135","Summary statistics such as the mean and variance are easily maintained for large, distributed data streams, but order statistics (i.e., sample quantiles) can only be approximately summarized. There is extensive literature on maintaining quantile sketches where the emphasis has been on bounding the rank error of the sketch while using little memory. Unfortunately, rank error guarantees do not preclude arbitrarily large relative errors, and this often occurs in practice when the data is heavily skewed.Given the distributed nature of contemporary large-scale systems, another crucial property for quantile sketches is mergeablility, i.e., several combined sketches must be as accurate as a single sketch of the same data. We present the first fully-mergeable, relative-error quantile sketching algorithm with formal guarantees. The sketch is extremely fast and accurate, and is currently being used by Datadog at a wide-scale.","2019-08","2025-03-18 09:07:50","2025-03-18 09:07:50","","2195–2205","","12","12","","","","","","","","","","","","","","","","","Publisher: VLDB Endowment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"26KWUWPI","journalArticle","2022","Son, Heesuk; Lee, Chaiwoo; FakhrHosseini, Shabnam; Lee, Shenghung; Coughlin, Joseph F.; Rudnik, John","Reshaping the Smart Home Research and Development in the Pandemic Era: Considerations around Scalable and Easy-to-Install Design","Proc. ACM Hum.-Comput. Interact.","","","10.1145/3512961","https://doi.org/10.1145/3512961","Smart home research has traditionally included visiting participants' homes to build testbed environments and evaluate their experience. However, in-person home deployment poses limitations around scalability and is not a feasible method in the context of the COVID-19 pandemic. The smart home research community is now facing the need to reshape and innovate research methods and design approaches. This study introduces a scalable smart home platform prototype that demonstrates possible solutions to address issues and limitations posed by the pandemic, such as improving package design, enabling user-driven installation, and facilitating remote evaluation and maintenance. The prototype uses off-the-shelf products with specially designed packaging to ensure interoperability as well as ease of shipping and installation. In this study, the prototype kits were shipped to participants' homes to understand and evaluate user perceptions and experiences around installation and initial use. Responses to a post-installation questionnaire and remote monitoring of system status showed that the participants easily completed their self-installation of the prototype without any on-site support. The study also showed potential for a scenario-based evaluation of the prototype using a remote, contactless research procedure.","2022-04","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","CSCW1","6","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","scalability; case study; pandemic; research method design; smart home; user experience","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2VA7PJPG","journalArticle","2021","Michel, Oliver; Bifulco, Roberto; Rétvári, Gábor; Schmid, Stefan","The Programmable Data Plane: Abstractions, Architectures, Algorithms, and Applications","ACM Comput. Surv.","","0360-0300","10.1145/3447868","https://doi.org/10.1145/3447868","Programmable data plane technologies enable the systematic reconfiguration of the low-level processing steps applied to network packets and are key drivers toward realizing the next generation of network services and applications. This survey presents recent trends and issues in the design and implementation of programmable network devices, focusing on prominent abstractions, architectures, algorithms, and applications proposed, debated, and realized over the past years. We elaborate on the trends that led to the emergence of this technology and highlight the most important pointers from the literature, casting different taxonomies for the field, and identifying avenues for future research.","2021-05","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","4","54","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","in-network computation; network programmability; packet processing; Programmable data planes; programmable switches","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9UZKAFYF","journalArticle","2018","Herbst, Nikolas; Bauer, André; Kounev, Samuel; Oikonomou, Giorgos; Eyk, Erwin Van; Kousiouris, George; Evangelinou, Athanasia; Krebs, Rouven; Brecht, Tim; Abad, Cristina L.; Iosup, Alexandru","Quantifying Cloud Performance and Dependability: Taxonomy, Metric Design, and Emerging Challenges","ACM Trans. Model. Perform. Eval. Comput. Syst.","","2376-3639","10.1145/3236332","https://doi.org/10.1145/3236332","In only a decade, cloud computing has emerged from a pursuit for a service-driven information and communication technology (ICT), becoming a significant fraction of the ICT market. Responding to the growth of the market, many alternative cloud services and their underlying systems are currently vying for the attention of cloud users and providers. To make informed choices between competing cloud service providers, permit the cost-benefit analysis of cloud-based systems, and enable system DevOps to evaluate and tune the performance of these complex ecosystems, appropriate performance metrics, benchmarks, tools, and methodologies are necessary. This requires re-examining old system properties and considering new system properties, possibly leading to the re-design of classic benchmarking metrics such as expressing performance as throughput and latency (response time). In this work, we address these requirements by focusing on four system properties: (i) elasticity of the cloud service, to accommodate large variations in the amount of service requested, (ii)&nbsp;performance isolation between the tenants of shared cloud systems and resulting performance variability, (iii)&nbsp;availability of cloud services and systems, and (iv) the operational risk of running a production system in a cloud environment. Focusing on key metrics for each of these properties, we review the state-of-the-art, then select or propose new metrics together with measurement approaches. We see the presented metrics as a foundation toward upcoming, future industry-standard cloud benchmarks.","2018-08","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","4","3","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","cloud; Metrics; benchmarking; availability; elasticity; operational risk; performance isolation; performance variability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LDGTRKP4","journalArticle","2023","Hoffpauir, Kyle; Simmons, Jacob; Schmidt, Nikolas; Pittala, Rachitha; Briggs, Isaac; Makani, Shanmukha; Jararweh, Yaser","A Survey on Edge Intelligence and Lightweight Machine Learning Support for Future Applications and Services","J. Data and Information Quality","","1936-1955","10.1145/3581759","https://doi.org/10.1145/3581759","As the number of devices connected to the Internet has grown larger, so too has the intensity of the tasks that these devices need to perform. Modern networks are more frequently working to perform computationally intensive tasks on low-power devices and low-end hardware. Current architectures and platforms tend towards centralized and resource-rich cloud computing approaches to address these deficits. However, edge computing presents a much more viable and flexible alternative. Edge computing refers to a distributed and decentralized network architecture in which demanding tasks such as image recognition, smart city services, and high-intensity data processing tasks can be distributed over a number of integrated network devices. In this article, we provide a comprehensive survey for emerging edge intelligence applications, lightweight machine learning algorithms, and their support for future applications and services. We start by analyzing the rise of cloud computing, discuss its weak points, and identify situations in which edge computing provides advantages over traditional cloud computing architectures. We then divulge details of the survey: the first section identifies opportunities and domains for edge computing growth, the second identifies algorithms and approaches that can be used to enhance edge intelligence implementations, and the third specifically analyzes situations in which edge intelligence can be enhanced using any of the aforementioned algorithms or approaches. In this third section, lightweight machine learning approaches are detailed. A more in-depth analysis and discussion of future developments follows. The primary discourse of this article is in service of an effort to ensure that appropriate approaches are applied adequately to artificial intelligence implementations in edge systems, mainly, the lightweight machine learning approaches.","2023-06","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","2","15","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","cloud computing; quality of service; edge computing; Edge intelligence; artificial intelligence; lightweight machine learning; network services","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M4UXA9AK","journalArticle","2024","Cederbladh, Johan; Cicchetti, Antonio; Suryadevara, Jagadish","Early Validation and Verification of System Behaviour in Model-based Systems Engineering: A Systematic Literature Review","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3631976","https://doi.org/10.1145/3631976","In the Systems Engineering (SE) domain there has been a paradigm shift from document-based to model-based system development artefacts; in fact, new methodologies are emerging to meet the increasing complexity of current systems and the corresponding growing need of digital workflows. In this regard, Model-Based Systems Engineering (MBSE) is considered as a key enabler by many central players of the SE community. MBSE has reached an adequate level of maturity, and there exist documented success stories in its adoption in industry. In particular, one significant benefit of utilising MBSE when compared to the traditional manual and document-centric workflows is that models are available from early phases of systems development; these enable a multitude of analyses prior any implementation effort together with other relevant capabilities, like the automation of development tasks. Nonetheless, it is noticeable there is a lack of a common understanding for how formal analyses for the verification and validation (V&amp;V) of systems behaviour, specifically in the early phases of development, could be placed in an MBSE setting.In this article, we report on the planning, execution, and results of a systematic literature review regarding the early V&amp;V of systems behaviour in the context of model-based systems engineering. The review aims to provide a structured representation of the state of the art with respect to motivations, proposed solutions, and limitations. From an initial set of potentially relevant 701 peer-reviewed publications we selected 149 primary studies, which we analysed according to a rigorous data extraction, analysis, and synthesis process. Based on our results, early V&amp;V has usually the goal of checking the quality of a system design to avoid discovering flaws when parts are being concretely realised; SysML is a de facto standard for describing the system under study, while the solutions for the analyses tend to be varied; also V&amp;V analyses tend to target varied properties with a slight predominance of functional concerns, and following the variation mentioned so far the proposed solutions are largely context specific; the proposed approaches are usually presented without explicit limitations, while when limitations are discussed, readiness of the solutions, handling of analyses simplifications/assumptions, and languages/tools integration are among the most frequently mentioned issues.Based on the survey results and the standard SE practices, we discuss how the current state-of-the-art MBSE supports early V&amp;V of systems behaviour with a special focus on industrial adoption and identify relevant challenges to be researched further.","2024-03","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","3","33","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","MBSE; system behaviour; systematic literature review; validation; verification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S3KWJKL7","journalArticle","2020","Macedo, Ricardo; Paulo, João; Pereira, José; Bessani, Alysson","A Survey and Classification of Software-Defined Storage Systems","ACM Comput. Surv.","","0360-0300","10.1145/3385896","https://doi.org/10.1145/3385896","The exponential growth of digital information is imposing increasing scale and efficiency demands on modern storage infrastructures. As infrastructure complexity increases, so does the difficulty in ensuring quality of service, maintainability, and resource fairness, raising unprecedented performance, scalability, and programmability challenges. Software-Defined Storage (SDS) addresses these challenges by cleanly disentangling control and data flows, easing management, and improving control functionality of conventional storage systems. Despite its momentum in the research community, many aspects of the paradigm are still unclear, undefined, and unexplored, leading to misunderstandings that hamper the research and development of novel SDS technologies. In this article, we present an in-depth study of SDS systems, providing a thorough description and categorization of each plane of functionality. Further, we propose a taxonomy and classification of existing SDS solutions according to different criteria. Finally, we provide key insights about the paradigm and discuss potential future research directions for the field.","2020-05","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","3","53","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","distributed storage; Software-defined storage; storage infrastructures","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GAMZT5ZL","journalArticle","2022","Elhoseny, Mohamed; Lakhan, Abdullah; Rashid, Ahmed; Mohammed, Mazin; Abdulkareem, Karrar","Underwater Sensor Multi-Parameter Scheduling for Heterogenous Computing Nodes","ACM Trans. Sen. Netw.","","1550-4859","10.1145/3476513","https://doi.org/10.1145/3476513","Sensor-aware distributed workflow applications are becoming increasingly popular underwater. The apps are marine operations that generate data and process it based on its characteristics. Mobile-fog-cloud paradigms, as well as computing such as sensor nodes, have emerged. As previously stated, the nodes can be combined into a single system to achieve several goals. Many factors are considered, including network contents, workload fluctuation, variable execution durations, deadlines, and bandwidth. As a result, scheduling mobile workflow systems with multiple parameters might be challenging. The study suggests a novel content-efficient decision-aware task scheduling (CATSA) method for defining and adapting to complicated environmental changes. The CATSA consists of several components that work together to perform various benchmarks in the system, including a decision planner, sequencing, and scheduling. As evidenced by test findings during evaluation, the suggested architecture outperforms current studies regarding workflow execution quality of services and improved the makespan 30% and deadline meeting 40% in the study.","2022-09","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","3","18","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","scheduling; content; decision; marine; parameters; sensor; Water","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SDUB4Y9B","journalArticle","2022","Husen, Arif; Chaudary, Muhammad Hasanain; Ahmad, Farooq","A Survey on Requirements of Future Intelligent Networks: Solutions and Future Research Directions","ACM Comput. Surv.","","0360-0300","10.1145/3524106","https://doi.org/10.1145/3524106","The context of this study examines the requirements of Future Intelligent Networks (FIN), solutions, and current research directions through a survey technique. The background of this study is hinged on the applications of Machine Learning (ML) in the networking field. Through careful analysis of literature and real-world reports, we noted that ML has significantly expedited decision-making processes, enhanced intelligent automation, and helped resolve complex problems economically in different fields of life. Various researchers have also envisioned future networks incorporating intelligent functions and operations with ML. Several efforts have been made to automate individual functions and operations in the networking domain; however, most of the existing ML models proposed in the literature lack several vital requirements. Hence, this study aims to present a comprehensive summary of the requirements of FIN and propose a taxonomy of different network functionalities that needs to be equipped with ML techniques. The core objectives of this study are to provide a taxonomy of requirements envisioned for end-to-end FIN, relevant ML techniques, and their analysis to find research gaps, open issues, and future research directions. The real benefit of ML applications in any domain can only be ensured if intelligent capabilities cover all of its components. We observed that future generations of networks are heterogeneous, multi-vendor, and multidimensional, and ML can provide optimal results only if intelligent capabilities are used on a holistic scale. Realizing intelligence on a holistic scale is only possible if the ML algorithms can solve heterogeneous problems in a multi-vendor and multidimensional environment. ML models must be reliable and efficient, support, and possess the capability to learn and share the knowledge across the network layers and administrative domains to solve issues. First, this study ascertains the requirements of the FIN and proposes their taxonomy through reviews on envisioned ideas by various researchers and articles gathered from reputed conferences and standard developing organizations using keyword queries. Second, we have reviewed existing studies on ML applications focusing on coverage, heterogeneity, distributed architecture, and cross-domain knowledge learning and sharing. Our study observed that in the past, ML applications were focused mainly on an individual/isolated level only, and aspects of global and deep holistic learning with cross-layer/cross-domain knowledge sharing with agile ML operations are not explored at large. We recommend that the issues mentioned previously be addressed with improved ML architecture and agile operations and propose an ML pipeline based architecture for FIN. The significant contribution of this study is the impetus for researchers to seek ML models suitable for a modular, distributed, multi-domain, and multi-layer environment and provide decision making on a global or holistic rather than an individual function level.","2022-11","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","4","55","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","cross-administrative domain learning; cross-layer learning; deep holistic learning; feature sharing; Future intelligent networks; global learning; knowledge sharing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DH7G7HJ6","journalArticle","2020","Fei, Xincai; Liu, Fangming; Zhang, Qixia; Jin, Hai; Hu, Hongxin","Paving the Way for NFV Acceleration: A Taxonomy, Survey and Future Directions","ACM Comput. Surv.","","0360-0300","10.1145/3397022","https://doi.org/10.1145/3397022","As a recent innovation, network functions virtualization (NFV)—with its core concept of replacing hardware middleboxes with software network functions (NFs) implemented in commodity servers—promises cost savings and flexibility benefits. However, transitioning NFs from special-purpose hardware to commodity servers has turned out to be more challenging than expected, as it inevitably incurs performance penalties due to bottlenecks in both software and hardware. To achieve performance comparable to hardware middleboxes, there is a strong demand for a speedup in NF processing, which plays a crucial role in the success of NFV. In this article, we study the performance challenges that exist in general-purpose servers and simultaneously summarize the typical performance bottlenecks in NFV. Through reviewing the progress in the field of NFV acceleration, we present a new taxonomy of the state-of-the-art efforts according to various acceleration approaches. We discuss the surveyed works and identify the respective advantages and disadvantages in each category. We then discuss the products, solutions, and projects emerged in industry. We also present a gap analysis to improve current solutions and highlight promising research trends that can be explored in the future.","2020-08","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","4","53","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","high performance; Network functions virtualization; NFV acceleration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X98WNQTB","journalArticle","2020","Davoudian, Ali; Liu, Mengchi","Big Data Systems: A Software Engineering Perspective","ACM Comput. Surv.","","0360-0300","10.1145/3408314","https://doi.org/10.1145/3408314","Big Data Systems (BDSs) are an emerging class of scalable software technologies whereby massive amounts of heterogeneous data are gathered from multiple sources, managed, analyzed (in batch, stream or hybrid fashion), and served to end-users and external applications. Such systems pose specific challenges in all phases of software development lifecycle and might become very complex by evolving data, technologies, and target value over time. Consequently, many organizations and enterprises have found it difficult to adopt BDSs. In this article, we provide insight into three major activities of software engineering in the context of BDSs as well as the choices made to tackle them regarding state-of-the-art research and industry efforts. These activities include the engineering of requirements, designing and constructing software to meet the specified requirements, and software/data quality assurance. We also disclose some open challenges of developing effective BDSs, which need attention from both researchers and practitioners.","2020-09","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","5","53","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Big Data; Big Data systems; quality assurance; requirements engineering; software engineering; software reference architecture","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"57YJTKMW","journalArticle","2024","Silva, Samira; Pelliccione, Patrizio; Bertolino, Antonia","Self-Adaptive Testing in the Field","ACM Trans. Auton. Adapt. Syst.","","1556-4665","10.1145/3627163","https://doi.org/10.1145/3627163","We are increasingly surrounded by systems connecting us with the digital world and facilitating our life by supporting our work, leisure, activities at home, health, and so on. These systems are pressed by two forces. On the one side, they operate in environments that are increasingly challenging due to uncertainty and uncontrollability. On the other side, they need to evolve, often in a continuous fashion, to meet changing needs, to offer new functionalities, or also to fix emerging failures. To make the picture even more complex, these systems rarely work in isolation and often need to collaborate with other systems, as well as humans. All such facets call for moving their validation during operation, as offered by approaches called testing in the field.In this article, we observe that even the field-based testing approaches should change over time to follow and adapt to the changes and evolution of collaborating systems or environments or users’ behaviors. We provide a taxonomy of this new category of testing that we call self-adaptive testing in the field (SATF), together with a reference architecture for SATF approaches. To achieve this objective, we surveyed the literature and collected feedback and contributions from experts in the domain via a questionnaire and interviews.","2024-02","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","1","19","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","knowledge gaps; self-adaptive testing; Software testing in the field","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5GBRSCQT","journalArticle","2024","Golec, Muhammed; Walia, Guneet Kaur; Kumar, Mohit; Cuadrado, Felix; Gill, Sukhpal Singh; Uhlig, Steve","Cold Start Latency in Serverless Computing: A Systematic Review, Taxonomy, and Future Directions","ACM Comput. Surv.","","0360-0300","10.1145/3700875","https://doi.org/10.1145/3700875","Recently, academics and the corporate sector have paid attention to serverless computing, which enables dynamic scalability and an economic model. In serverless computing, users only pay for the time they actually use resources, enabling zero scaling to optimise cost and resource utilisation. However, this approach also introduces the serverless cold start problem. Researchers have developed various solutions to address the cold start problem, yet it remains an unresolved research area. In this article, we propose a systematic literature review on cold start latency in serverless computing. Furthermore, we create a detailed taxonomy of approaches to cold start latency, which we use to investigate existing techniques for reducing the cold start time and frequency. We have classified the current studies on cold start latency into several categories such as caching and application-level optimisation-based solutions, as well as Artificial Intelligence/Machine Learning-based solutions. Moreover, we have analyzed the impact of cold start latency on quality of service, explored current cold start latency mitigation methods, datasets, and implementation platforms, and classified them into categories based on their common characteristics and features. Finally, we outline the open challenges and highlight the possible future directions.","2024-11","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","3","57","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","cloud computing; survey; Serverless computing; function as a service; cold start latency","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"35VDPHIA","journalArticle","2023","Picone, Marco; Mamei, Marco; Zambonelli, Franco","A Flexible and Modular Architecture for Edge Digital Twin: Implementation and Evaluation","ACM Trans. Internet Things","","","10.1145/3573206","https://doi.org/10.1145/3573206","IoT systems based on Digital Twins (DTs) — virtual copies of physical objects and systems — can be very effective to enable data-driven services and promote better control and decisions, in particular by exploiting distributed approaches where cloud and edge computing cooperate effectively. In this context, digital twins deployed on the edge represents a new strategic element to design a new wave of distributed cyber-physical applications. Existing approaches are generally focused on fragmented and domain-specific monolithic solutions and are mainly associated to model-driven, simulative or descriptive visions. The idea of extending the DTs role to support last-mile digitalization and interoperability through a set of general purpose and well-defined properties and capabilities is still underinvestigated. In this paper, we present the novel Edge Digital Twins (EDT) architectural model and its implementation, enabling the lightweight replication of physical devices providing an efficient digital abstraction layer to support the autonomous and standard collaboration of things and services. We model the core capabilities with respect to the recent definition of the state of the art, present the software architecture and a prototype implementation. Extensive experimental analysis shows the obtained performance in multiple IoT application contexts and compares them with that of state-of-the-art approaches.","2023-02","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","1","4","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Internet of Things; edge computing; Digital Twin","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BFWDAM2G","journalArticle","2017","Ferroni, Matteo; Corna, Andrea; Damiani, Andrea; Brondolin, Rolando; Kubiatowicz, John D.; Sciuto, Donatella; Santambrogio, Marco D.","MARC: A Resource Consumption Modeling Service for Self-Aware Autonomous Agents","ACM Trans. Auton. Adapt. Syst.","","1556-4665","10.1145/3127499","https://doi.org/10.1145/3127499","Autonomicity is a golden feature when dealing with a high level of complexity. This complexity can be tackled partitioning huge systems in small autonomous modules, i.e., agents. Each agent then needs to be capable of extracting knowledge from its environment and to learn from it, in order to fulfill its goals: this could not be achieved without proper modeling techniques that allow each agent to gaze beyond its sensors. Unfortunately, the simplicity of agents and the complexity of modeling do not fit together, thus demanding for a third party to bridge the gap.Given the opportunities in the field, the main contributions of this work are twofold: (1) we propose a general methodology to model resource consumption trends and (2) we implemented it into MARC, a Cloud-service platform that produces Models-as-a-Service, thus relieving self-aware agents from the burden of building their custom modeling framework. In order to validate the proposed methodology, we set up a custom simulator to generate a wide spectrum of controlled traces: this allowed us to verify the correctness of our framework from a general and comprehensive point of view.","2017-11","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","4","12","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","autoregressive with exogenous variable models; discrete Markov models; Model-as-a-service; resource consumption","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WYUDXQ43","journalArticle","2022","Kundan, Shivam; Marinakis, Theodoros; Anagnostopoulos, Iraklis; Kagaris, Dimitri","A Pressure-Aware Policy for Contention Minimization on Multicore Systems","ACM Trans. Archit. Code Optim.","","1544-3566","10.1145/3524616","https://doi.org/10.1145/3524616","Modern Chip Multiprocessors (CMPs) are integrating an increasing amount of cores to address the continually growing demand for high-application performance. The cores of a CMP share several components of the memory hierarchy, such as Last-Level Cache (LLC) and main memory. This allows for considerable gains in multithreaded applications while also helping to maintain architectural simplicity. However, sharing resources can also result in performance bottleneck due to contention among concurrently executing applications. In this work, we formulate a fine-grained application characterization methodology that leverages Performance Monitoring Counters (PMCs) and Cache Monitoring Technology (CMT) in Intel processors. We utilize this characterization methodology to develop two contention-aware scheduling policies, one static and one dynamic, that co-schedule applications based on their resource-interference profiles. Our approach focuses on minimizing contention on both the main-memory bandwidth and the LLC by monitoring the pressure that each application inflicts on these resources. We achieve performance benefits for diverse workloads, outperforming Linux and three state-of-the-art contention-aware schedulers in terms of system throughput and fairness for both single and multithreaded workloads. Compared with Linux, our policy achieves up to 16% greater throughput for single-threaded and up to 40% greater throughput for multithreaded applications. Additionally, the policies increase fairness by up to 65% for single-threaded and up to 130% for multithreaded ones.","2022-05","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","3","19","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Chip multiprocessors; contention-aware scheduling; fairness; Linux; pressure minimization; throughput","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"26X6Y8WF","journalArticle","2025","Åkesson, Alfred; Gehrmann, Christian; Hedin, Görel; Johnsson, Björn A.; Magnusson, Boris; Nordahl, Mattias; Ramezanian, Sara; Stankovski Wagner, Paul","A Trust Establishment and Key Management Architecture for Hospital-at-Home","ACM Trans. Comput. Healthcare","","","10.1145/3700144","https://doi.org/10.1145/3700144","The landscape of healthcare is experiencing a digitalization shift, transferring many medical activities to the patients’ homes, a phenomenon commonly referred to as Hospital-at-Home. While Internet of Things (IoT) devices facilitate the building of such systems, there is a need for powerful middleware that encapsulates device-to-device communication and enables the construction of user-friendly, secure, and robust Hospital-at-Home systems. A key challenge for such middleware is to build a trustworthy and lightweight key management system allowing different devices in the system to exchange messages securely. In this article, we present a simple, easily manageable and scalable such architecture which, in addition, supports long-term data protection using post-quantum cryptographic primitives. Our proposed solution utilizes a Merkle tree to enable the IoT devices to establish trust between each other automatically, even in the absence of an Internet connection. We have implemented the architecture and present performance figures as well as a security analysis of our approach.","2025-01","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","1","6","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Hospital-at-Home; IoT Middleware; Key Management; Post Quantum Cryptography; Trust Establishment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JCRGR9DQ","journalArticle","2023","Russo Russo, Gabriele; Cardellini, Valeria; Lo Presti, Francesco","Hierarchical Auto-scaling Policies for Data Stream Processing on Heterogeneous Resources","ACM Trans. Auton. Adapt. Syst.","","1556-4665","10.1145/3597435","https://doi.org/10.1145/3597435","Data Stream Processing (DSP) applications analyze data flows in near real-time by means of operators, which process and transform incoming data. Operators handle high data rates running parallel replicas across multiple processors and hosts. To guarantee consistent performance without wasting resources in the face of variable workloads, auto-scaling techniques have been studied to adapt operator parallelism at run-time. However, most of the effort has been spent under the assumption of homogeneous computing infrastructures, neglecting the complexity of modern environments.We consider the problem of deciding both how many operator replicas should be executed and which types of computing nodes should be acquired. We devise heterogeneity-aware policies by means of a two-layered hierarchy of controllers. While application-level components steer the adaptation process for whole applications, aiming to guarantee user-specified requirements, lower-layer components control auto-scaling of single operators. We tackle the fundamental challenge of performance and workload uncertainty, exploiting Bayesian optimization (BO) and reinforcement learning (RL) to devise policies. The evaluation shows that our approach is able to meet users’ requirements in terms of response time and adaptation overhead, while minimizing the cost due to resource usage, outperforming state-of-the-art baselines. We also demonstrate how partial model information is exploited to reduce training time for learning-based controllers.","2023-10","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","4","18","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","resource management; reinforcement learning; Auto-scaling; Data Stream Processing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YE5EMQHV","journalArticle","2024","Ball, Thomas; de Halleux, Peli; Devine, James; Hodges, Steve; Moskal, Michał","Jacdac: Service-Based Prototyping of Embedded Systems","Proc. ACM Program. Lang.","","","10.1145/3656405","https://doi.org/10.1145/3656405","The traditional approach to programming embedded systems is monolithic: firmware on a microcontroller contains both application code and the drivers needed to communicate with sensors and actuators, using low-level protocols such as I2C, SPI, and RS232. In comparison, software development for the cloud has moved to a service-based development and operation paradigm: a service provides a discrete unit of functionality that can be accessed remotely by an application, or other service, but is independently managed and updated. We propose, design, implement, and evaluate a service-based approach to prototyping embedded systems called Jacdac. Jacdac defines a service specification language, designed especially for embedded systems, along with a host of specifications for a variety of sensors and actuators. With Jacdac, each sensor/actuator in a system is paired with a low-cost microcontroller that advertises the services that represent the functionality of the underlying hardware over an efficient and low-cost single-wire bus protocol. A separate microcontroller executes the user's application program, which is a client of the Jacdac services on the bus. Our evaluation shows that Jacdac supports a service-based abstraction for sensors/actuators at low cost and reasonable performance, with many benefits for prototyping: ease of use via the automated discovery of devices and their capabilities, substitution of same-service devices for each other, as well as high-level programming, monitoring, and debugging. We also report on the experience of bringing Jacdac to commercial availability via third-party manufacturers.","2024-06","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","PLDI","8","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","embedded systems; microcontrollers; plug-and-play; services","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MZCN68FG","journalArticle","2021","Ray, Kaustabha; Banerjee, Ansuman","Horizontal Auto-Scaling for Multi-Access Edge Computing Using Safe Reinforcement Learning","ACM Trans. Embed. Comput. Syst.","","1539-9087","10.1145/3475991","https://doi.org/10.1145/3475991","Multi-Access Edge Computing (MEC) has emerged as a promising new paradigm allowing low latency access to services deployed on edge servers to avert network latencies often encountered in accessing cloud services. A key component of the MEC environment is an auto-scaling policy which is used to decide the overall management and scaling of container instances corresponding to individual services deployed on MEC servers to cater to traffic fluctuations. In this work, we propose a Safe Reinforcement Learning (RL)-based auto-scaling policy agent that can efficiently adapt to traffic variations to ensure adherence to service specific latency requirements. We model the MEC environment using a Markov Decision Process (MDP). We demonstrate how latency requirements can be formally expressed in Linear Temporal Logic (LTL). The LTL specification acts as a guide to the policy agent to automatically learn auto-scaling decisions that maximize the probability of satisfying the LTL formula. We introduce a quantitative reward mechanism based on the LTL formula to tailor service specific latency requirements. We prove that our reward mechanism ensures convergence of standard Safe-RL approaches. We present experimental results in practical scenarios on a test-bed setup with real-world benchmark applications to show the effectiveness of our approach in comparison to other state-of-the-art methods in literature. Furthermore, we perform extensive simulated experiments to demonstrate the effectiveness of our approach in large scale scenarios.","2021-10","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","6","20","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","auto-scaling; Multi-access edge computing; safe reinforcement learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8SGZJ8P9","journalArticle","2019","Or-Meir, Ori; Nissim, Nir; Elovici, Yuval; Rokach, Lior","Dynamic Malware Analysis in the Modern Era—A State of the Art Survey","ACM Comput. Surv.","","0360-0300","10.1145/3329786","https://doi.org/10.1145/3329786","Although malicious software (malware) has been around since the early days of computers, the sophistication and innovation of malware has increased over the years. In particular, the latest crop of ransomware has drawn attention to the dangers of malicious software, which can cause harm to private users as well as corporations, public services (hospitals and transportation systems), governments, and security institutions. To protect these institutions and the public from malware attacks, malicious activity must be detected as early as possible, preferably before it conducts its harmful acts. However, it is not always easy to know what to look for—especially when dealing with new and unknown malware that has never been seen. Analyzing a suspicious file by static or dynamic analysis methods can provide relevant and valuable information regarding a file's impact on the hosting system and help determine whether the file is malicious or not, based on the method's predefined rules. While various techniques (e.g., code obfuscation, dynamic code loading, encryption, and packing) can be used by malware writers to evade static analysis (including signature-based anti-virus tools), dynamic analysis is robust to these techniques and can provide greater understanding regarding the analyzed file and consequently can lead to better detection capabilities. Although dynamic analysis is more robust than static analysis, existing dynamic analysis tools and techniques are imperfect, and there is no single tool that can cover all aspects of malware behavior. The most recent comprehensive survey performed in this area was published in 2012. Since that time, the computing environment has changed dramatically with new types of malware (ransomware, cryptominers), new analysis methods (volatile memory forensics, side-channel analysis), new computing environments (cloud computing, IoT devices), new machine-learning algorithms, and more. The goal of this survey is to provide a comprehensive and up-to-date overview of existing methods used to dynamically analyze malware, which includes a description of each method, its strengths and weaknesses, and its resilience against malware evasion techniques. In addition, we include an overview of prominent studies presenting the usage of machine-learning methods to enhance dynamic malware analysis capabilities aimed at detection, classification, and categorization.","2019-09","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","5","52","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","behavioral analysis; detection; Dynamic analysis; evasion; malware","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U7PEEU4L","journalArticle","2024","Hannou, Fatma-Zohra; Lefrançois, Maxime; Jouvelot, Pierre; Charpenay, Victor; Zimmermann, Antoine","A Survey on IoT Programming Platforms: A Business-Domain Experts Perspective","ACM Comput. Surv.","","0360-0300","10.1145/3699954","https://doi.org/10.1145/3699954","The vast growth and digitalization potential offered by the Internet of Things (IoT) is hindered by substantial barriers in accessibility, interoperability, and complexity, mainly affecting small organizations and non-technical entities. This survey article provides a detailed overview of the landscape of IoT programming platforms, focusing specifically on the development support they offer for varying end user profiles, ranging from developers with IoT expertise to business experts willing to take advantage of IoT solutions to automate their organization processes. The survey examines a range of IoT platforms, classified according to their programming approach between general-purpose programming solutions, model-driven programming, mashups, and end-user programming. Necessary IoT and programming backgrounds are described to empower non-technical readers with a comprehensive field summary. In addition, the article compares the features of the most representative platforms and provides decision insights and guidelines to support end users in selecting appropriate IoT platforms for their use cases. This work contributes to narrowing the knowledge gap between IoT specialists and end users, breaking accessibility barriers and further promoting the integration of IoT technologies in various domains.1","2024-12","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","4","57","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Internet of Things; IoT platforms; IoT programming; smart agriculture; smart building","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SVCRNCL6","journalArticle","2022","Murturi, Ilir; Dustdar, Schahram","DECENT: A Decentralized Configurator for Controlling Elasticity in Dynamic Edge Networks","ACM Trans. Internet Technol.","","1533-5399","10.1145/3530692","https://doi.org/10.1145/3530692","Recent advancements in distributed systems have enabled deploying low-latency and highly resilient edge applications close to the IoT domain at the edge of the network. The broad range of edge application requirements combined with heterogeneous, resource-constrained, and dynamic edge networks make it particularly challenging to configure and deploy them. Besides that, missing elastic capabilities on the edge makes it difficult to operate such applications under dynamic workloads. To this end, this article proposes a lightweight, self-adaptive, and decentralized mechanism (DECENT) for (1) deploying edge applications on edge resources and on premises of Edge-Cloud infrastructure and (2) controlling elasticity requirements. DECENT enables developers to characterize their edge applications by specifying elasticity requirements, which are automatically captured, interpreted, and enforced by our decentralized elasticity interpreters. In response to dynamic workloads, edge applications automatically adapt in compliance with their elasticity requirements. We discuss the architecture, processes of the approach, and the experiment conducted on a real-world testbed to validate its feasibility on low-powered edge devices. Furthermore, we show performance and adaptation aspects through an edge safety application and its evolution in elasticity space (i.e., cost, resource, and quality).","2022-08","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","3","22","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","edge computing; Internet of Things (IoT); Elasticity; edge-cloud","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KELJ3REG","journalArticle","2023","Wen, Jinfeng; Chen, Zhenpeng; Jin, Xin; Liu, Xuanzhe","Rise of the Planet of Serverless Computing: A Systematic Review","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3579643","https://doi.org/10.1145/3579643","Serverless computing is an emerging cloud computing paradigm, being adopted to develop a wide range of software applications. It allows developers to focus on the application logic in the granularity of function, thereby freeing developers from tedious and error-prone infrastructure management. Meanwhile, its unique characteristic poses new challenges to the development and deployment of serverless-based applications. To tackle these challenges, enormous research efforts have been devoted. This article provides a comprehensive literature review to characterize the current research state of serverless computing. Specifically, this article covers 164 articles on 17 research directions of serverless computing, including performance optimization, programming framework, application migration, multi-cloud development, testing and debugging, and so on. It also derives research trends, focus, and commonly-used platforms for serverless computing, as well as promising research opportunities.","2023-07","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","5","32","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Serverless computing; literature view","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"773V8NVT","journalArticle","2023","Li, Ji You; Zhang, Jiachi; Zhou, Wenchao; Liu, Yuhang; Zhang, Shuai; Xue, Zhuoming; Xu, Ding; Fan, Hua; Zhou, Fangyuan; Li, Feifei","Eigen: End-to-End Resource Optimization for Large-Scale Databases on the Cloud","Proc. VLDB Endow.","","2150-8097","10.14778/3611540.3611565","https://doi.org/10.14778/3611540.3611565","Increasingly, cloud database vendors host large-scale geographically distributed clusters to provide cloud database services. When managing the clusters, we observe that it is challenging to simultaneously maximizing the resource allocation ratio and resource availability. This problem becomes more severe in modern cloud database clusters, where resource allocations occur more frequently and on a greater scale. To improve the resource allocation ratio without hurting resource availability, we introduce Eigen, a large-scale cloud-native cluster management system for large-scale databases on the cloud. Based on a resource flow model, we propose a hierarchical resource management system and three resource optimization algorithms that enable end-to-end resource optimization. Furthermore, we demonstrate the system optimization that promotes user experience by reducing scheduling latencies and improving scheduling throughput. Eigen has been launched in a large-scale public-cloud production environment for 30+ months and served more than 30+ regions (100+ available zones) globally. Based on the evaluation of real-world clusters and simulated experiments, Eigen can improve the allocation ratio by over 27% (from 60% to 87.0%) on average, while the ratio of delayed resource provisions is under 0.1%.","2023-08","2025-03-18 09:07:50","2025-03-18 09:07:50","","3795–3807","","12","16","","","","","","","","","","","","","","","","","Publisher: VLDB Endowment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TJKW94XU","journalArticle","2023","Coullon, Héléne; Henrio, Ludovic; Loulergue, Frédéric; Robillard, Simon","Component-based Distributed Software Reconfiguration:A Verification-oriented Survey","ACM Comput. Surv.","","0360-0300","10.1145/3595376","https://doi.org/10.1145/3595376","Distributed software built from components has become a mainstay of service-oriented applications, which frequently undergo reconfigurations to adapt to changes in their operating environment or their functional requirements. Given the complexity of distributed software and the adverse effects of incorrect reconfigurations, a suitable methodology is needed to ensure the correctness of reconfigurations in component-based systems. This survey gives the reader a global perspective over existing formal techniques that pursue this goal. It distinguishes different ways in which formal methods can improve the reliability of reconfigurations, and lists techniques that contribute to solving each of these particular scientific challenges.","2023-08","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","1","56","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","verification; component-based software engineering; formal methods; Reconfiguration; software adaptation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CB44KGWV","journalArticle","2021","Sarwar, Kinza; Yongchareon, Sira; Yu, Jian; Ur Rehman, Saeed","A Survey on Privacy Preservation in Fog-Enabled Internet of Things","ACM Comput. Surv.","","0360-0300","10.1145/3474554","https://doi.org/10.1145/3474554","Despite the rapid growth and advancement in the Internet of Things (IoT), there are critical challenges that need to be addressed before the full adoption of the IoT. Data privacy is one of the hurdles towards the adoption of IoT as there might be potential misuse of users’ data and their identity in IoT applications. Several researchers have proposed different approaches to reduce privacy risks. However, most of the existing solutions still suffer from various drawbacks, such as huge bandwidth utilization and network latency, heavyweight cryptosystems, and policies that are applied on sensor devices and in the cloud. To address these issues, fog computing has been introduced for IoT network edges providing low latency, computation, and storage services. In this survey, we comprehensively review and classify privacy requirements for an in-depth understanding of privacy implications in IoT applications. Based on the classification, we highlight ongoing research efforts and limitations of the existing privacy-preservation techniques and map the existing IoT schemes with Fog-enabled IoT schemes to elaborate on the benefits and improvements that Fog-enabled IoT can bring to preserve data privacy in IoT applications. Lastly, we enumerate key research challenges and point out future research directions.","2021-11","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","1","55","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Internet of Things; fog computing; Content privacy; context privacy; Fog-enabled IoT; IoT privacy","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AW32RV8L","journalArticle","2023","Toczé, Klervie; Fahs, Ali J.; Pierre, Guillaume; Nadjm-Tehrani, Simin","VioLinn: Proximity-aware Edge Placementwith Dynamic and Elastic Resource Provisioning","ACM Trans. Internet Things","","","10.1145/3573125","https://doi.org/10.1145/3573125","Deciding where to handle services and tasks, as well as provisioning an adequate amount of computing resources for this handling, is a main challenge of edge computing systems. Moreover, latency-sensitive services constrain the type and location of edge devices that can provide the needed resources. When available resources are scarce there is a possibility that some resource allocation requests are denied.In this work, we propose the VioLinn system to tackle the joint problems of task placement, service placement, and edge device provisioning. Dealing with latency-sensitive services is achieved through proximity-aware algorithms that ensure the tasks are handled close to the end-user. Moreover, the concept of spare edge device is introduced to handle sudden load variations in time and space without having to continuously overprovision. Several spare device selection algorithms are proposed with different cost/performance tradeoffs.Evaluations are performed both in a Kubernetes-based testbed and using simulations and show the benefit of using spare devices for handling localized load spikes with higher quality of service (QoS) and lower computing resource usage. The study of the different algorithms shows that it is possible to achieve this increase in QoS with different tradeoffs against cost and performance.","2023-02","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","1","4","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","resource management; Kubernetes; elasticity; Edge/fog computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q5JKPMMA","journalArticle","2020","Ali, Isra Mohamed; Caprolu, Maurantonio; Pietro, Roberto Di","Foundations, Properties, and Security Applications of Puzzles: A Survey","ACM Comput. Surv.","","0360-0300","10.1145/3396374","https://doi.org/10.1145/3396374","Cryptographic algorithms have been used not only to create robust ciphertexts but also to generate cryptograms that, contrary to the classic goal of cryptography, are meant to be broken. These cryptograms, generally called puzzles, require the use of a certain amount of resources to be solved, hence introducing a cost that is often regarded as a time delay—though it could involve other metrics as well, such as bandwidth. These powerful features have made puzzles the core of many security protocols, acquiring increasing importance in the IT security landscape. The concept of a puzzle has subsequently been extended to other types of schemes that do not use cryptographic functions, such as CAPTCHAs, which are used to discriminate humans from machines. Overall, puzzles have experienced a renewed interest with the advent of Bitcoin, which uses a CPU-intensive puzzle as proof of work. In this article, we provide a comprehensive study of the most important puzzle construction schemes available in the literature, categorizing them according to several attributes, such as resource type, verification type, and applications. We have redefined the term puzzle by collecting and integrating the scattered notions used in different works, to cover all the existing applications. Moreover, we provide an overview of the possible applications, identifying key requirements and different design approaches. Finally, we highlight the features and limitations of each approach, providing a useful guide for the future development of new puzzle schemes.","2020-08","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","4","53","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Bandwidth-bound puzzle; Bitcoin; CAPTCHA; CPU-bound puzzle; Cryptocurrency; Cryptographic puzzle; Human-bound puzzle; Memory-bound puzzle; Network-bound puzzle; Proof-of-Work","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZPQZWFKC","journalArticle","2023","Will, Newton C.; Maziero, Carlos A.","Intel Software Guard Extensions Applications: A Survey","ACM Comput. Surv.","","0360-0300","10.1145/3593021","https://doi.org/10.1145/3593021","Data confidentiality is a central concern in modern computer systems and services, as sensitive data from users and companies are being increasingly delegated to such systems. Several hardware-based mechanisms have been recently proposed to enforce security guarantees of sensitive information. Hardware-based isolated execution environments are a class of such mechanisms, in which the operating system and other low-level components are removed from the trusted computing base. One of such mechanisms is the Intel Software Guard Extensions (Intel SGX), which creates the concept of enclave to encapsulate sensitive components of applications and their data. Despite being largely applied in several computing areas, SGX has limitations and performance issues that must be addressed for the development of secure solutions. This text brings a categorized literature review of the ongoing research on the Intel SGX architecture, discussing its applications and providing a classification of the solutions that take advantage of SGX mechanisms. We analyze and categorize 293 papers that rely on SGX to provide integrity, confidentiality, and privacy to users and data, regarding different contexts and goals. We also discuss research challenges and provide future directions in the field of enclaved execution, particularly when using SGX.","2023-07","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","14s","55","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","confidentiality; Hardware-based security; integrity; processor extensions","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NAYJ7EIM","journalArticle","2019","Calegari, Patrice; Levrier, Marc; Balczyński, Paweł","Web Portals for High-performance Computing: A Survey","ACM Trans. Web","","1559-1131","10.1145/3197385","https://doi.org/10.1145/3197385","This article addresses web interfaces for High-performance Computing (HPC) simulation software. First, it presents a brief history, starting in the 1990s with Java applets, of web interfaces used for accessing and making best possible use of remote HPC resources. It introduces HPC web-based portal use cases. Then it identifies and discusses the key features, among functional and non-functional requirements, that characterize such portals. A brief state of the art is then presented. The design and development of Bull extreme factory Computing Studio v3 (XCS3) is chosen as a common thread for showing how the identified key features can all be implemented in one software: multi-tenancy, multi-scheduler compatibility, complete control through an HTTP RESTful API, customizable user interface with Responsive Web Design, HPC application template framework, remote visualization, and access through the Authentication, Authorization, and Accounting security framework with the Role-Based Access Control permission model. Non-functional requirements (security, usability, performance, reliability) are discussed, and the article concludes by giving perspective for future work.","2019-02","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","1","13","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","cloud computing; HPC; application templates; customizable GUI; dashboards; High-performance computing; HPC-as-a-service; HPCaaS; job management; RESTful API; SaaS; science gateway; service-oriented architectures; software-as-a-service; user interface; web portal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7RDMXG29","journalArticle","2019","Wu, Caesar; Buyya, Rajkumar; Ramamohanarao, Kotagiri","Cloud Pricing Models: Taxonomy, Survey, and Interdisciplinary Challenges","ACM Comput. Surv.","","0360-0300","10.1145/3342103","https://doi.org/10.1145/3342103","This article provides a systematic review of cloud pricing in an interdisciplinary approach. It examines many historical cases of pricing in practice and tracks down multiple roots of pricing in research. The aim is to help both cloud service provider (CSP) and cloud customers to capture the essence of cloud pricing when they need to make a critical decision either to achieve competitive advantages or to manage cloud resource effectively. Currently, the number of available pricing schemes in the cloud market is overwhelming. It is an intricate issue to understand these schemes and associated pricing models clearly due to involving several domains of knowledge, such as cloud technologies, microeconomics, operations research, and value theory. Some earlier studies have introduced this topic unsystematically. Their approaches inevitably lead to much confusion for many cloud decision-makers. To address their weaknesses, we present a comprehensive taxonomy of cloud pricing, which is driven by a framework of three fundamental pricing strategies that are built on nine cloud pricing categories. These categories can be further mapped onto a total of 60 pricing models. Many of the pricing models have been already adopted by CSPs. Others have been widespread across in other industries. We give descriptions of these model categories and highlight both advantages and disadvantages. Moreover, this article offers an extensive survey of many cloud pricing models that were proposed by many researchers during the past decade. Based on the survey, we identify four trends of cloud pricing and the general direction, which is moving from intrinsic value per physical box to extrinsic value per serverless sandbox. We conclude that hyper-converged cloud resources pool supported by cloud orchestration, virtual machine, Open Application Programming Interface, and serverless sandbox will drive the future of cloud pricing.","2019-10","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","6","52","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","cloud price model; Cloud services provider (CSP); cost-based pricing; market-based pricing; value-based pricing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ITZD36LP","journalArticle","2022","Ali, Ahsan; Pinciroli, Riccardo; Yan, Feng; Smirni, Evgenia","Optimizing inference serving on serverless platforms","Proc. VLDB Endow.","","2150-8097","10.14778/3547305.3547313","https://doi.org/10.14778/3547305.3547313","Serverless computing is gaining popularity for machine learning (ML) serving workload due to its autonomous resource scaling, easy to use and pay-per-use cost model. Existing serverless platforms work well for image-based ML inference, where requests are homogeneous in service demands. That said, recent advances in natural language processing could not fully benefit from existing serverless platforms as their requests are intrinsically heterogeneous.Batching requests for processing can significantly increase ML serving efficiency while reducing monetary cost, thanks to the pay-per-use pricing model adopted by serverless platforms. Yet, batching heterogeneous ML requests leads to additional computation overhead as small requests need to be ""padded"" to the same size as large requests within the same batch. Reaching effective batching decisions (i.e., which requests should be batched together and why) is non-trivial: the padding overhead coupled with the serverless auto-scaling forms a complex optimization problem.To address this, we develop Multi-Buffer Serving (MBS), a framework that optimizes the batching of heterogeneous ML inference serving requests to minimize their monetary cost while meeting their service level objectives (SLOs). The core of MBS is a performance and cost estimator driven by analytical models supercharged by a Bayesian optimizer. MBS is prototyped and evaluated on AWS using bursty workloads. Experimental results show that MBS preserves SLOs while outperforming the state-of-the-art by up to 8 x in terms of cost savings while minimizing the padding overhead by up to 37 x with 3 x less number of serverless function invocations.","2022-06","2025-03-18 09:07:50","2025-03-18 09:07:50","","2071–2084","","10","15","","","","","","","","","","","","","","","","","Publisher: VLDB Endowment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3GGEZ3DU","journalArticle","2022","Wang, Chengpeng; Yao, Peisen; Tang, Wensheng; Shi, Qingkai; Zhang, Charles","Complexity-guided container replacement synthesis","Proc. ACM Program. Lang.","","","10.1145/3527312","https://doi.org/10.1145/3527312","Containers, such as lists and maps, are fundamental data structures in modern programming languages. However, improper choice of container types may lead to significant performance issues. This paper presents Cres, an approach that automatically synthesizes container replacements to improve runtime performance. The synthesis algorithm works with static analysis techniques to identify how containers are utilized in the program, and attempts to select a method with lower time complexity for each container method call. Our approach can preserve program behavior and seize the opportunity of reducing execution time effectively for general inputs. We implement Cres and evaluate it on 12 real-world Java projects. It is shown that Cres synthesizes container replacements for the projects with 384.2 KLoC in 14 minutes and discovers six categories of container replacements, which can achieve an average performance improvement of 8.1%.","2022-04","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","OOPSLA1","6","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","data structure specification; program optimization; program synthesis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q47PKZY7","journalArticle","2021","Oikonomou, Panagiotis; Karanika, Anna; Anagnostopoulos, Christos; Kolomvatsos, Kostas","On the Use of Intelligent Models towards Meeting the Challenges of the Edge Mesh","ACM Comput. Surv.","","0360-0300","10.1145/3456630","https://doi.org/10.1145/3456630","Nowadays, we are witnessing the advent of the Internet of Things (IoT) with numerous devices performing interactions between them or with their environment. The huge number of devices leads to huge volumes of data that demand the appropriate processing. The “legacy” approach is to rely on Cloud where increased computational resources can realize any desired processing. However, the need for supporting real-time applications requires a reduced latency in the provision of outcomes. Edge Computing (EC) comes as the “solver” of the latency problem. Various processing activities can be performed at EC nodes having direct connection with IoT devices. A number of challenges should be met before we conclude a fully automated ecosystem where nodes can cooperate or understand their status to efficiently serve applications. In this article, we perform a survey of the relevant research activities towards the vision of Edge Mesh (EM), i.e., a “cover” of intelligence upon the EC. We present the necessary hardware and discuss research outcomes in every aspect of EC/EM nodes functioning. We present technologies and theories adopted for data, tasks, and resource management while discussing how machine learning and optimization can be adopted in the domain.","2021-07","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","6","54","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Internet of Things; Machine Learning; Edge Computing; Data Management; Edge Mesh; Resources Management; Tasks Management","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I8CHJNIC","journalArticle","2020","Luu, Quang-Trung; Kerboeuf, Sylvaine; Mouradian, Alexandre; Kieffer, Michel","A Coverage-Aware Resource Provisioning Method for Network Slicing","IEEE/ACM Trans. Netw.","","1063-6692","10.1109/TNET.2020.3019098","https://doi.org/10.1109/TNET.2020.3019098","With network slicing in 5G networks, Mobile Network Operators can create various slices for Service Providers (SPs) to accommodate customized services. Usually, the various Service Function Chains (SFCs) belonging to a slice are deployed on a best-effort basis. Nothing ensures that the Infrastructure Provider (InP) will be able to allocate enough resources to cope with the increasing demands of some SP. Moreover, in many situations, slices have to be deployed over some geographical area: coverage as well as minimum per-user rate constraints have then to be taken into account. This paper takes the InP perspective and proposes a slice resource &lt;italic&gt;provisioning&lt;/italic&gt; approach to cope with multiple slice demands in terms of computing, storage, coverage, and rate constraints. The resource requirements of the various SFCs within a slice are aggregated within a graph of Slice Resource Demands (SRD). Infrastructure nodes and links have then to be provisioned so as to satisfy all SRDs. This problem leads to a Mixed Integer Linear Programming formulation. A two-step approach is considered, with several variants, depending on whether the constraints of each slice to be provisioned are taken into account sequentially or jointly. Once provisioning has been performed, any slice deployment strategy may be considered on the reduced-size infrastructure graph on which resources have been provisioned. Simulation results demonstrate the effectiveness of the proposed approach compared to a more classical direct slice embedding approach.","2020-12","2025-03-18 09:07:50","2025-03-18 09:07:50","","2393–2406","","6","28","","","","","","","","","","","","","","","","","Publisher: IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E7RBKHG7","journalArticle","2017","Fokaefs, Marios; Barna, Cornel; Litoiu, Marin","From DevOps to BizOps: Economic Sustainability for Scalable Cloud Applications","ACM Trans. Auton. Adapt. Syst.","","1556-4665","10.1145/3139290","https://doi.org/10.1145/3139290","Virtualization of resources in cloud computing has enabled developers to commission and recommission resources at will and on demand. This virtualization is a coin with two sides. On one hand, the flexibility in managing virtual resources has enabled developers to efficiently manage their costs; they can easily remove unnecessary resources or add resources temporarily when the demand increases. On the other hand, the volatility of such environment and the velocity with which changes can occur may have a greater impact on the economic position of a stakeholder and the business balance of the overall ecosystem. In this work, we recognise the business ecosystem of cloud computing as an economy of scale and explore the effect of this fact on decisions concerning scaling the infrastructure of web applications to account for fluctuations in demand. The goal is to reveal and formalize opportunities for economically optimal scaling that takes into account not only the cost of infrastructure but also the revenue from service delivery and eventually the profit of the service provider. The end product is a scaling mechanism that makes decisions based on both performance and economic criteria and takes adaptive actions to optimize both performance and profitability for the system.","2017-11","2025-03-18 09:07:50","2025-03-18 09:07:50","","","","4","12","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Cloud computing; DevOps; self-adaptive systems; software engineering economics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""