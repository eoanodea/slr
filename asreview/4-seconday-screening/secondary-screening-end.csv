record_id,type_of_reference,doi,authors,title,journal_name,date,publisher,abstract,keywords,issn,year,urls,asreview_prior,exported_notes_1,included,asreview_ranking
38,JOUR,10.1145/3715700,"['Yang, Yihong', 'Zhou, Zhangbing', 'Shu, Lei', 'Zhou, Feng', 'Gaaloul, Walid', 'Khan, Arif Ali']",Web 3.0-Enabled Microservice Re-Scheduling for Heterogenous Resources Co-Optimization in Metaverse-Integrated Edge Networks,ACM Trans. Auton. Adapt. Syst.,2025-01,,"The Web 3.0 and metaverse can empower intelligent application of Connected Autonomous Vehicles (CAVs). The adoption of edge computing can contribute to the low latency interaction between CAVs and the metaverse. Microservices are widely deployed on edge networks and the cloud nowadays. User's requests from CAVs are typically fulfilled through the composition of microservices, which may be hosted by contiguous edge nodes. Requests may differ on their required resources at runtime. Consequently, when requests are continuously injected into edge networks, the usage of heterogenous resources, including CPU, memory, and network bandwidth, may not be the same, or differ significantly, on certain edge nodes. This happens especially when burst requests are injected into the network to be satisfied concurrently. Therefore, the usage of heterogenous resources provided by edge nodes should be co-optimized through re-scheduling microservices. To address this challenge, this paper proposes a Web 3.0-enabled Microservice Re-Scheduling approach (called MRS), which is a migration-based mechanism integrating a placement strategy. Specifically, we formulate the microservice re-scheduling task as a multi-objective and multi-constraint optimization problem, which can be solved through a penalty signal-integrated framework and an improved pointer network. Extensive experiments are conducted on two real-world datasets. Evaluation results show that our MRS performs better than the counterparts with improvements of at least 7.7%, 2.4% and 2.2% in terms of network throughput, latency and energy consumption.","['Connected Autonomous Vehicles (CAVs)', 'Heterogenous Resources Co-Optimization', 'Metaverse', 'Microservice Re-Scheduling', 'Web 3.0']",1556-4665,,['https://doi.org/10.1145/3715700'],1,,1,1
22,JOUR,10.3390/s23167142,"['Ashraf M.', 'Shiraz M.', 'Abbasi A.', 'Alqahtani O.', 'Badshah G.', 'Lasisi A.']",Microservice Application Scheduling in Multi-Tiered Fog-Computing-Enabled IoT,Sensors,,,"Fog computing extends mobile cloud computing facilities at the network edge, yielding low-latency application execution. To supplement cloud services, computationally intensive applications can be distributed on resource-constrained mobile devices by leveraging underutilized nearby resources to meet the latency and bandwidth requirements of application execution. Building upon this premise, it is necessary to investigate idle or underutilized resources that are present at the edge of the network. The utilization of a microservice architecture in IoT application development, with its increased granularity in service breakdown, provides opportunities for improved scalability, maintainability, and extensibility. In this research, the proposed schedule tackles the latency requirements of applications by identifying suitable upward migration of microservices within a multi-tiered fog computing infrastructure. This approach enables optimal utilization of network edge resources. Experimental validation is performed using the iFogSim2 simulator and the results are compared with existing baselines. The results demonstrate that compared to the edgewards approach, our proposed technique significantly improves the latency requirements of application execution, network usage, and energy consumption by 66.92%, 69.83%, and 4.16%, respectively. © 2023 by the authors.","['constrained devices', 'distributed application execution', 'fog computing', 'Internet of Things', 'microservice application scheduling', 'service delay', 'Energy utilization', 'Fog', 'Internet of things', 'Mobile cloud computing', 'Application execution', 'Application scheduling', 'Computing facilities', 'Constrained devices', 'Distributed application executions', 'Low latency', 'Microservice application scheduling', 'Multi-tiered', 'Network edges', 'Service delays', 'Fog computing']",,2023,,0,"IC7,IC8,IC9",1,2
95,JOUR,10.1002/spe.3217,"['Wu J.', 'Zhang J.', 'Zhang Y.', 'Wen Y.']",Constraint-aware and multi-objective optimization for micro-service composition in mobile edge computing,Software - Practice and Experience,,,"As a new paradigm of distributed computing, mobile edge computing (MEC) has gained increasing attention due to its ability to expand the capabilities of centralized cloud computing. In MEC environments, a software application typically consists of multiple micro-services, which can be composed together in a flexible manner to achieve various user requests. However, the composition of micro-services in MEC is still a challenging research issue arising from three aspects. Firstly, composite micro-services constructed by ignoring the processing capabilities of different micro-services may cause waste of edge resources. Secondly, edge servers' limitations in terms of computational power can easily cause service occupancy between composite micro-services, severely affecting the user experience. Thirdly, in dynamic and unstable mobile environments, different edge users have different sensitivities to request latency, which increases the complexity of micro-service composition. In order to improve edge resource utilization and user experience on micro-service invocations, in this paper, we comprehensively consider the above three factors, and we first model the micro-services composition problem in MEC as a constrained multi-objective optimization problem. Then, a micro-service composition optimization method M3C combining graph search and branch-and-bound strategy is proposed to find a composition solution set with low energy consumption and high success rate for multiple edge users. Finally, we perform a series of experiments on two widely used datasets. Experimental results show that our proposed approach significantly outperforms the four competing baseline approaches, and that it is sufficiently efficient for practical deployment. © 2023 John Wiley & Sons Ltd.","['micro-service composition', 'micro-services', 'mobile edge computing', 'multi-objective optimization', 'Application programs', 'Branch and bound method', 'Constrained optimization', 'Energy utilization', 'Mobile edge computing', 'Quality of service', 'Centralised', 'Cloud-computing', 'Computing environments', 'Edge resources', 'Micro services', 'Micro-service composition', 'Multi-objectives optimization', 'Services composition', ""Users' experiences"", 'Multiobjective optimization']",,2024,,0,IC6,1,3
83,JOUR,10.1109/GLOBECOM38437.2019.9014114,"['J. Gedeon', 'M. Wagner', 'J. Heuschkel', 'L. Wang', 'M. Muhlhauser']",A Microservice Store for Efficient Edge Offloading,2019 IEEE Global Communications Conference (GLOBECOM),27-Feb-20,IEEE,"Current edge computing frameworks require tight coupling between mobile clients and surrogates, i.e., the offloaded code has been preconfigured with its required execution environment. In many cases, this includes prior transfers of code blocks or execution environments from mobile devices to the offloading infrastructure. This approach incurs additional latency and is detrimental for the energy consumption of the mobile devices. In this paper, we propose the concept of a microservice store. Using the microservice abstraction common in software development and following the serverless paradigm, we envision a repository through which said services are made accessible to developers and can be re-used across applications. We implement a proof-of-concept edge computing system based on a microservice repository and demonstrate its benefits with real-world applications on mobile devices. Our results show that we were able to reduce latencies by up to 14x and save up to 94% of battery life.","['Edge computing', 'Object detection', 'Mobile handsets', 'Containers', 'Semantics', 'Cloud computing', 'Image edge detection']",2576-6813,,,0,IC9,1,4
4,JOUR,10.1109/TITS.2023.3274307,"['L. Wang', 'X. Deng', 'J. Gui', 'X. Chen', 'S. Wan']",Microservice-Oriented Service Placement for Mobile Edge Computing in Sustainable Internet of Vehicles,IEEE Transactions on Intelligent Transportation Systems,17-May-23,IEEE,"The integration of Mobile Edge Computing (MEC) and microservice architecture drives the implementation of the sustainable Internet of Vehicles (IoV). The microservice architecture enables the decomposition of a service into multiple independent, fine-grained microservices working independently. With MEC, microservices can be placed on Edge Service Providers (ESPs) dynamically, responding quickly and reducing service latency and resource consumption. However, the burgeoning of IoV leads to high computation and resource overheads, making service resource requirements an imminent issue. What’s more, due to the limited computation power of ESPs, they can only host a few services. Therefore, ESPs should judiciously decide which services to host. In this paper, we propose a Microservice-oriented Service Placement (MOSP) mechanism for MEC-enabled IoV to shorten service latency, reduce high resource consumption levels and guarantee long-term sustainability. Specifically, we formulate the service placement as an integer linear programming program, where service placement decisions are collaboratively optimized among ESPs, aiming to address spatial demand coupling, service heterogeneity, and decentralized coordination in MEC systems. MOSP comprises an upper layer to map the service requests to ESPs and a lower layer to adjust the service placement of ESPs. Evaluation results show that the microservice-oriented service deployment mechanism offers dramatic improvements in terms of resource savings, latency reduction, and service speed.","['Mobile edge computing', 'microservice', 'Internet of Vehicles', 'service placement', 'Microservice architectures', 'Servers', 'Cloud computing', 'Quality of service', 'Costs', 'Multi-access edge computing', 'Memory management']",1558-0016,,,0,IC9,1,5
12,JOUR,10.1109/ACCESS.2024.3461149,"['K. Afachao', 'A. M. Abu-Mahfouz', 'G. P. Hanke']",Efficient Microservice Deployment in the Edge-Cloud Networks With Policy-Gradient Reinforcement Learning,IEEE Access,16-Sep-24,IEEE,"The rise of user-centric design demands ubiquitous access to infrastructure and applications, facilitated by the Edge-Cloud network and microservices. However, efficiently managing resource allocation while orchestrating microservice placement in such dynamic environments presents a significant challenge. These challenges stem from the limited resources of edge devices, the need for low latency responses, and the potential for performance degradation due to service failures or inefficient deployments. This paper addresses the challenge of microservice placement in Edge-Cloud environments by proposing a novel Reinforcement Learning algorithm called Bi-Generic Advantage Actor-Critic for Microservice Placement Policy. This algorithm’s ability to learn and adapt to the dynamic environment makes it well-suited for optimizing resource allocation and service placement decisions within the Edge-Cloud. We compare this algorithm against three baseline algorithms through simulations on a real-world dataset, evaluating performance metrics such as execution time, network usage, average migration delay, and energy consumption. The results demonstrate the superiority of the proposed method, with an 8% reduction in execution time, translating to faster response times for users. Additionally, it achieves a 4% decrease in network usage and a 2% decrease in energy consumption compared to the best-performing baseline. This research contributes by reproducing the Edge-Cloud environment, applying the novel Bi-Generic Advantage Actor-Critic technique, and demonstrating significant improvements over the state-of-the-art baseline algorithms in microservice placement and resource management within Edge-Cloud environments.","['Edge computing', 'microservices', 'network optimization', 'online placement', 'scheduling algorithms', 'reinforcement learning', 'Microservice architectures', 'Heuristic algorithms', 'Reliability', 'Optimization methods', 'Resource management', 'Containers', 'Edge computing', 'Scheduling', 'User centered design', 'Reinforcement learning']",2169-3536,,,0,IC8,1,6
11,JOUR,10.1016/j.jnca.2019.102441,"['Yu Y.', 'Yang J.', 'Guo C.', 'Zheng H.', 'He J.']",Joint optimization of service request routing and instance placement in the microservice system,Journal of Network and Computer Applications,,,"Microservice architecture is a promising architectural style. It decomposes monolithic software into a set of loosely coupled containerized microservices and associates them into multiple microservice chains to serve service requests. The new architecture creates flexibility for service provisioning but also introduces increased energy consumption and low service performance. Efficient resource allocation is critical. Unfortunately, existing solutions are designed at a coarse level for virtual machine (VM)-based clouds and not optimized for such chain-oriented service provisioning. In this paper, we study the resource allocation optimization problem for service request routing and microservice instance placement, so as to jointly reduce both resource usage and chains’ end-to-end response time for saving energy and guaranteeing Quality of Service (QoS). We design detailed workload models for microservices and chains and formulate the optimization problem as a bi-criteria optimization problem. To address it, a three-stage scheme is proposed to search and optimize the trade-off decisions, route service requests into instances and deploy instances to servers in a balanced manner. Through numerical evaluations, we show that while assuring the same QoS, our scheme performs significantly better than and faster than benchmarking algorithms on reducing energy consumption and balancing load. © 2019","['Bi-criteria optimization', 'Energy consumption', 'Load balance', 'Microservice', 'Microservice chain', 'QoS', 'Balancing', 'Economic and social effects', 'Energy utilization', 'Optimization', 'Resource allocation', 'Benchmarking algorithm', 'Bicriteria optimization', 'Efficient resource allocation', 'End-to-end response time', 'Load balance', 'Microservice', 'Reducing energy consumption', 'Resource allocation optimization', 'Quality of service']",,2019,,0,IC7,1,7
13,JOUR,10.1109/TC.2025.3535826,"['Wang L.', 'Liu X.', 'Ding H.', 'Hu Y.', 'Peng K.', 'Hu M.']",Energy-Delay-Aware Joint Microservice Deployment and Request Routing with DVFS in Edge: A Reinforcement Learning Approach,IEEE Transactions on Computers,,,"The emerging microservice architecture offers opportunities for accommodating delay-sensitive applications in edge. However, such applications are computation-intensive and energy-consuming, imposing great difficulties to edge servers with limited computing resources, energy supply, and cooling capabilities. To reduce delay and energy consumption in edge, efficient microservice orchestration is necessary, but significantly challenging. Due to frequent communications among multiple microservices, service deployment and request routing are tightly-coupled, which motivates a complex joint optimization problem. When considering multi-instance modeling and fine-grained orchestration for massive microservices, the difficulty is extremely enlarged. Nevertheless, previous work failed to address the above difficulties. Also, they neglected to balance delay and energy, especially lacking dynamic energy-saving abilities. Therefore, this paper minimizes energy and delay by jointly optimizing microservice deployment and request routing via multi-instance modeling, fine-grained orchestration, and dynamic adaptation. Our queuing network model enables accurate end-to-end time analysis covering queuing, computing, and communicating delays. We then propose a delay-aware reinforcement learning algorithm, which derives the static service deployment and routing decisions. Moreover, we design an energy-aware dynamic frequency scaling algorithm, which saves energy with fluctuating request patterns. Experiment results demonstrate that our approaches significantly outperform baseline algorithms in both delay and energy consumption.  © 1968-2012 IEEE.","['dynamic voltage frequency scaling', 'Edge computing', 'queuing theory', 'reinforcement learning', 'service orchestration', 'Delay tolerant networks', 'Delay-sensitive applications', 'Dynamic frequency scaling', 'Edge computing', 'Energy efficiency', 'Energy utilization', 'Finite automata', 'Learning algorithms', 'Queueing networks', 'Reinforcement learning', 'Voltage scaling', 'Delay aware', 'Dynamic voltage', 'Dynamic voltage frequency scaling', 'Edge computing', 'Frequency-scaling', 'Queuing theory', 'Reinforcement learnings', 'Routings', 'Service orchestration', 'Voltage frequency', 'Queueing theory']",,2025,,0,IC9,1,8
17,JOUR,10.1109/JIOT.2022.3155598,"['Yu Y.', 'Liu J.', 'Fang J.']",Online Microservice Orchestration for IoT via Multiobjective Deep Reinforcement Learning,IEEE Internet of Things Journal,,,"By providing loosely coupled, lightweight, and independent services, the microservice architecture is promising for large-scale and complex service provision requirements in the Internet of Things (IoT). However, it requires more fine-grained resource management and orchestration for service provision. Most of the existing microservice orchestration solutions are based on those designed for the traditional cloud. They can only provide coarse-grained resource allocation using possibly conflicting weighted objectives. In this article, we present a fine-grained microservice orchestration approach to provide services online for dynamic requests of IoT applications. By using a fine-grained resource model of energy cost and service end-to-end response time of orchestrated microservices, we formulate the microservice orchestration problem as a multiobjective Markov decision process. We then propose a multiobjective optimization solution based on deep reinforcement learning (DRL) to simultaneously reduce energy consumption and response time. Through extensive experiments, our proposed algorithm presents significant performance results than the state of the art. To the best of our knowledge, this is the first work that addresses microservice orchestration using DRL for multiple conflicting objectives.  © 2014 IEEE.","['Deep reinforcement learning (DRL)', 'energy consumption, Internet of Things (IoT)', 'online microservice orchestration', 'Quality-of-Service (QoS) assurance', 'Deep learning', 'Internet of things', 'Markov processes', 'Multiobjective optimization', 'Natural resources management', 'Quality of service', 'Reinforcement learning', 'Resource allocation', 'Deep reinforcement learning', 'Energy-consumption', 'Fine grained', 'Microservice architecture', 'Multi objective', 'Online microservice orchestration', 'QoS assurance.', 'Quality-of-service', 'Reinforcement learnings', 'Resource management', 'Energy utilization']",,2022,,0,IC9,1,9
19,JOUR,10.1109/TNSM.2024.3497155,"['Adeppady M.', 'Conte A.', 'Giaccone P.', 'Karl H.', 'Chiasserini C.F.']",Dynamic Management of Constrained Computing Resources for Serverless Services,IEEE Transactions on Network and Service Management,,,"In resource-constrained cloud systems, e.g., at the network edge or in private clouds, serverless computing is increasingly adopted to deploy microservices-based applications, leveraging its promised high resource efficiency. Provisioning resources to serverless services, however, poses several challenges, due to the high cold-start latency of containers and stringent Service Level Agreement (SLA) requirements of the microservices. In response, we investigate the behavior of containers in different states (i.e., running, warm, or cold) and exploit our experimental observations to formulate an optimization problem that minimizes the energy consumption of the active servers while reducing SLA violations. In light of the problem complexity, we propose a low-complexity algorithm, named AiW, which utilizes a multi-queueing approach to balance energy consumption and system performance by reusing containers effectively and invoking cold-starts only when necessary. To further minimize the energy consumption of data centers, we introduce the two-timescale COmputing resource Management at the Edge (COME) framework, comprising an orchestrator running our proposed AiW algorithm for container provisioning and Dynamic Server Provisioner (DSP) for dynamically activating/deactivating servers in response to AiW's decisions on request scheduling. COME addresses the mismatch in timescales for resource provisioning decisions at the container and server levels. Extensive performance evaluation through simulation shows AiW's close match to the optimum and COME's significant reduction in power consumption by 22-64% compared state-of-the-art alternatives. © 2004-2012 IEEE.","['Cloud Computing Services', 'Edge Computing', 'Energy-aware Management', 'Orchestration', 'Edge computing', 'Energy utilization', 'Information management', 'Cloud computing services', 'Cold-start', 'Dynamic management', 'Edge computing', 'Energy aware', 'Energy-aware management', 'Energy-consumption', 'Orchestration', 'Servicelevel agreement (SLA)', 'Time-scales', 'Cloud platforms']",,2024,,0,IC9,1,10
80,JOUR,10.1109/JIOT.2020.3011057,"['S. R. Chaudhry', 'A. Palade', 'A. Kazmi', 'S. Clarke']",Improved QoS at the Edge Using Serverless Computing to Deploy Virtual Network Functions,IEEE Internet of Things Journal,21-Jul-20,IEEE,"Multiaccess edge computing (MEC) will strengthen forthcoming 5G networks by improving the Quality of Service (QoS), in particular, reducing latency, increasing data processing rates, and providing real-time information to develop high-value Internet-of-Things (IoT) services. To enable data-intensive network services and support advanced analytics, many network operators have proposed to integrate MEC systems with network function virtualization (NFV) consolidating virtual network functions (VNFs) and edge capabilities on a shared infrastructure. As of yet, this integration is not fully established, with various architectural issues currently open, even at standardization level. For instance, any update to VNFs deployed in a MEC system requires a time-consuming manual effort, which affects the overall infrastructure operations. To address these pitfalls, VNFs can be decomposed into microservices, which maintain their own states and exhibit different resource consumption requirements. This article presents an approach to integration that leverages serverless computing to merge MEC and NFV at the system level and to deploy VNFs on demand, by combining MEC functional blocks with an NFV orchestrator using a Kubernetes cluster. We further investigate whether the resource utilization of a MEC system can be improved by leveraging networked FPGA-enabled MEC servers, through an extension of the edge layer that takes advantage of available programmable hardware. We quantitatively evaluate and demonstrate the improvement of 75% end-to-end latency, 99.96% VNF execution time, 26.9% resource utilization, and 15.8% energy consumption in comparison with traditional baselines of cloud, edge, and serverless-edge test cases for a high-definition real-time video streaming application.","['Edge computing', 'networked FPGA', 'serverless computing', 'virtual network functions (VNFs) and data security', 'Hardware', 'Internet of Things', 'Field programmable gate arrays', 'Streaming media', 'Servers', 'Task analysis', 'Quality of service']",2327-4662,,,0,IC9,1,11
9,JOUR,10.1109/ICPADS56603.2022.00082,"['C. Song', 'F. Jiang', 'X. Liang', 'N. Ahuja', 'M. J. Kumar']",Service Level Objective Adaptive Energy Efficiency Management,2022 IEEE 28th International Conference on Parallel and Distributed Systems (ICPADS),27-Mar-23,IEEE,"Cloud applications are transforming to microservice based deployment. Microservice comes with many advantages-scalability, agility, availability – and it suits the complicated cloud applications. For microservice deployment, service quality metrics play important role to ensure reliable user experiences for those applications built upon many small services, and the service level objective (SLO) is adopted to govern resource provision for optimal operation cost. Amid growing awareness on data center carbon emission, more organizations are paying attention to energy-efficient practices in cloud deployment to reduce energy consumption and operational carbon footprint. Modern processors provide fine-grained power scaling capabilities, e.g., Intel Xeon PCPS (PerCore P-States), this paper presented a dynamic energy efficiency management framework to adaptively scale processor core frequency as well as implicated energy consumption in accordance with service quality requirements. In this paper, we discussed hurdles and solutions for applying platform energy intelligence to microservices that are often with intrinsic platform-agnostic. The experiments proved the proposed method can effectively reduce energy consumption with competitive cost benefits without compromising service quality goals.","['Microservices', 'Service Mesh', 'Per-Core P-States', 'Service Level Objective (SLO)', 'Service Level Indicator (SLI)', 'Energy Efficiency', 'Carbon Reduction', 'Sustainability', 'Adaptive Control', 'Core Frequency Scaling', 'Energy consumption', 'Program processors', 'Costs', 'Power system management', 'Memory management', 'Microservice architectures', 'Tail']",2690-5965,,,0,IC7,1,12
40,JOUR,10.1145/3423211.3425683,"['Gunasekaran, Jashwant Raj', 'Thinakaran, Prashanth', 'Nachiappan, Nachiappan C.', 'Kandemir, Mahmut Taylan', 'Das, Chita R.']",Fifer: Tackling Resource Underutilization in the Serverless Era,Proceedings of the 21st International Middleware Conference,2020,Association for Computing Machinery,"Datacenters are witnessing a rapid surge in the adoption of serverless functions for microservices-based applications. A vast majority of these microservices typically span less than a second, have strict SLO requirements, and are chained together as per the requirements of an application. The aforementioned characteristics introduce a new set of challenges, especially in terms of container provisioning and management, as the state-of-the-art resource management frameworks, employed in serverless platforms, tend to look at microservice-based applications similar to conventional monolithic applications. Hence, these frameworks suffer from microservice agnostic scheduling and colossal container over-provisioning, especially during workload fluctuations, thereby resulting in poor resource utilization.In this work, we quantify the above shortcomings using a variety of workloads on a multi-node cluster managed by the Kubernetes and Brigade serverless framework. To address them, we propose Fifer — an adaptive resource management framework to efficiently manage function-chains on serverless platforms. The key idea is to make Fifer (i) utilization conscious by efficiently bin packing jobs to fewer containers using function-aware container scaling and intelligent request batching, and (ii) at the same time, SLO-compliant by proactively spawning containers to avoid cold-starts, thus minimizing the overall response latency. Combining these benefits, Fifer improves container utilization and cluster-wide energy consumption by 4× and 31%, respectively, without compromising on SLO's, when compared to the state-of-the-art schedulers employed by serverless platforms.","['scheduling', 'serverless', 'resource-management', 'queuing']",978-1-4503-8153-6,,['https://doi.org/10.1145/3423211.3425683'],0,IC9,1,13
49,JOUR,10.1145/3472883.3486992,"['Bhasi, Vivek M.', 'Gunasekaran, Jashwant Raj', 'Thinakaran, Prashanth', 'Mishra, Cyan Subhra', 'Kandemir, Mahmut Taylan', 'Das, Chita']",Kraken: Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms,Proceedings of the ACM Symposium on Cloud Computing,2021,Association for Computing Machinery,"The growing popularity of microservices has led to the proliferation of online cloud service-based applications, which are typically modelled as Directed Acyclic Graphs (DAGs) comprising of tens to hundreds of microservices. The vast majority of these applications are user-facing, and hence, have stringent SLO requirements. Serverless functions, having short resource provisioning times and instant scalability, are suitable candidates for developing such latency-critical applications. However, existing serverless providers are unaware of the workflow characteristics of application DAGs, leading to container over-provisioning in many cases. This is further exacerbated in the case of dynamic DAGs, where the function chain for an application is not known a priori. Motivated by these observations, we propose Kraken, a workflow-aware resource management framework that minimizes the number of containers provisioned for an application DAG while ensuring SLO-compliance. We design and implement Kraken on OpenFaaS and evaluate it on a multi-node Kubernetes-managed cluster. Our extensive experimental evaluation using DeathStarbench workload suite and real-world traces demonstrates that Kraken spawns up to 76% fewer containers, thereby improving container utilization and saving cluster-wide energy by up to 4x and 48%, respectively, when compared to state-of-the art schedulers employed in serverless platforms.","['scheduling', 'serverless', 'resource-management', 'queuing']",978-1-4503-8638-8,,['https://doi.org/10.1145/3472883.3486992'],0,IC9,1,14
7,JOUR,10.1109/ACCESS.2021.3075973,"['E. Ahvar', 'S. Ahvar', 'Z. Á. Mann', 'N. Crespi', 'R. Glitho', 'J. Garcia-Alfaro']",DECA: A Dynamic Energy Cost and Carbon Emission-Efficient Application Placement Method for Edge Clouds,IEEE Access,27-Apr-21,IEEE,"As an increasing amount of data processing is done at the network edge, high energy costs and carbon emission of Edge Clouds (ECs) are becoming significant challenges. The placement of application components (e.g., in the form of containerized microservices) on ECs has an important effect on the energy consumption of ECs, impacting both energy costs and carbon emissions. Due to the geographic distribution of ECs, there is a variety of resources, energy prices and carbon emission rates to consider, which makes optimizing the placement of applications for cost and carbon efficiency even more challenging than in centralized clouds. This paper presents a Dynamic Energy cost and Carbon emission-efficient Application placement method (DECA) for ECs. DECA addresses both the initial placement of applications on ECs and the re-optimization of the placement using migrations. DECA considers geographically varying energy prices and carbon emission rates as well as optimizing the usage of both network and computing resources at the same time. By combining a prediction-based A* algorithm with a Fuzzy Sets technique, DECA makes intelligent decisions to optimize energy cost and carbon emissions. Simulation results show the ability of DECA in providing a tradeoff and optimizing energy cost and carbon emission at the same time.","['Edge cloud', 'energy consumption', 'energy costs', 'green computing', 'carbon emission', 'application placement', 'Carbon dioxide', 'Energy consumption', 'Internet of Things', 'Cloud computing', 'Optimization', 'Resource management', 'Time factors']",2169-3536,,,0,IC9,1,15
18,JOUR,10.1109/ICAC.2019.00018,"['N. Schmitt', 'L. Iffländer', 'A. Bauer', 'S. Kounev']",Online Power Consumption Estimation for Functions in Cloud Applications,2019 IEEE International Conference on Autonomic Computing (ICAC),12-Sep-19,IEEE,"The growth of cloud services leads to more and more data centers that are increasingly larger and consume considerable amounts of power. To increase energy efficiency, informed decisions on workload placement and provisioning are essential. Micro-services and the upcoming serverless platforms with more granular deployment options exacerbate this problem. For this reason, knowing the power consumption of the deployed application becomes crucial, providing the necessary information for autonomous decision making. However, the actual power draw of a server running a specific application under load is not available without specialized measurement equipment or power consumption models. Yet, granularity is often only down to machine level and not application level. In this paper, we propose a monitoring and modeling approach to estimate power consumption on an application function level. The model uses performance counters that are allocated to specific functions to assess their impact on the total power consumption. Hence our model applies to a large variety of servers and for micro-service and serverless workloads. Our model uses an additional correction to minimize falsely allocated performance counters and increase accuracy. We validate the proposed approach on real hardware with a dedicated benchmarking application. The evaluation shows that our approach can be used to monitor application power consumption down to the function level with high accuracy for reliable workload provisioning and placement decisions.","['Energy efficiency, serverless, micro-services, code offloading, DevOps', 'Power demand', 'Data centers', 'Power measurement', 'Load modeling', 'Monitoring', 'Hardware', 'Software']",2474-0756,,,0,IC8,1,16
10,JOUR,10.3390/app12125793,"['Saboor A.', 'Hassan M.F.', 'Akbar R.', 'Shah S.N.M.', 'Hassan F.', 'Magsi S.A.', 'Siddiqui M.A.']",Article Containerized Microservices Orchestration and Provisioning in Cloud Computing: A Conceptual Framework and Future Perspectives,Applied Sciences (Switzerland),,,"Cloud computing is a rapidly growing paradigm which has evolved from having a mono-lithic to microservices architecture. The importance of cloud data centers has expanded dramatically in the previous decade, and they are now regarded as the backbone of the modern economy. Cloud-based microservices architecture is incorporated by firms such as Netflix, Twitter, eBay, Amazon, Hailo, Groupon, and Zalando. Such cloud computing arrangements deal with the parallel deployment of data-intensive workloads in real time. Moreover, commonly utilized cloud services such as the web and email require continuous operation without interruption. For that purpose, cloud service providers must optimize resource management, efficient energy usage, and carbon footprint reduction. This study presents a conceptual framework to manage the high amount of microservice execution while reducing response time, energy consumption, and execution costs. The proposed framework suggests four key agent services: (1) intelligent partitioning: responsible for microservice classification; (2) dynamic allocation: used for pre-execution distribution of microservices among containers and then makes decisions for dynamic allocation of microservices at runtime; (3) resource optimization: in charge of shifting workloads and ensuring optimal resource use; (4) mutation ac-tions: these are based on procedures that will mutate the microservices based on cloud data center workloads. The suggested framework was partially evaluated using a custom-built simulation envi-ronment, which demonstrated its efficiency and potential for implementation in a cloud computing context. The findings show that the engrossment of suggested services can lead to a reduced number of network calls, lower energy consumption, and relatively reduced carbon dioxide emissions. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","['cloud computing', 'containers', 'microservices', 'multicloud', 'virtual machine']",,2022,,0,IC9,1,17
21,JOUR,10.1109/TGCN.2024.3420957,"['Z. Bellal', 'L. Lahlou', 'N. Kara', 'I. El Khayat']",GAS: DVFS-Driven Energy Efficiency Approach for Latency-Guaranteed Edge Computing Microservices,IEEE Transactions on Green Communications and Networking,01-Jul-24,IEEE,"Edge computing-based microservices (ECM) are pivotal infrastructure components for latency-critical applications such as Virtual Reality/Augmented Reality (VR/AR) and the Internet of Things (IoT). ECM involves strategically deploying microservices at the network’s edge to fulfill the low latency needs of modern applications. However, achieving efficient resource and energy consumption while meeting the latency requirement in the ECM environment remains challenging. Dynamic Voltage and Frequency Scaling (DVFS) is a common technique to address this issue. It adjusts the CPU frequency and voltage to balance energy cost and performance. However, selecting the optimal CPU frequency depends on the nature of the microservice workload (e.g., CPU-bound, memory-bound, or mixed). Moreover, various microservices with different latency requirement can be deployed on the same edge node. This makes the DVFS application extremely challenging, particularly for a chip-wide DVFS implementation for which CPU cores operate at the same frequency and voltage. To this end, we propose GAS, enerGy Aware microServices edge computing framework, which enables CPU frequency scaling to meet diverse microservice latency requirement with the minimum energy cost. Our evaluation indicates that our CPU scaling policy decreases energy consumption by 5% to 23% compared to Linux governors while maintaining latency requirement and significantly contributing to sustainable edge computing.","['Edge computing', 'microservice', 'DVFS', 'energy-efficient', 'container autoscaling', 'Microservice architectures', 'Edge computing', 'Energy consumption', 'Energy efficiency', 'Task analysis', 'Frequency diversity', 'Time-frequency analysis']",2473-2400,,,0,IC8,1,18
20,JOUR,10.1007/s10723-024-09789-9,"['Agos Jawaddi S.N.', 'Ismail A.', 'Sulaiman M.S.', 'Cardellini V.']",Analyzing Energy-Efficient and Kubernetes-Based Autoscaling of Microservices Using Probabilistic Model Checking,Journal of Grid Computing,,,"Microservices are widely used to enable agility and scalability in modern software systems, while cloud computing offers cost-effective ways to provision computing resources on demand. However, ensuring the correctness of scaling decisions and their impact on energy consumption is a challenging problem that has not been sufficiently addressed in previous research. Thus, in this paper, we present an innovative approach for analyzing host energy consumption and energy violations influenced by microservice autoscaling policies using probabilistic model checking (PMC). We propose four variations of the Markov Decision Process (MDP) models that incorporate various scaling constraints inspired by Kubernetes-based Horizontal Pod Autoscaler, and we encode these models using two different approaches, namely, bounded-by-action (BBA) and bounded-by-state (BBS). We use PMC to verify the scaling policies in terms of host energy consumption and energy violations, and we conduct sensitivity analysis to demonstrate the effectiveness of our models in generating energy-efficient scaling policies. Our results show that the latency and energy-based MDP model offers the most suitable policies for ensuring energy efficiency in microservice systems. Additionally, the number of pods and the scale-out action significantly affect energy consumption and violations. Sensitivity analysis also reveals that incorporating latency into scaling decisions is key to energy efficiency, while variations in the maximum pod threshold significantly influence energy consumption and violation. Our approach provides a formal method for ensuring the correctness of microservice autoscaling decisions in cloud environments at design time and can help reduce energy consumption and violations while ensuring service-level objectives are met. © The Author(s), under exclusive licence to Springer Nature B.V. 2024.","['Energy-driven microservice autoscaling', 'Formal verification', 'Kubernetes', 'Markov decision process', 'Microservices', 'Probabilistic model checking', 'Cloud computing', 'Energy efficiency', 'Formal verification', 'Health risks', 'Model checking', 'Program debugging', 'Sensitivity analysis', 'Autoscaling', 'Energy-consumption', 'Energy-driven', 'Energy-driven microservice autoscaling', 'Kubernetes', 'Markov Decision Processes', 'Microservice', 'Probabilistic model checking', 'Probabilistic model-checking', 'Scalings', 'Markov processes']",,2025,,0,IC8,1,19
16,JOUR,10.1109/TSUSC.2018.2808493,"['M. Xu', 'A. N. Toosi', 'R. Buyya']",iBrownout: An Integrated Approach for Managing Energy and Brownout in Container-Based Clouds,IEEE Transactions on Sustainable Computing,26-Feb-18,IEEE,"Energy consumption of Cloud data centers has been a major concern of many researchers, and one of the reasons for huge energy consumption of Clouds lies in the inefficient utilization of computing resources. Besides energy consumption, another challenge of data centers is the unexpected loads, which leads to the overloads and performance degradation. Compared with VM consolidation and Dynamic Voltage Frequency Scaling that cannot function well when the whole data center is overloaded, brownout has shown to be a promising technique to handle both overloads and energy consumption through dynamically deactivating application optional components, which are also identified as containers/microservices. In this work, we propose an integrated approach to manage energy consumption and brownout in container-based cloud data centers. We also evaluate our proposed scheduling policies with real traces in a prototype system. The results show that our approach reduces about 40, 20, and 10 percent energy than the approach without power-saving techniques, brownout-overbooking approach and auto-scaling approach, respectively, while ensuring Quality of Service.","['Cloud data centers', 'energy efficiency', 'QoS', 'containers', 'microservices', 'brownout', 'Data centers', 'Energy consumption', 'Cloud computing', 'Quality of service', 'Servers', 'Containers', 'Prototypes']",2377-3782,,,0,IC9,1,20
25,JOUR,10.1109/COMSNETS56262.2023.10041344,"['I. Syrigos', 'D. Kefalas', 'N. Makris', 'T. Korakis']",EELAS: Energy Efficient and Latency Aware Scheduling of Cloud-Native ML Workloads,2023 15th International Conference on COMmunication Systems & NETworkS (COMSNETS),15-Feb-23,IEEE,"The widespread use of microservices and the use of cloud-native methodologies for the deployment of services have increased the service providers' flexibility and management efficiency. As the available resources for scheduling such work-loads have extended the boundaries of traditional datacenters to the fog, edge, and beyond -edge, the scheduling of challenging workloads must also account for energy efficiency, as these devices are typically battery-powered and resource-constrained, while maintaining acceptable performance. Specifically for ML inference workloads, provisioning and access latency plays a cru-cial role in their successful operation. Towards combating these issues, we design, develop, and evaluate our platform for Energy Efficient Latency-Aware Scheduling (EELAS) of workloads. First, we formulate the scheduling problem as an ILP problem, and then we develop a less complex heuristic method that allows the efficient allocation of resources within the continuum. Our EELAS prototype integrates with Kubernetes and is capable of reducing the overall energy consumption of cloud-to-things resources while accounting for latency of ML workloads. Our evaluation in real-world settings reveals significant energy gains for scheduling ML inference tasks, also reachable with the minimum possible latency from far-edge devices.","['cloud-to-things continuum', 'energy efficiency', 'latency aware', 'workload scheduling', 'testbed', 'Training', 'Performance evaluation', 'Learning systems', 'Heuristic algorithms', 'Prototypes', 'Microservice architectures', 'Energy efficiency']",2155-2509,,,0,IC8,1,21
15,JOUR,10.1109/TSUSC.2017.2661339,"['Xu M.', 'Dastjerdi A.V.', 'Buyya R.']",Energy Efficient Scheduling of Cloud Application Components with Brownout,IEEE Transactions on Sustainable Computing,,,"It is common for cloud data centers meeting unexpected loads like request bursts, which may lead to overloaded situation and performance degradation. Dynamic Voltage Frequency Scaling and VM consolidation have been proved effective to manage overloads. However, they cannot function when the whole data center is overloaded. Brownout provides a promising direction to avoid overloads through configuring applications to temporarily degrade user experience. Additionally, brownout can also be applied to reduce data center energy consumption. As a complementary option for Dynamic Voltage Frequency Scaling and VM consolidation, our combined brownout approach reduces energy consumption through selectively and dynamically deactivating application optional components, which can also be applied to self-contained microservices. The results show that our approach can save more than 20 percent energy consumption and there are trade-offs between energy saving and discount offered to users. © 2016 IEEE.","['application component', 'brownout', 'Cloud data centers', 'energy efficient', 'microservices', 'Dynamic frequency scaling', 'Economic and social effects', 'Energy efficiency', 'Energy utilization', 'Voltage scaling', 'Application components', 'brownout', 'Cloud data centers', 'Energy efficient', 'microservices', 'Green computing']",,2016,,0,IC9,1,22
14,JOUR,10.1007/978-3-319-69035-3_14,"['Xu M.', 'Buyya R.']",Energy efficient scheduling of application components via brownout and approximate markov decision process,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,,"Unexpected loads in Cloud data centers may trigger overloaded situation and performance degradation. To guarantee system performance, cloud computing environment is required to have the ability to handle overloads. The existing approaches, like Dynamic Voltage Frequency Scaling and VM consolidation, are effective in handling partial overloads, however, they cannot function when the whole data center is overloaded. Brownout has been proved to be a promising approach to relieve the overloads through deactivating application non-mandatory components or microservices temporarily. Moreover, brownout has been applied to reduce data center energy consumption. It shows that there are trade-offs between energy saving and discount offered to users (revenue loss) when one or more services are not provided temporarily. In this paper, we propose a brownout-based approximate Markov Decision Process approach to improve the aforementioned trade-offs. The results based on real trace demonstrate that our approach saves 20% energy consumption than VM consolidation approach. Compared with existing energy-efficient brownout approach, our approach reduces the discount amount given to users while saving similar energy consumption. © Springer International Publishing AG 2017.","['Application component', 'Brownout', 'Cloud energy efficiency', 'Markov decision process', 'Microservices', 'Commerce', 'Decision making', 'Distributed computer systems', 'Dynamic frequency scaling', 'Economic and social effects', 'Energy conservation', 'Energy utilization', 'Green computing', 'Markov processes', 'Voltage scaling', 'Application components', 'Brownout', 'Cloud computing environments', 'Dynamic voltage frequency scaling', 'Energy-Efficient Scheduling', 'Markov Decision Processes', 'Microservices', 'Performance degradation', 'Energy efficiency']",,2017,,0,IC9,1,23
1,JOUR,10.1109/IC2E61754.2024.00021,"['A. Mokhtari', 'B. Jonglez', 'T. Ledoux']",Towards Digital Sustainability: Involving Cloud Users as Key Players,2024 IEEE International Conference on Cloud Engineering (IC2E),14-Nov-24,IEEE,"Due to the rapid growth of Cloud services, data centers have become major energy consumers, resulting in significant CO2 emissions. Several infrastructure-focused strategies, such as resource consolidation, have been used to reduce the carbon footprint of Cloud infrastructure. However, end-users are often left out of the picture. Since they are the primary target of Cloud applications, it would be beneficial to actively involve them in reducing the carbon footprint of Cloud applications. In this paper, we offer end-users a way to influence the carbon footprint of Cloud applications they use. To this end, we ask end-users to select a high-level mode to control the carbon footprint of a Cloud application. We then design a dynamic adaptation algorithm that determines an appropriate configuration for the application for each request, based on the end-user mode and on the carbon intensity of the infrastructure energy sources. We implement and evaluate our system on a simple image-resizing application. We run experiments on SeDuCe, a Cloud infrastructure testbed partially powered by solar panels. Our results show that we save energy consumption by up to $84 \%$ when all end-users agree to degrade the quality of the application’s output, and we provide a good quality-energy trade-off when end-users make heterogeneous choices. In addition, we are able to improve quality by leveraging the available green energy budget.","['cloud computing', 'human-centered computing', 'digital sustainability', 'carbon footprint', 'green energy', 'Green energy', 'Energy consumption', 'Data centers', 'Protocols', 'Heuristic algorithms', 'Calibration', 'Solar panels', 'Proposals', 'Carbon', 'Carbon footprint']",2694-0825,,,0,IC7,1,24
37,JOUR,10.1016/j.suscom.2025.101088,"['Laso S.', 'Rodríguez P.', 'Herrera J.L.', 'Berrocal J.', 'Murillo J.M.']",Energy consumption and workload prediction for edge nodes in the Computing Continuum,Sustainable Computing: Informatics and Systems,,,"The Computing Continuum paradigm provides developers with a distributed infrastructure for deploying applications through the network, improving performance and energy consumption. However, to maintain applications’ efficiency, their deployment in the Computing Continuum has to be continuously adapted to the varying load of different nodes of the network. In practice, existing support frameworks allow developers to automatically identify how to deploy applications based on the infrastructure status. However, as the application takes time to be deployed, the chosen deployment is outdated once it is applied through the network, as workloads change over time. To address this practical engineering challenge and plan deployments that foresee changes in energy consumption and workload, predictive solutions are needed. To fulfill this need, this work presents the Microservice Energy consumption and Workload Forecaster (MEWF), a prediction system that leverages artificial intelligence techniques to precisely predict CPU usage and energy consumption in varying circumstances. Our practical evaluation over multiple real microservices shows that MEWF improves prediction precision by up to 55% w.r.t. state-of-the-art benchmarks, enabling efficient resource management and demonstrating significant value for real-world deployments. © 2025","['Artificial Intelligence', 'Computing Continuum', 'Energy consumption', 'Microservices', 'Workload prediction', 'Change-over time', 'Computing continuum', 'Distributed infrastructure', 'Edge nodes', 'Energy consumption prediction', 'Energy-consumption', 'Improving performance', 'Microservice', 'Varying load', 'Workload predictions']",,2025,,0,IC8,1,25
3,JOUR,10.1109/ISCC61673.2024.10733735,"['C. Courageux-Sudan', 'A. -C. Orgerie', 'M. Quinson']","Studying the end-to-end performance, energy consumption and carbon footprint of fog applications",2024 IEEE Symposium on Computers and Communications (ISCC),31-Oct-24,IEEE,"The deployment of applications closer to end-users through fog computing has shown promise in improving network communication times and reducing contention. However, the use of fog applications such as microservices necessitates intricate network interactions among heterogeneous devices. Consequently, understanding the impact of different application and infrastructure parameters on performance becomes crucial. Current literature either offers end-to-end models that lack granularity and validation or fine-grained models that only consider a portion of the infrastructure. Our research first compares experimentally the accuracy of the existing integrated frameworks. We then combine one of these tools with a collection of validated models to obtain comprehensive metrics regarding microservice applications operating in the fog. Through a use-case, we demonstrate the effectiveness of our approach in investigating fog environments, from examining application latencies to greenhouse gas emissions.","['Modeling and simulation', 'Fog-computing', 'Microservice', 'Performance evaluation', 'Energy consumption', 'Performance evaluation', 'Computers', 'Energy consumption', 'Accuracy', 'Computational modeling', 'Microservice architectures', 'Greenhouse gases', 'Carbon footprint', 'Edge computing']",2642-7389,,,0,IC8,1,26
24,JOUR,10.1109/ACCESS.2023.3330649,"['Y. Huang', 'X. Zhang', 'Z. Xu']",SatEdge: Platform of Edge Cloud at Satellite and Scheduling Mechanism for Microservice Modules,IEEE Access,06-Nov-23,IEEE,"Edge cloud at satellite (ECS) is a newly developed edge computing (EC) technology that uses EC services offered by satellites to support high reliability and seamless global coverage. Satellites assume the role of computing and storage nodes for edge clouds, while terrestrial control centers function as cloud centers. In this paper, we propose a novel system and software architecture for the ECS to improve the cloud management of satellite networks and increase the flexibility of satellite service provision at the edge. Then, we propose a platform for the ECS based on KubeEdge called SatEdge. SatEdge has many function modules to meet the needs of the satellite-terrestrial network (STN) such as high reliability, high flexibility, and low latency. On this platform, we designed a microservice scheduling algorithm called optimal microservice scheduling with adaptivity and mobility (OMS-AM). OMS-AM can schedule a globally optimal workflow for microservice modules on the satellites to minimize task processing latency, failed task rate, and energy consumption. Compared with our last work, OMS-AM reduces the task processing latency by 14% at most. Additionally, OMS-AM improves the mobility of the current scheduling method put forth in our previous study, which may help lower the task failure rate. Energy usage and the total normalized costs are additional indicators of the efficiency of the microservice architecture.","['Edge cloud at satellite', 'satellite-terrestrial network', 'edge computing', 'microservice scheduling', 'Satellites', 'Microservice architectures', 'Cloud computing', 'Computer architecture', 'Containers', 'Edge computing', 'Task analysis']",2169-3536,,,0,IC8,1,27
31,JOUR,10.1145/3412841.3441888,"['Valera, Hernan Humberto Alvarez', 'Dalmau, Marc', 'Roose, Philippe', 'Larracoechea, Jorge', 'Herzog, Christina']",An energy saving approach: understanding microservices as multidimensional entities in p2p networks,Proceedings of the 36th Annual ACM Symposium on Applied Computing,2021,Association for Computing Machinery,"With the use of microservices, many software solutions have been improved in terms of scalability, response efficiency, ease of load balancing among others. However, it is still a challenge to dynamically deploy them according to devices' heterogeneity and energy consumption concerns, while maintaining a defined QoS. Centralized and decentralized approaches that manage microservices deployment have the traditional pros and cons long discussed over time. While the former offer greater control over distributed application components, the latter offers frugal network negotiations, no system-wide crashes, privacy, among others.This work focuses on identifying ""ideal"" host candidates for microservices' execution in a decentralized network, applying run-time scheduling operations (migration or duplication) to reduce energy consumption. To do this, we created a scheduling algorithm using MAAN (a P2P approach) to interpret a decentralized network as a multidimensional resource (capacity-demand) space, which supports range queries in a logarithmic quantity of hops. In this way, a node that runs a set of microservices is able to 1) map them in terms of their execution requirements (i.e. CPU frequency, RAM capacity, Network rate and disk speed) 2) Select an ideal microservice to be moved or duplicated, 3) find ideal node(s) that meet all those requirements in an optimal computational complexity and 4) negotiate the movement or duplication of the selected microservice, by analyzing energy consumption and QoS criteria.","['cloud computing', 'microservices', 'power consumption', 'energy consumption', 'distributed algorithms', 'P2P']",978-1-4503-8104-8,,['https://doi.org/10.1145/3412841.3441888'],0,IC9,1,28
51,JOUR,10.1109/ACCESS.2024.3462894,"['A. Alzahrani', 'M. Tang']",Energy-Aware Microservice-Based SaaS Deployment in a Cloud Data Center Using Hybrid Particle Swarm Optimization,IEEE Access,18-Sep-24,IEEE,"The deployment of software as a service (SaaS) using a microservice architecture offers several benefits, including scalability, flexibility, and ease of maintenance. One of the most important advantages of the new microservice-based SaaS deployment is that the increase in energy consumption incurred by the deployment of a new microservice-based SaaS can be considered. With the aim of reducing the increase in energy consumption, this paper proposes a new method, namely Hybrid Particle Swarm Optimization (HPSO), to solve the microservice-based SaaS deployment problem. The HPSO incorporates adaptive inertia weight, cognitive, and social parameters to balance the trade-off between exploration and exploitation during the optimization process. Furthermore, the HPSO incorporates a local optimizer to improve the best global solution within the swarm, with a specific emphasis on improving energy efficiency. To evaluate the performance of the HPSO, we have implemented it and compared it with a GA method by experiment. The experimental results have shown that the HPSO can further reduce the increase in energy consumption by 3.68% compared to GA.","['Cloud computing', 'data center', 'deployment', 'energy consumption', 'hybrid particle swarm optimization', 'microservice', 'optimization', 'software as a service', 'Microservice architectures', 'Data centers', 'Software as a service', 'Genetic algorithms', 'Energy consumption', 'Cloud computing', 'Servers', 'Particle swarm optimization']",2169-3536,,,0,IC9,1,29
70,JOUR,10.1007/978-3-030-71906-7_6,"['Pontes F.A.', 'Curry E.']",Cloud-Edge Microservice Architecture for DNN-based Distributed Multimedia Event Processing,Communications in Computer and Information Science,,,"The rise of Big Data, Internet of Multimedia Things (IoMT), and Deep Neural Network (DNN) enabled the growth of DNN-based Computer Vision solutions to Multimedia Event Processing (MEP) applications. When these are applied to a real-world scenario we notice the importance of having a system with a satisfactory speed that can fit in the limited resources of most IoMT devices. However, most solutions for distributed MEP are dependent on a Cloud architecture, which makes these applications migration to the Edge more challenging. As a response to this, we present a microservice architecture for DNN-based distributed MEP over heterogeneous Cloud-Edge environments. We describe our solution that allows for an easier deployment both on the Edge and on the Cloud. We show that choosing the proper tools for an Edge-Friendly solution can lead to 100 times less resource utilisation. Our preliminary investigation shows promising results, with a reduction in energy consumption by 8% with a minor drawback of 15% in throughput in the Edge and a negligible increase in energy consumption on the Cloud. © 2021, Springer Nature Switzerland AG.","['Cloud-Independent', 'Deep Neural Networks', 'Distributed computing', 'Edge-Friendly', 'Multimedia Event Processing', 'Cloud computing', 'Deep neural networks', 'Energy utilization', 'Green computing', 'Network architecture', 'Cloud architectures', 'Distributed multimedia', 'Edge-friendly', 'Event Processing', 'Real-world scenario', 'Reduction in energy consumption', 'Resource utilisation', 'Computer architecture']",,2021,,0,IC7,1,30
29,JOUR,10.1109/ICSA59870.2024.00012,"['Cortellessa V.', 'Di Pompeo D.', 'Tucci M.']",Exploring Sustainable Alternatives for the Deployment of Microservices Architectures in the Cloud,"Proceedings - IEEE 21st International Conference on Software Architecture, ICSA 2024",,,"As organizations increasingly migrate their applications to the cloud, the optimization of microservices architectures becomes imperative for achieving sustainability goals. Nonetheless, sustainable deployments may increase costs and deteriorate performance, thus the identification of optimal trade-offs among these conflicting requirements is a key objective not easy to achieve. This paper introduces a novel approach to support cloud deployment of microservices architectures by targeting optimal combinations of application performance, deployment costs, and power consumption. By leveraging genetic algorithms, specifically NSGA-II, we automate the generation of alternative architectural deployments. The results demonstrate the potential of our approach through a comprehensive assessment of the Train Ticket case study.  © 2024 IEEE.","['model-driven engineering', 'performance', 'refactoring', 'search-based software engineering', 'sustainability', 'Computer architecture', 'Economic and social effects', 'Genetic algorithms', 'Sustainable development', 'Cloud deployments', 'Key objective', 'Model-driven Engineering', 'Optimal combination', 'Optimisations', 'Performance', 'Refactorings', 'Search-based', 'Search-based software engineering', 'Trade off', 'Software engineering']",,2024,,0,IC8,1,31
33,JOUR,10.1145/3418899,"['Brondolin, Rolando', 'Santambrogio, Marco D.']",A Black-box Monitoring Approach to Measure Microservices Runtime Performance,ACM Trans. Archit. Code Optim.,2020-11,,"Microservices changed cloud computing by moving the applications’ complexity from one monolithic executable to thousands of network interactions between small components. Given the increasing deployment sizes, the architectural exploitation challenges, and the impact on data-centers’ power consumption, we need to efficiently track this complexity. Within this article, we propose a black-box monitoring approach to track microservices at scale, focusing on architectural metrics, power consumption, application performance, and network performance. The proposed approach is transparent w.r.t. the monitored applications, generates less overhead w.r.t. black-box approaches available in the state-of-the-art, and provides fine-grain accurate metrics.","['cloud computing', 'docker', 'kubernetes', 'Microservices', 'network performance monitoring', 'performance monitoring', 'power attribution']",1544-3566,,['https://doi.org/10.1145/3418899'],0,IC8,1,32
52,JOUR,10.1109/WETICE49692.2020.00026,"['H. H. A. Valera', 'M. Dalmau', 'P. Roose', 'J. Larracoechea', 'C. Herzog']",DRACeo: A smart simulator to deploy energy saving methods in microservices based networks,2020 IEEE 29th International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE),01-Feb-21,IEEE,"Nowadays, many researchers work to identify microservices-based application deployments and scheduling solutions to save energy without decreasing functional QoS. In this work, we present DRACeo: A simulator that allows facing this challenge in a simple and efficient way, enabling its users to focus uniquely on microservices deployment/scheduling algorithms and its hardware/software repercussions (load vs. energy consumption) without worrying about low-level network configurations or operating system issues. DRACeo is able to deploy and schedule (move, duplicate, start/stop) microservices and their dependencies on various devices with software and hardware heterogeneity (CPU, bandwidth, RAM, Battery, etc.), taking into account various scheduling heuristics algorithms: centralized vs non-centralized. To do this, DRACeo allows deploying custom network topologies based on client-server schemes or p2p distributions, where devices can (dis)appear, turn on/off obeying random circumstances or user strategies. Finally, the simulator performs relevant operations such as QoS definition, resource monitoring, calculation of energy saved and consumption tracking (at device and network level). We tested some ideas based on our previous work “Kaligreen” to demonstrate the effectiveness of DRACeo.","['microservices', 'middleware', 'energy', 'consumption', 'CPU', 'network', 'hard disk', 'prototype', 'simulator', 'Performance evaluation', 'Schedules', 'Software algorithms', 'Random access memory', 'Quality of service', 'Software', 'Scheduling']",2641-8169,,,0,IC8,1,33
53,JOUR,10.1109/IE54923.2022.9826775,"['H. Humberto Alvarez Valera', 'M. Dalmau', 'P. Roose', 'J. Larracoechea', 'C. Herzog']",PISCO: A smart simulator to deploy energy saving methods in microservices based networks,2022 18th International Conference on Intelligent Environments (IE),15-Jul-22,IEEE,"Nowadays, many researchers work to identify microservices-based application deployments and scheduling solutions to save energy without decreasing functional QoS. In this work, we present PISCO: A simulator that allows facing this challenge in a simple and efficient way, enabling its users to focus uniquely on microservices deployment/scheduling algorithms and its hardware/software repercussions (load vs. energy consumption) without worrying about low-level network configurations or operating system issues. PISCO is able to deploy and schedule (move, duplicate, start/stop) microservices and their dependencies on various devices with software and hardware heterogeneity (CPU, bandwidth, RAM, Battery, etc.), taking into account various scheduling heuristics algorithms: centralized vs non-centralized. To do this, PISCO allows deploying custom network topologies based on client-server schemes or p2p distributions, where devices can (dis)appear, turn on/off obeying random circumstances or user strategies.Finally, the simulator performs relevant operations such as QoS definition, resource monitoring, calculation of energy saved and consumption tracking (at device and network level). We tested some ideas based on our previous work “Kaligreen” to demonstrate the effectiveness of PISCO.","['microservices', 'middleware', 'energy', 'consumption', 'CPU', 'network', 'hard disk', 'prototype', 'simulator', 'Performance evaluation', 'Schedules', 'Network topology', 'Operating systems', 'Software algorithms', 'Microservice architectures', 'Random access memory']",2472-7571,,,0,IC8,1,34
28,JOUR,10.1145/3565010.3569065,"['Tootaghaj, Diman Zad', 'Mercian, Anu', 'Adarsh, Vivek', 'Sharifian, Mehrnaz', 'Sharma, Puneet']",SmartNICs at edge for transient compute elasticity,Proceedings of the 3rd International Workshop on Distributed Machine Learning,2022,Association for Computing Machinery,"This paper proposes a new architecture that strategically harvests the untapped compute capacity of the SmartNICs to offload transient microservices workload spikes, thereby reducing the SLA violations while providing better performance/energy consumption. This is particularly important for ML workloads at Edge deployments with stringent SLA requirements. Usage of the untapped compute capacity is more favorable than deploying extra servers, as SmartNICs are economically and operationally more desirable. We propose Spike-Offload, a low-cost and scalable platform that leverages machine learning to predict the spikes and orchestrates seamless offloading of generic microservices workloads to the SmartNICs, eliminating the need for pre-deploying expensive host servers and their under-utilization. Our SpikeOffload evaluation shows that SLA violations can be reduced by up to 20% for specific workloads. Furthermore, we demonstrate that for specific workloads our approach can potentially reduce capital expenditure (CAPEX) by more than 40%. Also, performance per unit energy consumption can be improved by upto 2X.","['edge', 'serverless computing', 'application offload', 'SmartNIC']",978-1-4503-9922-7,,['https://doi.org/10.1145/3565010.3569065'],0,IC9,1,35
71,JOUR,10.1109/TNSM.2021.3126822,"['Z. Xiang', 'M. Höweler', 'D. You', 'M. Reisslein', 'F. H. P. Fitzek']",X-MAN: A Non-Intrusive Power Manager for Energy-Adaptive Cloud-Native Network Functions,IEEE Transactions on Network and Service Management,10-Nov-21,IEEE,"Emerging microservices demand flexible low-latency processing of network functions in virtualized environments, e.g., as containerized network functions (CNFs). While ensuring highly responsive low-latency CNF processing, the computing environments should conserve energy to reduce costs. In this systems integration study, we develop and evaluate the novel XDP-Monitoring Energy-Adaptive Network Functions (X-MAN) framework for managing the CPU operational states (P-states) so as to reduce the power consumption while prioritizing low-latency service. Architecturally, X-MAN consists of lightweight traffic monitors that are attached to the virtual network interfaces in the kernel space for per-CNF traffic monitoring and a power manager in user space with a global view of the CNFs on a CPU core. Algorithmically, X-MAN monitors the CPU core utilization via hybrid simple and weighted moving average prediction fed by the traffic monitors and a power management based on step-based CPU core frequency (P-state) adjustments. We evaluate X-MAN through extensive measurements in a real physical testbed operating at up to 10 Gbps. We find that X-MAN incurs significantly shorter and more consistent monitoring latencies for the CPU utilization than a state-of-the-art CPU hardware counter approach. Also, X-MAN achieves more responsive CPU core frequency adjustments and more pronounced reductions of the CPU power consumption than a state-of-the-art code instrumentation approach. We make the X-MAN source code publicly available.","['Containerized network function (CNF)', 'CPU P-state', 'express data path (XDP)', 'load monitoring', 'microservice', 'network testbed', 'power management', 'Monitoring', 'Power system management', 'Codes', 'Central Processing Unit', 'Hardware', 'Instruments', 'Kernel']",1932-4537,,,0,IC9,1,36
97,JOUR,10.1109/INFOCOMWKSHPS61880.2024.10620732,"['J. Gómez-DelaHiz', 'A. Fakhreddine', 'J. M. Murillo', 'J. Galán-Jiménez']",Joint Optimization of Throughput and Energy Consumption in Microservices-Based UAV Networks,IEEE INFOCOM 2024 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),13-Aug-24,IEEE,"The application of Unmanned Aerial Vehicle (UAV) networks to the coverage problem in rural and low-income areas is actively studied nowadays by the research community. A UAV-based network infrastructure is different from traditional cellular networks relying on Base Stations. By mounting small cells on top of UAVs, coverage can be enhanced in regions where network operators avoid to invest due to the low Return on Investment. In case there is a requirement for an enhanced throughput in a rural area (e.g., users requesting IoT applications with strict QoS requriements), a potential solution is to place several UAV s over the same area, thus maximizing the offered throughput. However, this situation would lead to an increase in the energy consumption. To tackle this problem, this paper proposes an optimal solution to the problem of jointly maximizing the offered throughput in rural scenarios where users request microservice-based IoT applications, while minimizing the energy consumption of the swarm of UAV s. A Mixed Integer Linear Programming (MILP)-based formulation is defined and evaluated over realistic scenarios. Experimental results demonstrate how the solution is able to perform UAV placement in a way to maximize the offered throughput in the highest number of areas, while minimizing the total number of deployed UAVs.","['UAV', 'Throughput', 'Optimization', 'MILP', 'Rural areas', 'Energy consumption', 'Costs', 'Microservice architectures', 'Quality of service', 'Throughput', 'Autonomous aerial vehicles', 'Mixed integer linear programming']",2833-0587,,,0,IC7,1,37
78,JOUR,10.1016/j.comnet.2025.111158,"['Calagna A.', 'Ravera S.', 'Chiasserini C.F.']",Enabling efficient collection and usage of network performance metrics at the edge,Computer Networks,,,"Microservices (MSs)-based architectures have become the de facto standard for designing and implementing edge computing applications. In particular, by leveraging Network Performance Metrics (NPMs) coming from the Radio Access Network (RAN) and sharing context-related information, AI-driven MSs have demonstrated to be highly effective in optimizing RAN performance. In this context, this work addresses the critical challenge of ensuring efficient data sharing and consistency by proposing a holistic platform that regulates the collection and usage of NPMs. We first introduce two reference platform architectures and detail their implementation using popular, off-the-shelf database solutions. Then, to evaluate and compare such architectures and their implementation, we develop PACE, a highly configurable, scalable, MS-based emulation framework of producers and consumers of NPMs, capable of realistically reproducing a broad range of interaction patterns and load dynamics. Using PACE on our cloud computing testbed, we conduct a thorough characterization of various NPM platform architectures and implementations under a spectrum of realistic edge traffic scenarios, from loosely coupled control loops to latency- and mission- critical use cases. Our results reveal fundamental trade-offs in stability, availability, scalability, resource usage, and energy footprint, demonstrating how PACE effectively enables the identification of suitable platform solutions depending on the reference edge scenario and the required levels of reliability and data consistency. © 2025 The Authors","['Data availability', 'Data consistency', 'Edge computing', 'Microservices', 'Cloud platforms', 'Radio access networks', 'Computing applications', 'Data availability', 'Data consistency', 'De facto standard', 'Edge computing', 'Microservice', 'Network performance metrics', 'Performance', 'Platform architecture', 'Radio access networks', 'Cloud computing architecture']",,2025,,0,IC9,1,38
76,JOUR,10.1016/j.iot.2024.101463,"['García-Gil S.', 'Ramos-Ramos D.', 'Berrocal J.', 'Murillo J.M.', 'Galán-Jiménez J.']",Microservices migration: A pathway to improved energy efficiency in UAV networks,Internet of Things (The Netherlands),,,"The access to Internet and digital services play a key role in all aspects of development, from the economic to the socio-cultural dimensions, yet a substantial part of the world's population is deprived of this source of opportunities. Rural regions, characterized by having low population densities, suffer this lack of provision the most. On top of that, the remoteness and complicated orography of rural localities render traditional network infrastructure close to useless. To ensure that these localities benefit from access to digital services and to the Internet, we envision the use of swarms of Unmanned Aerial Vehicles (UAVs). Through computing enabled UAVs, the deployment of IoT applications decomposed into microservices that have an impact in the main socio-economic activities becomes a possibility. However, UAVs consume a lot of battery power, which complicates the feasibility of their use in real-world environments. To overcome this limitation, in this paper the energy optimal deployment and migration of microservices is studied, resulting in an Mix Integer Linear Programming (MILP) problem formulation. As a result, an optimal battery drain aware deployer and migrator of microservices in UAV-based networks is proposed. Our method has proved effective during the simulation, perfectly balancing the work load between UAVs, thus balancing also battery drain and maximizing fly time. © 2024 Elsevier B.V.","['Energy efficiency', 'IoT', 'Microservices migration', 'UAV']",,2025,,0,IC8,1,39
99,JOUR,10.23919/WMNC56391.2022.9954292,"['J. Galán-Jiménez', 'A. G. Vegas', 'J. Berrocal']",Energy-efficient deployment of IoT applications in remote rural areas using UAV networks,2022 14th IFIP Wireless and Mobile Networking Conference (WMNC),25-Nov-22,IEEE,"The Internet penetration rates are increasing every year, reaching more than 80% in developed countries. However, there are at least two billion people living in rural and low-income areas experiencing a complete lack of Internet connectivity, which prevents the deployment of key services such as remote health-care, emergency services, remote learning, or personal communications. In order to bring services closer to people living in rural areas, this paper exploits the capabilities of Unmanned Aerial Vehicles (UAVs) to propose an UAV-based network architecture and an energy-efficient algorithm to deploy IoT applications that can improve the quality of life of rural population. In particular, IoT applications are decomposed into microservices, which are deployed into a subset of UAVs to overcome the limitations of running the whole IoT application at a single UAV, which could result in worse outcomes due to their battery and computation limitations. Simulation results over a realistic scenario show the effectiveness of the proposed solution, evaluating the percentage of IoT requests that are served to users in the area under evaluation and reducing the energy consumption required by UAVs when handling such requests.","['Energy efficiency', 'UAV', 'IoT', 'microservices', 'digital divide', 'Wireless communication', 'Energy consumption', 'Simulation', 'Sociology', 'Microservice architectures', 'Network architecture', 'Autonomous aerial vehicles']",2473-3644,,,0,IC9,1,40
75,JOUR,10.1007/s10922-024-09825-9,"['Ramos-Ramos D.', 'González-Vegas A.', 'Berrocal J.', 'Galán-Jiménez J.']",Energy-Aware Microservice-Based Application Deployment in UAV-Based Networks for Rural Scenarios,Journal of Network and Systems Management,,,"Yearly, the rates of Internet penetration are on the rise, surpassing 80% in developed nations. Despite this progress, over two billion individuals in rural and low-income regions face a complete absence of Internet access. This lack of connectivity hinders the implementation of vital services like remote healthcare, emergency assistance, distance learning, and personal communications. To bridge this gap and bring essential services to rural populations, this paper leverages Unmanned Aerial Vehicles (UAVs). The proposal introduces a UAV-based network architecture and an energy-efficient algorithm to deploy Internet of Things (IoT) applications. These applications are broken down into microservices, strategically distributed among a subset of UAVs. This approach addresses the limitations associated with running an entire IoT application on a single UAV, which could lead to suboptimal outcomes due to battery and computational constraints. Simulation results conducted in a realistic scenario underscore the effectiveness of the proposed solution. The evaluation includes assessing the percentage of IoT requests successfully served to users in the designated area and reducing the energy consumption required by UAVs during the handling of such requests. © The Author(s) 2024.","['Digital divide', 'Energy efficiency', 'IoT', 'Microservices', 'UAV', 'Antennas', 'Distance education', 'Energy utilization', 'Internet of things', 'Network architecture', 'Power management (telecommunication)', 'Rural areas', 'Unmanned aerial vehicles (UAV)', 'Aerial vehicle', 'Application deployment', 'Digital divide', 'Distance-learning', 'Energy aware', 'Essential services', 'Internet access', 'Low incomes', 'Microservice', 'Unmanned aerial vehicle', 'Energy efficiency']",,2024,,0,IC9,1,41
61,JOUR,10.1109/ACCESS.2024.3486983,"['W. Villegas-Ch', 'R. Gutierrez', 'I. Sánchez-Salazar', 'A. Mera-Navarrete']",Adaptive Security Framework for the Internet of Things: Improving Threat Detection and Energy Optimization in Distributed Environments,IEEE Access,28-Oct-24,IEEE,"The increasing use of Internet of Things (IoT) devices in critical sectors has increased exposure to security threats, making protecting these systems a priority challenge. Based on static configurations, traditional security approaches have proven ineffective in the face of the dynamic nature of emerging threats, as they cannot adapt in real time to changes in the environment or new attack vectors. This work proposes an adaptive security framework for Internet of Things (IoT) systems capable of autonomously detecting, mitigating, and adapting to various threats, improving precision and response times, and optimizing energy consumption. The framework was implemented in a distributed Internet of Things environment, using adaptive architectures based on the Robot Operating System (ROS) and microservices orchestration with Kubernetes. The results showed a significant improvement in response time, with a reduction of 44%, reaching an average of 250 milliseconds, compared to 450 milliseconds for static approaches. Furthermore, a 92% precision in threat detection was achieved, reducing false positives to 4% and false negatives to 6%. Power consumption was controlled, reaching a maximum of 160 milliamp-hours after facing multiple threats, confirming the system’s efficiency in resource-limited environments. These results demonstrate that the proposed adaptive framework is a robust and efficient solution for security in Internet of Things environments, overcoming the limitations of traditional approaches and ensuring adequate protection without compromising energy efficiency.","['Adaptive security framework', 'Internet of Things', 'threat detection', 'energy efficiency in Internet of Things', 'Security', 'Internet of Things', 'Adaptive systems', 'Real-time systems', 'Time factors', 'Threat assessment', 'Energy consumption', 'Energy efficiency', 'Sensors', 'Logic gates']",2169-3536,,,0,IC8,1,42
64,JOUR,10.1007/978-3-031-07472-1_6,"['Vitali, Monica']",Towards Greener Applications: Enabling Sustainable-aware Cloud Native Applications Design,"Advanced Information Systems Engineering: 34th International Conference, CAiSE 2022, Leuven, Belgium, June 6–10, 2022, Proceedings",2022,Springer-Verlag,"Data centers energy demand is increasing. While a great deal of effort has been made to reduce the amount of CO2 generated by large cloud providers, too little has been done from the application perspective. We claim that application developers can impact the environmental footprint by enhancing the application design with additional features. Following the proposed Sustainable Application Design Process (SADP), the application design is enriched with information that can be leveraged by cloud providers to manage application execution in an energy-aware manner. This exploratory work aims to emphasize the awareness on the sustainability of applications by proposing a methodology for its evaluation. To this end, we first suggest possible actions to enrich the application design towards sustainability, and finally describe how this additional information can be leveraged in the application workflow. We discuss the feasibility of our methodology by referring to existing tools and technologies capable of supporting the design features proposed in a production environment.","['Cloud-native', 'Energy-efficiency', 'Sustainability-awareness', 'Sustainable applications', 'Workflow design']",978-3-031-07471-4,,['https://doi.org/10.1007/978-3-031-07472-1_6'],0,IC7,1,43
66,JOUR,10.1109/IC2E59103.2023.00011,"['M. Vitali', 'P. Schmiedmayer', 'V. Bootz']",Enriching Cloud-native Applications with Sustainability Features,2023 IEEE International Conference on Cloud Engineering (IC2E),06-Nov-23,IEEE,"Due to the ever-growing demand for computational resources, the environmental impact of data centers is continuously increasing. Recently, a great effort has been made to mitigate this impact, while the demand for computational resources has continued to grow. Current mitigation strategies focus on the infrastructure perspective, while the application perspective has been neglected. This paper aims to engage application designers and developers on the path to greener application design. Following the Sustainable Application Design Process (SADP) methodology, we introduce a Sustainable Application Design Architecture (SADA) for enriching cloud-native application components with sustainability features that can be exploited to adapt the application workflow to the environmental context. The architecture enables synergies from design to deployment between all stakeholders involved in the application management. The paper focuses on enriching the application with sustainability features in the design and development phases. We also present and discuss a prototype that can translate design-level sustainability features into development features.","['cloud-native applications', 'green IS', 'application design', 'adaptive workflow', 'microservices', 'Green products', 'Prototypes', 'Microservice architectures', 'Computer architecture', 'Quality of service', 'Energy efficiency', 'Stakeholders']",2694-0825,,,0,IC7,1,44
55,JOUR,10.1109/ACCESS.2023.3340195,"['I. Fé', 'T. A. Nguyen', 'A. B. Soares', 'S. Son', 'E. Choi', 'D. Min', 'J. -W. Lee', 'F. A. Silva']",Model-Driven Dependability and Power Consumption Quantification of Kubernetes-Based Cloud-Fog Continuum,IEEE Access,07-Dec-23,IEEE,"System dependability is pivotal for the reliable execution of designated computing functions. With the emergence of cloud-fog computing and microservices architectures, new challenges and opportunities arise in evaluating system dependability. Enhancing dependability in microservices often involves component replication, potentially increasing energy costs. Thus, discerning optimal redundancy strategies and understanding their energy implications is crucial for both cost efficiency and ecological sustainability. This paper presents a model-driven approach to evaluate the dependability and energy consumption of cloud-fog systems, utilizing Kubernetes, a container application orchestration platform. The developed model considers various determinants affecting system dependability, including hardware and software reliability, resource accessibility, and support personnel availability. Empirical studies validate the model’s effectiveness, demonstrating a 22.33% increase in system availability with only a 1.33% rise in energy consumption. Moreover, this methodology provides a structured framework for understanding cloud-fog system dependability, serves as a reference for comparing dependability across different systems, and aids in resource allocation optimization. This research significantly contributes to the efforts to enhance cloud-fog system dependability.","['Cloud-fog continuum', 'dependability', 'Kubernetes', 'stochastic modeling', 'Modeling', 'Stochastic processes', 'Power demand', 'Energy consumption', 'Edge computing', 'Cloud computing']",2169-3536,,,0,IC8,1,45
68,JOUR,10.1109/MASCOTS.2018.00030,"['J. von Kistowski', 'S. Eismann', 'N. Schmitt', 'A. Bauer', 'J. Grohmann', 'S. Kounev']","TeaStore: A Micro-Service Reference Application for Benchmarking, Modeling and Resource Management Research","2018 IEEE 26th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",08-Nov-18,IEEE,"Modern distributed applications offer complex performance behavior and many degrees of freedom regarding deployment and configuration. Researchers employ various methods of analysis, modeling, and management that leverage these degrees of freedom to predict or improve non-functional properties of the software under consideration. In order to demonstrate and evaluate their applicability in the real world, methods resulting from such research areas require test and reference applications that offer a range of different behaviors, as well as the necessary degrees of freedom. Existing production software is often inaccessible for researchers or closed off to instrumentation. Existing testing and benchmarking frameworks, on the other hand, are either designed for specific testing scenarios, or they do not offer the necessary degrees of freedom. Further, most test applications are difficult to deploy and run, or are outdated. In this paper, we introduce the TeaStore, a state-of-the-art micro-service-based test and reference application. TeaStore offers services with different performance characteristics and many degrees of freedom regarding deployment and configuration to be used as a benchmarking framework for researchers. The TeaStore allows evaluating performance modeling and resource management techniques; it also offers instrumented variants to enable extensive run-time analysis. We demonstrate TeaStore's use in three contexts: performance modeling, cloud resource management, and energy efficiency analysis. Our experiments show that TeaStore can be used for evaluating novel approaches in these contexts and also motivates further research in the areas of performance modeling and resource management.","['Microservice', 'Benchmarking', 'Performance', 'Power', 'Energy Efficiency', 'Models', 'Auto Scaler', 'Container', 'Cloud', 'Benchmark testing', 'Software', 'Analytical models', 'Resource management', 'Predictive models', 'Instruments']",2375-0227,,,0,IC8,1,46
47,JOUR,10.1109/ISCA59077.2024.00040,"['J. Stojkovic', 'P. A. Misra', 'Í. Goiri', 'S. Whitlock', 'E. Choukse', 'M. Das', 'C. Bansal', 'J. Lee', 'Z. Sun', 'H. Qiu', 'R. Zimmermann', 'S. Samal', 'B. Warrier', 'A. Raniwala', 'R. Bianchini']",SmartOClock: Workload- and Risk-Aware Overclocking in the Cloud,2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA),01-Aug-24,IEEE,"Operating server components beyond their voltage and power design limit (i.e., overclocking) enables improving performance and lowering cost for cloud workloads. However, overclocking can significantly degrade component lifetime, increase power draw, and cause power capping events, eventually diminishing the performance benefits. In this paper, we characterize the impact of overclocking on cloud workloads by studying their profiles from production deployments. Based on the characterization insights, we propose SmartOClock, the first distributed overclocking management platform specifically designed for cloud environments. SmartOClock is a workload-aware scheme that relies on power predictions to heterogeneously distribute the power budgets across its servers based on their needs and then enforce budget compliance locally, per-server, in a decentralized manner. SmartOClock reduces the tail latency by 9%, application cost by 30% and total energy consumption by 10% for latencysensitive microservices on a 36-server deployment. Simulation analysis using production traces show that SmartOClock reduces the number of power capping events by up to 95% while increasing the overclocking success rate by up to 62%. We also describe lessons from building a first-of-its-kind overclockable cluster in Microsoft Azure for production experiments.","['Energy consumption', 'Costs', 'Buildings', 'Microservice architectures', 'Production', 'Tail', 'Computer architecture']",,,,0,IC8,1,47
0,JOUR,10.1109/ICSCC62041.2024.10690844,"['G. H. Prathama', 'P. E. G. Gunawan', 'A. W. O. Gama']",Green Computing Implementation for Indonesian Higher Education Adopting Serverless Microservices Architecture,2024 10th International Conference on Smart Computing and Communication (ICSCC),01-Oct-24,IEEE,"Sustainability poses a significant challenge within the realm of global information technology, especially within the resource-intensive settings of higher education. The present research delves into the application of serverless microservices architecture in universities in Indonesia as a prospective solution for eco-friendly computing. Through the utilization of cutting-edge serverless technologies, a prototype representing the IT framework of an Indonesian institution of higher education was formulated and assessed. The outcomes of this investigation reveal that the adoption of serverless microservices not only improves operational effectiveness but also brings about a notable reduction in energy consumption and carbon footprints. These findings offer valuable perspectives for academic leaders and IT experts who aspire to shift towards more sustainable IT infrastructures.","['Cost Efficiency', 'cloud computing', 'private universities', 'serverless computation', 'microservices architecture', 'green computing', 'Energy consumption', 'Scalability', 'Education', 'Microservice architectures', 'Prototypes', 'Computer architecture', 'Computational efficiency', 'Resource management', 'Sustainable development', 'Information technology']",,,,0,IC9,1,48
54,JOUR,10.1145/3307650.3322227,"['Sriraman, Akshitha', 'Dhanotia, Abhishek', 'Wenisch, Thomas F.']",SoftSKU: optimizing server architectures for microservice diversity @scale,Proceedings of the 46th International Symposium on Computer Architecture,2019,Association for Computing Machinery,"The variety and complexity of microservices in warehouse-scale data centers has grown precipitously over the last few years to support a growing user base and an evolving product portfolio. Despite accelerating microservice diversity, there is a strong requirement to limit diversity in underlying server hardware to maintain hardware resource fungibility, preserve procurement economies of scale, and curb qualification/test overheads. As such, there is an urgent need for strategies that enable limited server CPU architectures (a.k.a ""SKUs"") to provide performance and energy efficiency over diverse microservices. To this end, we first undertake a comprehensive characterization of the top seven microservices that run on the compute-optimized data center fleet at Facebook.Our characterization reveals profound diversity in OS and I/O interaction, cache misses, memory bandwidth utilization, instruction mix, and CPU stall behavior. Whereas customizing a CPU SKU for each microservice might be beneficial, it is prohibitive. Instead, we argue for ""soft SKUs"", wherein we exploit coarse-grain (e.g., boot time) configuration knobs to tune the platform for a particular microservice. We develop a tool, μSKU, that automates search over a soft-SKU design space using A/B testing in production and demonstrate how it can obtain statistically significant gains (up to 7.2% and 4.5% performance improvement over stock and production servers, respectively) with no additional hardware requirements.","['microservice', 'resource fungibility', 'soft SKU']",978-1-4503-6669-4,,['https://doi.org/10.1145/3307650.3322227'],0,IC7,1,49
35,JOUR,10.1007/978-3-031-48421-6_13,"['Dinga, Madalina', 'Malavolta, Ivano', 'Giamattei, Luca', 'Guerriero, Antonio', 'Pietrantuono, Roberto']",An Empirical Evaluation of&nbsp;the&nbsp;Energy and&nbsp;Performance Overhead of&nbsp;Monitoring Tools on&nbsp;Docker-Based Systems,"Service-Oriented Computing: 21st International Conference, ICSOC 2023, Rome, Italy, November 28 – December 1, 2023, Proceedings, Part I",2023,Springer-Verlag,"Context. Energy efficiency is gaining importance in the design of software systems, but is still marginally addressed in the area of microservice-based systems. Energy-related aspects often get neglected in favor of other software quality attributes, such as performance, service composition, maintainability, and security.Goal. The aim of this study is to identify, synthesize and empirically evaluate the energy and performance overhead of monitoring tools employed in the microservices and DevOps context.Method. We selected four representative monitoring tools in the microservices and DevOps context. These were evaluated via a controlled experiment on an open-source Docker-based microservice benchmark system.Results. The results highlight: i) the specific frequency and workload conditions under which energy consumption and performance metrics are impacted by the tools; ii) the differences between the tools; iii) the relation between energy and performance overhead.",,978-3-031-48420-9,,['https://doi.org/10.1007/978-3-031-48421-6_13'],0,IC8,1,50
48,JOUR,10.1109/ICWS62655.2024.00112,"['V. Berry', 'A. Castelltort', 'B. Lange', 'J. Teriihoania', 'C. Tibermacine', 'C. Trubiani']","Is it Worth Migrating a Monolith to Microservices? An Experience Report on Performance, Availability and Energy Usage",2024 IEEE International Conference on Web Services (ICWS),15-Oct-24,IEEE,"The microservice architecture (MSA) emerged as an evolution of existing architectural styles with the promise of improving software quality by decomposing an app into modules that can be maintained, deployed, and scaled independently. However, the transition from a monolithic to a microservice architecture is fraught with difficulties, especially when it comes to assessing qualitative aspects, as controversial results can arise. In this paper, we present an experience report on the migration of a monolithic web application and use performance, availability and energy efficiency as quality attributes to shed light on such an architectural transition. Horizontal scaling, i.e., distributing the workload across several service instances, is applied and we study its impact.Our main findings are: i) when no app component is replicated, MSA outperforms the monolithic architecture; ii) the monolithic architecture shows performance and availability improvement when replicating the entire app; iii) the replicated MSA version reaches a ceiling when not replicating its routing part (i.e., the API gateway), showing worse response times compared to the replicated monolith; iv) when replicating the API gateway, the MSA version reaches optimal performance with fewer replicates than the monolith; v) when not replicating services, MSA consumes more CPU resources than the monolithic architecture; vi) when scaling up, the MSA version is more efficient than the replicated monolith in terms of memory usage, and it can better exploit CPU resources; vii) when not replicating services, MSA consumes more energy than the monolithic architecture, whereas when scaling up, the MSA version is more efficient than the replicated monolith; MSA version reaches a good balance between CPU and memory usage.","['Microservices', 'architecture migration', 'horizontal scaling', 'load tests', 'performance', 'availability', 'energy usage', 'Energy consumption', 'Web services', 'Memory management', 'Microservice architectures', 'Software quality', 'Logic gates', 'Routing', 'Energy efficiency', 'Time factors', 'Testing']",2836-3868,,,0,IC8,1,51
109,JOUR,10.3390/fi16080296,"['Miao J.', 'Rajasekhar D.', 'Mishra S.', 'Nayak S.K.', 'Yadav R.']",A Microservice-Based Smart Agriculture System to Detect Animal Intrusion at the Edge,Future Internet,,,"Smart agriculture stands as a promising domain for IoT-enabled technologies, with the potential to elevate crop quality, quantity, and operational efficiency. However, implementing a smart agriculture system encounters challenges such as the high latency and bandwidth consumption linked to cloud computing, Internet disconnections in rural locales, and the imperative of cost efficiency for farmers. Addressing these hurdles, this paper advocates a fog-based smart agriculture infrastructure integrating edge computing and LoRa communication. We tackle farmers’ prime concern of animal intrusion by presenting a solution leveraging low-cost PIR sensors, cameras, and computer vision to detect intrusions and predict animal locations using an innovative algorithm. Our system detects intrusions pre-emptively, identifies intruders, forecasts their movements, and promptly alerts farmers. Additionally, we compare our proposed strategy with other approaches and measure their power consumptions, demonstrating significant energy savings afforded by our strategy. Experimental results highlight the effectiveness, energy efficiency, and cost-effectiveness of our system compared to state-of-the-art systems. © 2024 by the authors.","['animal intrusion detection', 'fog computing', 'LoRa', 'smart agriculture', 'Fog computing', 'Agriculture systems', 'Animal intrusion detection', 'Bandwidth consumption', 'Cloud-computing', 'Cost-efficiency', 'Crop quality', 'Intrusion-Detection', 'Lora', 'Operational efficiencies', 'Smart agricultures', 'Smart agriculture']",,2024,,0,IC8,1,52
91,JOUR,10.1109/ICWS62655.2024.00079,"['M. S. Floroiu', 'S. Russo', 'L. Giamattei', 'A. Guerriero', 'I. Malavolta', 'R. Pietrantuono']",Anomaly Detection and Root Cause Analysis of Microservices Energy Consumption,2024 IEEE International Conference on Web Services (ICWS),15-Oct-24,IEEE,"With the expansion of cloud computing and data centers, the need has arisen to tackle their environmental impact. The increasing adoption of microservice architectures, while offering scalability and flexibility, poses new challenges in the effective management of systems’ energy consumption.This study analyzes experimentally the effectiveness, with respect to energy consumption, of algorithms for Anomaly Detection (AD) and Root Cause Analysis (RCA) for (containerized) microservices systems. The study analyzes five AD and three RCA algorithms. Metrics to assess the effectiveness of AD algorithms are Precision, Recall, and F-Score. For RCA algorithms, the chose metric is Precision at level k. Two subjects of different complexity are used: Sock Shop and UNI-Cloud. Experiments use a cross-over paired comparison design, involving multiple randomized runs for robust measures.The experiments show that AD algorithms exhibit a relatively moderate performance. The mean adjusted Precision for Sock Shop is 61.5%, while it is 75% for the best-performing algorithms (BIRCH, KNN, and SVM) on UNI-Cloud. The Recall and F-Score for UNI-Cloud, for the same algorithms, are 75%, while for Sock Shop KNN yields the best outcome at roughly 45%. MicroRCA and RCD emerge as the top-performing algorithms for RCA.We found that the effectiveness of AD algorithms is strongly influenced by anomaly thresholds, emphasizing the importance of careful tuning such algorithms. RCA algorithms reveal promising results, particularly RCD and MicroRCA, which showed robust performance. However, challenges remain, as seen with the ϵ-diagnosis algorithm, suggesting the need for further refinement.For DevOps engineers, the findings highlight the need to carefully select and tune AD and RCA algorithms for energy, and to take into account system topology and monitoring configurations.","['Microservices', 'Energy consumption', 'Anomaly Detection', 'Root Cause Analysis', 'Measurement', 'Energy consumption', 'Root cause analysis', 'Web services', 'Software algorithms', 'Microservice architectures', 'Nearest neighbor methods', 'Topology', 'Anomaly detection', 'Tuning']",2836-3868,,,0,IC8,1,53
110,JOUR,10.14569/IJACSA.2021.0120768,"['Thanh L.N.T.', 'Phien N.N.', 'Nguyen T.A.', 'Vo H.K.', 'Luong H.H.', 'Anh T.D.', 'Tuan K.N.H.', 'Son H.X.']",IoHT-MBA: An Internet of Healthcare Things (IoHT) Platform based on Microservice and Brokerless Architecture,International Journal of Advanced Computer Science and Applications,,,"Internet of Thing (IoT), currently, is one of the technology trends that are most interested. IoT can be divided into five main areas including: Health-care, Environmental, Smart city, Commercial and Industrial. The IoHT-MBA Platform is considered the backbone of every IoT architecture, so the optimal design of the IoHT-MBA Platform is essential issue, which should be carefully considered in the different aspects. Although, IoT is applied in multiple domains, however, there are still three main features that are challenge to improve: i) data collection, ii) users, devices management, and iii) remote device control. Today’s medical IoT systems, often too focused on the big data or access control aspects of participants, but not focused on collecting data accurately, quickly, and efficiently; power redundancy and system expansion. This is very important for the medical sector - which always prioritizes the availability of data for therapeutic purposes over other aspects. In this paper, we introduce the IoHT Platform for Healthcare environment which is designed by microservice and brokerless architecture, focusing strongly on the three aforementioned characteristics. In addition, our IoHT Platform considers the five other issues including (1) the limited processing capacity of the devices, (2) energy saving for the device, (3) speed and accurate of the data collection, (4) security mechanisms and (5) scalability of the system. Also, in order for the IoHT Platform to be suitable for the field of health monitoring, we also add realtime alerts for the medical team. In the evaluation section, moreover, we describe the evaluation to prove the effectiveness of the proposed IoHT Platform (i.e. the proof-of-concept) in the performance, non-error, and non affected by geographical distance. Finally, a complete code solution is publicized on the authors’ GitHub repository to engage further reproducibility and improvement. © 2021. All Rights Reserved.","['brokerless', 'gRPC', 'Internet of Health Things (IoHT)', 'kafka', 'microservice', 'RBAC', 'single sign-on', 'Access control', 'Data acquisition', 'Energy conservation', 'Health care', 'Information management', 'Broker-less', 'Data collection', 'GRPC', 'Internet of health thing', 'Kafkum', 'Microservice', 'RBAC', 'Single sign on', 'Single signs-on', 'Technology trends', 'Internet of things']",,2021,,0,IC8,1,54
112,JOUR,10.1109/BigData62323.2024.10825075,"['P. Bellini', 'E. Collini', 'M. Fanfani', 'L. A. I. Palesi', 'P. Nesi']",Smart City Digital Twin Platform Architecture for Mobility and Transport Decision Support Systems,2024 IEEE International Conference on Big Data (BigData),16-Jan-25,IEEE,"Addressing mobility and transport problems is nowadays of paramount importance for any city due to the increasing urbanization. Traffic congestion, pollutant emissions, energy consumption are some of the problems related to urban mobility. Therefore, there is the need of tools able to support decision-makers in studying, evaluating, and planning sustainable urban evolutions. A few open-source and proprietary solutions are available requiring on-premises installations, large effort, and providing limited capabilities to actually handle real-time data (from data spaces, and standards). Moreover, they are limited in terms of analytic integration and do not offer automatic generation of suggestions. In practice they do not manage the explosion of complexity regarding computational and storage/models aspects. For these reasons, this paper presents a comprehensive architecture for a Smart City Digital Twin platform, specifically designed to support mobility and transportation decision-making through advanced what-if analysis and optimization. The platform, integrated within the Snap4City system, enables real-time data processing and complex analytics to create virtual urban environments for evaluating potential infrastructure changes. Through microservice architecture, the platform supports massive data ingestion, scenario creation, and predictive modelling, facilitating both short-term and long-term planning. The solution leverages artificial intelligence (AI), machine learning (ML), and reinforcement learning (RL) to optimize city operations and suggest actionable insights, aiding city planners in strategic and tactical decisions. This architecture has been validated through implementations in Italian cities, demonstrating scalability and flexibility to accommodate diverse urban needs and improve traffic flow, energy efficiency, and environmental impact. This work has been performed in the context of OPTIFaaS Flagship of CN MOST, the National Centre for Sustainable Mobility in Italy, and for CN HPC Big Data and Quantum Computing, ICSC.","['Digital Twin', 'Urban Scenario', 'Optimization', 'Traffic', 'Decision Support System', 'Smart cities', 'Urban planning', 'Transportation', 'Computer architecture', 'Big Data', 'Real-time systems', 'Digital twins', 'Planning', 'Optimization', 'Traffic congestion']",2573-2978,,,1,,0,55
23,JOUR,10.1109/JIOT.2020.2981958,"['A. Samanta', 'J. Tang']",Dyme: Dynamic Microservice Scheduling in Edge Computing Enabled IoT,IEEE Internet of Things Journal,19-Mar-20,IEEE,"In recent years, the rapid development of mobile edge computing (MEC) provides an efficient execution platform at the edge for Internet-of-Things (IoT) applications. Nevertheless, the MEC also provides optimal resources to different microservices, however, underlying network conditions and infrastructures inherently affect the execution process in MEC. Therefore, in the presence of varying network conditions, it is necessary to optimally execute the available task of end users while maximizing the energy efficiency in edge platform and we also need to provide fair Quality-of-Service (QoS). On the other hand, it is necessary to schedule the microservices dynamically to minimize the total network delay and network price. Thus, in this article, unlike most of the existing works, we propose a dynamic microservice scheduling scheme for MEC. We design the microservice scheduling framework mathematically and also discuss the computational complexity of the scheduling algorithm. Extensive simulation results show that the microservice scheduling framework significantly improves the performance metrics in terms of total network delay, average price, satisfaction level, energy consumption rate (ECR), failure rate, and network throughput over other existing baselines.","['Dynamic microservice scheduling', 'edge computing', 'Internet of Things (IoT)', 'microservice', 'Quality-of-Service (QoS)', 'Dynamic scheduling', 'Task analysis', 'Internet of Things', 'Delays', 'Quality of service', 'Processor scheduling', 'Edge computing']",2327-4662,,,0,EC5,0,56
50,JOUR,10.1016/j.jnca.2024.104037,"['Rajagopal S.M.', 'M. S.', 'Buyya R.']",Leveraging blockchain and federated learning in Edge-Fog-Cloud computing environments for intelligent decision-making with ECG data in IoT,Journal of Network and Computer Applications,,,"Blockchain technology combined with Federated Learning (FL) offers a promising solution for enhancing privacy, security, and efficiency in medical IoT applications across edge, fog, and cloud computing environments. This approach enables multiple medical IoT devices at the network edge to collaboratively train a global machine learning model without sharing raw data, addressing privacy concerns associated with centralized data storage. This paper presents a blockchain and FL-based Smart Decision Making framework for ECG data in microservice-based IoT medical applications. Leveraging edge/fog computing for real-time critical applications, the framework implements a FL model across edge, fog, and cloud layers. Evaluation criteria including energy consumption, latency, execution time, cost, and network usage show that edge-based deployment outperforms fog and cloud, with significant advantages in energy consumption (0.1% vs. Fog, 0.9% vs. Cloud), network usage (1.1% vs. Fog, 31% vs. Cloud), cost (3% vs. Fog, 20% vs. Cloud), execution time (16% vs. Fog, 28% vs. Cloud), and latency (1% vs. Fog, 79% vs. Cloud). © 2024","['Blockchain', 'Edge computing', 'Federated learning', 'Fog computing', 'Internet of Things', 'Fog computing', 'Block-chain', 'Cloud computing environments', 'ECG data', 'Edge computing', 'Energy-consumption', 'Intelligent decision-making', 'Machine learning models', 'Network edges', 'Network usage', 'Privacy concerns', 'Federated learning']",,2025,,0,EC5,0,57
59,JOUR,10.1109/WCNCW.2019.8902860,"['G. Rigazzi', 'J. -P. Kainulainen', 'C. Turyagyenda', 'A. Mourad', 'J. Ahn']",An Edge and Fog Computing Platform for Effective Deployment of 360 Video Applications,2019 IEEE Wireless Communications and Networking Conference Workshop (WCNCW),18-Nov-19,IEEE,"Immersive video applications based on 360 video streaming require high-bandwidth, high-reliability and low-latency 5G connectivity but also flexible, low-latency and cost-effective computing deployment. This paper proposes a novel solution for decomposing and distributing the end-to-end 360 video streaming service across three computing tiers, namely cloud, edge and constrained fog, in order of proximity to the end user client. The streaming service is aided with an adaptive viewport technique. The proposed solution is based on the H2020 5G-CORAL system architecture using micro-services-based design and a unified orchestration and control across all three tiers based on Fog05. Performance evaluation of the proposed solution shows noticeable reduction in bandwidth consumption, energy consumption, and deployment costs, as compared to a solution where the streaming service is all delivered out of one computing location such as the Cloud.","['eMBB', '360 video', 'fog computing', 'edge computing', 'Streaming media', 'Cloud computing', 'Edge computing', 'Servers', '5G mobile communication', 'Bandwidth', 'Data centers']",,,,0,EC5,0,58
36,JOUR,10.1007/s00607-019-00733-4,"['Lakhan A.', 'Li X.']",Transient fault aware application partitioning computational offloading algorithm in microservices based mobile cloudlet networks,Computing,,,"Mobile Cloudlet Computing paradigm (MCC) allows execution of resource-intensive mobile applications using computation cloud resources by exploiting computational offloading method for resource-constrained mobile devices. Whereas, computational offloading needs the mobile application to be partitioned during the execution in the MCC so that total execution cost is minimized. In the MCC, at the run-time network contexts (i.e., network bandwidth, signal strength, latency, etc.) are intermittently changed, and transient failures (due to temporary network connection failure, services busy, database disk out of storage) often occur for a short period of time. Therefore, transient failure aware partitioning of the mobile application at run-time is a challenging task. Since, existing MCC offers computational monolithic services by exploiting heavyweight virtual machines, which incurs with long VM startup time and high overhead, and these cannot meet the requirements of fine-grained microservices applications (e.g., E-healthcare, E-business, 3D-Game, and Augmented Reality). To cope up with prior issues, we propose microservices based mobile cloud platform by exploiting containerization which replaces heavyweight virtual machines, and we propose the application partitioning task assignment (APTA) algorithm which determines application partitioning at run-time and adopts the fault aware (FA) policy to execute microservices applications robustly without interruption in the MCC. Simulation results validate that the proposed microservices mobile cloud platform not only shrinks the setup time of run-time platform but also reduce the energy consumption of nodes and improve the application response time by exploiting APTA and FA to the existing VM based MCC and application partitioning strategies. © 2019, Springer-Verlag GmbH Austria, part of Springer Nature.","['Application partitioning', 'Application programming interface (API)', 'APTA', 'FA', 'Microservices', 'Min-cut', 'Mobile cloudlet computing', 'Offloadi ng', 'Representational state transfer (REST)', 'Application programming interfaces (API)', 'Augmented reality', 'Energy utilization', 'Green computing', 'Interface states', 'mHealth', 'Mobile computing', 'Network security', 'Simulation platform', 'Virtual machine', 'Application partitioning', 'APTA', 'Microservices', 'Min-cut', 'Mobile cloudlet computing', 'Offloadi ng', 'Representational state transfer', 'Mobile cloud computing']",,2020,,0,EC5,0,59
82,JOUR,,"['Attarha, Shadi', 'Forster, Anna']","Empowering IoT Applications with Flexible, Energy-Efficient Remote Management of Low-Power Edge Devices",Proceedings of the 2023 International Conference on Embedded Wireless Systems and Networks,2023,Association for Computing Machinery,"In the context of the Internet of Things (IoT), reliable and energy-efficient provision of IoT applications has become critical. Equipping IoT systems with tools that enable a flexible, well-performing, and automated way of monitoring and managing IoT edge devices is an essential prerequisite. In current IoT systems, low-power edge appliances have been utilized in a way that can not be controlled and re-configured in a timely manner. Hence, conducting a trade-off solution between manageability, performance and design requirements are demanded. This paper introduces a novel approach for fine-grained monitoring and managing individual micro-services within low-power edge devices, which improves system reliability and energy efficiency. The proposed method enables operational flexibility for IoT edge devices by leveraging a modularization technique. Following a review of existing solutions for remote-managed IoT services, a detailed description of the suggested approach is presented. Also, to explore the essential design principles that must be considered in this approach, the suggested architecture is elaborated in detail. Finally, the advantages of the proposed solution to deal with disruptions are demonstrated in the proof of concept-based experiments.","['IoT', 'Energy Efficiency', 'Low-power', 'Operational Flexibility', 'Service Isolation', 'Service Management']",,,,0,EC5,0,60
87,JOUR,10.1007/978-3-030-59824-2_2,"['Kaneko, Yu', 'Yokoyama, Yuhei', 'Monma, Nobuyuki', 'Terashima, Yoshiki', 'Teramoto, Keiichi', 'Kishimoto, Takuya', 'Saito, Takeshi']",A Microservice-Based Industrial Control System Architecture Using Cloud and&nbsp;MEC,"Edge Computing – EDGE 2020: 4th International Conference, Held as Part of the Services Conference Federation, SCF 2020, Honolulu, HI, USA, September 18-20, 2020, Proceedings",2020,Springer-Verlag,"Cloud computing has been adapted for various application areas. Several research projects are underway to migrate Industrial Control Systems (ICSs) to the public cloud. Some functions of ICSs require real-time processing that is difficult to migrate to the public cloud because network latency of the internet is unpredictable. Fog computing is a new computing paradigm that could address this latency issue. In particular, Multi-access Edge Computing (MEC) is a fog computing environment integrated with the 5G network, and therefore the real-time processing requirement of ICSs could be satisfied by using MEC. In this paper, we propose a microservice-based ICS architecture using the cloud and fog computing. In the architecture, each function of an ICS is implemented as a microservice and its execution locations are determined by an algorithm minimizing the total usage fee for cloud and fog computing while satisfying the real-time processing requirement. The proposed architecture and placement algorithm are evaluated by simulation under the scenario of a virtual power plant that manages distributed energy resources. The simulation result shows the proposed placement algorithm suppresses VM usage fee while satisfying the requirement of a real-time control function.","['5G', 'Cloud', 'Fog', 'Industrial Control System', 'MEC']",978-3-030-59823-5,,['https://doi.org/10.1007/978-3-030-59824-2_2'],0,EC5,0,61
27,JOUR,10.1109/ACCESS.2022.3170918,"['A. Ali', 'M. M. Iqbal']",A Cost and Energy Efficient Task Scheduling Technique to Offload Microservices Based Applications in Mobile Cloud Computing,IEEE Access,28-Apr-22,IEEE,"The number of smartphone users and mobile devices has increased significantly. The Mobile Cloud Applications based on cloud computing have also been increased. The mobile apps can be used in Augmented Reality, E-Transportation, 2D/3-D Games, E-Healthcare, and Education. The modern cloud-based frameworks provide such services on Virtual Machines. The existing frameworks worked well, but these suffered the problems such as overhead, resource utilization, lengthy boot-time, and cost of running Mobile Applications. This study addresses these problems by proposing a Dynamic Decision-Based Task Scheduling Technique for Microservice-based Mobile Cloud Computing Applications (MSCMCC). The MSCMCC runs delay-sensitive applications and mobility with less cost than existing approaches. The study focused on Task Scheduling problems on heterogeneous Mobile Cloud servers. We further propose Task Scheduling and Microservices based Computational Offloading (TSMCO) framework to solve the Task Scheduling in steps, such as Resource Matching, Task Sequencing, and Task Scheduling. Furthermore, the experimental results elaborate that the proposed MSCMCC and TSMCO enhance the Mobile Server Utilization. The proposed system effectively minimizes the cost of healthcare applications by 25%, augmented reality by 23%, E-Transport tasks by 21%, and 3-D games tasks by 19%, the average boot-time of microservices applications by 17%, resource utilization by 36%, and tasks arrival time by 16%.","['Cloud computing', 'mobile cloud computing', 'task offloading', 'task sequencing', 'task scheduling', 'microservices', 'Task analysis', 'Cloud computing', 'Costs', 'Processor scheduling', 'Microservice architectures', 'Mobile handsets', 'Servers']",2169-3536,,,0,EC5,0,62
43,JOUR,10.1109/NetSoft57336.2023.10175438,"['M. Mekki', 'B. Brik', 'A. Ksentini', 'C. Verikoukis']",XAI-Enabled Fine Granular Vertical Resources Autoscaler,2023 IEEE 9th International Conference on Network Softwarization (NetSoft),13-Jul-23,IEEE,"Fine-granular management of cloud-native computing resources is one of the key features sought by cloud and edge operators. It consists in giving the exact amount of computing resources needed by a microservice to avoid resource over-provisioning, which is, by default, the adopted solution to prevent service degradation. Fine-granular resource management guarantees better computing resource usage, which is critical to reducing energy consumption and resource wastage (vital in edge computing). In this paper, we propose a novel Zero-touch management (ZSM) framework featuring a fine-granular computing resource scaler in a cloud-native environment. The proposed scaler algorithm uses Artificial Intelligence (AI)/Machine Learning (ML) models to predict microservice performances; if a service degradation is detected, then a root-cause analysis is conducted using eXplainable AI (XAI). Based on the XAI output, the proposed framework scales only the needed (exact amount) resources (i.e., CPU or memory) to overcome the service degradation. The proposed framework and resource scheduler have been implemented on top of a cloud-native platform based on the well-known Kubernetes tool. The obtained results clearly indicate that the proposed scheduler with lesser resources achieves the same service quality as the default scheduler of Kubernetes.","['Zero-touch Service Management (ZSM)', 'Cloud-native', 'Containerized Microservices', 'Machine Learning', 'Explainable Artificial Intelligence', 'Degradation', 'Energy consumption', 'Memory management', 'Microservice architectures', 'Learning (artificial intelligence)', 'Predictive models', 'Prediction algorithms']",2693-9789,,,0,EC5,0,63
2,JOUR,10.1109/IC2E59103.2023.00015,"['R. Cordingly', 'J. Kaur', 'D. Dwivedi', 'W. Lloyd']","Towards Serverless Sky Computing: An Investigation on Global Workload Distribution to Mitigate Carbon Intensity, Network Latency, and Cost",2023 IEEE International Conference on Cloud Engineering (IC2E),06-Nov-23,IEEE,"The high demand for energy consumption and the resulting carbon footprint of the cloud pose significant sustainability challenges, as cloud data centers consume vast amounts of energy. The emergence of serverless cloud computing platforms has opened up new avenues for more sustainable cloud computing. Serverless Function-as-a-Service (FaaS) cloud computing platforms facilitate deploying applications as decoupled microservices to leverage automatic rapid scaling, high availability, fault tolerance, and on-demand pricing. The absence of always-on hosting costs associated with virtual machines enables serverless functions to be deployed with many different function configurations and cloud regions to achieve high performance, low network latency, and reduced costs. In this paper, we investigate the utility of a global sky computing platform where serverless resources are aggregated between up to 19 distinct cloud regions. We prototype a serverless load distribution system to distribute client requests across serverless aggregations to minimize performance objectives, including network latency, runtime, hosting costs, and carbon footprint. To evaluate our serverless distribution system's ability to meet performance objectives, we continuously executed large experiments across 19 regions around the world from November 2022 through March 2023. Our serverless load distribution approach using aggregated resources reduced the carbon intensity of a globally distributed serverless application by up to 99.8%, network latency by 65%, or hosting costs by 58% by optimizing function routing to deployments with optimal hardware configurations.","['Sky Computing', 'Serverless Computing', 'Function-as-a-Service', 'Green Computing', 'Cloud computing', 'Costs', 'Runtime', 'Prototypes', 'Pricing', 'Routing', 'Virtual machining']",2694-0825,,,0,EC5,0,64
62,JOUR,10.1109/HPCA.2019.00032,"['C. -H. Chou', 'L. N. Bhuyan', 'D. Wong']",μDPM: Dynamic Power Management for the Microsecond Era,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),28-Mar-19,IEEE,"The complex, distributed nature of data centers have spawned the adoption of distributed, multi-tiered software architectures, consisting of many inter-connected microservices. These microservices exhibit extremely short request service times, often less than 250μs. We show that these “killer microsecond” service times can cause state-of-the-art dynamic power management techniques to break down, due to short idle period length and low power state transition overheads. In this paper, we propose μDPM, a dynamic power management scheme for the microsecond era that coordinates request delaying, per-core sleep states, and voltage frequency scaling. The idea is to postpone the wake up of a CPU as long as possible and then adjust the frequency so that the tail latency constraint of requests are satisfied just-in-time. μDPM reduces processor energy consumption by up to 32% and consistently outperforms state-of-the-art techniques by 2×.","['Dynamic power management', 'DVFS', 'Sleep states', 'Power system management', 'Servers', 'Energy consumption', 'Power demand', 'Data centers', 'Market research', 'Linux']",2378-203X,,,0,EC5,0,65
44,JOUR,10.1109/CLOUD60044.2023.00067,"['Alzahrani A.', 'Tang M.']",A Microservice-based SaaS Deployment in a Data Center Considering Computational Server and Network Energy Consumption,"IEEE International Conference on Cloud Computing, CLOUD",,,"In a data center, deploying a microservice-based SaaS will result in a spike in energy usage, since the computation servers where the microservices are deployed will consume more energy as a result of an increase in computation workload. The data center's communications network will also consume more energy due to the increased communication among the microservices of the SaaS. Due to the fact that microservice-based SaaS deployment is handled by the developer of the microservice-based SaaS and can only be deployed on virtual machines rented by the developer, this issue cannot be taken into account in traditional microservice-based SaaS deployment methods. In this paper, a new microservice-based SaaS deployment method is proposed. The new approach relies on the data center determining where microservices should be deployed. The new microservice-based SaaS deployment method considers the energy increase in the computation servers and in the communication network. The microservice-based SaaS deployment problem is a combinatorial optimization problem. Thus, a genetic algorithm with repairing mechanism is proposed to solve the problem. Compared to traditional deployment approach, the proposed method is capable of reducing the energy consumption associated with microservice-based SaaS deployment by 37.55%. © 2023 IEEE.","['Cloud computing', 'Data center', 'Deployment', 'Energy consumption', 'Genetic algorithm', 'Microservice', 'Optimization', 'Software as a Service', 'Genetic algorithms', 'Green computing', 'Software as a service (SaaS)', 'Cloud-computing', 'Communications networks', 'Datacenter', 'Deployment', 'Deployment methods', 'Energy', 'Energy-consumption', 'Microservice', 'Optimisations', 'Software-as-a- Service (SaaS)', 'Energy utilization']",,2023,,0,EC5,0,66
92,JOUR,10.1109/JSEN.2024.3502254,"['D. Loconte', 'S. Ieva', 'F. Gramegna', 'I. Bilenchi', 'C. Fasciano', 'A. Pinto', 'G. Loseto', 'F. Scioscia', 'M. Ruta', 'E. Di Sciascio']",Serverless Microservice Architecture for Cloud-Edge Intelligence in Sensor Networks,IEEE Sensors Journal,25-Nov-24,IEEE,"Machine learning (ML) is increasingly exploited in a wide range of application areas to analyze data streams from large-scale sensor networks, train predictive models, and perform inference. The cloud-edge intelligence (CEI) computing paradigm integrates cloud infrastructures for resource-intensive ML tasks with devices at the border of a local network for distributed data preprocessing, small-scale model training, and prediction tasks. This can achieve a tunable trade-off of ML accuracy with improved data privacy, response latency, and bandwidth usage. Prevalent CEI architectures are based on microservices encapsulated in containers, but serverless computing is emerging as an alternative model. It is based on stateless event-driven functions to facilitate the development and provisioning of application components, increase the infrastructure elasticity, and reduce management effort. This article proposes a novel CEI framework for sensor-based applications, exploiting serverless computing for data management and ML tasks. Small-scale model training occurs at the edge with local data for quick prediction response, while large-scale models are trained in the cloud with the full sensor network data, and then, they are fed back to edge nodes for a progressive accuracy improvement. A fully functional prototype has been built by leveraging open-source software tools, selected devices for field sensing and edge computing (EC), and a commercial cloud platform. Experiments validate the feasibility and sustainability of the proposal, compared with an existing container-oriented microservice architecture.","['Cloud-edge intelligence (CEI)', 'machine learning (ML)', 'microservices', 'sensor networks', 'serverless computing', 'Cloud computing', 'Sensors', 'Training', 'Computer architecture', 'Microservice architectures', 'Computational modeling', 'Data models', 'Accuracy', 'Internet of Things', 'Intelligent sensors']",1558-1748,,,0,EC5,0,67
42,JOUR,10.1109/ACCESS.2023.3281348,"['S. M. Rajagopal', 'M. Supriya', 'R. Buyya']",Resource Provisioning Using Meta-Heuristic Methods for IoT Microservices With Mobility Management,IEEE Access,30-May-23,IEEE,"The fog and edge computing paradigm provide a distributed architecture of nodes with processing capability for smart healthcare systems driven by Internet of Thing (IoT) applications. It also provides a method to reduce big data transmissions that cause latency and enhance the system’s efficiency. Resource provisioning and scheduling in edge and fog systems is a significant problem due to heterogeneity and dispersion of edge/fog/cloud resources. The goal of scheduling is to map tasks to appropriate resources, which belong to NP-hard problems, and it takes much time to find an optimal solution. Meta-heuristic methods achieve near-optimal solutions within a reasonable time. Current edge/fog resource allocation research does not sufficiently address resource allocation problems in mobility-aware microservice-based IoT applications. This paper proposes a meta-heuristic-based micro-service resource provisioning model with mobility management for smart healthcare systems. The proposed approach has been tested on an experimental set-up with a simulation of a critical real-time smart healthcare application with and without considering the mobility of the devices. It applies meta-heuristic methods such as modified genetic and flower pollination algorithms for resource management. The proposed method outperforms the existing solutions in energy consumption, network usage, cost, execution time, and latency by 17%, 20%, 22%, 17%, and 63%, respectively.","['Edge computing', 'fog computing', 'Internet of Things', 'meta-heuristic', 'microservices', 'mobility', 'smart healthcare', 'time critical applications', 'Edge computing', 'Internet of Things', 'Smart healthcare', 'Metaheuristics', 'Microservice architectures']",2169-3536,,,0,EC5,0,68
116,JOUR,10.1145/3539606,"['Carrión, Carmen']","Kubernetes Scheduling: Taxonomy, Ongoing Issues and Challenges",ACM Comput. Surv.,2022-12,,"Continuous integration enables the development of microservices-based applications using container virtualization technology. Container orchestration systems such as Kubernetes, which has become the de facto standard, simplify the deployment of container-based applications. However, developing efficient and well-defined orchestration systems is a challenge. This article focuses specifically on the scheduler, a key orchestrator task that assigns physical resources to containers. Scheduling approaches are designed based on different Quality of Service (QoS) parameters to provide limited response time, efficient energy consumption, better resource utilization, and other things. This article aims to establish insight knowledge into Kubernetes scheduling, find the main gaps, and thus guide future research in the area. Therefore, we conduct a study of empirical research on Kubernetes scheduling techniques and present a new taxonomy for Kubernetes scheduling. The challenges, future direction, and research opportunities are also discussed.","['scheduling', 'containers', 'Kubernetes', 'orchestration', 'survey']",0360-0300,,['https://doi.org/10.1145/3539606'],0,EC5,0,69
89,JOUR,10.23919/ITC.2017.8065706,"['Carrega A.', 'Repetto M.']",Energy-Aware Consolidation Scheme for Data Center Cloud Applications,"Proceedings of the 29th International Teletraffic Congress, ITC 2017",,,"The consolidation of resources is one of the most efficient strategies to reduce the power consumption in data centers. Various algorithms have been proposed in order to reduce the total number of required servers and network devices. The practice developed in response to the problem of server sprawl, a situation in which multiple, under-utilized servers (and/or network devices) take up more space and consume more resources than the ones justified by their workload; with the effect to power off unused equipment. Generally, consolidation mechanisms consider different parameters related to the services neglecting the specific function of the Virtual Machines (VMs) in the application framework (e.g., core component, backup replica, member of a set of workers for load balancing). In this work, we develop a new consolidation algorithm that takes into account the particular function of each VM with the aim to apply power saving mechanisms without compromising the desired service level. The results of the simulations show that it is possible to obtain significant energy savings. In particular, we show, with different heuristics, the optimal trade-off between service level and power efficiency achieved by the proposed model. © 2017 ITC Press.","['Economic and social effects', 'Energy conservation', 'Energy efficiency', 'Power management', 'Application frameworks', 'Cloud applications', 'Consolidation mechanisms', 'Core components', 'Efficient strategy', 'Network devices', 'Power efficiency', 'Power saving mechanism', 'Green computing']",,2017,,0,EC5,0,70
56,JOUR,10.1007/978-3-319-67639-5_17,"['Carrega A.', 'Repetto M.']",Green and heuristics-based consolidation scheme for data center cloud applications,Communications in Computer and Information Science,,,"The consolidation of resources is one of the most efficient strategies to reduce the power consumption in data centers. Various algorithms have been proposed in order to reduce the total number of required servers and network devices. The practice developed in response to the problem of server sprawl, a situation in which multiple, under-utilized servers (and/or network devices) take up more space and consume more resources than can be justified by their workload; with the effect to power off unused equipment. Generally, consolidation mechanisms consider different parameters related to the services neglecting the specific function of the Virtual Machines (VMs) in the application framework (e.g., core component, backup replica, member of a set of workers for load balancing). In this work, we developed a new consolidation algorithm that takes into account the particular function of each VM with the aim to apply power saving mechanisms without compromising the desired service level. The results of the simulations show that it is possible to obtain significant values of energy saving. In particular, we show, with different heuristics, the optimal trade-off between service level and power efficiency achieved by the proposed model. © Springer International Publishing AG 2017.","['Cloud applications', 'Consolidation', 'Energy-aware', 'Green', 'Heuristics', 'Optimization', 'Virtual machines', 'Consolidation', 'Digital communication systems', 'Economic and social effects', 'Energy conservation', 'Energy efficiency', 'Network security', 'Optimization', 'Power management', 'Virtual machine', 'Application frameworks', 'Cloud applications', 'Consolidation mechanisms', 'Efficient strategy', 'Energy aware', 'Green', 'Heuristics', 'Power saving mechanism', 'Green computing']",,2017,,0,EC6,0,71
57,JOUR,10.1109/MICRO61859.2024.00103,"['J. Stojkovic', 'E. Choukse', 'E. Saurez', 'Í. Goiri', 'J. Torrellas']",Mosaic: Harnessing the Micro-Architectural Resources of Servers in Serverless Environments,2024 57th IEEE/ACM International Symposium on Microarchitecture (MICRO),03-Dec-24,IEEE,"With serverless computing, users develop scalable applications using lightweight functions as building blocks, while cloud providers own most of the computing stack, allowing for better resource optimizations. In this paper, we observe that modern server-class processors are inefficiently utilized in serverless environments. Cores perform frequent context switches within function invocations and have a high degree of oversubscription. In such an environment, functions frequently lose their micro-architectural state in stateful hardware structures like caches, TLBs, and branch predictors, causing performance degradation. At the same time, modern processors are dimensioned for the needs of a broad set of applications, rendering them suboptimal for serverless workloads. Based on these insights, we propose Mosaic, an architecture optimized for serverless environments that maintains generality to efficiently support other workloads. Mosaic has two components: (1) MosaicCPU, a processor architecture that efficiently runs both serverless workloads and traditional monolithic applications, and (2) MosaicScheduler, a software stack for serverless systems that maximizes the benefits of MosaicCPU. MosaicCPU slices micro-architectural structures into small chunks and assigns tiles of such chunks to functions. The processor retains the state of functions in their tiles across context switches, thereby improving performance. Furthermore, currently-inactive tiles are set to a low power mode, thereby reducing energy consumption. In addition, MosaicScheduler maximizes efficiency by introducing predictive right-sizing of the per-function tiles, alongside with smart scheduling based on the state of the tiles. Overall, compared to conventional server-class processors, Mosaic improves the throughput of serverless workloads by 225% while using 22% less power.","['Cloud computing', 'Serverless computing', 'Hardware partitioning', 'Context', 'Schedules', 'Program processors', 'Processor scheduling', 'Full stack', 'Serverless computing', 'Computer architecture', 'Throughput', 'Rendering (computer graphics)', 'Optimization']",2379-3155,,,0,EC5,0,72
81,JOUR,10.1145/3147213.3147227,"['Hasan, MD Sabbir', 'Alvares, Frederico', 'Ledoux, Thomas']",GPaaScaler: Green Energy Aware Platform Scaler for Interactive Cloud Application,Proceedings of The10th International Conference on Utility and Cloud Computing,2017,Association for Computing Machinery,"Recently, smart usage of renewable energy has been a hot topic in the Cloud community. In this vein, we have recently proposed the creation of green energy awareness around Interactive Cloud Applications, but in static amount of underlying resources. This paper adds to previous ones as it considers elastic underlying infrastructure, that is, we propose a PaaS solution which efficiently utilize the elasticity nature at both infrastructure and application levels, by leveraging adaptation in facing to changing condition i.e., workload burst, performance degradation, quality of energy, etc. While applications are adapted by dynamically re-configuring their service level based on performance and/or green energy availability, the infrastructure takes care of addition/removal of resources based on application's resource demand. Both adaptive behaviors are implemented in separated modules and are coordinated in a sequential manner.We validate our approach by extensive experiments and results obtained over Grid'5000 test bed. Results show that, application can reduce significant amount of brown energy consumption by 35% and daily instance hour cost by 37% compared to a baseline approach when green energy aware adaptation is considered.","['energy consumption', 'autonomic computing', 'green it', 'interactive cloud application', 'paas', 'sustainable computing.']",978-1-4503-5149-2,,['https://doi.org/10.1145/3147213.3147227'],0,EC5,0,73
34,JOUR,10.1109/IPDPSW63119.2024.00159,"['T. Menouer', 'C. Cérin', 'P. Darmon']",KOptim: Kubernetes Optimization Framework,2024 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),26-Jul-24,IEEE,"As Kubernetes has become a de-facto standard for a portable, extensible, open-source platform for managing containerized workloads and services, this paper presents a new Kubernetes Optimization Framework called KOptim, which provides a generic schema for scheduling and resource management. Compared to existing Kubernetes frameworks and scheduling procedures, the advantages of KOptim are: (i) it facilitates the configuration of container resources using Service Level Agreement (SLA) classes, and (ii) it optimizes the online scheduling of containers using a Multi-Criteria Decision Making (MCDM) algorithm. In short, the main contribution of KOptim is to simplify Kubernetes scheduler optimization, as well as other issues related to container configuration and validation. In KOptim, we propose to configure and manage container resources using a new model with three SLA classes related to CPUs, memory, and storage space. Each SLA class has three Quality of Services (QoS): Premium, Advanced, and Best Effort. Using the KOptim SLA classes, users can select a range of values for a service instead of specifying a fixed amount of resources for each container. This is designed to help non-expert users who do not need to know exactly how many resources must be used to configure their containers easily. In the scheduling step, we suggest using a mechanism based on a multi-criteria decision algorithm that allows users to consider several criteria during the online scheduling decision. In summary, KOptim is a generic framework for Kubernetes distributions even K3s and MicroK8s which are adapted to Fog and Edge computing. Experimental results demonstrate the potential of our framework to improve performance in terms of runtime and energy consumption.","['Container technology', 'Service Level Agreement', 'Online multi-criteria scheduling', 'Kubernetes ecosystem', 'Energy consumption', 'Processor scheduling', 'Computational modeling', 'Quality of service', 'Containers', 'MCDM', 'Resource management']",,,,0,EC5,0,74
104,JOUR,10.1016/j.future.2022.03.027,"['Das A.', 'Chakraborty S.', 'Chakraborty S.']",Where do all my smart home data go? Context-aware data generation and forwarding for edge-based microservices over shared IoT infrastructure,Future Generation Computer Systems,,,"With the explosion of the Internet of Things (IoT) devices, the advent of the edge computing paradigm, and the rise of intelligent applications for smart infrastructure surveillance, in-network data management is gaining a lot of research attention these days. The challenge lies in accommodating multiple application microservices with varying Quality of Service (QoS) requirements to share the sensing infrastructure for better resource utilization. In this work, we propose a novel data collection framework, CaDGen (Context-aware Data Generation) for such a shared IoT infrastructure that enables integrated data filtration and forwarding towards minimizing the resource consumption footprint for the IoT infrastructure. The proposed filtration mechanism utilizes the contextual information associated with the running application for determining the relevance of the data. Whereas the proposed forwarding policy aims to satisfy the diverse QoS requirements for the running applications by selecting the suitable next-hop forwarder based on the microservices distribution across different edge devices. A thorough performance evaluation of CaDGen through a testbed implementation as well as a simulation study for diverse setups reveals promising results concerning network resource utilization, scalability, energy conservation, and distribution of computation for optimal service provisioning. It is observed that the CaDGen can achieve nearly 35% reduction in the generated data for a moderately dynamic scenario without compromising on the data quality. © 2022 Elsevier B.V.","['Context-aware', 'Data collection', 'Edge computing', 'Microservice', 'Sensing', 'Automation', 'Data acquisition', 'Edge computing', 'Information management', 'Network security', 'Petroleum reservoir evaluation', 'Quality of service', 'Context-Aware', 'Data collection', 'Data generation', 'Data-forwarding', 'Edge computing', 'Microservice', 'Quality-of-service', 'Running applications', 'Service requirements', 'Smart homes', 'Internet of things']",,2022,,0,EC5,0,75
77,JOUR,10.1109/ISCA59077.2024.00041,"['Wang J.', 'Berger D.S.', 'Kazhamiaka F.', 'Irvene C.', 'Zhang C.', 'Choukse E.', 'Frost K.', 'Fonseca R.', 'Warrier B.', 'Bansal C.', 'Stern J.', 'Bianchini R.', 'Sriraman A.']",Designing Cloud Servers for Lower Carbon,Proceedings - International Symposium on Computer Architecture,,,"To mitigate climate change, we must reduce carbon emissions from hyperscale cloud computing. We find that cloud compute servers cause the majority of emissions in a general-purpose cloud. Thus, we motivate designing carbon-efficient compute server SKUs, or GreenSKUs, using recently-available low-carbon server components. To this end, we design and build three GreenSKUs using low-carbon components, such as energy-efficient CPUs, reused old DRAM via CXL, and reused old SSDs.We detail several challenges that limit GreenSKUs, carbon savings at scale and may prevent their adoption by cloud providers. To address these challenges, we develop a novel methodology and associated framework, GSF (GreenSKU Framework), that enables a cloud provider to systematically evaluate a GreenSKU's carbon savings at scale. We implement GSF within Microsoft Azure's production constraints to evaluate our three GreenSKUs' carbon savings. Using GSF, we show that our most carbon-efficient GreenSKU reduces emissions per core by 28 % compared to currently-deployed cloud servers. When designing GreenSKUs to meet applications' performance requirements, we reduce emissions by 15 %. When incorporating overall data center overheads, our GreenSKU reduces Azure's net cloud emissions by 8 %. © 2024 IEEE.","['Carbon', 'Cloud Computing', 'Data Center', 'Server Design', 'Sustainability', 'Carbon', 'Climate change', 'Energy efficiency', 'Green computing', 'Program processors', 'Windows operating system', 'Carbon emissions', 'Carbon saving', 'Cloud providers', 'Cloud servers', 'Cloud-computing', 'Datacenter', 'Design and build', 'Low carbon', 'Server components', 'Server design', 'Cloud computing']",,2024,,0,EC5,0,76
85,JOUR,10.1088/1742-6596/1544/1/012020,"['Wu L.', 'Xia H.']",Particle Swarm Optimization Algorithm for Container Deployment,Journal of Physics: Conference Series,,,"In recent years, with the development of cloud computing, virtualization technology has received widespread attention. As a new representative of virtualization technology, containers have been widely used in software development, operation and maintenance, testing and other aspects, such as microservices and Docker Cloud. In cloud data centers, containers have gradually replaced virtual machines (VMs) as a new carrier for cloud tasks. However, with the increasing number of cloud products, the scale of tasks requested by users in cloud data centers continues to expand. The economic cost of tasks in the process of containerized deployment has become a concern of various cloud service vendors. The container deployment cost usually includes the data exchange cost, the image pull cost and the server energy cost. In order to solve the containerized deployment of application tasks in the container cloud environment with the lowest possible container deployment cost, this paper proposes a new cost calculation model in a container cloud environment, and then presents an improved particles swarm algorithm, namely a particle swarm optimization (PSO) algorithm for container deployment (CD-PSO) to provide the best solution for application task loading. Experimental results show that the proposed algorithm has a lower deployment cost than other scheduling algorithms. © 2019 Published under licence by IOP Publishing Ltd.","['Cloud computing', 'Containers', 'Electronic data interchange', 'Intelligent computing', 'Scheduling algorithms', 'Signal processing', 'Software design', 'Software testing', 'Virtualization', 'Application tasks', 'Cloud data centers', 'Cloud environments', 'Cost calculation', 'Deployment costs', 'Operation and maintenance', 'Particle swarm optimization algorithm', 'Virtualization technologies', 'Particle swarm optimization (PSO)']",,2020,,0,EC5,0,77
32,JOUR,10.1117/12.3024334,"['Zhang Z.', 'Xu Z.', 'Li R.']",Research and Application of Distributed Task Scheduling for Smart Energy Services,Proceedings of SPIE - The International Society for Optical Engineering,,,"Smart energy services are an important support platform for the development of a comprehensive energy business. Based on CPS, they connect with energy consumption scenarios such as communities, commercial buildings, industrial enterprises, and parks, achieve massive data acquisition of energy equipment, energy conservation, and consumption reduction, and smart regulation of flexible loads. They are also built on cloud platforms and microservice architectures. Distributed task scheduling technology is widely used in massive data acquisition, data archiving, data supplementary acquisition, data quality, data calculation, energy consumption analysis, energy efficiency analysis, energy efficiency diagnosis, monitoring alarm, instruction encryption, and issuance, energy efficiency report generation and push and many other scenarios for processing. There are various shortcomings in the existing distributed task scheduling under the microservice architecture mode. In order to fully leverage the important role of distributed task scheduling in smart energy services based on microservice architecture, this article analyzes the characteristics of microservice architecture and the shortcomings of existing distributed task scheduling and applies the design concepts of distributed application coordination services, balanced sharding, and microservice architecture to study a distributed task scheduling oriented towards microservice architecture, effectively supporting the widespread application of smart energy in distributed task scheduling. © 2024 SPIE.","['Comprehensive energy', 'Distributed application coordination services', 'Distributed task scheduling', 'Microservice architecture', 'Architecture', 'Cryptography', 'Data acquisition', 'Energy efficiency', 'Historic preservation', 'Metadata', 'Multitasking', 'Quality control', 'Comprehensive energy', 'Distributed application coordination service', 'Distributed applications', 'Distributed task scheduling', 'Distributed tasks', 'Energy services', 'Energy-consumption', 'Microservice architecture', 'Smart energies', 'Tasks scheduling', 'Energy utilization']",,2024,,0,EC6,0,78
58,JOUR,10.24425/ijet.2024.152511,"['Nuñez I.', 'Rovetto C.', 'Cruz E.', 'Smolarz A.', 'Concepcion D.', 'Cano E.E.']",Design of a microservices-based architecture for residential energy efficiency monitoring,International Journal of Electronics and Telecommunications,,,"With the significant advancement of electrical infrastructure in the context of smart buildings and smart homes, the need arises to overcome the limitations of the traditional energy efficiency control system based on service-oriented architecture (SOA). To address these challenges, this study proposes a distributed architecture based on microservices, with the main objective of improving the performance and stability of these systems. This proposal seeks to enable end users to effectively monitor and control their electrical devices while effectively integrating them into a wide network of power systems. The proposed architecture relies on a series of cloud services that enable better performance and control in energy efficiency management, highlighting key features of microservices such as fault tolerance, performance, and scalability. Using a structural methodology centered on preexisting components and an iterative approach, a versatile and scalable architecture was designed that addresses current challenges in energy efficiency management. The results show a significant impact on key performance indicators such as demand response, energy savings, and power quality, highlighting the resilience and scalability of the proposed architecture. The conclusions highlight the importance of energy efficiency in reducing the environmental impact and costs associated with electric power, suggesting future improvements in data access and the implementation of advanced machine learning algorithms. © The Author(s).","['energy efficiency', 'internet of things (IoT)', 'Microservices', 'smart homes', 'software architecture', 'Service oriented architecture (SOA)', 'Efficiency control', 'Efficiency managements', 'Electrical infrastructure', 'Energy', 'Internet of thing', 'Microservice', 'Performance', 'Proposed architectures', 'Residential energy efficiency', 'Smart homes', 'Smart homes']",,2024,,0,EC5,0,79
39,JOUR,10.1145/3674734,"['Antoniou, Georgia', 'Bartolini, Davide', 'Volos, Haris', 'Kleanthous, Marios', 'Wang, Zhe', 'Kalaitzidis, Kleovoulos', 'Rollet, Tom', 'Li, Ziwei', 'Mutlu, Onur', 'Sazeides, Yiannakis', 'Haj Yahya, Jawad']",Agile C-states: A Core C-state Architecture for Latency Critical Applications Optimizing both Transition and Cold-Start Latency,ACM Trans. Archit. Code Optim.,2024-11,,"Latency-critical applications running in modern datacenters exhibit irregular request arrival patterns and are implemented using multiple services with strict latency requirements (30–250μs). These characteristics render existing energy-saving idle CPU sleep states ineffective due to the performance overhead caused by the state’s transition latency. Besides the state transition latency, another important contributor to the performance overhead of sleep states is the cold-start latency, or in other words, the time required to warm up the microarchitectural state (e.g., cache contents, branch predictor metadata) that is flushed or discarded when transitioning to a lower-power state. Both the transition latency and cold-start latency can be particularly detrimental to the performance of latency critical applications with short execution times. While prior work focuses on mitigating the effects of transition and cold-start latency by optimizing request scheduling, in this work we propose a redesign of the core C-state architecture for latency-critical applications. In particular, we introduce C6Awarm, a new Agile core C-state that drastically reduces the performance overhead caused by idle sleep state transition latency and cold-start latency while maintaining significant energy savings. C6Awarm achieves its goals by (1) implementing medium-grained power gating, (2) preserving the microarchitectural state of the core, and (3) keeping the clock generator and PLL active and locked. Our analysis for a set of microservices based on an Intel Skylake server shows that C6Awarm manages to reduce the energy consumption by up to 70% with limited performance degradation (at most 2%).","['microservices', 'datacenters', 'Power management', 'C-states', 'idle states']",1544-3566,,['https://doi.org/10.1145/3674734'],0,EC5,0,80
46,JOUR,10.1007/978-3-031-20984-0_38,"['N. Toosi, Adel', 'Agarwal, Chayan', 'Mashayekhy, Lena', 'Moghaddam, Sara K.', 'Mahmud, Redowan', 'Tari, Zahir']",GreenFog: A Framework for&nbsp;Sustainable Fog Computing,"Service-Oriented Computing: 20th International Conference, ICSOC 2022, Seville, Spain, November 29 – December 2, 2022, Proceedings",2022,Springer-Verlag,"The alarming rate of increase in energy demand and carbon footprint of Fog environments has become a critical issue. It is, therefore, necessary to reduce the percentage of brown energy consumption in these systems and integrate renewable energy use into Fog. Renewables, however, are prone to availability fluctuations due to their variable and intermittent nature. In this paper, we propose a new Fog framework and design various optimization techniques, including linear programming optimization, linear regression estimation, and Multi-Armed Bandit (MAB) learning to optimize renewable energy use in the Fog based on a novel idea of load shaping with adaptive Quality of Service (QoS). The proposed framework, along with the optimization techniques, are tested on a real-world micro data center (Fog environment) powered by solar energy sources connected to multiple IoT devices. The results show that our proposed framework significantly reduces the difference between renewable energy generation and total energy consumption while efficiently adjusting the QoS of applications.",,978-3-031-20983-3,,['https://doi.org/10.1007/978-3-031-20984-0_38'],0,EC5,0,81
26,JOUR,10.1145/3663741.3664789,"['Ordonez, Carlos', 'Macyna, Wojciech', 'Bellatreche, Ladjel']",Energy-Aware Analytics in the Cloud,Proceedings of the International Workshop on Big Data in Emergent Distributed Environments,2024,Association for Computing Machinery,"Big data is now mostly processed in the cloud and will keep growing, fed by databases and the Internet of Things (IoT: sensors, mobile devices, edge computing). On the other hand, AI is pushing computers and data analysis to limits we had not witnessed before. Analytics in the cloud is now a major fraction of energy consumption, among other less CPU-intensive tasks like web services. With this green computing motivation in mind, we present a survey of past research and a vision of big data analytics in the cloud. Energy consumption is difficult to minimize because it has conflicting correlated variables behind: high performance, money cost and pollution, We identify which software subsystems and hardware components have a higher impact on energy consumption, understanding how they can be tweaked or tuned to optimize energy consumption.",,979-8-4007-0679-0,,['https://doi.org/10.1145/3663741.3664789'],0,EC5,0,82
65,JOUR,10.1007/978-3-031-71142-8_2,"['Poth A.', 'Saalfeld L.']",Improve IT Sustainability with IT Technology – Comparison of an Explore vs. Exploit Strategy (with a Case Study on Containerized Workloads),Communications in Computer and Information Science,,,"The size and complexity of IT systems is growing over time. Therefore, the resource unit footprint of an IT system grows, too. But what impact has an exploration strategy of new technologies to reduce the footprint? Based on a case study of a cloud service from the Volkswagen Group IT an exploration approach is compared with an exploitation approach. It presents the methodical approach and its effects on the resource unit footprint of a micro-service architecture of the IT service. The approach enables the DevOps team to beat Wirth’s law by keeping the footprint at least constant by offering more capabilities and features to their users. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.","['Containerization', 'Green IT', 'Micro-Service', 'Native Image', 'Sustainability', 'Wirth’s law', 'Case-studies', 'Containerization', 'Exploration strategies', 'Green-IT', 'IT system', 'Micro services', 'Native image', 'Resource units', 'Technology comparison', 'Wirth’s law']",,2024,,0,EC5,0,83
60,JOUR,10.1109/TITS.2021.3110725,"['C. Roy', 'R. Saha', 'S. Misra', 'K. Dev']",Micro-Safe: Microservices- and Deep Learning-Based Safety-as-a-Service Architecture for 6G-Enabled Intelligent Transportation System,IEEE Transactions on Intelligent Transportation Systems,27-Sep-21,IEEE,"In this paper, we propose a microservices and deep learning-based scheme, termed as Micro-Safe, for provisioning Safety-as-a-Service (Safe-aaS) in a 6G environment. A Safe-aaS infrastructure provides customized safety-related decisions dynamically to the registered end-users. As the decisions are time-sensitive in nature, the generation of these decisions should incur minimum latency and high accuracy. Further, scalability and extension of the coverage of the entire Safe-aaS platform are also necessary. Considering road transportation as the application scenario, we propose Safe-aaS, which is a microservices- and deep learning-based platform for provisioning ultra-low latency safety services to the end-users in a 6G scenario. We design the proposed solution in two stages. In the first stage, we develop the microservices-enabled application layer to improve the scalability and adaptability of the traditional Safe-aaS platform. Moreover, we apply the state space model to represent the decision parameters requested and the decision delivered to the end-users. During the second stage, we use deep learning models to improve the accuracy in the decisions delivered to the end-users. Additionally, we apply an assortment of activation functions to analyze and compare the accuracy of the decisions generated in the proposed scheme. Extensive simulation of our proposed scheme, Micro-Safe, demonstrates that latency is improved by 26.1 – 31.2%, energy consumption is reduced by 22.1 – 29.9%, throughput is increased by 26.1 – 31.7%, compared to the existing schemes.","['6G', 'microservices', 'deep learning', 'state space model', 'safety-as-a-service (safe-aas)', 'road transportation', 'decision parameters', '6G mobile communication', 'Safety', 'Deep learning', 'Scalability', 'Delays', 'Cloud computing', 'Throughput']",1558-0016,,,0,EC5,0,84
103,JOUR,10.1109/ACCESS.2022.3214229,"['L. C. B. C. Ferreira', 'A. D. R. Borchardt', 'G. D. S. Cardoso', 'D. A. Mendes Lemes', 'G. R. D. R. d. Sousa', 'F. B. Neto', 'E. R. de Lima', 'G. Fraidenraich', 'P. Cardieri', 'L. G. P. Meloni']",Edge Computing and Microservices Middleware for Home Energy Management Systems,IEEE Access,13-Oct-22,IEEE,"A middleware software can be seem as an abstraction layer between hardware and user applications, that facilitates the development and deployment of services in various scenarios, such as those found in Home Energy Management Systems (HEMS). There are several middleware proposals for HEMS, with most of them taking the cloud computing approach. This approach is unconcerned about computing resources but raises a dependency on external connections. This paper presents a middleware for energy management systems, based on the concept of edge computing for smart homes. The paper presents a reference model for the proposed architecture, considering specific requirements for this type of application. The proposed architecture employs the concept of microservices for data access and system configuration. The proposed middleware is designed to work with embedded systems under computational constraints, such as processing capability and storage, to reduce costs and allow its application closer to the user. The middleware is open and customizable to meet the developer’s needs. The proposed solution was implemented and tested in a university laboratory, as well as at the Eldorado Research Institute to confirm the effectiveness of the middleware. The proposal stands out from others found in the literature as it can be implemented using low cost hardware. In addition to using microservices concepts, the proposed middleware is a valuable option for applications that need an edge computing approach. A performance analysis was carried out, using low cost hardware with limited resources. The results show that the proposal can handle a significant number of devices, offering low latency and low error rate, and consuming few processing resources and memory.","['Home energy management systems', 'middleware', 'Internet of Things', 'Middleware', 'Internet of Things', 'Hardware', 'Edge computing', 'Computer architecture', 'Microservice architectures', 'Energy management systems', 'Computer applications']",2169-3536,,,0,EC5,0,85
45,JOUR,10.1109/IOTSMS48152.2019.8939237,"['Valera H.H.L.', 'Dalmau M.', 'Roose P.', 'Herzog C.']",The Architecture of Kaligreen V2: A Middleware Aware of Hardware Opportunities to Save Energy,"2019 6th International Conference on Internet of Things: Systems, Management and Security, IOTSMS 2019",,,"Nowadays, energy saving in the use of information technologies is a very important issue both from the economic and sustainability point of view. Many scientists investigate methods to save energy at different application levels (cloud: i.e., architectures, grid: i.e., middlewares and frameworks and hardware management: i.e., operating systems) and many of them agree on the strategy of executing programs, processes or virtual machines only using the time and resources that are strictly necessary. For this, it is necessary to plan strategies for deployment and relocation of processes; but always taking into account hardware repercussions and the knowledge of the architecture and applications behavior. On the other hand, it has already been demonstrated that the use of microservices brings numerous advantages in availability and efficiency; but we do not find many jobs that exploit this technique on the energy level. In this article, we present the architecture of a middleware for distributed microservices-based applications, which allows any negotiation-based scheduling algorithm to duplicate or move microservices from one device to another in a non-centralized way for energy savings, taking into account the consumption characteristics of the microservices and the capabilities that the hardware components offer. © 2019 IEEE.","['consumption', 'CPU', 'energy', 'hard disk', 'microservices', 'middleware', 'network', 'Application programs', 'Energy conservation', 'Hard disk storage', 'Information use', 'Internet of things', 'Middleware', 'Networks (circuits)', 'Program processors', 'Scheduling algorithms', 'Application level', 'consumption', 'energy', 'Hardware components', 'Hardware management', 'microservices', 'Save energy', 'Network architecture']",,2019,,0,EC5,0,86
63,JOUR,10.1016/j.procs.2018.10.172,"['Lvarez-Valera H.H.', 'Roose P.', 'Dalmau M.', 'Herzog C.', 'Respicio K.']",Kali green: A distributed scheduler for energy saving,Procedia Computer Science,,,"A commonplace issue with portable technology is battery efficiency. While many industries are trying their best to improve battery life without sacrificing a products quality and efficiency, we believe that further can be done to improve battery consumption on ones mobile devicefrom tablets to smartphones to laptops to everything else. Many applications on these devices are based on a microservice architecture. In this article, we introduce a new algorithm KaliGreen that can maneuver the microservices within a network of devices in order to maximize the run-time of a microservice-based application; moreover, KaliGreen allows a 54% increase in the average run-time of an application by shifting microservices from 6 devices (as example) with low battery or inefficient processing ratios to devices in better conditions. To achieve this, KaliGreen utilizes KaliMucho middleware, which is able manipulate microservices in run-time. This algorithm provides a plausible solution to maximizing energy consumption within a network of devices. © 2018 The Authors. Published by Elsevier Ltd.","['Distributed applicatioggns', 'Green Computing', 'Microservices', 'Smartphones', 'Electric batteries', 'Energy conservation', 'Energy utilization', 'mHealth', 'Middleware', 'Smartphones', 'Battery consumption', 'Battery efficiencies', 'Battery life', 'Distributed applicatioggns', 'Distributed schedulers', 'Microservices', 'Portable technologies', 'Products quality', 'Green computing']",,2018,,0,EC5,0,87
41,JOUR,10.1007/s11227-023-05202-6,"['Guamán D.', 'Pérez J.', 'Valdiviezo-Diaz P.']",Estimating the energy consumption of model-view-controller applications,Journal of Supercomputing,,,"For information and communication technology to reach its goal of zero emissions in 2050, power consumption must be reduced, including the energy consumed by software. To develop sustainability-aware software, green metrics have been implemented to estimate the energy consumed by the execution of an application. However, they have a rebound energy consumption effect because they require an application to be executed to estimate the energy consumed after each change. To address this problem, it is necessary to construct energy estimation models that do not require the execution of applications. This work addresses this problem by constructing a green model based on size, complexity and duplicated lines to estimate the energy consumed by model-view-controller applications without their execution. This article defines a model constructed based on 52 applications. The results were accurate in twelve applications, which showed that the joule estimation was very close to reality, avoiding the energy consumed by the execution of applications. © 2023, The Author(s).","['Architectural patterns', 'Energy consumption estimation', 'Green software', 'Model-view controller (MVC)', 'Software architectures', 'Application programs', 'Controllers', 'Software architecture', 'Architectural pattern', 'Energy', 'Energy consumption estimation', 'Energy-consumption', 'Green metrics', 'Green software', 'Information and Communication Technologies', 'Model-view controller', 'Zero emission', 'Energy utilization']",,2023,,0,EC5,0,88
94,JOUR,10.1109/SDS64317.2024.10883885,['U. Vora'],Architecting a SDS for Microservices-based Distributed Edge Computing Systems,2024 11th International Conference on Software Defined Systems (SDS),18-Feb-25,IEEE,"Software Defined Systems (SDS) abstract the actual hardware at different layers with software components, layers such as Networking, Storage, Security, Servers, Data Centers, Clouds etc. This abstraction facilitates construction, integration / composition and management of complex systems effectively and sustainably. Microservices architecture is increasingly being used for designing and deploying large-scale application systems in both cloud-based and enterprise infrastructures where relatively small, loosely coupled microservices communicate with each other using lightweight communication protocols and abstract complex software application(s). We propose a sustainable SDS architecture to manage microservices of a distributed edge computing system which reduces dependency on any data centre / cloud infrastructure.","['Microservices', 'Edge computing', 'Software Defined System', 'Sustainability', 'Cloud computing', 'Data centers', 'Microservice architectures', 'Computer architecture', 'Software', 'Software measurement', 'Servers', 'Transient analysis', 'Complex systems', 'Edge computing']",,,,0,EC5,0,89
114,JOUR,10.3390/s23136117,"['Kaur A.', 'Kumar S.', 'Gupta D.', 'Hamid Y.', 'Hamdi M.', 'Ksibi A.', 'Elmannai H.', 'Saini S.']",Algorithmic Approach to Virtual Machine Migration in Cloud Computing with Updated SESA Algorithm,Sensors,,,"Cloud computing plays an important role in every IT sector. Many tech giants such as Google, Microsoft, and Facebook as deploying their data centres around the world to provide computation and storage services. The customers either submit their job directly or they take the help of the brokers for the submission of the jobs to the cloud centres. The preliminary aim is to reduce the overall power consumption which was ignored in the early days of cloud development. This was due to the performance expectations from cloud servers as they were supposed to provide all the services through their services layers IaaS, PaaS, and SaaS. As time passed and researchers came up with new terminologies and algorithmic architecture for the reduction of power consumption and sustainability, other algorithmic anarchies were also introduced, such as statistical oriented learning and bioinspired algorithms. In this paper, an indepth focus has been done on multiple approaches for migration among virtual machines and find out various issues among existing approaches. The proposed work utilizes elastic scheduling inspired by the smart elastic scheduling algorithm (SESA) to develop a more energy-efficient VM allocation and migration algorithm. The proposed work uses cosine similarity and bandwidth utilization as additional utilities to improve the current performance in terms of QoS. The proposed work is evaluated for overall power consumption and service level agreement violation (SLA-V) and is compared with related state of art techniques. A proposed algorithm is also presented in order to solve problems found during the survey. © 2023 by the authors.","['cloud computing', 'migration', 'power consumption', 'SESA', 'virtual machine', 'Algorithms', 'Cloud Computing', 'Humans', 'Cloud computing', 'Computing power', 'Digital storage', 'Electric power utilization', 'Energy efficiency', 'Green computing', 'Network security', 'Quality of service', 'Scheduling algorithms', 'Algorithmic approach', 'Cloud-computing', 'Computation service', 'Datacenter', 'Facebook', 'Google+', 'MicroSoft', 'Migration', 'Smart elastic scheduling algorithm', 'Virtual machine migrations', 'algorithm', 'cloud computing', 'human', 'Virtual machine']",,2023,,0,EC5,0,90
30,JOUR,10.1109/JIOT.2024.3488283,"['Golec M.', 'Wu H.', 'Ozturac R.', 'Parlikad A.K.', 'Cuadrado F.', 'Gill S.S.', 'Uhlig S.']",CAPTAIN: A Testbed for Co-Simulation of Scalable Serverless Computing Environments for AIoT Enabled Predictive Maintenance in Industry 4.0,IEEE Internet of Things Journal,,,"The massive amounts of data generated by the Industrial Internet of Things (IIoT) require considerable processing power, which increases carbon emissions and energy usage, and we need sustainable solutions to enable flexible manufacturing. Serverless computing shows potential for meeting this requirement by scaling idle containers to zero energy-efficiency and cost, but this will lead to a cold start delay. Most solutions rely on idle containers, which necessitates dynamic request time forecasting and container execution monitoring. Furthermore, Artificial Intelligence of Things (AIoT) can provide autonomous and sustainable solutions by combining IIoT with Artificial Intelligence (AI) to solve this problem. Therefore, we develop a new testbed, CAPTAIN, to facilitate AI-based co-simulation of scalable and flexible serverless computing in IIoT environments. The AI module in the CAPTAIN framework employs Random Forest (RF) and Light Gradient-Boosting Machine (LightGBM) models to optimize cold start frequency and prevent cold starts based on their prediction results. The proxy module additionally monitors the client-server network and constantly updates the AI module training dataset via a message queue. Finally, we evaluated the proxy module's performance using a predictive maintenance-based real-world IIoT application and the AI module's performance in a realistic serverless environment using a Microsoft Azure dataset. The AI module of the CAPTAIN outperforms baselines in terms of cold start frequency, computational time with 0.5 milliseconds, energy consumption with 1161.0 joules, and CO2 emissions with 32.25e-05 gCO2. The CAPTAIN testbed provides a co-simulation of sustainable and scalable serverless computing environments for AIoT-enabled predictive maintenance in Industry 4.0. © 2014 IEEE.","['Artificial Intelligence', 'Cloud Computing', 'Flexible Manufacturing', 'Industrial Internet of Things', 'Predictive Maintenance', 'Serverless Computing', 'Competition', 'Flexible manufacturing systems', 'Glass plants', 'Plastic bottles', 'Windows operating system', 'Cloud-computing', 'Cold-start', 'Computing environments', 'Cosimulation', 'Flexible manufacturing', 'Industrial internet of thing', 'Module performance', 'Predictive maintenance', 'Serverless computing', 'Sustainable solution', 'Testbeds']",,2024,,0,EC5,0,91
84,JOUR,10.1109/CISIS.2016.107,"['P. Ruiu', 'A. Scionti', 'J. Nider', 'M. Rapoport']",Workload Management for Power Efficiency in Heterogeneous Data Centers,"2016 10th International Conference on Complex, Intelligent, and Software Intensive Systems (CISIS)",22-Dec-16,IEEE,"The cloud computing paradigm has recently emerged as a convenient solution for running different workloads on highly parallel and scalable infrastructures. One major appeal of cloud computing is its capability of abstracting hardware resources and making them easy to use. Conversely, one of the major challenges for cloud providers is the energy efficiency improvement of their infrastructures. Aimed at overcoming this challenge, heterogeneous architectures have started to become part of the standard equipment used in data centers. Despite this effort, heterogeneous systems remain difficult to program and manage, while their effectiveness has been proven only in the HPC domain. Cloud workloads are different in nature and a way to exploit heterogeneity effectively is still lacking. This paper takes a first step towards an effective use of heterogeneous architectures in cloud infrastructures. It presents an in-depth analysis of cloud workloads, highlighting where energy efficiency can be obtained. The microservices paradigm is then presented as a way of intelligently partitioning applications in such a way that different components can take advantage of the heterogeneous hardware, thus providing energy efficiency. Finally, the integration of microservices and heterogeneous architectures, as well as the challenge of managing legacy applications, is presented in the context of the OPERA project.","['cloud computing', 'power efficiency', 'workload management', 'microservices', 'heterogeneous data center', 'Servers', 'Cloud computing', 'Hardware', 'Computer architecture', 'Power demand', 'Programming', 'Cooling']",,,,0,EC5,0,92
74,JOUR,10.1109/ISCC61673.2024.10733667,"['J. Gómez-delaHiz', 'A. García-López', 'S. García-Gil', 'D. Ramos-Ramos', 'A. Fakhreddine', 'J. M. Murillo', 'J. Galán-Jiménez']",Throughput-Energy Efficiency Trade-off in Microservices-Based UAV Networks,2024 IEEE Symposium on Computers and Communications (ISCC),31-Oct-24,IEEE,"Rural areas broadband access suffers from the limited investment of network operators, due to the forecasted return on investment. As a consequence, digital services such as eHealth, remote education, or smart agriculture cannot be offered to the rural population. In this context, research on Unmanned Aerial Vehicles (UAV) networks has emerged, which aims to solve the coverage problem by relying on small cells mounted on UAVs to provide coverage. From the network Quality of Service (QoS) point of view, i.e., the performance offered to users according to certain parameters such as delay, reliability, and throughput provisioning can be identified as one of the weak points. For the problem of maximizing throughput, the main solution is to group several UAVs in the same area. However, as the offered throughput increases, the power consumption will also increase. In this context, this paper proposes a genetic algorithm to solve the problem of jointly maximizing the offered throughput in rural scenarios where users request microservice-based IoT applications while minimizing the energy consumption of the swarm of UAVs. The algorithm is defined and evaluated in realistic scenarios, demonstrating its effectiveness on increasing the throughput while decreasing the number of UAVs that are required.","['UAV', 'Throughput', 'Energy efficiency', 'Genetic algorithm', 'Rural areas', 'MSA', 'Smart agriculture', 'Energy consumption', 'Microservice architectures', 'Quality of service', 'Throughput', 'Autonomous aerial vehicles', 'Telecommunications', 'Telecommunication network reliability', 'Investment', 'Genetic algorithms']",2642-7389,,,0,EC5,0,93
106,JOUR,10.1016/j.jss.2024.112183,"['Meijer W.', 'Trubiani C.', 'Aleti A.']",Experimental evaluation of architectural software performance design patterns in microservices,Journal of Systems and Software,,,"Microservice architectures and design patterns enhance the development of large-scale applications by promoting flexibility. Industrial practitioners perceive the importance of applying architectural patterns but they struggle to quantify their impact on system quality requirements. Our research aims to quantify the effect of design patterns on system performance metrics, e.g., service latency and resource utilization, even more so when the patterns operate in real-world environments subject to heterogeneous workloads. We built a cloud infrastructure to host a well-established benchmark system that represents our test bed, complemented by the implementation of three design patterns: Gateway Aggregation, Gateway Offloading, Pipe and Filters. Real performance measurements are collected and compared with model-based predictions that we derived as part of our previous research, thus further consolidating the actual impact of these patterns. Our results demonstrate that, despite the difficulty to parameterize our benchmark system, model-based predictions are in line with real experimentation, since the performance behaviors of patterns, e.g., bottleneck switches, are mostly preserved. In summary, this is the first work that experimentally demonstrates the performance behavior of microservices-based architectural patterns. Results highlight the complexity of evaluating the performance of design patterns and emphasize the need for complementing theoretical models with empirical data. © 2024 The Authors","['Design patterns', 'Microservice architectures', 'Performance evaluation', 'Application programs', 'Architectural design', 'Software design', 'Testbeds', 'Architectural pattern', 'Benchmark system', 'Design Patterns', 'Experimental evaluation', 'Microservice architecture', 'Model-based prediction', 'Performance', 'Performances design', 'Performances evaluation', 'Software performance', 'Gateways (computer networks)']",,2024,,0,EC5,0,94
107,JOUR,10.1109/JLT.2021.3063325,"['O. O. Ajibola', 'T. E. H. El-Gorashi', 'J. M. H. Elmirghani']",Energy Efficient Placement of Workloads in Composable Data Center Networks,Journal of Lightwave Technology,03-Mar-21,IEEE,"This paper studies the energy efficiency of composable data center (DC) infrastructures over network topologies. Using a mixed integer linear programming (MILP) model, we compare the performance of disaggregation at rack-scale and pod-scale over selected electrical, optical and hybrid network topologies relative to a traditional DC. Relative to a pod-scale DC, the results show that physical disaggregation at rack-scale is sufficient for optimal efficiency when the optical network topology is adopted, and resource components are allocated in a suitable manner. The optical network topology also enables optimal energy efficiency in composable DCs. The paper also studies logical disaggregation of traditional DC servers over an optical network topology. Relative to physical disaggregation at rack-scale, logical disaggregation of server resources within each rack enables marginal fall in the total DC power consumption (TDPC) due to improved resource demands placement. Hence, an adaptable composable infrastructure that can support both in memory (access) latency sensitive and insensitive workloads is enabled. We also conduct a study of the adoption of micro-service architecture in both traditional and composable DCs. Our results show that increasing the modularity of workloads improves the energy efficiency in traditional DCs, but disproportionate utilization of DC resources persists. A combination of disaggregation and micro-services achieved up to 23% reduction in the TDPC of the traditional DC by enabling optimal resources utilization and energy efficiencies. Finally, we propose a heuristic for energy efficient placement of workloads in composable DCs which replicates the trends produced by the MILP model formulated in this paper.","['Composable infrastructures', 'energy efficient data centers', 'micro-services', 'MILP', 'optical networks', 'rack-scale data center', 'software defined infrastructures', 'Servers', 'Software', 'Network topology', 'Energy efficiency', 'Hardware', 'Topology', 'Optical fiber networks']",1558-2213,,,0,EC5,0,95
69,JOUR,10.1109/CASCON62161.2024.10837926,"['M. Bachras', 'S. Tijanic', 'S. Zhang', 'Y. Zhang', 'B. Yan', 'L. W. Piu', 'H. -A. Jacobsen']","Next-Generation Cloud Databases: Balancing Performance, Sustainability, and Resource Management",2024 34th International Conference on Collaborative Advances in Software and COmputiNg (CASCON),17-Jan-25,IEEE,"This study enhances cloud database technologies for cyber-physical systems and IoT applications, addressing four key challenges: memory constraints, environmental impact, query performance, and serverless resource management. We introduce REMON, a network-based solution extending server memory capacity, and Krysha, a framework optimizing serverless microservices. Our research also includes an environmental impact assessment of analytical databases and explores hardware-conscious query tuning. Through extensive experiments, we demonstrate significant improvements in efficiency, scalability, and sustainability of cloud database systems. These advancements offer valuable solutions for managing large-scale data in complex digital ecosystems, with implications for both performance optimization and environmental considerations in cloud computing.","['cloud computing', 'resource allocation', 'databases', 'query tuning', 'sustainability', 'Cloud computing', 'Scalability', 'Microservice architectures', 'Software', 'Resource management', 'Servers', 'Sustainable development', 'Tuning', 'Optimization', 'Next generation networking']",,,,0,EC5,0,96
101,JOUR,10.1007/978-3-031-44836-2_3,"['Nam, T. B.', 'Khiem, H. G.', 'Triet, M. N.', 'Hong, K. V.', 'Khoa, T. D.', 'Bao, Q. T.', 'Phuc, N. T.', 'Hieu, M. D.', 'Loc, V. C. P.', 'Quy, T. L.', 'Anh, N. T.', 'Hien, Q. N.', 'Bang, L. K.', 'Trong, D. P. N.', 'Ngan, N. T. K.', 'Son, H.', 'Luong, H. H.']",SPaMeR: Securing Patient Medical Records in&nbsp;the&nbsp;Cloud - A Microservice and&nbsp;Brokerless Architecture Approach,"Web Services – ICWS 2023: 30th International Conference, Held as Part of the Services Conference Federation, SCF 2023, Honolulu, HI, USA, September 23–26, 2023, Proceedings",2023,Springer-Verlag,"The expansion of Internet of Things (IoT) technologies has revolutionized various sectors, one of the most critical being healthcare. The effective management of Patient Medical Records (PMRs) is an area where IoT plays a significant role, and its integration with Cloud Computing offers an enormous opportunity to enhance data accessibility, efficiency, and cost-effectiveness. However, the challenge of securing PMRs in the cloud remains a key concern. This paper introduces SPaMeR, an innovative IoT platform based on microservice and brokerless architecture, tailored to address this challenge and the specific requirements of healthcare environments. SPaMeR platform incorporates and extends the core functionalities of the IoT platform designed in our previous work - data collection, device and user management, and remote device control - while specifically addressing six critical issues for healthcare data: a) secure and reliable transmission of medical data, b) energy efficiency for healthcare devices, c) high-speed and accurate data collection from medical devices, d) robust security mechanisms to protect sensitive patient information, e) scalability to accommodate the ever-growing number of patients and medical devices, and f) compliance with healthcare data regulations and standards. To demonstrate the effectiveness and feasibility of SPaMeR, we provide a comprehensive evaluation with two distinct healthcare scenarios. Our results indicate significant improvements in the areas of data security, energy efficiency, and system scalability compared to traditional healthcare platforms.","['microservice', 'micro-service', 'Internet of Things', 'Kafka', 'brokerless', 'gRPC', 'Medical record', 'RBAC', 'Single Sign-On']",978-3-031-44835-5,,['https://doi.org/10.1007/978-3-031-44836-2_3'],0,EC5,0,97
96,JOUR,10.1109/ACCESS.2024.3489892,"['S. D. Meglio', 'L. Libero Lucio Starace']",Evaluating Performance and Resource Consumption of REST Frameworks and Execution Environments: Insights and Guidelines for Developers and Companies,IEEE Access,01-Nov-24,IEEE,"The REST (REpresentational State Transfer) paradigm has become essential for designing distributed applications that leverage the HTTP protocol, enabling efficient data exchange and the development of scalable architectures such as microservices. However, selecting an appropriate framework among the myriad available options, especially given the diversity of emerging execution environments, presents a significant challenge. Often, this decision neglects crucial factors such as performance and energy efficiency, favoring instead developer familiarity and popularity within the industry. To address this, we conducted a comprehensive benchmark study using a prototype REST API application provided by an industry partner, which was implemented multiple times using different REST API frameworks. We evaluated five different REST API frameworks across three popular programming languages, incorporating both traditional and emerging execution environments, resulting in twelve distinct configurations. Our results reveal significant differences in performance and computational resource consumption across different frameworks and execution environments, highlighting the necessity of making informed technology choices based on thorough analysis rather than convenience or familiarity. In addition to our findings, we offer other contributions to the field: an automated pipeline that benchmarks different configurations with various frameworks and execution environments, and a reference benchmark REST API that can be used in other studies. This research provides valuable insights and tools for developers and organizations aiming to select high-performance, resource-efficient technologies that promote environmental sustainability and reduce operational costs.","['Execution environments', 'green computing', 'performance benchmark', 'performance testing', 'REST API', 'REST API frameworks', 'sustainable computing', 'web applications', 'Java', 'Benchmark testing', 'Python', 'Energy consumption', 'Energy efficiency', 'Sports', 'Software development management', 'Prototypes', 'Costs', 'Codes', 'Green computing', 'Green computing', 'Web services']",2169-3536,,,0,EC5,0,98
105,JOUR,10.1007/s00607-024-01305-x,"['Fé I.', 'Nguyen T.A.', 'Mauro M.D.', 'Postiglione F.', 'Ramos A.', 'Soares A.', 'Choi E.', 'Min D.', 'Lee J.W.', 'Silva F.A.']",Energy-aware dynamic response and efficient consolidation strategies for disaster survivability of cloud microservices architecture,Computing,,,"Computer system resilience refers to the ability of a computer system to continue functioning even in the face of unexpected events or disruptions. These disruptions can be caused by a variety of factors, such as hardware failures, software glitches, cyber attacks, or even natural disasters. Modern computational environments need applications that can recover quickly from major disruptions while also being environmentally sustainable. Balancing system resilience with energy efficiency is challenging, as efforts to improve one can harm the other. This paper presents a method to enhance disaster survivability in microservice architectures, particularly those using Kubernetes in cloud-based environments, focusing on optimizing electrical energy use. Aiming to save energy, our work adopt the consolidation strategy that means grouping multiple microservices on a single host. Our aproach uses a widely adopted analytical model, the Generalized Stochastic Petri Net (GSPN). GSPN are a powerful modeling technique that is widely used in various fields, including engineering, computer science, and operations research. One of the primary advantages of GSPN is its ability to model complex systems with a high degree of accuracy. Additionally, GSPN allows for the modeling of both logical and stochastic behavior, making it ideal for systems that involve a combination of both. Our GSPN models compute a number of metrics such as: recovery time, system availability, reliability, Mean Time to Failure, and the configuration of cloud-based microservices. We compared our approach against others focusing on survivability or efficiency. Our approach aligns with Recovery Time Objectives during sudden disasters and offers the fastest recovery, requiring 9% less warning time to fully recover in cases of disaster with alert when compared to strategies with similar electrical consumption. It also saves about 27% energy compared to low consolidation strategies and 5% against high consolidation under static conditions. © The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature 2024.","['68M14', '68M15', '68U01', 'Availability', 'Microservices', 'Petri nets', 'Reliability', 'Survivabiliry', 'Balancing', 'Energy efficiency', 'Network security', 'Petri nets', 'Power management', 'Recovery', 'Stochastic models', 'Stochastic systems', '68m14', '68m15', '68u01', 'Cloud-based', 'Energy aware', 'Generalized Stochastic Petri nets', 'Microservice', 'Survivabiliry', 'System resiliences', 'Unexpected events', 'Disasters']",,2024,,0,EC5,0,99
8,JOUR,10.3390/su16135738,"['Soongpol B.', 'Netinant P.', 'Rukhiran M.']",Practical Sustainable Software Development in Architectural Flexibility for Energy Efficiency Using the Extended Agile Framework,Sustainability (Switzerland),,,"Many regular business operations are transforming into digital services, increasing advanced multi-platforms, rapid operational alignment, flexibility, and environmental impact through energy consumption, hardware waste, and technology investments. Flexible and sustainable system development models emphasizing energy efficiency can help innovate software development as digital servicing applications shift. This research is motivated by the need to improve energy consumption in early software design and development due to rising technological efficiency and sustainability demands. Although effective in iterative development and stakeholder engagement, traditional Agile methodologies often struggle with long-term sustainability and energy efficiency. Extended Agile, combining Agile, layered architecture, and aspect-oriented frameworks (ALAI), promises to improve system modularity, flexibility, maintainability, and sustainability. This study’s findings are not just theoretical, but also practically relevant, as they explore the energy efficiency of ALAI software development methodologies, using graduate admission information system services (GAISS) as an example. GAISS is a complex system that handles the entire process of graduate admissions, from application submission to final decision. The study quantifies the energy usage of a student-list webpage by analyzing Microsoft IIS server logs from February 2022 to May 2024. Directly applicable findings show that the GAISS based on the ALAI framework reduces energy consumption by 10.7914% compared to traditional Agile software developments. ALAI used 892.80 kWh versus Agile’s 1000.80 kWh during operations, saving energy. These findings demonstrate the benefits of integrating aspect-oriented frameworks and layering approaches into Agile methodologies, contributing to sustainable software development discourse. The study emphasizes the importance of energy-efficient frameworks such as ALAI to reduce software systems’ environmental impact and promote software development sustainability. The findings of this study, with their practical relevance, assist software developers and organizations in choosing software design and development methods that maximize operational efficiency and environmental sustainability. © 2024 by the authors.","['agile', 'aspect-oriented', 'energy efficiency', 'flexibility', 'framework', 'layering', 'software development', 'sustainability', 'business development', 'energy efficiency', 'environmental impact', 'information system', 'software', 'stakeholder', 'sustainability', 'sustainable development']",,2024,,0,EC5,0,100
86,JOUR,10.1109/MICRO56248.2022.00040,"['Khairy, Mahmoud', 'Alawneh, Ahmad', 'Barnes, Aaron', 'Rogers, Timothy G.']",SIMR: Single Instruction Multiple Request Processing for Energy-Efficient Data Center Microservices,Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture,2023,IEEE Press,"Contemporary data center servers process thousands of similar, independent requests per minute. In the interest of programmer productivity and ease of scaling, workloads in data centers have shifted from single monolithic processes toward a micro and nanoservice software architecture. As a result, single servers are now packed with many threads executing the same, relatively small task on different data.State-of-the-art data centers run these microservices on multi-core CPUs. However, the flexibility offered by traditional CPUs comes at an energy-efficiency cost. The Multiple Instruction Multiple Data execution model misses opportunities to aggregate the similarity in contemporary microservices. We observe that the Single Instruction Multiple Thread execution model, employed by GPUs, provides better thread scaling and has the potential to reduce frontend and memory system energy consumption. However, contemporary GPUs are ill-suited for the latency-sensitive microservice space.To exploit the similarity in contemporary microservices, while maintaining acceptable latency, we propose the Request Processing Unit (RPU). The RPU combines elements of out-of-order CPUs with lockstep thread aggregation mechanisms found in GPUs to execute microservices in a Single Instruction Multiple Request (SIMR) fashion. To complement the RPU, we also propose a SIMR-aware software stack that uses novel mechanisms to batch requests based on their predicted control-flow, split batches based on predicted latency divergence and map per-request memory allocations to maximize coalescing opportunities. Our resulting RPU system processes 5.7× more requests/joule than multi-core CPUs, while increasing single thread latency by only 1.44×.","['data center', 'GPU', 'microservices', 'SIMT']",978-1-6654-6272-3,,['https://doi.org/10.1109/MICRO56248.2022.00040'],0,EC5,0,101
72,JOUR,10.1109/ISCC55528.2022.9912882,"['J. Georgiou', 'M. Symeonides', 'M. Kasioulis', 'D. Trihinas', 'G. Pallis', 'M. D. Dikaiakos']",BenchPilot: Repeatable & Reproducible Benchmarking for Edge Micro-DCs,2022 IEEE Symposium on Computers and Communications (ISCC),19-Oct-22,IEEE,"Micro-Datacenters (DCs) are emerging as key en-ablers for Edge computing and 5G mobile networks by pro-viding processing power closer to IoT devices to extract timely analytic insights. However, the performance evaluation of data stream processing on micro-DCs is a daunting task due to difficulties raised by the time-consuming setup, configuration and heterogeneity of the underlying environment. To address these challenges, we introduce BenchPilot, a modular and highly customizable benchmarking framework for edge micro-DCs. BenchPilot provides a high-level declarative model for describing experiment testbeds and scenarios that automates the bench-marking process on Streaming Distributed Processing Engines (SDPEs). The latter enables users to focus on performance analysis instead of dealing with the complex and time-consuming setup. BenchPilot instantiates the underlying cluster, performs repeatable experimentation, and provides a unified monitoring stack in heterogeneous Micro-DCs. To highlight the usability of BenchPilot, we conduct experiments on two popular streaming engines, namely Apache Storm and Flink. Our experiments compare the engines based on performance, CPU utilization, energy consumption, temperature, and network I/O.","['Edge Computing', 'Internet of Things', 'Energy consumption', 'Storms', 'Telecommunication traffic', 'Benchmark testing', 'Central Processing Unit', 'Servers', 'Usability']",2642-7389,,,0,EC5,0,102
5,JOUR,10.1109/WETICE64632.2024.00024,"['J. A. Larracoechea', 'S. Ilarri', 'P. Roose']",RADIANCE: A CASE Tool For Green Software Design,2024 32nd International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE),14-Feb-25,IEEE,"Regardless of the improvements in the efficiency of energy consumption of information and communication technology, energy consumption will forever be a requisite for software execution. Consequently, researchers have promoted the development of green and sustainable software with new development methods and tools. These, however, have been adopted with limited success due to technicalities and specific language/platform requirements. In this paper, we present RADIANCE: a web app for designing greener software with a model-driven approach based on the Behavior-Based Consumption Profiles (BBCP) external Domain-Specific Language (DSL). RADIANCE, in contrast to other tools, embraces users with different levels of knowledge about green software and software architecture. Moreover, RADIANCE assesses and rates, reports, and provides advice on the energy-consuming patterns of the software models created by the user, assisting them in identifying possible design changes that result in greener software designs from the initial stages of software development.","['Software design', 'Green software', 'Sustainable software', 'Energy consumption', 'Software development', 'Model Driven Software Development', 'Energy consumption', 'Software design', 'Computer aided software engineering', 'Software architecture', 'Green products', 'Software', 'Information and communication technology', 'DSL', 'Software development management', 'Domain specific languages']",,,,0,EC5 - DOUBLE CHECK,0,103
6,JOUR,10.3390/app14177456,"['Larracoechea J.A.', 'Ilarri S.', 'Roose P.']",A Proposal of Behavior-Based Consumption Profiles for Green Software Design,Applied Sciences (Switzerland),,,"Despite the increase in the efficiency of energy consumption in information and communication technology, software execution and its constraints are responsible for how energy is consumed in hardware hosts. Consequently, researchers have promoted the development of sustainable software with new development methods and tools to lessen its hardware demands. However, the approaches developed so far lack cohesiveness along the stages of the software development life cycle (SDLC) and exist outside of a holistic method for green software development (GSD). In addition, there is a severe lack of approaches that target the analysis and design stages of the SDLC, leaving software architects and designers unsupported. In this article, we introduce our behavior-based consumption profile (BBCP) external Domain-Specific Language (DSL), aimed at assisting software architects and designers in modeling the behavior of software. The models generated with our external DSL contain multiple sets of properties that characterize features of the software’s behavior. In contrast to other modeling languages, our BBCP emphasizes how time and probability are involved in software execution and its evolution over time, helping its users to gather an expectation of software usage and hardware consumption from the initial stages of software development. To illustrate the feasibility and benefits of our proposal, we conclude with an analysis of the model of a software service created using the BBCP, which is simulated using Insight Maker to obtain an estimation of hardware consumption and later translated to energy consumption. © 2024 by the authors.","['behavior with software', 'green software', 'SOA', 'software behavior', 'software engineering', 'software profiling', 'Program translators', 'Software design', 'Behavior with software', 'Behavior-based', 'Energy-consumption', 'Green software', 'SOA', 'Software architects', 'Software behavior', 'Software development life-cycle', 'Software execution', 'Software profiling', 'Green development']",,2024,,0,EC5,0,104
102,JOUR,10.1109/JIOT.2021.3086043,"['A. H. H. Ngu', 'J. S. Eyitayo', 'G. Yang', 'C. Campbell', 'Q. Z. Sheng', 'J. Ni']",An IoT Edge Computing Framework Using Cordova Accessor Host,IEEE Internet of Things Journal,03-Jun-21,IEEE,"The Internet of Things (IoT) is a rapidly growing system of physical sensors and connected devices, enabling advanced information gathering, interpretation, and monitoring. The realization of a versatile IoT edge computing framework will accelerate seamless integration of the cyber-world with new physical IoT devices, and will fundamentally change and empower the way humans interact with the world. While there are many cloud-based IoT computing frameworks, they cannot support the needs of IoT applications that require local processing and guarantee of consumer’s privacy. This article presents experimentation with the opensource plug-and-play IoT middleware, called Cordova Accesor Host. We demonstrated that Cordova Accessor Host supports the essential ingredients of the composition and reusability of IoT services using the accessor as the basic building block and adopting an accessor-module-plugin design pattern. The portability is demonstrated by using the same accessor for collecting sensor data from radically different IoT devices such as, wearables (e.g., smartwatches) and microcontrollers (e.g., Arduino). Our energy profiling experiments show that IoT services deployed using the Cordova Accessor Host consume around 35% less battery power than the same IoT services deployed in the native Android operating system.","['Edge computing', 'open service platform', 'service middleware and platform', 'Internet of Things', 'Sensors', 'Cloud computing', 'Edge computing', 'Servers', 'Programming', 'Operating systems']",2327-4662,,,0,EC5,0,105
108,JOUR,10.1145/3528535.3565242,"['Zandberg, Koen', 'Baccelli, Emmanuel', 'Yuan, Shenghao', 'Besson, Frédéric', 'Talpin, Jean-Pierre']",Femto-containers: lightweight virtualization and fault isolation for small software functions on low-power IoT microcontrollers,Proceedings of the 23rd ACM/IFIP International Middleware Conference,2022,Association for Computing Machinery,"Low-power operating system runtimes used on IoT microcontrollers typically provide rudimentary APIs, basic connectivity and, sometimes, a (secure) firmware update mechanism. In contrast, on less constrained hardware, networked software has entered the age of serverless, microservices and agility. With a view to bridge this gap, in the paper we design Femto-Containers, a new middleware runtime which can be embedded on heterogeneous low-power IoT devices. Femto-Containers enable the secure deployment, execution and isolation of small virtual software functions on low-power IoT devices, over the network. We implement Femto-Containers, and provide integration in RIOT, a popular open source IoT operating system. We then evaluate the performance of our implementation, which was formally verified for fault-isolation, guaranteeing that RIOT is shielded from logic loaded and executed in a Femto-Container. Our experiments on various popular micro-controller architectures (Arm Cortex-M, ESP32 and RISC-V) show that Femto-Containers offer an attractive trade-off in terms of memory footprint overhead, energy consumption, and security.","['security', 'IoT', 'middleware', 'container', 'function-as-a-service', 'low-power', 'microcontroller', 'virtual machine']",978-1-4503-9340-9,,['https://doi.org/10.1145/3528535.3565242'],0,EC5,0,106
88,JOUR,10.1109/INTLEC.2018.8612307,"['D. H. Harryvan', 'R. Chamberlane', 'A. SCionti', 'G. Urlini', 'O. Terzo']",The Potential Influence of Workload Management Across Heterogeneous Server Systems on Datacenter Energy Use and Power Draw,2018 IEEE International Telecommunications Energy Conference (INTELEC),17-Jan-19,IEEE,"Energy efficiency is a key part of the European energy policies and 2020 climate targets. Project OPERA is working to create an energy aware workload manager for heterogeneous systems that will allow microservices to migrate between systems with differing instruction set architectures. The Energy savings potential of such technologies is enormous and is estimated at 47 TWh per year in Europe, 95% of the energy consumed by servers in Europe. The impact of such technologies on datacenter operations is profound. Significant and fast variations in power draw over time are expected, a fact that operators need to consider when retrofitting or designing new facilities.","['workload management', 'heterogeneous server systems', 'project OPERA', 'energy savings', 'Energy efficiency', 'Servers', 'Europe', 'Data centers', 'Electric potential', 'Energy measurement']",0275-0473,,,0,EC5,0,107
90,JOUR,10.1109/TCSET49122.2020.235378,"['A. Luntovskyy', 'B. Shubyn']",Highly-Distributed Systems Based on Micro-Services and their Construction Paradigms,"2020 IEEE 15th International Conference on Advanced Trends in Radioelectronics, Telecommunications and Computer Engineering (TCSET)",07-May-20,IEEE,"A definition for the HDS, as well as the demarcation to conventional distributed systems, were given. Typical architectures for HDS were discussed which affect increasing of QoS and of so-called QoE (Quality of Experience). The distinguishing features for HDS are clearly formulated. The advanced SWT (Software Technologies) approaches lead to use of young flexible service-oriented architectures like Micro-Services, which provide higher performance and small latencies, as well as better scalability, energy-efficiency and autarky.One possible option in the frame of HDS regarding security, privacy, authentication and compulsoriness of workflow steps, modules and service execution for such apps Blockchain and Smart Contracting are. The theoretical issues are proven via the represented examples and case studies.","['Highly-Distributed Systems', 'Agile Process Models', 'Quality of Experience', 'Service-Oriented Architectures', 'Micro-Services', 'DevOps', 'Scrum', 'Conway’s Law', 'Blockchain']",,,,0,EC5,0,108
73,JOUR,10.1145/3129790.3129818,"['Munoz, Daniel-Jesus', 'Pinto, Mónica', 'Fuentes, Lidia']",Green software development and research with the HADAS toolkit,Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings,2017,Association for Computing Machinery,"Energy is a critical resource, and designing a sustainable software architecture is a non-trivial task. Developers require energy metrics that support sustainable software architectures reflecting quality attributes such as security, reliability, performance, etc., identifying what are the concerns that impact more in the energy consumption. A variability model of different designs and implementations of an energy model should exist for this task, as well as a service that stores and compares the experimentation results of energy and time consumption of each concern, finding out what is the most eco-efficient solution. The experimental measurements are performed by energy experts and researchers that share the energy model and metrics in a collaborative repository. HADAS confronts these tasks modelling and reasoning with the variability of energy consuming concerns for different energy contexts, connecting HADAS variability model with its energy efficiency collaborative repository, establishing a Software Product Line (SPL) service. Our main goal is to help developers to perform sustainability analyses finding out the eco-friendliest architecture configurations. A HADAS toolkit prototype is implemented based on a Clafer model and Choco solver, and it has been tested with several case studies.","['energy efficiency', 'variability', 'optimisation', 'clafer', 'CVL', 'metrics', 'repository', 'software product line']",978-1-4503-5217-8,,['https://doi.org/10.1145/3129790.3129818'],0,EC5 - DOUBLE CHECK,0,109
115,JOUR,10.1109/HPCA56546.2023.10071127,"['Yuan Y.', 'Huang J.', 'Sun Y.', 'Wang T.', 'Nelson J.', 'Ports D.R.K.', 'Wang Y.', 'Wang R.', 'Tai C.', 'Kim N.S.']",Rambda: RDMA-driven Acceleration Framework for Memory-intensive μs-scale Datacenter Applications,Proceedings - International Symposium on High-Performance Computer Architecture,,,"Responding to the ""datacenter tax""and ""killer microseconds""problems for memory-intensive datacenter applications, diverse solutions including Smart NIC-based ones have been proposed. Nonetheless, they often suffer from high overhead of communications over network and/or PCIe links. To tackle the limitations of the current solutions, this paper proposes RAMBDA, a holistic network and architecture co-design solution that leverages current RDMA and emerging cache-coherent off-chip interconnect technologies. Specifically, RAMBDA consists of four hardware and software components: (1) unified abstraction of inter- and intra-machine communications synergistically managed by one-sided RDMA write and cache-coherent memory write; (2) efficient notification of requests to accelerators assisted by cache coherence; (3) cache-coherent accelerator architecture directly interacting with NIC; and (4) adaptive device-to-host data transfer for modern server memory systems comprising both DRAM and NVM exploiting state-of-the-art features in CPUs and PCIe. We prototype RAMBDA with a commercial system and evaluate three popular datacenter applications: (1) in-memory key-value store, (2) chain replication-based distributed transaction system, and (3) deep learning recommendation model inference. The evaluation shows that RAMBDA provides 30.1~69.1% lower latency, 0.2~2.5× throughput, and ~ 3× higher energy efficiency than the current state-of-the-art solutions, including Smart NIC. For those cases where Rambda performs poorly, we also envision future architecture to improve it. © 2023 IEEE.","['cache-coherent interconnects and accelerators', 'datacenters', 'heterogeneous and disaggregated memory', 'RDMA', 'Cache memory', 'Data transfer', 'Deep learning', 'Dynamic random access storage', 'Energy efficiency', 'Integrated circuit interconnects', 'Machine components', 'Memory architecture', 'Network architecture', ""'current"", 'Cache-coherent interconnect and accelerator', 'Co-designs', 'Datacenter', 'Design solutions', 'Dis-aggregated memory', 'Diverse solutions', 'Heterogeneous memory', 'RDMA', 'State of the art', 'Program processors']",,2023,,0,EC5,0,110
113,JOUR,10.1109/ISIE51358.2023.10228045,"['M. Corn', 'N. Rožman', 'P. Podržaj', 'T. Berlec', 'T. Požrl', 'R. Vrabič']",Concept of blockchain-based micro-service control strategy for a domestic water heater,2023 IEEE 32nd International Symposium on Industrial Electronics (ISIE),31-Aug-23,IEEE,"This paper proposes a concept of a blockchain-based micro-service control strategy for a domestic water heater. The novelty of the proposed concept is in the employment of blockchain technology, which enables decentralized access to service providers, thereby improving the security of the system. The concept aims to increase security while preserving energy consumption and costs reductions benefits associated with water heating by utilizing advanced control strategies and algorithms. The proposed system consists of a smart device, a service provider, data storage, and a smart contract. The smart device collects and stores data on hot water consumption, which is used to generate accurate predictions of hot water draw profiles. The service provider provides advanced control strategies designed to enhance the performance of the smart device. The data storage is decentralized to ensure data privacy and security. Payments between providers and consumers of the services are executed by employing smart contract deployed on a blockchain network. Testing of the proposed concept was conducted in a hybrid simulation environment. The results of the tests showed that even in decentralized system an improvement in energy consumption efficiency and a reduction in operating costs due to the advance control algorithms is possible without a decrease in users’ comfort.","['Blockchain technology', 'Micro-service control strategy', 'Smart water heaters', 'Energy consumption', 'Data privacy', 'Costs', 'Smart contracts', 'Water heating', 'Memory', 'Blockchains']",2163-5145,,,0,EC5,0,111
93,JOUR,10.1145/3575693.3575710,"['Switzer, Jennifer', 'Marcano, Gabriel', 'Kastner, Ryan', 'Pannuto, Pat']",Junkyard Computing: Repurposing Discarded Smartphones to Minimize Carbon,"Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2",2023,Association for Computing Machinery,"1.5 billion smartphones are sold annually, and most are decommissioned less than two years later. Most of these unwanted smartphones are neither discarded nor recycled but languish in junk drawers and storage units. This computational stockpile represents a substantial wasted potential: modern smartphones have increasingly high-performance and energy-efficient processors, extensive networking capabilities, and a reliable built-in power supply. This project studies the ability to reuse smartphones as ""junkyard computers."" Junkyard computers grow global computing capacity by extending device lifetimes, which supplants the manufacture of new devices. We show that the capabilities of even decade-old smartphones are within those demanded by modern cloud microservices and discuss how to combine phones to perform increasingly complex tasks. We describe how current operation-focused metrics do not capture the actual carbon costs of compute. We propose Computational Carbon Intensity—a performance metric that balances the continued service of older devices with the superlinear runtime improvements of newer machines. We use this metric to redefine device service lifetime in terms of carbon efficiency. We develop a cloudlet of reused Pixel 3A phones. We analyze the carbon benefits of deploying large, end-to-end microservice-based applications on these smartphones. Finally, we describe system architectures and associated challenges to scale to cloudlets with hundreds and thousands of smartphones.","['cloud computing', 'sustainability', 'life cycle assessment']",978-1-4503-9916-6,,['https://doi.org/10.1145/3575693.3575710'],0,EC5,0,112
111,JOUR,10.1016/j.enconman.2024.118270,"['Taheri S.', 'Amiri A.J.', 'Razban A.']",Real-world implementation of a cloud-based MPC for HVAC control in educational buildings,Energy Conversion and Management,,,"In their quest to enhance energy efficiency and carbon footprint management, many enterprises are turning their attention towards optimizing their facilities through amplified data monitoring and innovative control methods. In this context, this research highlights a practical solution in a commercial building located in the United States. It explains the creation, implementation, and costs of a universally applicable model predictive control (MPC) framework for the building's heating, ventilation, and air conditioning (HVAC) system. The paper elaborates on both the hardware and software used to accumulate pertinent data and the formulation of the MPC system. This approach leverages cloud-based microservices and can be seamlessly integrated into current building management systems. In this regard, this paper presents three innovative strategies: Proportional–integral (PI)+Preheating, MPC, and occupancy-based control. These scenarios were designed to improve energy efficiency and thermal comfort significantly by reducing temperature fluctuations and adhering more closely to the setpoint temperature. The implementation of these strategies resulted in substantial energy savings, with reductions in energy consumption of 12.83%, 19.21%, and 14.98%, respectively. © 2024 Elsevier Ltd","['Building management systems', 'Cloud-based solutions', 'Educational Buildings', 'HVAC', 'Carbon footprint', 'Energy efficiency', 'Energy utilization', 'HVAC', 'Office buildings', 'Predictive control systems', 'Air conditioning controls', 'Building management system', 'Cloud-based', 'Cloud-based solution', 'Data monitoring', 'Educational buildings', 'Enterprise IS', 'Heating ventilation and air conditioning', 'Model-predictive control', 'Real-world implementation', 'Model predictive control']",,2024,,0,EC5,0,113
79,JOUR,10.1109/MICRO50266.2020.00074,"['A. Mirhosseini', 'H. Golestani', 'T. F. Wenisch']",HyperPlane: A Scalable Low-Latency Notification Accelerator for Software Data Planes,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),11-Nov-20,IEEE,"I/O software stacks have evolved rapidly due to the growing speed of I/O devices-including network adapters, storage devices, and accelerators-and the emergence of microservice-based programming models. Datacenters rely on fast, efficient Software Data Planes (SDPs), which orchestrate data transfer between applications and I/O devices. Modern data planes are user-level software stacks, wherein cores spin-poll a large number of queues to avoid the attendant overheads of kernel-based I/O. Cores often poll empty queues before finding work in non-empty ones. Interrogating empty queues hurts peak throughput, tail latency, and energy efficiency as it often entails fruitless cache misses. In this work, we propose HyperPlane, an efficient accelerator for the notification mechanism of SDPs. The key features of HyperPlane are (1) avoiding iteration over empty I/O queues, unlike software-only designs, resulting in queue scalability, (2) halting execution when I/O queues are idle, leading to work proportionality and energy efficiency, and (3) efficiently sharing queues across cores to enjoy strong theoretical properties of scale-up queuing. HyperPlane is realized through a hardware subsystem associated with a familiar programming model. HyperPlane's microarchitecture consists of a monitoring set that watches for work arrival from I/O, and a ready set, which tracks ready queues and distributes work to cores based on various service policies and priority levels. We show that HyperPlane improves peak throughput by 4.1× and tail latency by 16.4× compared to a state-of-the-art SDP.","['software data plane', 'acceleration', 'spin-polling', 'notification', 'Microarchitecture', 'Scalability', 'Programming', 'Throughput', 'Software', 'Queueing analysis', 'Monitoring']",,,,0,EC5,0,114
98,JOUR,10.1109/LATINCOM.2017.8240185,"['C. L. Chamas', 'D. Cordeiro', 'M. M. Eler']","Comparing REST, SOAP, Socket and gRPC in computation offloading of mobile applications: An energy cost analysis",2017 IEEE 9th Latin-American Conference on Communications (LATINCOM),28-Dec-17,IEEE,"There has been a high concern regarding the energy saving on mobile devices recently, for mobile devices have been performing increasingly complex tasks over time. The computation offloading is one of the most popular techniques used by developers as an effective way of saving energy on mobile devices, which consists on executing complex tasks in external servers with different purposes including save energy. Deciding towards offloading certain tasks requires to understand the influence of the amount of data, amount of computation, and the network profile. Several studies have investigated the influence of different wireless flavours, such as 3G, 4G and wifi, but no study has investigated the influence of the communication choices on the energy cost. Therefore, in this paper, we present an experiment we conducted to evaluate the energy consumption of different communication protocols and architectural styles, namely REST, SOAP, Socket and gRPC, when executing algorithms of different complexities and different input sizes and types. Results show that local execution is more economic with less complex algorithms and small input data. When it comes to remote execution, REST is the most economic choice followed by Socket. Moreover, our data show that computation offloading can save up to 10 time as much energy when compared to local execution for some executions configurations.","['Batteries', 'Energy consumption', 'Sorting', 'Mobile handsets', 'Sockets', 'Servers']",,,,0,EC5,0,115
67,JOUR,10.1109/ACCESS.2018.2825295,"['A. E. Khaled', 'A. Helal', 'W. Lindquist', 'C. Lee']",IoT-DDL–Device Description Language for the “T” in IoT,IEEE Access,10-Apr-18,IEEE,"We argue that the success of the Internet of Things (IoT) vision will greatly depend on how its main ingredient-the “thing”-is architected and prepared to engage. The IoT's fragmented and widevarying nature introduces the need for additional effort to homogenize these things so they may blend together with the surrounding space to create opportunities for powerful and unprecedented IoT applications. We introduce the IoT Device Description Language (IoT-DDL), a machineand human-readable descriptive language for things, seeking to achieve such integration and homogenization. IoT-DDL explicitly tools things to self-discover and securely share their own capabilities, entities, and services, including the various cloudbased accessories that may be attached to them. We also present the Atlas thing architecture-a lightweight architecture for things that fully exploits IoT-DDL and its specifications. Our architecture provides new OS layers, services, and capabilities we believe a thing must have in order to be prepared to engage in IoT scenarios and applications. The architecture and IoT-DDL enable things to generate their offered services and self-formulate APIs for such services, on the fly, at power-on or whenever a thing description changes. The architecture takes advantage of widely used device management, micro-services, security, and communication standards and protocols. We present details of IoT-DDL and corresponding parts of the thing architecture. We demonstrate some features of IoT-DDL and the architecture through proof-of-concept implementations. Finally, we present a benchmarking study to measure and assess time performance and energy consumption characteristics of our architecture and IoT-DDL on real hardware platforms.","['Internet of Things architecture', 'thing description', 'microservices', 'OMA', 'IPSO', 'CoAP', 'MQTT', 'Computer architecture', 'Cloud computing', 'Internet of Things', 'Tools', 'Ecosystems', 'Metadata']",2169-3536,,,0,EC5,0,116
100,JOUR,10.1007/978-3-030-26633-2_5,"['Stan O.', 'Zayani M.-H.', 'Sirdey R.', 'Ben Hamida A.', 'Mziou-Sallami M.', 'Ferreira Leite A.']",A SaaS implementation of a new generic crypto-classifier service for secure energy efficiency in smart cities,Communications in Computer and Information Science,,,"More and more, accessing data remotely in a secure manner appears like the backbone for providing new advanced and customized functionalities. The proliferation of social media and domotics urges a new kind of ubiquity where different kinds of features are deployed everywhere atop of the everyday life small devices and even embedded in our surroundings. While we measure the opportunities offered by such advancements in improving people lifestyles and obtaining immediate and useful services, it is necessary to consider the real impact on our privacy. Recently, a newly adopted European standard, the General Data Protection Regulation aims at codifying the rules of personal data use. We strongly believe in the necessity for a Smart City of adopting such privacy regulation while benefiting at the same time from new services, in a generic and easy way. In that sense, we build an approach that is at the frontier of the cryptoscience domain for a better privacy and the Micro Service-Oriented Architectures for an increased use by tiers. Indeed, our service is accessible in a Software-as-a-Service (SaaS) manner and uses encryption to ensure data privacy. The particularity of our system lies in the fact that the server performs a classification algorithm without any information about the sensitive data and without the capability to decrypt it. The underlying cryptographic technology used is homomorphic encryption, allowing to perform calculations directly on encrypted data. Our service is generic by design and could be applied over several metrics. We adopt an energy efficiency use case and propose a service for buildings energy diagnosis and classification towards renovations and/or reductions of the electric consumption. We showcase our prototype of crypto-classification service by involving different actors of a Smart City community. Finally, we assess our proposal thanks to a set of real data collected from an Irish residential district. Our SaaS crypto-classifier achieves acceptable performances in terms of security, execution times and memory requirements. © Springer Nature Switzerland AG 2019.","['Data privacy', 'Homomorphic encryption', 'Secure classification', 'Smart city', 'Classification (of information)', 'Computer aided diagnosis', 'Cryptography', 'Data privacy', 'Energy efficiency', 'Green computing', 'Information services', 'Intelligent systems', 'Intelligent vehicle highway systems', 'Service oriented architecture (SOA)', 'Smart city', 'Traffic control', 'Acceptable performance', 'Classification algorithm', 'Electric consumption', 'European Standards', 'General data protection regulations', 'Ho-momorphic encryptions', 'Memory requirements', 'Residential districts', 'Software as a service (SaaS)']",,2019,,0,EC6,0,117
