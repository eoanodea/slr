"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"On Multi-Access Edge Computing: A Survey of the Emerging 5G Network Edge Cloud Architecture and Orchestration","T. Taleb; K. Samdanis; B. Mada; H. Flinck; S. Dutta; D. Sabella","Aalto University, Espoo, Finland; Huawei European Research Center, Munich, Germany; Aalto University, Espoo, Finland; Nokia Bell Labs, Espoo, Finland; Aalto University, Espoo, Finland; Intel, Munich, Germany",IEEE Communications Surveys & Tutorials,"21 Aug 2017","2017","19","3","1657","1681","Multi-access edge computing (MEC) is an emerging ecosystem, which aims at converging telecommunication and IT services, providing a cloud computing platform at the edge of the radio access network. MEC offers storage and computational resources at the edge, reducing latency for mobile end users and utilizing more efficiently the mobile backhaul and core networks. This paper introduces a survey on MEC and focuses on the fundamental key enabling technologies. It elaborates MEC orchestration considering both individual services and a network of MEC platforms supporting mobility, bringing light into the different orchestration deployment options. In addition, this paper analyzes the MEC reference architecture and main deployment scenarios, which offer multitenancy support for application developers, content providers, and third parties. Finally, this paper overviews the current standardization activities and elaborates further on open research challenges.","1553-877X","","10.1109/COMST.2017.2705720","TAKE 5 Project; Finnish Funding Agency for Technology and Innovation; Finnish Ministry of Employment and the Economy; European Union’s Horizon 2020 Research and Innovation Programme through the EU/JP 5G!Pagoda Project(grant numbers:723172); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7931566","MEC;Multi-Access Edge Computing;Mobile Edge Computing;5G Mobile Communications;Edge Computing;Application Virtualization;Network Function Virtualization;Software Defined Networking;Network Softwarization","Cloud computing;Edge computing;Mobile computing;Computer architecture;5G mobile communication","","1361","","209","IEEE","18 May 2017","","","IEEE","IEEE Journals"
"Convergence of Edge Computing and Deep Learning: A Comprehensive Survey","X. Wang; Y. Han; V. C. M. Leung; D. Niyato; X. Yan; X. Chen","Tianjin Key Laboratory of Advanced Networking, College of Intelligence and Computing, Tianjin University, Tianjin, China; Tianjin Key Laboratory of Advanced Networking, College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; 2012 Lab, Huawei Technologies, Shenzhen, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China",IEEE Communications Surveys & Tutorials,"27 May 2020","2020","22","2","869","904","Ubiquitous sensors and smart devices from factories and communities are generating massive amounts of data, and ever-increasing computing power is driving the core of computation and services from the cloud to the edge of the network. As an important enabler broadly changing people's lives, from face recognition to ambitious smart factories and cities, developments of artificial intelligence (especially deep learning, DL) based applications and services are thriving. However, due to efficiency and latency issues, the current cloud computing service architecture hinders the vision of “providing artificial intelligence for every person and every organization at everywhere”. Thus, unleashing DL services using resources at the network edge near the data sources has emerged as a desirable solution. Therefore, edge intelligence, aiming to facilitate the deployment of DL services by edge computing, has received significant attention. In addition, DL, as the representative technique of artificial intelligence, can be integrated into edge computing frameworks to build intelligent edge for dynamic, adaptive edge maintenance and management. With regard to mutually beneficial edge intelligence and intelligent edge, this paper introduces and discusses: 1) the application scenarios of both; 2) the practical implementation methods and enabling technologies, namely DL training and inference in the customized edge computing framework; 3) challenges and future trends of more pervasive and fine-grained intelligence. We believe that by consolidating information scattered across the communication, networking, and DL areas, this survey can help readers to understand the connections between enabling technologies while promoting further discussions on the fusion of edge intelligence and intelligent edge, i.e., Edge DL.","1553-877X","","10.1109/COMST.2020.2970550","National Basic Research Program of China (973 Program)(grant numbers:2019YFB2101901,2018YFC0809803); National Science Foundation of China(grant numbers:61702364,61972432,U1711265); Program for Guangdong Introducing Innovative and Enterpreneurial Teams(grant numbers:2017ZT07X355); Chinese National Engineering Laboratory for Big Data System Computing Technology; Canadian Natural Sciences and Engineering Research Council; Singapore NRF National Satellite of Excellence, Design Science and Technology for Secure Critical Infrastructure National Satellite of Excellence(grant numbers:DeST-SCI2019-0007); A*STAR-NTU-SUTD Joint Research Grant Call on Artificial Intelligence for the Future of Manufacturing(grant numbers:RGANS1906); WASP/NTU(grant numbers:M4082187 (4080)); Singapore MOE Tier 1(grant numbers:2017-T1-002-007 RG122/17); MOE Tier 2(grant numbers:MOE2014-T2-2-015 ARC4/15); Singapore(grant numbers:NRF2015-NRF-ISF001-2277); Singapore Energy Market Authority Energy Resilience(grant numbers:NRF2017EWT-EP003-041); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8976180","Edge computing;deep learning;wireless communication;computation offloading;artificial intelligence","Edge computing;Cloud computing;Training;Computational modeling;Reliability;Wireless communication;Internet of Things","","910","","134","IEEE","30 Jan 2020","","","IEEE","IEEE Journals"
"6G Wireless Systems: Vision, Requirements, Challenges, Insights, and Opportunities","H. Tataria; M. Shafi; A. F. Molisch; M. Dohler; H. Sjöland; F. Tufvesson","Department of Electrical and Information Technology, Lund University, Lund, Sweden; Spark New Zealand, Wellington, New Zealand; Ming Hsieh Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA, USA; Center for Telecoms Research, King’s College London, London, U.K.; Department of Electrical and Information Technology, Lund University, Lund, Sweden; Department of Electrical and Information Technology, Lund University, Lund, Sweden",Proceedings of the IEEE,"18 Jun 2021","2021","109","7","1166","1199","Mobile communications have been undergoing a generational change every ten years or so. However, the time difference between the so-called “G’s” is also decreasing. While fifth-generation (5G) systems are becoming a commercial reality, there is already significant interest in systems beyond 5G, which we refer to as the sixth generation (6G) of wireless systems. In contrast to the already published papers on the topic, we take a top-down approach to 6G. More precisely, we present a holistic discussion of 6G systems beginning with lifestyle and societal changes driving the need for next-generation networks. This is followed by a discussion into the technical requirements needed to enable 6G applications, based on which we dissect key challenges and possibilities for practically realizable system solutions across all layers of the Open Systems Interconnection stack (i.e., from applications to the physical layer). Since many of the 6G applications will need access to an order-of-magnitude more spectrum, utilization of frequencies between 100 GHz and 1 THz becomes of paramount importance. As such, the 6G ecosystem will feature a diverse range of frequency bands, ranging from below 6 GHz up to 1 THz. We comprehensively characterize the limitations that must be overcome to realize working systems in these bands and provide a unique perspective on the physical and higher layer challenges relating to the design of next-generation core networks, new modulation and coding methods, novel multiple-access techniques, antenna arrays, wave propagation, radio frequency transceiver design, and real-time signal processing. We rigorously discuss the fundamental changes required in the core networks of the future, such as the redesign or significant reduction of the transport architecture that serves as a major source of latency for time-sensitive applications. This is in sharp contrast to the present hierarchical network architectures that are not suitable to realize many of the anticipated 6G services. While evaluating the strengths and weaknesses of key candidate 6G technologies, we differentiate what may be practically achievable over the next decade, relative to what is possible in theory. Keeping this in mind, we present concrete research challenges for each of the discussed system aspects, providing inspiration for what follows.","1558-2256","","10.1109/JPROC.2021.3061701","Ericsson AB, Sweden; ELLIIT: The Linköping-Lund Excellence Center on IT and Mobile Communication; National Science Foundation (NSF); National Institute of Standards and Technology (NIST); Samsung Research America; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9390169","Beamforming;next-generation core network;physical layer (PHY);radio frequency (RF) transceivers;signal processing;sixth-generation (6G);terahertz (THz);ultramassive multiple-input multiple-output (MIMO);waveforms","Array signal processing;Transceivers;Signal processing;6G mobile communication;Massive MIMO;Radio frequency;Physical layer;Next generation networking","","840","","194","CCBY","30 Mar 2021","","","IEEE","IEEE Journals"
"Network Slicing and Softwarization: A Survey on Principles, Enabling Technologies, and Solutions","I. Afolabi; T. Taleb; K. Samdanis; A. Ksentini; H. Flinck","Department of Communications and Networking, Aalto University, Espoo, Finland; Department of Computer and Information Security, Sejong University, Seoul, South Korea; Huawei European Research Center, Munich, Germany; Department of Communications System, Eurecom, Nice, France; Nokia Bell Labs, Espoo, Finland",IEEE Communications Surveys & Tutorials,"21 Aug 2018","2018","20","3","2429","2453","Network slicing has been identified as the backbone of the rapidly evolving 5G technology. However, as its consolidation and standardization progress, there are no literatures that comprehensively discuss its key principles, enablers, and research challenges. This paper elaborates network slicing from an end-to-end perspective detailing its historical heritage, principal concepts, enabling technologies and solutions as well as the current standardization efforts. In particular, it overviews the diverse use cases and network requirements of network slicing, the pre-slicing era, considering RAN sharing as well as the end-to-end orchestration and management, encompassing the radio access, transport network and the core network. This paper also provides details of specific slicing solutions for each part of the 5G system. Finally, this paper identifies a number of open research challenges and provides recommendations toward potential solutions.","1553-877X","","10.1109/COMST.2018.2815638","Academy of Finland(grant numbers:311654); European Union’s Horizon 2020 Research and Innovation Program through the 5G!Pagoda Project(grant numbers:723172); Global5G.org Project(grant numbers:761816); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8320765","Network slice;5G;network softwarization;orchestration;network management;NFV;SDN;cloud;mobile network;MANO;open source","5G mobile communication;Network slicing;Business;Cloud computing;Industries","","808","","169","IEEE","21 Mar 2018","","","IEEE","IEEE Journals"
"A Review on the Use of Blockchain for the Internet of Things","T. M. Fernández-Caramés; P. Fraga-Lamas","Department of Computer Engineering, Universidade da Coruña, A Coruña, Spain; Department of Computer Engineering, Universidade da Coruña, A Coruña, Spain",IEEE Access,"4 Jul 2018","2018","6","","32979","33001","The paradigm of Internet of Things (IoT) is paving the way for a world, where many of our daily objects will be interconnected and will interact with their environment in order to collect information and automate certain tasks. Such a vision requires, among other things, seamless authentication, data privacy, security, robustness against attacks, easy deployment, and self-maintenance. Such features can be brought by blockchain, a technology born with a cryptocurrency called Bitcoin. In this paper, a thorough review on how to adapt blockchain to the specific needs of IoT in order to develop Blockchain-based IoT (BIoT) applications is presented. After describing the basics of blockchain, the most relevant BIoT applications are described with the objective of emphasizing how blockchain can impact traditional cloud-centered IoT applications. Then, the current challenges and possible optimizations are detailed regarding many aspects that affect the design, development, and deployment of a BIoT application. Finally, some recommendations are enumerated with the aim of guiding future BIoT researchers and developers on some of the issues that will have to be tackled before deploying the next generation of BIoT applications.","2169-3536","","10.1109/ACCESS.2018.2842685","Xunta de Galicia(grant numbers:ED431C 2016-045,ED341D R2016/012,ED431G/01); Agencia Estatal de Investigación of Spain(grant numbers:TEC2015-69648-REDC,TEC2016-75067-C4-1-R); ERDF funds of EU, UE(grant numbers:AEI/FEDER); BBVA; BritishSpanish Society Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8370027","IoT;blockchain;traceability;consensus;distributed systems;BIoT;fog computing;edge computing","Peer-to-peer computing;Bitcoin;Internet of Things;Cloud computing;Computer architecture","","751","","209","OAPA","31 May 2018","","","IEEE","IEEE Journals"
"On the Road to 6G: Visions, Requirements, Key Technologies, and Testbeds","C. -X. Wang; X. You; X. Gao; X. Zhu; Z. Li; C. Zhang; H. Wang; Y. Huang; Y. Chen; H. Haas; J. S. Thompson; E. G. Larsson; M. D. Renzo; W. Tong; P. Zhu; X. Shen; H. V. Poor; L. Hanzo","National Mobile Communications Research Laboratory, School of Information Science and Engineering, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, School of Information Science and Engineering, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, School of Information Science and Engineering, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, School of Information Science and Engineering, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, School of Information Science and Engineering, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, School of Information Science and Engineering, Southeast University, Nanjing, China; Pervasive Communication Research Center, Purple Mountain Laboratories, Nanjing, China; National Mobile Communications Research Laboratory, School of Information Science and Engineering, Southeast University, Nanjing, China; Department of Engineering, University of Durham, Durham, U.K.; Department of Electronic and Electrical Engineering, LiFi Research and Development Center, University of Strathclyde, Glasgow, U.K.; Institute for Digital Communications, School of Engineering, University of Edinburgh, Edinburgh, U.K.; Department of Electrical Engineering (ISY), Linköping University, Linköping, Sweden; Université Paris-Saclay, CNRS, CentraleSupélec, Laboratoire des Signaux et Systèmes, Gif-sur-Yvette, France; Wireless Advanced System and Competency Centre, HUAWEI Technologies Canada Company Ltd., Ottawa, ON, Canada; Wireless Technology Lab, HUAWEI Technologies Canada Company Ltd., Ottawa, Canada; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada; Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ, USA; School of Electronics and Computer Science, University of Southampton, Southampton, U.K.",IEEE Communications Surveys & Tutorials,"22 May 2023","2023","25","2","905","974","Fifth generation (5G) mobile communication systems have entered the stage of commercial deployment, providing users with new services, improved user experiences as well as a host of novel opportunities to various industries. However, 5G still faces many challenges. To address these challenges, international industrial, academic, and standards organizations have commenced research on sixth generation (6G) wireless communication systems. A series of white papers and survey papers have been published, which aim to define 6G in terms of requirements, application scenarios, key technologies, etc. Although ITU-R has been working on the 6G vision and it is expected to reach a consensus on what 6G will be by mid-2023, the related global discussions are still wide open and the existing literature has identified numerous open issues. This paper first provides a comprehensive portrayal of the 6G vision, technical requirements, and application scenarios, covering the current common understanding of 6G. Then, a critical appraisal of the 6G network architecture and key technologies is presented. Furthermore, existing testbeds and advanced 6G verification platforms are detailed for the first time. In addition, future research directions and open challenges are identified to stimulate the on-going global debate. Finally, lessons learned to date concerning 6G networks are discussed.","1553-877X","","10.1109/COMST.2023.3249835","National Key Research and Development Program of China(grant numbers:2018YFB1801101); National Natural Science Foundation of China(grant numbers:61960206006,62122020); Key Technologies Research and Development Program of Jiangsu (Prospective and Key Technologies for Industry)(grant numbers:BE2022067,BE2022067-1,BE2022067-5); EU H2020 RISE TESTBED2 Project(grant numbers:872172); EU H2020 ARIADNE Project(grant numbers:871464); EU H2020 RISE-6G Project(grant numbers:101017011); U.S. National Science Foundation(grant numbers:CCF-1908308,CNS-2128448); Engineering and Physical Sciences Research Council Project(grant numbers:EP/W016605/1,EP/X01228X/1); European Research Council’s Advanced Fellow Grant QuantCom(grant numbers:789028); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10054381","6G vision;6G key performance indicators (KPIs);6G application scenarios;6G network architecture;6G key technologies;6G testbeds;6G challenges","6G mobile communication;5G mobile communication;Wireless communication;Industries;Tutorials;Millimeter wave communication;Research and development","","660","","662","IEEE","27 Feb 2023","","","IEEE","IEEE Journals"
"An Overview on Edge Computing Research","K. Cao; Y. Liu; G. Meng; Q. Sun","College of Information and Control Engineering, Shenyang Jianzhu University, Shenyang, China; College of Information and Control Engineering, Shenyang Jianzhu University, Shenyang, China; College of Information and Control Engineering, Shenyang Jianzhu University, Shenyang, China; College of Information and Control Engineering, Shenyang Jianzhu University, Shenyang, China",IEEE Access,"15 May 2020","2020","8","","85714","85728","With the rapid development of the Internet of Everything (IoE), the number of smart devices connected to the Internet is increasing, resulting in large-scale data, which has caused problems such as bandwidth load, slow response speed, poor security, and poor privacy in traditional cloud computing models. Traditional cloud computing is no longer sufficient to support the diverse needs of today's intelligent society for data processing, so edge computing technologies have emerged. It is a new computing paradigm for performing calculations at the edge of the network. Unlike cloud computing, it emphasizes closer to the user and closer to the source of the data. At the edge of the network, it is lightweight for local, small-scale data storage and processing. This article mainly reviews the related research and results of edge computing. First, it summarizes the concept of edge computing and compares it with cloud computing. Then summarize the architecture of edge computing, keyword technology, security and privacy protection, and finally summarize the applications of edge computing.","2169-3536","","10.1109/ACCESS.2020.2991734","National Natural Science Foundation of China(grant numbers:61602323); National Postdoctoral Foundation of China(grant numbers:2016M591455); Youth Seedling Foundation of Liaoning Province(grant numbers:lnqn201913); Natural Science Funds of Liaoning Province(grant numbers:2019MS264,20180550019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9083958","Edge computing;cloud computing;Internet of Things","Cloud computing;Edge computing;Real-time systems;Internet of Things;Bandwidth;Security;Data privacy","","606","","62","CCBY","1 May 2020","","","IEEE","IEEE Journals"
"Communications in the 6G Era","H. Viswanathan; P. E. Mogensen","Nokia Bell Labs, Murray Hill, USA; Nokia Bell Labs, Aalborg, Denmark",IEEE Access,"30 Mar 2020","2020","8","","57063","57074","The focus of wireless research is increasingly shifting toward 6G as 5G deployments get underway. At this juncture, it is essential to establish a vision of future communications to provide guidance for that research. In this paper, we attempt to paint a broad picture of communication needs and technologies in the timeframe of 6G. The future of connectivity is in the creation of digital twin worlds that are a true representation of the physical and biological worlds at every spatial and time instant, unifying our experience across these physical, biological and digital worlds. New themes are likely to emerge that will shape 6G system requirements and technologies, such as: (i) new man-machine interfaces created by a collection of multiple local devices acting in unison; (ii) ubiquitous universal computing distributed among multiple local devices and the cloud; (iii) multi-sensory data fusion to create multi-verse maps and new mixed-reality experiences; and (iv) precision sensing and actuation to control the physical world. With rapid advances in artificial intelligence, it has the potential to become the foundation for the 6G air interface and network, making data, compute and energy the new resources to be exploited for achieving superior performance. In addition, in this paper we discuss the other major technology transformations that are likely to define 6G: (i) cognitive spectrum sharing methods and new spectrum bands; (ii) the integration of localization and sensing capabilities into the system definition, (iii) the achievement of extreme performance requirements on latency and reliability; (iv) new network architecture paradigms involving sub-networks and RAN-Core convergence; and (v) new security and privacy schemes.","2169-3536","","10.1109/ACCESS.2020.2981745","Nokia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040431","6G;AI/ML driven air interface;network localization and sensing;cognitive spectrum sharing;sub-terahertz;RAN-Core convergence;subnetworks;security;privacy;network as a platform","6G mobile communication;5G mobile communication;Robot sensing systems;Biology;Digital twin;User interfaces","","529","","41","CCBY","18 Mar 2020","","","IEEE","IEEE Journals"
"Integrated Blockchain and Edge Computing Systems: A Survey, Some Research Issues and Challenges","R. Yang; F. R. Yu; P. Si; Z. Yang; Y. Zhang","Faculty of Information Technology, Beijing University of Technology, Beijing, China; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada; Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China",IEEE Communications Surveys & Tutorials,"31 May 2019","2019","21","2","1508","1532","Blockchain, as the underlying technology of crypto-currencies, has attracted significant attention. It has been adopted in numerous applications, such as smart grid and Internet-of-Things. However, there is a significant scalability barrier for blockchain, which limits its ability to support services with frequent transactions. On the other side, edge computing is introduced to extend the cloud resources and services to be distributed at the edge of the network, but currently faces challenges in its decentralized management and security. The integration of blockchain and edge computing into one system can enable reliable access and control of the network, storage, and computation distributed at the edges, hence providing a large scale of network servers, data storage, and validity computation near the end in a secure manner. Despite the prospect of integrated blockchain and edge computing systems, its scalability enhancement, self organization, functions integration, resource management, and new security issues remain to be addressed before widespread deployment. In this survey, we investigate some of the work that has been done to enable the integrated blockchain and edge computing system and discuss the research challenges. We identify several vital aspects of the integration of blockchain and edge computing: motivations, frameworks, enabling functionalities, and challenges. Finally, some broader perspectives are explored.","1553-877X","","10.1109/COMST.2019.2894727","Beijing University of Technology; Beijing Post-Doctoral Fund; Support Project of High-Level Teachers in Beijing Municipal Universities in the Period of 13th Five-Year Plan(grant numbers:067175315000); National Natural Science Foundation of China(grant numbers:61671029,61571021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8624417","Blockchain;edge computing;network;storage;computation","Blockchain;Edge computing;Bitcoin;Peer-to-peer computing;Scalability;Cloud computing","","513","","132","IEEE","23 Jan 2019","","","IEEE","IEEE Journals"
"Edge Computing in Industrial Internet of Things: Architecture, Advances and Challenges","T. Qiu; J. Chi; X. Zhou; Z. Ning; M. Atiquzzaman; D. O. Wu","School of Computer Science and Technology, College of Intelligence and Computing, Tianjin University, Tianjin, China; School of Computer Science and Technology, College of Intelligence and Computing, Tianjin University, Tianjin, China; School of Computer Science and Technology, College of Intelligence and Computing, Tianjin University, Tianjin, China; School of Software, Dalian University of Technology, Dalian, China; School of Computer Science, University of Oklahoma, Norman, OK, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA",IEEE Communications Surveys & Tutorials,"20 Nov 2020","2020","22","4","2462","2488","The Industrial Internet of Things (IIoT) is a crucial research field spawned by the Internet of Things (IoT). IIoT links all types of industrial equipment through the network; establishes data acquisition, exchange, and analysis systems; and optimizes processes and services, so as to reduce cost and enhance productivity. The introduction of edge computing in IIoT can significantly reduce the decision-making latency, save bandwidth resources, and to some extent, protect privacy. This paper outlines the research progress concerning edge computing in IIoT. First, the concepts of IIoT and edge computing are discussed, and subsequently, the research progress of edge computing is discussed and summarized in detail. Next, the future architecture from the perspective of edge computing in IIoT is proposed, and its technical progress in routing, task scheduling, data storage and analytics, security, and standardization is analyzed. Furthermore, we discuss the opportunities and challenges of edge computing in IIoT in terms of 5G-based edge communication, load balancing and data offloading, edge intelligence, as well as data sharing security. Finally, we introduce some typical application scenarios of edge computing in IIoT, such as prognostics and health management (PHM), smart grids, manufacturing coordination, intelligent connected vehicles (ICV), and smart logistics.","1553-877X","","10.1109/COMST.2020.3009103","National Key Research and Development Program of China(grant numbers:2019YFB1703601); National Natural Science Foundation of China(grant numbers:61672131,61971084,61702365); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139976","Industrial Internet of Things (IIoT);edge computing;reference architecture;advances and challenges;application scenarios","Edge computing;Cloud computing;Internet of Things;Computer architecture;Security;Delays","","482","","148","IEEE","14 Jul 2020","","","IEEE","IEEE Journals"
"Survey on 6G Frontiers: Trends, Applications, Requirements, Technologies and Future Research","C. D. Alwis; A. Kalla; Q. -V. Pham; P. Kumar; K. Dev; W. -J. Hwang; M. Liyanage","Department of Electrical and Electronic Engineering, University of Sri Jayewardenepura, Nugegoda, Sri Lanka; Centre for Wireless Communications, University of Oulu, Oulu, Finland; Korean Southeast Center for the 4th Industrial Revolution Leader Education, Pusan National University, Busan, Republic of Korea; Department of Computer Science, Swansea University, Swansea, U.K.; Nimbus Research Centre, Munster Technological University, Bishopstown, Cork, Ireland; Department of Biomedical Convergence Engineering, Pusan National University, Yangsan, South Korea; Centre for Wireless Communications, University of Oulu, Oulu, Finland",IEEE Open Journal of the Communications Society,"16 Apr 2021","2021","2","","836","886","Emerging applications such as Internet of Everything, Holographic Telepresence, collaborative robots, and space and deep-sea tourism are already highlighting the limitations of existing fifth-generation (5G) mobile networks. These limitations are in terms of data-rate, latency, reliability, availability, processing, connection density and global coverage, spanning over ground, underwater and space. The sixth-generation (6G) of mobile networks are expected to burgeon in the coming decade to address these limitations. The development of 6G vision, applications, technologies and standards has already become a popular research theme in academia and the industry. In this paper, we provide a comprehensive survey of the current developments towards 6G. We highlight the societal and technological trends that initiate the drive towards 6G. Emerging applications to realize the demands raised by 6G driving trends are discussed subsequently. We also elaborate the requirements that are necessary to realize the 6G applications. Then we present the key enabling technologies in detail. We also outline current research projects and activities including standardization efforts towards the development of 6G. Finally, we summarize lessons learned from state-of-the-art research and discuss technical challenges that would shed a new light on future research directions towards 6G.","2644-125X","","10.1109/OJCOMS.2021.3071496","National Research Foundation of Korea (NRF); Korean Government (MSIT)(grant numbers:NRF-2019R1C1C1006143,NRF-2019R1I1A3A01060518); European Union in RESPONSE 5G(grant numbers:789658); Academy of Finland in 6Genesis(grant numbers:318927); Secure Connect Projects; Institute of Information and Communications Technology Planning and Evaluation (IITP); Korea Government (MSIT)(grant numbers:2020-0-01450); Artificial Intelligence Convergence Research Center, Pusan National University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9397776","Beyond 5G;6G;mobile communication;emerging technologies;survey","6G mobile communication;5G mobile communication;Mobile communication;Internet of Things;Market research;Reliability;Broadband communication","","437","","378","CCBY","7 Apr 2021","","","IEEE","IEEE Journals"
"Federated Learning for Internet of Things: Recent Advances, Taxonomy, and Open Challenges","L. U. Khan; W. Saad; Z. Han; E. Hossain; C. S. Hong","Department of Computer Science and Engineering, Kyung Hee University, Yongin, South Korea; Bradley Department of Electrical and Computer Engineering, Wireless@VT, Virginia Tech, Blacksburg, VA, USA; Department of Computer Science and Engineering, Kyung Hee University, Seoul, South Korea; Department of Electrical and Computer Engineering, University of Manitoba, Winnipeg, MB, Canada; Department of Computer Science and Engineering, Kyung Hee University, Yongin, South Korea",IEEE Communications Surveys & Tutorials,"20 Aug 2021","2021","23","3","1759","1799","The Internet of Things (IoT) will be ripe for the deployment of novel machine learning algorithm for both network and application management. However, given the presence of massively distributed and private datasets, it is challenging to use classical centralized learning algorithms in the IoT. To overcome this challenge, federated learning can be a promising solution that enables on-device machine learning without the need to migrate the private end-user data to a central cloud. In federated learning, only learning model updates are transferred between end-devices and the aggregation server. Although federated learning can offer better privacy preservation than centralized machine learning, it has still privacy concerns. In this paper, first, we present the recent advances of federated learning towards enabling federated learning-powered IoT applications. A set of metrics such as sparsification, robustness, quantization, scalability, security, and privacy, is delineated in order to rigorously evaluate the recent advances. Second, we devise a taxonomy for federated learning over IoT networks. Finally, we present several open research challenges with their possible solutions.","1553-877X","","10.1109/COMST.2021.3090430","National Research Foundation of Korea (NRF) grant funded by the Korea Government (MSIT)(grant numbers:2020R1A4A1018607); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460016","Federated learning;Internet of Things;wireless networks","Collaborative work;Data models;Internet of Things;Computational modeling;Servers;Machine learning;Cloud computing","","434","","182","IEEE","18 Jun 2021","","","IEEE","IEEE Journals"
"Smart Cities: A Survey on Data Management, Security, and Enabling Technologies","A. Gharaibeh; M. A. Salahuddin; S. J. Hussini; A. Khreishah; I. Khalil; M. Guizani; A. Al-Fuqaha","German Jordanian University, Amman, Jordan; University of Waterloo, Waterloo, ON, Canada; Western Michigan University, Kalamazoo, MI, USA; New Jersey Institute of Technology, Newark, NJ, USA; Qatar Computing Research Institute, Doha, Qatar; University of Idaho, Moscow, ID, USA; Western Michigan University, Kalamazoo, MI, USA",IEEE Communications Surveys & Tutorials,"20 Nov 2017","2017","19","4","2456","2501","Integrating the various embedded devices and systems in our environment enables an Internet of Things (IoT) for a smart city. The IoT will generate tremendous amount of data that can be leveraged for safety, efficiency, and infotainment applications and services for city residents. The management of this voluminous data through its lifecycle is fundamental to the realization of smart cities. Therefore, in contrast to existing surveys on smart cities we provide a data-centric perspective, describing the fundamental data management techniques employed to ensure consistency, interoperability, granularity, and reusability of the data generated by the underlying IoT for smart cities. Essentially, the data lifecycle in a smart city is dependent on tightly coupled data management with cross-cutting layers of data security and privacy, and supporting infrastructure. Therefore, we further identify techniques employed for data security and privacy, and discuss the networking and computing technologies that enable smart cities. We highlight the achievements in realizing various aspects of smart cities, present the lessons learned, and identify limitations and research challenges.","1553-877X","","10.1109/COMST.2017.2736886","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8003273","Smart cities;Internet of Things (IoT);data management;data security;network functions virtualization (NFV);software-defined networking (SDN);cloud computing","Smart cities;Data security;Data privacy;Tutorials;Internet of Things;Virtualization;Cloud computing;Software defined networking","","426","","423","IEEE","7 Aug 2017","","","IEEE","IEEE Journals"
"Digital Twin in the IoT Context: A Survey on Technical Features, Scenarios, and Architectural Models","R. Minerva; G. M. Lee; N. Crespi","CNRS Lab UMR5157, Telecom SudParis, Institut Polytechnique de Paris, Evry, France; Computer Science Department, Liverpool John Moores University, Liverpool, U.K.; CNRS Lab UMR5157, Telecom SudParis, Institut Polytechnique de Paris, Evry, France",Proceedings of the IEEE,"28 Sep 2020","2020","108","10","1785","1824","Digital twin (DT) is an emerging concept that is gaining attention in various industries. It refers to the ability to clone a physical object (PO) into a software counterpart. The softwarized object, termed logical object, reflects all the important properties and characteristics of the original object within a specific application context. To fully determine the expected properties of the DT, this article surveys the state-of-the-art starting from the original definition within the manufacturing industry. It takes into account related proposals emerging in other fields, namely augmented and virtual reality (e.g., avatars), multiagent systems, and virtualization. This survey thereby allows for the identification of an extensive set of DT features that point to the “softwarization” of POs. To properly consolidate a shared DT definition, a set of foundational properties is identified and proposed as a common ground outlining the essential characteristics (must-haves) of a DT. Once the DT definition has been consolidated, its technical and business value is discussed in terms of applicability and opportunities. Four application scenarios illustrate how the DT concept can be used and how some industries are applying it. The scenarios also lead to a generic DT architectural model. This analysis is then complemented by the identification of software architecture models and guidelines in order to present a general functional framework for the DT. This article, eventually, analyses a set of possible evolution paths for the DT considering its possible usage as a major enabler for the softwarization process.","1558-2256","","10.1109/JPROC.2020.2998530","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9120192","Artificial intelligence (AI);business models;cyber physical systems (CPSs);digital twin (DT);Internet of Things (IoT);machine learning (ML);multiagent systems;network function virtualization;sensors;servitization;smart city;software architecture;softwarization;virtual and augmented reality","Internet of Things;Cloning;Digital twin;Software engineering;Solid modeling;Software architecture;Manufacturing processing","","413","","293","IEEE","18 Jun 2020","","","IEEE","IEEE Journals"
"Fog Computing: Survey of Trends, Architectures, Requirements, and Research Directions","R. K. Naha; S. Garg; D. Georgakopoulos; P. P. Jayaraman; L. Gao; Y. Xiang; R. Ranjan","School of Technology, Environments and Design, University of Tasmania, Hobart, TAS, Australia; School of Technology, Environments and Design, University of Tasmania, Hobart, TAS, Australia; Faculty of Science, Engineering and Technology, Swinburne University of Technology, Melbourne, VIC, Australia; Faculty of Science, Engineering and Technology, Swinburne University of Technology, Melbourne, VIC, Australia; School of Information Technology, Deakin University, Burwood, VIC, Australia; School of Information Technology, Deakin University, Burwood, VIC, Australia; School of Computing, Newcastle University, Newcastle upon Tyne, U.K.",IEEE Access,"20 Sep 2018","2018","6","","47980","48009","Emerging technologies such as the Internet of Things (IoT) require latency-aware computation for real-time application processing. In IoT environments, connected things generate a huge amount of data, which are generally referred to as big data. Data generated from IoT devices are generally processed in a cloud infrastructure because of the on-demand services and scalability features of the cloud computing paradigm. However, processing IoT application requests on the cloud exclusively is not an efficient solution for some IoT applications, especially time-sensitive ones. To address this issue, Fog computing, which resides in between cloud and IoT devices, was proposed. In general, in the Fog computing environment, IoT devices are connected to Fog devices. These Fog devices are located in close proximity to users and are responsible for intermediate computation and storage. One of the key challenges in running IoT applications in a Fog computing environment are resource allocation and task scheduling. Fog computing research is still in its infancy, and taxonomy-based investigation into the requirements of Fog infrastructure, platform, and applications mapped to current research is still required. This survey will help the industry and research community synthesize and identify the requirements for Fog computing. This paper starts with an overview of Fog computing in which the definition of Fog computing, research trends, and the technical differences between Fog and cloud are reviewed. Then, we investigate numerous proposed Fog computing architectures and describe the components of these architectures in detail. From this, the role of each component will be defined, which will help in the deployment of Fog computing. Next, a taxonomy of Fog computing is proposed by considering the requirements of the Fog computing paradigm. We also discuss existing research works and gaps in resource allocation and scheduling, fault tolerance, simulation tools, and Fog-based microservices. Finally, by addressing the limitations of current research works, we present some open issues, which will determine the future research direction for the Fog computing paradigm.","2169-3536","","10.1109/ACCESS.2018.2866491","University of Tasmania; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8444370","Fog computing;Internet of Things (IoT);fog devices;fault tolerance;IoT application;microservices","Edge computing;Cloud computing;Computer architecture;Market research;Internet of Things;Resource management;Taxonomy","","393","","142","OAPA","22 Aug 2018","","","IEEE","IEEE Journals"
"Toward Edge Intelligence: Multiaccess Edge Computing for 5G and Internet of Things","Y. Liu; M. Peng; G. Shou; Y. Chen; S. Chen","Yaqiong Liu and Guochu Shou are all with the School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering and the State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Yaqiong Liu and Guochu Shou are all with the School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Cyberspace Security, Beijing University of Posts and Telecommunications, Beijing, China; International School, Beijing University of Posts and Telecommunications, Beijing, China",IEEE Internet of Things Journal,"12 Aug 2020","2020","7","8","6722","6747","To satisfy the increasing demand of mobile data traffic and meet the stringent requirements of the emerging Internet-of-Things (IoT) applications such as smart city, healthcare, and augmented/virtual reality (AR/VR), the fifth-generation (5G) enabling technologies are proposed and utilized in networks. As an emerging key technology of 5G and a key enabler of IoT, multiaccess edge computing (MEC), which integrates telecommunication and IT services, offers cloud computing capabilities at the edge of the radio access network (RAN). By providing computational and storage resources at the edge, MEC can reduce latency for end users. Hence, this article investigates MEC for 5G and IoT comprehensively. It analyzes the main features of MEC in the context of 5G and IoT and presents several fundamental key technologies which enable MEC to be applied in 5G and IoT, such as cloud computing, software-defined networking/network function virtualization, information-centric networks, virtual machine (VM) and containers, smart devices, network slicing, and computation offloading. In addition, this article provides an overview of the role of MEC in 5G and IoT, bringing light into the different MEC-enabled 5G and IoT applications as well as the promising future directions of integrating MEC with 5G and IoT. Moreover, this article further elaborates research challenges and open issues of MEC for 5G and IoT. Last but not least, we propose a use case that utilizes MEC to achieve edge intelligence in IoT scenarios.","2327-4662","","10.1109/JIOT.2020.3004500","National Natural Science Foundation of China(grant numbers:61901052); Higher Education Discipline Innovation Project(grant numbers:B17007); Director Funds of Beijing Key Laboratory of Network System Architecture and Convergence(grant numbers:2017BKL-NSAC-ZJ-02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9123504","Edge computing;edge intelligence;fifth generation (5G);Internet of Things (IoT);low latency;mobile-edge computing;multiaccess edge computing (MEC)","Internet of Things;5G mobile communication;Cloud computing;Servers;Edge computing;Bandwidth;Real-time systems","","369","","192","IEEE","23 Jun 2020","","","IEEE","IEEE Journals"
"Understanding O-RAN: Architecture, Interfaces, Algorithms, Security, and Research Challenges","M. Polese; L. Bonati; S. D’Oro; S. Basagni; T. Melodia","Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA",IEEE Communications Surveys & Tutorials,"22 May 2023","2023","25","2","1376","1411","The Open Radio Access Network (RAN) and its embodiment through the O-RAN Alliance specifications are poised to revolutionize the telecom ecosystem. O-RAN promotes virtualized RANs where disaggregated components are connected via open interfaces and optimized by intelligent controllers. The result is a new paradigm for the RAN design, deployment, and operations: O-RAN networks can be built with multi-vendor, interoperable components, and can be programmatically optimized through a centralized abstraction layer and data-driven closed-loop control. Therefore, understanding O-RAN, its architecture, its interfaces, and workflows is key for researchers and practitioners in the wireless community. In this article, we present the first detailed tutorial on O-RAN. We also discuss the main research challenges and review early research results. We provide a deep dive of the O-RAN specifications, describing its architecture, design principles, and the O-RAN interfaces. We then describe how the O-RAN RAN Intelligent Controllers (RICs) can be used to effectively control and manage 3GPP-defined RANs. Based on this, we discuss innovations and challenges of O-RAN networks, including the Artificial Intelligence (AI) and Machine Learning (ML) workflows that the architecture and interfaces enable, security, and standardization issues. Finally, we review experimental research platforms that can be used to design and test O-RAN networks, along with recent research results, and we outline future directions for O-RAN development.","1553-877X","","10.1109/COMST.2023.3239220","U.S. National Science Foundation(grant numbers:CNS-1923789,CNS-2112471); U.S. Office of Naval Research(grant numbers:N00014-20-1-2132); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10024837","Open RAN;O-RAN;cellular;5G;6G","Computer architecture;Security;Precoding;3GPP;Radio frequency;Radio access networks;Optimization","","341","","215","CCBY","23 Jan 2023","","","IEEE","IEEE Journals"
"Federated-Learning-Based Anomaly Detection for IoT Security Attacks","V. Mothukuri; P. Khare; R. M. Parizi; S. Pouriyeh; A. Dehghantanha; G. Srivastava","College of Computing and Software Engineering, Kennesaw State University, Kennesaw, GA, USA; College of Computing and Software Engineering, Kennesaw State University, Kennesaw, GA, USA; College of Computing and Software Engineering, Kennesaw State University, Kennesaw, GA, USA; College of Computing and Software Engineering, Kennesaw State University, Kennesaw, GA, USA; Cyber Science Lab, University of Guelph, Guelph, ON, Canada; Department of Mathematics and Computer Science, Brandon University, Brandon, MB, Canada",IEEE Internet of Things Journal,"4 Feb 2022","2022","9","4","2545","2554","The Internet of Things (IoT) is made up of billions of physical devices connected to the Internet via networks that perform tasks independently with less human intervention. Such brilliant automation of mundane tasks requires a considerable amount of user data in digital format, which, in turn, makes IoT networks an open source of personally identifiable information data for malicious attackers to steal, manipulate, and perform nefarious activities. A huge interest has been developed over the past years in applying machine learning (ML)-assisted approaches in the IoT security space. However, the assumption in many current works is that big training data are widely available and transferable to the main server because data are born at the edge and are generated continuously by IoT devices. This is to say that classic ML works on the legacy set of entire data located on a central server, which makes it the least preferred option for domains with privacy concerns on user data. To address this issue, we propose the federated-learning (FL)-based anomaly detection approach to proactively recognize intrusion in IoT networks using decentralized on-device data. Our approach uses federated training rounds on gated recurrent units (GRUs) models and keeps the data intact on local IoT devices by sharing only the learned weights with the central server of FL. Also, the approach’s ensembler part aggregates the updates from multiple sources to optimize the global ML model’s accuracy. Our experimental results demonstrate that our approach outperforms the classic/centralized machine learning (non-FL) versions in securing the privacy of user data and provides an optimal accuracy rate in attack detection.","2327-4662","","10.1109/JIOT.2021.3077803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9424138","Federated learning (FL);gated recurrent units (GRUs);Internet of Things (IoT);recurrent neural networks (RNNs);security","Logic gates;Security;Computer architecture;Anomaly detection;Training;Servers;Data models","","327","","48","IEEE","5 May 2021","","","IEEE","IEEE Journals"
"Internet of Things for the Future of Smart Agriculture: A Comprehensive Survey of Emerging Technologies","O. Friha; M. A. Ferrag; L. Shu; L. Maglaras; X. Wang","Networks and Systems Laboratory, University of Badji Mokhtar-Annaba, Annaba, Algeria; Department of Computer Science, Guelma University, Gulema, Algeria; School of Engineering, University of Lincoln, Lincoln, UK; School of Computer Science and Informatics, De Montfort University, Leicester, UK; Department of Electrical Engineering, Nanjing Agricultural University, Nanjing, China",IEEE/CAA Journal of Automatica Sinica,"10 Mar 2021","2021","8","4","718","752","This paper presents a comprehensive review of emerging technologies for the internet of things (IoT)-based smart agriculture. We begin by summarizing the existing surveys and describing emergent technologies for the agricultural IoT, such as unmanned aerial vehicles, wireless technologies, open-source IoT platforms, software defined networking (SDN), network function virtualization (NFV) technologies, cloud/fog computing, and middleware platforms. We also provide a classification of IoT applications for smart agriculture into seven categories: including smart monitoring, smart water management, agrochemicals applications, disease management, smart harvesting, supply chain management, and smart agricultural practices. Moreover, we provide a taxonomy and a side-by-side comparison of the state-of-the-art methods toward supply chain management based on the blockchain technology for agricultural IoTs. Furthermore, we present real projects that use most of the aforementioned technologies, which demonstrate their great performance in the field of smart agriculture. Finally, we highlight open research challenges and discuss possible future research directions for agricultural IoTs.","2329-9274","","10.1109/JAS.2021.1003925","Research Start-Up Fund for Talent Researcher of Nanjing Agricultural University(grant numbers:77H0603); National Natural Science Foundation of China(grant numbers:62072248); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9374808","Agricultural internet of things (IoT);internet of things (IoT);smart agriculture;smart farming;sustainable agriculture","Wireless communication;Supply chain management;Agriculture;Unmanned aerial vehicles;Internet of Things;Software defined networking;Open source software","","325","","258","","10 Mar 2021","","","IEEE","IEEE Journals"
"Computation Offloading Toward Edge Computing","L. Lin; X. Liao; H. Jin; P. Li","College of Mathematics and Informatics, Fujian Normal University, Fuzhou, China; Service Computing Technology and System Laboratory, Cluster and Grid Computing Laboratory, National Engineering Research Center for Big Data Technology and System, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Service Computing Technology and System Laboratory, Cluster and Grid Computing Laboratory, National Engineering Research Center for Big Data Technology and System, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science and Engineering, The University of Aizu, Aizuwakamatsu, Japan",Proceedings of the IEEE,"6 Aug 2019","2019","107","8","1584","1607","We are living in a world where massive end devices perform computing everywhere and everyday. However, these devices are constrained by the battery and computational resources. With the increasing number of intelligent applications (e.g., augmented reality and face recognition) that require much more computational power, they shift to perform computation offloading to the cloud, known as mobile cloud computing (MCC). Unfortunately, the cloud is usually far away from end devices, leading to a high latency as well as the bad quality of experience (QoE) for latency-sensitive applications. In this context, the emergence of edge computing is no coincidence. Edge computing extends the cloud to the edge of the network, close to end users, bringing ultra-low latency and high bandwidth. Consequently, there is a trend of computation offloading toward edge computing. In this paper, we provide a comprehensive perspective on this trend. First, we give an insight into the architecture refactoring in edge computing. Based on that insight, this paper reviews the state-of-the-art research on computation offloading in terms of application partitioning, task allocation, resource management, and distributed execution, with highlighting features for edge computing. Then, we illustrate some disruptive application scenarios that we envision as critical drivers for the flourish of edge computing, such as real-time video analytics, smart “things” (e.g., smart city and smart home), vehicle applications, and cloud gaming. Finally, we discuss the opportunities and future research directions.","1558-2256","","10.1109/JPROC.2019.2922285","National Basic Research Program of China (973 Program)(grant numbers:2018YFB1003500); National Natural Science Foundation of China(grant numbers:61832006,61825202,61502103); Japan Society for the Promotion of Science(grant numbers:19K20258); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8758310","Computation offloading;edge computing;Internet of Things (IoT);mobile cloud computing (MCC);mobile edge computing (MEC)","Backscatter;Wireless communication;Throughput;Radio frequency;Edge computing;Resource management;Energy harvesting;Internet of Things;Cloud computing","","321","","236","IEEE","9 Jul 2019","","","IEEE","IEEE Journals"
"A Survey on Emerging SDN and NFV Security Mechanisms for IoT Systems","I. Farris; T. Taleb; Y. Khettab; J. Song","Department of Communications and Networking, Aalto University, Espoo, Finland; Computer and Information Security Department, Sejong University, Seoul, South Korea; Department of Communications and Networking, Aalto University, Espoo, Finland; Computer and Information Security Department, Sejong University, Seoul, South Korea",IEEE Communications Surveys & Tutorials,"22 Feb 2019","2019","21","1","812","837","The explosive rise of Internet of Things (IoT) systems have notably increased the potential attack surfaces for cybercriminals. Accounting for the features and constraints of IoT devices, traditional security countermeasures can be inefficient in dynamic IoT environments. In this vein, the advantages introduced by software defined networking (SDN) and network function virtualization (NFV) have the potential to reshape the landscape of cybersecurity for IoT systems. To this aim, we provide a comprehensive analysis of security features introduced by NFV and SDN, describing the manifold strategies able to monitor, protect, and react to IoT security threats. We also present lessons learned in the adoption of SDN/NFV-based protection approaches in IoT environments, comparing them with conventional security countermeasures. Finally, we deeply discuss the open challenges related to emerging SDN- and NFV-based security mechanisms, aiming to provide promising directives to conduct future research in this fervent area.","1553-877X","","10.1109/COMST.2018.2862350","ANASTACIA Project through the European Unions Horizon 2020 Research and Innovation Programme(grant numbers:731558); Swiss State Secretariat for Education, Research, and Innovation; National Research Foundation of Korea; Ministry of Education(grant numbers:NRF-2017R1D1A1B03036285); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8424018","Internet of Things;security;SDN;NFV;cloud;edge computing","Security;Wireless sensor networks;Protocols;Internet of Things;Cloud computing;Robot sensing systems","","305","","204","IEEE","1 Aug 2018","","","IEEE","IEEE Journals"
"Industrial Artificial Intelligence in Industry 4.0 - Systematic Review, Challenges and Outlook","R. S. Peres; X. Jia; J. Lee; K. Sun; A. W. Colombo; J. Barata","Centre of Technology and Systems, UNINOVA Instituto Desenvolvimento de Novas Tecnologias, Caparica, Portugal; Department of Mechanical and Materials Engineering, University of Cincinnati, Cincinnati, OH, USA; Department of Mechanical and Materials Engineering, University of Cincinnati, Cincinnati, OH, USA; Foxconn Industrial Internet, Milwaukee, WI, USA; Department of Electrotechnical and Industrial Informatics, University of Applied Sciences Emden/Leer, Emden, Germany; Centre of Technology and Systems, UNINOVA Instituto Desenvolvimento de Novas Tecnologias, Caparica, Portugal",IEEE Access,"16 Dec 2020","2020","8","","220121","220139","The advent of the Industry 4.0 initiative has made it so that manufacturing environments are becoming more and more dynamic, connected but also inherently more complex, with additional inter-dependencies, uncertainties and large volumes of data being generated. Recent advances in Industrial Artificial Intelligence have showcased the potential of this technology to assist manufacturers in tackling the challenges associated with this digital transformation of Cyber-Physical Systems, through its data-driven predictive analytics and capacity to assist decision-making in highly complex, non-linear and often multistage environments. However, the industrial adoption of such solutions is still relatively low beyond the experimental pilot stage, as real environments provide unique and difficult challenges for which organizations are still unprepared. The aim of this paper is thus two-fold. First, a systematic review of current Industrial Artificial Intelligence literature is presented, focusing on its application in real manufacturing environments to identify the main enabling technologies and core design principles. Then, a set of key challenges and opportunities to be addressed by future research efforts are formulated along with a conceptual framework to bridge the gap between research in this field and the manufacturing industry, with the goal of promoting industrial adoption through a successful transition towards a digitized and data-driven company-wide culture. This paper is among the first to provide a clear definition and holistic view of Industrial Artificial Intelligence in the Industry 4.0 landscape, identifying and analysing its fundamental building blocks and ongoing trends. Its findings are expected to assist and empower researchers and manufacturers alike to better understand the requirements and steps necessary for a successful transition into Industry 4.0 supported by AI, as well as the challenges that may arise during this process.","2169-3536","","10.1109/ACCESS.2020.3042874","FCT/MCTES (UNINOVA-CTS)(grant numbers:UIDB/00066/2020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9285283","Artificial intelligence;Industry 4.0;digital transformation;guidelines;systematic review;framework;manufacturing","Artificial intelligence;Industries;Robots;Systematics;Manufacturing;Decision making;Service robots","","290","","102","CCBY","7 Dec 2020","","","IEEE","IEEE Journals"
"Edge-Computing-Enabled Smart Cities: A Comprehensive Survey","L. U. Khan; I. Yaqoob; N. H. Tran; S. M. A. Kazmi; T. N. Dang; C. S. Hong","Department of Computer Science and Engineering, Kyung Hee University, Yongin, South Korea; Department of Computer Science and Engineering, Kyung Hee University, Yongin, South Korea; School of Computer Science, University of Sydney, Sydney, NSW, Australia; Department of Computer Science and Engineering, Kyung Hee University, Yongin, South Korea; Department of Computer Science and Engineering, Kyung Hee University, Yongin, South Korea; Department of Computer Science and Engineering, Kyung Hee University, Yongin, South Korea",IEEE Internet of Things Journal,"9 Oct 2020","2020","7","10","10200","10232","Recent years have disclosed a remarkable proliferation of compute-intensive applications in smart cities. Such applications continuously generate enormous amounts of data which demand strict latency-aware computational processing capabilities. Although edge computing is an appealing technology to compensate for stringent latency-related issues, its deployment engenders new challenges. In this article, we highlight the role of edge computing in realizing the vision of smart cities. First, we analyze the evolution of edge computing paradigms. Subsequently, we critically review the state-of-the-art literature focusing on edge computing applications in smart cities. Later, we categorize and classify the literature by devising a comprehensive and meticulous taxonomy. Furthermore, we identify and discuss key requirements, and enumerate recently reported synergies of edge computing-enabled smart cities. Finally, several indispensable open challenges along with their causes and guidelines are discussed, serving as future research directions.","2327-4662","","10.1109/JIOT.2020.2987070","Institute of Information and Communications Technology Planning and Evaluation (IITP); Korea Government [Ministry of Science and ICT (MSIT), Evolvable Deep Learning Model Generation Platform for Edge Computing](grant numbers:2019-0-01287); MSIT, South Korea, through the Grand Information Technology Research Center Support Program; IITP(grant numbers:IITP-2020-2015-0-00742); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9063670","Cloudlet;fog computing;Internet of Things (IoT);micro data centers;mobile cloud computing (MCC);mobile-edge computing;smart cities","Edge computing;Smart cities;Cloud computing;Internet of Things;Autonomous automobiles;Accidents","","269","","241","IEEE","10 Apr 2020","","","IEEE","IEEE Journals"
"Elasticity in Cloud Computing: State of the Art and Research Challenges","Y. Al-Dhuraibi; F. Paraiso; N. Djarallah; P. Merle","Scalair Company, Hem, France; Inria, Lille - Nord Europe, Villeneuve d'Ascq, France; Scalair Company, Hem, France; Inria, Lille - Nord Europe, Villeneuve d'Ascq, France",IEEE Transactions on Services Computing,"6 Apr 2018","2018","11","2","430","447","Elasticity is a fundamental property in cloud computing that has recently witnessed major developments. This article reviews both classical and recent elasticity solutions and provides an overview of containerization, a new technological trend in lightweight virtualization. It also discusses major issues and research challenges related to elasticity in cloud computing. We comprehensively review and analyze the proposals developed in this field. We provide a taxonomy of elasticity mechanisms according to the identified works and key properties. Compared to other works in literature, this article presents a broader and detailed analysis of elasticity approaches and is considered as the first survey addressing the elasticity of containers.","1939-1374","","10.1109/TSC.2017.2711009","OCCIware research and development project; French Programme d'Investissements d'Avenir; Scalair company; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7937885","Elasticity;cloud computing;auto-scaling;resource provision;scalability;containers","Elasticity;Cloud computing;Virtualization;Containers;Scalability;Operating systems;Taxonomy","","268","","197","IEEE","1 Jun 2017","","","IEEE","IEEE Journals"
"Fog of Everything: Energy-Efficient Networked Computing Architectures, Research Challenges, and a Case Study","E. Baccarelli; P. G. V. Naranjo; M. Scarpiniti; M. Shojafar; J. H. Abawajy","Department of Information Engineering, Electronics and Telecommunications, Sapienza University of Rome, Rome, Italy; Department of Information Engineering, Electronics and Telecommunications, Sapienza University of Rome, Rome, Italy; Department of Information Engineering, Electronics and Telecommunications, Sapienza University of Rome, Rome, Italy; Department of Information Engineering, Electronics and Telecommunications, Sapienza University of Rome, Rome, Italy; School of Information Technology, Deakin University, Geelong, VIC, Australia",IEEE Access,"19 Jun 2017","2017","5","","9882","9910","Fog computing (FC) and Internet of Everything (IoE) are two emerging technological paradigms that, to date, have been considered standing-alone. However, because of their complementary features, we expect that their integration can foster a number of computing and network-intensive pervasive applications under the incoming realm of the future Internet. Motivated by this consideration, the goal of this position paper is fivefold. First, we review the technological attributes and platforms proposed in the current literature for the standing-alone FC and IoE paradigms. Second, by leveraging some use cases as illustrative examples, we point out that the integration of the FC and IoE paradigms may give rise to opportunities for new applications in the realms of the IoE, Smart City, Industry 4.0, and Big Data Streaming, while introducing new open issues. Third, we propose a novel technological paradigm, the Fog of Everything (FoE) paradigm, that integrates FC and IoE and then we detail the main building blocks and services of the corresponding technological platform and protocol stack. Fourth, as a proof-of-concept, we present the simulated energy-delay performance of a small-scale FoE prototype, namely, the V-FoE prototype. Afterward, we compare the obtained performance with the corresponding one of a benchmark technological platform, e.g., the V-D2D one. It exploits only device-to-device links to establish inter-thing “ad hoc” communication. Last, we point out the position of the proposed FoE paradigm over a spectrum of seemingly related recent research projects.","2169-3536","","10.1109/ACCESS.2017.2702013","GAUChO-A Green Adaptive Fog Computing and Networking Architectures Project through the MIUR Progetti di Ricerca di Rilevante Interesse Nazionale (PRIN) Bando 2015(grant numbers:2015YPXH4W 004); Vehicular Fog Energy-Efficient QoS Mining and Dissemination of Multimedia Big Data Streams Project (V-FOG) through Sapienza University of Rome Bando 2016; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7921687","Fog of IoE;virtualized networked computing platforms for IoE;context-aware networking-plus-computing distributed resource management;Internet of Energy;Smart City;Industry 4.0;Big Data Streaming;future Internet","Cloud computing;Big Data;Edge computing;Biological system modeling;Ecosystems;Smart cities","","265","","159","OAPA","7 May 2017","","","IEEE","IEEE Journals"
"Internet of Things (IoT): A Review of Its Enabling Technologies in Healthcare Applications, Standards Protocols, Security, and Market Opportunities","M. N. Bhuiyan; M. M. Rahman; M. M. Billah; D. Saha","Institute of Information Technology, Noakhali Science and Technology University, Noakhali, Bangladesh; Department of Information and Communication Technology, Islamic University, Kushtia, Bangladesh; Department of Computer Science, American International University-Bangladesh, Dhaka, Bangladesh; Institute of Information Technology, Noakhali Science and Technology University, Noakhali, Bangladesh",IEEE Internet of Things Journal,"22 Jun 2021","2021","8","13","10474","10498","The Internet of Things (IoT) is a methodology or a system that encompasses real-world things to interact and communicate with each other with the assistance of networking technologies. This article describes surveys on advances in IoT-based healthcare methods and reviews the state-of-the-art technologies in detail. Moreover, this review classifies an existing IoT-based healthcare network and represents a summary of all perspective networks. IoT healthcare protocols are analyzed in this context and provide a broad discussion on it. It also initiates a comprehensive survey on IoT healthcare applications and services. Extensive insights into IoT healthcare security, its requirements, challenges, and privacy issues are visualized in IoT surrounding healthcare. In this review, we analyze security and privacy features consisting of data protection, network architecture, Quality of Services (QoS), app development, and continuous monitoring of healthcare that are facing difficulties in many IoT-based healthcare architectures. To mitigate the security problems, an IoT-based security architectural model has been proposed in this review. Furthermore, this review discloses the market opportunity that will enhance the IoT healthcare market development. To conduct the survey, we searched through established journal and conference databases using specific keywords to find scholarly works. We applied a filtering mechanism to collect only papers that were relevant to our research works. The selected papers were then examined carefully to understand their contributions/research focus. Eventually, the paper reviews were analyzed to identify any existing research gaps and untouched areas of research and to discover possible features for sustainable IoT healthcare development.","2327-4662","","10.1109/JIOT.2021.3062630","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9365708","Architectures;healthcare;Internet of Things (IoT);networks;security","Medical services;Security;Internet of Things;Monitoring;Sensors;Protocols;Analytical models","","250","","165","IEEE","1 Mar 2021","","","IEEE","IEEE Journals"
"A Full Dive Into Realizing the Edge-Enabled Metaverse: Visions, Enabling Technologies, and Challenges","M. Xu; W. C. Ng; W. Y. B. Lim; J. Kang; Z. Xiong; D. Niyato; Q. Yang; X. Shen; C. Miao","School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; Alibaba Group and Alibaba-NTU Joint Research Institute, Nanyang Technological University, Jurong West, Singapore; Alibaba Group and Alibaba-NTU Joint Research Institute, Nanyang Technological University, Jurong West, Singapore; School of Automation, Guangdong University of Technology, Guangzhou, China; Pillar of Information Systems Technology and Design, Singapore University of Technology and Design, Tampines, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada; School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore",IEEE Communications Surveys & Tutorials,"23 Feb 2023","2023","25","1","656","700","Dubbed “the successor to the mobile Internet,” the concept of the Metaverse has grown in popularity. While there exist lite versions of the Metaverse today, they are still far from realizing the full vision of an immersive, embodied, and interoperable Metaverse. Without addressing the issues of implementation from the communication and networking, as well as computation perspectives, the Metaverse is difficult to succeed the Internet, especially in terms of its accessibility to billions of users today. In this survey, we focus on the edge-enabled Metaverse to realize its ultimate vision. We first provide readers with a succinct tutorial of the Metaverse, an introduction to the architecture, as well as current developments. To enable ubiquitous, seamless, and embodied access to the Metaverse, we discuss the communication and networking challenges and survey cutting-edge solutions and concepts that leverage next-generation communication systems for users to immerse as and interact with embodied avatars in the Metaverse. Moreover, given the high computation costs required, e.g., to render 3D virtual worlds and run data-hungry artificial intelligence-driven avatars, we discuss the computation challenges and cloud-edge-end computation framework-driven solutions to realize the Metaverse on resource-constrained edge devices. Next, we explore how blockchain technologies can aid in the interoperable development of the Metaverse, not just in terms of empowering the economic circulation of virtual user-generated content but also to manage physical edge resources in a decentralized, transparent, and immutable manner. Finally, we discuss the future research directions towards realizing the true vision of the edge-enabled Metaverse.","1553-877X","","10.1109/COMST.2022.3221119","National Research Foundation (NRF) and Infocomm Media Development Authority through the Future Communications Research and Development Programme (FCP), through the AI Singapore Programme (AISG) and DSO National Laboratories(grant numbers:AISG2-RP-2020-019); Energy Research Test-Bed and Industry Partnership Funding Initiative; Energy Grid (EG) 2.0 Programme through the DesCartes and the Campus for Research Excellence and Technological Enterprise (CREATE) Programme; Alibaba Group through Alibaba Innovative Research (AIR) Program and Alibaba-NTU Singapore Joint Research Institute (JRI); NSFC(grant numbers:62102099,U22A2054); SUTD(grant numbers:SRG-ISTD-2021-165); SUTD-ZJU IDEA Grant [SUTDZJU (VP)](grant numbers:202102); SUTD-ZJU IDEA Seed Grant [SUTD-ZJU (SD)](grant numbers:202101); Ministry of Education, Singapore, through its SUTD Kickstarter Initiative(grant numbers:SKI 20210204); Wallenberg-NTU Presidential Postdoctoral Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9944868","Metaverse;edge networks;communication and networking;computation;blockchain;Internet technology","Metaverse;Internet;Artificial intelligence;Blockchains;Tutorials;Computer architecture;Avatars","","241","","322","IEEE","10 Nov 2022","","","IEEE","IEEE Journals"
"A Prospective Look: Key Enabling Technologies, Applications and Open Research Topics in 6G Networks","L. Bariah; L. Mohjazi; S. Muhaidat; P. C. Sofotasios; G. K. Kurt; H. Yanikomeroglu; O. A. Dobre","Department of Electrical and Computer Engineering, KU Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, United Arab Emirates; School of Engineering, University of Glasgow, Glasgow, U.K.; Department of Electrical and Computer Engineering, KU Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical and Computer Engineering, KU Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, United Arab Emirates; Electronics and Communication Engineering Department, Istanbul Technical University, Istanbul, Turkey; Department of Systems and Computer Engineering, Carleton University, Ottawa, Canada; Department of Electrical and Computer Engineering, Memorial University, St. John’s, Canada",IEEE Access,"2 Oct 2020","2020","8","","174792","174820","The fifth generation (5G) mobile networks are envisaged to enable a plethora of breakthrough advancements in wireless technologies, providing support of a diverse set of services over a single platform. While the deployment of 5G systems is scaling up globally, it is time to look ahead for beyond 5G systems. This is mainly driven by the emerging societal trends, calling for fully automated systems and intelligent services supported by extended reality and haptics communications. To accommodate the stringent requirements of their prospective applications, which are data-driven and defined by extremely low-latency, ultra-reliable, fast and seamless wireless connectivity, research initiatives are currently focusing on a progressive roadmap towards the sixth generation (6G) networks, which are expected to bring transformative changes to this premise. In this article, we shed light on some of the major enabling technologies for 6G, which are expected to revolutionize the fundamental architectures of cellular networks and provide multiple homogeneous artificial intelligence-empowered services, including distributed communications, control, computing, sensing, and energy, from its core to its end nodes. In particular, the present paper aims to answer several 6G framework related questions: What are the driving forces for the development of 6G? How will the enabling technologies of 6G differ from those in 5G? What kind of applications and interactions will they support which would not be supported by 5G? We address these questions by presenting a comprehensive study of the 6G vision and outlining seven of its disruptive technologies, i.e., mmWave communications, terahertz communications, optical wireless communications, programmable metasurfaces, drone-based communications, backscatter communications and tactile internet, as well as their potential applications. Then, by leveraging the state-of-the-art literature surveyed for each technology, we discuss the associated requirements, key challenges, and open research problems. These discussions are thereafter used to open up the horizon for future research directions.","2169-3536","","10.1109/ACCESS.2020.3019590","Khalifa University of Science, Technology and Research(grant numbers:KU/FSU-8474000122,KU/RC1-C2PS-2/847400012237); Natural Sciences and Engineering Research Council of Canada (NSERC), through its Discovery Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9178307","6G;backscatter communications;drone-based communications;terahertz communications;metasurfaces;mm-wave;optical wireless communications;tactile internet","5G mobile communication;Wireless communication;Wireless sensor networks;Market research;Artificial intelligence;Reliability;Haptic interfaces","","239","","350","CCBY","26 Aug 2020","","","IEEE","IEEE Journals"
"Security and Privacy for 6G: A Survey on Prospective Technologies and Challenges","V. -L. Nguyen; P. -C. Lin; B. -C. Cheng; R. -H. Hwang; Y. -D. Lin","Department of Computer Science and Information Engineering, National Chung Cheng University, Chiayi, Minhsiung, Taiwan; Department of Computer Science and Information Engineering, National Chung Cheng University, Chiayi, Minhsiung, Taiwan; Department of Communications Engineering, National Chung Cheng University, Chiayi, Minhsiung, Taiwan; Department of Computer Science and Information Engineering, Advanced Institute of Manufacturing with High-Tech Innovations, National Chung Cheng University, Chiayi, Minhsiung, Taiwan; Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan",IEEE Communications Surveys & Tutorials,"19 Nov 2021","2021","23","4","2384","2428","Sixth-generation (6G) mobile networks will have to cope with diverse threats on a space-air-ground integrated network environment, novel technologies, and an accessible user information explosion. However, for now, security and privacy issues for 6G remain largely in concept. This survey provides a systematic overview of security and privacy issues based on prospective technologies for 6G in the physical, connection, and service layers, as well as through lessons learned from the failures of existing security architectures and state-of-the-art defenses. Two key lessons learned are as follows. First, other than inheriting vulnerabilities from the previous generations, 6G has new threat vectors from new radio technologies, such as the exposed location of radio stripes in ultra-massive MIMO systems at Terahertz bands and attacks against pervasive intelligence. Second, physical layer protection, deep network slicing, quantum-safe communications, artificial intelligence (AI) security, platform-agnostic security, real-time adaptive security, and novel data protection mechanisms such as distributed ledgers and differential privacy are the top promising techniques to mitigate the attack magnitude and personal data breaches substantially.","1553-877X","","10.1109/COMST.2021.3108618","Ministry of Science and Technology (MOST) of Taiwan(grant numbers:110-2811-E-194-501-MY2,108-2221-E-194-022-MY3,108-2221-E-194-019-MY3); Advanced Institute of Manufacturing with High-Tech Innovations (AIM-HI) through the Featured Areas Research Center Program within the framework of the Higher Education Sprout Project by the Ministry of Education (MOE) in Taiwan; Ministry of Education and Training (MOET) of Vietnam and Thai Nguyen University(grant numbers:B2021-TNA-02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9524814","6G;security and privacy;AI security;physical layer security;connection security;service security","Security;6G mobile communication;Privacy;5G mobile communication;Computer architecture;Systematics;Standards","","235","","251","IEEE","30 Aug 2021","","","IEEE","IEEE Journals"
"The Roadmap to 6G Security and Privacy","P. Porambage; G. Gür; D. P. M. Osorio; M. Liyanage; A. Gurtov; M. Ylianttila","Centre for Wireless Communications, University of Oulu, Oulu, Finland; Institute of Applied Information Technology (InIT), Zurich University of Applied Sciences (ZHAW), Winterthur, Switzerland; Centre for Wireless Communications, University of Oulu, Oulu, Finland; Centre for Wireless Communications, University of Oulu, Oulu, Finland; Department of Computer and Information Science, Linköping University, Linköping, Sweden; Centre for Wireless Communications, University of Oulu, Oulu, Finland",IEEE Open Journal of the Communications Society,"20 May 2021","2021","2","","1094","1122","Although the fifth generation (5G) wireless networks are yet to be fully investigated, the visionaries of the 6th generation (6G) echo systems have already come into the discussion. Therefore, in order to consolidate and solidify the security and privacy in 6G networks, we survey how security may impact the envisioned 6G wireless systems, possible challenges with different 6G technologies, and the potential solutions. We provide our vision on 6G security and security key performance indicators (KPIs) with the tentative threat landscape based on the foreseen 6G network architecture. Moreover, we discuss the security and privacy challenges that may encounter with the available 6G requirements and potential 6G applications. We also give the reader some insights into the standardization efforts and research-level projects relevant to 6G security. In particular, we discuss the security considerations with 6G enabling technologies such as distributed ledger technology (DLT), physical layer security, distributed AI/ML, visible light communication (VLC), THz, and quantum computing. All in all, this work intends to provide enlightening guidance for the subsequent research of 6G security and privacy at this initial phase of vision towards reality.","2644-125X","","10.1109/OJCOMS.2021.3078081","6Genesis Flagship(grant numbers:318927); 5GEAR Projects; European Union’s Horizon 2020 Research and Innovation Programme(grant numbers:871808); CENIIT Project 17.01; Excellence Center at Linköping-Lund in IT (ELLIIT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9426946","6G;security;security threats;AI/ML security;DLT;physical layer security;privacy;quantum computing","6G mobile communication;Security;Privacy;5G mobile communication;Computer architecture;Wireless communication;Distributed ledger","","212","","176","CCBY","10 May 2021","","","IEEE","IEEE Journals"
"Survey on Multi-Access Edge Computing Security and Privacy","P. Ranaweera; A. D. Jurcut; M. Liyanage","School of Computer Science, University College Dublin, Dublin 4, Ireland; School of Computer Science, University College Dublin, Dublin 4, Ireland; School of Computer Science, University College Dublin, Dublin 4, Ireland",IEEE Communications Surveys & Tutorials,"21 May 2021","2021","23","2","1078","1124","The European Telecommunications Standards Institute (ETSI) has introduced the paradigm of Multi-Access Edge Computing (MEC) to enable efficient and fast data processing in mobile networks. Among other technological requirements, security and privacy are significant factors in the realization of MEC deployments. In this paper, we analyse the security and privacy of the MEC system. We introduce a thorough investigation of the identification and the analysis of threat vectors in the ETSI standardized MEC architecture. Furthermore, we analyse the vulnerabilities leading to the identified threat vectors and propose potential security solutions to overcome these vulnerabilities. The privacy issues of MEC are also highlighted, and clear objectives for preserving privacy are defined. Finally, we present future directives to enhance the security and privacy of MEC services.","1553-877X","","10.1109/COMST.2021.3062546","European Union under RESPONSE 5G(grant numbers:789658); Academy of Finland under 6Genesis Flagship Projects(grant numbers:318927); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9364272","Multi-access edge computing (MEC);security;privacy;Internet of Things (IoT);5G;cloud computing;future networks","Security;Privacy;Cloud computing;5G mobile communication;Servers;Real-time systems;Outsourcing","","211","","208","IEEE","26 Feb 2021","","","IEEE","IEEE Journals"
"A Survey on Edge Computing Systems and Tools","F. Liu; G. Tang; Y. Li; Z. Cai; X. Zhang; T. Zhou","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; Key Laboratory of Science and Technology on Information System Engineering, National University of Defense Technology, Changsha, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; College of Computer, National University of Defense Technology, Changsha, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; College of Computer, National University of Defense Technology, Changsha, China",Proceedings of the IEEE,"6 Aug 2019","2019","107","8","1537","1562","Driven by the visions of Internet of Things and 5G communications, the edge computing systems integrate computing, storage, and network resources at the edge of the network to provide computing infrastructure, enabling developers to quickly develop and deploy edge applications. At present, the edge computing systems have received widespread attention in both industry and academia. To explore new research opportunities and assist users in selecting suitable edge computing systems for specific applications, this survey paper provides a comprehensive overview of the existing edge computing systems and introduces representative projects. A comparison of open-source tools is presented according to their applicability. Finally, we highlight energy efficiency and deep learning optimization of edge computing systems. Open issues for analyzing and designing an edge computing system are also studied in this paper.","1558-2256","","10.1109/JPROC.2019.2920341","National Basic Research Program of China (973 Program)(grant numbers:2016YFB1000302); National Natural Science Foundation of China(grant numbers:61433019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8746691","Deep learning optimization;edge computing systems;energy efficiency;open-source tools;survey","Cloud computing;Edge computing;Computer architecture;Energy efficiency;Internet of Things;Computational modeling;Deep learning;Open source software;Optimization","","210","","84","IEEE","26 Jun 2019","","","IEEE","IEEE Journals"
"Fog/Edge Computing-Based IoT (FECIoT): Architecture, Applications, and Research Issues","B. Omoniwa; R. Hussain; M. A. Javed; S. H. Bouk; S. A. Malik","Computer Science Programme, National Mathematical Centre, Abuja, Nigeria; Department of Electrical Engineering, COMSATS University, Islamabad, Pakistan; Department of Electrical Engineering, COMSATS University, Islamabad, Pakistan; Information and Communication Engineering, Daegu Gyeongbuk Institute of Science and Technology (DGIST), Daegu, South Korea; Department of Electrical Engineering, COMSATS University, Islamabad, Pakistan",IEEE Internet of Things Journal,"18 Jun 2019","2019","6","3","4118","4149","The Internet-of-Things (IoT) is the future of the Internet, where everything will be connected. Studies have revealed that fog/edge computing-based services will play a major role in extending the cloud by carrying out intermediary services at the edge of the network. Fog/edge computing-based IoT's (FECIoT) distributed architecture enhances service provisioning along the Cloud-to-Things continuum, thereby making it suitable for mission-critical applications. Furthermore, the proximity of fog/edge devices to where the data is produced makes it stand-out in terms of resource allocation, service delivery, and privacy. From the business perspective, FECIoT will lead to a boom and spring up of small-to-medium-sized enterprises, thereby encouraging inclusion for all. To this end, we present a comprehensive survey on state-of-the-art IoT literature over the period 2008-2018 and propose the FECIoT framework which covers the enabling technologies, services, and open research issues. A tutorial approach is employed, progressing from basic to more advanced concepts within the IoT domain. Lastly, we show how FECIoT can be deployed in real-life cyber-physical systems, such as the intelligent transportation system, smart grid, smart health-care, smart homes, and smart environment.","2327-4662","","10.1109/JIOT.2018.2875544","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8489908","Cyber-physical systems (CPSs);enabling technologies;fog/edge computing (FEC);Internet-of-Things (IoT);service-oriented architecture (SoA)","Cloud computing;Protocols;Computational modeling;Computer architecture;Security;Privacy;Internet of Things","","195","","181","IEEE","11 Oct 2018","","","IEEE","IEEE Journals"
"Smart Contract Privacy Protection Using AI in Cyber-Physical Systems: Tools, Techniques and Challenges","R. Gupta; S. Tanwar; F. Al-Turjman; P. Italiya; A. Nauman; S. W. Kim","Department of Computer Science and Engineering, Institute of Technology, Nirma University, Ahmedabad, India; Department of Computer Science and Engineering, Institute of Technology, Nirma University, Ahmedabad, India; Artificial Intelligence Department, Near East University, Mersin, Turkey; Department of Computer Science and Engineering, Institute of Technology, Nirma University, Ahmedabad, India; Department of Information and Communication Engineering, Yeungnam University, Gyeongsan, South Korea; Department of Information and Communication Engineering, Yeungnam University, Gyeongsan, South Korea",IEEE Access,"7 Feb 2020","2020","8","","24746","24772","Applications of Blockchain (BC) technology and Cyber-Physical Systems (CPS) are increasing exponentially. However, framing resilient and correct smart contracts (SCs) for these smart application is a quite challenging task because of the complexity associated with them. SC is modernizing the traditional industrial, technical, and business processes. It is self-executable, self-verifiable, and embedded into the BC that eliminates the need for trusted third-party systems, which ultimately saves administration as well as service costs. It also improves system efficiency and reduces the associated security risks. However, SCs are well encouraging the new technological reforms in Industry 4.0, but still, various security and privacy challenges need to be addressed. In this paper, a survey on SC security vulnerabilities in the software code that can be easily hacked by a malicious user or may compromise the entire BC network is presented. As per the literature, the challenges related to SC security and privacy are not explored much by the authors around the world. From the existing proposals, it has been observed that designing a complex SCs cannot mitigate its privacy and security issues. So, this paper investigates various Artificial Intelligence (AI) techniques and tools for SC privacy protection. Then, open issues and challenges for AI-based SC are analyzed. Finally, a case study of retail marketing is presented, which uses AI and SC to preserve its security and privacy.","2169-3536","","10.1109/ACCESS.2020.2970576","National Research Foundation of Korea(grant numbers:22A20130012814); MSIT (Ministry of Science and ICT), South Korea, through the Information Technology Research Center (ITRC) Support Program supervised by the Institute for Information and communications Technology Planning and Evaluation (IITP)(grant numbers:IITP-2019-2016-0-00313); National Research Foundation of Korea(grant numbers:2018R1D1A1A09082266); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8976143","Cyber-physical system;blockchain;smart contract;artificial intelligence;security;privacy","Artificial intelligence;Privacy;Security;Smart contracts;Tools;Blockchain","","184","","159","CCBY","30 Jan 2020","","","IEEE","IEEE Journals"
"A Survey of Recent Advances in Edge-Computing-Powered Artificial Intelligence of Things","Z. Chang; S. Liu; X. Xiong; Z. Cai; G. Tu","School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; School of National Cybersecurity, Wuhan University, Wuhan, China",IEEE Internet of Things Journal,"6 Sep 2021","2021","8","18","13849","13875","The Internet of Things (IoT) has created a ubiquitously connected world powered by a multitude of wired and wireless sensors generating a variety of heterogeneous data over time in a myriad of fields and applications. To extract complete information from these data, advanced artificial intelligence (AI) technology, especially deep learning (DL), has proved successful in facilitating data analytics, future prediction and decision making. The collective integration of AI and the IoT has greatly promoted the rapid development of AI-of-Things (AIoT) systems that analyze and respond to external stimuli more intelligently without involvement by humans. However, it is challenging or infeasible to process massive amounts of data in the cloud due to the destructive impact of the volume, velocity, and veracity of data and fatal transmission latency on networking infrastructures. These critical challenges can be adequately addressed by introducing edge computing. This article conducts an extensive survey of an end-edge-cloud orchestrated architecture for flexible AIoT systems. Specifically, it begins with articulating fundamental concepts including the IoT, AI and edge computing. Guided by these concepts, it explores the general AIoT architecture, presents a practical AIoT example to illustrate how AI can be applied in real-world applications and summarizes promising AIoT applications. Then, the emerging technologies for AI models regarding inference and training at the edge of the network are reviewed. Finally, the open challenges and future directions in this promising area are outlined.","2327-4662","","10.1109/JIOT.2021.3088875","Major Projects of Technical Innovation of Hubei Province(grant numbers:2018AAA046); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9453402","Artificial intelligence (AI);deep learning (DL);edge computing;Internet of Things (IoT);machine learning (ML)","Artificial intelligence;Internet of Things;Cloud computing;Edge computing;Computer architecture;Computational modeling;Training","","178","","131","IEEE","14 Jun 2021","","","IEEE","IEEE Journals"
"6G Ecosystem: Current Status and Future Perspective","J. R. Bhat; S. A. Alqahtani","St. Joseph Engineering College, Mangalore, India; Research Chair of Pervasive and Mobile Computing, King Saud University, Riyadh, Saudi Arabia",IEEE Access,"19 Mar 2021","2021","9","","43134","43167","Next-generation of the cellular network will attempt to overcome the limitations of the current Fifth Generation (5G) networks and equip itself to address the challenges which become obvious in the future. Currently, academia and industry have focused their attention on the Sixth Generation (6G) network, which is anticipated to be the next big game-changer in the telecom industry. The outbreak of COVID'19 has made the whole world to opt for virtual meetings, live video interactions ranging from healthcare, business to education. However, we miss an immersive experience due to the lack of supporting technology. Experts have anticipated that starting from the post-pandemic age, the performance requirements of technology for virtual and real-time communication, the rise of several verticals such as industrial automation, robotics, and autonomous driving will increase tremendously, and will skyrocket during the next decade. In this manuscript, we study the latest perspectives and future megatrends that are most likely to drive 6G. Initially, we describe the instances that lead us to the vision of 6G. Later, we narrate some of the use cases and the KPIs essential to meet their performance requirement. Further, we highlight the key requirements of 6G based on contemporary research such as UN sustainability goals, business model, edge intelligence, digital divide, and the trends in machine learning for 6G.","2169-3536","","10.1109/ACCESS.2021.3054833","Deanship of Scientific Research, Research Chair of Pervasive and Mobile Computing, King Saud University, Riyadh, Saudi Arabia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9335927","6G;artificial intelligence;cloud computing;sustainability goals;digital divide;healthcare;machine learning;Tera hertz communication;cellular network;6G architecture","6G mobile communication;5G mobile communication;Reliability;Security;Industries;Surgery;Ecosystems","","172","","159","CCBYNCND","26 Jan 2021","","","IEEE","IEEE Journals"
"A Survey on Edge and Edge-Cloud Computing Assisted Cyber-Physical Systems","K. Cao; S. Hu; Y. Shi; A. W. Colombo; S. Karnouskos; X. Li","College of Information Science and Technology, Jinan University, Guangzhou, China; School of Electronics and Computer Science, University of Southampton, Highfield, Southampton, U.K.; Department of Mechanical Engineering, University of Victoria, Victoria, BC, Canada; Institute for Industrial Informatics, Automation and Robotics, University of Applied Sciences Emden/Leer, Emden, Germany; Research, SAP, Walldorf, Germany; Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA",IEEE Transactions on Industrial Informatics,"30 Jul 2021","2021","17","11","7806","7819","In recent years, the investigations on cyber-physical systems (CPS) have become increasingly popular in both academia and industry. A primary obstruction against the booming deployment of CPS applications lies in how to process and manage large amounts of generated data for decision making. To tackle this predicament, researchers advocate the idea of coupling edge computing, or edge-cloud computing into the design of CPS. However, this coupling process raises a diversity of challenges to the quality-of-services (QoS) of CPS applications. In this article, we present a survey on edge computing or edge-cloud computing assisted CPS designs from the QoS optimization perspective. We first discuss critical challenges in service latency, energy consumption, security, privacy, and reliability during the integration of CPS with edge computing or edge-cloud computing. Afterwards, we give an overview on the state-of-the-art works tackling different challenges for QoS optimization, and present a systematic classification during outlining literature for highlighting their similarities and differences. We finally summarize the experiences learned from surveyed works and envision future research directions on edge computing or edge-cloud computing assisted CPS optimization.","1941-0050","","10.1109/TII.2021.3073066","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2020YFB1005600); Guangdong Key R&D Plan 2020(grant numbers:2020B0101090002); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2019B1515120010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9403939","Cyber-physical systems (CPS);edge computing;edge-cloud computing;energy;latency;privacy;reliability;security","Edge computing;Optimization;Cloud computing;Servers;Computer architecture;Reliability;Energy consumption","","171","","76","IEEE","13 Apr 2021","","","IEEE","IEEE Journals"
"Advancing Software-Defined Networks: A Survey","J. H. Cox; J. Chung; S. Donovan; J. Ivey; R. J. Clark; G. Riley; H. L. Owen","Soar Technology, Inc., Ann Arbor, MI, USA; Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; College of Computing, Georgia Institute of Technology, Atlanta, GA, USA; Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA",IEEE Access,"7 Dec 2017","2017","5","","25487","25526","Having gained momentum from its promise of centralized control over distributed network architectures at bargain costs, software-defined Networking (SDN) is an ever-increasing topic of research. SDN offers a simplified means to dynamically control multiple simple switches via a single controller program, which contrasts with current network infrastructures where individual network operators manage network devices individually. Already, SDN has realized some extraordinary use cases outside of academia with companies, such as Google, AT&T, Microsoft, and many others. However, SDN still presents many research and operational challenges for government, industry, and campus networks. Because of these challenges, many SDN solutions have developed in an ad hoc manner that are not easily adopted by other organizations. Hence, this paper seeks to identify some of the many challenges where new and current researchers can still contribute to the advancement of SDN and further hasten its broadening adoption by network operators.","2169-3536","","10.1109/ACCESS.2017.2762291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8066287","Software-defined networking (SDN);network virtualization (NV);network functions virtualization (NFV);standards;SDN interfaces and APIs;data plane;middleboxes;SDN security;hybrid networks;software-defined exchange (SDX);software-defined infrastructure (SDI);software-defined wireless networks (SDWN);Internet of Things (IoT);information-centric networking (ICN);cloud;software-defined RAN;5G","Security;Government;Industries;Virtualization;Standards organizations;Centralized control","","170","","240","CCBY","12 Oct 2017","","","IEEE","IEEE Journals"
"Edge Mesh: A New Paradigm to Enable Distributed Intelligence in Internet of Things","Y. Sahni; J. Cao; S. Zhang; L. Yang","Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; School of Information Science and Engineering, Central South University, Changsha, China; School of Software Engineering, South China University of Technology, Guangzhou, China",IEEE Access,"6 Sep 2017","2017","5","","16441","16458","In recent years, there has been a paradigm shift in Internet of Things (IoT) from centralized cloud computing to edge computing (or fog computing). Developments in ICT have resulted in the significant increment of communication and computation capabilities of embedded devices and this will continue to increase in coming years. However, existing paradigms do not utilize low-level devices for any decision-making process. In fact, gateway devices are also utilized mostly for communication interoperability and some low-level processing. In this paper, we have proposed a new computing paradigm, named Edge Mesh, which distributes the decision-making tasks among edge devices within the network instead of sending all the data to a centralized server. All the computation tasks and data are shared using a mesh network of edge devices and routers. Edge Mesh provides many benefits, including distributed processing, low latency, fault tolerance, better scalability, better security, and privacy. These benefits are useful for critical applications, which require higher reliability, real-time processing, mobility support, and context awareness. We first give an overview of existing computing paradigms to establish the motivation behind Edge Mesh. Then, we describe in detail about the Edge Mesh computing paradigm, including the proposed software framework, research challenges, and benefits of Edge Mesh. We have also described the task management framework and done a preliminary study on task allocation problem in Edge Mesh. Different application scenarios, including smart home, intelligent transportation system, and healthcare, are presented to illustrate the significance of Edge Mesh computing paradigm.","2169-3536","","10.1109/ACCESS.2017.2739804","RGC General Research Fund(grant numbers:PolyU 152244/15E); NSFC Key Project(grant numbers:61332004); National Natural Science Foundation of China(grant numbers:61502312); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8010408","Edge devices;Internet of Things;distributed intelligence;distributed computing;mesh network","Cloud computing;Edge computing;Servers;Resource management;Security;Decision making;Sensors","","161","","48","OAPA","14 Aug 2017","","","IEEE","IEEE Journals"
"Physical-Layer Security in Space Information Networks: A Survey","B. Li; Z. Fei; C. Zhou; Y. Zhang","School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Institute of High Energy Physics, Chinese Academy of Sciences, Beijing, China; Department of Informatics, University of Oslo, Oslo, Norway",IEEE Internet of Things Journal,"10 Jan 2020","2020","7","1","33","52","Research and processing development on satellite communications has strongly re-emerged in recent years. Following the prosperity of various wireless services provided by satellite communications, the security issue has raised growing concerns since the space information network is susceptible to be eavesdropped by illegal adversaries in such a large-scale wireless network. Recently, the physical-layer security (PLS) has emerged as an alternative security paradigm that explores the randomness of the wireless channel to achieve confidentiality and authentication. The success story of the PLS technique now spans a decade and thrives to provide a layer of defense in satellite communications. With this position, a comprehensive survey of satellite communications is conducted in this article with an emphasis on PLS. We first briefly introduce essential background and the view of the satellite Internet of Things (IoT), as well as discuss related research challenges faced by the emerging integrated network architecture. Then, we revisit the most popular satellite channel model influenced by many factors and list the commonly used secrecy performance metrics. Also, we provide an exhaustive review of state-of-the-art research activity on PLS in satellite communications, which we categorize by different architectures including land mobile satellite communication networks, hybrid satellite-terrestrial relay networks, and satellite-terrestrial integrated networks. In addition, a number of open research problems are identified as possible future research directions.","2327-4662","","10.1109/JIOT.2019.2943900","National Natural Science Foundation of China(grant numbers:61871032,61901447); European Union’s Horizon 2020 Research and Innovation Programme under Marie Skłodowska-Curie(grant numbers:824019); Sichuan Science and Technology Program(grant numbers:2019YFH0033); Startup Foundation for Introducing Talent of Nanjing University of Information Science and Technology; Guilin University of Electronic Technology(grant numbers:CRKL190204); Priority Academic Program Development of Jiangsu Higher Education Institutions; Natural Science Foundation of Shandong Province(grant numbers:ZR2019PF018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8850067","Fifth generation (5G) and beyond;heterogeneous networks;satellite communication;satellite Internet of Things (IoT);security","Satellite broadcasting;Security;Satellites;Internet of Things;Earth;Wireless communication","","161","","135","IEEE","26 Sep 2019","","","IEEE","IEEE Journals"
"Mobility Digital Twin: Concept, Architecture, Case Study, and Future Challenges","Z. Wang; R. Gupta; K. Han; H. Wang; A. Ganlath; N. Ammar; P. Tiwari","InfoTech Labs, Toyota Motor North America Research and Development, Mountain View, CA, USA; InfoTech Labs, Toyota Motor North America Research and Development, Mountain View, CA, USA; InfoTech Labs, Toyota Motor North America Research and Development, Mountain View, CA, USA; InfoTech Labs, Toyota Motor North America Research and Development, Mountain View, CA, USA; InfoTech Labs, Toyota Motor North America Research and Development, Mountain View, CA, USA; InfoTech Labs, Toyota Motor North America Research and Development, Mountain View, CA, USA; InfoTech Labs, Toyota Motor North America Research and Development, Mountain View, CA, USA",IEEE Internet of Things Journal,"7 Sep 2022","2022","9","18","17452","17467","A Digital Twin is a digital replica of a living or nonliving physical entity, and this emerging technology attracted extensive attention from different industries during the past decade. Although a few Digital Twin studies have been conducted in the transportation domain very recently, there is no systematic research with a holistic framework connecting various mobility entities together. In this study, a mobility digital twin (MDT) framework is developed, which is defined as an artificial intelligence (AI)-based data-driven cloud–edge–device framework for mobility services. This MDT consists of three building blocks in the physical space (namely, Human, Vehicle, and Traffic), and their associated Digital Twins in the digital space. An example cloud–edge architecture is built with Amazon Web Services (AWS) to accommodate the proposed MDT framework and to fulfill its digital functionalities of storage, modeling, learning, simulation, and prediction. A case study of the personalized adaptive cruise control (P-ACC) system is conducted, which integrates the key microservices of all three digital building blocks of the MDT framework: 1) the Human Digital Twin with user management and driver type classification; 2) the Vehicle Digital Twin with cloud-based advanced driver-assistance systems (ADAS); and 3) the Traffic Digital Twin with traffic flow monitoring and variable speed limit. Future challenges of the proposed MDT framework are discussed toward the end of the article, including standardization, AI for computing, public or private cloud service, and network heterogeneity.","2327-4662","","10.1109/JIOT.2022.3156028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9724183","Amazon Web services (AWS);cloud computing;connected vehicles;digital twin;edge computing","Digital twin;Cloud computing;Connected vehicles;Internet of Things;Vehicles;Space vehicles;Computational modeling","","140","","98","IEEE","2 Mar 2022","","","IEEE","IEEE Journals"
"An Introduction to Dew Computing: Definition, Concept and Implications","P. P. Ray","Department of Computer Applications, Sikkim University, Gangtok, India",IEEE Access,"15 Feb 2018","2018","6","","723","737","Since the end of the 1990s, the world has witnessed a tremendous growth in the area of information and communication technology (ICT), starting with grid computing, cloud computing (CC), and fog computing to recently introduced edge computing. Although, these technologies are still in very good shape, they do heavily rely on connectivity, i.e., Internet. To address this challenge, this paper proposes a novel dew-cloud architecture that brings the power of CC together with the dew computing (DC). Originally, the dew-cloud architecture is an extension of the existing client-server architecture, where two servers are placed at both ends of the communication link. With the help of a dew server, a user has more control and flexibility to access his/her personal data in the absence of an Internet connection. Primarily, the data are stored at the dew server as a local copy upon which instantiation of the Internet is synchronized with the master copy at the cloud side. Users can browse, read, write, or append data on the local dew site, which is a local Web form of an actual website. With the incorporation of the dew domain naming system and dew domain name redirection, mapping between different local dew sites has become possible. Novel services, such as infrastructure-as-a-dew, software-as-a-dew service, and software-as-a-dew product, are, hereby, introduced along with the DC. This paper presents the following as key contributions: 1) a precise and concrete definition of DC; 2) detailed and comprehensive discussions of its concept and working principle; 3) application potentials; and 4) technical challenges. The motto of this paper is to conceptualize the fact of empowerment of the ICT-user base with almost an Internet-free surfing experience in coming days.","2169-3536","","10.1109/ACCESS.2017.2775042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8114187","Dew-cloud architecture;dew server;dew site;dew database;DDNS;DDNR;dew service","Cloud computing;Edge computing;Computer architecture;Computational modeling;Servers;Biological system modeling","","137","","31","OAPA","17 Nov 2017","","","IEEE","IEEE Journals"
"Cloud-Edge Orchestration for the Internet of Things: Architecture and AI-Powered Data Processing","Y. Wu","College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, U.K.",IEEE Internet of Things Journal,"5 Aug 2021","2021","8","16","12792","12805","The Internet of Things (IoT) has been deeply penetrated into a wide range of important and critical sectors, including smart city, water, transportation, manufacturing, and smart factory. Massive data are being acquired from a fast growing number of IoT devices. Efficient data processing is a necessity to meet diversified and stringent requirements of many emerging IoT applications. Due to the constrained computation and storage resources, IoT devices have resorted to the powerful cloud computing to process their data. However, centralized and remote cloud computing may introduce unacceptable communication delay since its physical location is far away from IoT devices. Edge cloud has been introduced to overcome this issue by moving the cloud in closer proximity to IoT devices. The orchestration and cooperation between the cloud and the edge provides a crucial computing architecture for IoT applications. Artificial intelligence (AI) is a powerful tool to enable the intelligent orchestration in this architecture. This article first introduces such a kind of computing architecture from the perspective of IoT applications. It then investigates the state-of-the-art proposals on AI-powered cloud-edge orchestration for the IoT. Finally, a list of potential research challenges and open issues is provided and discussed, which can provide useful resources for carrying out future research in this area.","2327-4662","","10.1109/JIOT.2020.3014845","Engineering and Physical Sciences Research Council of United Kingdom(grant numbers:EP/R030863/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9162084","Artificial intelligence (AI);cloud computing;edge computing;Internet of Things (IoT);offloading","Cloud computing;Servers;Computer architecture;Internet of Things;Edge computing;Data processing;Medical services","","133","","124","IEEE","7 Aug 2020","","","IEEE","IEEE Journals"
"6G Architecture to Connect the Worlds","V. Ziegler; H. Viswanathan; H. Flinck; M. Hoffmann; V. Räisänen; K. Hätönen","Nokia Bell Labs and CTO, Munich, Germany; Nokia Bell Labs, Murray Hill, USA; Nokia Bell Labs, Espoo, Finland; Nokia Bell Labs, Munich, Germany; Nokia Bell Labs, Espoo, Finland; Nokia Bell Labs, Espoo, Finland",IEEE Access,"28 Sep 2020","2020","8","","173508","173520","The post-pandemic future will offer tremendous opportunity and challenge from transformation of the human experience linking physical, digital and biological worlds: 6G should be based on a new architecture to fully realize the vision to connect the worlds. We explore several novel architecture concepts for the 6G era driven by a decomposition of the architecture into platform, functions, orchestration and specialization aspects. With 6G, we associate an open, scalable, elastic, and platform agnostic het-cloud, with converged applications and services decomposed into micro-services and serverless functions, specialized architecture for extreme attributes, as well as open service orchestration architecture. Key attributes and characteristics of the associated architectural scenarios are described. At the air-interface level, 6G is expected to encompass use of sub-Terahertz spectrum and new spectrum sharing technologies, air-interface design optimized by AI/ML techniques, integration of radio sensing with communication, and meeting extreme requirements on latency, reliability and synchronization. Fully realizing the benefits of these advances in radio technology will also call for innovations in 6G network architecture as described.","2169-3536","","10.1109/ACCESS.2020.3025032","Nokia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200631","6G;architecture;B5G;cellular communication;convergence;orchestration;sub-networks;wireless networks","Computer architecture;Cloud computing;5G mobile communication;Sensors;Microprocessors;Technological innovation;Industries","","130","","30","CCBY","18 Sep 2020","","","IEEE","IEEE Journals"
"Blockchain for Edge of Things: Applications, Opportunities, and Challenges","T. R. Gadekallu; Q. -V. Pham; D. C. Nguyen; P. K. R. Maddikunta; N. Deepa; B. Prabadevi; P. N. Pathirana; J. Zhao; W. -J. Hwang","School of Information Technology and Engineering, Vellore Institute of Technology, Vellore, India; Korean Southeast Center for the 4th Industrial Revolution Leader Education, Pusan National University, Busan, South Korea; School of Engineering, Deakin University, Waurn Ponds, VIC, Australia; School of Information Technology and Engineering, Vellore Institute of Technology, Vellore, India; School of Information Technology and Engineering, Vellore Institute of Technology, Vellore, India; School of Information Technology and Engineering, Vellore Institute of Technology, Vellore, India; School of Engineering, Deakin University, Waurn Ponds, VIC, Australia; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Department of Biomedical Convergence Engineering, Pusan National University, Yangsan, South Korea",IEEE Internet of Things Journal,"6 Jan 2022","2022","9","2","964","988","In recent years, blockchain networks have attracted significant attention in many research areas beyond cryptocurrency, one of them being the Edge of Things (EoT) that is enabled by the combination of edge computing and the Internet of Things (IoT). In this context, blockchain networks enabled with unique features, such as decentralization, immutability, and traceability, have the potential to reshape and transform the conventional EoT systems with higher security levels. Particularly, the convergence of blockchain and EoT leads to a new paradigm, called BEoT that has been regarded as a promising enabler for future services and applications. In this article, we present a state-of-the-art review of recent developments in the BEoT technology and discover its great opportunities in many application domains. We start our survey by providing an updated introduction to blockchain and EoT along with their recent advances. Subsequently, we discuss the use of BEoT in a wide range of industrial applications, from smart transportation, smart city, smart healthcare to smart home, and smart grid. Security challenges in the BEoT paradigm are also discussed and analyzed, with some key services, such as access authentication, data privacy preservation, attack detection, and trust management. Finally, some key research challenges and future directions are also highlighted to instigate further research in this promising area.","2327-4662","","10.1109/JIOT.2021.3119639","National Research Foundation of Korea (NRF); Korean Government (MSIT)(grant numbers:NRF-2019R1C1C1006143,NRF-2019R1I1A3A01060518); Institute of Information and Communications Technology Planning and Evaluation (IITP); Korea Government (MSIT)(grant numbers:2020-0-01450); Artificial Intelligence Convergence Research Center [Pusan National University]); Ministry of Science and ICT (MSIT), South Korea; Grand Information Technology Research Center Support Program; Institute for Information Communications, Technology, Planning and Evaluation (IITP)(grant numbers:IITP-2021-2016-0-00318); BK21 Four, Korean Southeast Center for the 4th Industrial Revolution Leader Education; Pusan National University Research Grant, 2020; Nanyang Technological University Startup Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9569742","Blockchain;edge computing;Edge of Things (EoT);industrial applications;Internet of Things (IoT);security","Blockchains;Internet of Things;Security;Edge computing;Medical services;Smart transportation;Smart grids","","130","","139","IEEE","13 Oct 2021","","","IEEE","IEEE Journals"
"Digital Twin of Wireless Systems: Overview, Taxonomy, Challenges, and Opportunities","L. U. Khan; Z. Han; W. Saad; E. Hossain; M. Guizani; C. S. Hong","Department of Computer Science and Engineering, Kyung Hee University, Yongin, South Korea; Electrical and Computer Engineering Department and Computer Science Department, University of Houston, Houston, TX, USA; Department of Computer Science and Engineering, Kyung Hee University, Seoul, South Korea; Department of Electrical and Computer Engineering, University of Manitoba, Winnipeg, MB, Canada; Machine Learning Department, Mohamed Bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE; Department of Computer Science and Engineering, Kyung Hee University, Yongin, South Korea",IEEE Communications Surveys & Tutorials,"21 Nov 2022","2022","24","4","2230","2254","Future wireless services will focus on improving the quality of life by enabling various applications, such as extended reality, brain-computer interaction, and healthcare. These applications will have diverse performance requirements (e.g., user-defined quality of experience metrics, latency, and reliability) which will be challenging to be fulfilled by existing wireless systems. To meet the diverse requirements of the emerging applications, the concept of digital twins has been recently proposed. A digital twin uses a virtual representation along with security-related technologies (e.g., blockchain), communication technologies (e.g., 6G), computing technologies (e.g., edge computing), and machine learning, so as to enable the smart applications. In this tutorial, we present a comprehensive overview on digital twins for wireless systems. First, we present the fundamental concepts (i.e., design aspects, high-level architecture, and frameworks) of digital twins for wireless systems. Second, a comprehensive taxonomy is devised for two aspects, namely, twins for wireless and wireless for twins. For the twins for wireless aspect, we consider issues related to design of twin objects, physical devices, and interface, as well as prototyping, deployment trends, incentive mechanism, isolation of twins, and decoupling. For the wireless for twins aspect, we consider issues related to accessing twin objects, security and privacy, and air interface design are considered. Finally, open research challenges and opportunities are discussed.","1553-877X","","10.1109/COMST.2022.3198273","National Research Foundation of Korea (NRF); Korea Government (Ministry of Science and ICT: MSIT)(grant numbers:2020R1A4A1018607); Institute of Information and Communications Technology Planning and Evaluation (IITP); Korea Government (MSIT, Evolvable Deep Learning Model Generation Platform for Edge Computing)(grant numbers:2019-0-01287); IITP Grant; Korea Government (MSIT, Artificial Intelligence Innovation Hub)(grant numbers:2021-0-02068); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9854866","Digital twin;wireless system;machine learning;federated learning;virtual modeling","Digital twins;Wireless communication;Tutorials;Computational modeling;Communication system security;Taxonomy;Market research","","126","","141","IEEE","11 Aug 2022","","","IEEE","IEEE Journals"
"IoT Software Infrastructure for Energy Management and Simulation in Smart Cities","F. G. Brundu; E. Patti; A. Osello; M. D. Giudice; N. Rapetti; A. Krylovskiy; M. Jahn; V. Verda; E. Guelpa; L. Rietto; A. Acquaviva","Politecnico di Torino, Torino, Italy; Politecnico di Torino, Torino, Italy; Politecnico di Torino, Torino, Italy; Politecnico di Torino, Torino, Italy; Politecnico di Torino, Torino, Italy; Fraunhofer Institute for Applied Information Technology FIT, Sankt Augustin, Germany; Fraunhofer Institute for Applied Information Technology FIT, Sankt Augustin, Germany; Politecnico di Torino, Torino, Italy; Politecnico di Torino, Torino, Italy; Politecnico di Torino, Torino, Italy; Politecnico di Torino, Torino, Italy",IEEE Transactions on Industrial Informatics,"19 May 2017","2017","13","2","832","840","This paper presents an Internet-of-Things software infrastructure that enables energy management and simulation of new control policies in a city district. The proposed platform enables the interoperability and the correlation of (near-)real-time building energy profiles with environmental data from sensors as well as building and grid models. In a smart city context, this platform fulfills 1) the integration of heterogeneous data sources at the building and district level, and 2) the simulation of novel energy policies at the district level aimed at the optimization of the energy usage accounting also for its impact on building comfort. The platform has been deployed in a real-world district and a novel control policy for the heating distribution network has been developed and tested. Results are presented and discussed in the paper.","1941-0050","","10.1109/TII.2016.2627479","EU; FP7; District Information Modeling and Management for Energy Reduction; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7740051","Distributed software infrastructure;energy flow simulation;energy saving;Internet-of-Things (IoT);middleware;smart city","Sensors;Data models;Smart cities;Heating systems;Middleware","","126","","35","IEEE","9 Nov 2016","","","IEEE","IEEE Journals"
"Fog Computing: A Comprehensive Architectural Survey","P. Habibi; M. Farhoudi; S. Kazemian; S. Khorsandi; A. Leon-Garcia","Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Department of Computer Engineering and Information Technology, Amirkabir University of Technology, Tehran, Iran; Department of Computer Engineering and Information Technology, Amirkabir University of Technology, Tehran, Iran; Department of Computer Engineering and Information Technology, Amirkabir University of Technology, Tehran, Iran; Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada",IEEE Access,"22 Apr 2020","2020","8","","69105","69133","Fog computing is an emerging technology to address computing and networking bottlenecks in large scale deployment of IoT applications. It is a promising complementary computing paradigm to cloud computing where computational, networking, storage and acceleration elements are deployed at the edge and network layers in a multi-tier, distributed and possibly cooperative manner. These elements may be virtualized computing functions placed at edge devices or network elements on demand, realizing the “computing everywhere” concept. To put the current research in perspective, this paper provides an inclusive taxonomy for architectural, algorithmic and technologic aspects of fog computing. The computing paradigms and their architectural distinctions, including cloud, edge, mobile edge and fog computing are subsequently reviewed. Practical deployment of fog computing includes a number of different aspects such as system design, application design, software implementation, security, computing resource management and networking. A comprehensive survey of all these aspects from the architectural point of view is covered. Current reference architectures and major application-specific architectures describing their salient features and distinctions in the context of fog computing are explored. Base architectures for application, software, security, computing resource management and networking are presented and are evaluated using a proposed maturity model.","2169-3536","","10.1109/ACCESS.2020.2983253","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9046806","Cloud Computing;edge computing;fog computing;Internet of Things (IoT);advanced internet architecture","Edge computing;Cloud computing;Computer architecture;Resource management;Security;Taxonomy;Software","","126","","191","CCBY","25 Mar 2020","","","IEEE","IEEE Journals"
"Point-to-Multipoint Optical Networks Using Coherent Digital Subcarriers","D. Welch; A. Napoli; J. Bäck; W. Sande; J. Pedro; F. Masoud; C. Fludger; T. Duthel; H. Sun; S. J. Hand; T. -K. Chiang; A. Chase; A. Mathur; T. A. Eriksson; M. Plantare; M. Olson; S. Voll; K. -T. Wu","Infinera Corporation, 6373 San Ignacio Ave, San Jose, CA, USA; Infinera Limited, c/o Fitzgerald and Law LLP, New Penderel House, London, U.K.; Infinera, Stockholm, Sweden; Infinera Corporation, 6373 San Ignacio Ave, San Jose, CA, USA; Infinera Unipessoal Lda, Carnaxide, Portugal; Infinera Limited, Ottawa, Ontario, Canada; Infinera GmbH, Nordostpark, Nuremberg, Germany; Infinera GmbH, Nordostpark, Nuremberg, Germany; Infinera Limited, Ottawa, Ontario, Canada; Infinera Corporation, 6373 San Ignacio Ave, San Jose, CA, USA; Infinera Corporation, 6373 San Ignacio Ave, San Jose, CA, USA; Infinera Corporation, 6373 San Ignacio Ave, San Jose, CA, USA; Infinera Corporation, 6373 San Ignacio Ave, San Jose, CA, USA; Infinera, Stockholm, Sweden; Infinera, Stockholm, Sweden; Infinera, Stockholm, Sweden; Coriant, part of Infinera Group, Munich, Germany; Infinera Limited, Ottawa, Ontario, Canada",Journal of Lightwave Technology,"30 Aug 2021","2021","39","16","5232","5247","A paradigm shift in optical communication networks is proposed, with the introduction of a new ecosystem of devices and components with the capability of transforming current point-to-point optical networks (with their entailed, limiting, electrical aggregation) into flexible, scalable and cost-effective point-to-multipoint networks. In the new architecture, which better aligns with the hub-and-spoke traffic patterns observed in today's metro and access network segments, interoperability across a variety of transceivers operating at different speeds is achieved using individually routed, digitally generated subcarriers. The first comprehensive demonstration of the technical feasibility of the proposed point-to-multipoint architecture based on digital subcarrier multiplexing is presented, along with the remarkable cost savings and simplification of the network it enables.","1558-2213","","10.1109/JLT.2021.3097163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9488196","Point-to-Multipoint;digital subcarrier multiplexing;5G;metro aggregation;fronthaul;coherent access","Transceivers;Optical fiber networks;Optical switches;Optical fibers;Point-to-multipoint communications;Integrated optics;IP networks","","125","","53","CCBYNCND","16 Jul 2021","","","IEEE","IEEE Journals"
"Investigating Messaging Protocols for the Internet of Things (IoT)","E. Al-Masri; K. R. Kalyanam; J. Batts; J. Kim; S. Singh; T. Vo; C. Yan","School of Engineering and Technology, University of Washington Tacoma, Tacoma, USA; School of Engineering and Technology, University of Washington Tacoma, Tacoma, USA; School of Engineering and Technology, University of Washington Tacoma, Tacoma, USA; School of Engineering and Technology, University of Washington Tacoma, Tacoma, USA; School of Engineering and Technology, University of Washington Tacoma, Tacoma, USA; School of Engineering and Technology, University of Washington Tacoma, Tacoma, USA; School of Engineering and Technology, University of Washington Tacoma, Tacoma, USA",IEEE Access,"28 May 2020","2020","8","","94880","94911","As the number of Internet of Things (IoT) devices proliferates, the magnitude and velocity of data continues to increase rapidly. IoT systems rely primarily on using messaging protocols for exchanging IoT data and there exists several protocols or frameworks that support distinct types of messaging patterns. Given that IoT devices typically have limited computational resources and processing power, choosing a lightweight, reliable, scalable, interoperable, extensible and secure messaging protocol becomes a very challenging task. As a result, it is not uncommon that IoT systems may employ multiple messaging protocols for supporting device heterogeneity and different message exchange patterns. In addition, basic similarities among existing several messaging protocols or frameworks that exist today for exchanging IoT data within IoT systems suggest the potential of interoperability. Given that IoT systems help facilitate the interconnectivity among distributed, heterogeneous entities, interoperability among existing messaging protocols will play an increasingly important role in simplifying the development and deployment of IoT systems. In this paper, we present a comprehensive review of the existing messaging protocols that can be used in deploying IoT systems. Throughout this paper, we highlight the protocols' distinctive approaches and applicability of using them across various IoT environments. In addition, we highlight challenges, strengths and weaknesses of these messaging protocols in the context of IoT.","2169-3536","","10.1109/ACCESS.2020.2993363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9090208","Internet of Things;IoT;HTTP;MQTT;CoAP;AMQP;XMPP;DDS;data distribution service;constrained application protocol;message queuing telemetry transport;extensible messaging and presence protocol;HyperText transfer protocol;edge computing;fog computing;cloud applications","Protocols;Internet of Things;Object recognition;Open systems;Middleware;Wireless sensor networks;Hardware","","123","","176","CCBY","8 May 2020","","","IEEE","IEEE Journals"
"Distributed Artificial Intelligence Empowered by End-Edge-Cloud Computing: A Survey","S. Duan; D. Wang; J. Ren; F. Lyu; Y. Zhang; H. Wu; X. Shen","School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, China; School of Computer Science and Engineering, Central South University, Changsha, China; Computer School, Beijing Information Science and Technology University, Beijing, China; Department of Electrical and Software Engineering, University of Calgary, Calgary, Canada; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada",IEEE Communications Surveys & Tutorials,"23 Feb 2023","2023","25","1","591","624","As the computing paradigm shifts from cloud computing to end-edge-cloud computing, it also supports artificial intelligence evolving from a centralized manner to a distributed one. In this paper, we provide a comprehensive survey on the distributed artificial intelligence (DAI) empowered by end-edge-cloud computing (EECC), where the heterogeneous capabilities of on-device computing, edge computing, and cloud computing are orchestrated to satisfy the diverse requirements raised by resource-intensive and distributed AI computation. Particularly, we first introduce several mainstream computing paradigms and the benefits of the EECC paradigm in supporting distributed AI, as well as the fundamental technologies for distributed AI. We then derive a holistic taxonomy for the state-of-the-art optimization technologies that are empowered by EECC to boost distributed training and inference, respectively. After that, we point out security and privacy threats in DAI-EECC architecture and review the benefits and shortcomings of each enabling defense technology in accordance with the threats. Finally, we present some promising applications enabled by DAI-EECC and highlight several research challenges and open issues toward immersive performance acquisition.","1553-877X","","10.1109/COMST.2022.3218527","National Key Research and Development Program of China(grant numbers:2022YFF0604502); National Natural Science Foundation of China(grant numbers:62122095,62002389,62072472,U19A2067); Grant from the Guoqiang Institute, Tsinghua University; Natural Science Foundation of Hunan Province, China(grant numbers:2020JJ2050,2021JJ20079); Young Elite Scientist Sponsorship Program by CAST(grant numbers:YESS20200238); Young Talents Plan of Hunan Province of China(grant numbers:2021RC3004); Higher Education Discipline Innovation Project(grant numbers:B18059); Central South University Innovation-Driven Research Programme(grant numbers:2023CXQD029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9933792","Distributed artificial intelligence;end-edge-cloud computing;network computing;federated learning","Artificial intelligence;Cloud computing;Privacy;Security;Training;Edge computing;Servers","","115","","318","IEEE","1 Nov 2022","","","IEEE","IEEE Journals"
"Edge Computing for Internet of Everything: A Survey","X. Kong; Y. Wu; H. Wang; F. Xia","College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; School of Software, Dalian University of Technology, Dalian, China; School of Software, Dalian University of Technology, Dalian, China; Institute of Innovation, Science and Sustainability, Federation University Australia, Ballarat, VIC, Australia",IEEE Internet of Things Journal,"18 Nov 2022","2022","9","23","23472","23485","In this era of the Internet of Everything (IoE), edge computing has emerged as the critical enabling technology to solve a series of issues caused by an increasing amount of interconnected devices and large-scale data transmission. However, the deficiencies of edge computing paradigm are gradually being magnified in the context of IoE, especially in terms of service migration, security and privacy preservation, and deployment issues of edge node. These issues can not be well addressed by conventional approaches. Thanks to the rapid development of upcoming technologies, such as artificial intelligence (AI), blockchain, and microservices, novel and more effective solutions have emerged and been applied to solve existing challenges. In addition, edge computing can be deeply integrated with technologies in other domains (e.g., AI, blockchain, 6G, and digital twin) through interdisciplinary intersection and practice, releasing the potential for mutual benefit. These promising integrations need to be further explored and researched. In addition, edge computing provides strong support in applications scenarios, such as remote working, new physical retail industries, and digital advertising, which has greatly changed the way we live, work, and study. In this article, we present an up-to-date survey of the edge computing research. In addition to introducing the definition, model, and characteristics of edge computing, we discuss a set of key issues in edge computing and novel solutions supported by emerging technologies in IoE era. Furthermore, we explore the potential and promising trends from the perspective of technology integration. Finally, new application scenarios and the final form of edge computing are discussed.","2327-4662","","10.1109/JIOT.2022.3200431","National Natural Science Foundation of China(grant numbers:62072409); Zhejiang Provincial Natural Science Foundation(grant numbers:LR21F020003); Fundamental Research Funds for the Provincial Universities of Zhejiang(grant numbers:RF-B2020001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9863881","6G;artificial intelligence (AI);blockchain;digital twin (DT);edge computing;microservices","Edge computing;Cloud computing;Internet of Things;Blockchains;Security;Computational modeling;Task analysis","","109","","99","IEEE","22 Aug 2022","","","IEEE","IEEE Journals"
"KEIDS: Kubernetes-Based Energy and Interference Driven Scheduler for Industrial IoT in Edge-Cloud Ecosystem","K. Kaur; S. Garg; G. Kaddoum; S. H. Ahmed; M. Atiquzzaman","Electrical Engineering Department, École de technologie supérieure, Université du Québec, Montreal, Canada; Electrical Engineering Department, École de technologie supérieure, Université du Québec, Montreal, Canada; Electrical Engineering Department, École de technologie supérieure, Université du Québec, Montreal, Canada; Department of Computer Science, Georgia Southern University, Statesboro, USA; School of Computer Science, University of Oklahoma, Norman, USA",IEEE Internet of Things Journal,"13 May 2020","2020","7","5","4228","4237","With the rapid explosion of Industrial Internet of Things (IIoT), the need for real-time data processing with enhanced flexibility and scalability has increased manifold. However, the newly evolved containerization technology offers lucrative advantages in comparison to the conventional virtual machines. However, management of these light-weight containers is a tedious task, but Google Kubernetes offers a consolidated container management and scheduling for successful execution of various lightweight containers. Nevertheless, the existing Kubernetes solutions fall short in efficiently handling the “interference” and “energy minimization” challenges in IIoT set-up. Hence, in this article, we present a competent controller, named Kubernetes-based energy and interference driven scheduler (KEIDS), for container management on edge-cloud nodes taking into account the emission of carbon footprints, interference, and energy consumption. The problem of task scheduling has been formulated using integer linear programming based on multiobjective optimization problem. In detail, KEIDS minimizes the energy utilization of edge-cloud nodes in IIoT for optimal green energy utilization. Henceforth, the applications are scheduled on the available nodes in less time with minimum interference from other applications, which in turn guarantees an optimal performance to the end-users. An extensive evaluation of the proposed KEIDS scheduler in comparison to the existing state-of-the-art schemes indicates its superior performance on real-time data acquired from Google compute cluster.","2327-4662","","10.1109/JIOT.2019.2939534","Natural Sciences and Engineering Research Council of Canada (NSERC); Tier 2 Canada Research Chair on the Next Generations of Wireless IoT Networks; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8825476","Cloud computing;edge computing;energy minimization;Industrial Internet of Things (IIoT);job scheduling;Kubernetes and multiobjective optimization","Cloud computing;Internet of Things;Interference;Containers;Task analysis;Minimization;Resource management","","108","","39","IEEE","5 Sep 2019","","","IEEE","IEEE Journals"
"Research on Artificial Intelligence Enhancing Internet of Things Security: A Survey","H. Wu; H. Han; X. Wang; S. Sun","School of Software and Microelectronics, Peking University, Beijing, China; Department of Food and Resource Economics, University of Copenhagen, Copenhagen, Denmark; School of Software and Microelectronics, Peking University, Beijing, China; School of Software and Microelectronics, Peking University, Beijing, China",IEEE Access,"1 Sep 2020","2020","8","","153826","153848","Through three development routes of authentication, communication, and computing, the Internet of Things (IoT) has become a variety of innovative integrated solutions for specific applications. However, due to the openness, extensiveness and resource constraints of IoT, each layer of the three-tier IoT architecture suffers from a variety of security threats. In this work, we systematically review the particularity and complexity of IoT security protection, and then find that Artificial Intelligence (AI) methods such as Machine Learning (ML) and Deep Learning (DL) can provide new powerful capabilities to meet the security requirements of IoT. We analyze the technical feasibility of AI in solving IoT security problems and summarize a general process of AI solutions for IoT security. For four serious IoT security threats: device authentication, Denial of Service (DoS) and Distributed Denial of Service (DDoS) attacks defense, intrusion detection and malware detection, we summarize representative AI solutions and compare the different algorithms and technologies used by various solutions. It should be noted that although AI provides many new capabilities for the security protection of IoT, it also brings new potential challenges and possible negative effects to IoT in terms of data, algorithm and architecture. In the future, how to solve these challenges can serve as potential research directions.","2169-3536","","10.1109/ACCESS.2020.3018170","National Basic Research Program of China (973 Program)(grant numbers:2018YFB1402900,2018YFB1403000); Natural Science Foundation of Jiangsu Province(grant numbers:BK20151132); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9172062","Artificial intelligence;deep learning;Internet of Things;machine learning;security","Internet of Things;Artificial intelligence;Authentication;Computer architecture;Software;Computer crime","","108","","100","CCBY","20 Aug 2020","","","IEEE","IEEE Journals"
"A Smart System for Sleep Monitoring by Integrating IoT With Big Data Analytics","D. C. Yacchirema; D. Sarabia-JáCome; C. E. Palau; M. Esteve","Departamento de Informática y Ciencias de la Computación, Escuela Politécnica Nacional, Quito, Ecuador; Communication Department, Universitat Politècnica de València, Valencia, Spain; Communication Department, Universitat Politècnica de València, Valencia, Spain; Communication Department, Universitat Politècnica de València, Valencia, Spain",IEEE Access,"16 Jul 2018","2018","6","","35988","36001","Obtrusive sleep apnea (OSA) is one of the most important sleep disorders because it has a direct adverse impact on the quality of life. Intellectual deterioration, decreased psychomotor performance, behavior, and personality disorders are some of the consequences of OSA. Therefore, a real-time monitoring of this disorder is a critical need in healthcare solutions. There are several systems for OSA detection. Nevertheless, despite their promising results, these systems not guiding their treatment. For these reasons, this research presents an innovative system for both to detect and support of treatment of OSA of elderly people by monitoring multiple factors such as sleep environment, sleep status, physical activities, and physiological parameters as well as the use of open data available in smart cities. Our system architecture performs two types of processing. On the one hand, a pre-processing based on rules that enables the sending of real-time notifications to responsible for the care of elderly, in the event of an emergency situation. This pre-processing is essentially based on a fog computing approach implemented in a smart device operating at the edge of the network that additionally offers advanced interoperability services: technical, syntactic, and semantic. On the other hand, a batch data processing that enables a descriptive analysis that statistically details the behavior of the data and a predictive analysis for the development of services, such as predicting the least polluted place to perform outdoor activities. This processing uses big data tools on cloud computing. The performed experiments show a 93.3% of effectivity in the air quality index prediction to guide the OSA treatment. The system's performance has been evaluated in terms of latency. The achieved results clearly demonstrate that the pre-processing of data at the edge of the network improves the efficiency of the system.","2169-3536","","10.1109/ACCESS.2018.2849822","European Union’s Horizon 2020 Research and Innovation Programme through the Interoperability of Heterogeneous IoT Platforms Project (INTER-IoT)(grant numbers:687283); Escuela Politécnica Nacional, Ecuador; Secretaría de Educación Superior, Ciencia, Tecnología e Innovación; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8392685","Internet-of-Things;big data;interoperability;sleep monitoring;health monitoring;open data;fog computing;cloud computing","Sleep apnea;Monitoring;Biomedical monitoring;Real-time systems;Computer architecture;Big Data;Sensors","","107","","27","CCBY","22 Jun 2018","","","IEEE","IEEE Journals"
"A Survey on the Integration of Blockchain With IoT to Enhance Performance and Eliminate Challenges","A. A. Sadawi; M. S. Hassan; M. Ndiaye","Department of Engineering Systems Management, American University of Sharjah, Sharjah, United Arab Emirates; Department of Electrical Engineering, American University of Sharjah, Sharjah, United Arab Emirates; Department of Engineering Systems Management, American University of Sharjah, Sharjah, United Arab Emirates",IEEE Access,"13 Apr 2021","2021","9","","54478","54497","Internet of things IoT is playing a remarkable role in the advancement of many fields such as healthcare, smart grids, supply chain management, etc. It also eases people's daily lives and enhances their interaction with each other as well as with their surroundings and the environment in a broader scope. IoT performs this role utilizing devices and sensors of different shapes and sizes ranging from small embedded sensors and wearable devices all the way to automated systems. However, IoT networks are growing in size, complexity, and number of connected devices. As a result, many challenges and problems arise such as security, authenticity, reliability, and scalability. Based on that and taking into account the anticipated evolution of the IoT, it is extremely vital not only to maintain but to increase confidence in and reliance on IoT systems by tackling the aforementioned issues. The emergence of blockchain opened the door to solve some challenges related to IoT networks. Blockchain characteristics such as security, transparency, reliability, and traceability make it the perfect candidate to improve IoT systems, solve their problems, and support their future expansion. This paper demonstrates the major challenges facing IoT systems and blockchain's proposed role in solving them. It also evaluates the position of current researches in the field of merging blockchain with IoT networks and the latest implementation stages. Additionally, it discusses the issues related to the IoT-blockchain integration itself. Finally, this research proposes an architectural design to integrate IoT with blockchain in two layers using dew and cloudlet computing. Our aim is to benefit from blockchain features and services to guarantee a decentralized data storage and processing and address security and anonymity challenges and achieve transparency and efficient authentication service.","2169-3536","","10.1109/ACCESS.2021.3070555","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9393912","Blockchain;IoT;smart contract;trust;IoT challenges;IoT security;decentralized IoT;cloudlet computing;dew computing;cloudlet-dew architecture","Internet of Things;Blockchain;Security;Sensors;Reliability;Performance evaluation;Wireless sensor networks","","103","","91","CCBY","2 Apr 2021","","","IEEE","IEEE Journals"
"Evaluating Performance of Containerized IoT Services for Clustered Devices at the Network Edge","R. Morabito; I. Farris; A. Iera; T. Taleb","Ericsson Research, Kirkkonummi, Finland; Department of Communications and Networking, Aalto University, Espoo, Finland; DIIES Department, University “Mediterranea” of Reggio Calabria, Reggio Calabria, Italy; Department of Communications and Networking, Aalto University, Espoo, Finland",IEEE Internet of Things Journal,"8 Aug 2017","2017","4","4","1019","1030","The constant and fast increase in the number of heterogeneous Internet of Things (IoT) devices that populate everyday life environments brings new challenges to the full exploitation of the computation, memory, sensing, and actuation resources associated to them. In this context, device virtualization solutions and platforms may definitely play a key role in enabling the desired tradeoff between flexibility and performance. This paper focuses on lightweight virtualization technologies for IoT devices, suitably thought to effectively deploy new integrated applications and to create a novel distributed and virtualized ecosystem. Two different frameworks for container-based IoT service provisioning are compared, the one based on a direct interaction between two cooperating devices and the other based on the presence of a manager supervising the operations between cooperating devices forming a cluster. In the latter case, accounting for the growing impetus to move intelligence toward the edge of the network, management features are implemented at the network access point to provide short latency responses. We also introduce the outcomes of a thorough performance evaluation campaign conducted via a real IoT testbed. The measurements, performed by accounting for the constraints of typical IoT nodes, shed light on the actual feasibility of container-based IoT frameworks.","2327-4662","","10.1109/JIOT.2017.2714638","FP7 Marie Curie Initial Training Network METRICS Project(grant numbers:607728); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7945515","Container virtualization;Internet of Things (IoT);multi-access edge computing;service orchestration","Containers;Virtualization;Cloud computing;Edge computing;Performance evaluation;Engines;Internet of Things","","98","","41","IEEE","9 Jun 2017","","","IEEE","IEEE Journals"
"Dyme: Dynamic Microservice Scheduling in Edge Computing Enabled IoT","A. Samanta; J. Tang","Shien-Ming Wu School of Intelligent Engineering, South China University of Technology, Guangzhou, China; Shien-Ming Wu School of Intelligent Engineering, South China University of Technology, Guangzhou, China",IEEE Internet of Things Journal,"10 Jul 2020","2020","7","7","6164","6174","In recent years, the rapid development of mobile edge computing (MEC) provides an efficient execution platform at the edge for Internet-of-Things (IoT) applications. Nevertheless, the MEC also provides optimal resources to different microservices, however, underlying network conditions and infrastructures inherently affect the execution process in MEC. Therefore, in the presence of varying network conditions, it is necessary to optimally execute the available task of end users while maximizing the energy efficiency in edge platform and we also need to provide fair Quality-of-Service (QoS). On the other hand, it is necessary to schedule the microservices dynamically to minimize the total network delay and network price. Thus, in this article, unlike most of the existing works, we propose a dynamic microservice scheduling scheme for MEC. We design the microservice scheduling framework mathematically and also discuss the computational complexity of the scheduling algorithm. Extensive simulation results show that the microservice scheduling framework significantly improves the performance metrics in terms of total network delay, average price, satisfaction level, energy consumption rate (ECR), failure rate, and network throughput over other existing baselines.","2327-4662","","10.1109/JIOT.2020.2981958","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9042873","Dynamic microservice scheduling;edge computing;Internet of Things (IoT);microservice;Quality-of-Service (QoS)","Dynamic scheduling;Task analysis;Internet of Things;Delays;Quality of service;Processor scheduling;Edge computing","","98","","56","IEEE","19 Mar 2020","","","IEEE","IEEE Journals"
"Online Orchestration of Cross-Edge Service Function Chaining for Cost-Efficient Edge Computing","Z. Zhou; Q. Wu; X. Chen","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China",IEEE Journal on Selected Areas in Communications,"6 Aug 2019","2019","37","8","1866","1880","Edge computing (EC) has quickly ascended to be the de-facto standard for hosting emerging low-latency applications, as exemplified by intelligent video surveillance, Internet of Vehicles, and augmented reality. For EC, service function chaining is envisioned as a promising approach to configure various services in an agile, flexible, and cost-efficient manner. When running on top of geographically dispersed edge clouds, fully unleashing the benefits of service function chaining is, however, by no means trivial. In this paper, we propose an online orchestration framework for cross-edge service function chaining, which aims to maximize the holistic cost efficiency, via jointly optimizing the resource provisioning and traffic routing on-the-fly. This long-term cost minimization problem is difficult since it is NP-hard and involves future uncertain information. To simultaneously address these dual challenges, we carefully combine an online optimization technique with an approximate optimization method in a joint optimization framework, through: 1) decomposing the long-term problem into a series of one-shot fractional problem with a regularization technique and 2) rounding the fractional solution to a near-optimal integral solution with a randomized dependent scheme that preserves the solution feasibility. The resulting online algorithm achieves an outstanding performance guarantee, as verified by both rigorous theoretical analysis and extensive trace-driven simulations.","1558-0008","","10.1109/JSAC.2019.2927070","National Basic Research Program of China (973 Program)(grant numbers:2017YFB1001703); National Natural Science Foundation of China(grant numbers:U1711265,61802449); Program for Guangdong Introducing Innovative and Enterpreneurial Teams(grant numbers:2017ZT07X355); Guangdong Natural Science Funds(grant numbers:2018A030313032); Fundamental Research Funds for the Central Universities(grant numbers:17lgjc40); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8755998","Edge Computing;Service Function Chaining;Online Optimization","Cloud computing;Edge computing;Routing;Minimization;Computational modeling;Heuristic algorithms;Internet of Things","","95","","38","IEEE","5 Jul 2019","","","IEEE","IEEE Journals"
"Comprehensive Survey on T-SDN: Software-Defined Networking for Transport Networks","R. Alvizu; G. Maier; N. Kukreja; A. Pattavina; R. Morro; A. Capello; C. Cavazzoni","SWAN Networks, Politecnico di Milano, Milan, Italy; SWAN Networks, Politecnico di Milano, Milan, Italy; Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milan, Italy; SWAN Networks, Politecnico di Milano, Milan, Italy; Telecom Italia SpA, Turin, Italy; Telecom Italia SpA, Turin, Italy; Telecom Italia SpA, Turin, Italy",IEEE Communications Surveys & Tutorials,"20 Nov 2017","2017","19","4","2232","2283","Paradoxically, with an ever-increasing traffic demand, today transport-network operators experience a progressive erosion of their margins. The alarms of change are set, and software define networking (SDN) is coming to the rescue with the promise of reducing capital expenditures and operational expenses. Driven by economic needs and network innovation facilities, today transport SDN (T-SDN) is a reality. It gained big momentum in the last years, however, in the networking industry, the transport network will be perhaps the last segment to embrace SDN, mainly due to the heterogeneous nature and complexity of the optical equipment composing it. This survey guides the reader through a fascinating technological adventure that provides an organic analysis of the T-SDN development and evolution considering contributions from: academic research, standardization bodies, industrial development, open source projects, and alliances among them. After creating a comprehensive picture of T-SDN, we provide an analysis of many open issues that are expected to need significant future work, and give our vision in this path toward a fully programmable and dynamic transport network.","1553-877X","","10.1109/COMST.2017.2715220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7947156","Software defined networking;optical transport network;transport SDN;software defined optical networking;network programmability;orchestration;transport API;network controller;network virtualization;network function virtualization;OpenFlow;GMPLS","Optical fiber networks;Hardware;Technological innovation;Operating systems;Optical sensors;Tutorials","","94","","250","IEEE","13 Jun 2017","","","IEEE","IEEE Journals"
"Design and Implementation of Open LoRa for IoT","Q. Zhou; K. Zheng; L. Hou; J. Xing; R. Xu","Intelligent Computing and Communication (IC) Laboratory, Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Intelligent Computing and Communication (IC) Laboratory, Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Intelligent Computing and Communication (IC) Laboratory, Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Intelligent Computing and Communication (IC) Laboratory, Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, Beijing, China",IEEE Access,"6 Aug 2019","2019","7","","100649","100657","Long Range (LoRa) network is emerging as one of the most promising low-power wide-area (LPWA) networks, since it enables the energy-constraint devices distributed over wide areas to establish affordable connectivity. However, how to implement a cost-effective and flexible LoRa network is still an open challenge. This paper aims at exposing a feasible solution of design and implementation, allowing users to conveniently build a private LoRa network for various IoT applications. First, several typical application scenarios of LoRa network are discussed. Then, the LoRa system architecture is presented with the functionality of each component. We address the hardware design and implementation of LoRa Gateway, which is the bridge between LoRa nodes and LoRa network server. Especially, this paper contributes by proposing an improved software architecture of LoRa network server whose source codes are open on GitHub. Under the architecture, LoRa network server is divided into four decoupled modules and uses the messaging system based on streaming data for the interaction between modules to guarantee scalability and flexibility. Finally, the extensive experiments are conducted to evaluate the performance of LoRa networks in typical environments.","2169-3536","","10.1109/ACCESS.2019.2930243","National Natural Science Foundation of China(grant numbers:61671089); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8768288","LoRa;LPWA;IoT;microservice;open source","Network servers;Logic gates;Servers;Internet of Things;Hardware;Computer architecture;Sensors","","93","","24","CCBY","22 Jul 2019","","","IEEE","IEEE Journals"
"A Comprehensive Survey of the Tactile Internet: State-of-the-Art and Research Directions","N. Promwongsa; A. Ebrahimzadeh; D. Naboulsi; S. Kianpisheh; F. Belqasmi; R. Glitho; N. Crespi; O. Alfandi","CIISE, Concordia University, Montréal, QC, Canada; CIISE, Concordia University, Montréal, QC, Canada; Département de Génie Logiciel et des Technologies de l’information, école de Technologie Supérieure, Montréal, QC, Canada; CIISE, Concordia University, Montréal, QC, Canada; College of Technological Innovation, Zayed University, Abu Dhabi, UAE; CIISE, Concordia University, Montréal, QC, Canada; Institut Mines-Telecom, Telecom SudParis, Evry, France; College of Technological Innovation, Zayed University, Abu Dhabi, UAE",IEEE Communications Surveys & Tutorials,"23 Feb 2021","2021","23","1","472","523","The Internet has made several giant leaps over the years, from a fixed to a mobile Internet, then to the Internet of Things, and now to a Tactile Internet. The Tactile Internet goes far beyond data, audio and video delivery over fixed and mobile networks, and even beyond allowing communication and collaboration among things. It is expected to enable haptic communications and allow skill set delivery over networks. Some examples of potential applications are tele-surgery, vehicle fleets, augmented reality and industrial process automation. Several papers already cover many of the Tactile Internet-related concepts and technologies, such as haptic codecs, applications, and supporting technologies. However, none of them offers a comprehensive survey of the Tactile Internet, including its architectures and algorithms. Furthermore, none of them provides a systematic and critical review of the existing solutions. To address these lacunae, we provide a comprehensive survey of the architectures and algorithms proposed to date for the Tactile Internet. In addition, we critically review them using a well-defined set of requirements and discuss some of the lessons learned as well as the most promising research directions.","1553-877X","","10.1109/COMST.2020.3025995","Canadian Natural Sciences and Engineering Research Council and by Ericsson through a CRD Grant; Canada Research Chair Program and Zayed University (United Arab Emirates) through the Research Incentive Fund Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9203885","5G/6G;artificial intelligence;edge computing;machine learning;tactile Internet","Haptic interfaces;Tutorials;Computer architecture;Surgery;Internet of Things;Reliability","","93","","192","CCBY","22 Sep 2020","","","IEEE","IEEE Journals"
"Machine Learning-Based Scaling Management for Kubernetes Edge Clusters","L. Toka; G. Dobreff; B. Fodor; B. Sonkoly","MTA-BME Network Softwarization Research Group, Budapest University of Technology and Economics, Budapest, Hungary; MTA-BME Network Softwarization Research Group, Budapest University of Technology and Economics, Budapest, Hungary; MTA-BME Network Softwarization Research Group, Budapest University of Technology and Economics, Budapest, Hungary; MTA-BME Network Softwarization Research Group, Budapest University of Technology and Economics, Budapest, Hungary",IEEE Transactions on Network and Service Management,"10 Mar 2021","2021","18","1","958","972","Kubernetes, the container orchestrator for cloud-deployed applications, offers automatic scaling for the application provider in order to meet the ever-changing intensity of processing demand. This auto-scaling feature can be customized with a parameter set, but those management parameters are static while incoming Web request dynamics often change, not to mention the fact that scaling decisions are inherently reactive, instead of being proactive. We set the ultimate goal of making cloud-based applications' management easier and more effective. We propose a Kubernetes scaling engine that makes the auto-scaling decisions apt for handling the actual variability of incoming requests. In this engine various machine learning forecast methods compete with each other via a short-term evaluation loop in order to always give the lead to the method that suits best the actual request dynamics. We also introduce a compact management parameter for the cloud-tenant application provider to easily set their sweet spot in the resource over-provisioning vs. SLA violation trade-off. We motivate our scaling solution with analytical modeling and evaluation of the current Kubernetes behavior. The multi-forecast scaling engine and the proposed management parameter are evaluated both in simulations and with measurements on our collected Web traces to show the improved quality of fitting provisioned resources to service demand. We find that with just a few, but fundamentally different, and competing forecast methods, our auto-scaler engine, implemented in Kubernetes, results in significantly fewer lost requests with just slightly more provisioned resources compared to the default baseline.","1932-4537","","10.1109/TNSM.2021.3052837","National Research, Development and Innovation Fund of Hungary under the FK_20 funding scheme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9328525","Cloud computing;machine learning;auto-scaling;Kubernetes;forecast;resource management","Cloud computing;Containers;Engines;Servers;Resource management;Analytical models;Adaptation models","","92","","44","IEEE","19 Jan 2021","","","IEEE","IEEE Journals"
"Zero Trust Architecture (ZTA): A Comprehensive Survey","N. F. Syed; S. W. Shah; A. Shaghaghi; A. Anwar; Z. Baig; R. Doss","Centre for Cyber Security Research and Innovation (CSRI), Deakin University, Geelong, VIC, Australia; Centre for Cyber Security Research and Innovation (CSRI), Deakin University, Geelong, VIC, Australia; Centre for Cyber Security Research and Innovation (CSRI), Deakin University, Geelong, VIC, Australia; Centre for Cyber Security Research and Innovation (CSRI), Deakin University, Geelong, VIC, Australia; Centre for Cyber Security Research and Innovation (CSRI), Deakin University, Geelong, VIC, Australia; Centre for Cyber Security Research and Innovation (CSRI), Deakin University, Geelong, VIC, Australia",IEEE Access,"7 Jun 2022","2022","10","","57143","57179","We present a detailed survey of the Zero Trust (ZT) security paradigm which has a growing number of advocates in the critical infrastructure risk management space. The article employs a descriptive approach to present the fundamental tenets of ZT and provides a review of numerous potential options available for successful realization of this paradigm. We describe the role of authentication and access control in Zero Trust Architectures (ZTA) and present an in-depth discussion of state-of-the-art techniques for authentication and access control in different scenarios. Furthermore, we comprehensively discuss the conventional approaches to encryption, micro-segmentation, and security automation available for instantiating a ZTA. The article also details various challenges associated with contemporary authentication mechanisms, access control schemes, trust and risk computation techniques, micro-segmentation approaches, and Software-Defined Perimeter, that can impact the implementation of ZT in its true sense. Based upon our analysis, we finally pinpoint the potential future research directions for successful realization of ZT in critical infrastructures.","2169-3536","","10.1109/ACCESS.2022.3174679","Australian Department of Defence; Defence Science and Technology Group’s Operations Research Network; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9773102","Zero trust architecture (ZTA);access control;authentication;micro-segmentation;software-defined parameter (SDP)","Access control;Authentication;Computer architecture;NIST;Encryption;Critical infrastructure;Automation","","91","","190","CCBY","12 May 2022","","","IEEE","IEEE Journals"
"A Roadmap Toward the Resilient Internet of Things for Cyber-Physical Systems","D. Ratasich; F. Khalid; F. Geissler; R. Grosu; M. Shafique; E. Bartocci","Institute of Computer Engineering, TU Wien, Vienna, Austria; Institute of Computer Engineering, TU Wien, Vienna, Austria; CPS Dependability Research Lab, Intel Deutschland GmbH, Neubiberg, Germany; Institute of Computer Engineering, TU Wien, Vienna, Austria; Institute of Computer Engineering, TU Wien, Vienna, Austria; Institute of Computer Engineering, TU Wien, Vienna, Austria",IEEE Access,"5 Feb 2019","2019","7","","13260","13283","The Internet of Things (IoT) is a ubiquitous system connecting many different devices - the things - which can be accessed from the distance. The cyber-physical systems (CPSs) monitor and control the things from the distance. As a result, the concepts of dependability and security get deeply intertwined. The increasing level of dynamicity, heterogeneity, and complexity adds to the system's vulnerability, and challenges its ability to react to faults. This paper summarizes the state of the art of existing work on anomaly detection, fault-tolerance, and self-healing, and adds a number of other methods applicable to achieve resilience in an IoT. We particularly focus on non-intrusive methods ensuring data integrity in the network. Furthermore, this paper presents the main challenges in building a resilient IoT for the CPS, which is crucial in the era of smart CPS with enhanced connectivity (an excellent example of such a system is connected autonomous vehicles). It further summarizes our solutions, work-in-progress and future work to this topic to enable “Trustworthy IoT for CPS”. Finally, this framework is illustrated on a selected use case: a smart sensor infrastructure in the transport domain.","2169-3536","","10.1109/ACCESS.2019.2891969","IoT4CPS; Österreichische Forschungsförderungsgesellschaft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8606923","Anomaly detection;cyber-physical systems (CPS);Internet of Things (IoT);monitoring;resilience;long-term dependability and security;self-adaptation;self-healing","Resilience;Security;Internet of Things;Cyber-physical systems;Robustness;Safety;Monitoring","","89","","203","CCBY","10 Jan 2019","","","IEEE","IEEE Journals"
"Multi-Access Edge Computing Architecture, Data Security and Privacy: A Review","B. Ali; M. A. Gregory; S. Li","School of Engineering, RMIT University, Melbourne, VIC, Australia; School of Engineering, RMIT University, Melbourne, VIC, Australia; School of Engineering, RMIT University, Melbourne, VIC, Australia",IEEE Access,"17 Feb 2021","2021","9","","18706","18721","Multi-Access Edge Computing (MEC) is an extension of cloud computing that aims to provide computation, storage, and networking capabilities at the edge of the network in close proximity to end-users. The MEC architecture supports applications and services that bridge between cloud computing and end-users. The architecture includes devices and systems that are interconnected, layered, and flexibly deployed. As a result of the technological advancements, MEC is facing a myriad of highly sophisticated threats. This paper provides a review of MEC Architecture, use cases, conceptual guidelines for MEC security architecture, security and privacy techniques, and identifies current and future challenges, their implications, and approaches to overcome the challenges. This research examined significant threats, described the MEC architecture, identified the susceptible functional layers, the different categories of threats, and the potential security safeguards. The research recommends that MEC providers should implement multiple layers of security controls to mitigate targeted attacks.","2169-3536","","10.1109/ACCESS.2021.3053233","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9330515","Multi-access edge computing;MEC;security;privacy;SDN;5G","Security;Computer architecture;Data security;Cloud computing;Servers;Data privacy;Edge computing","","85","","105","CCBY","21 Jan 2021","","","IEEE","IEEE Journals"
"Green Mobile Networks for 5G and Beyond","M. Masoudi; M. G. Khafagy; A. Conte; A. El-Amine; B. Françoise; C. Nadjahi; F. E. Salem; W. Labidi; A. Süral; A. Gati; D. Bodéré; E. Arikan; F. Aklamanu; H. Louahlia-Gualous; J. Lallet; K. Pareek; L. Nuaymi; L. Meunier; P. Silva; N. T. Almeida; T. Chahed; T. Sjölund; C. Cavdar","KTH Royal Institute of Technology, Kista, Sweden; KTH Royal Institute of Technology, Kista, Sweden; Nokia Bell Labs, Nozay, France; Departement Système Réseaux, Cybersécurité et Droit du numérique (SRCD), IMT Atlantique, CS 17607 35576, Cesson-Sévigné, France; Orange Gardens, Chátillon, France; LUSAC, UNICAEN, Normandie University, Caen, France; Orange Gardens, Chátillon, France; Institut Polytechnique Paris, Telecom SudParis, UMR CNRS Samovar, Evry, France; Polaran Ltd, Ankara, Turkey; Orange Gardens, Chátillon, France; Orange Gardens, Chátillon, France; Polaran Ltd, Ankara, Turkey; Nokia Bell Labs, Nozay, France; LUSAC, UNICAEN, Normandie University, Caen, France; Nokia Bell Labs, Nozay, France; MIC Nordic AB, Kista, Sweden; Departement Système Réseaux, Cybersécurité et Droit du numérique (SRCD), IMT Atlantique, CS 17607 35576, Cesson-Sévigné, France; Tele2, Sverige AB, Kista, Sweden; INESC TEC and Faculty of Engineering, University of Porto, Porto, Portugal; INESC TEC and Faculty of Engineering, University of Porto, Porto, Portugal; Institut Polytechnique Paris, Telecom SudParis, UMR CNRS Samovar, Evry, France; MIC Nordic AB, Kista, Sweden; KTH Royal Institute of Technology, Kista, Sweden",IEEE Access,"14 Aug 2019","2019","7","","107270","107299","The heated 5G network deployment race has already begun with the rapid progress in standardization efforts, backed by the current market availability of 5G-enabled network equipment, ongoing 5G spectrum auctions, early launching of non-standalone 5G network services in a few countries, among others. In this paper, we study current and future wireless networks from the viewpoint of energy efficiency (EE) and sustainability to meet the planned network and service evolution toward, along, and beyond 5G, as also inspired by the findings of the EU Celtic-Plus SooGREEN Project. We highlight the opportunities seized by the project efforts to enable and enrich this green nature of the network as compared to existing technologies. In specific, we present innovative means proposed in SooGREEN to monitor and evaluate EE in 5G networks and beyond. Further solutions are presented to reduce energy consumption and carbon footprint in the different network segments. The latter spans proposed virtualized/cloud architectures, efficient polar coding for fronthauling, mobile network powering via renewable energy and smart grid integration, passive cooling, smart sleeping modes in indoor systems, among others. Finally, we shed light on the open opportunities yet to be investigated and leveraged in future developments.","2169-3536","","10.1109/ACCESS.2019.2932777","EU Celtic Plus Projects, mainly by SooGREEN: Service-Oriented Optimization of Green Mobile Networks and in part by AI4Green: Artificial Intelligence for Green Mobile Networks; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8786138","CRAN;DAS;energy efficiency;monitoring;storage;green mobile networks;passive cooling;renewable energy;sleep modes;smart grid;virtualization;Wi-Fi","Energy consumption;5G mobile communication;Green products;Optimization;Sustainable development;Cooling;Quality of service","","84","","73","CCBY","5 Aug 2019","","","IEEE","IEEE Journals"
"Application-Driven Network-Aware Digital Twin Management in Industrial Edge Environments","P. Bellavista; C. Giannelli; M. Mamei; M. Mendula; M. Picone","Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Mathematics and Computer Science, University of Ferrara, Ferrara, Italy; Department of Sciences and Methods for Engineering, University of Modena and Reggio Emilia, Modena, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Sciences and Methods for Engineering, University of Modena and Reggio Emilia, Modena, Italy",IEEE Transactions on Industrial Informatics,"30 Jul 2021","2021","17","11","7791","7801","The application of Internet of Things (IoT) within industrial environments is fostering the adoption of the digital twin (DT) approach, applied at the edge of the network to handle heterogeneity stemming from siloed application management solutions and from protocols originated by different manufacturing tools and enterprise services. In this challenging context, network heterogeneity also represents a critical element that can significantly limit the design and deployment of DT-oriented applications. This article proposes the Application-driven digital twin networking middleware with the twofold objective of: 1) Simplifying the interaction among heterogeneous devices by allowing DTs to exploit IP-based protocols instead of specialized industrial ones and to enhance packet content expressiveness, by enriching data via well-defined standards. 2) Dynamically managing network resources in edge industrial environments, applying software defined networking to exploit the communication mechanisms most suitable to application requirements, ranging from native IP to more articulated based on packet content.","1941-0050","","10.1109/TII.2021.3067447","Distributed Digital Twins for industrial SMEs: A big-data platform; EU under the call ICT-11-2018-2019(grant numbers:857191); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9382113","Application-driven management;digital twin (DT);edge computing;industrial IoT;software defined networking (SDN)","Digital twin;Quality of service;Protocols;Vibrations;Middleware;Production;Payloads","","84","","36","IEEE","19 Mar 2021","","","IEEE","IEEE Journals"
"Cloud-Native Network Slicing Using Software Defined Networking Based Multi-Access Edge Computing: A Survey","S. D. A. Shah; M. A. Gregory; S. Li","School of Engineering, RMIT University, Melbourne, VIC, Australia; School of Engineering, RMIT University, Melbourne, VIC, Australia; School of Engineering, RMIT University, Melbourne, VIC, Australia",IEEE Access,"20 Jan 2021","2021","9","","10903","10924","Fifth-Generation (5G) mobile cellular networks provide a promising platform for new, innovative and diverse IoT applications, such as ultra-reliable and low latency communication, real-time and dynamic data processing, intensive computation, and massive device connectivity. End-to-End (E2E) network slicing candidates present a promising approach to resource allocation and distribution that permit operators to flexibly provide scalable virtualized and dedicated logical networks over common physical infrastructure. Though network slicing promises the provision of services on demand, many of its use cases, such as self-driving cars and Google's Stadia, would require the integration of a Multi-Access Edge Computing (MEC) platform in 5G networks. Edge Computing is envisioned as one of the key drivers for 5G and Sixth-Generation (6G) mobile cellular networks, but its role in network slicing remains to be fully explored. We investigate MEC and network slicing for the provision of 5G service focused use cases. Recently, changes to the cloud-native 5G core are a focus with MEC use cases providing network scalability, elasticity, flexibility, and automation. A cloud-native microservices architecture, along with its potential use cases for 5G network slicing, is envisioned. This paper also elaborates on the recent advances made in enabling E2E network slicing, its enabling technologies, solutions, and current standardization efforts. Finally, this paper identifies open research issues and challenges and provides possible solutions and recommendations.","2169-3536","","10.1109/ACCESS.2021.3050155","Royal Melbourne Institute of Technology (RMIT) University Research Stipend Scholarship (RRSS); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9317860","Network slicing;software defined networking;multi-access edge computing;cloud native;ultra-reliable;low latency communication","5G mobile communication;Network slicing;Cloud computing;Computer architecture;Edge computing;Software;Servers","","83","","102","CCBY","8 Jan 2021","","","IEEE","IEEE Journals"
"Towards 6G-Enabled Internet of Vehicles: Security and Privacy","D. P. Moya Osorio; I. Ahmad; J. D. V. Sánchez; A. Gurtov; J. Scholliers; M. Kutila; P. Porambage","Centre for Wireless Communications, University of Oulu, Oulu, Finland; VTT Technical Research Centre of Finland Ltd., Espoo, Finland; Departamento de Electrónica, Telecomunicaciones y Redes de Información, Escuela Politécnica Nacional, Quito, Ecuador; Department of Computer and Information Science (IDA), Linköping University, Linköping, Sweden; VTT Technical Research Centre of Finland Ltd., Espoo, Finland; VTT Technical Research Centre of Finland Ltd., Espoo, Finland; Centre for Wireless Communications, University of Oulu, Oulu, Finland",IEEE Open Journal of the Communications Society,"3 Feb 2022","2022","3","","82","105","The conceptualisation of the sixth generation of mobile wireless networks (6G) has already started with some potential disruptive technologies resonating as enablers for driving the emergence of a number of innovative applications. Particularly, 6G will be a prominent supporter for the evolution towards a truly Intelligent Transportation System and the realization of the Smart City concept by fulfilling the limitations of 5G, once vehicular networks are becoming highly dynamic and complex with stringent requirements on ultra-low latency, high reliability, and massive connections. More importantly, providing security and privacy to such critical systems should be a top priority as vulnerabilities can be catastrophic, thus there are huge concerns regarding data collected from sensors, people and their habits. In this paper, we provide a timely deliberation of the role that promissory 6G enabling technologies such as artificial intelligence, network softwarisation, network slicing, blockchain, edge computing, intelligent reflecting surfaces, backscatter communications, terahertz links, visible light communications, physical layer authentication, and cell-free massive multiple-input multiple-output (MIMO) will play on providing the expected level of security and privacy for the Internet of Vehicles.","2644-125X","","10.1109/OJCOMS.2022.3143098","Academy of Finland 6G Flagship(grant numbers:318927,334280); 5G-SAFEplus Celtic+ Project; Business Finland; EU Project AirMOUR (Enabling Sustainable Airmobility in Urrban Contexts via Emergency and Medical Services)(grant numbers:101006601); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9681822","6G networks;Internet of Vehicles;privacy;security;vehicle-to-everything communications","Security;6G mobile communication;5G mobile communication;Privacy;Vehicle-to-everything;Long Term Evolution;Data privacy","","83","","153","CCBY","14 Jan 2022","","","IEEE","IEEE Journals"
"A Survey of Decentralizing Applications via Blockchain: The 5G and Beyond Perspective","K. Yue; Y. Zhang; Y. Chen; Y. Li; L. Zhao; C. Rong; L. Chen","College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; Science and Technology on Security Communication Laboratory, Institute of Southwestern Communication, Chengdu, China; Department of Electrical, Computer, and Biomedical Engineering, Ryerson University, Toronto, ON, Canada; Center for IP-Based Service Innovation, University of Stavanger, Stavanger, Norway; College of Computer Science, Sichuan University, Chengdu, China",IEEE Communications Surveys & Tutorials,"19 Nov 2021","2021","23","4","2191","2217","Trusted third parties (TTPs) are frequently used for serving as an authority to issue and verify transactions in applications. Although the TTP-based paradigm provides customers with convenience, it causes a whole set of inevitable problems such as security threats, privacy vulnerabilities, and censorship. The TTP-based paradigm is not suitable for all modern networks, e.g., 5G and beyond networks, which are been evolving to support ubiquitous, decentralized, and autonomous services. Driven by the vision of blockchain technologies, there has been a paradigm shift in applications, from TTP-based to decentralized-trust-based. Decentralized applications (DApps) with blockchains promise no trust on authorities, tackling the key challenges of security and privacy problems. A main thrust of blockchain research is to explore frameworks and paradigms for decentralizing applications, fostering a number of new designs ranging from network architectures to business models. Therefore, this paper provides a compact and concise survey on the state-of-the-art research of decentralizing applications with blockchain in the 5G and beyond perspective. We provide four burning 5G and beyond challenges and discuss five aspects of motivation for decentralizing applications with blockchain. Then, we define nine fundamental modules of blockchains and explain the potential influence of these modules on decentralization in depth. We also discuss the interrelation between decentralization and some desired blockchain properties. Particularly, we present the capabilities of blockchain for decentralizing applications through reviewing DApps for 5G and beyond. We clearly distinguish three blockchain paradigms and discuss how developers to make right choices for 5G and beyond. Finally, we highlight important learned lessons and open issues in applying blockchain for decentralizing applications. Lessons learned and open issues from this survey will facilitate the transformation of centralized applications to DApps.","1553-877X","","10.1109/COMST.2021.3115797","National Natural Science Foundation of China(grant numbers:62072319); Science and Technology on Communication Security Laboratory(grant numbers:6142103190415); Sichuan Science and Technology Program(grant numbers:2020YFG0254,2019JDTD0001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9548956","Blockchain;decentralize;DApp;5G and beyond;permissionless blockchain;permissioned blockchain","Blockchains;5G mobile communication;Security;Privacy;Smart contracts;Tutorials;Technological innovation","","81","","181","IEEE","27 Sep 2021","","","IEEE","IEEE Journals"
"AFED-EF: An Energy-Efficient VM Allocation Algorithm for IoT Applications in a Cloud Data Center","Z. Zhou; M. Shojafar; M. Alazab; J. Abawajy; F. Li","School of Computer Engineering and Applied Mathematics, Changsha University, Changsha, China; 5GIC & 6GIC, Institute for Communication Systems (ICS), University of Surrey, Guildford, U.K.; College of Engineering, IT and Environment, Charles Darwin University, Casuarina, NT, Australia; Faculty of Science, Engineering and Built Environment, Deakin University, Geelong, VIC, Australia; School of Computer Engineering and Applied Mathematics, Changsha University, Changsha, China",IEEE Transactions on Green Communications and Networking,"20 May 2021","2021","5","2","658","669","Cloud Data Centers (CDCs) have become a vital computing infrastructure for enterprises. However, CDCs consume substantial energy due to the increased demand for computing power, especially for the Internet of Things (IoT) applications. Although a great deal of research in green resource allocation algorithms have been proposed to reduce the energy consumption of the CDCs, existing approaches mostly focus on minimizing the number of active Physical Machines (PMs) and rarely address the issue of load fluctuation and energy efficiency of the Virtual Machine (VM) provisions jointly. Moreover, existing approaches lack mechanisms to consider and redirect the incoming traffics to appropriate resources to optimize the Quality of Services (QoSs) provided by the CDCs. We propose a novel adaptive energy-aware VM allocation and deployment mechanism called AFED-EF for IoT applications to handle these problems. The proposed algorithm can efficiently handle the fluctuation of load and has good performance during the VM allocation and placement. We carried out extensive experimental analysis using a real-world workload based on more than a thousand PlanetLab VMs. The experimental results illustrate that AFED-EF outperforms other energy-aware algorithms in energy consumption, Service Level Agreements (SLA) violation, and energy efficiency.","2473-2400","","10.1109/TGCN.2021.3067309","National Natural Science Foundation of China(grant numbers:61772088); Hunan Province Key Laboratory of Industrial Internet Technology and Security(grant numbers:2019TP1011); Marie Curie Global Fellowship funded by European Commission(grant numbers:MSCA-IF-GF-839255); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9382007","Cloud data center (CDC);Internet of Thing (IoT);energy efficiency;resource provision;virtual machine allocation (VMA);service level agreement (SLA)","Energy consumption;Cloud computing;Resource management;Internet of Things;Data centers;Servers;Quality of service","","80","","45","IEEE","19 Mar 2021","","","IEEE","IEEE Journals"
"Utilizing Blockchain to Overcome Cyber Security Concerns in the Internet of Things: A Review","B. Alotaibi","Department of Information Technology, University of Tabuk, Tabuk, Saudi Arabia",IEEE Sensors Journal,"14 Nov 2019","2019","19","23","10953","10971","The Internet of Things (IoT) is a wide network consisting of Internet-connected objects using installed software, such as home appliances, vehicles, and other entities embedded with sensors, actuators, radio-frequency identification (RFID), and electronics to exchange data. In the last two decades, numerous IoT solutions have been developed by small, medium-sized, and large enterprises to make our lives easier. Furthermore, private and academic researchers have extensively investigated some practical IoT solutions. The rapid expansion of IoT solutions accompanies numerous security concerns because the underlying IoT protocols and communication technologies have not considered security. Recently, blockchain has emerged to become one of the promising technologies that might overcome some of the IoT limitations (security limitations, in particular). Blockchain technology is a database ledger that uses a peer-to-peer (P2P) network and stores transactions and asset registries. Blockchain can be described as a mounting list of records (i.e., blocks) with the following properties: distributed, decentralized, immutable, and shared. This paper surveyed recent security advances to overcome IoT limitations using blockchain. In this article, the blockchain attempts to overcome IoT limitations that are related to cyber security have been classified into four categories: end-to-end traceability; data privacy and anonymity; identity verification and authentication; and confidentiality, data integrity, and availability (CIA). Intended as a guideline for future research, this paper also explores systematic processes.","1558-1748","","10.1109/JSEN.2019.2935035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8795541","Internet of Things;blockchain;cyber security;end-to-end traceability;data privacy;anonymity;identity verification;data integrity;big data;cloud computing","Internet of Things;Blockchain;Protocols;Radiofrequency identification;Computer crime","","79","","141","IEEE","13 Aug 2019","","","IEEE","IEEE Journals"
"Beyond 100 Gb/s: capacity, flexibility, and network optimization [Invited]","K. Roberts; Q. Zhuge; I. Monga; S. Gareau; C. Laperle","Ciena Corporation, Ottawa, Ontario K2H 8E9, Canada; Ciena Corporation, Ottawa, Ontario K2H 8E9, Canada; Energy Sciences Network (ESnet), Berkeley, California 94720, USA.; Ciena Corporation, Ottawa, Ontario K2H 8E9, Canada; Ciena Corporation, Ottawa, Ontario K2H 8E9, Canada",Journal of Optical Communications and Networking,"14 Apr 2017","2017","9","4","C12","C23","In this paper, we discuss building blocks that enable the exploitation of optical capacities beyond 100 Gb∕s. Optical networks will benefit from more flexibility and agility in their network elements, especially from coherent transceivers. To achieve capacities of 400 Gb∕s and more, coherent transceivers will operate at higher symbol rates. This will be made possible with higher bandwidth components using new electro-optic technologies implemented with indium phosphide and silicon photonics. Digital signal processing will benefit from new algorithms. Multi-dimensional modulation, of which some formats are already in existence in current flexible coherent transceivers, will provide improved tolerance to noise and fiber nonlinearities. Constellation shaping will further improve these tolerances while allowing a finer granularity in the selection of capacity. Frequency-division multiplexing will also provide improved tolerance to the nonlinear characteristics of fibers. Algorithms with reduced computation complexity will allow the implementation, at speeds, of direct pre-compensation of nonlinear propagation effects. Advancement in forward error correction will shrink the performance gap with Shannon’s limit. At the network control and management level, new tools are being developed to achieve a more efficient utilization of networks. This will also allow for network virtualization, orchestration, and management. Finally, FlexEthernet and FlexOTN will be put in place to allow network operators to optimize capacity in their optical transport networks without manual changes to the client hardware.","1943-0639","","10.1364/JOCN.9.000C12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7901442","Digital signal processing; Modulation formats;Optical coherent transceivers; Optical networks","Transceivers;Optical receivers;Modulation;Optical transmitters;Optical fiber networks;Adaptive optics;Nonlinear optics","","78","7","","","14 Apr 2017","","","IEEE","IEEE Journals"
"Blockchain for Industry 5.0: Vision, Opportunities, Key Enablers, and Future Directions","A. Verma; P. Bhattacharya; N. Madhani; C. Trivedi; B. Bhushan; S. Tanwar; G. Sharma; P. N. Bokoro; R. Sharma","Department of Computer Science and Engineering, Institute of Technology, Nirma University, Gujarat, Ahmedabad, India; Department of Computer Science and Engineering, Institute of Technology, Nirma University, Gujarat, Ahmedabad, India; Department of Computer Science and Engineering, Institute of Technology, Nirma University, Gujarat, Ahmedabad, India; Department of Computer Science and Engineering, Institute of Technology, Nirma University, Gujarat, Ahmedabad, India; School of Engineering and Technology, Sharda University, Greater Noida, India; Department of Computer Science and Engineering, Institute of Technology, Nirma University, Gujarat, Ahmedabad, India; Department of Electrical Engineering Technology, University of Johannesburg, Johannesburg, South Africa; Department of Electrical Engineering Technology, University of Johannesburg, Johannesburg, South Africa; Centre for Inter-Disciplinary Research and Innovation, University of Petroleum and Energy Studies, Dehradun, India",IEEE Access,"13 Jul 2022","2022","10","","69160","69199","Industry 4.0 have witnessed a paradigm shift from cyber-physical systems (CPS) that aims at massive automation, to a more customer-driven approach. The shift has been attributed to the design of hyper-cognitive systems, integration of virtual and extended reality, digital machinery prototyping and twin designs, trusted machine boundaries, collaborative robots, and artificial intelligence (AI)-based supply chains. This new wave, termed Industry 5.0, is expected to leverage massive production with user-centric customization outside the scope of Industry 4.0 ecosystems. Industry 5.0 is expected to assist diverse industrial verticals like healthcare, smart farming, drones, smart grids, and supply chain production ecosystems. However, data is shared among multiple heterogeneous networks, spanning different authoritative domains. Thus, trusted and secured data transfer is crucial to synergize and secure the industrial perimeters. Blockchain (BC) is a preferred choice as a security enabler to Industry 5.0 ecosystems owing to its inherent property of immutability, chronology, and auditability in industrial systems. Limited works are proposed that present the vision and holistic view of BC-assisted Industry 5.0 applications. The article presents a first-of-its-kind survey on BC as a security enabler in Industry 5.0. Based on a descriptive survey methodology and research questions, we presented the key drivers, and potential applications, and propose an architectural vision of BC-based Industry 5.0 in diverse applicative verticals. The survey intends to present solutions that would assist industry practitioners, academicians, and researchers to drive novel BC-assisted solutions in Industry 5.0 verticals.","2169-3536","","10.1109/ACCESS.2022.3186892","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9809962","Blockchain;Industry 5.0;Internet-of-Things;security;privacy","Industries;Production;Fourth Industrial Revolution;Security;Service robots;Artificial intelligence;Blockchains","","77","","168","CCBY","28 Jun 2022","","","IEEE","IEEE Journals"
"Distributed Redundant Placement for Microservice-based Applications at the Edge","H. Zhao; S. Deng; Z. Liu; J. Yin; S. Dustdar","College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Distributed Systems Group, Technische Universität Wien, Vienna, Austria",IEEE Transactions on Services Computing,"17 Jun 2022","2022","15","3","1732","1745","Multi-access edge computing (MEC) is booming as a promising paradigm to push the computation and communication resources from cloud to the network edge to provide services and to perform computations. With container technologies, mobile devices with small memory footprint can run composite microservice-based applications without time-consuming backbone. Service placement at the edge is of importance to put MEC from theory into practice. However, current state-of-the-art research does not sufficiently take the composite property of services into consideration. Besides, although Kubernetes has certain abilities to heal container failures, high availability cannot be ensured due to heterogeneity and variability of edge sites. To deal with these problems, we propose a distributed redundant placement framework SAA-RP and a GA-based Server Selection (GASS) algorithm for microservice-based applications with sequential combinatorial structure. We formulate a stochastic optimization problem with the uncertainty of microservice request considered, and then decide for each microservice, how it should be deployed and with how many instances as well as on which edge sites to place them. Benchmark policies are implemented in two scenarios, where redundancy is allowed and not, respectively. Numerical results based on a real-world dataset verify that GASS significantly outperforms all the benchmark policies.","1939-1374","","10.1109/TSC.2020.3013600","National Key Research and Development Program of China(grant numbers:2017YFB1400601); Key Research and Development Project of Zhejiang Province(grant numbers:2017C01015); National Natural Science Foundation of China(grant numbers:61772461); Natural Science Foundation of Zhejiang Province(grant numbers:LR18F020003,LY17F020014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9154603","Redundancy;service placement;multi-access edge computing;composite service;sample average approximation","Redundancy;Containers;Mobile handsets;Cloud computing;Numerical models;Stochastic processes;Edge computing","","75","","40","IEEE","3 Aug 2020","","","IEEE","IEEE Journals"
"A Cloud-Based On-Line Disaggregation Algorithm for Home Appliance Loads","M. A. Mengistu; A. A. Girmay; C. Camarda; A. Acquaviva; E. Patti","Department of Control and Computer Engineering, Politecnico di Torino, Torino, Italy; Midori S.R.L., Politecnico di Torino, Torino, Italy; Midori S.R.L., Politecnico di Torino, Torino, Italy; Department of Control and Computer Engineering, Politecnico di Torino, Torino, Italy; Department of Control and Computer Engineering, Politecnico di Torino, Torino, Italy",IEEE Transactions on Smart Grid,"22 Apr 2019","2019","10","3","3430","3439","In this paper, we address the problem of providing fast and on-line households appliance load detection in a non-intrusive way from aggregate electric energy consumption data. Enabling on-line load detection is a relevant research problem as it can unlock new grid services such as demand-side management and raises interactivity in energy awareness possibly leading to more green behaviors. To this purpose, we propose an on-line-non-intrusive load monitoring machine learning algorithm combining two methodologies: 1) unsupervised event-based profiling and 2) Markov chain appliance load modeling. The event-based part performs event detection through contiguous and transient data segments, events clustering and matching. The resulting features are used to build household-specific appliance models from generic appliance models. Disaggregation is then performed on-line using an additive factorial hidden Markov model from the generated appliance model parameters. Our solution is implemented on the cloud and tested with public benchmark datasets. Accuracy results are presented and compared with literature solutions, showing that the proposed solution achieves on-line detection with comparable detection performance with respect to non on-line approaches.","1949-3061","","10.1109/TSG.2018.2826844","EU Project Flexmeter(grant numbers:646568); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8337762","Non-intrusive load monitoring;load disaggregation;event detection;hidden Markov model;smart metering","Hidden Markov models;Home appliances;Load modeling;Aggregates;Data models;Event detection;Feature extraction","","75","","33","IEEE","13 Apr 2018","","","IEEE","IEEE Journals"
"Microservices Scheduling Model Over Heterogeneous Cloud-Edge Environments As Support for IoT Applications","I. -D. Filip; F. Pop; C. Serbanescu; C. Choi","Computer Science Department, University Politehnica of Bucharest, Bucharest, Romania; Computer Science Department, University Politehnica of Bucharest, Bucharest, Romania; Department of Mathematical Methods and Models, University Politehnica of Bucharest, Bucharest, Romania; IT Research Institute, Chosun University, Gwangju, South Korea",IEEE Internet of Things Journal,"9 Aug 2018","2018","5","4","2672","2681","Motivated by the high-interest in increasing the utilization of nongeneral purpose devices in reaching computational objectives with a reduced cost, we propose a new model for scheduling microservices over heterogeneous cloud-edge environments. Our model uses a particular mathematical formulation for describing an architecture that includes heterogeneous machines that can handle different microservices. Since any new model asks for an early risk-analysis of the solution, we improved the CloudSim simulation framework to be suitable for an experiment that includes that kind of systems. In this paper, we discuss two examples of real-life utilizations of our proposed scheduling architecture. For an objective appreciation of the first example, we also include some experimental results based on the developed simulation tool. As a result of our interpretation of the experimental results we find out that some very simple scheduling algorithms may outperform some others in given situations that are frequently present in cloud-edge environments when we are using a microservice-oriented approach.","2327-4662","","10.1109/JIOT.2018.2792940","MONROE—Toff Project (H2020)(grant numbers:644399); NETIO-ForestMon(grant numbers:53/05.09.2016,SMIS2014+105976); SPERO(grant numbers:PN-III-P2-2.1-SOL-2016-03-0046,3Sol/2017); ROBIN(grant numbers:PN-III-P1-1.2-PCCDI-2017-0734); National Research Foundation of Korea(grant numbers:2017R1A6A1A03015496); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8255573","Cloud computing;edge computing;energy efficiency;heterogeneous systems;microservice scheduling","Computational modeling;Cloud computing;Internet of Things;Mathematical model;Processor scheduling;Computer architecture","","74","","25","IEEE","12 Jan 2018","","","IEEE","IEEE Journals"
"Big Data Driven Edge-Cloud Collaboration Architecture for Cloud Manufacturing: A Software Defined Perspective","C. Yang; S. Lan; L. Wang; W. Shen; G. G. Q. Huang","School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Economics and Management, University of Chinese Academy of Science, Beijing, China; Department of Production Engineering, KTH Royal Institute of Technology, Stockholm, Sweden; State Key Lab of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Industrial and Manufacturing Systems Engineering, The University of Hong Kong, Hong Kong",IEEE Access,"12 Mar 2020","2020","8","","45938","45950","In the practice of cloud manufacturing, there still exist some major challenges, including: 1) cloud based big data analytics and decision-making cannot meet the requirements of many latency-sensitive applications on shop floors; 2) existing manufacturing systems lack enough reconfigurability, openness and evolvability to deal with shop-floor disturbances and market changes; and 3) big data from shop-floors and the Internet has not been effectively utilized to guide the optimization and upgrade of manufacturing systems. This paper proposes an open evolutionary architecture of the intelligent cloud manufacturing system with collaborative edge and cloud processing. Hierarchical gateways connecting and managing shop-floor things at the “edge” side are introduced to support latency-sensitive applications for real-time responses. Big data processed both at the gateways and in the cloud will be used to guide continuous improvement and evolution of edge-cloud systems for better performance. As software tools are becoming dominant as the “brain” of manufacturing control and decision-making, this paper also proposes a new mode - “AI-Mfg-Ops” (AI enabled Manufacturing Operations) with a supporting software defined framework, which can promote fast operation and upgrading of cloud manufacturing systems with smart monitoring-analysis-planning-execution in a closed loop. This research can contribute to the rapid response and efficient operation of cloud manufacturing systems.","2169-3536","","10.1109/ACCESS.2020.2977846","Beijing Institute of Technology Research Fund Program for Young Scholars; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9020166","Cloud manufacturing;big data;edge-cloud collaboration;software-defined architecture;Internet of Things","Cloud computing;Big Data;Computer architecture;Collaboration;Real-time systems;Manufacturing systems","","73","","36","CCBY","2 Mar 2020","","","IEEE","IEEE Journals"
"Emerging Trends, Techniques and Open Issues of Containerization: A Review","J. Watada; A. Roy; R. Kadikar; H. Pham; B. Xu","Research Institute of Quantitative Economics, Zhejiang Gongshang University, Hangzhou, China; Department of CIS, Universiti Technologi Petronas, Seri Iskandar, Malaysia; InfraCloud Technologies Pvt. Ltd., Pune, India; Department of ISE, State University of New Jersey, Piscataway, NJ, USA; Research Institute of Quantitative Economics, Zhejiang Gongshang University, Hangzhou, China",IEEE Access,"28 Oct 2019","2019","7","","152443","152472","Containerization is revolutionizing the way that many industries operate, provisioning major impact to modern computing technologies because it is extra lightweight, highly portable, energy, resource and storage efficient, cost-effective, performance efficient, and extremely quick during boot up. These often facilitate efficient load balancing, low-level system maintenance, server consolidation (for efficient energy and resource utilization) and replication of instances over geographical locations for better fault tolerance to escalate application reliability. However, some recent literature have addressed various challenges (such as complex networking, persistent storage facilities, cross data centers and multicloud supports, security issues, and lack of available, capable container management APIs, etc.) regarding successful container adoption in industries, which might have resulted in a seemingly meager increase in industrial deployments of containerization over the past few years despite bestowing efficient lightweight virtualization. Moreover, a comprehensive overview of containerizations along with their popularity dynamics has still not been found in contemporary literature, which further extends knowledge gap between developers and available technologies. Hence, current study touches upon different technicalities involved in containerization with potential problems and possible solutions along with various important industrial applications to manifest its existing supports and technical hardships. Finally, we have conducted a comprehensive experimental study to compare the performance of VMs, containers and unikernels in terms of CPU utilization, memory footprints, network bandwidth, execution time and technological maturity using standard benchmarks and observed containers to deliver satisfactory performance in almost all aspects, however, are still not free from issues regarding isolation & security, performance stability, lack of available efficient tools for crossplatform support and persistent storage. Unikernels deliver good performance with VM-like isolation but still need to achieve desired technical maturity (in terms of microprocessor stability, process containment, persistent storage, etc.). VMs, on the other hand, are found to provide stable performance throughout, though bigger memory footprints and slower spin up/down remain their biggest weaknesses.","2169-3536","","10.1109/ACCESS.2019.2945930","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8861307","Virtualization;containerization;management and orchestration;isolation and security","Containers;Virtualization;Security;Cloud computing;Virtual machine monitors;Linux;Tools","","72","","138","CCBY","7 Oct 2019","","","IEEE","IEEE Journals"
"Blockchain Meets Metaverse and Digital Asset Management: A Comprehensive Survey","V. T. Truong; L. Le; D. Niyato","Institut National De La Recherche Scientifique (INRS), University of Québec, Montréal, QC, Canada; Institut National De La Recherche Scientifique (INRS), University of Québec, Montréal, QC, Canada; School of Computer Science and Engineering (SCSE), Nanyang Technological University, Jurong West, Singapore",IEEE Access,"29 Mar 2023","2023","11","","26258","26288","Envisioned to be the next-generation Internet, the metaverse has been attracting enormous attention from both the academia and industry. The metaverse can be viewed as a 3D immersive virtual world, where people use Augmented/Virtual Reality (AR/VR) devices to access and interact with others through digital avatars. While early versions of the metaverse exist in several Massively Multiplayer Online (MMO) games, the full-flesh metaverse is expected to be more complex and enabled by various advanced technologies. Blockchain is one of the crucial technologies that could revolutionize the metaverse to become a decentralized and democratic virtual society with its own economic and governance system. Realizing the importance of blockchain for the metaverse, our goal in this paper is to provide a comprehensive survey that clarifies the role of blockchain in the metaverse including in-depth analysis of digital asset management. To this end, we discuss how blockchain can enable the metaverse from different perspectives, ranging from user applications to virtual services and the blockchain-enabled economic system. Furthermore, we describe how blockchain can shape the metaverse from the system perspective, including various solutions for the decentralized governance system and data management. The potential of blockchain for security and privacy aspects of the metaverse infrastructure is also figured out, while a full flow of blockchain-based digital asset management for the metaverse is investigated. Finally, we discuss a wide range of open challenges of the blockchain-empowered metaverse.","2169-3536","","10.1109/ACCESS.2023.3257029","National Research Foundation (NRF), Singapore, and Infocomm Media Development Authority under the Future Communications Research Development Programme (FCP)(grant numbers:FCP-NTU-RG-2022-010); DSO National Laboratories under the AI Singapore Programme(grant numbers:AISG2-RP-2020-019); Energy Research Test-Bed and Industry Partnership Funding Initiative; Energy Grid (EG) 2.0 programme, and under DesCartes and the Campus for Research Excellence and Technological Enterprise (CREATE) programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10068493","Metaverse;blockchain;artificial intelligence;digital asset management;the Internet of Things;digital twin;VR/AR","Metaverse;Blockchains;Asset management;Security;Games;Decentralized applications;Artificial intelligence","","71","","171","CCBYNCND","14 Mar 2023","","","IEEE","IEEE Journals"
"MOERA: Mobility-Agnostic Online Resource Allocation for Edge Computing","L. Wang; L. Jiao; J. Li; J. Gedeon; M. Mühlhäuser","Telecooperation Lab, Technische Universität Darmstadt, Darmstadt, Germany; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA; Telecooperation Lab, Technische Universität Darmstadt, Darmstadt, Germany; Telecooperation Lab, Technische Universität Darmstadt, Darmstadt, Germany",IEEE Transactions on Mobile Computing,"1 Jul 2019","2019","18","8","1843","1856","To better support emerging interactive mobile applications such as those VR-/AR-based, cloud computing is quickly evolving into a new computing paradigm called edge computing. Edge computing has the promise of bringing cloud resources to the network edge to augment the capability of mobile devices in close proximity to the user. One big challenge in edge computing is the efficient allocation and adaptation of edge resources in the presence of high dynamics imposed by user mobility. This paper provides a formal study of this problem. By characterizing a variety of static and dynamic performance measures with a comprehensive cost model, we formulate the online edge resource allocation problem with a mixed nonlinear optimization problem. We propose MOERA, a mobility-agnostic online algorithm based on the “regularization” technique, which can be used to decompose the problem into separate subproblems with regularized objective functions and solve them using convex programming. Through rigorous analysis we are able to prove that MOERA can guarantee a parameterized competitive ratio, without requiring any a priori knowledge on input. We carry out extensive experiments with various real-world data and show that MOERA can achieve an empirical competitive ratio of less than 1.2, reduces the total cost by $4 \times$4× compared to static approaches, and outperforms the online greedy one-shot solution by 70 percent. Moreover, we verify that even being future-agnostic, MOERA can achieve comparable performance to approaches with perfect partial future knowledge. We also discuss practical issues with respect to the implementation of our algorithm in real edge computing systems.","1558-0660","","10.1109/TMC.2018.2867520","Deutsche Forschungsgemeinschaft(grant numbers:392046569); National Natural Science Foundation of China(grant numbers:61761136014); DFG Collaborative Research Center; National Science Foundation(grant numbers:1564348); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449107","Edge computing;resource allocation;online optimization;competitive analysis","Cloud computing;Resource management;Edge computing;Delays;Mobile computing;Mobile applications;Dynamic scheduling","","71","","45","IEEE","28 Aug 2018","","","IEEE","IEEE Journals"
"Dynamic On-Demand Fog Formation Offering On-the-Fly IoT Service Deployment","H. Sami; A. Mourad","Department of Computer Science and Mathematics, Lebanese American University, Beirut, Lebanon; Department of Computer Science and Mathematics, Lebanese American University, Beirut, Lebanon",IEEE Transactions on Network and Service Management,"10 Jun 2020","2020","17","2","1026","1039","With the increasing number of IoT devices, fog computing has emerged, providing processing resources at the edge for the tremendous amount of sensed data and IoT computation. The advantage of the fog gets eliminated if it is not present near IoT devices. Fogs nowadays are pre-configured in specific locations with pre-defined services, which limit their diverse availabilities and dynamic service update. In this paper, we address the aforementioned problem by benefiting from the containerization and micro-service technologies to build our on-demand fog framework with the help of the volunteering devices. Our approach overcomes the current limitations by providing available fog devices with the ability to have services deployed on the fly. Volunteering devices form a resource capacity for building the fog computing infrastructure. Moreover, our framework leverages intelligent container placement scheme that produces efficient volunteers' selection and distribution of services. An Evolutionary Memetic Algorithm (MA) is elaborated to solve our multi-objective container placement optimization problem. Real life and simulated experiments demonstrate various improvements over existing approaches interpreted by the relevance and efficiency of (1) forming volunteering fog devices near users with maximum time availability and shortest distance, and (2) deploying services on the fly on selected fogs with improved QoS.","1932-4537","","10.1109/TNSM.2019.2963643","Lebanese American University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8947936","IoT;fog computing;on-demand fog formation;edge computing;docker;Kubernetes;Kubeadm;container placement;evolutionary memetic algorithm;micro-services","Containers;Edge computing;Computer architecture;Cloud computing;Internet of Things;Memetics;Quality of service","","70","","32","IEEE","1 Jan 2020","","","IEEE","IEEE Journals"
"Virtual Fog: A Virtualization Enabled Fog Computing Framework for Internet of Things","J. Li; J. Jin; D. Yuan; H. Zhang","School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, VIC, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, VIC, Australia; School of Electrical and Information Engineering, University of Sydney, Sydney, NSW, Australia; National Engineering Laboratory on Interconnection Technology for Next Generation Internet, Beijing Jiaotong University, Beijing, China",IEEE Internet of Things Journal,"9 Feb 2018","2018","5","1","121","131","The prosperity of Internet of Things (IoT) and the success of rich Cloud services have expedited the emergence of a new computing paradigm called Fog computing, which promotes the processing of data at the proximity of their sources. Complementary to the Cloud, Fog promises to offer many appealing features, such as low latency, low cost, high multitenancy, high scalability, and to consolidate the IoT ecosystem. Although the Fog concept has been widely adopted in many areas, a comprehensive realization has yet been adequately researched. To address all these issues, in this paper, object virtualization is investigated to overcome obstacles resulting from resource constraints on sensory-level nodes while service virtualization is explored to easily create tailored applications for end users. Moreover, network function virtualization is studied to perform the flexibility of network service provisioning. Grounded on object virtualization, network function virtualization and service virtualization, a layered framework that encompasses smart objects, Fog and Cloud is presented to illustrate the realization of virtual Fog along IoT continuum. This proposed virtual Fog framework is applied to a smart living case for verification, then quantitative analysis is conducted to demonstrate the low latency, low operating expense, high multitenancy and scalability, followed by an experimental evaluation to further confirm that delay and jitter can be decreased through virtualization.","2327-4662","","10.1109/JIOT.2017.2774286","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8113467","Fog computing;Internet of Things (IoT);network function virtualization;object virtualization;service virtualization","Virtualization;Cloud computing;Hardware;Internet of Things;Network function virtualization;Sensor phenomena and characterization","","69","","24","IEEE","16 Nov 2017","","","IEEE","IEEE Journals"
"Adaptive Resource Efficient Microservice Deployment in Cloud-Edge Continuum","K. Fu; W. Zhang; Q. Chen; D. Zeng; M. Guo","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Computer Science, China University of Geosciences, Wuhan, Hubei, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China",IEEE Transactions on Parallel and Distributed Systems,"8 Dec 2021","2022","33","8","1825","1840","User-facing services are now evolving towards the microservice architecture where a service is built by connecting multiple microservice stages. Since the entire service is heavy, the microservice architecture shows the opportunity to only offload some microservice stages to the edge devices that are close to the end users. However, emerging techniques often result in the violation of Quality-of-Service (QoS) of microservice-based services in cloud-edge continuum, as they do not consider the communication overhead or the resource contention between microservices and external co-located tasks. We propose Nautilus, a runtime system that effectively deploys microservice-based user-facing services in cloud-edge continuum. Nautilus ensures the QoS of microservice-based user-facing services while minimizing the required computational resources, which is comprised of a communication-aware microservice mapper, a contention-aware resource manager and an IO-sensitive and load-aware microservice migration scheduler. The mapper divides the microservice graph into multiple partitions based on the communication overhead and maps the partitions to appropriate nodes. On each node, the resource manager determines the optimal resource allocation for its microservices based on reinforcement learning that may capture the complex contention behaviors. Once the microservices are suffered from external IO pressure, the IO-sensitive microservice scheduler migrates the critical one to idle nodes. Furthermore, when the load of microservices changes dynamically, the load-aware microservice scheduler migrates microservices from busy nodes to idle ones to ensure the QoS goal of the entire service. Our experimental results show that Nautilus can guarantee the required QoS target under external shared resources contention while the state-of-the-art suffers from QoS violations. Meanwhile, Nautilus reduces the computational resource usage by 23.9% and the network bandwidth usage by 53.4%, while achieving the required 99%-ile latency.","1558-2183","","10.1109/TPDS.2021.3128037","National Key Research and Development Program of China(grant numbers:2018YFB1004800); National Natural Science Foundation of China(grant numbers:62022057,61832006,61632017,61872240); Open Research Projects of Zhejiang Lab(grant numbers:2021KE0AB02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9615028","Cloud-edge continuum;QoS;microservice resources management","Quality of service;Cloud computing;Task analysis;Resource management;Computer architecture;Runtime;Bandwidth","","69","","57","IEEE","15 Nov 2021","","","IEEE","IEEE Journals"
"Machine-Learning-Assisted Security and Privacy Provisioning for Edge Computing: A Survey","S. Singh; R. Sulthana; T. Shewale; V. Chamola; A. Benslimane; B. Sikdar","Department of Electrical and Electronics Engineering & APPCAIR, Birla Institute of Technology and Science-Pilani (Pilani Campus), Pilani, India; Department of Computer Science, Birla Institute of Technology and Science-Pilani (Dubai Campus), Dubai, UAE; Department of Electrical and Electronics Engineering & APPCAIR, Birla Institute of Technology and Science-Pilani (Pilani Campus), Pilani, India; Department of Electrical and Electronics Engineering & APPCAIR, Birla Institute of Technology and Science-Pilani (Pilani Campus), Pilani, India; Department of Computer Science, University of Avignon, Avignon, France; Department of Electrical and Computer Engineering, National University of Singapore, Singapore",IEEE Internet of Things Journal,"22 Dec 2021","2022","9","1","236","260","Edge computing (EC), is a technological game changer that has the ability to connect millions of sensors and provide services at the device end. The broad vision of EC integrates storage, processing, monitoring, and control of operations in the Edge of the network. Though EC provides end-to-end connectivity, speeds up operation, and reduces latency of data transfer, security is a major concern. The tremendous growth in the number of Edge Devices and the amount of sensitive information generated at the device and the cloud creates a broad surface of attack and therefore, the need to secure the static and mobile data is imperative. This article is a comprehensive survey that describes the security and privacy issues in various layers of the EC architecture that result from the networking of heterogeneous devices. Second, it discusses the wide range of machine learning and deep learning algorithms that are applied in EC use cases. Following this, this article broadly details the different types of attacks that the Edge network confronts, and the intrusion detection systems and the corresponding machine learning algorithms that overcome these security and privacy concerns. The details of machine learning and deep learning techniques for EC security are tabulated. Finally, the open issues in securing Edge networks and future research directions are provided.","2327-4662","","10.1109/JIOT.2021.3098051","ASEAN—India Collaborative Research and Development Scheme (ASEAN-India S&T Development Fund (AISTDF) sponsored)(grant numbers:CRD/2020/000369); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9490350","Attacks;deep learning;edge computing (EC);machine learning;privacy;security","Security;Edge computing;Cloud computing;Machine learning algorithms;Computer architecture;Servers;Protocols","","68","","172","IEEE","19 Jul 2021","","","IEEE","IEEE Journals"
"Decentralized SDN Control Plane for a Distributed Cloud-Edge Infrastructure: A Survey","D. Espinel Sarmiento; A. Lebre; L. Nussbaum; A. Chari","CISS/IIE/NAVI, Orange Labs, Lannion, France; STACK, IMT Atlantique Bretagne-Pays de la Loire (Campus de Nantes), Nantes, France; RESIST, Universite de Lorraine, Nancy, France; CISS/IIE/NAVI, Orange Labs, Lannion, France",IEEE Communications Surveys & Tutorials,"23 Feb 2021","2021","23","1","256","281","Today's emerging needs (Internet of Things applications, Network Function Virtualization services, Mobile Edge computing, etc.) are challenging the classic approach of deploying a few large data centers to provide cloud services. A massively distributed Cloud-Edge architecture could better fit these new trends' requirements and constraints by deploying on-demand infrastructure services in Point-of-Presences within backbone networks. In this context, a key feature is establishing connectivity among several resource managers in charge of operating, each one a subset of the infrastructure. After explaining the networking management challenges related to distributed Cloud-Edge infrastructures, this article surveys and analyzes the characteristics and limitations of existing technologies in the Software Defined Network field that could be used to provide the inter-site connectivity feature. We also introduce Kubernetes, the new de facto container orchestrator platform, and analyze its use in the proposed context. This survey is concluded by providing a discussion about some research directions in the field of SDN applied to distributed Cloud-Edge infrastructures' management.","1553-877X","","10.1109/COMST.2021.3050297","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9319748","IaaS;SDN;virtualization;networking;automation","Cloud computing;Neutrons;Computer architecture;Routing;Wide area networks;Tutorials;Quality of service","","68","","196","IEEE","11 Jan 2021","","","IEEE","IEEE Journals"
"An Ingestion and Analytics Architecture for IoT Applied to Smart City Use Cases","P. Ta-Shma; A. Akbar; G. Gerson-Golan; G. Hadash; F. Carrez; K. Moessner","IBM Research, Haifa, Israel; Institute for Communication Systems, University of Surrey, Guildford, U.K.; IBM Research, Haifa, Israel; IBM Research, Haifa, Israel; Institute for Communication Systems, University of Surrey, Guildford, U.K.; Institute for Communication Systems, University of Surrey, Guildford, U.K.",IEEE Internet of Things Journal,"10 Apr 2018","2018","5","2","765","774","As sensors are adopted in almost all fields of life, the Internet of Things (IoT) is triggering a massive influx of data. We need efficient and scalable methods to process this data to gain valuable insight and take timely action. Existing approaches which support both batch processing (suitable for analysis of large historical data sets) and event processing (suitable for realtime analysis) are complex. We propose the hut architecture, a simple but scalable architecture for ingesting and analyzing IoT data, which uses historical data analysis to provide context for real-time analysis. We implement our architecture using open source components optimized for Big Data applications and extend them, where needed. We demonstrate our solution on two real-world smart city use cases in transportation and energy management.","2327-4662","","10.1109/JIOT.2017.2722378","European Union’s FP7 Project COSMOS(grant numbers:609043); European Union’s Horizon 2020 Project CPaaS.io(grant numbers:723076); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7964673","Big data;complex event processing (CEP);context-aware;energy management;ingestion;Internet of Things (IoT);machine learning;smart cities;spark;transportation","Real-time systems;Computer architecture;Internet of Things;Transportation;Big Data;Batch production systems;Sparks","","67","","36","IEEE","30 Jun 2017","","","IEEE","IEEE Journals"
"Survey on Digital Twin Edge Networks (DITEN) Toward 6G","F. Tang; X. Chen; T. K. Rodrigues; M. Zhao; N. Kato","School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; Graduate School of Information Sciences, Tohoku University, Sendai, Japan; School of Computer Science and Engineering, Central South University, Changsha, China; Graduate School of Information Sciences, Tohoku University, Sendai, Japan",IEEE Open Journal of the Communications Society,"23 Aug 2022","2022","3","","1360","1381","The next generation (6G) wireless systems aim to cater to the Internet of Everything (IoE) and revolutionize customer services and applications to a fully intelligent and autonomous system. To achieve this, the digital twin edge network (DITEN) is proposed to combine mobile/multi-access edge computing (MEC) and digital twin (DT), thereby improving the network performance such as throughput and security, and reducing the cost of communication, computation, and caching. In DITENs, the network status can be continuously monitored, and based on the obtained network states, the networking schemes, such as routing and resource management, can be studied in the established DITENs from a centralized perspective. In this survey, we present a comprehensive overview of DITEN for 6G. First, we present the fundamental aspects of DITEN, including concept, framework, and potential. Second, a comprehensive design of DITEN is devised, including the DT modeling/updating, DT deployment, key issues, and enabling technologies. Then, the typical applications of DITEN towards 6G are provided, including the Internet of Things (IoT), vehicular network, space-air-ground integrated network (SAGIN), healthcare, wireless systems, and other applications, along with the design of DITEN in each application, such as DT modeling, DT association, incentive mechanisms, and so on. Finally, challenges and open issues are discussed.","2644-125X","","10.1109/OJCOMS.2022.3197811","National Natural Science Foundation of China(grant numbers:6200239); Key Research and Development Program of Xinjiang Autonomous Region(grant numbers:2021B01002); Scientific and Technological Innovation 2030 of China(grant numbers:2020AAA0109600); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9854182","Digital twin edge networks (DITEN);6G;DT modeling;DT deployment;applications;challenges","Digital twins;Wireless communication;Internet of Things;Computational modeling;Monitoring;Cloud computing;Task analysis","","66","","135","CCBY","10 Aug 2022","","","IEEE","IEEE Journals"
"Computing Paradigms in Emerging Vehicular Environments: A Review","L. Silva; N. Magaia; B. Sousa; A. Kobusińska; A. Casimiro; C. X. Mavromoustakis; G. Mastorakis; V. H. C. de Albuquerque","Department of Computer Science, LASIGE, Faculty of Sciences, University of Lisbon, Lisbon, Portugal; Department of Computer Science, LASIGE, Faculty of Sciences, University of Lisbon, Lisbon, Portugal; Department of Computer Science, LASIGE, Faculty of Sciences, University of Lisbon, Lisbon, Portugal; Laboratory of Computing Science, Poznań University of Technology, Poznan, Poland; Department of Computer Science, LASIGE, Faculty of Sciences, University of Lisbon, Lisbon, Portugal; Department of Computer Science, University of Nicosia, Nicosia, Cyprus; Department of Management Science and Technology, Hellenic Mediterranean University, Agios Nikolaos, Greece; Science and Technology of Ceará, LAPISCO, Federal Institute of Education, Fortaleza, Brazil",IEEE/CAA Journal of Automatica Sinica,"3 Feb 2021","2021","8","3","491","511","Determining how to structure vehicular network environments can be done in various ways. Here, we highlight vehicle networks' evolution from vehicular ad-hoc networks (VANET) to the internet of vehicles (IoVs), listing their benefits and limitations. We also highlight the reasons in adopting wireless technologies, in particular, IEEE 802.11p and 5G vehicle-to-everything, as well as the use of paradigms able to store and analyze a vast amount of data to produce intelligence and their applications in vehicular environments. We also correlate the use of each of these paradigms with the desire to meet existing intelligent transportation systems' requirements. The presentation of each paradigm is given from a historical and logical standpoint. In particular, vehicular fog computing improves on the deficiences of vehicular cloud computing, so both are not exclusive from the application point of view. We also emphasize some security issues that are linked to the characteristics of these paradigms and vehicular networks, showing that they complement each other and share problems and limitations. As these networks still have many opportunities to grow in both concept and application, we finally discuss concepts and technologies that we believe are beneficial. Throughout this work, we emphasize the crucial role of these concepts for the well-being of humanity.","2329-9274","","10.1109/JAS.2021.1003862","Brazilian National Council for Research and Development (CNPq)(grant numbers:304315/2017-6,430274/2018-1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9346073","Computing paradigm;cloud;edge;fog;internet of vehicle (IoV);vehicular networks","Wireless communication;5G mobile communication;Urban areas;Vehicular ad hoc networks;Security;Wireless fidelity;Vehicle-to-everything","","66","","164","","3 Feb 2021","","","IEEE","IEEE Journals"
"IoT Service Slicing and Task Offloading for Edge Computing","J. Hwang; L. Nkenyereye; N. Sung; J. Kim; J. Song","Sejong University, Seoul, South Korea; Sejong University, Seoul, South Korea; Autonomous IoT Research Center, Korea Electronics Technology Institute, Seongnam, South Korea; Sejong University, Seoul, South Korea; Autonomous IoT Research Center, Korea Electronics Technology Institute, Seongnam, South Korea",IEEE Internet of Things Journal,"6 Jul 2021","2021","8","14","11526","11547","With the advancement of Internet-of-Things (IoT) technology, various domains, such as smart factories and smart cars have used this new technology to provide value-added services. In addition, technologies, such as multiaccess edge computing (MEC) and network slicing provide another opportunity for the IoT to support more advanced and real-time services that could not have been previously supported. However, the simple integration of such technologies into the IoT does not take the full advantage of MEC and network slicing or the reduction of latency and traffic prioritization, respectively. Therefore, there is a strong need for an efficient integration mechanism for IoT platforms to maximize the benefit of using such technologies. In this article, we introduce a novel architectural framework that enables the virtualization of an IoT platform with minimum functions to support specific IoT services and host the instance in an edge node, close to the end user. As the instance provides its service at the edge node, where the MEC node and network slice are located, the traffic for the end user does not need to traverse back to the cloud. This architecture guarantees not only low latency but also efficient management of IoT services at the edge node. To show the feasibility of the proposed architecture, we conducted an experimental evaluation by comparing the transmission time of both IoT services running on the central cloud and those using sliced IoT functions in the edge gateway. The results show that the proposed architecture provides twice as much transmission speed as that from the conventional cloud-based IoT platform.","2327-4662","","10.1109/JIOT.2021.3052498","Institute of Information and Communications Technology Planning & Evaluation (IITP); Korea Government (MSIT) (Fast Intelligence Analysis HW/SW Engine Exploiting IoT Platform for Boosting On-device AI in 5G Environment)(grant numbers:2020-0-00959); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9328448","Internet of Things (IoT);multiaccess edge computing (MEC);network slicing;oneM2M standard;virtualization","Internet of Things;Cloud computing;Task analysis;Network slicing;Computer architecture;Edge computing;5G mobile communication","","66","","72","IEEE","19 Jan 2021","","","IEEE","IEEE Journals"
"Algorithmics of Cost-Driven Computation Offloading in the Edge-Cloud Environment","M. Du; Y. Wang; K. Ye; C. Xu","Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Faculty of Science and Technology, State Key Lab of IoTSC, University of Macau, Macau, China",IEEE Transactions on Computers,"7 Sep 2020","2020","69","10","1519","1532","Computation offloading between the edge and the cloud is an effective way for deployed service to fully utilize the resources at both sides for its QoS improvement and overall cost reduction. Although the offloading problem has been intensively studied in the context of mobile computing, existing algorithms in most cases cannot be effectively migrated to the edge-cloud environment because their inter-partition communication costs are always deemed as symmetric, and their intra-partition communication costs are often ignored, which, though reasonable to the traditional case, are not valid to our settings anymore. In this article, we propose a new algorithmic approach to the offloading problem in the edge-cloud environment, where a heterogeneous model is advocated to incorporate the communication cost between co-resident tasks while considering the asymmetry of communication costs between non-coresident tasks. We prove the offloading problem with respect to this model is NP-hard, and thereby designing an efficient algorithm to obtain a sub-optimal solution. Additionally, we also show that in a homogeneous case when the intra-partition and inter-partition communication costs between any pair of interactive tasks are symmetric, an optimal offloading algorithm can be devised by transforming the problem into a classical min-cut problem. We implemented and evaluated the algorithms by offloading a PageRank-based application in a controlled edge-cloud setting. Our empirical results show that the proposed algorithm for the heterogeneous case is always efficient to find a better offloading scheme, compared with the selected existing algorithms, while for the homogeneous case, the proposed solution can efficiently achieve the optimal strategy.","1557-9956","","10.1109/TC.2020.2976996","National Key R&D Program of China(grant numbers:2018YFB1004800); Science and Technology Planning Project of Guangdong Province(grant numbers:2019B010137002); Research Center for Ecology and Environment of Central Asia; Chinese Academy of Sciences(grant numbers:SQ2016YFHZ020520); National Natural Science Foundation of China(grant numbers:61672513); Shenzhen Oversea High-Caliber Personnel Innovation Funds(grant numbers:KQCX20170331161854); Shenzhen Basic Research Program(grant numbers:JCYJ20170818153016513); Science and Technology Development Fund(grant numbers:0015/2019/AKP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9018141","Cloud computing;edge computing;computation offloading;min-cut;MAX-2SAT","Task analysis;Computational modeling;Databases;Cloud computing;Bandwidth;Servers;Analytical models","","64","","42","IEEE","28 Feb 2020","","","IEEE","IEEE Journals"
"Offloading Using Traditional Optimization and Machine Learning in Federated Cloud–Edge–Fog Systems: A Survey","B. Kar; W. Yahya; Y. -D. Lin; A. Ali","Department of Computer Science and Information Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Electrical Engineering and Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Department of Computer Science, National Yang Ming Chiao Tung University, Hsnchu, Taiwan; Department of Electrical Engineering and Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan",IEEE Communications Surveys & Tutorials,"22 May 2023","2023","25","2","1199","1226","The huge amount of data generated by the Internet of Things (IoT) devices needs the computational power and storage capacity provided by cloud, edge, and fog computing paradigms. Each of these computing paradigms has its own pros and cons. Cloud computing provides enhanced data storage and computing power but causes high communication latency. Edge and fog computing provide similar services with lower latency but limited capacity, capability, and coverage. A single computing paradigm cannot fulfill all the requirements of IoT devices and a federation between them is needed to extend their capacity, capability, and services. This federation is beneficial to both subscribers and providers and also reveals research issues in traffic offloading between clouds, edges, and fogs. Optimization has traditionally been used to solve the problem of traffic offloading. However, in such a complex federated system, traditional optimization cannot keep up with the strict latency requirements of decision-making, ranging from milliseconds to sub-seconds. Machine learning approaches, especially reinforcement learning, are consequently becoming popular because they could quickly solve offloading problems in dynamic environments with some unknown information. This study provides a novel federal classification between cloud, edge, and fog and presents a comprehensive research roadmap on offloading for different federated scenarios. We survey the relevant literature on the various optimization approaches used to solve this offloading problem and compare their salient features. We then provide a comprehensive survey on offloading in federated systems with machine learning approaches and the lessons learned as a result of these surveys. Finally, we outline several directions for future research and challenges that have to be faced in order to achieve such a federation.","1553-877X","","10.1109/COMST.2023.3239579","Ministry of Science and Technology (MOST), Taiwan(grant numbers:109-2221-E-011-104-MY3); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10025811","Offloading;cloud computing;edge computing;fog computing;federation;optimization;machine learning","Cloud computing;Optimization;Edge computing;Internet of Things;Task analysis;Wireless fidelity;Surveys","","63","","152","IEEE","25 Jan 2023","","","IEEE","IEEE Journals"
"Communication-Aware Container Placement and Reassignment in Large-Scale Internet Data Centers","L. Lv; Y. Zhang; Y. Li; K. Xu; D. Wang; W. Wang; M. Li; X. Cao; Q. Liang","Department of Computer Science and Technology, Tsinghua University, Beijing, China; School of Software Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Department of Computer Science, Nankai University, Tianjin, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Baidu, Beijing, China; Baidu, Beijing, China; Baidu, Beijing, China",IEEE Journal on Selected Areas in Communications,"14 Feb 2019","2019","37","3","540","555","Containerization has been used in many applications for isolation purposes due to its lightweight, scalable, and highly portable properties. However, to apply containerization in large-scale Internet data centers faces a big challenge. Services in data centers are always instantiated as a group of containers, which often generate heavy communication workloads and therefore resulting in inefficient communications and downgraded service performance. Although assigning the containers of the same service to the same server can reduce the communication overhead, this may cause heavily imbalanced resource utilization since containers of the same service are usually intensive to the same resource. To reduce communication cost as well as balance the resource utilization in large-scale data centers, we further explore the container distribution issues in a real industrial environment and find that such conflict lies in two phases-container placement and container reassignment. The objective of this paper is to address the container distribution problem in these two phases. For the container placement problem, we propose an efficient communication aware worst fit decreasing algorithm to place a set of new containers into data centers. For the container reassignment problem, we propose a two-stage algorithm called Sweep&Search to optimize a given initial distribution of containers by migrating containers among servers. We implement the proposed algorithms in Baidu's data centers and conduct extensive evaluations. Compared with the state-of-the-art strategies, the evaluation results show that our algorithms perform better up to 70% and increase the overall service throughput up to 90% simultaneously.","1558-0008","","10.1109/JSAC.2019.2895473","National Key Research and Development Program of China(grant numbers:2018YFB0803405); China Postdoctoral Science Foundation(grant numbers:2018M630117); National Natural Science Foundation of China(grant numbers:61802024); Huawei Autonomous and Service 2.0 Project(grant numbers:A2018185); Baidu Songguo Plan; NSF of China(grant numbers:61602266); NSF of Tianjin(grant numbers:16JCYBJC41900); National Key Research and Development Program of China(grant numbers:2018YFB0803405); China National Funds for Distinguished Young Scientists(grant numbers:61825204); Beijing Outstanding Young Scientist Project; PolyU G-YBAG; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8631120","Container communication;multi-resource load balance;large-scale data centers;container placement;container reassignment","Containers;Servers;Resource management;Data centers;Internet;Throughput;Time factors","","63","","88","IEEE","31 Jan 2019","","","IEEE","IEEE Journals"
"Edge Computing and Networking: A Survey on Infrastructures and Applications","Y. Zhao; W. Wang; Y. Li; C. Colman Meixner; M. Tornatore; J. Zhang","Institute of Information Photonics and Optical Communications, Beijing University of Posts and Telecommunications, Beijing, China; Institute of Information Photonics and Optical Communications, Beijing University of Posts and Telecommunications, Beijing, China; Institute of Information Photonics and Optical Communications, Beijing University of Posts and Telecommunications, Beijing, China; Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K.; Department of Electronics and Information, Politecnico di Milano, Milan, Italy; Institute of Information Photonics and Optical Communications, Beijing University of Posts and Telecommunications, Beijing, China",IEEE Access,"6 Aug 2019","2019","7","","101213","101230","As a concept to enhance and extend cloud-computing capabilities, edge computing aims to provide Internet-based services in the close proximity to users by placing IT infrastructures at the network edge in forms of tiny datacenters. Taking advantage of the close distance to end user and access networks, edge datacenters can provide low-latency and context-aware services and further improve users’ quality of experience. As the network edge is a geographically spread concept, the edge datacenters are usually highly distributed so that they can provide nearby storage and processing capabilities to most of the end users. Furthermore, edge datacenters also co-work with centralized cloud datacenters for service orchestration. Such decentralization and collaboration are expected to introduce significant transformations to both infrastructures and applications. To provide an overview of how edge can be integrated with cloud-computing and how edge computing can benefit applications, this paper studies the infrastructure and application issues of edge computing and networking in several sub-aspects, including related concepts, infrastructures, resource management and virtualization, performance, and applications.","2169-3536","","10.1109/ACCESS.2019.2927538","National Science and Technology Major Project(grant numbers:2017ZX03001016); State Key Laboratory of Advanced Optical Communication Systems and Networks.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8758431","Edge computing;cloud-computing;datacenter;SDN;NFV;C-RAN;IoT","Quantization (signal);Signal to noise ratio;Receivers;Bandwidth;White noise;Timing;Gaussian noise","","63","","99","CCBY","9 Jul 2019","","","IEEE","IEEE Journals"
"Digital Subcarrier Multiplexing: Enabling Software-Configurable Optical Networks","D. Welch; A. Napoli; J. Bäck; S. Buggaveeti; C. Castro; A. Chase; X. Chen; V. Dominic; T. Duthel; T. A. Eriksson; S. Erkilinç; P. Evans; C. R. S. Fludger; B. Foo; T. Frost; P. Gavrilovic; S. J. Hand; A. Kakkar; A. Kumpera; V. Lal; R. Maher; F. Marques; F. Masoud; A. Mathur; R. Milano; M. I. Olmedo; M. Olson; D. Pavinski; J. Pedro; A. Rashidinejad; P. Samra; W. Sande; A. Somani; H. Sun; N. Swenson; H. -S. Tsai; A. Yekani; J. Zhang; M. Ziari","Infinera Corporation, San Jose, CA, USA; Infinera, Munich, Germany; Infinera, Stockholm, Sweden; Infinera Corporation, San Jose, CA, USA; Infinera, Munich, Germany; Infinera Corporation, San Jose, CA, USA; Infinera, Stockholm, Sweden; Infinera Corporation, San Jose, CA, USA; Infinera, Munich, Germany; Infinera, Stockholm, Sweden; Infinera, Stockholm, Sweden; Infinera Corporation, San Jose, CA, USA; Infinera, Munich, Germany; Infinera Corporation, San Jose, CA, USA; Infinera Corporation, San Jose, CA, USA; Infinera Corporation, San Jose, CA, USA; Infinera Corporation, San Jose, CA, USA; Infinera, Ottawa, Canada; Infinera, Ottawa, Canada; Infinera Corporation, San Jose, CA, USA; Infinera Corporation, San Jose, CA, USA; Infinera Unipessoal Lda, Carnaxide, Portugal; Infinera, Ottawa, Canada; Infinera Corporation, San Jose, CA, USA; Infinera Corporation, San Jose, CA, USA; Infinera Corporation, San Jose, CA, USA; Infinera, Stockholm, Sweden; Infinera Corporation, Allentown, PA, USA; Infinera Unipessoal Lda, Carnaxide, Portugal; Infinera, Ottawa, Canada; Infinera Corporation, San Jose, CA, USA; Infinera Corporation, San Jose, CA, USA; Infinera, Ottawa, Canada; Infinera, Ottawa, Canada; Infinera Corporation, San Jose, CA, USA; Infinera Corporation, San Jose, CA, USA; Infinera, Ottawa, Canada; Infinera Corporation, Allentown, PA, USA; Infinera Corporation, San Jose, CA, USA",Journal of Lightwave Technology,"31 Jan 2023","2023","41","4","1175","1191","The various topologies, traffic patterns and cost targets of optical networks have prevented the deployment of end-to-end solutions across multi-domains, and the optimization of the network as a whole. The consequent limitations in flexibility, scalability, and adaptability of optical networks will become increasingly important with new applications, such as 5G/6G. Coherent transceivers based on digital subcarrier multiplexing (DSCM) are proposed to address these current constraints. In particular, DSCM allows (i) the design of high-capacity point-to-point (P2P) and -multipoint (P2MP) optical networks; (ii) simplified aggregation with passive optics; and (iii) connections between low- and high-speed transceivers. Furthermore, DSCM-based networks reduce the number of opto-electro-opto stages, halve the number of bookended transceivers, and provide a better match for existing hub-and-spoke (H&S) traffic patterns in fast-growing and dynamic access/metro segments. A DSCM-based transceiver will pave the way for the deployment of next-generation flexible, adaptable, and scalable software-configurable optical networks. Key steps and elements to realize this solution are laid out, and promising applications outlined. The first real-time experimental results of coherent P2MP transceivers are presented.","1558-2213","","10.1109/JLT.2022.3211466","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9907792","Digital subcarrier multiplexing;point-to-multipoint;coherent access;metro aggregation;fronthaul;5G","Transceivers;Point-to-multipoint communications;Traffic control;Passive optical networks;Costs;Optical transmitters;OFDM","","61","","62","CCBYNCND","3 Oct 2022","","","IEEE","IEEE Journals"
"IoT Microservice Deployment in Edge-Cloud Hybrid Environment Using Reinforcement Learning","L. Chen; Y. Xu; Z. Lu; J. Wu; K. Gai; P. C. K. Hung; M. Qiu","School of Computer Science and the Engineering Research Center of Cyber Security Auditing and Monitoring, Ministry of Education, Fudan University, Shanghai, China; School of Computer Science and the Engineering Research Center of Cyber Security Auditing and Monitoring, Ministry of Education, Fudan University, Shanghai, China; School of Computer Science and the Shanghai Blockchain Engineering Research Center, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Cyberspace Security, Beijing Institute of Technology, Beijing, China; Faulty of Business and Information Technology, Ontario Tech University, Oshawa, Canada; Department of Computer Science, Texas A&M University--Commerce, Commerce, TX, USA",IEEE Internet of Things Journal,"5 Aug 2021","2021","8","16","12610","12622","The edge-cloud hybrid environment requires complex deployment strategies to enable the smart Internet-of-Things (IoT) system. However, current service deployment strategies use simple, generalized heuristics and ignore the heterogeneous characteristics in the edge-cloud hybrid environment. In this article, we devise a method to find a microservice-based service deployment strategy that can reduce the average waiting time of IoT devices in the hybrid environment. For this purpose, we first propose a microservice-based deployment problem (MSDP) based on the heterogeneous and dynamic characteristics in the edge-cloud hybrid environment, including heterogeneity of edge server capacities, dynamic geographical information of IoT devices, and changing device preference for applications and complex application structures. We then propose a multiple buffer deep deterministic policy gradient (MB_DDPG) to provide more preferable service deployment solutions. Our algorithm leverages reinforcement learning and neural network to learn a deployment strategy without any human instruction. Therefore, the service provider can make full use of limited resources to improve the Quality of Service (QoS). Finally, we implement MB_DDPG based on real-world data sets and some synthetic data, and we also implement another two algorithms, genetic algorithm and random algorithm, as a contrast. The experimental results demonstrate that MB_DDPG is able to learn a preferable strategy which, in terms of average waiting time, outperforms genetic algorithm and the random algorithm by 32% and 44%, respectively.","2327-4662","","10.1109/JIOT.2020.3014970","National Key Research and Development Program of China(grant numbers:2019YFB1405000); National Natural Science Foundation of China(grant numbers:61873309,61972034,61572137,61728202); Shanghai Innovation Action Plan Project(grant numbers:19510710500,18510760200,18510732000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9162056","Edge-cloud hybrid environment;microservice deployment;reinforcement learning;smart Internet-of-Things (IoT) system","Servers;Cloud computing;Heuristic algorithms;Internet of Things;Edge computing;Quality of service;Image edge detection","","60","","45","IEEE","7 Aug 2020","","","IEEE","IEEE Journals"
"One-Dimensional CNN Approach for ECG Arrhythmia Analysis in Fog-Cloud Environments","O. Cheikhrouhou; R. Mahmud; R. Zouari; M. Ibrahim; A. Zaguia; T. N. Gia","CES Laboratory, National School of Engineers of Sfax, University of Sfax, Sfax, Tunisia; School of Computing Technologies, STEM College, RMIT University, Melbourne, VIC, Australia; CES Laboratory, National School of Engineers of Sfax, University of Sfax, Sfax, Tunisia; Department of Information Technology, The University of Haripur, Haripur, Pakistan; Department of Computer Science, College of Computers and Information Technology, Taif University, Taif, Saudi Arabia; Department of Computing, University of Turku, Turku, Finland",IEEE Access,"27 Jul 2021","2021","9","","103513","103523","Cardiovascular diseases are considered the number one cause of death across the globe which can be primarily identified by the abnormal heart rhythms of the patients. By generating electrocardiogram (ECG) signals, wearable Internet of Things (IoT) devices can consistently track the patient's heart rhythms. Although Cloud-based approaches for ECG analysis can achieve some levels of accuracy, they still have some limitations, such as high latency. Conversely, the Fog computing infrastructure is more powerful than edge devices but less capable than Cloud computing for executing compositionally intensive data analytic software. The Fog infrastructure can consist of Fog-based gateways directly connected with the wearable devices to offer many advanced benefits, including low latency and high quality of services. To address these issues, a modular one-dimensional convolution neural network (1D-CNN) approach is proposed in this work. The inference module of the proposed approach is deployable over the Fog infrastructure for analysing the ECG signals and initiating the emergency countermeasures within a minimum delay, whereas its training module is executable on the computationally enriched Cloud data centers. The proposed approach achieves the F1-measure score ≈1 on the MIT-BIH Arrhythmia database when applying GridSearch algorithm with the cross-validation method. This approach has also been implemented on a single-board computer and Google Colab-based hybrid Fog-Cloud infrastructure and embodied to a remote patient monitoring system that shows 25% improvement in the overall response time.","2169-3536","","10.1109/ACCESS.2021.3097751","Taif University Researchers Supporting Project through Taif University, Taif, Saudi Arabia(grant numbers:TURSP-2020/114); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9489314","Internet of Things;ECG analysis;1D-CNN;fog computing;hybrid fog-cloud;heart disease","Electrocardiography;Cloud computing;Logic gates;Feature extraction;Security;Edge computing;Wearable computers","","60","","47","CCBY","16 Jul 2021","","","IEEE","IEEE Journals"
"BlockEdge: Blockchain-Edge Framework for Industrial IoT Networks","T. Kumar; E. Harjula; M. Ejaz; A. Manzoor; P. Porambage; I. Ahmad; M. Liyanage; A. Braeken; M. Ylianttila","Centre for Wireless Communication, University of Oulu, Oulu, Finland; Centre for Wireless Communication, University of Oulu, Oulu, Finland; Centre for Wireless Communication, University of Oulu, Oulu, Finland; Rovio Entertainment Company, Espoo, Finland; Centre for Wireless Communication, University of Oulu, Oulu, Finland; VTT Technical Research Centre of Finland, Espoo, Finland; Centre for Wireless Communication, University of Oulu, Oulu, Finland; Industrial Sciences Department (INDI), Vrije Universiteit Brussel, Brussels, Belgium; Centre for Wireless Communication, University of Oulu, Oulu, Finland",IEEE Access,"28 Aug 2020","2020","8","","154166","154185","Industry 4.0 encompasses a promise of a new industrial revolution in terms of providing secure, intelligent, autonomous and self-adaptive industrial IoT (IIoT) networks. Key industrial applications and systems will be significantly more complex due to the involvement of the vast number of different devices and diverse nature of various stakeholders and service providers. These complex industrial processes, services and applications also have strict requirements in terms of performance - latency in particular - and resource-efficiency, together with high standards for security and trust. In this context, Blockchain and Edge Computing emerge as prominent technologies to address the mentioned essential requirements and to further strengthen the rise of the new era of digitization. The Edge computing paradigm ensures low latency services for IIoT applications while optimizing the network usage, whereas Blockchain provides a decentralized way for ensuring data integrity, trust and security. In this paper, we propose a `BlockEdge' framework that combines these two enabling technologies to address some of the critical issues faced by the current IIoT networks. We verify the feasibility of our approach by evaluating the performance and resource-efficiency of BlockEdge in terms of latency, power consumption and network usage, through simulations against non-Blockchain solution.","2169-3536","","10.1109/ACCESS.2020.3017891","Academy of Finland through the projects Industrial Edge, SecureConnet, WiFiUS: Massive IoT, 6G Flagship Project(grant numbers:318927); European Union through the Resilient and Secure Multi-controller Communication Platform for 5G Networks (RESPONSE 5G)(grant numbers:789658); Technology Industries of Finland Centennial Foundation through the Edge Computing Enhanced by Artificial Intelligence (MEC-AI) project; Jane ja Aatos Erkon Säätiö; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9171246","Blockchain;edge computing;fog computing;cloud computing;industrial IoT;industry 4.0;performance evaluation","Blockchain;Edge computing;Computational modeling;Industries;Security","","60","","72","CCBY","19 Aug 2020","","","IEEE","IEEE Journals"
"Service Coverage for Satellite Edge Computing","Q. Li; S. Wang; X. Ma; Q. Sun; H. Wang; S. Cao; F. Yang","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Key Laboratory of Space Utilization, Technology and Engineering Center for Space Utilization, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Space Utilization, Technology and Engineering Center for Space Utilization, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China",IEEE Internet of Things Journal,"22 Dec 2021","2022","9","1","695","705","Recently, increasing investments in satellite-related technologies make the low earth orbit (LEO) satellite constellation a strong complement to terrestrial networks. To mitigate the limitations of the traditional satellite constellation “bent-pipe” architecture, satellite edge computing (SEC) has been proposed by placing computing resources at the LEO satellite constellation. Most existing works focus on space-air-ground integrated network architecture and SEC computing framework. Beyond these works, we are the first to investigate how to efficiently deploy services on the SEC nodes to realize robustness aware service coverage with constrained resources. Facing the challenges of spatial-temporal system dynamics and service coverage-robustness conflict, we propose a novel online service placement algorithm with a theoretical performance guarantee by leveraging Lyapunov optimization and Gibbs sampling. Extensive simulation results show that our algorithm can improve the service coverage by  $4.3\times $  compared with the baseline.","2327-4662","","10.1109/JIOT.2021.3085129","Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B010164002); NSFC(grant numbers:62032003,61922017); Open Research Fund of Key Laboratory of Space Utilization, Chinese Academy of Sciences(grant numbers:LSUKFJJ-2019-03); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9444334","Satellite edge computing (SEC);service coverage;space-air-ground integrated networks","Satellites;Robustness;Low earth orbit satellites;Edge computing;Orbits;Internet of Things;Computer architecture","","60","","51","IEEE","31 May 2021","","","IEEE","IEEE Journals"
"Materializing the Promises of Cognitive IoT: How Cognitive Buildings Are Shaping the Way","J. Ploennigs; A. Ba; M. Barry","Smarter Cities Technology Centre, IBM Dublin Technology Campus, IBM Research-Ireland, Dublin, Ireland; Smarter Cities Technology Centre, IBM Dublin Technology Campus, IBM Research-Ireland, Dublin, Ireland; Smarter Cities Technology Centre, IBM Dublin Technology Campus, IBM Research-Ireland, Dublin, Ireland",IEEE Internet of Things Journal,"9 Aug 2018","2018","5","4","2367","2374","Relatively tiny examples have demonstrated the potential of cognitive IoT (CIoT) in its full-stack, namely, semantic modeling, learning and reasoning over sensors data, and machine learning, to uncover and expose actionable insights via advanced user interfaces. In this paper, we make the case for the feasibility of CIoT in all of its dimensions. We devise a CIoT architecture that integrates thousands of sensors present in our buildings in order to learn the buildings' behavior and intuitively assist users in diagnosing and mitigating undesired events. With our architecture, we place emphasis on the scalability and flexibility that reduce the configuration effort. The solution shows the potential of CIoT to create highly scalable, adaptable and interactive IoT systems functioning for buildings and capable of addressing the challenges encountered in the realm of homes, Smart Cities and Industry 4.0.","2327-4662","","10.1109/JIOT.2017.2755376","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8047951","Auagmented reality;automated analytics;cognitive Internet of Things (CIoT);fog and cloud computing;learning and reasoning;semantic modeling","Buildings;Computer architecture;Architecture;Cognition;Semantics;Cognitive systems;Internet of Things","","59","","31","IEEE","21 Sep 2017","","","IEEE","IEEE Journals"
"Internet of Things: Device Capabilities, Architectures, Protocols, and Smart Applications in Healthcare Domain","M. M. Islam; S. Nooruddin; F. Karray; G. Muhammad","Department of Electrical and Computer Engineering, Centre for Pattern Analysis and Machine Intelligence, University of Waterloo, Waterloo, Canada; Department of Electrical and Computer Engineering, Centre for Pattern Analysis and Machine Intelligence, University of Waterloo, Waterloo, Canada; Department of Electrical and Computer Engineering, Centre for Pattern Analysis and Machine Intelligence, University of Waterloo, Waterloo, Canada; Department of Computer Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia",IEEE Internet of Things Journal,"6 Feb 2023","2023","10","4","3611","3641","Nowadays, the Internet has spread to practically every country around the world and is having unprecedented effects on people’s lives. The Internet of Things (IoT) is getting more popular and has a high level of interest in both practitioners and academicians in the age of wireless communication due to its diverse applications. The IoT is a technology that enables everyday things to become savvier, everyday computation toward becoming intellectual, and everyday communication to become a little more insightful. In this article, the most common and popular IoT device capabilities, architectures, and protocols are demonstrated in brief to provide a clear overview of the IoT technology to the researchers in this area. The common IoT device capabilities, including hardware (Raspberry Pi, Arduino, and ESP8266) and software (operating systems (OSs), and built-in tools) platforms are described in detail. The widely used architectures that have recently evolved and used are the three-layer architecture, service-oriented architecture, and middleware-based architecture. The popular protocols for IoT are demonstrated which include constrained application protocol, message queue telemetry transport, extensible messaging and presence protocol, advanced message queuing protocol, data distribution service, low power wireless personal area network, Bluetooth low energy, and ZigBee that are frequently utilized to develop smart IoT applications. Additionally, this research provides an in-depth overview of the potential healthcare applications based on IoT technologies in the context of addressing various healthcare concerns. Finally, this article summarizes state-of-the-art knowledge, highlights open issues and shortcomings, and provides recommendations for further studies which would be quite beneficial to anyone with a desire to work in this field and make breakthroughs to get expertise in this area.","2327-4662","","10.1109/JIOT.2022.3228795","Natural Sciences and Engineering Research Council (NSERC) Discovery Grant, Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9983826","Communication protocol;device capabilities;healthcare applications;Internet of Things (IoT);IoT architecture","Internet of Things;Medical services;Computer architecture;Protocols;Hardware;Software;Security","","59","","250","IEEE","13 Dec 2022","","","IEEE","IEEE Journals"
"Towards Low-Latency Service Delivery in a Continuum of Virtual Resources: State-of-the-Art and Research Directions","J. Santos; T. Wauters; B. Volckaert; F. De Turck","Department of Information Technology, imec, IDLab, Ghent University, Gent, Belgium; Department of Information Technology, imec, IDLab, Ghent University, Gent, Belgium; Department of Information Technology, imec, IDLab, Ghent University, Gent, Belgium; Department of Information Technology, imec, IDLab, Ghent University, Gent, Belgium",IEEE Communications Surveys & Tutorials,"19 Nov 2021","2021","23","4","2557","2589","The advent of softwarized networks has enabled the deployment of chains of virtual network and service components on computational resources from the cloud up to the edge, creating a continuum of virtual resources. The next generation of low latency applications (e.g., Virtual Reality (VR), autonomous cars) adds even more stringent requirements to the infrastructure, calling for considerable advancements towards cloud-native micro-service-based architectures. This article presents a comprehensive survey on ongoing research aiming to effectively support low latency services throughout their execution lifetime in next-generation networks. The current state-of-the-art is critically reviewed to identify the most promising trends that will strongly impact the full applicability and high performance of low latency services. This article proposes a taxonomy as well as specific evaluation criteria to classify research across different domains addressing low latency service delivery. Current architectural paradigms such as Multi-access Edge Computing (MEC) and Fog Computing (FC) alongside novel trends on communication networks are discussed. Among these, the integration of Machine Learning (ML) and Artificial intelligence (AI) is introduced as a key research field in current literature towards autonomous network management. A discussion on open challenges and future research directions on low-latency service delivery leads to the conclusion, offering lessons learned and prospects on emerging use cases such as Extended Reality (XR), in which novel trends will play a major role.","1553-877X","","10.1109/COMST.2021.3095358","Scientific Research-Flanders (FWO-V) through the Project “Intelligent Dense and Longe Range IoT Networks (IDEAL-IoT)”(grant numbers:S004017N); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9476028","Low latency;next-generation networks;cloud-native;autonomous networks;orchestration;5G/6G","Tutorials;Next generation networking;Taxonomy;Market research;5G mobile communication;Computer architecture;Cloud computing","","57","","189","IEEE","7 Jul 2021","","","IEEE","IEEE Journals"
"An IoT-Aware Approach for Elderly-Friendly Cities","R. Mulero; A. Almeida; G. Azkune; P. Abril-Jiménez; M. T. Arredondo Waldmeyer; M. Páramo Castrillo; L. Patrono; P. Rametta; I. Sergi","DeustoTech–Deusto Institute of Technology, University of Deusto, Bilbao, Spain; DeustoTech–Deusto Institute of Technology, University of Deusto, Bilbao, Spain; DeustoTech–Deusto Institute of Technology, University of Deusto, Bilbao, Spain; Universidad Politécnica de Madrid, Madrid, Spain; Universidad Politécnica de Madrid, Madrid, Spain; Universidad Politécnica de Madrid, Madrid, Spain; University of Salento, Lecce, Italy; University of Salento, Lecce, Italy; University of Salento, Lecce, Italy",IEEE Access,"7 Mar 2018","2018","6","","7941","7957","The ever-growing life expectancy of people requires the adoption of proper solutions for addressing the particular needs of elderly people in a sustainable way, both from service provision and economic point of view. Mild cognitive impairments and frailty are typical examples of elderly conditions which, if not timely addressed, can turn out into more complex diseases that are harder and costlier to treat. Information and communication technologies, and in particular Internet of Things technologies, can foster the creation of monitoring and intervention systems, both on an ambient-assisted living and smart city scope, for early detecting behavioral changes in elderly people. This allows to timely detect any potential risky situation and properly intervene, with benefits in terms of treatment's costs. In this context, as part of the H2020-funded City4Age project, this paper presents the data capturing and data management layers of the whole City4Age platform. In particular, this paper deals with an unobtrusive data gathering system implementation to collect data about daily activities of elderly people, and with the implementation of the related linked open data (LOD)-based data management system. The collected data are then used by other layers of the platform to perform risk detection algorithms and generate the proper customized interventions. Through the validation of some use-cases, it is demonstrated how this scalable approach, also characterized by unobtrusive and low-cost sensing technologies, can produce data with a high level of abstraction useful to define a risk profile of each elderly person.","2169-3536","","10.1109/ACCESS.2018.2800161","City4Age Project through the European Union’s Horizon 2020 Research and Innovation Programme(grant numbers:689731); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8277181","Elderly;IoT;LOD;MCI;monitoring systems;risk detection;smart environments","Senior citizens;Middleware;Smart cities;Monitoring;Sensors;Europe;Aging","","56","","36","OAPA","31 Jan 2018","","","IEEE","IEEE Journals"
"Trust Management in Social Internet of Things: Architectures, Recent Advancements, and Future Challenges","W. Z. Khan; Q. -u. -A. Arshad; S. Hakak; M. K. Khan; Saeed-Ur-Rehman","Center of Excellence in Information Assurance, King Saud University, Riyadh, Saudi Arabia; Center of Excellence in Information Assurance, King Saud University, Riyadh, Saudi Arabia; Canadian Institute for Cybersecurity, Faculty of Computer Science, University of New Brunswick, Fredericton, Canada; Center of Excellence in Information Assurance, King Saud University, Riyadh, Saudi Arabia; Department of Computer Science, COMSATS University Islamabad, Islamabad, Pakistan",IEEE Internet of Things Journal,"7 May 2021","2021","8","10","7768","7788","Social Internet of Things (SIoT) is an extension of the Internet of Things (IoT) that converges with social networking concepts to create social networks of interconnected smart objects. This convergence allows the enrichment of the two paradigms, resulting into new ecosystems. While IoT follows two interaction paradigms, human to human (H2H) and thing to thing (T2T), SIoT adds on human-to-thing (H2T) interactions. SIoT enables smart “social objects” that intelligently mimic the social behavior of human in the daily life. These social objects (SOs) are equipped with social functionalities capable of discovering other SOs in the surroundings and establishing social relationships. They crawl through the social network of objects for the sake of searching for services and information of interest. The notion of trust and trustworthiness in social communities formed in SIoT is still new and in an early stage of investigation. In this article, our contributions are threefold. First, we present the fundamentals of SIoT and trust concepts in SIoT, clarifying the similarities and differences between IoT and SIoT. Second, we categorize the trust management solutions proposed so far in the literature for SIoT over the last six years and provide a comprehensive review. We then perform a comparison of the state-of-the-art trust management schemes devised for SIoT by performing comparative analysis in terms of trust management process. Third, we identify and discuss the challenges and requirements in the emerging new wave of SIoT, and also highlight the challenges in developing trust and evaluating trustworthiness among the interacting SOs.","2327-4662","","10.1109/JIOT.2020.3039296","Deputyship for Research and Innovation, “Ministry of Education” in Saudi Arabia(grant numbers:IFKSURP-43); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264256","Challenges;Internet of Things (IoT);Social IoT (SIoT);social objects (SOs);social relationships;trust management","Internet of Things;Trust management;Social networking (online);Analytical models;Scalability;Vehicle dynamics;Search problems","","55","","99","IEEE","19 Nov 2020","","","IEEE","IEEE Journals"
"Cryptomining Detection in Container Clouds Using System Calls and Explainable Machine Learning","R. R. Karn; P. Kudva; H. Huang; S. Suneja; I. M. Elfadel","Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, UAE; IBM Research, Yorktown Heights, NY, USA; IBM Research, Yorktown Heights, NY, USA; IBM Research, Yorktown Heights, NY, USA; Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, UAE",IEEE Transactions on Parallel and Distributed Systems,"26 Oct 2020","2021","32","3","674","691","The use of containers in cloud computing has been steadily increasing. With the emergence of Kubernetes, the management of applications inside containers (or pods) is simplified. Kubernetes allows automated actions like self-healing, scaling, rolling back, and updates for the application management. At the same time, security threats have also evolved with attacks on pods to perform malicious actions. Out of several recent malware types, cryptomining has emerged as one of the most serious threats with its hijacking of server resources for cryptocurrency mining. During application deployment and execution in the pod, a cryptomining process, started by a hidden malware executable can be run in the background, and a method to detect malicious cryptomining software running inside Kubernetes pods is needed. One feasible strategy is to use machine learning (ML) to identify and classify pods based on whether or not they contain a running process of cryptomining. In addition to such detection, the system administrator will need an explanation as to the reason(s) of the ML's classification outcome. The explanation will justify and support disruptive administrative decisions such as pod removal or its restart with a new image. In this article, we describe the design and implementation of an ML-based detection system of anomalous pods in a Kubernetes cluster by monitoring Linux-kernel system calls (syscalls). Several types of cryptominers images are used as containers within an anomalous pod, and several ML models are built to detect such pods in the presence of numerous healthy cloud workloads. Explainability is provided using SHAP, LIME, and a novel auto-encoding-based scheme for LSTM models. Seven evaluation metrics are used to compare and contrast the explainable models of the proposed ML cryptomining detection engine.","1558-2183","","10.1109/TPDS.2020.3029088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9215018","Cryptomining;docker;kubernetes;containers;machine learning;explainability;pod;anomaly","Containers;Cloud computing;Malware;Machine learning;Cryptocurrency;Data mining","","54","","93","CCBY","6 Oct 2020","","","IEEE","IEEE Journals"
"Blockchain-Powered Value Creation in the 5G and Smart Grid Use Cases","K. Valtanen; J. Backman; S. Yrjölä","VTT Technical Research Centre of Finland Ltd., Oulu, Finland; VTT Technical Research Centre of Finland Ltd., Oulu, Finland; Nokia Corporate Strategy and Development, Oulu, Finland",IEEE Access,"7 Mar 2019","2019","7","","25690","25707","Before the implementation of a solution, it is cost-efficient and practical to be able to evaluate and analyze the expected value of use cases. Especially, this is emphasized in blockchain (BC) use cases, which typically have a wide business ecosystem and possibly disruptive business models. This paper presents two BC use case value evaluations and results. The IoT use cases were selected from two different industry segments: telecommunications-oriented 5G network slice brokering and the energy industry-related internal electricity allocation in a housing society. The use case value was assessed by applying a resource configuration framework and 4C - commerce, context, content, and connection - business model typology against BC and smart contracts characteristics and capabilities. The results derived from the data collected from the expert workshops proved the expected value of the use cases, and in general, the feasibility of BC technology for facilitating various value-creating resource configuration processes was shown. Furthermore, the resource configuration framework proved to be a valuable theoretical approach for analyzing and developing also the BC-enabled novel use cases and business models. According to the findings, further development of the framework is proposed with an introduced novel decentralized resource configuration prototype that can replace predominant platform-based business models.","2169-3536","","10.1109/ACCESS.2019.2900514","Blockchains Boosting Finnish Industry (BOND) project through Business Finland, the Finnish innovation funding, trade, investment, and travel promotion organization (formerly Tekes, the Finnish Funding Agency for Innovation); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8648405","Blockchain;business models;decentralization;resource configuration;smart grid;4C typology;5G","Business;Blockchain;Biological system modeling;Industries;5G mobile communication;Conferences;Smart grids","","54","","73","CCBY","21 Feb 2019","","","IEEE","IEEE Journals"
"Survey on Placement Methods in the Edge and Beyond","B. Sonkoly; J. Czentye; M. Szalay; B. Németh; L. Toka","Department of Telecommunications and Media Informatics, Faculty of Electrical Engineering and Informatics, MTA-BME Network Softwarization Research Group, Budapest University of Technology and Economics, Budapest, Hungary; Department of Telecommunications and Media Informatics, Faculty of Electrical Engineering and Informatics, MTA-BME Network Softwarization Research Group, Budapest University of Technology and Economics, Budapest, Hungary; Department of Telecommunications and Media Informatics, Faculty of Electrical Engineering and Informatics, MTA-BME Network Softwarization Research Group, Budapest University of Technology and Economics, Budapest, Hungary; Department of Telecommunications and Media Informatics, Faculty of Electrical Engineering and Informatics, MTA-BME Network Softwarization Research Group, Budapest University of Technology and Economics, Budapest, Hungary; Department of Telecommunications and Media Informatics, Faculty of Electrical Engineering and Informatics, MTA-BME Network Softwarization Research Group, Budapest University of Technology and Economics, Budapest, Hungary",IEEE Communications Surveys & Tutorials,"19 Nov 2021","2021","23","4","2590","2629","Edge computing is a (r)evolutionary extension of traditional cloud computing. It expands central cloud infrastructure with execution environments close to the users in terms of latency in order to enable a new generation of cloud applications. This paradigm shift has opened the door for telecommunications operators, mobile and fixed network vendors: they have joined the cloud ecosystem as essential stakeholders considerably influencing the future success of the technology. A key problem in edge computing is the optimal placement of computational units (virtual machines, containers, tasks or functions) of novel distributed applications. These components are deployed to a geographically distributed virtualized infrastructure and heterogeneous networking technologies are invoked to connect them while respecting quality requirements. The optimal hosting environment should be selected based on multiple criteria by novel scheduler algorithms which can cope with the new challenges of distributed cloud architecture where networking aspects cannot be ignored. The research community has dedicated significant efforts to this topic during recent years and a vast number of theoretical results have been published addressing different variants of the related mathematical problems. However, a comprehensive survey focusing on the technical and analytical aspects of the placement problem in various edge architectures is still missing. This survey provides a comprehensive summary and a structured taxonomy of the vast research on placement of computational entities in emerging edge infrastructures. Following the given taxonomy, the research papers are analyzed and categorized according to several dimensions, such as the capabilities of the underlying platforms, the structure of the supported services, the problem formulation, the applied mathematical methods, the objectives and constraints incorporated in the optimization problems, and the complexity of the proposed methods. We summarize the gained insights and important lessons learned, and finally, we reveal some important research gaps in the current literature.","1553-877X","","10.1109/COMST.2021.3101460","Ministry of Innovation and Technology of Hungary from the National Research, Development and Innovation Fund through 2018-2.1.17-TÉT-KR, FK_20; 2019-2.1.13-TÉT-IN Funding Schemes(grant numbers:2018-2.1.17-TÉT-KR-2018-00012,135074,2019-2.1.13-TÉT-IN-2020-00021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9502167","Edge/fog computing;MEC;cloudlets;resource orchestration;function placement optimization;offloading","Cloud computing;Optimization;Data centers;Taxonomy;Tutorials;Telecommunications;Task analysis","","53","","204","CCBY","30 Jul 2021","","","IEEE","IEEE Journals"
"Zero Touch Management: A Survey of Network Automation Solutions for 5G and 6G Networks","E. Coronado; R. Behravesh; T. Subramanya; A. Fernàndez-Fernàndez; M. S. Siddiqui; X. Costa-Pérez; R. Riggio","Software Networks, i2CAT Foundation, Barcelona, Spain; Wireless and Networked Systems, Fondazione Bruno Kessler, Trento, Italy; Architecture, Security and Automation, Nokia Standards, Munich, Germany; Software Networks, i2CAT Foundation, Barcelona, Spain; Software Networks, i2CAT Foundation, Barcelona, Spain; AI-Driven Systems, i2CAT Foundation, Barcelona, Spain; Information Engineering Department, Polytechnic University of Marche, Ancona, Italy",IEEE Communications Surveys & Tutorials,"21 Nov 2022","2022","24","4","2535","2578","Mobile networks are facing an unprecedented demand for high-speed connectivity originating from novel mobile applications and services and, in general, from the adoption curve of mobile devices. However, coping with the service requirements imposed by current and future applications and services is very difficult since mobile networks are becoming progressively more heterogeneous and more complex. In this context, a promising approach is the adoption of novel network automation solutions and, in particular, of zero-touch management techniques. In this work, we refer to zero-touch management as a fully autonomous network management solution with human oversight. This survey sits at the crossroad between zero-touch management and mobile and wireless network research, effectively bridging a gap in terms of literature review between the two domains. In this paper, we first provide a taxonomy of network management solutions. We then discuss the relevant state-of-the-art on autonomous mobile networks. The concept of zero-touch management and the associated standardization efforts are then introduced. The survey continues with a review of the most important technological enablers for zero-touch management. The network automation solutions from the RAN to the core network, including end-to-end aspects such as security, are then surveyed. Finally, we close this article with the current challenges and research directions.","1553-877X","","10.1109/COMST.2022.3212586","European Union’s Horizon 2020 through the Project AI@EDGE; EU(grant numbers:101015922,5GZORRO); EU(grant numbers:871533); CERCA Programme/Generalitat de Catalunya; EU “NextGenerationEU/PRTR,” MCIN and AEI (Spain)(grant numbers:IJC2020-043058-I); ONOFRE-3 through the Project MCIN/AEI/10.13039/501100011033(grant numbers:PID2020-112675RB-C43); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9913206","Network management;autonomous networks;zero-touch management;mobile networking;wireless networking;5G;6G","5G mobile communication;Automation;Radio access networks;Protocols;Tutorials;Terminology;Cloud computing","","53","","284","CCBY","6 Oct 2022","","","IEEE","IEEE Journals"
"A Survey on In-Network Computing: Programmable Data Plane and Technology Specific Applications","S. Kianpisheh; T. Taleb","Centre for Wireless Communications, University of Oulu, Oulu, Finland; Centre for Wireless Communications, University of Oulu, Oulu, Finland",IEEE Communications Surveys & Tutorials,"23 Feb 2023","2023","25","1","701","761","In comparison with cloud computing, edge computing offers processing at locations closer to end devices and reduces the user experienced latency. The new recent paradigm of in-network computing employs programmable network elements to compute on the path and prior to traffic reaching the edge or cloud servers. It advances common edge/cloud server based computing through proposing line rate processing capabilities at closer locations to the end devices. This paper discusses use cases, enabler technologies and protocols for in-network computing. According to our study, considering programmable data plane as an enabler technology, potential in-network computing applications are in-network analytics, in-network caching, in-network security, and in-network coordination. There are also technology specific applications of in-network computing in the scopes of cloud computing, edge computing, 5G/6G, and NFV. In this survey, the state of the art, in the framework of the proposed categorization, is reviewed. Furthermore, comparisons are provided in terms of a set of proposed criteria which assess the methods from the aspects of methodology, main results, as well as application-specific criteria. Finally, we discuss lessons learned and highlight some potential research directions.","1553-877X","","10.1109/COMST.2022.3213237","European Unions Horizon 2020 Research and Innovation Program through the Charity and aerOS Projects(grant numbers:101016509,101069732); Academy of Finland 6Genesis Project(grant numbers:318927); Academy of Finland IDEA-MILL Project(grant numbers:352428); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9919270","In-network computing;programmable data plane;software defined networking;cloud computing;edge computing;6G;and network function virtualization","Servers;Cloud computing;Switches;Tutorials;Protocols;Field programmable gate arrays;Fabrics","","53","","177","CCBY","14 Oct 2022","","","IEEE","IEEE Journals"
"Security and Trust in the 6G Era","V. Ziegler; P. Schneider; H. Viswanathan; M. Montag; S. Kanugovi; A. Rezaki","Nokia Bell Labs, Munich, Germany; Nokia Bell Labs, Munich, Germany; Nokia Bell Labs, Murray Hill, NJ, USA; Nokia Bell Labs, Munich, Germany; Nokia Standards, Bengaluru, India; Nokia Standards, Munich, Germany",IEEE Access,"26 Oct 2021","2021","9","","142314","142327","A comprehensive set of security technology enablers will be critically required for communication systems for the 6G era of the 2030s. Trustworthiness must be assured across IoT, heterogenous cloud and networks, devices, sub-networks, and applications. The 6G threat vector will be defined by 6G architectural disaggregation, open interfaces and an environment with multiple stakeholders. Broadly decomposed into domains of cyber-resilience, privacy and trust and their respective intersection, we explore relevant security technology enablers including automated software creation and automated closed-loop security operation, privacy preserving technologies, hardware and cloud embedded anchors of trust, quantum-safe security, jamming protection and physical layer security as well as distributed ledger technologies. Artificial intelligence and machine learning (AI/ML) as a key technology enabler will be pervasive and of pivotal relevance across the security technology stack and architecture. A novel vision for a trustworthy Secure Telecom Operation Map is developed as part of the automated closed loop operations paradigm.","2169-3536","","10.1109/ACCESS.2021.3120143","Nokia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9570274","6G;security;cyber-resilience;privacy;trustworthiness;sub-networks;wireless networks","Security;6G mobile communication;5G mobile communication;Privacy;Cloud computing;Authentication;3GPP","","53","","44","CCBY","14 Oct 2021","","","IEEE","IEEE Journals"
"Toward Software-Defined Networking-Based IoT Frameworks: A Systematic Literature Review, Taxonomy, Open Challenges and Prospects","S. Siddiqui; S. Hameed; S. A. Shah; I. Ahmad; A. Aneiba; D. Draheim; S. Dustdar","Department of Computer Science, NUCES, Karachi, Pakistan; Department of Computer Science, NUCES, Karachi, Pakistan; School of Computing and Digital Technology, Birmingham City University, Birmingham, U.K.; VTT Technical Research Centre of Finland, Espoo, Finland; School of Computing and Digital Technology, Birmingham City University, Birmingham, U.K.; Information Systems Group, Tallinn University of Technology, Tallinn, Estonia; Distributed Systems Group, Vienna University of Technology, Vienna, Austria",IEEE Access,"12 Jul 2022","2022","10","","70850","70901","Internet of Things (IoT) is characterized as one of the leading actors for the next evolutionary stage in the computing world. IoT-based applications have already produced a plethora of novel services and are improving the living standard by enabling innovative and smart solutions. However, along with its rapid adoption, IoT technology also creates complex challenges regarding the management of IoT networks due to its resource limitations (computational power, energy, and security). Hence, it is urgently needed to refine the IoT-based application’s architectures to robustly manage the overall IoT infrastructure. Software-defined networking (SDN) has emerged as a paradigm that offers software-based controllers to manage hardware infrastructure and traffic flow on a network effectively. SDN architecture has the potential to provide efficient and reliable IoT network management. This research provides a comprehensive survey investigating the published studies on SDN-based frameworks to address IoT management issues in the dimensions of fault tolerance, energy management, scalability, load balancing, and security service provisioning within the IoT networks. We conducted a Systematic Literature Review (SLR) on the research studies (published from 2010 to 2022) focusing on SDN-based IoT management frameworks. We provide an extensive discussion on various aspects of SDN-based IoT solutions and architectures. We elaborate a taxonomy of the existing SDN-based IoT frameworks and solutions by classifying them into categories such as network function virtualization, middleware, OpenFlow adaptation, and blockchain-based management. We present the research gaps by identifying and analyzing the key architectural requirements and management issues in IoT infrastructures. Finally, we highlight various challenges and a range of promising opportunities for future research to provide a roadmap for addressing the weaknesses and identifying the benefits from the potentials offered by SDN-based IoT solutions.","2169-3536","","10.1109/ACCESS.2022.3188311","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9815064","Internet of Things (IoT);software-defined networking (SDN);SDN-based IoT management frameworks;systematic literature review;network function virtualization;OpenFlow;middleware;blockchain;security management;fault tolerance;load balancing;scalability;energy management","Internet of Things;Blockchains;Middleware;Load management;Computer architecture;Sensors;Security","","53","","230","CCBY","4 Jul 2022","","","IEEE","IEEE Journals"
"Reliable and Resilient AI and IoT-Based Personalised Healthcare Services: A Survey","N. Taimoor; S. Rehman","Institute of Computer Technology, Technische Universität Wien (TU Wien), Wien, Austria; Institute of Computer Technology, Technische Universität Wien (TU Wien), Wien, Austria",IEEE Access,"4 Jan 2022","2022","10","","535","563","Recent technological (e.g., IoT, 5G), and economic (e.g., UN 2030 Sustainable Development Goals) developments have transformed the healthcare sector towards more personalized and IoT-based healthcare services. These services are realized through control and monitoring applications that are typically developed using artificial intelligence (AI)/machine learning (ML) based algorithms, that play a significant role to highlight the efficiency of traditional healthcare systems. Current personalized healthcare services are dedicated in a specific environment to support technological personalization (e.g., personalized gadgets/devices). However, they are unable to consider different inter-related health conditions, leading to inappropriate diagnosis and affect sustainability and the long-term health/life of patients. Towards this problem, the state-of-the-art Healthcare 5.0 technology has evolved that supersede previous healthcare technologies. The goal of healthcare 5.0 is to achieve a fully autonomous healthcare service, that takes into account the interdependent effect of different health conditions of a patient. This paper conducts a comprehensive survey on personalized healthcare services. In particular, we first present an overview of key requirements of comprehensive personalized healthcare services (CPHS) in modern healthcare Internet of Things (HIoT), including the definition of personalization and an example use case scenario as a representative for modern HIoT. Second, we explored a fundamental three-layer architecture for IoT-based healthcare systems using both AI and non-AI-based approaches, considering key requirements for CPHS followed by their strengths and weaknesses in the frame of personalized healthcare services. Third, we highlighted different security threats against each layer of IoT architecture along with the possible AI and non-AI-based solutions. Finally, we propose a methodology to develop reliable, resilient, and personalized healthcare services that address the identified weaknesses of existing approaches.","2169-3536","","10.1109/ACCESS.2021.3137364","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9658494","Healthcare 5.0;IoT;medicine 4.0;reliability;resilience;personalization;sustainability","Medical services;Monitoring;Resilience;Biomedical monitoring;Artificial intelligence;Software reliability","","53","","187","CCBY","22 Dec 2021","","","IEEE","IEEE Journals"
"Dynamic Service Placement in Multi-Access Edge Computing: A Systematic Literature Review","H. Tabatabaee Malazi; S. R. Chaudhry; A. Kazmi; A. Palade; C. Cabrera; G. White; S. Clarke","Department of Computer Science, Maynooth University, Maynooth, Co. Kildare, Ireland; Department of Computer Science, Munster Technological University, Cork, Ireland; School of Computer Science and Statistics, Trinity College Dublin, Dublin 2, Ireland; School of Computer Science and Statistics, Trinity College Dublin, Dublin 2, Ireland; Department of Computer Science and Technology, University of Cambridge, Cambridge, U.K.; School of Computer Science and Statistics, Trinity College Dublin, Dublin 2, Ireland; School of Computer Science and Statistics, Trinity College Dublin, Dublin 2, Ireland",IEEE Access,"30 Mar 2022","2022","10","","32639","32688","The advent of new cloud-based applications such as mixed reality, online gaming, autonomous driving, and healthcare has introduced infrastructure management challenges to the underlying service network. Multi-access edge computing (MEC) extends the cloud computing paradigm and leverages servers near end-users at the network edge to provide a cloud-like environment. The optimum placement of services on edge servers plays a crucial role in the performance of such service-based applications. Dynamic service placement problem addresses the adaptive configuration of application services at edge servers to facilitate end-users and those devices that need to offload computation tasks. While reported approaches in the literature shed light on this problem from a particular perspective, a panoramic study of this problem reveals the research gaps in the big picture. This paper introduces the dynamic service placement problem and outline its relations with other problems such as task scheduling, resource management, and caching at the edge. We also present a systematic literature review of existing dynamic service placement methods for MEC environments from networking, middleware, applications, and evaluation perspectives. In the first step, we review different MEC architectures and their enabling technologies from a networking point of view. We also introduce different cache deployment solutions in network architectures and discuss their design considerations. The second step investigates dynamic service placement methods from a middleware viewpoint. We review different service packaging technologies and discuss their trade-offs. We also survey the methods and identify eight research directions that researchers follow. Our study categorises the research objectives into six main classes, proposing a taxonomy of design objectives for the dynamic service placement problem. We also investigate the reported methods and devise a solutions taxonomy comprising six criteria. In the third step, we concentrate on the application layer and introduce the applications that can take advantage of dynamic service placement. The fourth step investigates evaluation environments used to validate the solutions, including simulators and testbeds. We introduce real-world datasets such as edge server locations, mobility traces, and service requests used to evaluate the methods. We compile a list of open issues and challenges categorised by various viewpoints in the last step.","2169-3536","","10.1109/ACCESS.2022.3160738","Science Foundation Ireland (SFI), Enable Project(grant numbers:16/SP/3804); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9738624","Mobile edge computing;decentralised cloud;MEC server;service caching;service offloading;computational offloading;service deployment;resource management;service orchestration","Cloud computing;Servers;Vehicle dynamics;Resource management;Wireless fidelity;Taxonomy;Task analysis","","51","","284","CCBY","21 Mar 2022","","","IEEE","IEEE Journals"
"A Two-Layer Water Demand Prediction System in Urban Areas Based on Micro-Services and LSTM Neural Networks","A. A. Nasser; M. Z. Rashad; S. E. Hussein","Faculty of Computers and Information Systems, Mansoura University, Mansoura, Egypt; Faculty of Computers and Information Systems, Mansoura University, Mansoura, Egypt; Department of Computer Engineering and Control Systems, Faculty of Engineering, Mansoura University, Mansoura, Egypt",IEEE Access,"18 Aug 2020","2020","8","","147647","147661","In recent years, scarce water resources became one of the main problems that endanger human species existence and the advancement of any nation. In this research, smart water meters were implemented, distributed, and installed in a regional area in Cairo while data were collected at uniform intervals then sent to the cloud instantly. The solution paradigm uses an Internet of Things (IoT) based on micro-services and containers. The design incorporates real-time streaming and infrastructure performance optimization to store data. A second layer to analyze the acquired data was used to model water consumption using Long Short-Term Memory (LSTM). The designed LSTM is validated and tested to be utilized in the forecast of future water demand. Moreover, two alternative machine learning methods, namely Support Vector Regression and Random Forest commonly utilized in time series forecasting applications, were used for a comparative analysis of which LSTM has proven to be superior. The proper integration of the system elements is the key to the proposed system success. Based on the success of the designed system, it can be applicable on a national scale. That can enable the optimal management of consumers’ demand and improve water infrastructure utilization. The proposed paradigm presents a testbed for various scenarios that can be used in water resources management.","2169-3536","","10.1109/ACCESS.2020.3015655","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9163328","Water smart meters;time series;resources management;IoT;LSTM;micro-services","Water resources;Computer architecture;Demand forecasting;Real-time systems;Artificial intelligence;Cloud computing;Support vector machines","","49","","68","CCBY","10 Aug 2020","","","IEEE","IEEE Journals"
"Performance Modeling of Serverless Computing Platforms","N. Mahmoudi; H. Khazaei","Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada; Department of Electrical Engineering and Computer Science, York University, Toronto, ON, Canada",IEEE Transactions on Cloud Computing,"5 Dec 2022","2022","10","4","2834","2847","Analytical performance models have been leveraged extensively to analyze and improve the performance and cost of various cloud computing services. However, in the case of serverless computing, which is projected to be the dominant form of cloud computing in the future, we have not seen analytical performance models to help with the analysis and optimization of such platforms. In this work, we propose an analytical performance model that captures the unique details of serverless computing platforms. The model can be leveraged to improve the quality of service and resource utilization and reduce the operational cost of serverless platforms. Also, the proposed performance model provides a framework that enables serverless platforms to become workload-aware and operate differently for different workloads to provide a better trade-off between the cost and performance depending on the user's preferences. The current serverless offerings require the user to have extensive knowledge of the internals of the platform to perform efficient deployments. Using the proposed analytical model, the provider can simplify the deployment process by calculating the performance metrics for users even before physical deployments. We validate the applicability and accuracy of the proposed model by extensive experimentation on AWS Lambda. We show that the proposed model can calculate essential performance metrics such as average response time, probability of cold start, and the average number of function instances in the steady-state. Also, we show how the performance model can be used to tune the serverless platform for each workload, which will result in better performance or lower cost without scarifying the other. The presented model assumes no non-realistic restrictions, so that it offers a high degree of fidelity while maintaining tractability at large scale.","2168-7161","","10.1109/TCC.2020.3033373","Shared Hierarchical Academic Research Computing Network; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9238484","Serverless computing;performance modeling;optimization;queuing theory;stochastic processes","Computational modeling;Analytical models;Cloud computing;Concurrent computing;Measurement;Servers;Quality of service","","49","","61","IEEE","23 Oct 2020","","","IEEE","IEEE Journals"
"Evaluating Docker for Lightweight Virtualization of Distributed and Time-Sensitive Applications in Industrial Automation","M. Sollfrank; F. Loch; S. Denteneer; B. Vogel-Heuser","Technical University of Munich, Garching, Germany; Technical University of Munich, Garching, Germany; Technical University of Munich, Garching, Germany; Technical University of Munich, Garching, Germany",IEEE Transactions on Industrial Informatics,"23 Feb 2021","2021","17","5","3566","3576","A trend, accompanying the change of automation systems and their architectures, is the virtualization of software components. Virtualization strengthens platform-independent development and the provision of secure and isolated applications. Virtualization introduces well-defined interfaces to strengthen modularity, which facilitates the scalability of applications. However, virtualization includes additional software components and layers and, thus, additional computing costs. This additional effort can conflict with the real-time requirements of automation processes. Current research lacks the investigation of the time behavior of container-based virtualizations concerning their use in real-time systems. An assessment concerning real-time applications is required to prepare it for use in industrial automation. This article examines the effects of virtualization on the time delays of a software component based on Docker containers by providing measurements on a hardware testbed in a realistic use case. The experiments indicate that Docker virtualization can meet soft real-time requirements and can be used in industrial automation.","1941-0050","","10.1109/TII.2020.3022843","VDI/VDE(grant numbers:IUK459/002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9187833","Embedded software;real-time systems;software architecture;software tools;virtual machines (VMs)","Real-time systems;Containers;Automation;Virtualization;Computer architecture;Delays;Software","","49","","32","IEEE","8 Sep 2020","","","IEEE","IEEE Journals"
"Internet of Things for Agricultural Applications: The State of the Art","T. Ojha; S. Misra; N. S. Raghuwanshi","Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Agricultural and Food Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India",IEEE Internet of Things Journal,"6 Jul 2021","2021","8","14","10973","10997","The advent of the Internet of Things (IoT) inspired various new and enhanced sets of applications in multiple domains including agriculture. The recent drive in the adoption of IoT technologies offers a major enhancement for the agricultural sectors in terms of efficiency and scalability. In this article, we investigate the specific issues and challenges associated with IoT, and review various IoT architectures, communication, middleware, and information processing technologies. We, then, discuss few IoT applications for agriculture-presenting various case studies to thoroughly analyze the solutions along with their design and implementation related parameters. Consequently, we provide a comprehensive review of the available simulation tools, data sets, and testbeds which provisions experimentation with IoT in agriculture. We enumerate open issues and challenges present in enabling IoT for agriculture. Finally, this article concludes while giving directions for future research.","2327-4662","","10.1109/JIOT.2021.3051418","Information Technology Research Academy (ITRA), Government of India(grant numbers:ITRA/15(69)/WATER/M2M/01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9321474","Agriculture;automation;Internet of Things (IoT);precision agriculture","Internet of Things;Agriculture;Cloud computing;Sensors;Production;Irrigation;Diseases","","48","","196","IEEE","13 Jan 2021","","","IEEE","IEEE Journals"
"What the Fog? Edge Computing Revisited: Promises, Applications and Future Challenges","J. Gedeon; F. Brandherm; R. Egert; T. Grube; M. Mühlhäuser","Department of Computer Science, Technische Universität Darmstadt, Darmstadt, Germany; Department of Computer Science, Technische Universität Darmstadt, Darmstadt, Germany; Department of Computer Science, Technische Universität Darmstadt, Darmstadt, Germany; Department of Computer Science, Technische Universität Darmstadt, Darmstadt, Germany; Department of Computer Science, Technische Universität Darmstadt, Darmstadt, Germany",IEEE Access,"28 Oct 2019","2019","7","","152847","152878","Edge computing brings computing and storage resources closer to (mobile) end users and data sources, thus bypassing expensive and slow links to distant cloud computing infrastructures. Often leveraged opportunistically, these heterogeneous resources can be used to offload data and computations, enabling upcoming demanding applications such as augmented reality and autonomous driving. Research in this direction has addressed various challenges, from architectural concerns to runtime optimizations. As of today, however, we lack a widespread availability of edge computing-partly because it remains unclear which of the promised benefits of edge computing are relevant for what types of applications. This article provides a comprehensive snapshot of the current edge computing landscape, with a focus on the application perspective. We outline the characteristics of edge computing and its postulated benefits and drawbacks. To understand the functional composition of applications, we first define common application components that are relevant w.r.t. edge computing. We then present a classification of proposed use cases and analyze them according to their expected benefits from edge computing and which components they use. Furthermore, we illustrate existing products and industry solutions that have recently surfaced and outline future research challenges.","2169-3536","","10.1109/ACCESS.2019.2948399","National Natural Science Foundation of China(grant numbers:392046569 (DFG),61761136014 (NSFC)); Collaborative Research Center 1053–MAKI (DFG) and as part of the project D.4 within the RTG 2050 “Privacy and Trust for Mobile Users” (DFG); Technische Universität Darmstadt; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8877785","Edge computing;heterogeneous networks;next generation networking mobile applications;Internet of Things;ubiquitous computing","Edge computing;Cloud computing;Mobile handsets;Task analysis;Taxonomy;Industries;Hardware","","48","","293","CCBY","21 Oct 2019","","","IEEE","IEEE Journals"
"Container Placement and Migration in Edge Computing: Concept and Scheduling Models","O. Oleghe","Systems Engineering Department, University of Lagos, Lagos, Nigeria",IEEE Access,"11 May 2021","2021","9","","68028","68043","Containers are a form of software virtualization, rapidly becoming the de facto way of providing edge computing services. Research on container-based edge computing is plentiful, and this has been buoyed by the increasing demand for single digit, milliseconds latency computations. A container scheduler is part of the architecture that is used to manage and orchestrate multiple container-based applications on heterogenous computing nodes. The scheduler decides how incoming computing requests are allocated to containers, which edge nodes the containers are placed on, and where already deployed containers are migrated to. This paper aims to clarify the concept of container placement and migration in edge servers and the scheduling models that have been developed for this purpose. The study illuminates the frameworks and algorithms upon which the scheduling models are built. To convert the problem to one that can be solved using an algorithm, the container placement problem in mostly abstracted using multi-objective optimization models or graph network models. The scheduling algorithms are predominantly heuristic-based algorithms, which are able to arrive at sub-optimal solutions very quickly. There is paucity of container scheduling models that consider distributed edge computing tasks. Research in decentralized scheduling systems is gaining momentum and the future outlook is in scheduling containers for mobile edge nodes.","2169-3536","","10.1109/ACCESS.2021.3077550","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9422781","Algorithm;container;edge computing;migration;placement;scheduling","Containers;Edge computing;Processor scheduling;Servers;Computational modeling;Cloud computing;Task analysis","","46","","105","CCBY","4 May 2021","","","IEEE","IEEE Journals"
"Digital Twins: A Maturity Model for Their Classification and Evaluation","J. -F. Uhlenkamp; J. B. Hauge; E. Broda; M. Lütjen; M. Freitag; K. -D. Thoben","BIBA—Bremer Institut für Produktion und Logistik GmbH, University of Bremen, Bremen, Germany; BIBA—Bremer Institut für Produktion und Logistik GmbH, University of Bremen, Bremen, Germany; Faculty of Production Engineering, University of Bremen, Bremen, Germany; BIBA—Bremer Institut für Produktion und Logistik GmbH, University of Bremen, Bremen, Germany; BIBA—Bremer Institut für Produktion und Logistik GmbH, University of Bremen, Bremen, Germany; BIBA—Bremer Institut für Produktion und Logistik GmbH, University of Bremen, Bremen, Germany",IEEE Access,"7 Jul 2022","2022","10","","69605","69635","Digital Twins represent a powerful tool for transforming production and logistics towards Industry 4.0. They mirror physical assets in the digital world, enriching them with additional capabilities and features such as decision-making or lifecycle management. Due to the diverse possibilities associated with the Digital Twin, their design and implementation are also wide-ranging. This paper aims to contribute to the formalization and standardization of the description of Digital Twins. It presents a method for evaluating them through their lifecycle, from design to operation. The paper is based on an overview of their potential functionalities and properties with ranked stages of development. This method allows for an application-specific evaluation of Digital Twins and describes how they can be improved to suit the application better. The maturity model development follows the procedure for developing maturity models for IT management. Relevant capabilities and features were identified with a systematic literature review following the PRISMA guidelines. The results of this review were ranked and categorized and constitute the core of the maturity model, which was validated on five use-cases from different domains in production and logistics. The maturity model assesses Digita Twins in seven categories (context, data, computing capabilities, model, integration, control, human-machine interface) with 31 ranked characteristics. It evaluates existing solutions for potential improvements for a given application or the transfer to a new use-case. The resulting method and a supplementary web service present a generalized model for the evaluation of Digital Twins. Based on a description of a potential application, this is the first step towards a systematic evaluation, improving the structured development of such applications","2169-3536","","10.1109/ACCESS.2022.3186353","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9807313","Digital twin;maturity model;characteristics;dimensions;literature review;use cases;product development;systematization;digital twin application","Digital twins;Analytical models;Bibliographies;Computational modeling;Logistics;Fourth Industrial Revolution;Adaptation models","","46","","98","CCBYNCND","27 Jun 2022","","","IEEE","IEEE Journals"
"IoT-to-the-Rescue: A Survey of IoT Solutions for COVID-19-Like Pandemics","N. Pathak; P. K. Deb; A. Mukherjee; S. Misra","Advanced Technology Development Center, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India",IEEE Internet of Things Journal,"23 Aug 2021","2021","8","17","13145","13164","The atmospheric buoyancy and intangible nature of fatal communicable viruses lead to rapid transmissions among individuals, resulting in global pandemics. Strategic lockdowns and mandatory social distancing are immediate solutions in such scenarios. However, this leads to operational disruptions in education, manufacturing, economy, transportation, governance, and community. Although technological assistance is beneficial in overcoming such issues, the current Internet of Things (IoT) infrastructure has limitations. In this article, we provide a comprehensive review of the possible IoT-based solutions that have the capacity of combating the COVID-19-like viruses. We highlight the societal impacts due to pandemics and identify the specific lacunae in current IoT solutions. We also provide comprehensive detail on how to overcome the challenges along with directions toward the possible technological trends for future research. Compared to existing reviews, our work offers a holistic view of the cause, effects, and the possible solutions that are existing, along with already existing solutions that can be customized to serve the special needs during the pandemic.","2327-4662","","10.1109/JIOT.2021.3082838","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9438712","COVID-19;education;governance;healthcare;industry;Internet of Things (IoT);IoT architecture;pandemic;SARS-COV-2;transportation","COVID-19;Pandemics;Coronaviruses;Internet of Things;Medical services;Diseases;Government","","45","","142","IEEE","21 May 2021","","","IEEE","IEEE Journals"
"Architecting Digital Twins","E. Ferko; A. Bucaioni; M. Behnam","Division of Product Realisation, Mälardalen University, Eskilstuna, Sweden; Division of Product Realisation, Mälardalen University, Eskilstuna, Sweden; Division of Product Realisation, Mälardalen University, Eskilstuna, Sweden",IEEE Access,"16 May 2022","2022","10","","50335","50350","In 2002, Grieves defined the concept of the digital twin as a virtual instance of physical assets capable of continuously mirroring them. Ever since then, driven by remarkable industrial attention, digital twins flourished and ripened in several sectors. The notable industrial adoption has been sided by a growing interest from the software engineering community in general and the software architecture community in particular as demonstrated by the growing number of published peer-reviewed publications and proposed software architectural solutions for digital twins. In this paper, we report on the planning, execution, and results of a systematic mapping study on architecting digital twins. The study captures crucial aspects of software architectures for digital twins as types of architectural solutions, quality attributes, and architectural patterns. It supports practitioners in creating digital twins tailored to their specific needs and researchers in identifying trends and open challenges. Starting from an initial set of potentially relevant 1630 peer-reviewed publications, we selected 140 primary studies. We analysed the set of primary studies using thorough data extraction, analysis, and synthesis process. To compensate for single method limitations and reduce possible threats to conclusion validity, we discussed the results of our study with experts in the software architecture community. Based on our results, the field of software architecture for digital twins is lively and an increasing number of architectural solutions are being proposed. Although there is a lack of widely accepted reference architectural solutions for digital twins, most of them are built using a combination of the layered and service-oriented patterns and address maintainability, performance efficiency, and compatibility quality attributes.","2169-3536","","10.1109/ACCESS.2022.3172964","Swedish Knowledge Foundation [Stiftelsen för kunskaps- och kompetensutveckling (KKS)] through the Assured Cloud Platforms for Industrial Cyber-physical Systems (ACICS) and Model-Based DevOps for Cyber-Physical System Product Lines (Modev) projects; Sweden’s Innovation Agency [Verket för innovationssystem (VINNOVA)] through the PANORAMA Project; Excellence in Production Research [Excellence in Production Research (XPRES)] Framework; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770073","Architectural patterns;digital twin;software architectures;quality attributes.","Digital twin;Systematics;Software architecture;Computer architecture;Standards;Market research;Planning","","45","","40","CCBY","5 May 2022","","","IEEE","IEEE Journals"
"A Theoretical Discussion and Survey of Network Automation for IoT: Challenges and Opportunity","S. T. Arzo; C. Naiga; F. Granelli; R. Bassoli; M. Devetsikiotis; F. H. P. Fitzek","Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Deutsche Telekom Chair of Communication Networks, Institute of Communication Technology, Faculty of Electrical and Computer Engineering, and the Centre for Tactile Internet with Human-in-the-Loop, Cluster of Excellence, Technische Universität Dresden, Dresden, Germany; Department of Electrical and Computer Engineering, University of New Mexico, Albuquerque, NM, USA; Deutsche Telekom Chair of Communication Networks, Institute of Communication Technology, Faculty of Electrical and Computer Engineering, and the Centre for Tactile Internet with Human-in-the-Loop, Cluster of Excellence, Technische Universität Dresden, Dresden, Germany",IEEE Internet of Things Journal,"26 Jul 2021","2021","8","15","12021","12045","The introduction of the Internet of Things (IoT) and massive machine-type communications has implied an increase in network size and complexity. In particular, there is already a huge number of IoT devices in the market in various sectors, such as smart agriculture, smart city, smart home, smart transportation, etc. The IoT interconnectivity technologies are also increasing. Therefore, these are increasingly overwhelming the efforts of network administrators as they try to design, reconfigure and manage such networks. Relying on humans to manage such complex and dynamic networks is becoming unsustainable. Network automation promises to reduce the cost of administration and maintenance of network infrastructure, by offering networks the capability to manage themselves. Network automation is the ability of the network to manage itself. Various standardization organizations are taking the initiative in introducing network automation, such as European Telecommunication Standardization Institute (ETSI). ETSI is leading the standardization activities for network automation. It has provided different versions of reference architecture called generic autonomic network architecture (GANA), which describes a four-level abstraction for network-management decision elements (DEs), protocol level, function level, node level, and network level. In this article, we review and survey the existing works before and after the introduction of software-defined networking (SDN) and network-function-virtualization (NFV). We relate the main trending paradigms being followed, such as SDN, NFV, machine learning (ML), microservices, multiagent system (MAS), containerization, and cloudification, as a pivotal enabler of full network automation. We also discuss the autonomic architectures proposed in the literature. Finally, we presented possible future research directions and challenges that need to be tackled to progress in achieving full network automation.","2327-4662","","10.1109/JIOT.2021.3075901","NATO Science for Peace and Security (SPS) Programme in the Framework of the Project “Dynamic Architecture based on UAVs Monitoring for Border Security and Safety”(grant numbers:SPS G5428); German Research Foundation (DFG, Deutsche Forschungsgemeinschaft) as part of Germany’s Excellence Strategy—EXC 2050/1—Project ID 390696704—Cluster of Excellence “Centre for Tactile Internet with Human-in-the-Loop” (CeTI) of Technische Universität Dresden; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9416288","Autonomic networking;machine learning;multiagent system (MAS);network function virtualization;network management system (NMS);network softwarization;software-defined networking (SDN)","Automation;Internet of Things;Computer architecture;Market research;Software;Monitoring;Cloud computing","","44","","156","IEEE","26 Apr 2021","","","IEEE","IEEE Journals"
"Differentiated Service/Data Migration for Edge Services Leveraging Container Characteristics","P. Bellavista; A. Corradi; L. Foschini; D. Scotece","Dipartimento di Informatica: Scienza e Ingegneria (DISI), University of Bologna, Bologna, Italy; Dipartimento di Informatica: Scienza e Ingegneria (DISI), University of Bologna, Bologna, Italy; Dipartimento di Informatica: Scienza e Ingegneria (DISI), University of Bologna, Bologna, Italy; Dipartimento di Informatica: Scienza e Ingegneria (DISI), University of Bologna, Bologna, Italy",IEEE Access,"3 Oct 2019","2019","7","","139746","139758","The Multi-access Edge Computing (MEC) and Fog Computing paradigms are enabling the opportunity to have middleboxes either statically or dynamically deployed at network edges acting as local proxies with virtualized resources for supporting and enhancing service provisioning in edge localities. However, migration of edge-enabled services poses significant challenges in the edge computing environment. In this paper, we propose an edge computing platform architecture that supports service migration with different options of granularity (either entire service/data migration, or proactive application-aware data migration) across heterogeneous edge devices (either MEC-based servers or resource-poor Fog devices) that host virtualized resources (Docker Containers). The most innovative elements of the technical contribution of our work include i) the possibility to select either an application-agnostic or an application-aware approach, ii) the possibility to choose the appropriate application-aware approach (e.g., based on data access frequencies), iii) an automatic edge services placement support with the aim of finding a more effective placement with low energy consumption, and iv) the in-lab experimentation of the performance achieved over rapidly deployable environments with resource-limited edges such as Raspberry Pi devices.","2169-3536","","10.1109/ACCESS.2019.2943848","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8850058","Container migration;Docker containers;edge computing;service migration","6G mobile communication;Containers;Indexes","","43","","34","CCBY","26 Sep 2019","","","IEEE","IEEE Journals"
"AI-Empowered Fog/Edge Resource Management for IoT Applications: A Comprehensive Review, Research Challenges, and Future Perspectives","G. K. Walia; M. Kumar; S. S. Gill","Department of Information Technology, Dr. B.R. Ambedkar National Institute of Technology, Jalandhar, India; Department of Information Technology, Dr. B.R. Ambedkar National Institute of Technology, Jalandhar, India; School of Electronic Engineering and Computer Science, Queen Mary University of London, London, U.K.",IEEE Communications Surveys & Tutorials,"26 Feb 2024","2024","26","1","619","669","The proliferation of ubiquitous Internet of Things (IoT) sensors and smart devices in several domains embracing healthcare, Industry 4.0, transportation and agriculture are giving rise to a prodigious amount of data requiring ever-increasing computations and services from cloud to the edge of the network. Fog/Edge computing is a promising and distributed computing paradigm that has drawn extensive attention from both industry and academia. The infrastructural efficiency of these computing paradigms necessitates adaptive resource management mechanisms for offloading decisions and efficient scheduling. Resource Management (RM) is a non-trivial issue whose complexity is the result of heterogeneous resources, incoming transactional workload, edge node discovery, and Quality of Service (QoS) parameters at the same time, which makes the efficacy of resources even more challenging. Hence, the researchers have adopted Artificial Intelligence (AI)-based techniques to resolve the above-mentioned issues. This paper offers a comprehensive review of resource management issues and challenges in Fog/Edge paradigm by categorizing them into provisioning of computing resources, task offloading, resource scheduling, service placement, and load balancing. In addition, existing AI and non-AI based state-of-the-art solutions have been discussed, along with their QoS metrics, datasets analysed, limitations and challenges. The survey provides mathematical formulation corresponding to each categorized resource management issue. Our work sheds light on promising research directions on cutting-edge technologies such as Serverless computing, 5G, Industrial IoT (IIoT), blockchain, digital twins, quantum computing, and Software-Defined Networking (SDN), which can be integrated with the existing frameworks of fog/edge-of-things paradigms to improve business intelligence and analytics amongst IoT-based applications.","1553-877X","","10.1109/COMST.2023.3338015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10335918","Edge computing;resource management;fog computing;artificial intelligence;machine learning;cloud computing;IoT","Internet of Things;Cloud computing;Sensors;Task analysis;Resource management;Surveys;Intelligent sensors","","43","","205","IEEE","30 Nov 2023","","","IEEE","IEEE Journals"
"Hierarchical Blockchain Design for Distributed Control and Energy Trading Within Microgrids","J. Yang; J. Dai; H. B. Gooi; H. D. Nguyen; P. Wang","School of Electrical and Electronics Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronics Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of EEE, Nanyang Technological University, Singapore; Department of Electrical Engineering and Computer Science, York University, Toronto, ON, Canada",IEEE Transactions on Smart Grid,"21 Jun 2022","2022","13","4","3133","3144","Blockchain technology is recognized as a suitable tool to secure the energy trading because it could perfectly match the distributed structure of peer-to-peer (P2P) energy market. But its usage is stuck on the transaction level. Control systems are significant to the microgrid as they ensure a stable power delivery system and regulate the performance of parameters such as active power and frequency. This paper proves that the blockchain technology is also effective in securing the distributed control systems against the false data injection attack. A six-prosumer microgrid is tested with the implementation of the hierarchical blockchain system. The security of both the control system and energy trading system of the microgrid is ensured. Smart contracts are created to calculate the feedback measurements for the control system and execute the energy transactions. According to the hierarchical structure, the private blockchain with static nodes is implemented for the distributed control to match the sampling rate. A Proof-of-Authority based blockchain is utilized to support the energy trading. In addition, a double auction based simple iteration (DA-SI) pricing scheme is designed to improve the social welfare of the microgrid. Finally, case studies are presented to verify the proposed hierarchical blockchain system as an effective method to safeguard the control system and maximize the benefits of prosumers. Numerical results show the effectiveness and feasibility of the proposed approach.","1949-3061","","10.1109/TSG.2022.3153693","National Research Foundation, Singapore, and EMA-ESG, Singapore(grant numbers:NRF2019NRF-CG002-002); Agency for Science, Technology and Research (A*SATR), Singapore, under its Singapore-Germany Academic-Industry (2 + 2) International Collaboration(grant numbers:A1990b0060); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9718520","Distributed control;peer-to-peer energy trading;cyber-security;hierarchical blockchain;smart grid","Blockchains;Microgrids;Peer-to-peer computing;Decentralized control;Pricing;Smart contracts;Game theory","","42","","44","IEEE","23 Feb 2022","","","IEEE","IEEE Journals"
"IoT Application Modules Placement and Dynamic Task Processing in Edge-Cloud Computing","J. Fang; A. Ma","Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China",IEEE Internet of Things Journal,"5 Aug 2021","2021","8","16","12771","12781","In today's era of Internet of Things (IoT), efficient and real-time processing of massive data generated by IoT device has become the primary issue for traditional cloud computing network architectures. As a supplement of cloud computing, edge computing enhances the real-time performance of service completion by offloading services to edge servers closer to the terminal device for execution, while reducing power consumption and computing load in the cloud. In this article, we propose the following solutions to resolve the different requests of the IoT device: in an “edge-cloud” heterogeneous network environment, create a mapping scheme between application modules and basic resource equipment, considering the two factors of tolerant task latency and system power consumption. In the application step-by-step execution process, heuristic dynamic task processing algorithm is used to reduce the task latency time. Experiments with the “iFogSim” simulator show that, application service quality is significantly improved and system power consumption is greatly reduced, which compared with the stable application module placement strategy and the static task scheduling strategy.","2327-4662","","10.1109/JIOT.2020.3007751","Beijing Natural Science Foundation(grant numbers:4192007); National Natural Science Foundation of China(grant numbers:61202076); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9134385","Edge computing;Internet-of-Things (IoT) service placement;resource allocation;task scheduling","Task analysis;Cloud computing;Internet of Things;Edge computing;Processor scheduling;Network architecture;Servers","","42","","33","IEEE","7 Jul 2020","","","IEEE","IEEE Journals"
"Serverless Management of Sensing Systems for Fog Computing Framework","S. Sarkar; R. Wankar; S. N. Srirama; N. K. Suryadevara","School of Computer and Information Sciences, University of Hyderabad, Telangana, India; School of Computer and Information Sciences, University of Hyderabad, Telangana, India; Institute of Computer Science, University of Tartu, Tartu, Estonia; School of Computer and Information Sciences, University of Hyderabad, Telangana, India",IEEE Sensors Journal,"17 Jan 2020","2020","20","3","1564","1572","In this paper, we propose a framework for the management of the Internet of Things (IoT) devices in a smart building to model services based on the serverless computing paradigm. The deployment of an IoT compatible serverless paradigm consists of a hierarchical structural design across the edge, fog, and cloud computing layers. The fog/edge nodes collect the data generated from various sensors, process the data in the intermediate nodes, and then forward certain data to a cloud for future analysis. The framework consists of a heterogeneous IoT network. We proposed a data distribution algorithm in the framework to make sure management, maintenance and availability of heterogeneous IoT network in the serverless computing paradigm are effective and efficient. The experiments conducted are validated at the developed fog and edge gateways using API mechanism. The response times for an application doing the computation at fog level and at the cloud level are compared. The experimentation shows that latency is less for the fog model as compared to the data sent to the cloud model.","1558-1748","","10.1109/JSEN.2019.2939182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8822951","Wireless sensor network;smart building;fog computing;serverless computing;sensing systems","Cloud computing;Computational modeling;Internet of Things;Edge computing;Data models;Intelligent sensors","","41","","52","IEEE","3 Sep 2019","","","IEEE","IEEE Journals"
"Advancements in Industrial Cyber-Physical Systems: An Overview and Perspectives","K. Zhang; Y. Shi; S. Karnouskos; T. Sauter; H. Fang; A. W. Colombo","Department of Mechanical Engineering, University of Victoria, Victoria, BC, Canada; Department of Mechanical Engineering, University of Victoria, Victoria, BC, Canada; SAP, Walldorf, Germany; Institute of Computer Technology, Vienna University of Technology, Vienna, Austria; Department of Mechanical Engineering, University of Kansas, Lawrence, KS, USA; Institute for Industrial Informatics, Automation and Robotics, University of Applied Sciences Emden/Leer, Emden, Germany",IEEE Transactions on Industrial Informatics,"9 Nov 2022","2023","19","1","716","729","Cyber-physical systems (CPSs) have attracted increasing attention in recent years due to their promise for substantial and long-term benefits to society, economy, environment, and citizens. In addition, the rapid advances in computing, communication, and storage technologies have resulted in a revolution in the information communication technology domain and domination in the industry context. The utilization of CPSs in industrial settings has led to industrial cyber-physical systems (ICPSs), which, in conjunction with the information-driven interactions, enables large-scale cooperation in industrial facilities and among all the stakeholders of the value chain. Hence, the research on ICPSs is essential, especially with respect to the engineering of such systems for industrial applications. This article presents an overview of recent developments in ICPSs. We first introduce the architecture of ICPSs. Then, we review the developments of ICPSs in relevant research domains. Finally, this article concludes by presenting some potential future research directions on ICPSs.","1941-0050","","10.1109/TII.2022.3199481","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9860057","Communication and networking;control;cyber security;industrial cyber-physical system (ICPS);information acquisition","Production;Industries;Internet;Informatics;Fourth Industrial Revolution;Process control;Cloud computing","","41","","100","IEEE","17 Aug 2022","","","IEEE","IEEE Journals"
"5G RAN: Functional Split Orchestration Optimization","S. Matoussi; I. Fajjari; S. Costanzo; N. Aitsaadi; R. Langar","LIGM CNRS-UMR 8049, University Gustave Eiffel (UGE), Marne-la-Vallée, France; Orange-Labs, Châtillon, France; Orange-Labs, Châtillon, France; LI-PaRAD EA 7432, University UVSQ Paris-Saclay, Guyancourt, France; LIGM CNRS-UMR 8049, University Gustave Eiffel (UGE), Marne-la-Vallée, France",IEEE Journal on Selected Areas in Communications,"26 Jun 2020","2020","38","7","1448","1463","5G RAN aims to evolve new technologies spanning the Cloud infrastructure, virtualization techniques and Software Defined Network capabilities. Advanced solutions are introduced to split the functions of the Radio Access Network (RAN) between centralized and distributed locations. Such paradigms improve RAN flexibility and reduce the infrastructure deployment cost without impacting the user quality of service. We propose a novel functional split orchestration scheme that aims at minimizing the RAN deployment cost, while considering the requirements of its processing network functions and the capabilities of the Cloud infrastructure. With a fine grained approach on user basis, we show that the proposed solution optimizes both processing and bandwidth resource usage, while minimizing the overall energy consumption compared to i) cell-centric, ii) distributed and iii) centralized Cloud-RAN approaches. Moreover, we evaluate the effectiveness of our proposal in a 5G experimental prototype, based on Open Air Interface (OAI). We show that our solution achieves good performance in terms of total deployment cost and resolution time.","1558-0008","","10.1109/JSAC.2020.2999685","FUI SCORPION Project(grant numbers:17/00464); CNRS PRESS Project(grant numbers:239953); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9107209","C-RAN;NFV;functional split;heuristic;PSO;optimization;OAI","Computer architecture;Computational modeling;Cloud computing;Bandwidth;5G mobile communication;Resource management;Optimization","","41","","36","IEEE","3 Jun 2020","","","IEEE","IEEE Journals"
"Reinforcement Learning-Empowered Mobile Edge Computing for 6G Edge Intelligence","P. Wei; K. Guo; Y. Li; J. Wang; W. Feng; S. Jin; N. Ge; Y. -C. Liang","Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Shanghai Key Laboratory of Multidimensional Information Processing, School of Communications and Electronics Engineering, East China Normal University, Shanghai, China; School of Information Science and Technology, Nantong University, Nantong, China; School of Information Science and Technology, Nantong University, Nantong, China; Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; National Key Laboratory on Communications, Center for Intelligent Networking and Communications, University of Electronic Science and Technology of China, Chengdu, China",IEEE Access,"24 Jun 2022","2022","10","","65156","65192","Mobile edge computing (MEC) is considered a novel paradigm for computation-intensive and delay-sensitive tasks in fifth generation (5G) networks and beyond. However, its uncertainty, referred to as dynamic and randomness, from the mobile device, wireless channel, and edge network sides, results in high-dimensional, nonconvex, nonlinear, and NP-hard optimization problems. Thanks to the evolved reinforcement learning (RL), upon iteratively interacting with the dynamic and random environment, its trained agent can intelligently obtain the optimal policy in MEC. Furthermore, its evolved versions, such as deep reinforcement learning (DRL), can achieve higher convergence speed efficiency and learning accuracy based on the parametric approximation for the large-scale state-action space. This paper provides a comprehensive research review on RL-enabled MEC and offers insight for development in this area. More importantly, associated with free mobility, dynamic channels, and distributed services, the MEC challenges that can be solved by different kinds of RL algorithms are identified, followed by how they can be solved by RL solutions in diverse mobile applications. Finally, the open challenges are discussed to provide helpful guidance for future research in RL training and learning MEC.","2169-3536","","10.1109/ACCESS.2022.3183647","National Key Research and Development Program of China(grant numbers:2020YFA0711301); National Natural Science Foundation of China(grant numbers:61941104,61901298,61922049,61801248,61771264); Natural Science Foundation of Tianjin(grant numbers:20JCQNJC00300); Science and Technology Development Fund of Tianjin Education Commission for Higher Education(grant numbers:2018KJ211); Shanghai Municipal Science and Technology Major Project(grant numbers:2018SHZDZX04); Tsinghua University–China Mobile Communications Group Company Ltd. Joint Institute; Natural Science Foundation of Jiangsu Province(grant numbers:BK20180943); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9797722","Mobile edge computing (MEC);network uncertainty;reinforcement learning (RL)","Task analysis;Optimization;Uncertainty;Servers;Ions;Resource management;Multi-access edge computing","","40","","185","CCBY","16 Jun 2022","","","IEEE","IEEE Journals"
"Automated Fine-Grained CPU Cap Control in Serverless Computing Platform","Y. K. Kim; M. R. HoseinyFarahabady; Y. C. Lee; A. Y. Zomaya","University of Sydney, Camperdown, Australia; University of Sydney, Camperdown, Australia; Macquarie University, Macquarie Park, Australia; University of Sydney, Camperdown, Australia",IEEE Transactions on Parallel and Distributed Systems,"8 May 2020","2020","31","10","2289","2301","Serverless computing has emerged as a new cloud computing execution model that liberates users and application developers from explicitly managing `physical' resources, leaving such a resource management burden to service providers. In this article, we study the problem of resource allocation for multi-tenant serverless computing platforms explicitly taking into account workload fluctuations including sudden surges. In particular, we investigate different root causes of performance degradation in these platforms where tenants (their applications) have different workload characteristics. To this end, we develop a fine-grained CPU cap control solution as a resource manager that dynamically adjusts CPU usage limit (or CPU cap) concerning applications with same/similar performance requirements, i.e., application groups. The adjustment of CPU caps applies primarily to co-located worker processes of serverless computing platforms to minimize resource contention, which is the major source of performance degradation. The actual adjustment decisions are made based on performance metrics (e.g., throttled time and queue length) using a group-aware scheduling algorithm. The extensive experimental results performed in our local cluster confirm that the proposed resource manager can effectively eliminate the burden of explicit reservation of computing capacity, even when fluctuations and sudden surges in the incoming workload exist. We measure the robustness of the proposed resource manager by comparing it with several heuristics which extensively used in practice, including the enhanced version of round robin and the least length queue scheduling policies, under various workload intensities driven by real-world scenarios. Notably, our resource manager outperforms other heuristics by decreasing skewness and average response time up to 44 and 94 percent, respectively, while it does not over-use the CPU resources.","1558-2183","","10.1109/TPDS.2020.2989771","Australian Research Council(grant numbers:DP190103710); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9076860","Serverless computing;virtualized cloud platforms;operating system process management;dynamic CPU scheduling;performance modeling","Interference;Round robin;Time factors;Resource management;Measurement;Computer architecture","","40","","29","IEEE","23 Apr 2020","","","IEEE","IEEE Journals"
"Disrupting Healthcare Silos: Addressing Data Volume, Velocity and Variety With a Cloud-Native Healthcare Data Ingestion Service","R. Ranchal; P. Bastide; X. Wang; A. Gkoulalas-Divanis; M. Mehra; S. Bakthavachalam; H. Lei; A. Mohindra","IBM Watson Health, Cambridge, MA, USA; IBM Watson Health, Cambridge, MA, USA; IBM Watson Health, Cambridge, MA, USA; IBM Watson Health, Cambridge, MA, USA; IBM Watson Health, Cambridge, MA, USA; IBM Watson Health, Cambridge, MA, USA; IBM Watson Health, Cambridge, MA, USA; IBM Watson Health, Cambridge, MA, USA",IEEE Journal of Biomedical and Health Informatics,"4 Nov 2020","2020","24","11","3182","3188","Healthcare enterprises are starting to adopt cloud computing due to its numerous advantages over traditional infrastructures. This has become a necessity because of the increased volume, velocity and variety of healthcare data, and the need to facilitate data correlation and large-scale analysis. Cloud computing infrastructures have the power to offer continuous acquisition of data from multiple heterogeneous sources, efficient data integration, and big data analysis. At the same time, security, availability, and disaster recovery are critical factors aiding towards the adoption of cloud computing. However, the migration of healthcare workloads to cloud is not straightforward due to the vagueness in healthcare data standards, heterogeneity and sensitive nature of healthcare data, and many regulations that govern its usage. This paper highlights the need for providing healthcare data acquisition using cloud infrastructures and presents the challenges, requirements, use-cases, and best practices for building a state-of-the-art healthcare data ingestion service on cloud.","2168-2208","","10.1109/JBHI.2020.3001518","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9113446","Cloud computing;electronic health record;healthcare information system;healthcare data ingestion","Medical services;Cloud computing;Biomedical imaging;Informatics;Standards organizations;Interoperability","Big Data;Cloud Computing;Computer Security;Delivery of Health Care;Eating;Humans","40","","27","IEEE","10 Jun 2020","","","IEEE","IEEE Journals"
"Reliable and Secure Vehicular Fog Service Provision","Y. Yao; X. Chang; J. Mišić; V. Mišić","Beijing Key Laboratory of Security and Privacy in Intelligent Transportation, Beijing Jiaotong University, Beijing, China; Beijing Key Laboratory of Security and Privacy in Intelligent Transportation, Beijing Jiaotong University, Beijing, China; Department of Computer Science, Ryerson University, Toronto, Canada; Department of Computer Science, Ryerson University, Toronto, Canada",IEEE Internet of Things Journal,"24 Feb 2019","2019","6","1","734","743","Vehicular fog computing (VFC) complements vehicular cloud computing as a promising solution for accommodating the surge of mobile traffic and reducing latency. This paper considers vehicular fog service (VFS) provided by a vehicular fog (VF), which is formed on-the-fly by integrating computing and storage resources of parked vehicles. VF dynamicity, due to vehicles' random arrivals and departures, poses a number of challenges for reliable and secure VFS provision to client vehicles. We propose a novel mechanism which consists of a VF construction method and a VFS access method to ensure VFS reliability and security without sacrificing performance. The reliability and security of VFS under our mechanism are discussed in detail. Moreover, we investigate the impact of the proposed mechanism on VF throughput and show that the mechanism is lightweight enough to be used in the latency-sensitive VFC.","2327-4662","","10.1109/JIOT.2018.2855718","National Natural Science Foundation of China(grant numbers:61572066); Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8410666","Quality of service;reliability;security;vehicular fog service (VFS)","Reliability;Security;Cloud computing;Quality of service;Internet of Things;Edge computing;Delays","","39","","39","IEEE","13 Jul 2018","","","IEEE","IEEE Journals"
"Serverless Blockchain-Enabled Architecture for IoT Societal Applications","S. Benedict","Indian Institute of Information Technology Kottayam, Valavoor, India",IEEE Transactions on Computational Social Systems,"10 Nov 2020","2020","7","5","1146","1158","IoT-enabled applications, such as cloud manufacturing, guided water quality or air quality analysis, energy-conscious societal applications, and smart agricultural economics, are designed using a blend of high-end computing technologies, such as cloud, edge, and fog. Smart cities and governmental authorities keep a keen eye out for implementing IoT applications in an automated/decentralized approach with enhanced security measures so that tens of thousands of users, including entrepreneurs, are benefited. Existing IoT architectures are prone to energy inefficiency or resource underutilization problems due to the avoidance of apt technologies, such as serverless computing. This article proposes to set forth a serverless blockchain-enabled IoT architecture for societal applications. It explores the existing IoT architectures and pinpoints the advantages of applying serverless blockchains on IoT architectures. In addition, the proposed IoT architecture is illustrated with a specific use case of IoT societal applications namely air quality monitoring for smart cities (AQMS). This article discloses how air quality sensor data from defective industries were securely transacted to blockchain networks surpassing from the three levels of computing namely edge, fog, and cloud while utilizing serverless and server-oriented functions. In addition, this article exposes a list of the most potent serverless functions that assist AQMS IoT societal applications in detail. The IoT architecture, discussed in this article, will enable innovations and research works for IoT developers and researchers.","2329-924X","","10.1109/TCSS.2020.3008995","AIC-IIITKottayam; BEL Projects; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9149700","Blockchain;cloud computing;decentralized services;IoT;societal applications","Cloud computing;Computer architecture;Air quality;Atmospheric measurements;Pollution measurement;Servers","","39","","39","IEEE","27 Jul 2020","","","IEEE","IEEE Journals"
"A Survey on Observability of Distributed Edge & Container-Based Microservices","M. Usman; S. Ferlin; A. Brunstrom; J. Taheri","Department of Computer Science, Karlstad University, Karlstad, Sweden; Department of Computer Science, Karlstad University, Karlstad, Sweden; Department of Computer Science, Karlstad University, Karlstad, Sweden; Department of Computer Science, Karlstad University, Karlstad, Sweden",IEEE Access,"24 Aug 2022","2022","10","","86904","86919","Edge computing is proposed as a technical enabler for meeting emerging network technologies (such as 5G and Industrial Internet of Things), stringent application requirements and key performance indicators (KPIs). It aims to alleviate the problems associated with centralized cloud computing systems by placing computational resources to the network’s edge, closer to the users. However, the complexity of distributed edge infrastructures grows when hosting containerized workloads as microservices, resulting in hard to detect and troubleshoot outages on critical use cases such as industrial automation processes. Observability aims to support operators in managing and operating complex distributed infrastructures and microservices architectures by instrumenting end-to-end runtime performance. To the best of our knowledge, no survey article has been recently proposed for distributed edge and containerized microservices observability. Thus, this article surveys and classifies state-of-the-art solutions from various communities. Besides surveying state-of-the-art, this article also discusses the observability concept, requirements, and design considerations. Finally, we discuss open research issues as well as future research directions that will inspire additional research in this area.","2169-3536","","10.1109/ACCESS.2022.3193102","Knowledge Foundation of Sweden (KKS) through the Synergy Project AIDA—A Holistic AI-Driven Networking and Processing Framework for Industrial IoT(grant numbers:Rek:20200067); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9837035","Automation;cloud-native;containers;DevOps;edge;Industrial Internet of Things (IIoT);microservices;monitoring;observability;operation;software-defined infrastructure;visibility","Monitoring;Observability;Cloud computing;Microservice architectures;Computer architecture;Industrial Internet of Things;Image edge detection","","38","","106","CCBY","21 Jul 2022","","","IEEE","IEEE Journals"
"Network Functions Virtualization: The Long Road to Commercial Deployments","A. U. Rehman; R. L. Aguiar; J. P. Barraca","Instituto de Telecomunicações, Aveiro, Portugal; Instituto de Telecomunicações, Aveiro, Portugal; Instituto de Telecomunicações, Aveiro, Portugal",IEEE Access,"20 May 2019","2019","7","","60439","60464","Network operators are under pressure to offer efficient network-based services while keeping service deployment costs to a minimum. Network functions virtualization (NFV) can potentially revolutionize network-based services bringing low-deployment costs for network operators. The NFV has been introduced to ultimately extend the non-proprietary and open-standard-based model to network and service deployments, significant improvements to today’s proprietary locked implementations. Notwithstanding the continuous efforts of both academia and industry to support the NFV paradigm, the current NFV solutions offered are still in its infancy. In this survey, we provide a detailed background of NFV to establish a comprehensive understanding of the subject, ranging from the basics to more advanced topics. Moreover, we offer a comprehensive overview of the NFV main concepts, standardization efforts, the benefits of NFV, and discussions of the NFV architecture as defined by the European telecommunications standardization institute (ETSI). Furthermore, we discuss the NFV applicability and current open source projects. We then highlight NFV requirements, design considerations, and developmental architectural impairments and barriers to commercial NFV deployments. Finally, we conclude enumerating future directions for NFV development.","2169-3536","","10.1109/ACCESS.2019.2915195","FCT/MEC through national funds; FEDER - PT2020 partnership agreement(grant numbers:UID/EEA/50008/2019); Fundação para a Ciência e a Tecnologia(grant numbers:PD/BD/113822/2015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8708314","Network functions virtualization;virtual network functions;server virtualization;hypervisors;containers;Unikernels;network management and orchestration;network programmability;network softwarization","Virtualization;Telecommunications;Hardware;Software;Roads;Computer architecture;Overlay networks","","38","","202","OAPA","7 May 2019","","","IEEE","IEEE Journals"
"Multi-Objective Optimization for Virtual Machine Allocation and Replica Placement in Virtualized Hadoop","C. Guerrero; I. Lera; B. Bermejo; C. Juiz","Computer Science Department, Balearic Islands University, Palma, Spain; Computer Science Department, Balearic Islands University, Palma, Spain; Computer Science Department, Balearic Islands University, Palma, Spain; Computer Science Department, Balearic Islands University, Palma, Spain",IEEE Transactions on Parallel and Distributed Systems,"9 Oct 2018","2018","29","11","2568","2581","Resource management is a key factor in the performance and efficient utilization of cloud systems, and many research works have proposed efficient policies to optimize such systems. However, these policies have traditionally managed the resources individually, neglecting the complexity of cloud systems and the interrelation between their elements. To illustrate this situation, we present an approach focused on virtualized Hadoop for a simultaneous and coordinated management of virtual machines and file replicas. Specifically, we propose determining the virtual machine allocation, virtual machine template selection, and file replica placement with the objective of minimizing the power consumption, physical resource waste, and file unavailability. We implemented our solution using the non-dominated sorting genetic algorithm-II, which is a multi-objective optimization algorithm. Our approach obtained important benefits in terms of file unavailability and resource waste, with overall improvements of approximately 400 and 170 percent compared to three other optimization strategies. The benefits for the power consumption were smaller, with an improvement of approximately 1.9 percent.","1558-2183","","10.1109/TPDS.2018.2837743","Spanish Government; Agencia Estatal de Investigación; European Commission; Fondo Europeo de Desarrollo Regional(grant numbers:TIN2017-88547-P); MINECO/AEI/FEDER; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8360490","Virtual machine allocation;file replica placement;hadoop;evolutionary computing and genetic algorithms","Resource management;Power demand;Genetic algorithms;Optimization;Virtual machining;Cloud computing;Sorting","","38","","59","IEEE","17 May 2018","","","IEEE","IEEE Journals"
"Autonomous Shuttle-as-a-Service (ASaaS): Challenges, Opportunities, and Social Implications","A. Bucchiarone; S. Battisti; A. Marconi; R. Maldacea; D. C. Ponce","Fondazione Bruno Kessler, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy; I-Mobility Garage, Verona, Italy; Navya Tech, Lyon, France",IEEE Transactions on Intelligent Transportation Systems,"31 May 2021","2021","22","6","3790","3799","Providing mobility services effectively to residents and visitors is a complex socio-technical system task to city public managers. Smart mobility systems aim to support the efficient exploitation of city transport facilities and sustainable mobility within the urban environment. People need to travel quickly and conveniently between locations at different scales, ranging from a few blocks within a city to a journey across cities. At the same time, goods need to be timely delivered, considering both the users and the businesses' needs. Several cities indicated an interest in using Autonomous Vehicles (AV) for the “last-mile” mobility services in the last few years. With them, it seems to be easier to get people and goods around using fewer vehicles. In this context, Autonomous Shuttles (AS) are beginning to be thought of as a new mobility/delivery service into the city center where narrow streets are not easily served by traditional buses. They allow them to perform critical areas with minimal new infrastructure and reduce noise and pollution. The article analyses the state-of-art on autonomous shuttles by proposing four application scenarios targeting the last-mile delivery of goods, the tourist experiences, and the shared and integrated mobility. Furthermore, we contribute with the proposition of the Autonomous Shuttles-as-a service (ASaaS) concept as the key pillar for the realization of innovative and sustainable proximity mobility. Our research proposed new research challenges for ASaaS, and we discuss social implications and governance challenges that consider user engagement and sustainability. It also recommended extending new research to focus on simulation and machine learning techniques for last-mile mobility planning and explore the journeys tracking certification via artificial intelligence and blockchain-based techniques.","1558-0016","","10.1109/TITS.2020.3025670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9210552","Smart mobility;autonomous shuttles;proximity mobility;last mile delivery;mobility services","Autonomous vehicles;Biological system modeling;Automobiles;Safety;Smart cities","","37","","62","IEEE","1 Oct 2020","","","IEEE","IEEE Journals"
"Federated Learning for the Healthcare Metaverse: Concepts, Applications, Challenges, and Future Directions","A. K. Bashir; N. Victor; S. Bhattacharya; T. Huynh-The; R. Chengoden; G. Yenduri; P. K. R. Maddikunta; Q. -V. Pham; T. R. Gadekallu; M. Liyanage","Department of Computing and Mathematics, Manchester Metropolitan University, Manchester, U.K.; School of Information Technology, Vellore Institute of Technology, Vellore, India; School of Information Technology, Vellore Institute of Technology, Vellore, India; Department of Computer and Communications Engineering, Ho Chi Minh City University of Technology and Education, Ho Chi Minh City, Vietnam; School of Information Technology, Vellore Institute of Technology, Vellore, India; School of Information Technology, Vellore Institute of Technology, Vellore, India; School of Information Technology, Vellore Institute of Technology, Vellore, India; School of Computer Science and Statistics, Trinity College Dublin, The University of Dublin, Dublin 2, Ireland; School of Information Technology, Vellore Institute of Technology, Vellore, India; School of Computer Science, University College Dublin, D04 V1W8, Ireland",IEEE Internet of Things Journal,"11 Dec 2023","2023","10","24","21873","21891","Recent technological advancements have considerably improved healthcare systems to provide various intelligent services, improving life quality. The Metaverse, often described as the next evolution of the Internet, helps the users interact with each other and the environment, thus offering a seamless connection between the virtual and physical worlds. Additionally, the Metaverse, by integrating emerging technologies, such as artificial intelligence (AI), cloud edge computing, Internet of Things (IoT), blockchain, and semantic communications, can potentially transform many vertical domains in general and the healthcare sector (healthcare Metaverse) in particular. The healthcare Metaverse holds huge potential to revolutionize the development of intelligent healthcare systems, thus presenting new opportunities for significant advancements in healthcare delivery, personalized healthcare experiences, medical education, collaborative research, and so on. However, various challenges are associated with the realization of the healthcare Metaverse, such as privacy, interoperability, data management, and security. Federated learning (FL), a new branch of AI, opens up enormous opportunities to deal with the aforementioned challenges in the healthcare Metaverse by exploiting the data and computing resources available at the distributed devices. This motivated us to present a survey on adopting FL for the healthcare Metaverse. Initially, we present the preliminaries of IoT-based healthcare systems, FL in conventional healthcare, and the healthcare Metaverse. Furthermore, the benefits of the FL in the healthcare Metaverse are discussed. Subsequently, we discuss the several applications of FL-enabled healthcare Metaverse, including medical diagnosis, patient monitoring, medical education, infectious disease, and drug discovery. Finally, we highlight the significant challenges and potential solutions toward realizing FL in the healthcare Metaverse.","2327-4662","","10.1109/JIOT.2023.3304790","European Union in SPATIAL(grant numbers:101021808); CONFIDENTIAL-6G(grant numbers:101096435); Science Foundation Ireland under CONNECT Phase 2 Projects(grant numbers:13/RC/2077_P2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10215363","Cobots;digital twins;disease diagnosis;federated learning (FL);healthcare;healthcare metaverse;metaverse","Medical services;Metaverse;Data privacy;Medical diagnostic imaging;Artificial intelligence;Collaboration;Security","","36","","111","IEEE","14 Aug 2023","","","IEEE","IEEE Journals"
"Improved QoS at the Edge Using Serverless Computing to Deploy Virtual Network Functions","S. R. Chaudhry; A. Palade; A. Kazmi; S. Clarke","School of Computer Science and Statistics, Trinity College Dublin, Dublin, Ireland; School of Computer Science and Statistics, Trinity College Dublin, Dublin, Ireland; School of Computer Science and Statistics, Trinity College Dublin, Dublin, Ireland; School of Computer Science and Statistics, Trinity College Dublin, Dublin, Ireland",IEEE Internet of Things Journal,"9 Oct 2020","2020","7","10","10673","10683","Multiaccess edge computing (MEC) will strengthen forthcoming 5G networks by improving the Quality of Service (QoS), in particular, reducing latency, increasing data processing rates, and providing real-time information to develop high-value Internet-of-Things (IoT) services. To enable data-intensive network services and support advanced analytics, many network operators have proposed to integrate MEC systems with network function virtualization (NFV) consolidating virtual network functions (VNFs) and edge capabilities on a shared infrastructure. As of yet, this integration is not fully established, with various architectural issues currently open, even at standardization level. For instance, any update to VNFs deployed in a MEC system requires a time-consuming manual effort, which affects the overall infrastructure operations. To address these pitfalls, VNFs can be decomposed into microservices, which maintain their own states and exhibit different resource consumption requirements. This article presents an approach to integration that leverages serverless computing to merge MEC and NFV at the system level and to deploy VNFs on demand, by combining MEC functional blocks with an NFV orchestrator using a Kubernetes cluster. We further investigate whether the resource utilization of a MEC system can be improved by leveraging networked FPGA-enabled MEC servers, through an extension of the edge layer that takes advantage of available programmable hardware. We quantitatively evaluate and demonstrate the improvement of 75% end-to-end latency, 99.96% VNF execution time, 26.9% resource utilization, and 15.8% energy consumption in comparison with traditional baselines of cloud, edge, and serverless-edge test cases for a high-definition real-time video streaming application.","2327-4662","","10.1109/JIOT.2020.3011057","Science Foundation Ireland (SFI)(grant numbers:16/SP/3804); Enable Research Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9145582","Edge computing;networked FPGA;serverless computing;virtual network functions (VNFs) and data security","Hardware;Internet of Things;Field programmable gate arrays;Streaming media;Servers;Task analysis;Quality of service","","36","","52","IEEE","21 Jul 2020","","","IEEE","IEEE Journals"
"Smart Audio Sensors in the Internet of Things Edge for Anomaly Detection","M. Antonini; M. Vecchio; F. Antonelli; P. Ducange; C. Perera","OpenIoT Research Unit, FBK CREATE-NET, Povo (TN), Italy; OpenIoT Research Unit, FBK CREATE-NET, Povo (TN), Italy; OpenIoT Research Unit, FBK CREATE-NET, Povo (TN), Italy; SMARTEST Research Centre, ECampus University, Novedrate (CO), Italy; School of Computer Science and Informatics, Cardiff University, Cardiff, U.K.",IEEE Access,"28 Nov 2018","2018","6","","67594","67610","Everyday objects are becoming smart enough to directly connect to other nearby and remote objects and systems. These objects increasingly interact with machine learning applications that perform feature extraction and model inference in the cloud. However, this approach poses several challenges due to latency, privacy, and dependency on network connectivity between data producers and consumers. To alleviate these limitations, computation should be moved as much as possible towards the IoT edge, that is on gateways, if not directly on data producers. In this paper, we propose a design framework for smart audio sensors able to record and pre-process raw audio streams, before wirelessly transmitting the computed audio features to a modular IoT gateway. In this paper, an anomaly detection algorithm executed as a micro-service is capable of analyzing the received features, hence detecting audio anomalies in real-time. First, to assess the effectiveness of the proposed solution, we deployed a real smart environment showcase. More in detail, we adopted two different anomaly detection algorithms, namely Elliptic Envelope and Isolation Forest, that were purposely trained and deployed on an affordable IoT gateway to detect anomalous sound events happening in an office environment. Then, we numerically compared both the deployments, in terms of end-to-end latency and gateway CPU load, also deriving some ideal capacity bounds.","2169-3536","","10.1109/ACCESS.2018.2877523","Horizon 2020 Framework Programme(grant numbers:688088); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8502761","Anomaly detection;digital signal processing;edge computing;embedded devices;Internet of Things;IoT gateway;machine learning;novelty detection;open-source platforms;outlier detection","Anomaly detection;Logic gates;Intelligent sensors;Synthetic aperture sonar;Internet of Things;Software","","36","","66","CCBY","23 Oct 2018","","","IEEE","IEEE Journals"
"A Survey on Mobility of Edge Computing Networks in IoT: State-of-the-Art, Architectures, and Challenges","F. Shirin Abkenar; P. Ramezani; S. Iranmanesh; S. Murali; D. Chulerttiyawong; X. Wan; A. Jamalipour; R. Raad","WiNG Lab, School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; WiNG Lab, School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; School of Electrical, Computer and Telecommunication Engineering, University of Wollongong, Wollongong, NSW, Australia; WiNG Lab, School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; WiNG Lab, School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; WiNG Lab, School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; WiNG Lab, School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; School of Electrical, Computer and Telecommunication Engineering, University of Wollongong, Wollongong, NSW, Australia",IEEE Communications Surveys & Tutorials,"21 Nov 2022","2022","24","4","2329","2365","Edge computing leverages computing resources closer to the end-users at the edge of the network, rather than distant cloud servers in the centralized IoT architecture. Edge computing nodes (ECNs), experience less transmission latency and usually save on energy while network overheads are mitigated. The ECNs can be fixed or mobile in their positions. We will focus on mobile ECNs in this survey. This paper presents a comprehensive survey on mobile ECNs and identifies some open research questions. In particular, mobile ECNs are classified into four categories, namely aerial, ground vehicular, spatial, and maritime nodes. For each specific group, any mutual basic terms used in the state-of-the-art are described, different types of nodes employed in the group are reviewed, the general network architecture is introduced, the existing methods and algorithms are studied, and the challenges that the group is scrimmaging against are explored. Moreover, the integrated architectures are surveyed, wherein two different categories of the aforementioned nodes jointly play the role of ECNs in the network. Finally, the research gaps, that are yet to be filled in the area of mobile ECNs, are discussed along with directions for future research and investigation in this promising area.","1553-877X","","10.1109/COMST.2022.3211462","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910565","Mobile edge computing nodes;aerial nodes;ground vehicular nodes;spatial nodes;maritime vessels;architectures;challenges;applications","Cloud computing;Task analysis;Internet of Things;Edge computing;Computer architecture;Servers;Delays","","36","","171","IEEE","4 Oct 2022","","","IEEE","IEEE Journals"
"Autonomous Lifecycle Management for Resource-Efficient Workload Orchestration for Green Edge Computing","F. Guim; T. Metsch; H. Moustafa; T. Verrall; D. Carrera; N. Cadenelli; J. Chen; D. Doria; C. Ghadie; R. G. Prats","Network and Edge Group, Intel Corporation, Santa Clara, CA, USA; Network and Edge Group, Intel Corporation, Santa Clara, CA, USA; Network and Edge Group, Intel Corporation, Santa Clara, CA, USA; Network and Edge Group, Intel Corporation, Santa Clara, CA, USA; Edge Computing Department, NearbyComputing, Barcelona, Spain; Edge Computing Department, NearbyComputing, Barcelona, Spain; Edge Computing Department, Lenovo Group Ltd., Ottawa, ON, Canada; Edge Computing Department, Lenovo Group Ltd., Ottawa, ON, Canada; Edge Computing Department, Lenovo Group Ltd., Ottawa, ON, Canada; Edge Computing Department, Cellnex Telecom, Barcelona, Spain",IEEE Transactions on Green Communications and Networking,"16 Feb 2022","2022","6","1","571","582","Edge computing is an important pillar for green computation by bringing the Cloud resources to the Edge, serving real-time applications, and reducing the computing and network resources required to transfer data for processing in the Cloud. 5G brings network densification and enables massive IoT and V2X applications, which triggers the need for edge computing to host network functions and user-facing services in a converged edge platform(s). Several edge computing deployments are being observed by ecosystem players (telco, ISVs, chip vendors, CSPs, … etc.) for IoT or V2X services, however, focusing on converged network functions and services. The point that is still in its early stages is the dynamic workload orchestration across the converged edge platforms running network functions and multi-tenant IoT services with different compute requirements and different Service Level Objectives (SLOs). This paper focuses on autonomous life cycle management for converged edge platform(s) to enable resource-efficient workload orchestration, contributing to the green goal. We present a solution for intelligent dynamic resources configuration on edge computing platforms hosting multi-tenant services while guaranteeing the SLO for each service and helping green communication goal. The presented solution has been deployed in a trial, and we present results on efficient resources configuration.","2473-2400","","10.1109/TGCN.2021.3127531","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9612603","Edge computing;edge analytics;intelligent orchestration;resource efficient orchestration;platform resources awareness","Edge computing;Real-time systems;Logic gates;Dynamic scheduling;Cloud computing;Quality of service;5G mobile communication","","36","","41","IEEE","12 Nov 2021","","","IEEE","IEEE Journals"
"Combining Federated Learning and Edge Computing Toward Ubiquitous Intelligence in 6G Network: Challenges, Recent Advances, and Future Directions","Q. Duan; J. Huang; S. Hu; R. Deng; Z. Lu; S. Yu","Information Sciences and Technology Department, The Pennsylvania State University, Abington, PA, USA; Electrical Engineering and Computer Science Department, South Dakota State University, Brookings, SD, USA; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, University of Technology Sydney, Sydney, NSW, Australia",IEEE Communications Surveys & Tutorials,"21 Nov 2023","2023","25","4","2892","2950","Full leverage of the huge volume of data generated on a large number of user devices for providing intelligent services in the 6G network calls for Ubiquitous Intelligence (UI). A key to developing UI lies in the involvement of the large number of network devices, which contribute their data to collaborative Machine Learning (ML) and provide their computational resources to support the learning process. Federated Learning (FL) is a new ML method that enables data owners to collaborate in model training without exposing private data, which allows user devices to contribute their data to developing UI. Edge computing deploys cloud-like capabilities at the network edge, which enables network devices to offer their computational resources for supporting FL. Therefore, a combination of FL and edge computing may greatly facilitate the development of ubiquitous intelligence in the 6G network. In this article, we present a comprehensive survey of the recent developments in technologies for combining FL and edge computing with a holistic vision across the fields of FL and edge computing. We conduct our survey from both the perspective of an FL framework deployed in an edge computing environment (FL in Edge) and the perspective of an edge computing system providing a platform for FL (Edge for FL). From the FL in Edge perspective, we first identify the main challenges to FL in edge computing and then survey the representative technical strategies for addressing the challenges. From the Edge for FL perspective, we first analyze the key requirements for edge computing to support FL and then review the recent advances in edge computing technologies that may be exploited to meet the requirements. Then we discuss open problems and identify some possible directions for future research on combining FL and edge computing, with the hope of arousing the research community’s interest in this emerging and exciting interdisciplinary field.","1553-877X","","10.1109/COMST.2023.3316615","Shanghai Science and Technology Project(grant numbers:22510761000); National Natural Science Foundation of China(grant numbers:61873309,92046024,92146002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10258360","Federated learning;edge computing;ubiquitous intelligence;6G network","Edge computing;Surveys;6G mobile communication;Training;Computational modeling;Internet of Things;Federated learning","","36","","312","IEEE","22 Sep 2023","","","IEEE","IEEE Journals"
"Data Collection in Studies on Internet of Things (IoT), Wireless Sensor Networks (WSNs), and Sensor Cloud (SC): Similarities and Differences","I. Ali; I. Ahmedy; A. Gani; M. U. Munir; M. H. Anisi","Department of Information Systems, Faculty of Computer Science and Information Technology, Universiti Malaya, Kuala Lumpur, Malaysia; Department of Information Systems, Faculty of Computer Science and Information Technology, Universiti Malaya, Kuala Lumpur, Malaysia; Department of Information Systems, Faculty of Computer Science and Information Technology, Universiti Malaya, Kuala Lumpur, Malaysia; Department of Information Systems, Faculty of Computer Science and Information Technology, Universiti Malaya, Kuala Lumpur, Malaysia; School of Computer Science and Electronic Engineering, University of Essex, Colchester, U.K.",IEEE Access,"1 Apr 2022","2022","10","","33909","33931","Data collection is an essential part of sensor devices, particularly in such technologies like Internet of Things (IoT), wireless sensor networks (WSN), and sensor cloud (SC). In recent years, various literature had been published in these research areas to propose different models, architectures, and contributions in the domains. Due to the importance of efficient data collection regarding reducing energy consumption, latency, network lifetime, and general cost, a momentous literature volume has been published to facilitate data collection. Hence, review studies have been conducted on data collection in these domains in isolation. However, a lack of comprehensive review collectively identifies and analyzes the differences and similarities among the data collection proposals in IoT, WSN, and SC. The main objective of this research is to conduct a comprehensive survey to explore the current state, use cases, contributions, performance measures, evaluation measures, and architecture in the IoT, WSN, and SC research domains. The findings indicate that studies on data collection in IoT, WSN, and SC are relatively consistent with stable output in the last five years. Nine novel contributions are found with models, algorithms, and frameworks being the most utilized by the selected studies. In conclusion, key research challenges and future research directions have been identified and discussed.","2169-3536","","10.1109/ACCESS.2022.3161929","Ministry of Higher Education Malaysia Fundamental Research Grant Scheme (FRGS)(grant numbers:FRGS/1/2019/ICT03/UM/02/2); Faculty of Computer Science and Information Technology, University of Malaya(grant numbers:PG035-2016A); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9740691","Data collection in sensor cloud (SC);data collection in wireless sensor networks (WSNs);data collection in Internet of Things (IoT);sensor cloud;similarity and difference;architecture;models;algorithms;frameworks","Data collection;Wireless sensor networks;Cloud computing;Internet of Things;Wireless communication;Taxonomy;Sensors","","35","","110","CCBY","24 Mar 2022","","","IEEE","IEEE Journals"
"Edge Computing and Its Convergence With Blockchain in 5G and Beyond: Security, Challenges, and Opportunities","S. A. Bhat; I. B. Sofi; C. -Y. Chi","Department of Electrical Engineering, Institute of Communications Engineering, National Tsing Hua University, Hsinchu, Taiwan; School of Electronics and Electrical Engineering, Lovely Professional University, Phagwara, India; Department of Electrical Engineering, Institute of Communications Engineering, National Tsing Hua University, Hsinchu, Taiwan",IEEE Access,"18 Nov 2020","2020","8","","205340","205373","The internet is progressing towards a new technology archetype grounded on smart systems, heavily relying on artificial intelligence (AI), machine learning (ML), blockchain platforms, edge computing, and the internet of things (IoT). The merging of IoT, edge computing, and blockchain will be the most important factor of empowering new automatic service and commercial models with various desirable properties, such as self-verifying, self-executing, immutability, data reliability, and confidentiality provided by the advancement in blockchain smart contracts and containers. Motivated by the potential paradigm shift and the security features brought by blockchain from the traditional centralized model to a more robust and resilient decentralized model, this tutorial article proposes a multi-tier integrated blockchain and edge computing architecture for 5G and beyond for solving some security issues faced by resource-constrained edge devices. We begin with a comprehensive overview of different edge computing paradigms and their research challenges. Next, we present the classification of security threats and current defense mechanisms. Then, we present an overview of blockchain and its potential solutions to the main security issues in edge computing. Furthermore, we present the classification of facilitating developers of different architectures to select an appropriate platform for particular applications and offer insights for potential research directions. Finally, we provide key convergence features of the blockchain and edge computing, followed by some conclusions.","2169-3536","","10.1109/ACCESS.2020.3037108","Ministry of Science and Technology (MOST), Taiwan(grant numbers:MOST 108-2221-E-007-012,MOST 109-2221-E007-088); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9253517","Cloud computing;blockchain;edge computing;mist computing;fog computing;security and privacy;5G;cloudlets;server-less computing;consensus process;smart contracts;blockchain platforms","Blockchain;Edge computing;Security;5G mobile communication;Computer architecture;Convergence;Cloud computing","","35","","193","CCBYNCND","10 Nov 2020","","","IEEE","IEEE Journals"
"Microservice-Oriented Service Placement for Mobile Edge Computing in Sustainable Internet of Vehicles","L. Wang; X. Deng; J. Gui; X. Chen; S. Wan","School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; Shenzhen Institute for Advanced Study, University of Electronic Science and Technology of China, Shenzhen, China",IEEE Transactions on Intelligent Transportation Systems,"30 Aug 2023","2023","24","9","10012","10026","The integration of Mobile Edge Computing (MEC) and microservice architecture drives the implementation of the sustainable Internet of Vehicles (IoV). The microservice architecture enables the decomposition of a service into multiple independent, fine-grained microservices working independently. With MEC, microservices can be placed on Edge Service Providers (ESPs) dynamically, responding quickly and reducing service latency and resource consumption. However, the burgeoning of IoV leads to high computation and resource overheads, making service resource requirements an imminent issue. What’s more, due to the limited computation power of ESPs, they can only host a few services. Therefore, ESPs should judiciously decide which services to host. In this paper, we propose a Microservice-oriented Service Placement (MOSP) mechanism for MEC-enabled IoV to shorten service latency, reduce high resource consumption levels and guarantee long-term sustainability. Specifically, we formulate the service placement as an integer linear programming program, where service placement decisions are collaboratively optimized among ESPs, aiming to address spatial demand coupling, service heterogeneity, and decentralized coordination in MEC systems. MOSP comprises an upper layer to map the service requests to ESPs and a lower layer to adjust the service placement of ESPs. Evaluation results show that the microservice-oriented service deployment mechanism offers dramatic improvements in terms of resource savings, latency reduction, and service speed.","1558-0016","","10.1109/TITS.2023.3274307","National Natural Science Foundation of China Projects(grant numbers:62172441,62172449,61772553); Local Science and Technology Developing Foundation Guided by the Central Government through the Free Exploration Project 2021Szvup166;; Postgraduate Research and Innovation Project of Hunan Province(grant numbers:CX20200211); Fundamental Research Funds for the Central Universities of Central South University(grant numbers:2020zzts138); Opening Project of State Key Laboratory of Nickel and Cobalt Resources Comprehensive Utilization(grant numbers:GZSYS-KY-2022-018,GZSYS-KY-2022-024); Key Project of Shenzhen City Special Fund for Fundamental Research(grant numbers:202208183000751); Hunan Provincial Department of Transportation Foundation(grant numbers:201929); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10128791","Mobile edge computing;microservice;Internet of Vehicles;service placement","Microservice architectures;Servers;Cloud computing;Quality of service;Costs;Multi-access edge computing;Memory management","","35","","25","IEEE","17 May 2023","","","IEEE","IEEE Journals"
"Scalable IoT Platform for Heterogeneous Devices in Smart Environments","A. Javed; A. Malhi; T. Kinnunen; K. Främling","Department of Computer Science, Aalto University, Espoo, Finland; Department of Computer Science, Aalto University, Espoo, Finland; Department of Computer Science, Aalto University, Espoo, Finland; Department of Computer Science, Aalto University, Espoo, Finland",IEEE Access,"3 Dec 2020","2020","8","","211973","211985","The Internet of Things (IoT) is envisioned as a ubiquitous computing infrastructure in which everything becomes connected, enabling gigantic information exchange among Things and people. These connected smart Things generate an enormous amount of data which need to be efficiently managed to form a unified global IoT. Unfortunately, due to the lack of acceptable open standards, communication protocols, and support for device/service discovery, the recent IoT deployments in smart environments (e.g., smart home, smart building, smart city) are posing imperative challenges related to interoperability, discovery, and the configuration of deployed objects, since the number of objects is expected to grow over time. Therefore, it is of utmost importance to provide open and scalable solutions for the discovery of devices (i.e., Things), their configuration, and data management. This paper introduces an open and scalable IoT platform by adopting the modular characteristics of edge computing for smart environments. This paper: (i) performs a systematic literature review of IoT-based infrastructures and analyzes the scalability requirements; (ii) proposes a layered IoT platform for smart environments that fosters heterogeneity, interoperability, discovery, and scalability; and (iii) demonstrates the applicability of the proposed solution by relying on a comprehensive study of a Väre smart building use case at Aalto University.","2169-3536","","10.1109/ACCESS.2020.3039368","European Union’s (EU’s) Horizon 2020 Research and Innovation Program(grant numbers:bIoTope: 688203); Horizon 2020 Project FINEST TWINS(grant numbers:856602); Academy of Finland (Open Messaging Interface)(grant numbers:296096); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264173","Internet of Things;hashing;edge computing;discovery;communication standards;scalability","Internet of Things;Cloud computing;Scalability;Interoperability;Edge computing;Computer architecture;Smart cities","","35","","60","CCBY","19 Nov 2020","","","IEEE","IEEE Journals"
"Is Fragmentation a Threat to the Success of the Internet of Things?","M. Aly; F. Khomh; Y. -G. Guéhéneuc; H. Washizaki; S. Yacout","SWAT Laboratory, Polytechnique Montréal, Montreal, QC, Canada; SWAT Laboratory, Polytechnique Montréal, Montreal, QC, Canada; Ptidej Team, Concordia University, Montreal, QC, Canada; Reliable Software Engineering Laboratory, Waseda University, Tokyo, Japan; MAGI, Polytechnique Montréal, Montreal, QC, Canada",IEEE Internet of Things Journal,"24 Feb 2019","2019","6","1","472","487","Internet of Things (IoT) aims to bring connectivity to almost every objects, i.e., things, found in the physical space. It extends connectivity to everyday things, however, such increase in the connectivity creates many prominent challenges. Context: Generally, IoT opens the door for new applications for machine-to-machine and human-to-human communications. The current trend of collaborating, distributed teams through the Internet, mobile communications, and autonomous entities, e.g., robots, is the first phase of the IoT to develop and deliver diverse services and applications. However, such collaborations is threatened by the fragmentation that we witness in the industry nowadays as it brings difficulty to integrate the diverse technologies of the various objects found in IoT systems. Diverse technologies induce interoperability issues while designing and developing various services and applications, hence, limiting the possibility of reusing the data, more specifically, the software (including frameworks, firmware, applications programming interfaces, and user interfaces) as well as of facing issues, like security threats and bugs, when developing new services or applications. Different aspects of handling data collection ranging from discovering smart sensors for data collection, integrating and applying reasoning on them must be available to provide interoperability and flexibility to the diverse objects interacting in the system. However, such approaches are bound to be challenged in future IoT scenarios as they bring substantial performance impairments in settings with the very large number of collaborating devices and technologies. Objective: We raise the awareness of the community about the lack of interoperability among technologies developed for IoT and challenges that their integration poses. We also provide guidelines for researchers and practitioners interested in connecting IoT networks and devices to develop services and applications. Method: We apply the methods advocated by the evidence-based software engineering paradigm. This paradigm and its core tool, the systematic literature review (SLR), were introduced to the software-engineering research community early 2004 to help researchers and industry systematically and objectively gather and aggregate evidences about different topics. In this paper, we conduct an SLR of both IoT interoperability issues and the state-of-practice of IoT technologies in the industry, highlighting the integration challenges related to the IoT that have significantly shifted the landscape of Internet-based collaborative services and applications nowadays. Results: Our SLR identifies a number of studies from journals, conferences, and workshops with the highest quality in the field. This SLR reports different trends, including frameworks and technologies, for the IoT for better comprehension of the paradigm and discusses the integration and interoperability challenges across the different layers of this technology while shedding light on the current IoT state-of-practice. It also discusses some future research directions for the community.","2327-4662","","10.1109/JIOT.2018.2863180","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8424819","Internet of Things (IoT);interoperability;models;protocols;standards;technologies and frameworks","Interoperability;Internet of Things;Biological system modeling;Machine-to-machine communications;Wireless sensor networks;Data models","","34","","130","IEEE","3 Aug 2018","","","IEEE","IEEE Journals"
"MLOps: A Taxonomy and a Methodology","M. Testi; M. Ballabio; E. Frontoni; G. Iannello; S. Moccia; P. Soda; G. Vessio","Integrated Research Centre, Università Campus Bio-Medico di Roma, Rome, Italy; DeepLearningItalia, Bergamo, Italy; Department of Political Sciences, Communication and International Relations, VRAI Laboratory, Università degli Studi di Macerata, Macerata, Italy; Department of Engineering, Unit of Computer Systems and Bioinformatics, Università Campus Bio-Medico di Roma, Rome, Italy; The BioRobotics Institute, Scuola Superiore Sant’Anna, Pisa, Italy; Department of Engineering, Unit of Computer Systems and Bioinformatics, Università Campus Bio-Medico di Roma, Rome, Italy; Department of Computer Science, Università degli Studi di Bari Aldo Moro, Bari, Italy",IEEE Access,"20 Jun 2022","2022","10","","63606","63618","Over the past few decades, the substantial growth in enterprise-data availability and the advancements in Artificial Intelligence (AI) have allowed companies to solve real-world problems using Machine Learning (ML). ML Operations (MLOps) represents an effective strategy for bringing ML models from academic resources to useful tools for solving problems in the corporate world. The current literature on MLOps is still mostly disconnected and sporadic. In this work, we review the existing scientific literature and we propose a taxonomy for clustering research papers on MLOps. In addition, we present methodologies and operations aimed at defining an ML pipeline to simplify the release of ML applications in the industry. The pipeline is based on ten steps: business problem understanding, data acquisition, ML methodology, ML training & testing, continuous integration, continuous delivery, continuous training, continuous monitoring, explainability, and sustainability. The scientific and business interest and the impact of MLOps have grown significantly over the past years: the definition of a clear and standardized methodology for conducting MLOps projects is the main contribution of this paper.","2169-3536","","10.1109/ACCESS.2022.3181730","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9792270","MLOps;continuous monitoring;continuous integration;continuous delivery;continuous training;XAI;sustainability","Pipelines;Production;Monitoring;Training;Business;Automation;Surgery","","34","","98","CCBY","9 Jun 2022","","","IEEE","IEEE Journals"
"Decentralized Iot Edge Nanoservice Architecture for Future Gadget-Free Computing","E. Harjula; P. Karhula; J. Islam; T. Leppänen; A. Manzoor; M. Liyanage; J. Chauhan; T. Kumar; I. Ahmad; M. Ylianttila","Centre for Wireless Communications, University of Oulu, Oulu, Finland; VTT Technical Research Centre of Finland Ltd., Espoo, Finland; Centre for Wireless Communications, University of Oulu, Oulu, Finland; Centre for Ubiquitous Computing, University of Oulu, Oulu, Finland; Rovio Entertainment Ltd., Espoo, Finland; Centre for Wireless Communications, University of Oulu, Oulu, Finland; Department of Computer Science and Technology, University of Cambridge, Cambridge, U.K.; Centre for Wireless Communications, University of Oulu, Oulu, Finland; VTT Technical Research Centre of Finland Ltd., Espoo, Finland; Centre for Wireless Communications, University of Oulu, Oulu, Finland",IEEE Access,"2 Sep 2019","2019","7","","119856","119872","In the envisioned ubiquitous world, services will follow users as they move across smart surroundings. Services are instantiated to users through the environment, appearing and disappearing as they move, which reduces the need for personal communication devices such as smartphones or tablets. To facilitate this development, service architectures need to support virtualized, on-demand service composition based on the hardware and software resources available at the current user location. The technical context for this type of user interaction with digital services through smart surroundings is called Internet of Everything (IoE). Today's service architectures will be too inflexible in this highly decentralized and dynamic environment. Hence, in this article we propose a novel service model called nanoEdge, where nodes collaboratively provide needed functions for virtual services that need to be deployed locally due to performance, efficiency or reliability requirements, for example. The main contributions of this article are the nanoEdge conceptual model and its proof-of-concept (PoC) implementation to show that the model is feasible with regard to performance and resource-efficiency. The successful demonstration of PoC implementation exemplifies future IoE service scenarios with today's hardware components.","2169-3536","","10.1109/ACCESS.2019.2936714","Academy of Finland; Technology Industries of Finland Centennial Foundation; Jane and Aatos Erkko Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8808867","Edge computing;fog computing;IoT;IoE;virtualization;microservices;nanoservices;gadget-free computing","Edge computing;Computer architecture;Internet of Things;Data centers;Hardware;Cloud computing;Virtualization","","34","","49","CCBY","21 Aug 2019","","","IEEE","IEEE Journals"
"Microservice Deployment in Edge Computing Based on Deep Q Learning","W. Lv; Q. Wang; P. Yang; Y. Ding; B. Yi; Z. Wang; C. Lin","School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China",IEEE Transactions on Parallel and Distributed Systems,"25 May 2022","2022","33","11","2968","2978","The microservice deployment strategy is promising in reducing the overall service response time in the microservice-oriented edge computing platform. However, existing works ignore the effect of different interaction frequencies among microservices and the decrease in service execution performance caused by the increased node loads. In this article, we first model the invocation relationships among microservices as an undirected and weighted interaction graph to characterize the communication overhead. Then, we propose a multi-objective microservice deployment problem (MMDP) in edge computing. MMDP aims to minimize the communication overhead while achieving load balance between edge nodes. Without the requirement for domain experts, we propose Reward Sharing Deep Q Learning (RSDQL), a learning-based algorithm, to solve MMDP and obtain the optimal deployment strategy. In addition, to improve the scalability of the services, we propose an Elastic Scaling algorithm (ES) based on heuristics to deal with the dynamic pressure of requests. Finally, we conduct a series of experiments in Kubernetes to evaluate the performance of our approach. Experimental results indicate that, compared with interaction-aware strategy and Kubernetes default strategy, RSDQL has shorter response times, more balanced resource loads, and makes services scale elastically according to the request pressure.","1558-2183","","10.1109/TPDS.2022.3150311","National Natural Science Foundation of China(grant numbers:61972302,61962019); Shaanxi Key Technology R&D Program(grant numbers:2021ZDLGY07-01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9712168","Microservice;interaction awareness;load balancing;multi-objective model;deep Q learning;elastic scaling","Microservice architectures;Containers;Load modeling;Edge computing;Time factors;Load management;Scalability","","34","","49","IEEE","11 Feb 2022","","","IEEE","IEEE Journals"
"Bi-Objective Optimization for Energy Aware Internet of Things Service Composition","O. Alsaryrah; I. Mashal; T. -Y. Chung","Yuan Ze University, Chung-Li, Taoyuan, TW; Computer Science Department, Aqaba University of Technology, Aqaba, Jordan; Yuan Ze University, Chung-Li, Taoyuan, TW",IEEE Access,"5 Jun 2018","2018","6","","26809","26819","In recent years, service-oriented-based Internet of Things (IoT) has received massive attention from research and industry. Integrating and composing smart objects functionalities or their services is required to create and promote more complex IoT applications with advanced features. When many smart objects are deployed, selecting the most appropriate set of smart objects to compose a service by considering both energy and quality of service (QoS) is an essential and challenging task. In this paper, we reduced the problem of finding an optimal balance between QoS level and the consumed energy of the IoT service composition to a bi-objective shortest path optimization (BSPO) problem and used an exact algorithm named pulse to solve the problem. The BSPO has two objectives, minimizing the QoS including execution time, network latency, and service price, and minimize the energy consumption of the composite service. Experimental evaluations show that the proposed approach has short execution time in various complex service profiles. Meanwhile, it can obtain good performance in energy consumption and thus network lifetime while maintaining a reasonable QoS level.","2169-3536","","10.1109/ACCESS.2018.2836334","Ministry of Science and Technology, Taiwan(grant numbers:MOST 106-2221-E-155-014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8359282","IoT services;energy efficiency;service composition","Quality of service;Energy consumption;Optimization;Task analysis;Energy efficiency;Internet of Things;Wireless sensor networks","","34","","29","OAPA","15 May 2018","","","IEEE","IEEE Journals"
"Multi-Agent Based Autonomic Network Management Architecture","S. T. Arzo; R. Bassoli; F. Granelli; F. H. P. Fitzek","Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Deutsche Telekom Chair of Communication Networks, Institute of Communication Technology, Faculty of Electrical and Computer Engineering, Technische Universität Dresden, Dresden, Germany; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Deutsche Telekom Chair of Communication Networks, Institute of Communication Technology, Faculty of Electrical and Computer Engineering, Technische Universität Dresden, Dresden, Germany",IEEE Transactions on Network and Service Management,"8 Sep 2021","2021","18","3","3595","3618","The advent of network softwarization is enabling multiple innovative solutions through software-defined networking (SDN) and network function virtualization (NFV). Specifically, network softwarization paves the way for autonomic and intelligent networking, which has gained popularity in the research community. Along with the arrival of 5G and beyond, which interconnects billions of devices, the complexity of network management is significantly increasing both investments and operational costs. Autonomic networking is the creation of self-organizing, self-managing, and self-protecting networks, to afford the network management complexes and heterogeneous networks. To achieve full network automation, various aspects of networking need to be addressed. So, this article proposes a novel architecture for the multi-agent-based network automation of the network management system (MANA-NMS). The architecture rely on network function atomization, which defines atomic decision-making units. Such units could represent virtual network functions. These atomic units are autonomous and adaptive. First, the article presents a theoretical discussion of the challenges arisen by automating the decision-making process. Next, the proposed multi-agent system is presented along with its mathematical modeling. Finally, MANA-NMS architecture is mathematically evaluated from functionality, reliability, latency, and resource consumption performance perspectives.","1932-4537","","10.1109/TNSM.2021.3059752","NATO Science for Peace and Security (SPS) Programme in the framework of the project SPS G5428 “Dynamic Architecture based on UAVs Monitoring for Border Security and Safety.”; German Research Foundation (DFG, Deutsche Forschungsgemeinschaft) as part of Germany’s Excellence Strategy Ű EXC 2050/1 Ű Project ID 390696704 Ű Cluster of Excellence “Centre for Tactile Internet with Human-in-the-Loop” (CeTI) of Technische Universität Dresden; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354865","Autonomic networking;multi-agent system;network management;network function virtualization;software defined networking;C-RAN;5G;future networks","Automation;Computer architecture;Software;Network function virtualization;Quality of service;Monitoring;Mathematical model","","33","","88","IEEE","16 Feb 2021","","","IEEE","IEEE Journals"
"AI and 6G Into the Metaverse: Fundamentals, Challenges and Future Research Trends","M. Zawish; F. A. Dharejo; S. A. Khowaja; S. Raza; S. Davy; K. Dev; P. Bellavista","Walton Institute for Information and Communication Systems Science, South East Technological University, Waterford, Ireland; Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, UAE; Faculty of Engineering and Technology, University of Sindh, Jamshoro, Pakistan; Department of Electronic Engineering, Quaid-e-Awam University of Engineering, Science and Technology, Larkana, Pakistan; Centre for Sustainable Digital Technologies, Technological University Dublin, Dublin, Ireland; Department of Computer Science, Munster Technological University, Cork, Ireland; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy",IEEE Open Journal of the Communications Society,"29 Jan 2024","2024","5","","730","778","Since Facebook was renamed Meta, a lot of attention, debate, and exploration have intensified about what the Metaverse is, how it works, and the possible ways to exploit it. It is anticipated that Metaverse will be a continuum of rapidly emerging technologies, usecases, capabilities, and experiences that will make it up for the next evolution of the Internet. Several researchers have already surveyed the literature on artificial intelligence (AI) and wireless communications in realizing the Metaverse. However, due to the rapid emergence and continuous evolution of technologies, there is a need for a comprehensive and in-depth survey of the role of AI, 6G, and the nexus of both in realizing the immersive experiences of Metaverse. Therefore, in this survey, we first introduce the background and ongoing progress in augmented reality (AR), virtual reality (VR), mixed reality (MR) and spatial computing, followed by the technical aspects of AI and 6G. Then, we survey the role of AI in the Metaverse by reviewing the state-of-the-art in deep learning, computer vision, and Edge AI to extract the requirements of 6G in Metaverse. Next, we investigate the promising services of B5G/6G towards Metaverse, followed by identifying the role of AI in 6G networks and 6G networks for AI in support of Metaverse applications, and the need for sustainability in Metaverse. Finally, we enlist the existing and potential applications, usecases, and projects to highlight the importance of progress in the Metaverse. Moreover, in order to provide potential research directions to researchers, we underline the challenges, research gaps, and lessons learned identified from the literature review of the aforementioned technologies.","2644-125X","","10.1109/OJCOMS.2024.3349465","Science Foundation Ireland(grant numbers:21/FFP-A/9174); Science Foundation Ireland and the Department of Agriculture, Food and Marine on behalf of the Government of Ireland VistaMilk Research Centre(grant numbers:16/RC/3835); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10415393","Metaverse;5G;6G;AI;cloud and edge computing;AR/VR/XR;spatial computing","6G mobile communication;Surveys;Wireless communication;Computer vision;Telepresence;Metaverse;Artificial intelligence","","33","","307","CCBYNCND","29 Jan 2024","","","IEEE","IEEE Journals"
"A Cooperative Coevolution Genetic Programming Hyper-Heuristics Approach for On-Line Resource Allocation in Container-Based Clouds","B. Tan; H. Ma; Y. Mei; M. Zhang","School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand",IEEE Transactions on Cloud Computing,"2 Sep 2022","2022","10","3","1500","1514","Containers are lightweight and provide the potential to reduce more energy consumption of data centers than Virtual Machines (VMs) in container-based clouds. On-line resource allocation is the most common operation in clouds. However, the on-line Resource Allocation in Container-based clouds (RAC) is new and challenging because of its two-level architecture, i.e., the allocations of containers to VMs and the allocation of VMs to physical machines. These two allocations interact with each other, and hence cannot be made separately. Since on-line container allocation requires a real-time response, most current allocation techniques rely on heuristics (e.g., First Fit and Best Fit), which do not consider the comprehensive information such as workload patterns and VM types. As a result, resources are not used efficiently and the energy consumption is not sufficiently optimized. We first propose a novel model of the on-line RAC problem with the consideration of VM overheads, VM types and an affinity constraint. Then, we design a Cooperative Coevolution Genetic Programming (CCGP) hyper-heuristic approach to solve the RAC problem, named CCGP-RAC. CCGP-RAC can learn the workload patterns and VM types from historical workload traces and generate allocation rules. The experiments show significant improvement in energy consumption compared to the state-of-the-art algorithms.","2168-7161","","10.1109/TCC.2020.3026338","New Zealand Marsden Fund(grant numbers:VUW1510,VUW1614); Royal Society of New Zealand; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9205601","Container-based clouds;container allocation;energy efficiency;genetic programming;hyper-heuristic","Resource management;Containers;Cloud computing;Energy consumption;Genetic programming;Mathematical model;Data centers","","33","","62","IEEE","24 Sep 2020","","","IEEE","IEEE Journals"
"Management and Orchestration of Edge Computing for IoT: A Comprehensive Survey","Y. Chiang; Y. Zhang; H. Luo; T. -Y. Chen; G. -H. Chen; H. -T. Chen; Y. -J. Wang; H. -Y. Wei; C. -T. Chou","Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Communication Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Communication Engineering, National Taiwan University, Taipei, Taiwan",IEEE Internet of Things Journal,"7 Aug 2023","2023","10","16","14307","14331","With the development of telecommunication technologies and the proliferation of network applications in the past decades, the traditional cloud network architecture becomes unable to accommodate such demands due to the heavy burden on the backhaul links and long latency. Therefore, edge computing, which brings network functions close to end-users by providing caching, computing and communication resources at network edges, turns into a promising paradigm. Benefit from its nature, edge computing enables emerging scenarios and use cases, such as augmented reality (AR) and Internet of Things (IowT). However, it also creates complexities to efficiently orchestrate heterogeneous services and manage distributed resources in the edge network. In this survey, we make a comprehensive review of the research efforts on service orchestration and resource management for edge computing. We first give an overview of edge computing, including architectures, advantages, enabling technologies and standardization. Next, a comprehensive survey of state-of-the-art techniques in the management and orchestration of edge computing is presented. Subsequently, the state-of-the-art research on the infrastructure of edge computing is discussed in various aspects. Finally, open research challenges and future directions are presented as well.","2327-4662","","10.1109/JIOT.2023.3245611","National Science and Technology Council (NSTC) of Taiwan(grant numbers:111-2221-E-002-097-MY3,111-2622-8-002-030-,109-2221-E-002-148-MY2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10045724","Computation offloading;content caching;edge computing;resource management;virtual network embedding (VNE)","Edge computing;Cloud computing;Computer architecture;Internet of Things;Resource management;Wireless fidelity;Task analysis","","32","","204","IEEE","16 Feb 2023","","","IEEE","IEEE Journals"
"Toward Greener 5G and Beyond Radio Access Networks—A Survey","L. M. P. Larsen; H. L. Christiansen; S. Ruepp; M. S. Berger","Department of Radio Access Networks, TDC Net, Copenhagen, Denmark; Department of Electrical and Photonics Engineering, Technical University of Denmark, Lyngby, Denmark; Department of Electrical and Photonics Engineering, Technical University of Denmark, Lyngby, Denmark; Department of Electrical and Photonics Engineering, Technical University of Denmark, Lyngby, Denmark",IEEE Open Journal of the Communications Society,"28 Mar 2023","2023","4","","768","797","Mobile network traffic is increasing and so is the energy consumption. The Radio Access Network (RAN) part is responsible for the largest share of the mobile network energy consumption, and thus; an important consideration when expanding mobile networks to meet traffic demands. This work analyses how the energy consumption of future mobile networks can be minimised by using the right RAN architecture, share the network with other operators and implementing the most efficient energy minimising technologies in the RAN. It is explored how the different approaches can be realised in real life networks as well as the research state of the art is highlighted. Furthermore, this work provides an overview of future research directions for 6G energy saving potentials. Different energy saving contributions are evaluated by a common methodology for more realistic comparison, based on the potential energy saving of the overall mobile network consumption. Results show that implementing selected technologies and architectures, the mobile network overall energy consumption can be reduced by approximately 30%, corresponding to almost half of the RAN energy consumption. Following this, a set of guidelines towards an energy optimised mobile network is provided, proposing changes to be made initially and in the longer run for brownfield network operators as well as a target network for greenfield network operators.","2644-125X","","10.1109/OJCOMS.2023.3257889","Innovationsfonden(grant numbers:1045-00047B); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10076806","Green RAN;C-RAN;vRAN;O-RAN;MOCN;MORAN;network slicing;5G;6G;AI","Energy consumption;5G mobile communication;Energy efficiency;Radio access networks;Computer architecture;6G mobile communication;Optimization","","32","","196","CCBY","20 Mar 2023","","","IEEE","IEEE Journals"
"Rusty: Runtime Interference-Aware Predictive Monitoring for Modern Multi-Tenant Systems","D. Masouros; S. Xydis; D. Soudris","Department of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; Department of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; Department of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece",IEEE Transactions on Parallel and Distributed Systems,"18 Aug 2020","2021","32","1","184","198","Modern micro-service and container-based cloud-native applications have leveraged multi-tenancy as a first class system design concern. The increasing number of co-located services/workloads into server facilities stresses resource availability and system capability in an unconventional and unpredictable manner. To efficiently manage resources in such dynamic environments, run-time observability and forecasting are required to capture workload sensitivities under differing interference effects, according to applied co-location scenarios. While several research efforts have emerged on interference-aware performance modelling, they are usually applied at a very coarse-grained manner e.g., estimating the overall performance degradation of an application, thus failing to effectively quantify, predict or provide educated insights on the impact of continuous runtime interference on per-resource allocations. In this paper, we present Rusty, a predictive monitoring system that leverages the power of Long Short-Term Memory networks to enable fast and accurate runtime forecasting of key performance metrics and resource stresses of cloud-native applications under interference. We evaluate Rusty under a diverse set of interference scenarios for a plethora of representative cloud workloads, showing that Rusty i) achieves extremely high prediction accuracy, average R2 value of 0.98, ii) enables very deep prediction horizons retaining high accuracy, e.g., R2 of around 0.99 for a horizon of 1 sec ahead and around 0.94 for an horizon of 5 sec ahead, while iii) satisfying, at the same time, the strict latency constraints required to make Rusty practical for continuous predictive monitoring at runtime.","1558-2183","","10.1109/TPDS.2020.3013948","European Union's Horizon 2020 Research and Innovation programme(grant numbers:825061); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9158547","predictive monitoring;system predictability;LSTM networks;interference aware;multi-tenant systems","Monitoring;Interference;Runtime;Resource management;Measurement;Degradation;Servers","","32","","88","IEEE","4 Aug 2020","","","IEEE","IEEE Journals"
"On Construction of Sensors, Edge, and Cloud (iSEC) Framework for Smart System Integration and Applications","E. Kristiani; C. -T. Yang; C. -Y. Huang; P. -C. Ko; H. Fathoni","Department of Industrial Engineering and Enterprise Information, Tunghai University, Taichung, Taiwan; Department of Computer Science, Tunghai University, Taichung, Taiwan; Department of Industrial Engineering and Enterprise Information, Tunghai University, Taichung, Taiwan; Department of Computer Science, Tunghai University, Taichung, Taiwan; Department of Industrial Engineering and Enterprise Information, Tunghai University, Taichung, Taiwan",IEEE Internet of Things Journal,"22 Dec 2020","2021","8","1","309","319","Intelligent systems influence many aspects of daily life. With the emergence of the Internet of Things (IoT), artificial intelligence (AI), and machine learning (ML), opportunities have been created for smart computing infrastructure. However, problems might arise from the lack of interconnectivity, higher reliability, real-time predictive analytics, and low-latency requirements. Therefore, in this article, we propose the sensors, edge, and cloud (iSEC) framework. The project deploys a smart cloud edge-computing architecture to provide ML and deep learning in the cloud edge environment. Two pilot projects of air quality monitoring system and object detection are demonstrated to evaluate the iSEC framework.","2327-4662","","10.1109/JIOT.2020.3004244","Ministry of Science and Technology (MOST), Taiwan(grant numbers:108-2221-E-029-010,108-2745-8-029-007,108-2622-E-029-007-CC3); National Applied Research Laboratories (NARLabs), Taiwan(grant numbers:03108F1106); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9122603","Cloud computing;edge computing;Internet of Things (IoT);long-range low-power wide-area network (LPWAN);message passing interface (MPI)","Cloud computing;Internet of Things;Sensors;Edge computing;Image edge detection;Wireless sensor networks;Servers","","32","","32","IEEE","22 Jun 2020","","","IEEE","IEEE Journals"
"Validation of IoT Infrastructure for the Construction of Smart Cities Solutions on Living Lab Platform","O. B. Mora-Sánchez; E. López-Neri; E. J. Cedillo-Elias; E. Aceves-Martínez; V. M. Larios","Smart Cities Innovation Center CUCEA University of Guadalajara, Zapopan, Mexico; CIIDETEC, Universidad del Valle de México, Tlaquepaque, México; Smart Cities Innovation Center CUCEA University of Guadalajara, Zapopan, Mexico; Smart Cities Innovation Center CUCEA University of Guadalajara, Zapopan, Mexico; Smart Cities Innovation Center at the Department of Information Systems CUCEA University of Guadalajara, Zapopan, Mexico",IEEE Transactions on Engineering Management,"20 Apr 2021","2021","68","3","899","908","The use of Internet-of-things (IoT) applications (solutions) in the real world has increased exponentially. In smart cities, networked IoT devices are collecting data from the physical medium to optimize the decisions to improve city services to citizens. One way to evaluate this services solution is the use of the living labs, shown as a good option to evaluate previous real applications. However, when this is implemented in the real-world cases, most of them are no scalar to the complexity of the city. One of the factors is that it is assumed an IoT infrastructure designed to meet the properties of the scalability for a smart city. This article proposes a validation methodology for the scalability compliant infrastructure: modularity, interoperability, and resiliency properties. The proposed methodology is based on the best practices achieved during a living lab of the Smart Cities Innovation Center in the Universidad de Guadalajara implementation.","1558-0040","","10.1109/TEM.2020.3002250","Mexican National Research Council(grant numbers:221198 PROINNOVA 2015); HPE Mexico for the LilaSCI IoT devices; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9149637","Internet of Things (IoT);modularity;resilience;scalability;smart city","Smart cities;Scalability;Technological innovation;Resilience;Protocols","","32","","39","IEEE","27 Jul 2020","","","IEEE","IEEE Journals"
"Driver Digital Twin for Online Prediction of Personalized Lane-Change Behavior","X. Liao; X. Zhao; Z. Wang; Z. Zhao; K. Han; R. Gupta; M. J. Barth; G. Wu","CE-CERT, University of California at Riverside, Riverside, CA, USA; CE-CERT, University of California at Riverside, Riverside, CA, USA; College of Engineering, Purdue University, West Lafayette, IN, USA; CE-CERT, University of California at Riverside, Riverside, CA, USA; Toyota Motor North America, InfoTech Labs, Mountain View, CA, USA; Toyota Motor North America, InfoTech Labs, Mountain View, CA, USA; CE-CERT, University of California at Riverside, Riverside, CA, USA; CE-CERT, University of California at Riverside, Riverside, CA, USA",IEEE Internet of Things Journal,"25 Jul 2023","2023","10","15","13235","13246","Connected and automated vehicles (CAVs) are supposed to share the road with human-driven vehicles (HDVs) in a foreseeable future. Therefore, considering the mixed traffic environment is more pragmatic, as the well-planned operation of CAVs may be interrupted by HDVs. In the circumstance that human behaviors have significant impacts, CAVs need to understand HDV behaviors to make safe actions. In this study, we develop a driver digital twin (DDT) for the online prediction of personalized lane-change behavior, allowing CAVs to predict surrounding vehicles’ behaviors with the help of the digital twin technology. DDT is deployed on a vehicle-edge–cloud architecture, where the cloud server models the driver behavior for each HDV based on the historical naturalistic driving data, while the edge server processes the real-time data from each driver with his/her digital twin on the cloud to predict the personalized lane-change maneuver. The proposed system is first evaluated on a human-in-the-loop co-simulation platform, and then in a field implementation with three passenger vehicles driving along an on/off ramp segment connecting to the edge server and cloud through the 4G/LTE cellular network. The lane-change intention can be recognized in 6 s on average before the vehicle crosses the lane separation line, and the Mean Euclidean Distance between the predicted trajectory and GPS ground truth is 1.03 m within a 4-s prediction window. Compared to the general model, using a personalized model can improve prediction accuracy by 27.8%. The demonstration video of the proposed system can be watched at https://youtu.be/5cbsabgIOdM.","2327-4662","","10.1109/JIOT.2023.3262484","Toyota Motor North America, InfoTech Labs; Digital Twin Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10082926","Connected and automated vehicle (CAV);digital twin;driver behavior modeling;field implementation;lane-change prediction","Vehicles;Behavioral sciences;Digital twins;Predictive models;Hidden Markov models;Computational modeling;Data models","","31","","44","IEEE","27 Mar 2023","","","IEEE","IEEE Journals"
"GDPR Compliance Verification in Internet of Things","M. Barati; O. Rana; I. Petri; G. Theodorakopoulos","School of Computer Science and Informatics, Cardiff University, Cardiff, U.K; School of Computer Science and Informatics, Cardiff University, Cardiff, U.K; School of Engineering, Cardiff University, Cardiff, U.K; School of Computer Science and Informatics, Cardiff University, Cardiff, U.K",IEEE Access,"7 Jul 2020","2020","8","","119697","119709","Data privacy in Internet of Things (IoT) applications remains a major concern of regulation bodies. The introduction of the European General Data Protection Regulation (GDPR) enables users to control how their data is accessed and processed, requiring consent from users before any data manipulation is carried out on their (personal) data by smart devices or cloud-hosted services. Blockchains provide the benefits of a distributed and immutable ledger recording digital transactions across a global network of peer nodes. Blockchain support for tracking of operations carried out by an IoT-based system provides greater confidence to a user that the IoT device is not infringing user privacy (as the Blockchain can be audited to verify which operation was carried out, by which actor). A formal model (following the privacy-by-design approach) is proposed for supporting GDPR compliance checking for smart devices. The privacy requirements of such applications are related to GDPR obligations of device (and software systems) operators (such as user consent, data protection, right to forget etc). Three smart contracts are proposed as a practical solution to support automated verification of operations carried out by devices on user data, in accordance with GDPR rules. We evaluate the performance and scalability costs of our approach using a Blockchain test network.","2169-3536","","10.1109/ACCESS.2020.3005509","EPSRC(grant numbers:EP/R033439/1); Project title: “PACE: Privacy-aware Cloud Ecosystems.”; extend appreciation to the Deanship of Scientific Research at Princess Nourah bint Abdualrahman University for supporting his work through the visiting scholar program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9127459","Blockchain-based auditing;business processes;general data protection regulation;Internet of Things;user privacy","Blockchain;Internet of Things;Biomedical monitoring;Data privacy;Privacy;Smart contracts","","31","","46","CCBY","29 Jun 2020","","","IEEE","IEEE Journals"
"A Flexible Distributed Infrastructure for Real-Time Cosimulations in Smart Grids","L. Bottaccioli; A. Estebsari; E. Pons; E. Bompard; E. Macii; E. Patti; A. Acquaviva","Department of Control and Computer Engineering, Politecnico di Torino, Torino, Italy; Department of Energy, Politecnico di Torino, Torino, Italy; Department of Energy, Politecnico di Torino, Torino, Italy; Department of Energy, Politecnico di Torino, Torino, Italy; Department of Control and Computer Engineering, Politecnico di Torino, Torino, Italy; Department of Control and Computer Engineering, Politecnico di Torino, Torino, Italy; Department of Control and Computer Engineering, Politecnico di Torino, Torino, Italy",IEEE Transactions on Industrial Informatics,"7 Dec 2017","2017","13","6","3265","3274","Due to the increasing penetration of distributed generation, storage, electric vehicles, and new information communication technologies, distribution networks are evolving toward the smart grid paradigm. For this reason, new control strategies, algorithms, and technologies need to be tested and validated before their actual field implementation. In this paper, we present a novel modular distributed infrastructure, based on real-time simulation, for multipurpose smart grid studies. The different components of the infrastructure are described, and the system is applied to a case study based on a real urban district located in northern Italy. The presented infrastructure is shown to be flexible and useful for different and multidisciplinary smart grid studies.","1941-0050","","10.1109/TII.2017.2702206","EU project FLEXMETER; Edifici a Zero Consumo Energetico in Distretti Urbani Intelligenti; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7921440","Cosimulation;distributed infrastructure;photovoltaic (PV);real-time simulation;renewable energy;smart grid","Real-time systems;Smart grids;Load modeling;Internet of things;Performance evaluation;Renewable energy sources","","31","","39","IEEE","8 May 2017","","","IEEE","IEEE Journals"
"A Comprehensive Survey of Load Balancing Strategies Using Hadoop Queue Scheduling and Virtual Machine Migration","N. S. Dey; T. Gunasekhar","Department of Computer Science and Engineering, Koneru Lakshmaiah Educational Foundation, Guntur, India; Department of Information Technology, B. V. Raju Institute of Technology, Hyderabad, India",IEEE Access,"23 Jul 2019","2019","7","","92259","92284","The recent growth in the demand for scalable applications from the consumers of the services has motivated the application development community to build and deploy the applications on cloud in the form of services. The deployed applications have significant dependency on the infrastructure available with the application providers. Bounded by the limitations of available resource pools on-premises, many application development companies have migrated the applications to third party cloud environments called data centers. The data center owners or the cloud service providers are entitled to ensure high performance and high availability of the applications and at the same time the desired scalability for the applications. Also, the cloud service providers are also challenging in terms of cost reduction and energy consumption reductions for better manageability of the data center without degrading the performance of the deployed applications. It is to be noted that the performance of the application does not only depend on the responsiveness of the applications rather also must be measured in terms of service level agreements. The violation of the service level agreements or SLA can easily disprove the purpose of application deployments on cloud-based data centers. Thus, the data center owners apply multiple load balancing strategies for maintaining the desired outcomes from the application owners at the minimized cost of data center maintainability. Hence, the demand of the research is to thoroughly study and identify the scopes for improvements in the parallel research outcomes. As the number of applications ranging from small data-centric applications coming with the demand of frequent updates with higher computational capabilities to the big data-centric application as big data analytics applications coming with efficient algorithms for data and computation load managements, the data center owners are forced to think for efficient algorithms for load managements. The algorithms presented by various research attempts have engrossed on application specific demands for load balancing using virtual machine migrations and the solution as the proposed algorithms have become application problem specific. Henceforth, the further demand of the research is a guideline for selecting the appropriate load balancing algorithm via virtual machine migration for characteristics-based specific applications. Hence, this paper presents a comprehensive survey on existing virtual machine migration and selection processes to understand the specific application-oriented capabilities of these strategies with the advantages and bottlenecks. Also, with the understanding of the existing measures for load balancing, it is also important to furnish the further improvement strategies, which can be made possible with a detailed understanding of the parallel research outcomes. Henceforth, this paper also equips the study with guidelines for improvements and for further study. Nonetheless, the study cannot be completed without the mathematical analysis for better understanding and experimental analysis on different standards of datasets for better conclusive decisions. Hence, this paper also presents the discussion on mathematical models and experimental result analysis for the conclusive decision on the improvement factors and the usability of the migration methods for various purposes. Finally, this paper is a comprehensive survey on the background of the research, recent research outcomes using mathematical modeling and experimental studies on various available datasets, and finally identify the scopes of improvements considering various aspects such as execution time, mean time before a VM migration, mean time before a host shutdown, number of node shutdowns, SLA performance degradation, VM migrations, and energy consumption.","2169-3536","","10.1109/ACCESS.2019.2927076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8762113","Data center;load balancing;task scheduler;FIFO;FAIR;capacity;hybrid;LATE;SAMR;context-aware;threshold;IQR;LR;MAD;LRR;THR;VM consolidation;VM migration;MC;MMT;RS;MU;PlanetLab;metric;VM migration analysis;energy consumption analysis;SLA analysis","Task analysis;Load management;Cloud computing;Data centers;Resource management;Virtual machining;Indexes","","31","","74","CCBY","15 Jul 2019","","","IEEE","IEEE Journals"
"Joint Online Optimization of Data Sampling Rate and Preprocessing Mode for Edge–Cloud Collaboration-Enabled Industrial IoT","Y. Shi; C. Yi; B. Chen; C. Yang; K. Zhu; J. Cai","College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Electrical and Computer Engineering, Concordia University, Montreal, QC, Canada",IEEE Internet of Things Journal,"24 Aug 2022","2022","9","17","16402","16417","Edge–cloud collaboration is critical in the Industrial Internet of Things (IIoT) for serving computation-intensive tasks (e.g., bearing fault monitoring) that require low-response delay, low energy consumption, and high processing accuracy. In this article, an energy-efficient resource management framework for IIoT with closed-loop control on end devices, edge servers, and cloud center is studied. In the considered model, each edge server aggregates the data collected by industrial sensors (i.e., end devices) and forms computation tasks for corresponding data analysis. In order to minimize the system-wide energy consumption, while maintaining a guaranteed service delay and a satisfied data processing accuracy for each IIoT application, a joint optimization of: 1) sensors’ sampling rate adaption; 2) edge servers’ preprocessing mode selection; and 3) edge–cloud communication and computing resource allocation is formulated. Further taking into account the time-varying channel conditions and randomness of data arrivals, we propose a low-complexity online algorithm, which solves the problem in a dynamic manner. Particularly, the Lyapunov optimization method is first utilized to decompose the long-term problem into a series of instant ones [mixed-integer nonlinear programming (MINLP) problems], and then a Markov approximation algorithm is applied to solve such instant problems to near optimum with the consideration of future impacts. Performance analyses and simulation results show that the proposed algorithm is feasible under long-term service satisfaction constraints, and its energy consumption and service delay are approximately 20% and 28% lower than those of the benchmark schemes, respectively.","2327-4662","","10.1109/JIOT.2022.3150386","National Natural Science Foundation of China (NSFC)(grant numbers:62002164,61701230,62176122); Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery Grant, CRD Grant, and Concordia University PERFORM Research Chair Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9709555","Communication and computing resource allocation;edge–cloud collaboration;Industrial Internet of Things (IIoT);online optimization;preprocessing mode selection;sampling rate adaption","Industrial Internet of Things;Servers;Delays;Collaboration;Cloud computing;Resource management;Energy consumption","","31","","49","IEEE","10 Feb 2022","","","IEEE","IEEE Journals"
"Edge Computing Technology Enablers: A Systematic Lecture Study","S. Douch; M. R. Abid; K. Zine-Dine; D. Bouzidi; D. Benhaddou","National School of Computer Science and Systems Analysis (ENSIAS), Mohammed V University in Rabat, Rabat, Morocco; TSYS School of Computer Science, Columbus State University, Columbus, GA, USA; Faculty of Sciences (FSR), Mohammed V University in Rabat, Rabat, Morocco; National School of Computer Science and Systems Analysis (ENSIAS), Mohammed V University in Rabat, Rabat, Morocco; Department of Engineering Technology, University of Houston, Houston, TX, USA",IEEE Access,"13 Jul 2022","2022","10","","69264","69302","With the increasing stringent QoS constraints (e.g., latency, bandwidth, jitter) imposed by novel applications (e.g., e-Health, autonomous vehicles, smart cities, etc.), as well as the rapidly increasing number of connected IoT (Internet of Things) devices, the core network is becoming increasingly congested. To cope with those constraints, Edge Computing (EC) is emerging as an innovative computing paradigm that leverages Cloud computing and brings it closer to the customer. “EC” refers to transferring computing power and intelligence from the central Cloud to the network’s Edge. With that, EC promotes the idea of processing and caching data at the Edge, thus reducing network congestion and latency. This paper presents a detailed, thorough, and well-structured assessment of Edge Computing and its enabling technologies. Initially, we start by defining EC from the ground up, outlining its architectures and evolution from Cloudlets to Multi-Access Edge Computing. Next, we survey recent studies on the main cornerstones of an EC system, including resource management, computation offloading, data management, network management, etc. Besides, we emphasized EC technology enablers, starting with Edge Intelligence, the branch of Artificial Intelligence (AI) that integrates AI models at resource-constrained edge nodes with significant heterogeneity and mobility. Then, moving on to 5G and its empowering technologies, we explored how EC and 5G complement each other. After that, we studied virtualization and containerization as promising hosting runtime for edge applications. Further to that, we delineated a variety of EC use-case scenarios, e.g., smart cities, e-Health, military applications, etc. Finally, we concluded our survey by highlighting the role of EC integration with future concerns regarding green energy and standardization.","2169-3536","","10.1109/ACCESS.2022.3183634","National Academy of Sciences (NAS)/United States AID (USAID) under the PEER Cycle 5 Project(grant numbers:5-398); Towards Smart Microgrid: Renewable Energy Integration Into Smart Buildings; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9797685","Edge computing;cloud computing;fog computing;multi-access edge computing;edge intelligence;5G;containerization","Edge computing;Cloud computing;Quality of service;5G mobile communication;Internet of Things;Terminology","","31","","373","CCBY","16 Jun 2022","","","IEEE","IEEE Journals"
"Quantum Cryptography in 5G Networks: A Comprehensive Overview","M. Mehic; L. Michalek; E. Dervisevic; P. Burdiak; M. Plakalovic; J. Rozhon; N. Mahovac; F. Richter; E. Kaljic; F. Lauterbach; P. Njemcevic; A. Maric; M. Hamza; P. Fazio; M. Voznak","Department of Telecommunications, Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina; VSB—Technical University of Ostrava, Ostrava, Czechia; Department of Telecommunications, Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina; VSB—Technical University of Ostrava, Ostrava, Czechia; Department of Telecommunications, Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina; VSB—Technical University of Ostrava, Ostrava, Czechia; Department of Telecommunications, Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina; Deutsche Telekom Security, T-Mobile CZ, Prague, Czechia; Department of Telecommunications, Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina; VSB—Technical University of Ostrava, Ostrava, Czechia; Department of Telecommunications, Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina; Department of Telecommunications, Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina; Department of Telecommunications, Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina; VSB—Technical University of Ostrava, Ostrava, Czechia; Department of Telecommunications, Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina",IEEE Communications Surveys & Tutorials,"26 Feb 2024","2024","26","1","302","346","Every attempt to access to the Internet through a Web browser, email sent, VPN connection, VoIP call, instant message or other use of telecommunications systems involves cryptographic techniques. The most commonly applied technique is asymmetric cryptography, which is generally executed in the background without the user even being aware. It establishes a cryptographic code based on the computational complexity of mathematical problems. However, this type of cryptography, which is widely used in today’s telecommunications systems, is under threat as electronics and computing rapidly develop. The development of fifth-generation cellular networks (5G) is gaining momentum, and given its wide field of application, security requires special attention. This is especially true faced with the development of quantum computers. One solution to this security challenge is to use more advanced techniques to establish cryptographic keys that are not susceptible to attack. An essential part of quantum cryptography, Quantum Key Distribution (QKD) uses the principles of quantum physics to establish and distribute symmetric cryptographic keys between two geographically distant users. QKD establishes information-theoretically secure cryptographic keys that are resistant to eavesdropping when they are created. In this paper, we survey the security challenges and approaches in 5G networks concerning network protocols, interfaces and management organizations. We begin by examining the fundamentals of QKD and discuss the creation of QKD networks and their applications. We then outline QKD network architecture and its components and standards, following with a summary of QKD and post-quantum key distribution techniques and approaches for its integration into existing security frameworks such as VPNs (IPsec and MACsec). We also discuss the requirements, architecture and methods for implementing the FPGA-based encryptors needed to execute cryptographic algorithms with security keys. We discuss the performance and technologies of post-quantum cryptography, and finally, examine reported 5G demonstrations which have used quantum technologies, highlighting future research directions.","1553-877X","","10.1109/COMST.2023.3309051","Ministry of the Interior of the Czech Republic Project “Network Cybersecurity in the Post-Quantum Era”(grant numbers:VJ01010008); NATO SPS G5894 Project “Quantum Cybersecurity in 5G Networks (QUANTUM5)”; H2020 Project OPENQKD(grant numbers:857156); Ministry of Civil Affairs of Bosnia and Herzegovina(grant numbers:10-33-11-7111/22); Ministry of Science, Higher Education and Youth of Canton Sarajevo, Bosnia and Herzegovina(grant numbers:27-02-35-35137-29/22,27-02-35-35143-6/22); Project DQKDNM 2023; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10231341","5G;quantum cryptography;post-quantum cryptography;security","Security;Cryptography;Surveys;Quantum computing;5G mobile communication;Tutorials;Protocols","","31","","236","IEEE","28 Aug 2023","","","IEEE","IEEE Journals"
"Human in the Loop: Industry 4.0 Technologies and Scenarios for Worker Mediation of Automated Manufacturing","C. J. Turner; R. Ma; J. Chen; J. Oyekan","Surrey Business School, University of Surrey, Surrey, Guildford, U.K.; Department of Automatic Control and Systems Engineering, The University of Sheffield, Sheffield, U.K.; Department of Automatic Control and Systems Engineering, The University of Sheffield, Sheffield, U.K.; Department of Automatic Control and Systems Engineering, The University of Sheffield, Sheffield, U.K.",IEEE Access,"29 Jul 2021","2021","9","","103950","103966","Industry 4.0 derived technologies have the potential to enable a new wave of digital manufacturing solutions for semi and fully automated production. In addition, this paradigm encompasses the use of communication technologies to transmit data to processing stations as well as the utilization of cloud based computational resources for data mining. Despite the rise in automation, future manufacturing systems will initially still require humans in the loop to provide supervisory level mediation for even the most autonomous production scenarios. Through a structured review, this paper details a number of key technologies that are most likely to shape this future and describes a range of scenarios for their use in delivering human mediated automated and autonomous production. This paper argues that in all cases of future manufacturing management it is key that the human has oversight of critical information flows and remains an active participant in the delivery of the next generation of production systems.","2169-3536","","10.1109/ACCESS.2021.3099311","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9493179","Human computer interaction;intelligent systems;visualization;interactive systems;context awareness","Manufacturing;Manufacturing systems;Production;Collaboration;Service robots;Job shop scheduling;Context-aware services","","31","","123","CCBYNCND","26 Jul 2021","","","IEEE","IEEE Journals"
"Design and Development of an IoT Gateway for Smart Building Applications","A. Nugur; M. Pipattanasomporn; M. Kuzlu; S. Rahman","Bradley Department of Electrical and Computer Engineering, Advanced Research Institute, Virginia Tech, Arlington, VA, USA; Advanced Research Institute, Virginia Tech, Arlington, VA, USA; Department of Engineering Technology, Old Dominion University, Norfolk, VA, USA; Bradley Department of Electrical and Computer Engineering, Advanced Research Institute, Virginia Tech, Arlington, VA, USA",IEEE Internet of Things Journal,"9 Oct 2019","2019","6","5","9020","9029","Due to the depletion of energy resources and increased energy demand, there is an increased focus on the energy consumption and management in buildings. Many building energy management (BEM) software platforms are commercially available to monitor and control energy consumption. These platforms are hosted on the physical hardware within the building, due to which the hardware specifications limit their performance. To address this limitation, cloud technology emerged which facilitates software to be deployed at a remote location that has scalable hardware resources. Conventional BEM software can leverage such a cloud platform to offer scalable and maintenance-free installation. Once hosted on a remote cloud platform, BEM software lacks direct connectivity to building sensors/controllers, hence requires a device to support remote accessibility. Most devices are bound to a local area network and therefore, need an additional functional layer on top of its communication stack to perform network address translation (NAT)-Traversal. This functionality is implemented on a scalable software which connects to the devices in the network and acts as a gateway for cloud-based BEM software to access devices in the local area network. Any message sent to this gateway is translated to a respective device protocol. This paper describes the design and implementation of such an Internet of Things (IoT) gateway for a cloud-based BEM system that requires support for BACnet, Modbus, and HTTP RESTful interface devices.","2327-4662","","10.1109/JIOT.2019.2926099","U.S. Department of Energy(grant numbers:DE-EE-0006352); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8752396","Building energy management;cloud computing;fog computing;Internet of Things (IoT);IoT gateway","Cloud computing;Logic gates;Software;Protocols;Performance evaluation;Internet of Things;Buildings","","31","","29","IEEE","1 Jul 2019","","","IEEE","IEEE Journals"
"Intelligent Machine Tool Based on Edge-Cloud Collaboration","P. Lou; S. Liu; J. Hu; R. Li; Z. Xiao; J. Yan","School of Information Engineering, Wuhan University of Technology, Wuhan, China; School of Information Engineering, Wuhan University of Technology, Wuhan, China; School of Information Engineering, Wuhan University of Technology, Wuhan, China; School of Mechanical and Electronic Engineering, Wuhan University of Technology, Wuhan, China; School of Mechanical and Electronic Engineering, Wuhan University of Technology, Wuhan, China; School of Information Engineering, Wuhan University of Technology, Wuhan, China",IEEE Access,"7 Aug 2020","2020","8","","139953","139965","As a key equipment of manufacturing, the intelligence of CNC machine tools affects the development and progress of intelligent manufacturing. At the same time, the rapid development of various technologies in recent years, such as cloud computing and edge computing, has brought new methods to improve the intelligence of CNC machine tools. This article proposes a new architecture of intelligent machine tools based on edge-cloud collaboration (IMT-ECC). The hierarchical structure of IMT-ECC is introduced and consists of three layers, data acquisition, network communication and edge-cloud collaboration. Combining the real-time characteristic of edge computing and complex problem processing ability of cloud computing, the edge-cloud collaboration is designed to improve the intelligence of machine tools through data collaboration, information collaboration and knowledge collaboration. Finally, the feasibility of the new intelligent machine tool architecture based on edge-cloud collaboration is verified by experiments with gantry heavy-duty CNC machine tools.","2169-3536","","10.1109/ACCESS.2020.3012829","National Natural Science Foundation Committee (NSFC) of China(grant numbers:51905397); Fundamental Research Funds for the Central Universities(grant numbers:WUT 2018III069GX,WUT 2019III071GX); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9152086","Machine tool 4.0;edge-cloud collaboration;edge computing;cloud computing;intelligent machine tool","Machine tools;Cloud computing;Collaboration;Edge computing;Real-time systems;Computer architecture;Data processing","","29","","31","CCBY","29 Jul 2020","","","IEEE","IEEE Journals"
"Offloading Mechanisms Based on Reinforcement Learning and Deep Learning Algorithms in the Fog Computing Environment","D. H. Abdulazeez; S. K. Askar","Department of Computer Science, University of Duhok, Duhok, Iraq; Erbil Technical Engineering College, Erbil Polytechnic University, Erbil, Iraq",IEEE Access,"10 Feb 2023","2023","11","","12555","12586","Fog computing has emerged as a computing paradigm for resource-restricted Internet of things (IoT) devices to support time-sensitive and computationally intensive applications. Offloading can be utilized to transfer resource-intensive tasks from resource-limited end devices to a resource-rich fog or cloud layer to reduce end-to-end latency and enhance the performance of the system. However, this advantage is still challenging to achieve in systems with a high request rate because it leads to long queues of tasks in fog nodes and reveals inefficiencies in terms of delays. In this regard, reinforcement learning (RL) is a well-known method for addressing such decision-making issues. However, in large-scale wireless networks, both action and state spaces are complex and extremely extensive. Consequently, reinforcement learning techniques may not be able to identify an efficient strategy within an acceptable time frame. Hence, deep reinforcement learning (DRL) was developed to integrate RL and deep learning (DL) to address this problem. This paper presents a systematic analysis of using RL or DRL algorithms to address offloading-related issues in fog computing. First, the taxonomy of fog computing offloading mechanisms based on RL and DRL algorithms was divided into three major categories: value-based, policy-based, and hybrid-based algorithms. These categories were then compared based on important features, including offloading problem formulation, utilized techniques, performance metrics, evaluation tools, case studies, their strengths and drawbacks, offloading directions, offloading mode, SDN-based architecture, and offloading decisions. Finally, the future research directions and open issues are discussed thoroughly.","2169-3536","","10.1109/ACCESS.2023.3241881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10035389","Fog computing;Internet of Things (IoT);offloading;reinforcement learning;deep reinforcement learning","Deep learning;Edge computing;Reinforcement learning;Systematics;Resource management;Heuristic algorithms;Internet of Things","","29","","147","CCBY","2 Feb 2023","","","IEEE","IEEE Journals"
"IoT-DDL–Device Description Language for the “T” in IoT","A. E. Khaled; A. Helal; W. Lindquist; C. Lee","Computer and Information Science and Engineering Department, University of Florida, Gainesville, FL, USA; School of Computing and Communication, Lancaster University, Lancaster, U.K.; School of Computing and Communication, Lancaster University, Lancaster, U.K.; Division of Computer Science and Engineering, Hanyang University, Seoul, South Korea",IEEE Access,"15 May 2018","2018","6","","24048","24063","We argue that the success of the Internet of Things (IoT) vision will greatly depend on how its main ingredient-the “thing”-is architected and prepared to engage. The IoT's fragmented and widevarying nature introduces the need for additional effort to homogenize these things so they may blend together with the surrounding space to create opportunities for powerful and unprecedented IoT applications. We introduce the IoT Device Description Language (IoT-DDL), a machineand human-readable descriptive language for things, seeking to achieve such integration and homogenization. IoT-DDL explicitly tools things to self-discover and securely share their own capabilities, entities, and services, including the various cloudbased accessories that may be attached to them. We also present the Atlas thing architecture-a lightweight architecture for things that fully exploits IoT-DDL and its specifications. Our architecture provides new OS layers, services, and capabilities we believe a thing must have in order to be prepared to engage in IoT scenarios and applications. The architecture and IoT-DDL enable things to generate their offered services and self-formulate APIs for such services, on the fly, at power-on or whenever a thing description changes. The architecture takes advantage of widely used device management, micro-services, security, and communication standards and protocols. We present details of IoT-DDL and corresponding parts of the thing architecture. We demonstrate some features of IoT-DDL and the architecture through proof-of-concept implementations. Finally, we present a benchmarking study to measure and assess time performance and energy consumption characteristics of our architecture and IoT-DDL on real hardware platforms.","2169-3536","","10.1109/ACCESS.2018.2825295","Philips; Basic Science Research Program through the National Research Foundation of Korea; Ministry of Science; ICT(grant numbers:2017R1A2B4010395); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8334820","Internet of Things architecture;thing description;microservices;OMA;IPSO;CoAP;MQTT","Computer architecture;Cloud computing;Internet of Things;Tools;Ecosystems;Metadata","","29","","52","OAPA","10 Apr 2018","","","IEEE","IEEE Journals"
"An Autonomic Workload Prediction and Resource Allocation Framework for Fog-Enabled Industrial IoT","M. Kumar; A. Kishor; J. K. Samariya; A. Y. Zomaya","Department of Information Technology, NIT Jalandhar, Jalandhar, India; Department of Computer Science and Engineering, ABV-Indian Institute of Information Technology and Management Gwalior, Gwalior, India; Department of Computer Science and Engineering, Graphic Era University Dehradun, Dehradun, India; Centre for Distributed and High-Performance Computing, University of Sydney, Sydney, NSW, Australia",IEEE Internet of Things Journal,"17 May 2023","2023","10","11","9513","9522","The Internet of Things (IoT) has revolutionized the industrial field with numerous facilities and advancements. The industrial IoT system demands delay-aware workload execution with the aid of a fog computing platform, and precise resource allocation is required in fog nodes (FNs) to execute the fluctuating industrial IoT workloads with minimal cost and delay. In view of the issue mentioned above, we introduce an autonomic workload prediction and resource allocation framework that efficiently allocates resources among FNs. In the proposed framework, the workloads are predicted in the analysis phase with the guidance of the deep autoencoder (DAE) model, and the FNs are scaled based on the demand of Industrial IoT workloads. The crow search algorithm (CSA) is integrated with the framework for optimal FN selection to improve cost and delay objectives. The proposed scheme is evaluated and compared with the existing optimization models in terms of execution cost, request rejection ratio, throughput, and response time. The simulation results establish that the proposed scheme outperformed other optimization models. The method provided a suitable solution for the optimal FN placement problems in efficiently executing dynamic industrial IoT workloads.","2327-4662","","10.1109/JIOT.2023.3235107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10012055","Delay;execution time;fog node (FN);resource allocation;workload prediction","Task analysis;Industrial Internet of Things;Resource management;Delays;Servers;Computational modeling;Cloud computing","","29","","24","IEEE","9 Jan 2023","","","IEEE","IEEE Journals"
"Congestion-Aware and Energy-Aware Virtual Network Embedding","M. Pham; D. B. Hoang; Z. Chaczko","Faculty of Engineering and IT, University of Technology Sydney, Ultimo, Australia; Faculty of Engineering and IT, University of Technology Sydney, Ultimo, Australia; Faculty of Engineering and IT, University of Technology Sydney, Ultimo, Australia",IEEE/ACM Transactions on Networking,"14 Feb 2020","2020","28","1","210","223","Network virtualization is an inherent component of future internet architectures. Network resources are virtualized from the underlying substrate and elastically provisioned and offered to customers on-demand. Optimal allocation of network resources in terms of utilization, quality of service, and energy consumption has been a challenge. Existing solutions consider congestion control in a single-objective virtual network embedding (VNE) problem. This paper defines a multiple-objective VNE problem called the congestion-aware, energy-aware VNE (CEVNE). The aim is to seek a solution that saves cost, saves energy and avoids network congestion simultaneously. CEVNE modelling techniques and solution approaches apply both the weighting method and the constraint method to search for pareto-optimal solutions that produce the best compromised solutions for all three objectives. Solving VNE problem is, however, NP-hard. A heuristic solution is proposed involving a two-stage coordinated CEVNE. The node-mapping algorithm searches for the sub-optimal solutions for three objectives. The link mapping process is an SDN-based heuristic algorithm that deploys a path service and a resource monitoring application on an SDN controller. The solution is realized using SDN, Segment Routing, and open network operating system platform (ONOS) technologies. The energy minimization is implemented with a registry that keeps track of active nodes and sets inactive nodes to sleep mode. The evaluation results showed that the multiple-objective CEVNE approach is feasible and achieves its goals of optimizing the resource allocation, improving the runtime, saving the energy consumption and controlling the network congestion.","1558-2566","","10.1109/TNET.2019.2958367","Australian Government Research Training Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8945162","Virtual network embedding;network virtualization;software-defined networks;segment routing;energy saving;congestion control;path services;multiple-objective optimization","Substrates;Virtualization;Optimization;Bandwidth;Energy consumption;Routing;Resource management","","29","","39","IEEE","30 Dec 2019","","","IEEE","IEEE Journals"
"Self-Learning Multi-Objective Service Coordination Using Deep Reinforcement Learning","S. Schneider; R. Khalili; A. Manzoor; H. Qarawlus; R. Schellenberg; H. Karl; A. Hecker","Department for Computer Science, Computer Networks Research Group, Paderborn University, Paderborn, Germany; Munich Research Center, Huawei Technologies Düsseldorf, Munich, Germany; Department for Computer Science, Computer Networks Research Group, Paderborn University, Paderborn, Germany; Logistics Department, Fraunhofer ISST, Dortmund, Germany; Department for Computer Science, Computer Networks Research Group, Paderborn University, Paderborn, Germany; Department for Computer Science, Computer Networks Research Group, Paderborn University, Paderborn, Germany; Department for Computer Science, Computer Networks Research Group, Paderborn University, Paderborn, Germany",IEEE Transactions on Network and Service Management,"8 Sep 2021","2021","18","3","3829","3842","Modern services consist of interconnected components, e.g., microservices in a service mesh or machine learning functions in a pipeline. These services can scale and run across multiple network nodes on demand. To process incoming traffic, service components have to be instantiated and traffic assigned to these instances, taking capacities, changing demands, and Quality of Service (QoS) requirements into account. This challenge is usually solved with custom approaches designed by experts. While this typically works well for the considered scenario, the models often rely on unrealistic assumptions or on knowledge that is not available in practice (e.g., a priori knowledge). We propose DeepCoord, a novel deep reinforcement learning approach that learns how to best coordinate services and is geared towards realistic assumptions. It interacts with the network and relies on available, possibly delayed monitoring information. Rather than defining a complex model or an algorithm on how to achieve an objective, our model-free approach adapts to various objectives and traffic patterns. An agent is trained offline without expert knowledge and then applied online with minimal overhead. Compared to a state-of-the-art heuristic, DeepCoord significantly improves flow throughput (up to 76%) and overall network utility (more than 2x) on real-world network topologies and traffic traces. It also supports optimizing multiple, possibly competing objectives, learns to respect QoS requirements, generalizes to scenarios with unseen, stochastic traffic, and scales to large real-world networks. For reproducibility and reuse, our code is publicly available.","1932-4537","","10.1109/TNSM.2021.3076503","German Research Foundation within the Collaborative Research Centre “On-The-Fly Computing” (SFB 901), the German Federal Ministry of Education and Research under Software Campus(grant numbers:01IS17046 (RealVNF)); European Commission under 5G-PPP project FUDGE-5G(grant numbers:H2020-ICT-42-2020,957242); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9419067","Network and service management;reinforcement learning;self-learning;self-adaptation;multi-objective","Quality of service;Monitoring;Runtime;Network topology;Edge computing;Dynamic scheduling;Delays","","29","","68","IEEE","29 Apr 2021","","","IEEE","IEEE Journals"
"VNF and CNF Placement in 5G: Recent Advances and Future Trends","W. Attaoui; E. Sabir; H. Elbiaze; M. Guizani","Intelligent Network and Mobile Charging Department, INWI Corporate, Casablanca, Morocco; Department of Computer Science, University of Quebec at Montreal, Montreal, Canada; Department of Computer Science, University of Quebec at Montreal, Montreal, Canada; Machine Learning Department, Mohamed Bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE",IEEE Transactions on Network and Service Management,"12 Dec 2023","2023","20","4","4698","4733","With the growing demand for openness, scalability, and granularity, mobile network function virtualization (NFV) has emerged as a key enabler for the most of mobile network operators. NFV decouples network functions from hardware devices. This decoupling allows network services, called Virtualized Network Functions (VNFs), to be hosted on commodity hardware which simplifies and enhances service deployment and management for providers, improves flexibility, and leads to efficient and scalable resource usage, and lower costs. The proper placement of VNFs in the hosting infrastructures is one of the main technical challenges. This placement significantly influences the network’s performance, reliability, and operating costs. The VNF placement is NP-Hard. Therefore, there is a need for placement methods that can cope with the complexity of the problem and find appropriate solutions in a reasonable duration. The primary purpose of this study is to provide a taxonomy of optimization techniques used to tackle the VNF placement problems. We classify the studied papers based on performance metrics, methods, algorithms, and environment. Virtualization is not limited to simply replacing physical machines with virtual machines or VNFs, but may also include micro-services, containers, and cloud-native systems. In this context, the second part of our article focuses on the placement of Containers Network Functions (CNFs) in edge/fog computing. Many issues have been considered as traffic congestion, resource utilization, energy consumption, performance degradation, etc. For each matter, various solutions are proposed through different surveys and research papers in which each one addresses the placement problem in a specific manner by suggesting single objective or multi-objective methods based on different types of algorithms such as heuristic, meta-heuristic, and machine learning algorithms.","1932-4537","","10.1109/TNSM.2023.3264005","Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE(grant numbers:8481000021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10090468","Virtual network function;container;placement;5G network slicing;cloud native","5G mobile communication;Containers;Cloud computing;Virtualization;Network slicing;Internet of Things;Quality of service","","29","","165","CCBY","31 Mar 2023","","","IEEE","IEEE Journals"
"Microservice Based Computational Offloading Framework and Cost Efficient Task Scheduling Algorithm in Heterogeneous Fog Cloud Network","X. Zhao; C. Huang","School of Digital Media, Shenzhen Institute of Information Technology, Shenzhen, China; Department of Computer Science, Wenzhou University, Wenzhou, China",IEEE Access,"31 Mar 2020","2020","8","","56680","56694","Nowadays, the usage of the fog cloud-based Internet of Things (IoT) applications among users has been growing progressively. These applications may be E-Transport, E-Healthcare, Augmented Reality, and 3D-Game. Generally, contemporary cloud frameworks offer services based on virtual machines. These frameworks incurred with following issues such as long boot-time, overhead and unnecessary cost to run the IoT applications. We propose a new Microservice container fog system (MSCFS) based framework to run the mobility and delay-sensitive applications with minimum cost. The study the cost-efficient task scheduling problem in the heterogeneous fog servers. Furthermore, the study introduces the Cost Aware Computational Offloading and task scheduling (CACOTS)framework, which solves the task scheduling into multiple steps. Such as task sequencing step, resource matching step and scheduling step. The experimental results prove that the proposed MSCFS and CACOTS schemes can enhance server utilization. As decrease the services latency and average services bootup time more effectively, and minimize costs.","2169-3536","","10.1109/ACCESS.2020.2981860","Natural Science Foundation of Guangdong Province(grant numbers:2018A030313339); Ministry of Education in China (MOE) Youth Fund Project of Humanities and Social Sciences(grant numbers:17YJCZH261); Scientific Research Team Project of Shenzhen Institute of Information Technology(grant numbers:SZIIT2019KJ022); National Natural Science Foundation of China(grant numbers:71803136,61471133); Key Research and Development Program Projects in Zhejiang Province(grant numbers:2019C01041); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9042334","Task scheduling;fog servers;boot-up time;microservices;container","Task analysis;Cloud computing;Processor scheduling;Servers;Internet of Things;Virtual machining;Containers","","29","","27","CCBY","19 Mar 2020","","","IEEE","IEEE Journals"
"The Evolution of Federated Learning-Based Intrusion Detection and Mitigation: A Survey","L. Lavaur; M. -O. Pahl; Y. Busnel; F. Autrel","IMT Atlantique and the Chair Cybersecurity of Critical Networked Infrastructures (Cyber CNI), Rennes, France; IMT Atlantique and the Chair Cybersecurity of Critical Networked Infrastructures (Cyber CNI), Rennes, France; Department of Network Systems, Cybersecurity and Digital Law (SRCD), IMT Atlantique, IRISA, Rennes, France; Department of Network Systems, Cybersecurity and Digital Law (SRCD), IMT Atlantique, IRISA, Rennes, France",IEEE Transactions on Network and Service Management,"12 Oct 2022","2022","19","3","2309","2332","In 2016, Google introduced the concept of Federated Learning (FL), enabling collaborative Machine Learning (ML). FL does not share local data but ML models, offering applications in diverse domains. This paper focuses on the application of FL to Intrusion Detection Systems (IDSs). There, common criteria to compare existing solutions are missing. In particular, this survey shows: (i) how FL-based IDSs are used in different domains; (ii) what differences exist between architectures; (iii) the state of the art of FL-based IDS. With a structured literature survey, this work identifies the relevant state of the art in FL–based intrusion detection from its creation in 2016 until 2021. It provides a reference architecture and a taxonomy to serve as guidelines to compare and design FL-based IDSs. Both are validated with the existing works. Finally, it identifies research directions for the application of FL to intrusion detection systems.","1932-4537","","10.1109/TNSM.2022.3177512","chair CyberCNI.fr; FEDER development fund of the Brittany region; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9780571","Federated learning;machine learning;intrusion detection systems;collaborative sharing;network security management;attack mitigation","Collaboration;Taxonomy;Statistical analysis;Organizations;Collaborative work;Standardization;Semantics","","29","","118","IEEE","24 May 2022","","","IEEE","IEEE Journals"
"iBrownout: An Integrated Approach for Managing Energy and Brownout in Container-Based Clouds","M. Xu; A. N. Toosi; R. Buyya","Cloud Computing and Distributed Systems (CLOUDS) Lab, University of Melbourne, Parkville, VIC, Australia; Cloud Computing and Distributed Systems (CLOUDS) Lab, University of Melbourne, Parkville, VIC, Australia; Cloud Computing and Distributed Systems (CLOUDS) Lab, University of Melbourne, Parkville, VIC, Australia",IEEE Transactions on Sustainable Computing,"7 Mar 2019","2019","4","1","53","66","Energy consumption of Cloud data centers has been a major concern of many researchers, and one of the reasons for huge energy consumption of Clouds lies in the inefficient utilization of computing resources. Besides energy consumption, another challenge of data centers is the unexpected loads, which leads to the overloads and performance degradation. Compared with VM consolidation and Dynamic Voltage Frequency Scaling that cannot function well when the whole data center is overloaded, brownout has shown to be a promising technique to handle both overloads and energy consumption through dynamically deactivating application optional components, which are also identified as containers/microservices. In this work, we propose an integrated approach to manage energy consumption and brownout in container-based cloud data centers. We also evaluate our proposed scheduling policies with real traces in a prototype system. The results show that our approach reduces about 40, 20, and 10 percent energy than the approach without power-saving techniques, brownout-overbooking approach and auto-scaling approach, respectively, while ensuring Quality of Service.","2377-3782","","10.1109/TSUSC.2018.2808493","China Scholarship Council; Australia Research Council Future Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8302602","Cloud data centers;energy efficiency;QoS;containers;microservices;brownout","Data centers;Energy consumption;Cloud computing;Quality of service;Servers;Containers;Prototypes","","29","","36","IEEE","26 Feb 2018","","","IEEE","IEEE Journals"
"NFV Platforms: Taxonomy, Design Choices and Future Challenges","T. Zhang; H. Qiu; L. Linguaglossa; W. Cerroni; P. Giaccone","Department of Network and Computer Science, Telecom Paris, Paris, France; Department of Network and Computer Science, Telecom Paris, Paris, France; Department of Network and Computer Science, Telecom Paris, Paris, France; Department of Electrical, Electronic, and Information Engineering “Guglielmo Marconi,”, University of Bologna, Bologna, Italy; Consorzio Nazionale Interuniversitario per le Telecomunicazioni, Parma, Italy",IEEE Transactions on Network and Service Management,"10 Mar 2021","2021","18","1","30","48","Due to the intrinsically inefficient service provisioning in traditional networks, Network Function Virtualization (NFV) keeps gaining attention from both industry and academia. By replacing the purpose-built, expensive, proprietary network equipment with software network functions consolidated on commodity hardware, NFV envisions a shift towards a more agile and open service provisioning paradigm. During the last few years, a large number of NFV platforms have been implemented to facilitate the development, deployment, and management of Virtual Network Functions (VNFs). Nonetheless, just like any complex system, such platforms commonly consist of abounding software and hardware components and usually incorporate disparate design choices based on distinct motivations or use cases. This broad collection of convoluted alternatives makes it extremely arduous for network operators to make proper choices. Although numerous efforts have been devoted to investigating different aspects of NFV, none of them specifically focused on NFV platforms or attempted to explore their design space. In this article, we present a comprehensive survey on the NFV platform design. Our study solely targets existing NFV platform implementations. We begin with a top-down architectural view of the standard reference NFV platform and present our taxonomy of existing NFV platforms based on what features they provide in terms of a typical network function life cycle. Then we thoroughly explore the design space and elaborate on the implementation choices each platform opts for. We also envision future challenges for NFV platform design in the incoming 5G era. We believe that our study gives a detailed guideline for network operators or service providers to choose the most appropriate NFV platform based on their respective requirements. Our work also provides guidelines for implementing new NFV platforms.","1932-4537","","10.1109/TNSM.2020.3045381","European Horizon 2020 Programme through the project 5G-EVE on “European 5G validation platform for extensive trials”(grant numbers:815074); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9302614","Network function virtualization;service function chaining;service management and orchestration;NFV infrastructure;VNF life cycle","Hardware;Software;Middleboxes;Space exploration;Monitoring;Switches;Virtualization","","29","","162","IEEE","22 Dec 2020","","","IEEE","IEEE Journals"
"A Survey on Blockchain-Based Telecommunication Services Marketplaces","R. -V. Tkachuk; D. Ilie; K. Tutschku; R. Robert","Department of Computer Science, Blekinge Institute of Technology, Karlshamn, Sweden; Department of Computer Science, Blekinge Institute of Technology, Karlshamn, Sweden; Department of Computer Science, Blekinge Institute of Technology, Karlshamn, Sweden; Ericsson Research, Stockholm, Sweden",IEEE Transactions on Network and Service Management,"10 Mar 2022","2022","19","1","228","255","Digital marketplaces were created recently to accelerate the delivery of applications and services to customers. Their appealing feature is to activate and dynamize the demand, supply, and development of digital goods, applications, or services. By being an intermediary between producer and consumer, the primary business model for a marketplace is to charge the producer with a commission on the amount paid by the consumer. However, most of the time, the commission is dictated by the marketplace facilitator itself and creates an imbalance in value distribution, where producer and consumer sides suffer monetarily. In order to eliminate the need for a centralized entity between the producer and consumer, a blockchain-based decentralized digital marketplace concept was introduced. It provides marketplace actors with the tools to perform business transactions in a trusted manner and without the need for an intermediary. In this work, we provide a survey on Telecommunication Services Marketplaces (TSMs) which employ blockchain technology as the main trust enabling entity in order to avoid any intermediaries. We provide an overview of scientific and industrial proposals on the blockchain-based online digital marketplaces at large, and TSMs in particular. We consider in this study the notion of telecommunication services as any service enabling the capability for information transfer and, increasingly, information processing provided to a group of users by a telecommunications system. We discuss the main standardization activities around the concepts of TSMs and provide particular use-cases for the TSM business transactions such as SLA settlement. Also, we provide insights into the main foundational services provided by the TSM, as well as a survey of the scientific and industrial proposals for such services. Finally, a prospect for future developments is given.","1932-4537","","10.1109/TNSM.2021.3123680","Swedish Knowledge Foundation through the project Symphony - Supply-and-Demand-based Service Exposure using Robust Distributed Concepts; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9592675","Digital marketplace;telecommunication services marketplace;blockchain technology;communication service provider;distributed ledger technology","Blockchains;Business;Distributed ledger;Communications technology;Proposals;Internet of Things;Telecommunication services","","28","","171","CCBYNCND","28 Oct 2021","","","IEEE","IEEE Journals"
"Smart Mobility in the Cloud: Enabling Real-Time Situational Awareness and Cyber-Physical Control Through a Digital Twin for Traffic","H. Xu; A. Berres; S. B. Yoginath; H. Sorensen; P. J. Nugent; J. Severino; S. A. Tennille; A. Moore; W. Jones; J. Sanyal","Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; National Renewable Energy Laboratory, Golden, CO, USA; Descartes Labs, Santa Fe, NM, USA; National Renewable Energy Laboratory, Golden, CO, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Y-12 National Security Complex, Oak Ridge, TN, USA; National Renewable Energy Laboratory, Golden, CO, USA; National Renewable Energy Laboratory, Golden, CO, USA",IEEE Transactions on Intelligent Transportation Systems,"1 Mar 2023","2023","24","3","3145","3156","This article presents the design, implementation, and use cases of the Chattanooga Digital Twin (CTwin) towards the vision for next-generation smart city applications for urban mobility management. CTwin is an end-to-end web-based platform that incorporates various aspects of the decision-making process for optimizing urban transportation systems in Chattanooga, Tennessee, to reduce traffic congestion, incidents, and vehicle fuel consumption. The platform serves as a cyberinfrastructure to collect and integrate multi-domain urban mobility data from various online repositories and Internet of Things (IoT) sensors, covering multiple urban aspects (e.g., traffic, natural hazards, weather, and safety) that are relevant to urban mobility management. The platform enables advanced capabilities for: (a) real-time situational awareness on traffic and infrastructure conditions on highways and urban roads, (b) cyber-physical control for optimizing traffic signal timing, and (c) interactive visual analytics on big urban mobility data and various metrics for traffic prediction and transportation performance evaluation. The platform is designed using a multi-level componentization paradigm and is implemented using modular and adaptive architecture, rendering it as a generalizable and extendable prototype for other urban management applications. We present several use cases to demonstrate CTwin’s core capabilities for supporting decision-making in smart urban mobility management.","1558-0016","","10.1109/TITS.2022.3226746","United States Department of Energy (U.S. DOE) through UT-Battelle, LLC.(grant numbers:DE-AC05-00OR22725); U.S. DOE, Office of Energy Efficiency and Renewable Energy, Vehicle Technologies Office; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10018121","Traffic flow visualization;level of detail;situational awareness;traffic sensor network;urban mobility;traffic monitoring","Digital twins;Real-time systems;Transportation;Urban areas;Measurement;Analytical models;Sensors","","28","","44","IEEE","16 Jan 2023","","","IEEE","IEEE Journals"
"Aveiro Tech City Living Lab: A Communication, Sensing, and Computing Platform for City Environments","P. Rito; A. Almeida; A. Figueiredo; C. Gomes; P. Teixeira; R. Rosmaninho; R. Lopes; D. Dias; G. Vítor; G. Perna; M. Silva; C. Senna; D. Raposo; M. Luís; S. Sargento; A. Oliveira; N. B. de Carvalho","Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal",IEEE Internet of Things Journal,"25 Jul 2023","2023","10","15","13489","13510","This article presents the deployment and experimentation architecture of the Aveiro Tech City Living Lab (ATCLL) in Aveiro, Portugal. This platform comprises a large number of Internet of Things (IoT) devices with communication, sensing, and computing capabilities. The communication infrastructure, built on fiber and millimeter-wave (mmWave) links, integrates a communication network with radio terminals [WiFi, ITS-G5, cellular vehicular-to-everything, 5G and LoRa(WAN)], multiprotocol, spread throughout 44 connected points of access in the city. Additionally, public transportation has also been equipped with communication and sensing units. All these points combine and interconnect a set of sensors, such as mobility (radars, light detection and rangings (LiDARs), and video cameras) and environmental sensors. Combining edge computing and cloud management to deploy the services and manage the platform, and a data platform to gather and process the data, the living lab supports a wide range of services and applications: IoT, intelligent transport systems (ITSs) and assisted driving, environmental monitoring, emergency and safety, and among others. This article describes the architecture, implementation, and deployment to make the overall platform to work and integrate researchers and citizens. Moreover, it showcases some examples of the performance metrics achieved in the city infrastructure, the data that can be collected, visualized, and used to build services and applications to the cities, and, finally, different use cases in the mobility and safety scenarios.","2327-4662","","10.1109/JIOT.2023.3262627","European Regional Development Fund (ERDF), included in the “Urban Innovative Actions” Programme, through Project Aveiro STEAM City(grant numbers:UIA03-084); FCT/MEC through National Funds(grant numbers:PTDC/EEI-COM/5284/2020); FCT/MCTES through National Funds and when applicable co-funded EU funds(grant numbers:UIDB/50008/2020-UIDP/50008/2020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10083171","Connectivity management;smart cities;software-defined networks;test-bed and trials;vehicular networks","Sensors;Smart cities;Internet of Things;Wireless fidelity;Safety;Temperature sensors;Computer architecture","","28","","49","IEEE","28 Mar 2023","","","IEEE","IEEE Journals"
"Empowering the 6G Cellular Architecture With Open RAN","M. Polese; M. Dohler; F. Dressler; M. Erol-Kantarci; R. Jana; R. Knopp; T. Melodia","Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Advanced Technology Group, Ericsson Inc., Santa Clara, CA, USA; School of Electrical Engineering and Computer Science, TU Berlin, Berlin, Germany; School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Canada; Google, New York City, NY, USA; EURECOM, Sophia Antipolis, Biot, France; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA",IEEE Journal on Selected Areas in Communications,"16 Jan 2024","2024","42","2","245","262","Innovation and standardization in 5G have brought advancements to every facet of the cellular architecture. This ranges from the introduction of new frequency bands and signaling technologies for the radio access network (RAN), to a core network underpinned by micro-services and network function virtualization (NFV). However, like any emerging technology, the pace of real-world deployments does not instantly match the pace of innovation. To address this discrepancy, one of the key aspects under continuous development is the RAN with the aim of making it more open, adaptive, functional, and easy to manage. In this paper, we highlight the transformative potential of embracing novel cellular architectures by transitioning from conventional systems to the progressive principles of Open RAN. This promises to make 6G networks more agile, cost-effective, energy-efficient, and resilient. It opens up a plethora of novel use cases, ranging from ubiquitous support for autonomous devices to cost-effective expansions in regions previously underserved. The principles of Open RAN encompass: (i) a disaggregated architecture with modular and standardized interfaces; (ii) cloudification, programmability and orchestration; and (iii) AI-enabled data-centric closed-loop control and automation. We first discuss the transformative role Open RAN principles have played in the 5G era. Then, we adopt a system-level approach and describe how these Open RAN principles will support 6G RAN and architecture innovation. We qualitatively discuss potential performance gains that Open RAN principles yield for specific 6G use cases. For each principle, we outline the steps that research, development and standardization communities ought to take to make Open RAN principles central to next-generation cellular network designs.","1558-0008","","10.1109/JSAC.2023.3334610","U.S. National Science Foundation(grant numbers:CNS-2117814); Federal Ministry of Education and Research (BMBF), Germany, within the 6G Platform(grant numbers:16KISK050); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10329947","Open RAN;6G;architecture;end-to-end","5G mobile communication;Cellular networks;6G mobile communication;Wireless communication;Technological innovation;3GPP;Quality of service","","28","","153","IEEE","28 Nov 2023","","","IEEE","IEEE Journals"
"Toward an Open, Intelligent, and End-to-End Architectural Framework for Network Slicing in 6G Communication Systems","M. A. Habibi; B. Han; A. Fellan; W. Jiang; A. G. Sánchez; I. L. Pavon; A. Boubendir; H. D. Schotten","Department of Electrical and Computer Engineering, Division of Wireless Communications and Radio Navigation, Rheinland-Pfälzische Technische Universität, Kaiserslautern, Germany; Department of Electrical and Computer Engineering, Division of Wireless Communications and Radio Navigation, Rheinland-Pfälzische Technische Universität, Kaiserslautern, Germany; Department of Electrical and Computer Engineering, Division of Wireless Communications and Radio Navigation, Rheinland-Pfälzische Technische Universität, Kaiserslautern, Germany; Intelligent Networking Research Group, German Research Center for Artificial Intelligence, Kaiserslautern, Germany; Research and Innovation Department, ATOS Spain, Madrid, Spain; Research and Innovation Department, ATOS Spain, Madrid, Spain; Network Architecture and Automation Department, Orange Labs Networks, Issy-les-Moulineaux, France; Department of Electrical and Computer Engineering, Division of Wireless Communications and Radio Navigation, Rheinland-Pfälzische Technische Universität, Kaiserslautern, Germany",IEEE Open Journal of the Communications Society,"4 Aug 2023","2023","4","","1615","1658","The definition of the fundamental concepts and the design of the architectural framework for network slicing in fifth-generation communication systems have been successfully concluded; the standardization activities are almost over; and the commercial deployment has already commenced worldwide. To compete for digital supremacy and to be seen as front-runners in the international technological race, researchers from various regions and countries have begun exploring the technical requirements, envisioning potential applications, identifying innovative enablers, developing testbeds for the preliminary validation of several terrestrial and non-terrestrial technologies, and conceptualizing the architectural design for the next generation of mobile communication systems – the sixth-generation (6G) – aiming to connect the human, physical, and digital worlds with a high level of intelligence and openness for the 2030s. In support of such an ambitious vision, this article extends the end-to-end network slicing concepts, methods, solutions, and functioning architectures towards 6G. To this intent, the study first presents several decisive motivating trends behind such an extension of network slicing in order to make forthcoming mobile networks fully slicing-aware. Following that, the paper attempts to highlight the intelligentization of a number of key enabling technologies that will bring a renaissance to network slicing in the next decade. It then proposes a unified architectural framework and its principal building blocks in several layers, paving the way for the implementation of an open and intelligent network and network slicing in 6G. The proposed architectural solution harmonizes the most recent specifications of the relevant de jure and de facto standards development organizations in their applicable layers with the aim of architecting a pre-standard-compliant and preliminary framework for slicing the 6G network. Finally, the article is intended to spur interest and lay the groundwork for further investigations and subsequent research and development by highlighting a number of open research challenges and directions in this flourishing field.","2644-125X","","10.1109/OJCOMS.2023.3294445","Horizon 2020 Research and Innovation Program of the European Union through Hexa-X(grant numbers:101015956); Hexa-X-II(grant numbers:101095759); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10178010","6G;automation;cloudification;end-to-end architecture;intelligentization;intelligent networks;intelligent network slice;management and orchestration;mobile communication systems;network architecture;network exposure;network slicing;network slice;openness;open network;open network slice;softwarization;standardization;standards;virtualization","6G mobile communication;Network slicing;Computer architecture;Automation;Optimization;Industries;Next generation networking","","28","","145","CCBY","11 Jul 2023","","","IEEE","IEEE Journals"
"A Cost and Energy Efficient Task Scheduling Technique to Offload Microservices Based Applications in Mobile Cloud Computing","A. Ali; M. M. Iqbal","Department of Computer Science, University of Engineering and Technology, Taxila, Taxila, Pakistan; Department of Computer Science, Government Akhtar Nawaz Khan (Shaheed) Degree College KTS, Haripur, Pakistan",IEEE Access,"5 May 2022","2022","10","","46633","46651","The number of smartphone users and mobile devices has increased significantly. The Mobile Cloud Applications based on cloud computing have also been increased. The mobile apps can be used in Augmented Reality, E-Transportation, 2D/3-D Games, E-Healthcare, and Education. The modern cloud-based frameworks provide such services on Virtual Machines. The existing frameworks worked well, but these suffered the problems such as overhead, resource utilization, lengthy boot-time, and cost of running Mobile Applications. This study addresses these problems by proposing a Dynamic Decision-Based Task Scheduling Technique for Microservice-based Mobile Cloud Computing Applications (MSCMCC). The MSCMCC runs delay-sensitive applications and mobility with less cost than existing approaches. The study focused on Task Scheduling problems on heterogeneous Mobile Cloud servers. We further propose Task Scheduling and Microservices based Computational Offloading (TSMCO) framework to solve the Task Scheduling in steps, such as Resource Matching, Task Sequencing, and Task Scheduling. Furthermore, the experimental results elaborate that the proposed MSCMCC and TSMCO enhance the Mobile Server Utilization. The proposed system effectively minimizes the cost of healthcare applications by 25%, augmented reality by 23%, E-Transport tasks by 21%, and 3-D games tasks by 19%, the average boot-time of microservices applications by 17%, resource utilization by 36%, and tasks arrival time by 16%.","2169-3536","","10.1109/ACCESS.2022.3170918","University of Engineering and Technology, Taxila, Pakistan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9764765","Cloud computing;mobile cloud computing;task offloading;task sequencing;task scheduling;microservices","Task analysis;Cloud computing;Costs;Processor scheduling;Microservice architectures;Mobile handsets;Servers","","28","","52","CCBY","28 Apr 2022","","","IEEE","IEEE Journals"
"Study on the Reference Architecture and Assessment Framework of Industrial Internet Platform","J. Li; J. -J. Qiu; Y. Zhou; S. Wen; K. -Q. Dou; Q. Li","Department of Automation, Tsinghua University, Beijing, China; China Industrial Control Systems Cyber Emergency Response Team, Beijing, China; China Industrial Control Systems Cyber Emergency Response Team, Beijing, China; China Industrial Control Systems Cyber Emergency Response Team, Beijing, China; China Industrial Control Systems Cyber Emergency Response Team, Beijing, China; Department of Automation, Tsinghua University, Beijing, China",IEEE Access,"17 Sep 2020","2020","8","","164950","164971","With the continuous innovation of new-generation information technology and its accelerated integration with manufacturing industry, industrial internet platforms (IIPs) are rapidly emerging worldwide. Construction and application of IIP has become a new focus in international competition for leading enterprises, and also a new direction of industrial development for many countries worldwide. However, the development of IIP is still in the stage of exploration, and the industry sector still lacks unified understanding of the IIP. Therefore, this study firstly proposes a reference architecture of IIP to clarify its framework and core functions, so as to provide a general reference model for the industry to understand and jointly promote construction of IIP. Secondly, an assessment system is proposed to evaluate the usage of IIP. The assessment framework is composed from three domains namely the foundation, key capability, value and benefit. Finally, the practical value of the reference architecture and the assessment framework of IIP is verified by an industry practice.","2169-3536","","10.1109/ACCESS.2020.3021719","National Natural Science Foundation of China(grant numbers:61771281); National Social Science Foundation of China(grant numbers:15ZDB151); National Basic Research Program of China (973 Program)(grant numbers:2018YFF0213700); 2018 Industrial Internet Platform Innovation and Development Project of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9186625","Industrial Internet platform (IIP);reference architecture;assessment framework;assessment indictor","Industries;Computer architecture;Information systems;Manufacturing;Internet of Things;Cloud computing","","28","","61","CCBY","4 Sep 2020","","","IEEE","IEEE Journals"
"Software-Defined “Hardware” Infrastructures: A Survey on Enabling Technologies and Open Research Directions","A. Roozbeh; J. Soares; G. Q. Maguire; F. Wuhib; C. Padala; M. Mahloo; D. Turull; V. Yadhav; D. Kostić","Ericsson Research, Ericsson AB, Stockholm, Sweden; Ericsson Research, Ericsson AB, Stockholm, Sweden; Communication Systems Department, KTH Royal Institute of Technology, Stockholm, Sweden; Ericsson Research, Ericsson AB, Stockholm, Sweden; Ericsson Research, Ericsson AB, Stockholm, Sweden; Ericsson Research, Ericsson AB, Stockholm, Sweden; Ericsson Research, Ericsson AB, Stockholm, Sweden; Ericsson Research, Ericsson AB, Stockholm, Sweden; Communication Systems Department, KTH Royal Institute of Technology, Stockholm, Sweden",IEEE Communications Surveys & Tutorials,"21 Aug 2018","2018","20","3","2454","2485","This paper provides an overview of software-defined “hardware” infrastructures (SDHI). SDHI builds upon the concept of hardware (HW) resource disaggregation. HW resource disaggregation breaks today's physical server-oriented model where the use of a physical resource (e.g., processor or memory) is constrained to a physical server's chassis. SDHI extends the definition of of software-defined infrastructures (SDI) and brings greater modularity, flexibility, and extensibility to cloud infrastructures, thus allowing cloud operators to employ resources more efficiently and allowing applications not to be bounded by the physical infrastructure's layout. This paper aims to be an initial introduction to SDHI and its associated technological advancements. This paper starts with an overview of the cloud domain and puts into perspective some of the most prominent efforts in the area. Then, it presents a set of differentiating use-cases that SDHI enables. Next, we state the fundamentals behind SDI and SDHI, and elaborate why SDHI is of great interest today. Moreover, it provides an overview of the functional architecture of a cloud built on SDHI, exploring how the impact of this transformation goes far beyond the cloud infrastructure level in its impact on platforms, execution environments, and applications. Finally, an in-depth assessment is made of the technologies behind SDHI, the impact of these technologies, and the associated challenges and potential future directions of SDHI.","1553-877X","","10.1109/COMST.2018.2834731","Knut och Alice Wallenbergs Stiftelse; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8356736","CR-software-defined infrastructure;resource disaggregation;cloud infrastructure;rack-scale;hyperscale computing;disaggregated DC","Servers;Cloud computing;Virtualization;Hardware;Virtual machine monitors;Computer architecture","","27","","202","IEEE","9 May 2018","","","IEEE","IEEE Journals"
"Toward Energy-Awareness Smart Building: Discover the Fingerprint of Your Electrical Appliances","M. Ma; W. Lin; J. Zhang; P. Wang; Y. Zhou; X. Liang","School of Electronics Engineering and Computer Science, Peking University, Beijing, China; School of Software and Microelectronics, Peking University, Beijing, China; School of Electronics Engineering and Computer Science, Peking University, Beijing, China; National Engineering Research Center for Software Engineering and the School of Software and Microelectronics, Peking University, Key Laboratory of High Confidence Software Technologies (PKU), Ministry of Education, Beijing, China; IBM Research—China, Beijing, China; IBM Research—China, Beijing, China",IEEE Transactions on Industrial Informatics,"4 Apr 2018","2018","14","4","1458","1468","Energy efficiency raises significant concerns as it is one of the most promising ways to mitigate climate change. Disaggregation and identification of individual electrical appliances activities are one of the essentials for energy preservation especially for smart buildings. This paper proposes a lightweight electrical appliance activity detection approach for smart building, which leverages a single smart metering device to establish a learning and detection processing for multiple appliances. In this system, data interpolation and transition detection algorithm are proposed to effectively reduce the cost of model training and optimize the detection accuracy. The concept of appliance fingerprint is proposed and a variety of fingerprints, including appliance-based and context-based, are defined to depict fine-grained appliance characteristics. Based on these fingerprints, the paper proposes a multisource fingerprint-weighting KNN (FWKNN) classification algorithm and presents a boosting framework for continuous online learning and detection. A prototype system is implemented and demonstrated in IBM Bluemix PaaS cloud platform. Experimental result and analysis prove that FWKNN outperforms other benchmark methods in detection accuracy.","1941-0050","","10.1109/TII.2017.2776300","National Natural Science Foundation of China(grant numbers:61701007); National Key Research and Development Program of China(grant numbers:2017YFB1200700); China Postdoctoral Science Foundation(grant numbers:2016M600865); IBM Shared University Research Project(grant numbers:TII-17-1292.R1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8118142","Activity learning;appliance load monitoring (ALM);classification algorithm;fingerprint extraction;Internet of Things;smart building","Fingerprint recognition;Time series analysis;Electrical products;Smart buildings;Detection algorithms;Home appliances;Data models;Climate change","","27","","22","IEEE","22 Nov 2017","","","IEEE","IEEE Journals"
"Energy-Aware Service Function Chain Embedding in Edge–Cloud Environments for IoT Applications","N. H. Thanh; N. Trung Kien; N. V. Hoa; T. T. Huong; F. Wamser; T. Hossfeld","School of Electronics and Telecommunications, Hanoi University of Science and Technology, Hanoi, Vietnam; School of Electronics and Telecommunications, Hanoi University of Science and Technology, Hanoi, Vietnam; School of Electronics and Telecommunications, Hanoi University of Science and Technology, Hanoi, Vietnam; School of Electronics and Telecommunications, Hanoi University of Science and Technology, Hanoi, Vietnam; Chair of Communication Networks, University of Würzburg, Würzburg, Germany; Chair of Communication Networks, University of Würzburg, Würzburg, Germany",IEEE Internet of Things Journal,"23 Aug 2021","2021","8","17","13465","13486","The implementation of Internet-of-Things (IoT) applications faces several challenges in practice, such as compliance with Quality-of-Service requirements, resource constraints, and energy consumption. In this context, the joint edge–cloud paradigm for IoT applications can resolve some of the issues arising in pure cloud computing scenarios, such as those related to latency, energy, or privacy. Therefore, an edge–cloud environment could be promising for resource and energy-efficient IoT applications that implement virtual network functions (VNFs) bound together into service function chains (SFCs). However, a resource and energy-efficient SFC placement requires smart SFC embedding mechanisms in the edge–cloud environment, as several challenges arise, such as IoT service chain modeling and evaluation, the tradeoff between resource allocation, energy efficiency and performance, and the resource dynamics. In this article, we address issues in modeling resource and energy utilization for IoT applications in edge–cloud environments. A smart traffic monitoring IP camera system is deployed as a use case for a realistic modeling of a service chain. The system is implemented in our testbed, which is designed and developed specifically to model and investigate the resource and energy utilization of SFC embedding strategies. A resource and energy-aware SFC strategy in the edge–cloud environment for IoT applications is then proposed. Our algorithm is able to cope with dynamic load and resource situations emerging from dynamic SFC requests. The strategy is evaluated systematically in terms of the acceptance ratio of SFC requests, resource efficiency and utilization, power consumption, and VNF migrations depending on the offered system load. Results show that our strategy outperforms some existing approaches in terms of resource and energy efficiency, thus it overcomes the relevant challenges from practice and meets the demands of IoT applications.","2327-4662","","10.1109/JIOT.2021.3064986","Hanoi University of Science and Technology (HUST)(grant numbers:T2020-SAHEP-009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9373577","Edge–cloud computing;Internet-of-Things (IoT) applications;network function virtualization (NFV);resource and energy-aware service chain embedding (RE-SCE);smart city","Internet of Things;Cloud computing;Data models;Computational modeling;Computer architecture;Edge computing;Heuristic algorithms","","27","","91","IEEE","9 Mar 2021","","","IEEE","IEEE Journals"
"IECL: An Intelligent Energy Consumption Model for Cloud Manufacturing","Z. Zhou; M. Shojafar; M. Alazab; F. Li","School of Computer Engineering and Applied Mathematics, Changsha University, Changsha, China; 5G/6GIC, Institute for Communication Systems (ICS), University of Surrey, Guildford, U.K.; Charles Darwin University, Casuarina, NT, Australia; School of Computer Engineering and Applied Mathematics, Changsha University, Changsha, China",IEEE Transactions on Industrial Informatics,"30 Sep 2022","2022","18","12","8967","8976","The high computational capability provided by a data center makes it possible to solve complex manufacturing issues and carry out large-scale collaborative cloud manufacturing. Accurately, real-time estimation of the power required by a data center can help resource providers predict the total power consumption and improve resource utilization. To enhance the accuracy of server power models, we propose a real-time energy consumption prediction method called IECL that combines the support vector machine, random forest, and grid search algorithms. The random forest algorithm is used to screen the input parameters of the model, while the grid search method is used to optimize the hyperparameters. The error confidence interval is also leveraged to describe the uncertainty in the energy consumption by the server. Our experimental results suggest that the average absolute error for different workloads is less than 1.4% with benchmark models.","1941-0050","","10.1109/TII.2022.3165085","Scientific Research Project of Education Department of Hunan Province(grant numbers:21B0779); Hunan Province Key Laboratory of Industrial Internet Technology and Security(grant numbers:2019TP1011); Marie Curie Global Fellowship; European Commission(grant numbers:MSCA-IF-GF-839255); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9749900","Cloud manufacturing;data center;energy consumption prediction;power model;support vector machine (SVM)","Energy consumption;Servers;Data centers;Data models;Manufacturing;Predictive models;Feature extraction","","27","","29","IEEE","5 Apr 2022","","","IEEE","IEEE Journals"
"Comparing Apples and Oranges in IoT Context: A Deep Dive Into Methods for Comparing IoT Platforms","A. Mijuskovic; I. Ullah; R. Bemthuis; N. Meratnia; P. Havinga","EEMCS Faculty, Pervasive Systems Group, University of Twente, NB Enschede, The Netherlands; EEMCS Faculty, Pervasive Systems Group, University of Twente, NB Enschede, The Netherlands; EEMCS Faculty, Pervasive Systems Group, University of Twente, NB Enschede, The Netherlands; EEMCS Faculty, Pervasive Systems Group, University of Twente, NB Enschede, The Netherlands; EEMCS Faculty, Pervasive Systems Group, University of Twente, NB Enschede, The Netherlands",IEEE Internet of Things Journal,"22 Jan 2021","2021","8","3","1797","1816","Many researchers try to make a comparison between various Internet-of-Things (IoT) platforms based on specific requirements. However, none of the reviewed studies proposed a thorough analysis of the variety of comparative methods. Since there is a lack of comparison frameworks for IoT platforms, individuals or companies have difficulties when selecting a suitable IoT platform matching their associated business requirements. In order to support this selection process, a set of functional and nonfunctional requirements is identified. A framework containing methods in selecting an IoT platform is presented. The methodology is based on statistical and visualization techniques to recommend a suitable IoT platform. Five IoT platforms: 1) Azure; 2) AWS; 3) SaS; 4) ThingWorx; and 5) Kaa IoT are studied to evaluate the performance of the framework. Different comparison methods are proposed and a multicriteria decision analysis method was applied by using an analytical hierarchical process (AHP). One of the methods clusters the functional requirements and compares the IoT platforms based on their ability in supporting a specific requirement or not. The  $K$ -means clustering was applied to determine the clusters of functional requirements. The comparison was made based on the hierarchical level of requirements per main requirement. The other methods use the following statistical tests: error bar test, one-way Anova test, and Tukey’s honest significant difference test. Based on the selected requirements, an approach is suggested for which IoT platform can be used.","2327-4662","","10.1109/JIOT.2020.3016921","OP Oost; Project CountDown; Netherlands Organization for Scientific Research (NWO), Project DataRel(grant numbers:628.009.015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9169714","Functional requirements;Internet-of-Things (IoT) platforms;statistical comparative techniques;visualization","Internet of Things;Security;Taxonomy;Scalability;Resource management;Protocols","","27","","66","IEEE","17 Aug 2020","","","IEEE","IEEE Journals"
"Microservices Monitoring with Event Logs and Black Box Execution Tracing","M. Cinque; R. D. Corte; A. Pecchia","Dipartimento di Ingegneria Elettrica e delle Tecnologie dell'Informazione (DIETI), Università degli Studi di Napoli Federico II, via Claudio 21, Napoli, Italy; Dipartimento di Ingegneria Elettrica e delle Tecnologie dell'Informazione (DIETI), Università degli Studi di Napoli Federico II, via Claudio 21, Napoli, Italy; Dipartimento di Ingegneria Elettrica e delle Tecnologie dell'Informazione (DIETI), Università degli Studi di Napoli Federico II, via Claudio 21, Napoli, Italy",IEEE Transactions on Services Computing,"3 Feb 2022","2022","15","1","294","307","Monitoring is a core practice in any software system. Trends in microservices systems exacerbate the role of monitoring and pose novel challenges to data sources being used for monitoring, such as event logs. Current deployments create a distinct log per microservice; moreover, composing microservices by different vendors exacerbates format and semantic heterogeneity of logs. Understanding and traversing the logs from different microservices demands for substantial cognitive work by human experts. This paper proposes a novel approach to accompany microservices logs with black box tracing to help practitioners in making informed decisions for troubleshooting. Our approach is based on the passive tracing of request-response messages of the REpresentational State Transfer (REST) communication model. Differently from many existing tools for microservices, our tracing is application transparent and non-intrusive. We present an implementation called MetroFunnel and conduct an assessment in the context of two case studies: a Clearwater IP Multimedia Subsystem (IMS) setup consisting of Docker microservices and a Kubernetes orchestrator deployment hosting tens of microservices. MetroFunnel allows making useful attributions in traversing the logs; more important, it reduces the size of collected monitoring data at negligible performance overhead with respect to traditional logs.","1939-1374","","10.1109/TSC.2019.2940009","Compagnia di San Paolo(grant numbers:Programme STAR); Ministero dell Istruzione dell Universita e della Ricerca; Universita degli Studi di Napoli Federico II; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8826375","monitoring;microservices;REST;docker;clearwater;kubernetes;log analysis","Monitoring;Instruments;Microservice architecture;Measurement;Semantics;Runtime;Software systems;Market research;IP networks","","26","","52","IEEE","6 Sep 2019","","","IEEE","IEEE Journals"
"Emerging Technologies for Next Generation Remote Health Care and Assisted Living","I. Ahmad; Z. Asghar; T. Kumar; G. Li; A. Manzoor; K. Mikhaylov; S. A. Shah; M. Höyhtyä; J. Reponen; J. Huusko; E. Harjula","VTT Technical Research Centre of Finland, Espoo, Finland; Navigil, Espoo, Finland; Centre for Wireless Communications, University of Oulu, Oulu, Finland; Institute of Cyber Security, Shanghai Jiao Tong University, Shanghai, China; Centre for Wireless Communications, University of Oulu, Oulu, Finland; Centre for Wireless Communications, University of Oulu, Oulu, Finland; School of Computing and Digital Technology, Birmingham City University, Birmingham, U.K.; VTT Technical Research Centre of Finland, Espoo, Finland; Centre for Wireless Communications, University of Oulu, Oulu, Finland; VTT Technical Research Centre of Finland, Espoo, Finland; Centre for Wireless Communications, University of Oulu, Oulu, Finland",IEEE Access,"1 Jun 2022","2022","10","","56094","56132","Remote health care is currently one of the most promising solutions to ensure a high level of treatment outcome, cost-efficiency and sustainability of the healthcare systems worldwide. Even though research on remote health care can be traced back to the early days of the Internet, the recent COVID-19 has necessitated further improvement in existing health care systems with invigorated research on remote health care technologies. In this article we delve into the state-of-the-art research in latest technologies and technological paradigms that play a vital role in enabling the next generation remote health care and assisted living. First the need of using the latest technological developments in the domain of remote health care is briefly discussed. Then the most important technologies and technological paradigms that are crucial in enabling remote health care and assisted living are emphasised. Henceforth, a detailed survey of existing technologies, potential challenges in those technologies, and possible solutions is conducted. Finally, missing research gaps and important future research directions in each enabling technology are brought forth to motivate further research in remote health care.","2169-3536","","10.1109/ACCESS.2022.3177278","Academy of Finland through the DigiHealth Project(grant numbers:326291); 6G Flagship Project(grant numbers:318927,346208); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9780134","Telemedicine;5G/6G;medical IoT;remote care;big data;AI/ML;medical informatics","Medical services;Aging;Older adults;Statistics;Sociology;Diseases;Next generation networking","","26","","317","CCBY","23 May 2022","","","IEEE","IEEE Journals"
"Kubernetes-Oriented Microservice Placement With Dynamic Resource Allocation","Z. Ding; S. Wang; C. Jiang","Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, China; Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, China; Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, China",IEEE Transactions on Cloud Computing,"6 Jun 2023","2023","11","2","1777","1793","Microservices and Kubernetes are widely used in the development and operations of cloud-native applications. By providing automated placement and scaling, Kubernetes has become the main tool for managing microservices. However, existing work and Kubernetes fail to consider the dynamic competition and availability of microservices as well as the problem of shared dependency libraries among multiple microservice instances. To this end, this article proposes an integer nonlinear microservice placement model for Kubernetes with the goal of cost minimization. Specifically, we calculate the number of instances based on microservice availability and construct a model in which the total resource demand of multiple microservice instances exceeds the appropriate proportion of node resources when dynamic resource competition exists and the size of the shared dependency library is less than the node storage capacity. Finally, this article solves the microservice placement model using an improved genetic algorithm. The experimental results demonstrate that higher throughput is obtained with the same costs and that the same throughput is obtained with lower costs.","2168-7161","","10.1109/TCC.2022.3161900","National Key Research and Development Program of China(grant numbers:2019YFB1704102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9741392","Microservice placement;dynamic resource competition;microservice availability;shared dependency libraries;heuristic solving","Microservice architectures;Containers;Cloud computing;Costs;Libraries;Dynamic scheduling;Throughput","","26","","50","IEEE","24 Mar 2022","","","IEEE","IEEE Journals"
"Exploring the Potential of Blockchain Technology in an IoT-Enabled Environment: A Review","Deepak; P. Gulia; N. S. Gill; M. Yahya; P. Gupta; P. K. Shukla; P. K. Shukla","Department of Computer Science and Applications, Maharshi Dayanand University, Rohtak, India; Department of Computer Science and Applications, Maharshi Dayanand University, Rohtak, India; Department of Computer Science and Applications, Maharshi Dayanand University, Rohtak, India; Department of Computer Science, Oakland University, Rochester, MI, USA; School of Computer Science, University College Dublin, Dublin 4, Ireland; Department of Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, Guntur, Andhra Pradesh, India; Department of Computer Science and Engineering, University Institute of Technology RGPV, Bhopal, Madhya Pradesh, India",IEEE Access,"1 Mar 2024","2024","12","","31197","31227","Internet of Things (IoT) plays an essential contribution in connecting devices and enabling seamless data exchange, leading to increased efficiency and convenience. However, security concerns in IoT systems are significant, as compromised devices can lead to data breaches and privacy violations. Blockchain technology can enhance IoT security by providing decentralized consensus, immutability, and transparent transaction records, ensuring secure and trustworthy communication and data integrity. This review article gives a succinct but thorough understanding of blockchain technology, covering architecture of blockchain, working principles, types, applications, platforms, and its role in the IoT environment. The study highlights potential benefits of blockchain like enhanced security and privacy, and explores its integration with IoT. Additionally, the study discusses various real-world applications, examines blockchain platforms, and addresses the limitations and challenges associated with blockchain technology. This review serves as a valuable resource for researchers and practitioners seeking a deeper understanding of blockchain’s potential and its implications in the IoT landscape.","2169-3536","","10.1109/ACCESS.2024.3366656","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10445250","Blockchain;data exchange;Internet of Things (IoT);privacy;security","Blockchains;Security;Internet of Things;Smart contracts;Proof of Work;Reviews;Computer science;Data transfer;Information exchange;Privacy","","26","","175","CCBYNCND","26 Feb 2024","","","IEEE","IEEE Journals"
"A Data-Centric Internet of Things Framework Based on Azure Cloud","Y. Liu; K. Akram Hassan; M. Karlsson; Z. Pang; S. Gong","Department of Science and Technology, Linköping University, Campus Norrköping, Norrköping, Sweden; Department of Science and Technology, Linköping University, Campus Norrköping, Norrköping, Sweden; Department of Science and Technology, Linköping University, Campus Norrköping, Norrköping, Sweden; Corporate Research, ABB AB, Västerås, Sweden; Department of Science and Technology, Linköping University, Campus Norrköping, Norrköping, Sweden",IEEE Access,"2 May 2019","2019","7","","53839","53858","Internet of Things (IoT) has been found pervasive use cases and become a driving force to constitute a digital society. The ultimate goal of IoT is data and the intelligence generated from data. With the progress in public cloud computing technologies, more and more data can be stored, processed and analyzed in cloud to release the power of IoT. However, due to the heterogeneity of hardware and communication protocols in the IoT world, the interoperability and compatibility among different link layer protocols, sub-systems, and back-end services have become a significant challenge to IoT practices. This challenge cannot be addressed by public cloud suppliers since their efforts are mainly put into software and platform services but can hardly be extended to end devices. In this paper, we propose a data-centric IoT framework that incorporates three promising protocols with fundamental security schemes, i.e., WiFi, Thread, and LoRaWAN, to cater to massive IoT and broadband IoT use cases in local, personal, and wide area networks. By taking advantages of the Azure cloud infrastructure, the framework features a unified device management model and data model to conquer the interoperability challenge. We also provide implementation and a case study to validate the framework for practical applications.","2169-3536","","10.1109/ACCESS.2019.2913224","Environmental Protection Agency; Norrköping Fund for Research and Development, Sweden; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698762","Internet of Things;framework;cloud;azure;IoT hub;thread;WiFi;lorawan","Internet of Things;Cloud computing;Protocols;Wireless fidelity;Broadband communication;Monitoring;Interoperability","","26","","38","OAPA","25 Apr 2019","","","IEEE","IEEE Journals"
"Control Networks and Smart Grid Teleprotection: Key Aspects, Technologies, Protocols, and Case-Studies","L. F. F. De Almeida; j. R. D. Santos; L. A. M. Pereira; A. C. Sodré; L. L. Mendes; J. J. P. C. Rodrigues; R. A. L. Rabelo; A. M. Alberti","National Institute of Telecommunications (Inatel), Santa Rita do Sapucaí, Brazil; National Institute of Telecommunications (Inatel), Santa Rita do Sapucaí, Brazil; National Institute of Telecommunications (Inatel), Santa Rita do Sapucaí, Brazil; National Institute of Telecommunications (Inatel), Santa Rita do Sapucaí, Brazil; National Institute of Telecommunications (Inatel), Santa Rita do Sapucaí, Brazil; Post-Graduation Program in Electrical Engineering, Federal University of Piauí (UFPI), Teresina, Brazil; Computing Department, Federal University of Piauí (UFPI), Teresina, Brazil; National Institute of Telecommunications (Inatel), Santa Rita do Sapucaí, Brazil",IEEE Access,"29 Sep 2020","2020","8","","174049","174079","The appeal for more reliable, efficient and resilient power grids has become indispensable with the recent technological development and ever-increasing demand for electrical energy. These goals might be achieved by introducing sensors and actuators in the power grid, with the purpose of enabling a smart control of the power network. All power devices must be connected using a reliable communication network, which should be able to operate even when the power grid fails. Currently, several communication technologies have been applied for supporting this new application scenario. This article aims for reviewing both academic and market use cases for technologies applied to the mission-critical, control networks and Smart Grid applications, such as teleprotection, self-healing, communication with control centers and field devices, among others. More specifically, the key aspects, potential technologies, main protocols and use cases of operating data networks in energy transmission, power distribution and smart grid environments, including circuit and packet switching technologies, are discussed into details. Furthermore, resilience and the main telecommunications technologies used in power grids, as well, the correlation among them, are exploited in practical point of view. The article provides a broad discussion on the best telecommunication options to build the emerging intelligent energy distribution systems, covering control networks, teleprotection, and smart grid applications.","2169-3536","","10.1109/ACCESS.2020.3025235","Modelo de Referência para a Rede Operativa de Dados da Companhia Energética de Minas Gerais (CEMIG) funded by the Fundação de Amparo à Pesquisa do Estado de Minas Gerais/Companhia Energética de Minas Gerais/Agência Nacional de Energia Elétrica (FAPEMIG/CEMIG/ANEEL)(grant numbers:D0640); Rede Nacional de Ensino e Pesquisa (RNP), with resources from Ministério da Ciência, Tecnologia e Inovações (MCTIC), through the Radiocommunication Reference Center (Centro de Referência em Radiocomunicações—CRR) Project of the National Institute of Telecommunications (Instituto Nacional de Telecomunicações—Inatel), Brazil(grant numbers:01250.075413/2018-04); Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES), Brazil(grant numbers:Finance Code 001); Fundação para a Ciência e a Tecnologia/Ministério da Ciência, Tecnologia e Ensino Superior (FCT/MCTES) through national funds and when applicable co-funded EU(grant numbers:UIDB/EEA/50008/2020); Brazilian National Council for Scientific and Technological Development (CNPq)(grant numbers:309335/2017-5); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200485","Power grids;telecommunications;power system protection;smart grids;packet switching;circuit and systems;optical fiber networks;SCADA systems;wireless networks;distributed control","Smart grids;Telecommunications;Mission critical systems;Wireless communication;Protocols","","25","","285","CCBY","18 Sep 2020","","","IEEE","IEEE Journals"
"IndustEdge: A Time-Sensitive Networking Enabled Edge-Cloud Collaborative Intelligent Platform for Smart Industry","Y. Wang; S. Yang; X. Ren; P. Zhao; C. Zhao; X. Yang","National Engineering Laboratory for Big Data Analytics and School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China; National Engineering Laboratory for Big Data Analytics and Ministry of Education Key Lab for Intelligent Networks and Network Security, Xi’an Jiaotong University, Xi’an, China; National Engineering Laboratory for Big Data Analytics and School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China; National Engineering Laboratory for Big Data Analytics and School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China; Department of Computing, Imperial College London, London, U.K.; National Engineering Laboratory for Big Data Analytics and School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China",IEEE Transactions on Industrial Informatics,"5 Jan 2022","2022","18","4","2386","2398","An edge-cloud collaborative intelligent (ECCI) platform is of great significance for the agile development and rapid deployment of ECCI applications, which are essential for realizing smart industry in the era of Industry 4.0. However, the existing platforms lack considering the high real-time latency demand of industrial operations, which severely hinders the development of smart industry and may even lead to severe industrial accidents. To effectively reduce the response latency of industrial applications, in this article, we propose an ECCI platform IndustEdge. It takes time-sensitive networking as the deterministic transport for the link layer, and provides an extensible ECCI orchestration component to reduce the system level latency. Furthermore, IndustEdge has an ECCI algorithm library for different collaborative modes and provides the complete life cycle management for ECCI applications. We implement platforms for both the real-world prototype and emulated-world emulation, and conduct two case studies to evaluate the effectiveness of IndustEdge.","1941-0050","","10.1109/TII.2021.3104003","National Key Research and Development Program of China(grant numbers:2017YFB1010004); National Natural Science Foundation of China(grant numbers:61772410,61802298,U1811461,11690011); China Postdoctoral Science Foundation(grant numbers:2020T130513,2019M663726); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9531530","Edge cloud collaborative intelligence (ECCI);edge computing platform;Industry 4.0;smart industry;time -sensitive networking (TSN)","Collaboration;Cloud computing;Industries;Emulation;Real-time systems;Bridges;Prototypes","","25","","29","IEEE","8 Sep 2021","","","IEEE","IEEE Journals"
"A Multi-Clustering Approach to Scale Distributed Tenant Networks for Mobile Edge Computing","R. Bruschi; F. Davoli; P. Lago; J. F. Pajo","National Laboratory of Smart, Sustainable and Secure Internet Technologies and Infrastructures (S3ITI), Italian National Consortium for Telecommunications (CNIT), Genoa, Italy; Department of Electrical, Electronic and Telecommunications Engineering, and Naval Architecture (DITEN), University of Genoa, Genoa, Italy; National Laboratory of Smart, Sustainable and Secure Internet Technologies and Infrastructures (S3ITI), Italian National Consortium for Telecommunications (CNIT), Genoa, Italy; Department of Electrical, Electronic and Telecommunications Engineering, and Naval Architecture (DITEN), University of Genoa, Genoa, Italy",IEEE Journal on Selected Areas in Communications,"14 Feb 2019","2019","37","3","499","514","Fifth generation (5G) mobile networks will lead to a deep integration between networks and applications. Through novel paradigms like network functions virtualization and edge computing, the new classes of heterogeneous application services will be enabled to run close to the mobile end-user devices with zero-perceived latency and fully-cognitive dynamic reconfiguration capabilities. Such “vertical” applications exhibit diverse performance/scalability requirements, and will rely on highly distributed, extremely virtualized, multi-tenant, and software-defined infrastructures. In such a context, handling the required operations in a scalable and dynamic fashion will be of paramount importance. A specific aspect, addressed by software-defined networking (SDN), regards the provision of suitable communication channels, once resource allocation mechanisms have performed the most efficient deployment of virtual network function instances, and VNF chaining needs to be implemented to enable network services. In this respect, this paper introduces the multi-cluster overlay (MCO) network paradigm: a tunnel-less SDN scheme for scalable realization of virtual tenant networks across the 5G distributed infrastructure, able to support (bulk) migrations of software instances among geo-distributed computing resources in a seamless and effective fashion. The numerical simulation and experimental results show that the MCO achieves up to over one order of magnitude smaller number of forwarding rules than the other state-of-the-art SDN mechanisms, while also assuring high performance during reconfiguration operations.","1558-0008","","10.1109/JSAC.2019.2894236","INPUT and MATILDA projects; European Commission(grant numbers:644672,761898); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8624501","Edge computing;5G;SDN;mobility","5G mobile communication;Logic gates;Edge computing;Network function virtualization;Resource management;Cloud computing;Quality of service","","25","","55","IEEE","23 Jan 2019","","","IEEE","IEEE Journals"
"Cloud Customer's Historical Record Based Resource Pricing","M. Aazam; E. -N. Huh; M. St-Hilaire; C. -H. Lung; I. Lambadaris","Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada; Computer Engineering Department, Kyung Hee University, Korea; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada",IEEE Transactions on Parallel and Distributed Systems,"10 Jun 2016","2016","27","7","1929","1940","Media content in its digital form has been rapidly scaling up, resulting in popularity gain of cloud computing. Cloud computing makes it easy to manage the vastly increasing digital content. Moreover, additional features like, omnipresent access, further service creation, discovery of services, and resource management also play an important role in this regard. The forthcoming era is interoperability of multiple clouds, known as cloud federation or inter-cloud computing. With cloud federation, services would be provided through two or more clouds. Once matured and standardized, inter-cloud computing is supposed to provide services which would be more scalable, better managed, and efficient. Such tasks are provided through a middleware entity called cloud broker. A broker is responsible for reserving resources, managing them, discovering services according to customer's demands, Service Level Agreement (SLA) negotiation, and match-making between the involved service provider and the customer. So far existing studies discuss brokerage in a narrow focused way. In the research outcome presented in this paper, we provide a holistic brokerage model to manage on-demand and advance service reservation, pricing, and reimbursement. A unique feature of this study is that we have considered dynamic management of customer's characteristics and historical record in evaluating the economics related factors. Additionally, a mechanism of incentive and penalties is provided, which helps in trust build-up for the customers and service providers, prevention of resource underutilization, and profit gain for the involved entities. For practical implications, the framework is modeled on Amazon Elastic Compute Cloud (EC2) On-Demand and Reserved Instances service pricing. For certain features required in the model, data was gathered from Google Cluster trace.","1558-2183","","10.1109/TPDS.2015.2473850","Basic Science Research Program; National Research Foundation of Korea; Ministry of Education(grant numbers:NRF-2013R1A1A2013620); Institute for Information & communications Technology Promotion (IITP); Korea government(grant numbers:B0101-15-0535); Development of Modularized In-Memory Virtual Desktop System Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7226829","cloud broker;resource management;pricing;Inter-cloud computing;cloud federation;Cloud broker;resource management;pricing;inter-cloud computing;cloud federation","Pricing;Resource management;Mathematical model;Cloud computing;Quality of service;Economics;Electronic mail","","25","","42","IEEE","27 Aug 2015","","","IEEE","IEEE Journals"
"A Secure Blockchain Platform for Supporting AI-Enabled IoT Applications at the Edge Layer","S. M. Alrubei; E. Ball; J. M. Rigelsford","Department of Electronic and Electrical Engineering, The University of Sheffield, Sheffield, U.K.; Department of Electronic and Electrical Engineering, The University of Sheffield, Sheffield, U.K.; Sensata Technologies, Swindon, U.K.",IEEE Access,"21 Feb 2022","2022","10","","18583","18595","In this study, a new blockchain protocol and a novel architecture that integrate the advantages offered by edge computing, artificial intelligence (AI), IoT end-devices, and blockchain were designed, developed, and validated. This new architecture has the ability to monitor the environment, collect data, analyze it, process it using an AI-expert engine, provide predictions and actionable outcomes, and finally share it on a public blockchain platform. For the use-case implementation, the pandemic caused by the wide and rapid spread of the novel coronavirus COVID-19 was used to test and evaluate the proposed system. Recently, various authors traced the spread of viruses in sewage water and studied how it can be used as a tracking system. Early warning notifications can allow governments and organizations to take appropriate actions at the earliest stages possible. The system was validated experimentally using 14 Raspberry Pis, and the results and analyses proved that the system is able to utilize low-cost and low-power flexible IoT hardware at the processing layer to detect COVID-19 and predict its spread using the AI engine, with an accuracy of 95%, and share the outcome over the blockchain platform. This is accomplished when the platform is secured by the honesty-based distributed proof of authority (HDPoA) and without any substantial impact on the devices’ power sources, as there was only a power consumption increase of 7% when the Raspberry Pi was used for blockchain mining and 14% when used to produce an AI prediction.","2169-3536","","10.1109/ACCESS.2022.3151370","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9713867","Edge computing;blockchain protocol;the Internet of Things (IoT);flexible IoT hardware;artificial intelligence (AI);coronavirus disease (COVID-19)","Blockchains;Internet of Things;Artificial intelligence;Security;Computer architecture;Smart contracts;Edge computing","","25","","30","CCBY","14 Feb 2022","","","IEEE","IEEE Journals"
"In.IoT—A New Middleware for Internet of Things","M. A. A. da Cruz; J. J. P. C. Rodrigues; P. Lorenz; V. V. Korotaev; V. H. C. de Albuquerque","Instituto de Telecomunicações, Santa Rita do Sapucaí, Brazil; PPGEE, Federal University of Piauí, Teresina, Brazil; IUT, University of Haute Alsace, Colmar, France; International Institute of Photonics and Optoinformatics, ITMO University, St. Petersburg, Russia; ARMTEC Tecnologia em Robótica, Fortaleza, Brazil",IEEE Internet of Things Journal,"7 May 2021","2021","8","10","7902","7911","The evolution of Internet of Things (IoT) led to the construction of many IoT middleware, a software that plays a key role since it supports the communication among devices, users, and applications. Although various solutions and studies were proposed, they rarely address crucial privacy and security considerations, especially regarding the message queuing telemetry transport (MQTT) protocol. Moreover, in the majority of the solutions, integrating new devices is a time-consuming task performed manually that cannot be accomplished in a scenario with thousands, maybe millions of devices. In this sense, this article proposes a new IoT middleware, called In.IoT, a scalable, secure, and innovative middleware solution that addresses the middleware concerns identified in this article. In.IoT architectural recommendations and requirements are detailed and can be replicated by new and available solutions. It supports MQTT, CoAP, and HTTP as application-layer protocols. Its performance is evaluated in comparison with the most promising solutions available in the literature and the results obtained by the proposed solution are extremely promising. In.IoT is evaluated, demonstrated, validated, and it is ready and available for use.","2327-4662","","10.1109/JIOT.2020.3041699","Fundo de Apoio ao Desenvolvimento das Comunicações, presidential decree no 264/10, November 26, 2010, Republic of Angola; FCT/MCTES through National Funds and when Applicable Co-Funded EU Funds(grant numbers:UIDB/50008/2020); RNP, with resources from MCTIC(grant numbers:01250.075413/2018-04); Centro de Referência em Radiocomunicações Project of the Instituto Nacional de Telecomunicações (Inatel), Brazil; Government of Russian Federation(grant numbers:08-08); Brazilian National Council for Research and Development (CNPq)(grant numbers:304315/2017-6,430274/2018-1,431726/2018-3,309335/2017-5); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9274362","Constrained application protocol (CoAP);hypertext transfer protocol (HTTP);InIoT;Internet of Things (IoT);message queuing telemetry transport (MQTT);middleware;middleware architecture;platform;software","Middleware;Internet of Things;Protocols;Computer architecture;Security;Service-oriented architecture;Servers","","25","","27","IEEE","1 Dec 2020","","","IEEE","IEEE Journals"
"Broadcast Core-Network: Converging Broadcasting With the Connected World","J. Montalban; R. Cabrera; E. Iradier; P. Angueira; Y. Wu; L. Zhang; W. Li; Z. Hong","Department of Communications Engineering, University of the Basque County, Bilbao, Spain; Department of Communications Engineering, University of the Basque County, Bilbao, Spain; Department of Communications Engineering, University of the Basque County, Bilbao, Spain; Department of Communications Engineering, University of the Basque County, Bilbao, Spain; Communications Research Centre Canada, Ottawa, ON, Canada; Communications Research Centre Canada, Ottawa, ON, Canada; Communications Research Centre Canada, Ottawa, ON, Canada; Communications Research Centre Canada, Ottawa, ON, Canada",IEEE Transactions on Broadcasting,"2 Sep 2021","2021","67","3","558","569","The new ways to consume and deliver media content in a wireless environment have led to a significant increase in the demand for spectrum resources. In addition, the arrival of next-generation wireless communication technologies has encouraged the creating of new verticals targeting novel applications, such as industry IoT, e-health, distance learning, smart city, smart-metering. All these new use cases are centred on an IP-based infrastructure. A promising alternative to satisfy the demand for resources is to incorporate a new delivery mechanism that can alleviate the congestion of classical cell-based point-to-point broadband wireless infrastructures. The best-positioned candidate is the next-generation Digital Terrestrial Television system, which offers a very spectrum efficient one-to-many PHY, and for the first time, native IP support. In ATSC 3.0, the whole system was designed with an IP-centric view, whereas DVB has created standards to deliver services to generic IP devices (DVB-I). Nevertheless, there is still a last missing piece to bring the broadcast infrastructure to the IP-connected world: a Broadcast Core Network (BCN). In this paper, the basics of a BCN are presented with new use cases that are attractive to Broadcast Network Operator (BNO). A possible roadmap towards implementation is described, and a service-based architecture for a BCN is presented and discussed.","1557-9611","","10.1109/TBC.2021.3105026","Eusko Jaurlaritza(grant numbers:IT1234-19); PREDOC Grant Program(grant numbers:PRE_2019_2_0037); Spanish Government through Project PHANTOM(grant numbers:RTI2018-099162-B-I00 (MCIU/AEI/FEDER, UE)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9521563","ATSC 3.0;DVB;core network;digital terrestrial television;NFV;SDN","Media;Digital video broadcasting;Broadcasting;Production;IP networks;Wireless communication;Standards","","24","","47","IEEE","24 Aug 2021","","","IEEE","IEEE Journals"
"A Secure and Decentralized Trust Management Scheme for Smart Health Systems","M. Ebrahimi; M. S. Haghighi; A. Jolfaei; N. Shamaeian; M. H. Tadayon","Iran Telecom Research Center (ITRC), Tehran, Iran; School of Electrical and Computer Engineering, College of Engineering, University of Tehran, Tehran, Iran; Department of Computing, Macquarie University, Sydney, NSW, Australia; School of Electrical and Computer Engineering, College of Engineering, University of Tehran, Tehran, Iran; Iran Telecom Research Center (ITRC), Tehran, Iran",IEEE Journal of Biomedical and Health Informatics,"5 May 2022","2022","26","5","1961","1968","The Internet of Things (IoT) growth is extremely fast and it now has found its way to healthcare applications too. Many smart health gadgets and devices are helping practitioners in collecting medical information and monitoring patients. In this distributed system, information or service is sometimes shared and used by other devices. Considering the importance of health-related information and the decisions made based on it, there should be some sort of assurance on the security and quality of the services or information provided. Trust management is an efficient means of promoting application security and reliability in these cases. However, due to some limitations that are specific to IoT, traditional trust evaluation algorithms cannot be employed or do not yield satisfactory results. In this paper, evidence theory is exploited to design a decentralized service-oriented trust management model for healthcare IoT. A measure of evidence distance is used to reward well-behaving healthcare service/information providers as well as referrers and punish malicious entities. In this context-aware model, trust is estimated based on direct experiences and indirect feedbacks of recommenders. The process runs in two contexts; trust to healthcare service and trust to recommendation. When personal direct experience does not exist, trust to a source or service is estimated by applying the combinatorial laws of evidence theory and integrating indirect trust values. The proposed model is secure against bad-mouthing, good-mouthing, and on-off attacks due to its dynamic parameters and using the concept of evidence distance. Our results confirm the robustness and efficiency of this scheme.","2168-2208","","10.1109/JBHI.2021.3107339","Iran National Science Foundation(grant numbers:96003291); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9521715","Internet of Things;smart health;health- care;trust management;cybersecurity;evidence theory","Medical services;Trust management;Computational modeling;Evidence theory;Bioinformatics;Data models;Smart healthcare","Algorithms;Computer Security;Humans;Internet of Things;Reproducibility of Results;Trust","24","","41","IEEE","24 Aug 2021","","","IEEE","IEEE Journals"
"Online Deployment Algorithms for Microservice Systems With Complex Dependencies","X. He; Z. Tu; M. Wagner; X. Xu; Z. Wang","Faculty of Computing, Harbin Institute of Technology, Harbin, Heilongjiang, China; Faculty of Computing, Harbin Institute of Technology, Harbin, Heilongjiang, China; School of Computer Science, The University of Adelaide, Adelaide, SA, Australia; Faculty of Computing, Harbin Institute of Technology, Harbin, Heilongjiang, China; Faculty of Computing, Harbin Institute of Technology, Harbin, Heilongjiang, China",IEEE Transactions on Cloud Computing,"6 Jun 2023","2023","11","2","1746","1763","Cloud and edge computing have been widely adopted in many application scenarios. With the increasing demand of fast iteration and complexity of business logic, it is challenging to achieve rapid development and continuous delivery in such highly distributed cloud and edge computing environment. At present, the microservice-based architecture has been the dominant deployment style, and a microservice system has to evolve agilely to offer stable Quality of Service (QoS) in the situation where user requirement changes frequently. A lot of research have been conducted to optimally re-deploy microservices to adapt to changing requirements. Nevertheless, complex dependencies between microservices and the existence of multiple instances of one single microservice in a microservice system together have not been fully considered in existing work. This article defines SPPMS, the Service Placement Problem in Microservice Systems that feature complex dependencies and multiple instances, as a Fractional Polynomial Problem (FPP). Considering the high computation complexity of FPP, it is then transformed into a Quadratic Sum-of-Ratios Fractional Problem (QSRFP) which is further solved by the our proposed greedy-based algorithms. Experiments demonstrate that our models and algorithms outperform existing approaches in both qualities of the generated solutions and computation speed.","2168-7161","","10.1109/TCC.2022.3161684","National Key Research and Development Program of China(grant numbers:2018YFB1402500); National Natural Science Foundation of China(grant numbers:61832014,61772155,61832004); Australian Research Council(grant numbers:DP200102364,DP210102670); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9740415","Cloud computing;microservice systems;multiple instance coexistence;service dependencies;service placement","Microservice architectures;Quality of service;Servers;Cloud computing;Time factors;Production facilities;Task analysis","","24","","55","IEEE","23 Mar 2022","","","IEEE","IEEE Journals"
"Digital Twin Bionics: A Biological Evolution-Based Digital Twin Approach for Rapid Product Development","L. Li; F. Gu; H. Li; J. Guo; X. Gu","State Key Laboratory of Fluid Power and Mechatronic Systems, School of Mechanical Engineering, Zhejiang University, Hangzhou, China; State Key Laboratory of Fluid Power and Mechatronic Systems, School of Mechanical Engineering, Zhejiang University, Hangzhou, China; Henan Provincial Key Laboratory of Intelligent Manufacturing of Mechanical Equipment, Zhengzhou University of Light Industry, Zhengzhou, China; Institutes of Science and Development, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Fluid Power and Mechatronic Systems, School of Mechanical Engineering, Zhejiang University, Hangzhou, China",IEEE Access,"9 Sep 2021","2021","9","","121507","121521","With intensified market competition, product development procedure is accelerated, requiring rapid product innovation and efficient collaboration between design and manufacturing. However, there still exist information islands, prohibiting integration of product life cycle processes. To address this issue, bionics and digital twin (DT) are combined as a potential solution. The concepts, framework, and features of digital twin bionics (DTB) is originally proposed, with co-evolution mechanism of product-twins (including virtual and physical products) and production-twins (including virtual and physical production) elaborated. A symbiotic co-evolution mechanism is presented to integrate the processes of product development and manufacturing. In detail, the supporting technologies, such as industrial internet of things, cloud edge computing, big data, and artificial intelligence, as well as the potential key technologies, are illustrated. A case study for rapid development of automotive body-in-white in an industrial robotics welding production line is investigated to verify the applicability of the proposed framework. The results suggest that the integration of bionics and DTs can accelerate the innovations and developments of new products and also help to achieve efficient management of production construction.","2169-3536","","10.1109/ACCESS.2021.3108218","National Natural Science Foundation of China(grant numbers:71901194,51775517,51775493); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9523744","Digital twin;bionics;product development;co-evolution;product life cycle","Biomimetics;Manufacturing;Production;Product development;Digital twin;Product design;Biological system modeling","","24","","56","CCBY","26 Aug 2021","","","IEEE","IEEE Journals"
"VR-Based Immersive Service Management in B5G Mobile Systems: A UAV Command and Control Use Case","T. Taleb; N. Sehad; Z. Nadir; J. Song","Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, Finland; Department of Communications and Networking, Aalto University, Espoo, Finland; Department of Communications and Networking, Aalto University, Espoo, Finland; Department of Computer and Information Security, Sejong University, Seoul, South Korea",IEEE Internet of Things Journal,"7 Mar 2023","2023","10","6","5349","5363","The management of remote services, such as remote surgery, remote sensing, or remote driving, has become increasingly important, especially with the emerging 5G and Beyond 5G technologies. However, the strict network requirements of these remote services represent one of the major challenges that hinder their fast and large-scale deployment in critical infrastructures. This article addresses certain issues inherent in remote and immersive control of virtual reality (VR)-based unmanned aerial vehicles (UAVs), whereby a user remotely controls UAVs, equipped with 360° cameras, using their head-mounted devices (HMD) and their respective controllers. Remote and immersive control services, using 360° video streams, require much lower latency and higher throughput for true immersion and high service reliability. To assess and analyze these requirements, this article introduces a real-life testbed system that leverages different technologies (e.g., VR, 360° video streaming over 4G/5G, and edge computing). In the performance evaluation, different latency types are considered. They are namely: 1) glass-to-glass latency between the 360° camera of a remote UAV and the HMD display; 2) user/pilot’s reaction latency; and 3) the command/execution latency. The obtained results indicate that the responsiveness (dubbed Glass-to-Reaction-to-Execution—GRE–latency) of a pilot, using our system, to a sudden event is within an acceptable range, i.e., around 900 ms.","2327-4662","","10.1109/JIOT.2022.3222282","European Union’s Horizon 2020 Research and Innovation Programme through CHARITY Project(grant numbers:101016509); Academy of Finland 6Genesis Project(grant numbers:318927); Idea-Mill(grant numbers:33593); Technology Innovation Program; Ministry of Trade, Industry and Energy (MOTIE), South Korea(grant numbers:RS-2022-00154678); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9951155","5G and beyond;edge computing;immersive services;mobile networking;unmanned aerial vehicle (UAV);virtual reality (VR)","Streaming media;Cameras;Reliability;Delays;5G mobile communication;Internet of Things;Autonomous aerial vehicles","","24","","82","CCBY","15 Nov 2022","","","IEEE","IEEE Journals"
"Toward Ubiquitous Semantic Metaverse: Challenges, Approaches, and Opportunities","K. Li; B. P. L. Lau; X. Yuan; W. Ni; M. Guizani; C. Yuen","Real-Time and Embedded Computing Systems Research Centre, Porto, Portugal; Department of Engineering Product Development, Singapore University of Technology and Design, Tampines, Singapore; Digital Productivity and Services Flagship, Commonwealth Scientific and Industrial Research Organization, Sydney, NSW, Australia; Digital Productivity and Services Flagship, Commonwealth Scientific and Industrial Research Organization, Sydney, NSW, Australia; Machine Learning Department, Mohamed Bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE; School of Electrical and Electronics Engineering, Nanyang Technological University, Jurong West, Singapore",IEEE Internet of Things Journal,"11 Dec 2023","2023","10","24","21855","21872","In recent years, ubiquitous semantic Metaverse has been studied to revolutionize immersive cyber-virtual experiences for augmented reality (AR) and virtual reality (VR) users, which leverages advanced semantic understanding and representation to enable seamless, context-aware interactions within mixed-reality environments. This survey focuses on the intelligence and spatiotemporal characteristics of four fundamental system components in ubiquitous semantic Metaverse, i.e., artificial intelligence (AI), spatiotemporal data representation (STDR), Semantic Internet of Things (SIoT), and semantic-enhanced digital twin (SDT). We thoroughly survey the representative techniques of the four fundamental system components that enable intelligent, personalized, and context-aware interactions with typical use cases of the ubiquitous semantic Metaverse, such as remote education, work and collaboration, entertainment and socialization, healthcare, and e-commerce marketing. Furthermore, we outline the opportunities for constructing the future ubiquitous semantic Metaverse, including scalability and interoperability, privacy and security, performance measurement and standardization, as well as ethical considerations and responsible AI. Addressing those challenges is important for creating a robust, secure, and ethically sound system environment that offers engaging immersive experiences for the users and AR/VR applications.","2327-4662","","10.1109/JIOT.2023.3302159","National Funds through the FCT/MCTES (Portuguese Foundation for Science and Technology) through the CISTER Research Unit(grant numbers:UIDP/UIDB/04234/2020); Project ADANET(grant numbers:PTDC/EEICOM/3362/2021); Project IBEX(grant numbers:PTDC/CCI-COM/4280/2021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10208153","Artificial intelligence (AI);augmented reality (AR);data representation;digital twin;Internet of Things;semantic communications;survey;ubiquitous semantic metaverse;virtual reality (VR)","Semantics;Metaverse;Social Internet of Things;Urban areas;Surveys;Ethics;Security;Artificial intelligence;Augmented reality;Digital twins;Virtual reality","","24","","153","IEEE","4 Aug 2023","","","IEEE","IEEE Journals"
"Online Service Migration in Mobile Edge With Incomplete System Information: A Deep Recurrent Actor-Critic Learning Approach","J. Wang; J. Hu; G. Min; Q. Ni; T. El-Ghazawi","Department of Computer Science, University of Exeter, Exeter, U.K.; Department of Computer Science, University of Exeter, Exeter, U.K.; Department of Computer Science, University of Exeter, Exeter, U.K.; School of Computing and Communications, InfoLab21, Lancaster University, Lancaster, U.K.; Department of Electrical and Computer Engineering, The George Washington University, Washington, DC, USA",IEEE Transactions on Mobile Computing,"3 Oct 2023","2023","22","11","6663","6675","Multi-access Edge Computing (MEC) is an emerging computing paradigm that extends cloud computing to the network edge to support resource-intensive applications on mobile devices. As a crucial problem in MEC, service migration needs to decide how to migrate user services for maintaining the Quality-of-Service when users roam between MEC servers with limited coverage and capacity. However, finding an optimal migration policy is intractable due to the dynamic MEC environment and user mobility. Many existing studies make centralized migration decisions based on complete system-level information, which is time-consuming and also lacks desirable scalability. To address these challenges, we propose a novel learning-driven method, which is user-centric and can make effective online migration decisions by utilizing incomplete system-level information. Specifically, the service migration problem is modeled as a Partially Observable Markov Decision Process (POMDP). To solve the POMDP, we design a new encoder network that combines a Long Short-Term Memory (LSTM) and an embedding matrix for effective extraction of hidden information, and further propose a tailored off-policy actor-critic algorithm for efficient training. The extensive experimental results based on real-world mobility traces demonstrate that this new method consistently outperforms both the heuristic and state-of-the-art learning-driven algorithms and can achieve near-optimal results on various MEC scenarios.","1558-0660","","10.1109/TMC.2022.3197706","EU Horizon 2020 INITIATE(grant numbers:101008297); Royal Society International Exchanges(grant numbers:IEC\NSFC\211460); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9853218","Deep reinforcement learning;multi-access edge computing;partial observable markov decision process;service migration","Servers;Delays;Task analysis;Quality of service;Mobile handsets;Computational modeling;Bandwidth","","23","","41","IEEE","9 Aug 2022","","","IEEE","IEEE Journals"
"A Survey on Matching Theory for Distributed Computation Offloading in IoT-Fog-Cloud Systems: Perspectives and Open Issues","H. Tran-Dang; D. -S. Kim","Department of Electronic Engineering, Kumoh National Institute of Technology, Gumi-si, South Korea; Department of Electronic Engineering, Kumoh National Institute of Technology, Gumi-si, South Korea",IEEE Access,"15 Nov 2022","2022","10","","118353","118369","Fog computing has been widely integrated in the IoT-based systems, creating IoT-Fog- Cloud (IFC) systems to improve the system performances and satisfy the quality of services (QoS) and quality of experience (QoE) requirements for the end users (EUs). This improvement is enabled by computational offloading schemes, which perform the task computation nearby the task generation sources (i.e., IoT devices, EUs) on behalf of remote cloud servers. To realize the benefits of offloading techniques, however, there is a need to incorporate efficient resource allocation frameworks, which can deal effectively with intrinsic properties of computing environment in the IFC systems such as resource heterogeneity of computing devices, various requirements of computation tasks, high task request rates, and so on. While the centralize optimization and non-cooperative game theory based solutions are applicable in a certain number of application scenarios, they fail to be efficient in many of cases, where the global information and control might be unavailable or cost-intensive to achieve it in the large-scale systems. The need of distributed computational offloading algorithms with low computation complexity has motivated a surge of solutions using matching theory. In the present review, we first describe the fundamental concept of this emerging tool enabling the distributed implementation in the computing environment. Then the key solution concepts and algorithmic implementations proposed in the framework of literature are highlighted and discussed. Given the powerful tool of matching theory, its full capability is still unexplored and unexploited in the literature. We thereby discover and discuss existing challenges and corresponding solutions that the matching theory can be applied to resolve them. Furthermore, new problems and open issues for application scenarios of modern IFC systems are also investigated thoroughly.","2169-3536","","10.1109/ACCESS.2022.3219427","Ministry of Science and ICT (MSIT), South Korea, under the Grand Information Technology Research Center Support Program Supervised by the Institute for Information & Communications Technology Planning & Evaluation (IITP)(grant numbers:IITP-2020-2020-0-01612); Priority Research Centers Program through the National Research Foundation of Korea (NRF); Ministry of Education, Science and Technology(grant numbers:2018R1A6A1A03024003); Korea Research Fellowship Program through the National Research Foundation of Korea (NRF); Ministry of Science and ICT(grant numbers:NRF-2020R1I1A1A01073019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9938439","IoT-Fog-cloud systems;matching theory;distributed algorithm;computational offloading","Computational modeling;Optimization;Edge computing;Cloud computing;Resource management;Quality of service;Internet of Things;Distributed algorithms","","23","","84","CCBY","4 Nov 2022","","","IEEE","IEEE Journals"
"Edge Learning for 6G-Enabled Internet of Things: A Comprehensive Survey of Vulnerabilities, Datasets, and Defenses","M. A. Ferrag; O. Friha; B. Kantarci; N. Tihanyi; L. Cordeiro; M. Debbah; D. Hamouda; M. Al-Hawawreh; K. -K. R. Choo","AI and Digital Science Research Center, Technology Innovation Institute, Abu Dhabi, UAE; Networks and Systems Laboratory, Badji Mokhtar-Annaba University, Annaba, Algeria; School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Canada; AI and Digital Science Research Center, Technology Innovation Institute, Abu Dhabi, UAE; Department of Computer Science, The University of Manchester, Manchester, U.K; Khalifa University 6G Research Center, Khalifa University of Science and Technology, Abu Dhabi, UAE; Department of Computer Science, Labstic Laboratory, Guelma University, Guelma, Algeria; School of Information Technology, Deakin University, Burwood, VIC, Australia; Department of Information Systems and Cyber Security, University of Texas at San Antonio, San Antonio, TX, USA",IEEE Communications Surveys & Tutorials,"21 Nov 2023","2023","25","4","2654","2713","The deployment of the fifth-generation (5G) wireless networks in Internet of Everything (IoE) applications and future networks (e.g., sixth-generation (6G) networks) has raised a number of operational challenges and limitations, for example in terms of security and privacy. Edge learning is an emerging approach to training models across distributed clients while ensuring data privacy. Such an approach when integrated in future network infrastructures (e.g., 6G) can potentially solve challenging problems such as resource management and behavior prediction. However, edge learning (including distributed deep learning) are known to be susceptible to tampering and manipulation. This survey article provides a holistic review of the extant literature focusing on edge learning-related vulnerabilities and defenses for 6G-enabled Internet of Things (IoT) systems. Existing machine learning approaches for 6G–IoT security and machine learning-associated threats are broadly categorized based on learning modes, namely: centralized, federated, and distributed. Then, we provide an overview of enabling emerging technologies for 6G–IoT intelligence. We also provide a holistic survey of existing research on attacks against machine learning and classify threat models into eight categories, namely: backdoor attacks, adversarial examples, combined attacks, poisoning attacks, Sybil attacks, byzantine attacks, inference attacks, and dropping attacks. In addition, we provide a comprehensive and detailed taxonomy and a comparative summary of the state-of-the-art defense methods against edge learning-related vulnerabilities. Finally, as new attacks and defense technologies are realized, new research and future overall prospects for 6G-enabled IoT are discussed.","1553-877X","","10.1109/COMST.2023.3317242","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255264","Edge learning;6G;IoT;federated learning;AI vulnerabilities;security","6G mobile communication;Security;Internet of Things;Surveys;Peer-to-peer computing;Image edge detection;Federated learning","","23","","300","IEEE","19 Sep 2023","","","IEEE","IEEE Journals"
"Resource Allocation With Workload-Time Windows for Cloud-Based Software Services: A Deep Reinforcement Learning Approach","X. Chen; L. Yang; Z. Chen; G. Min; X. Zheng; C. Rong","College of Computer and Data Science, Fuzhou University, Fuzhou, China; College of Computer and Data Science, Fuzhou University, Fuzhou, China; College of Computer and Data Science, Fuzhou University, Fuzhou, China; Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, U.K.; College of Computer and Data Science, Fuzhou University, Fuzhou, China; Department of Electronic Engineering and Computer Science, University of Stavanger, Stavanger, Norway",IEEE Transactions on Cloud Computing,"6 Jun 2023","2023","11","2","1871","1885","As the workloads and service requests in cloud computing environments change constantly, cloud-based software services need to adaptively allocate resources for ensuring the Quality-of-Service (QoS) while reducing resource costs. However, it is very challenging to achieve adaptive resource allocation for cloud-based software services with complex and variable system states. Most of the existing methods only consider the current condition of workloads, and thus cannot well adapt to real-world cloud environments subject to fluctuating workloads. To address this challenge, we propose a novel Deep Reinforcement learning based resource Allocation method with workload-time Windows (DRAW) for cloud-based software services that considers both the current and future workloads in the resource allocation process. Specifically, an original Deep Q-Network (DQN) based prediction model of management operations is trained based on workload-time windows, which can be used to predict appropriate management operations under different system states. Next, a new feedback-control mechanism is designed to construct the objective resource allocation plan under the current system state through iterative execution of management operations. Extensive simulation results demonstrate that the prediction accuracy of management operations generated by the proposed DRAW method can reach 90.69%. Moreover, the DRAW can achieve the optimal/near-optimal performance and outperform other classic methods by 3$\sim$∼13% under different scenarios.","2168-7161","","10.1109/TCC.2022.3169157","National Natural Science Foundation of China(grant numbers:62072108); Natural Science Foundation of Fujian Province for Distinguished Young Scholars(grant numbers:2020J06014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9761727","Cloud-based software services;resource allocation;workload-time windows;deep reinforcement learning;feedback control","Resource management;Cloud computing;Software;Quality of service;Costs;Control theory;Predictive models","","23","","40","IEEE","21 Apr 2022","","","IEEE","IEEE Journals"
"Distributed Clustering-Based Cooperative Vehicular Edge Computing for Real-Time Offloading Requests","J. Wang; K. Zhu; B. Chen; Z. Han","College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; University of Houston, Houston, TX, USA",IEEE Transactions on Vehicular Technology,"20 Jan 2022","2022","71","1","653","669","Mobile vehicles have been considered as potential edge servers to provide computation resources for the emerging Intelligent Transportation System (ITS) applications. However, how to fully utilize the mobile computation resources to satisfy the real-time arrived computation requests has not been explored yet. This work will address the critical challenges of limited computation resources, stringent computation delay and unknown requirement statistics of real-time tasks in realistic vehicular edge computing scenarios. Specifically, we design a distributed clustering strategy to classify vehicles into multiple cooperative edge servers according to the available computation resources, effective connection time and the distribution of tasks’ expected deadlines. Then, a ‘Less than or Equal to’ Generalized Assignment Problem (i.e., LEGAP) is formulated to maximize the system service revenue, and on this basis, we propose an offline Bound-and-Bound based Optimal (BBO) algorithm to make periodical scheduling with a global view of tasks’ requirement statistics. The quick branching is conducted by following a greedy solution and the upper bound at each branch is derived by solving a multiple-choice knapsack problem. In addition, we present an online heuristic algorithm which makes real-time offloading decision with the guarantees that the resource capacities of all the computing servers are never exceeded with new tasks arriving. Through comparing with the other four online algorithms, the BBO algorithm achieves the highest service revenues by offloading tasks with shorter delays, and the online heuristic algorithm has the best performance in improving the service ratios.","1939-9359","","10.1109/TVT.2021.3122001","National Natural Science Foundation of China(grant numbers:62002166,62071230); Natural Science Foundation of Jiangsu Province(grant numbers:BK20200419,BK20211567); China Postdoctoral Science Foundation(grant numbers:2020M671483); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9583910","Cooperative vehicular edge computing;distributed clustering;online scheduling;bound-and-bound algorithm","Task analysis;Servers;Heuristic algorithms;Processor scheduling;Edge computing;Real-time systems;Vehicle dynamics","","23","","62","IEEE","21 Oct 2021","","","IEEE","IEEE Journals"
"An Integrated Scalable Framework for Cloud and IoT Based Green Healthcare System","M. M. Islam; Z. A. Bhuiyan","Department of Computer Science and Engineering, United International University, Badda, Dhaka, Bangladesh; Department of Computer Science and Engineering, United International University, Badda, Dhaka, Bangladesh",IEEE Access,"9 Mar 2023","2023","11","","22266","22282","Recent developments in IoT-enabled cloud computing and interactive applications have made researchers rethink how healthcare services are currently provided. The IoT-cloud-based systems facilitate remote monitoring and support for patients. However, in the existing area, much emphasis has not been given to making the healthcare systems green. So, in this paper, we present an integrated framework for green healthcare and use cutting-edge technology to make an interactive user interface. We have also ensured the system’s scalability and performance ratio. This system interface has been designed and developed for patients and doctors, where patients can send their healthcare data using wearable sensors, and doctors can receive those data in real-time. For data identification and analysis, we have adopted Hierarchical Clustering Algorithms. Finally, we have come up with a solution for how to make the interactive healthcare experience better for everyone.","2169-3536","","10.1109/ACCESS.2023.3250849","Institute of Advanced Research (IAR), United International University (UIU), through the Research Grant Scheme(grant numbers:UIU/IAR/01/2021/SE/27); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10056935","Internet of Things;smart healthcare framework;cloud computing;interactive digital healthcare;green healthcare","Medical services;Sensors;Internet of Things;Cloud computing;Monitoring;Sensor systems;Green products;Smart healthcare;Green products","","23","","44","CCBYNCND","1 Mar 2023","","","IEEE","IEEE Journals"
"SimulateIoT: Domain Specific Language to Design, Code Generation and Execute IoT Simulation Environments","J. A. Barriga; P. J. Clemente; E. Sosa-Sánchez; Á. E. Prieto","Department of Computer Science, Quercus Software Engineering Group, University of Extremadura, Cáceres, Spain; Department of Computer Science, Quercus Software Engineering Group, University of Extremadura, Cáceres, Spain; Department of Computer Science, Quercus Software Engineering Group, University of Extremadura, Cáceres, Spain; Department of Computer Science, Quercus Software Engineering Group, University of Extremadura, Cáceres, Spain",IEEE Access,"1 Jul 2021","2021","9","","92531","92552","Internet of Things (IoT) is being applied to areas as smart-cities, home environment, agriculture, industry, etc. Developing, deploying and testing IoT projects require high investments on devices, fog nodes, cloud nodes, analytic nodes, hardware and software. New projects require high investments on devices, fog nodes, cloud nodes, analytic nodes, hardware and software before each system can be developed. In addition, the systems should be developed to test them, which implies time, effort and development costs. However, in order to decrease the cost associated to develop and test the system the IoT system can be simulated. Thus, simulating environments help to model the system, reasoning about it, and take advantage of the knowledge obtained to optimize it. Designing IoT simulation environments has been tackled focusing on low level aspects such as networks, motes and so on more than focusing on the high level concepts related to IoT environments. Additionally, the simulation users require high IoT knowledge and usually programming capabilities in order to implement the IoT environment simulation. The concepts to manage in an IoT simulation includes the common layers of an IoT environment including Edge, Fog and Cloud computing and heterogeneous technology. Model-driven development is an emerging software engineering area which aims to develop the software systems from domain models which capture at high level the domain concepts and relationships, generating from them the software artefacts by using code-generators. In this paper, a model-driven development approach has been developed to define, generate code and deploy IoT systems simulation. This approach makes it possible to design complex IoT simulation environments and deploy them without writing code. To do this, a domain metamodel, a graphical concrete syntax and a model to text transformation have been developed. The IoT simulation environment generated from each model includes the sensors, actuators, fog nodes, cloud nodes and analytical characteristics, which are deployed as microservices and Docker containers and where elements are connected by using publish-subscribe communication protocol. Additionally, two case studies, focused on smart building and agriculture IoT environments, are presented to show the simulation expressiveness.","2169-3536","","10.1109/ACCESS.2021.3092528","Ministry of Science and Innovation (MCI), for the State Research Agency (AEI)(grant numbers:RTI2018-098652-B-I00); Government of Extremadura, Council for Economy, Science and Digital Agenda(grant numbers:GR18112); European Regional Development Fund (ERDF)(grant numbers:IB20058); Cátedra Telefónica de la Universidad de Extremadura (Red de Cátedras Telefónica); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9465106","IoT systems;IoT simulation;fog computing;model-driven development;model to text transformation;data analysis","Tools;Context modeling;Sensors;Computational modeling;Actuators;Wireless sensor networks;Cloud computing","","23","","53","CCBY","25 Jun 2021","","","IEEE","IEEE Journals"
"DECA: A Dynamic Energy Cost and Carbon Emission-Efficient Application Placement Method for Edge Clouds","E. Ahvar; S. Ahvar; Z. Á. Mann; N. Crespi; R. Glitho; J. Garcia-Alfaro","Learning, Data and Robotics Laboratory, ESIEA, Ivry-sur-Seine, France; ISEP-Institut Supérieur d’Électronique de Paris, Paris, France; paluno-The Ruhr Institute for Software Technology, University of Duisburg-Essen, Duisburg, Germany; Télécom SudParis, SAMOVAR, Institut Polytechnique de Paris, Palaiseau, France; Concordia Institute for Information Systems Engineering (CIISE), Concordia University, Montréal, Canada; Télécom SudParis, SAMOVAR, Institut Polytechnique de Paris, Palaiseau, France",IEEE Access,"17 May 2021","2021","9","","70192","70213","As an increasing amount of data processing is done at the network edge, high energy costs and carbon emission of Edge Clouds (ECs) are becoming significant challenges. The placement of application components (e.g., in the form of containerized microservices) on ECs has an important effect on the energy consumption of ECs, impacting both energy costs and carbon emissions. Due to the geographic distribution of ECs, there is a variety of resources, energy prices and carbon emission rates to consider, which makes optimizing the placement of applications for cost and carbon efficiency even more challenging than in centralized clouds. This paper presents a Dynamic Energy cost and Carbon emission-efficient Application placement method (DECA) for ECs. DECA addresses both the initial placement of applications on ECs and the re-optimization of the placement using migrations. DECA considers geographically varying energy prices and carbon emission rates as well as optimizing the usage of both network and computing resources at the same time. By combining a prediction-based A* algorithm with a Fuzzy Sets technique, DECA makes intelligent decisions to optimize energy cost and carbon emissions. Simulation results show the ability of DECA in providing a tradeoff and optimizing energy cost and carbon emission at the same time.","2169-3536","","10.1109/ACCESS.2021.3075973","European Union’s Horizon 2020 Research and Innovation Programme (FogProtect)(grant numbers:871525); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9416667","Edge cloud;energy consumption;energy costs;green computing;carbon emission;application placement","Carbon dioxide;Energy consumption;Internet of Things;Cloud computing;Optimization;Resource management;Time factors","","22","","54","CCBY","27 Apr 2021","","","IEEE","IEEE Journals"
"SCADA Systems With Focus on Continuous Manufacturing and Steel Industry: A Survey on Architectures, Standards, Challenges and Industry 5.0","M. Sverko; T. G. Grbac; M. Mikuc","Faculty of Electrical Engineering and Computing, University of Zagreb, 10000, Croatia; Department of Engineering, Juraj Dobrila University of Pula, Pula, Croatia; Faculty of Electrical Engineering and Computing, University of Zagreb, 10000, Croatia",IEEE Access,"19 Oct 2022","2022","10","","109395","109430","Recent technological advances encompassed by the smart factory concept have fundamentally changed industrial control systems in the way they are structured and how they operate. Majority of these changes affect Supervisory Control And Data Acquisition (SCADA) systems, shifting them to a higher level of interoperability, heterogeneous networks, big data and toward internet technologies and services in general. However, this transformation does not affect all SCADA systems equally. The immediate industrial environment and controlled processes have a significant impact as well. This paper presents a holistic approach to SCADA systems implemented in continuous flow production control within the steel industry production environment. We outline the multi-layer architecture of the SCADA control framework and the aspects of interoperability and interconnectivity within the architecture reference models, together with the research challenges and opportunities arising from the recent rapid increasement of the industrial control systems complexity and digital transformation under the Industry 4.0 paradigm, resulting in disrupting levels of the traditional automation pyramid based on Purdue model toward a higher level of integration and interoperability enabling cross-level data exchange empowered by the Industrial Internet of Things. Furthermore, the paper addresses the problem of proprietary SCADA systems and elaborates the causal correlation between SCADA quality requirements and adoption of new technologies in relation to the specific industrial environment of the steel manufacturing process.","2169-3536","","10.1109/ACCESS.2022.3211288","Croatian Science Foundation(grant numbers:HRZZ-IP-2019-04-4216); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9907002","Supervisory control and data acquisition;SCADA;supervisory control;data acquisition;industrial process control;cyber-physical;continuous flow production;manufacturing;steel industry;industry 4.0;industry 5.0;smart factory","SCADA systems;Production processes;Data acquisition;Supervisory control;Manufacturing processes;Smart manufacturing;Steel industry;Process control;Cyber-physical systems;Fourth Industrial Revolution;Fifth Industrial Revolution","","22","","215","CCBY","3 Oct 2022","","","IEEE","IEEE Journals"
"SDN Enhanced Resource Orchestration of Containerized Edge Applications for Industrial IoT","J. Okwuibe; J. Haavisto; E. Harjula; I. Ahmad; M. Ylianttila","Center for Wireless Communication, University of Oulu, Oulu, Finland; Center for Ubiquitous Computing, University of Oulu, Oulu, Finland; Center for Wireless Communication, University of Oulu, Oulu, Finland; VTT Technical Research Center of Finland, Espoo, Finland; Center for Wireless Communication, University of Oulu, Oulu, Finland",IEEE Access,"30 Dec 2020","2020","8","","229117","229131","With the rise of the Industrial Internet of Things (IIoT), there is an intense pressure on resource and performance optimization leveraging on existing technologies, such as Software Defined Networking (SDN), edge computing, and container orchestration. Industry 4.0 emphasizes the importance of lean and efficient operations for sustainable manufacturing. Achieving this goal would require engineers to consider all layers of the system, from hardware to software, and optimizing for resource efficiency at all levels. This emphasizes the need for container-based virtualization tools such as Docker and Kubernetes, offering Platform as a Service (PaaS), while simultaneously leveraging on edge technologies to reduce related latencies. For network management, SDN is poised to offer a cost-effective and dynamic scalability solution by customizing packet handling for various edge applications and services. In this paper, we investigate the energy and latency trade-offs involved in combining these technologies for industrial applications. As a use case, we emulate a 3D-drone-based monitoring system aimed at providing real-time visual monitoring of industrial automation. We compare a native implementation to a containerized implementation where video processing is orchestrated while streaming is handled by an external UE representing the IIoT device. We compare these two scenarios for energy utilization, latency, and responsiveness. Our test results show that only roughly 16 percent of the total power consumption happens on the mobile node when orchestrated. Virtualization adds up about 4.5 percent of the total power consumption while the latency difference between the two approaches becomes negligible after the streaming session is initialized.","2169-3536","","10.1109/ACCESS.2020.3045563","Academy of Finland 6Genesis Flagship(grant numbers:318927); AI Enhanced Mobile Edge Computing project; Future Makers program of Jane and Aatos Erkko Foundation and Technology Industries of Finland Centennial Foundation; Jorma Ollila Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9296769","5G;AR;containerization;cloud;Docker;edge computing;HWPC;industry 40;InfluxDB;IoT;IIoT;latency;MongoDB;multi-access;MNO;NFV;PowerAPI;power consumption;software defined networking (SDN);VR","Containers;Industries;Cloud computing;Software;Monitoring;Automation;Hardware","","22","","54","CCBY","17 Dec 2020","","","IEEE","IEEE Journals"
"Robust Learning-Enabled Intelligence for the Internet of Things: A Survey From the Perspectives of Noisy Data and Adversarial Examples","Y. Wu","College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, U.K.",IEEE Internet of Things Journal,"4 Jun 2021","2021","8","12","9568","9579","The Internet of Things (IoT) has been widely adopted in a range of verticals, e.g., automation, health, energy, and manufacturing. Many of the applications in these sectors, such as self-driving cars and remote surgery, are critical and high stakes applications, calling for advanced machine learning (ML) models for data analytics. Essentially, the training and testing data that are collected by massive IoT devices may contain noise (e.g., abnormal data, incorrect labels, and incomplete information) and adversarial examples. This requires high robustness of ML models to make reliable decisions for IoT applications. The research of robust ML has received tremendous attention from both academia and industry in recent years. This article will investigate the state of the art and representative works of robust ML models that can enable high resilience and reliability of IoT intelligence. Two aspects of robustness will be focused on, i.e., when the training data of ML models contain noises and adversarial examples, which may typically happen in many real-world IoT scenarios. In addition, the reliability of both neural networks and reinforcement learning framework will be investigated. Both of these two ML paradigms have been widely used in handling data in IoT scenarios. The potential research challenges and open issues will be discussed to provide future research directions.","2327-4662","","10.1109/JIOT.2020.3018691","Engineering and Physical Sciences Research Council of United Kingdom(grant numbers:EP/R030863/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9174633","Efficiency;Internet of Things (IoT);machine learning (ML);reliability;robustness","Internet of Things;Data models;Noise measurement;Robustness;Artificial intelligence;Training","","22","","105","IEEE","24 Aug 2020","","","IEEE","IEEE Journals"
"An NFV-Based Service Framework for IoT Applications in Edge Computing Environments","Y. -Y. Shih; H. -P. Lin; A. -C. Pang; C. -C. Chuang; C. -T. Chou","Department of Communications Engineering, National Chung Cheng University, Minxiong, Taiwan; Chunghwa Telecom Laboratories, Chunghwa Telecom Company Ltd., Taipei, Taiwan; Graduate Institute of Networking and Multimedia, National Taiwan University, Taipei, Taiwan; Graduate Institute of Networking and Multimedia, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan",IEEE Transactions on Network and Service Management,"10 Dec 2019","2019","16","4","1419","1434","Emerging Internet of Things (IoT) applications share the same characteristics of involving multiple processing components (i.e., function modules) and requiring a massive amount of data to be processed with low latency. To meet these needs, edge/fog computing has been proposed for next-generation mobile networks to migrate the computing from the cloud to the edge of the network. Thanks to the development of Network Functions Virtualization (NFV), with which edge computing platform can virtualize function modules and deploy them on any edge devices to provide flexible services on the edge networks. However, such platform would need to deal with complicated function module calling relationship (i.e., call graph) of applications and user mobility, and both are not thoroughly considered by existing works of NFV and edge computing. In this paper, based on our previous idea of virtual local-hub (VLH), we propose a complete design of edge computing framework, which applies NFV technology on edge computing environment for IoT applications. To handle the complicated call graphs of IoT applications with better resource utilization, the VLH framework adapts the technologies of container-based virtualization and microservice architecture, which enables remote function module sharing on the edge computing environment. The framework includes the heuristic algorithm for function module allocation with the objective of minimizing total bandwidth consumption. We also present a design of protocols for system operations and mobility handling in the framework. Then we implement the framework on commodity hardware as a testbed. Via simulations under a large-scale environment with practical settings and experiments on the testbed under real-world scenarios, we demonstrate and verify the effectiveness and practicability of the proposed VLH framework for IoT application service provision.","1932-4537","","10.1109/TNSM.2019.2948764","Ministry of Science and Technology(grant numbers:108-2221-E-002-069-MY3,107-2923-E-002-006-MY3,106-2221-E-002-MY2,105-2221-E-002-144-MY3); National Taiwan University(grant numbers:108L880503,108L891904); Ministry of Economic Affairs(grant numbers:107-EC-17-A-02-S5-007); National Chung-Hsing University(grant numbers:2019-5G-0202); MOXA(grant numbers:108-S-C05); Microsoft Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8880517","Network functions virtualization;edge computing;fog computing;Internet of Things;wearable device;microservices;container-based virtualization","IEEE transactions","","22","","47","IEEE","23 Oct 2019","","","IEEE","IEEE Journals"
"Privacy-Preserving Serverless Computing Using Federated Learning for Smart Grids","P. Singh; M. Masud; M. S. Hossain; A. Kaur; G. Muhammad; A. Ghoneim","School of Computer Science and Engineering, Lovely Professional University, Phagwara, India; Department of Computer Science, College of Computers and Information Technology, Taif University, Taif, Saudi Arabia; Department of Software Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; School of Computer Science and Engineering, Lovely Professional University, Phagwara, India; Department of Computer Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Department of Software Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia",IEEE Transactions on Industrial Informatics,"19 Sep 2022","2022","18","11","7843","7852","The smart power grid is a critical energy infrastructure where real-time electricity usage data is collected to predict future energy requirements. The existing prediction models focus on the centralized frameworks, where the collected data from various home area networks (HANs) are forwarded to a central server. This process leads to cybersecurity threats. This article proposes a federated learning based model with privacy preservation of smart grids data using serverless cloud computing. The model considers the blockchain-enabled dew servers in each HAN for local data storage and local model training. Advanced perturbation and normalization techniques are used to reduce the inverse impact of irregular workload on the training results. The experiment conducted on benchmarks datasets demonstrates that the proposed model minimizes the computation and communication costs, attacking probability, and improves the test accuracy. Overall, the proposed model enables smart grids with robust privacy preservation and high accuracy.","1941-0050","","10.1109/TII.2021.3126883","Deanship of Scientific Research, King Saud University, Riyadh, Saudi Arabia(grant numbers:RGP-229); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9611004","Blockchain;dew computing;federated learning;privacy-preserving;serverless computing;smart grid","Computational modeling;Smart grids;Data models;Data privacy;Servers;Training;Collaborative work","","22","","25","IEEE","10 Nov 2021","","","IEEE","IEEE Journals"
"RLOps: Development Life-Cycle of Reinforcement Learning Aided Open RAN","P. Li; J. Thomas; X. Wang; A. Khalil; A. Ahmad; R. Inacio; S. Kapoor; A. Parekh; A. Doufexi; A. Shojaeifard; R. J. Piechocki","Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K; Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K; Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K; Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K; Vilicom U.K. Ltd., Reading, U.K; Vilicom U.K. Ltd., Reading, U.K; Applied Research, Suffolk, U.K; Applied Research, Suffolk, U.K; Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K; InterDigital Communications Inc., Wilmington, DE, USA; Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K",IEEE Access,"7 Nov 2022","2022","10","","113808","113826","Radio access network (RAN) technologies continue to evolve, with Open RAN gaining the most recent momentum. In the O-RAN specifications, the RAN intelligent controllers (RICs) are software-defined orchestration and automation functions for the intelligent management of RAN. This article introduces principles for machine learning (ML), in particular, reinforcement learning (RL) applications in the O-RAN stack. Furthermore, we review the state-of-the-art research in wireless networks and cast it onto the RAN framework and the hierarchy of the O-RAN architecture. We provide a taxonomy for the challenges faced by ML/RL models throughout the development life-cycle: from the system specification to production deployment (data acquisition, model design, testing and management, etc.). To address the challenges, we integrate a set of existing MLOps principles with unique characteristics when RL agents are considered. This paper discusses a systematic model development, testing and validation life-cycle, termed: RLOps. We discuss fundamental parts of RLOps, which include: model specification, development, production environment serving, operations monitoring and safety/security. Based on these principles, we propose the best practices for RLOps to achieve an automated and reproducible model development process. At last, a holistic data analytics platform rooted in the O-RAN deployment is designed and implemented, aiming to embrace and fulfil the aforementioned principles and best practices of RLOps.","2169-3536","","10.1109/ACCESS.2022.3217511","Innovate UK/CELTIC-NEXT European collaborative project on AI-enabled Massive MIMO (AIMM); Next-Generation Converged Digital Infrastructure (NG-CDI) Project; BT and Engineering and Physical Sciences Research Council (EPSRC)(grant numbers:EP/R004935/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9931127","O-RAN;machine learning;reinforcement learning;MLOps;RLOps;digital twins;data engineering","Radio access networks;Computer architecture;Reinforcement learning;Task analysis;Adaptation models;3GPP;Biological system modeling","","22","","96","CCBY","26 Oct 2022","","","IEEE","IEEE Journals"
"The Many Faces of Edge Intelligence","E. Peltonen; I. Ahmad; A. Aral; M. Capobianco; A. Y. Ding; F. Gil-Castiñeira; E. Gilman; E. Harjula; M. Jurmu; T. Karvonen; M. Kelanti; T. Leppänen; L. Lovén; T. Mikkonen; N. Mohan; P. Nurmi; S. Pirttikangas; P. Sroka; S. Tarkoma; T. Yang","Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, Finland; VTT Technical Research Centre of Finland Ltd., Espoo, Finland; Faculty of Computer Science, University of Vienna, Wien, Austria; Business Innovation Manager, Pordenone, Italy; Department of Engineering Systems and Services, TU Delft, Delft, The Netherlands; Enxeñaría telemática, University of Vigo, Vigo, Spain; Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, Finland; Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, Finland; VTT Technical Research Centre of Finland Ltd., Espoo, Finland; Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, Finland; Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, Finland; Information Technology, Oulu University of Applied Sciences, Oulu, Finland; Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, Finland; Department of Computer Science, University of Helsinki, Helsinki, Finland; Connected Mobility, Technical University of Munich, München, Germany; Department of Computer Science, University of Helsinki, Helsinki, Finland; Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, Finland; Institute of Radiocommunications, Poznan University of Technology, Poznań, Poland; Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, Finland; Pengcheng Laboratory, Shenzhen, China",IEEE Access,"11 Oct 2022","2022","10","","104769","104782","Edge Intelligence (EI) is an emerging computing and communication paradigm that enables Artificial Intelligence (AI) functionality at the network edge. In this article, we highlight EI as an emerging and important field of research, discuss the state of research, analyze research gaps and highlight important research challenges with the objective of serving as a catalyst for research and innovation in this emerging area. We take a multidisciplinary view to reflect on the current research in AI, edge computing, and communication technologies, and we analyze how EI reflects on existing research in these fields. We also introduce representative examples of application areas that benefit from, or even demand the use of EI.","2169-3536","","10.1109/ACCESS.2022.3210584","an international expert group, led by the 6G Flagship at the University of Oulu, Finland(grant numbers:318927,326291,323630); European Union’s Horizon 2020 research and innovation programme(grant numbers:101021808); Marie Skłodowska-Curie(grant numbers:956090); National Science Centre in Poland(grant numbers:2018/29/B/ST7/01241); Austrian Science Fund(grant numbers:Y 904-N31,I 5201-N); CHIST-ERA(grant numbers:CHIST-ERA-19-CES-005); City of Vienna (5G Use Case Challenge InTraSafEd 5G); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9905603","Edge intelligence;edge computing;5G;6G","Artificial intelligence;Edge computing;Cloud computing;Low latency communication;Real-time systems;Technological innovation;Image edge detection","","22","","74","CCBY","28 Sep 2022","","","IEEE","IEEE Journals"
"Resource Management for Latency-Sensitive IoT Applications With Satisfiability","C. Avasalcai; C. Tsigkanos; S. Dustdar","Distributed Systems Group, Technische Universität Wien, Vienna, Austria; Technische Universität Wien, Vienna, Austria; Distributed Systems Group (DSG), Information Systems Institute, Vienna University of Technology, Vienna, Austria",IEEE Transactions on Services Computing,"6 Oct 2022","2022","15","5","2982","2993","Satisfying the software requirements of emerging service-based Internet of Things (IoT) applications has become challenging for cloud-centric architectures, as applications demand fast response times and availability of computational resources closer to end-users. Meeting application demands must occur at runtime, facing uncertainty and in a decentralized manner, something that must be reflected in system deployment. We propose a decentralized resource management technique and accompanying technical framework for the deployment of service-based IoT applications at the edge. Faithful to services engineering, applications we consider are composed of interdependent tasks, which in the IoT setting may be concretized as containerized microservices or serverless functions. A deployment for an arbitrary application is found at runtime through satisfiability; the mapping produced is compliant with tasks’ individual resource requirements and latency constraints by construction. Our approach ensures seamless deployment at runtime, assuming no design-time knowledge of device resources or the current network topology. We evaluate the applicability and realizability of our technique over single-board computers as edge devices, particularly in the absence of cloud resources.","1939-1374","","10.1109/TSC.2021.3074188","EU's H2020 Research and Innovation Programme(grant numbers:764785); FORA— Fog Computing for Robotics and Industrial Automation; FWF Austria(grant numbers:M 2778-N); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9409738","Resource management;edge computing services;decentralization;Internet of Things","Task analysis;Internet of Things;Resource management;Runtime;Image edge detection;Edge computing;Computer architecture","","21","","34","IEEE","20 Apr 2021","","","IEEE","IEEE Journals"
"Exploring the Effectiveness of Service Decomposition in Fog Computing Architecture for the Internet of Things","B. Alturki; S. Reiff-Marganiec; C. Perera; S. De","Department of Informatics, University of Leicester, Leicester, United Kingdom; Department of Informatics, University of Leicester, Leicester, United Kingdom; School of Computer Science and Informatics, Cardiff University, Cardiff, United Kingdom; Institute for Communication Systems, University of Surrey, Guildford, United Kingdom",IEEE Transactions on Sustainable Computing,"9 Jun 2022","2022","7","2","299","312","The Internet of Things (IoT) aims to connect everyday physical objects to the internet. These objects will produce a significant amount of data. The traditional cloud computing architecture aims to process data in the cloud. As a result, a significant amount of data needs to be communicated to the cloud. This creates a number of challenges, such as high communication latency between the devices and the cloud, increased energy consumption of devices during frequent data upload to the cloud, high bandwidth consumption, while making the network busy by sending the data continuously, and less privacy because of less control on the transmitted data to the server. Fog computing has been proposed to counter these weaknesses. Fog computing aims to process data at the edge and substantially eliminate the necessity of sending data to the cloud. However, combining the Service Oriented Architecture (SOA) with the fog computing architecture is still an open challenge. In this paper, we propose to decompose services to create linked-microservices (LMS). Linked-microservices are services that run on multiple nodes but closely linked to their linked-partners. Linked-microservices allow distributing the computation across different computing nodes in the IoT architecture. Using four different types of architectures namely cloud, fog, hybrid, and fog+cloud, we explore and demonstrate the effectiveness of service decomposition by applying four experiments to three different type of datasets. Evaluation of the four architectures shows that decomposing services into nodes reduce the data consumption over the network by 10 - 70 percent. Overall, these results indicate that the importance of decomposing services in the context of fog computing for enhancing the quality of service.","2377-3782","","10.1109/TSUSC.2019.2907405","Saudi Arabian Cultural Mission; King Abdul Aziz University; TagItSmart!; European Horizon 2020 programme(grant numbers:688061); EPSRC PETRAS 2(grant numbers:EP/S035362/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8676374","Internet of Things (IoT);cloud computing;fog computing;edge computing;data analytics;distributed data analytics;constraint awareness","Edge computing;Computer architecture;Cloud computing;Internet of Things;Machine learning algorithms;Data processing;Distributed databases","","21","","56","IEEE","29 Mar 2019","","","IEEE","IEEE Journals"
"A Service Oriented Architecture for the Digitalization and Automation of Distribution Grids","M. Pau; M. Mirz; J. Dinkelbach; P. Mckeever; F. Ponci; A. Monti","Institute for Automation of Complex Power Systems, RWTH Aachen University, Aachen, Germany; Institute for Automation of Complex Power Systems, RWTH Aachen University, Aachen, Germany; Institute for Automation of Complex Power Systems, RWTH Aachen University, Aachen, Germany; Institute for Automation of Complex Power Systems, RWTH Aachen University, Aachen, Germany; Institute for Automation of Complex Power Systems, RWTH Aachen University, Aachen, Germany; Institute for Automation of Complex Power Systems, RWTH Aachen University, Aachen, Germany",IEEE Access,"11 Apr 2022","2022","10","","37050","37063","Modern distribution grids are complex systems that need advanced management for their secure and reliable operation. The Information and Communication Technology domain today offers unprecedented opportunities for the smart design of tools in support of grid operators. This paper presents a new philosophy for the digitalization and automation of distribution grids, based on a modular architecture of microservices implemented via container technology. This architecture enables a service-oriented deployment of the intelligence needed in the Distribution Management Systems, moving beyond the traditional view of monolithic software installations in the control rooms. The proposed architecture unlocks a broad set of possibilities, including cloud-based implementations, extension of legacy systems and fast integration of machine learning-based analytic tools. Moreover, it potentially opens a completely new market of turnkey services for distribution grid management, thus avoiding large upfront investments for grid operators. This paper presents the main concepts and benefits of the proposed philosophy, together with an example of field implementation based on open source components carried out in the context of the European project SOGNO.","2169-3536","","10.1109/ACCESS.2022.3164393","Service Oriented Grid for the Network of the Future (SOGNO) and Platform for Operation of Distribution Networks (PLATONE), which are European Projects; European Union’s Horizon 2020 Research and Innovation Program(grant numbers:774613,864300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9748116","Cloud computing;distribution grid automation;distribution management system;IoT platform;microservice architecture;smart grid","Computer architecture;Cloud computing;Automation;Software;Power systems;Smart grids;Microservice architectures","","21","","73","CCBY","4 Apr 2022","","","IEEE","IEEE Journals"
"A Self-Adapting Task Scheduling Algorithm for Container Cloud Using Learning Automata","L. Zhu; K. Huang; Y. Hu; X. Tai","School of Information Science and Technology, University of Science and Technology of China, Hefei, China; Chinese Academy of Sciences, Institute of Electronics, Suzhou, China; Chinese Academy of Sciences, Institute of Electronics, Suzhou, China; Chinese Academy of Sciences, Institute of Electronics, Suzhou, China",IEEE Access,"11 Jun 2021","2021","9","","81236","81252","With the rapid development of cloud computing and container technology, more and more applications are deployed to the cloud, and the scale of cloud platform is expanding. Due to the large number of container instances running in the platform, complex dependency relationship, fast version iteration and other characteristics, the update of business can often cause the change of the whole cloud resource environment, which triggers the repetitive scheduling problem of related tasks and affects stability of the business. In this paper, we propose a self-adapting task scheduling algorithm (ADATSA) using learning automata to solve these problems. Firstly, we design a learning automata model and objective function for the system on task scheduling problem. Then, we realize an effective reward-penalty mechanism for scheduling actions in combination with the idle state of resources and the running state of tasks in the current environment. Meanwhile, the environment is modeled by cluster, node and task, and the probability of action selected is optimized by scheduling execution, thus enhancing the adaptability to the cloud environment of the scheduling and accelerating convergence. Finally, we construct a framework of task load monitoring with buffer queue to achieve dynamic scheduling based on priority. The experimental part verifies the effectiveness of proposed algorithm with different angles such as resource imbalance degree, resource residual degree and QoS. Compared with other learning automata scheduling models such as LAEAS, non-automata technology based algorithms such as PSOS and K8S scheduling engine, ADATSA shows the better performance of environment adaptability, resource optimization efficiency and QoS in dynamic scheduling. The theoretical analysis was consistent with the experimental results.","2169-3536","","10.1109/ACCESS.2021.3078773","Strategic Priority Research Program of the Chinese Academy of Sciences(grant numbers:XDA19080201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9427182","Container cloud;learning automata;self-adapting scheduling;reward-penalty strategy","Task analysis;Containers;Job shop scheduling;Cloud computing;Optimal scheduling;Learning automata;Dynamic scheduling","","21","","39","CCBY","10 May 2021","","","IEEE","IEEE Journals"
"Challenges in Building an End-to-End System for Acquisition, Management, and Integration of Diverse Data From Sensor Networks in Watersheds: Lessons From a Mountainous Community Observatory in East River, Colorado","C. Varadharajan; D. A. Agarwal; W. Brown; M. Burrus; R. W. H. Carroll; D. S. Christianson; B. Dafflon; D. Dwivedi; B. J. Enquist; B. Faybishenko; A. Henderson; M. Henderson; V. C. Hendrix; S. S. Hubbard; Z. Kakalia; A. Newman; B. Potter; H. Steltzer; R. Versteeg; K. H. Williams; C. Wilmer; Y. Wu","Lawrence Berkeley National Laboratory, Earth and Environmental Sciences Area, Berkeley, USA; Lawrence Berkeley National Laboratory, Computing Sciences Area, Berkeley, USA; Bugs Unlimited, LLC Crested Butte, USA; Lawrence Berkeley National Laboratory, Earth and Environmental Sciences Area, Berkeley, USA; Desert Research Institute, Reno, USA; Lawrence Berkeley National Laboratory, Computing Sciences Area, Berkeley, USA; Lawrence Berkeley National Laboratory, Earth and Environmental Sciences Area, Berkeley, USA; Lawrence Berkeley National Laboratory, Earth and Environmental Sciences Area, Berkeley, USA; Department of Ecology and Evolutionary Biology, The University of Arizona, Tucson, USA; Lawrence Berkeley National Laboratory, Earth and Environmental Sciences Area, Berkeley, USA; Rocky Mountain Biological Laboratory, Crested Butte, USA; Lawrence Berkeley National Laboratory, Computing Sciences Area, Berkeley, USA; Lawrence Berkeley National Laboratory, Computing Sciences Area, Berkeley, USA; Lawrence Berkeley National Laboratory, Earth and Environmental Sciences Area, Berkeley, USA; Lawrence Berkeley National Laboratory, Earth and Environmental Sciences Area, Berkeley, USA; Rocky Mountain Biological Laboratory, Crested Butte, USA; Navarro Research and Engineering, Grand Junction, USA; Fort Lewis College, Durango, USA; Subsurface Insights, LLC, Hanover, USA; Lawrence Berkeley National Laboratory, Earth and Environmental Sciences Area, Berkeley, USA; Rocky Mountain Biological Laboratory, Crested Butte, USA; Lawrence Berkeley National Laboratory, Earth and Environmental Sciences Area, Berkeley, USA",IEEE Access,"30 Dec 2019","2019","7","","182796","182813","The U.S. Department of Energy's Watershed Function Scientific Focus Area (SFA), centered in the East River, Colorado, generates diverse datasets including hydrological, geological, geochemical, geophysical, ecological, microbiological and remote sensing data. The project has deployed extensive field infrastructure involving hundreds of sensors that measure highly diverse phenomena (e.g. stream and groundwater hydrology, water quality, soil moisture, weather) across the watershed. Data from the sensor network are telemetered and automatically ingested into a queryable database. The data are subsequently quality checked, integrated with the United States Geological Survey's stream monitoring network using a custom data integration broker, and published to a portal with interactive visualizations. The resulting data products are used in a variety of scientific modeling and analytical efforts. This paper describes the SFA's end-to-end infrastructure and services that support the generation of integrated datasets from a watershed sensor network. The development and maintenance of this infrastructure, presents a suite of challenges from practical field logistics to complex data processing, which are addressed through various solutions. In particular, the SFA adopts a holistic view for data collection, assessment and integration, which dramatically improves the products generated, and enables a co-design approach wherein data collection is informed by model results and vice-versa.","2169-3536","","10.1109/ACCESS.2019.2957793","U.S. Department of Energy(grant numbers:DE-AC02-05CH11231); WatershedFunction Scientific Focus Area funded by the U.S. Department of Energy, Office of Science, Office of Biological, and Environmental Research(grant numbers:DE-AC02-05CH11231); National Energy Research Scientific Computing Center (NERSC), U.S. Department of Energy Office of Science User Facility operated(grant numbers:DE-AC02-05CH11231); Environmental Systems Science Data Infrastructure for a Virtual Ecosystem (ESS-DIVE) data repository operated(grant numbers:DE-AC02-05CH11231); Subsurface Insights for development of some of the core abilities used here(grant numbers:DE-SC0009732,DE-SC0018447); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8924700","Sensor systems and applications;sensors;geoscience;water resources;watershed;data management;data integration;data processing;co-design","Rivers;Data models;Monitoring;Temperature measurement;Data collection;Snow;Biology","","21","","58","CCBY","5 Dec 2019","","","IEEE","IEEE Journals"
"SEGA: Secured Edge Gateway Microservices Architecture for IIoT-Based Machine Monitoring","A. Ghosh; A. Mukherjee; S. Misra","Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India",IEEE Transactions on Industrial Informatics,"7 Dec 2021","2022","18","3","1949","1956","In this article, we propose SEGA, a secured edge gateway microservices architecture for industrial Internet of things-based monitoring of machines in industries. SEGA allows the secured collection, transmission, and temporary storage of data within the edge network. A k-nearest neighbors-based analytics module hosted on the edge gateway processes time-sensitive machine monitoring data on the gateway itself and identifies machines’ operational status. The system predicts the machine state and displays the monitored parameters such as current consumed, power factor, power consumption, and vibrational state of machinery. SEGA also enables secured offloading of data and advanced analytical functions from the edge gateway to the cloud. SEGA's deployment results show negligible changes in the edge gateway's performance due to the inclusion of various security and encryption mechanisms. However, the resource-constrained edge sensor nodes show an increase in wireless packet transmission latencies between them and the gateway by approximately 84.12 ms.","1941-0050","","10.1109/TII.2021.3102158","SERB/IMPRINT-2(grant numbers:SERB/F/12680/2018-2019,IMP/2018/000451); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9507367","Edge intelligence;edge networks;industrial IoT (IIoT) gateway;machine monitoring;microservices","Logic gates;Industrial Internet of Things;Security;Computer architecture;Monitoring;Cloud computing;Encryption","","20","","19","IEEE","4 Aug 2021","","","IEEE","IEEE Journals"
"Big Data Analytics for Event Detection in the IoT-Multicriteria Approach","J. Granat; J. M. Batalla; C. X. Mavromoustakis; G. Mastorakis","National Institute of Telecommunications, Warsaw, Poland; Warsaw University of Technology, Warsaw, Poland; University of Nicosia, Nicosia, Cyprus; Hellenic Mediterranean University, Heraklion, Greece",IEEE Internet of Things Journal,"13 May 2020","2020","7","5","4418","4430","Security requirements applicable to the Internet of Things (IoT) should aim to ensure integrity, authenticity and authorization, confidentiality/privacy, nonrepudiation, and last but not least, availability. Classic data analysis algorithms are no longer valid for assuring security at all levels and a new approach to data sciences is required, which would consider the complex heterogeneous nature of the IoT, taking also into consideration, its potential to deploy cross layer for security assessment mechanisms. Furthermore, data collected from sensors should be processed and analyzed nearly in real time. The classical algorithms have two main drawbacks: 1) they deal with unidimensional data and 2) they fail to assume limited information available in the stream data processing. In this article, new solutions are discussed and presented that detect anomalies in data streams nearly in real time. Specifically, we propose: 1) an event detection method used in unidimensional data streams and relying on the event strength function, which is an extension of the typical “True or False” decision-making scheme; 2) multiple-criteria event detection approaches based on the Dynamic Pareto Set, introducing a time-depending decision set; and 3) an anomaly detection method based on multicriteria temporal graphs, combining the dynamics of decision making and multicriteria. All the proposed algorithms are presented by means of their formal description and are illustrated with examples.","2327-4662","","10.1109/JIOT.2019.2957320","National Science Centre, Poland, through the Context-Aware Adaptation Framework for eMBB Services in 5G Networks(grant numbers:2018/30/E/ST7/00413); National Centre of Research and Development, Poland(grant numbers:CYBERSECIDENT/369195/I/NCBR/2017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8920092","Anomaly detection;data mining;Internet of Things (IoT) security;multicriteria decision making","Security;Internet of Things;Sensors;Logic gates;Event detection;Data analysis","","20","","42","IEEE","3 Dec 2019","","","IEEE","IEEE Journals"
"Virtualized Control Over Fog: Interplay Between Reliability and Latency","H. Inaltekin; M. Gorlatova; M. Chiang","Department of Electrical and Electronic Engineering, University of Melbourne, Parkville, VIC, Australia; Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA; School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA",IEEE Internet of Things Journal,"16 Jan 2019","2018","5","6","5030","5045","This paper introduces an analytical framework to investigate optimal design choices for the placement of virtual controllers along the cloud-to-things continuum. The main application scenarios include low-latency cyber-physical systems in which real-time control actions are required in response to the changes in states of an Internet of Things (IoT) node. In such cases, deploying controller software on a cloud server is often not tolerable due to delay from the network edge to the cloud. Hence, it is desirable to trade reliability with latency by moving controller logic closer to the network edge. Modeling the IoT node as a dynamical system that evolves linearly in time with quadratic penalty for state deviations, recursive expressions for the optimum control policy and the resulting minimum cost value are obtained by taking virtual fog controller reliability and response time latency into account. Our results indicate that latency is more critical than reliability in provisioning virtualized control services over fog endpoints, as it determines the swiftness of the fog control system as well as the timeliness of state measurements. Based on a drone trajectory tracking model, an extensive simulation study is also performed to illustrate the influence of reliability and latency on the control of autonomous vehicles over fog.","2327-4662","","10.1109/JIOT.2018.2881202","Comcast Innovation Fund Research Grant; AWS Cloud Credits for Research; Microsoft(grant numbers:NSF CSR-1812797); National Science Foundation(grant numbers:1759652); NSF NeTS Award(grant numbers:1759656); Defense Advanced Research Projects Agency(grant numbers:HR001117C0052,HR001117C0048); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8533367","Control;distributed systems;fog computing;Internet of Things (IoT);latency;reliability","Edge computing;Cloud computing;Drones;Computer architecture;Software reliability;Control systems","","20","","77","IEEE","13 Nov 2018","","","IEEE","IEEE Journals"
"Performance Evaluation of Containerization in Edge-Cloud Computing Stacks for Industrial Applications: A Client Perspective","Y. Liu; D. Lan; Z. Pang; M. Karlsson; S. Gong","Department of Science and Technology, Linköping University, Norrköping, Sweden; Department of Informatics, University of Oslo, Oslo, Norway; ABB AB, Corporate Research, Forskargränd 7, Västerås, Västmanland, Sweden; Department of Science and Technology, Linköping University, Norrköping, Sweden; Department of Science and Technology, Linköping University, Norrköping, Sweden",IEEE Open Journal of the Industrial Electronics Society,"19 Feb 2021","2021","2","","153","168","Today, the edge-cloud computing paradigm starts to gain increasing popularity, aiming to enable short latency, fast decision-making and intelligence at the network edge, especially for industrial applications. The container-based virtualization technology has been put on the roadmap by the industry to implement edge-cloud computing infrastructures. Has the performance of the container-based edge-cloud computing stacks reached industry requirement? In this paper, from the industrial client perspective, we provide a performance evaluation methodology and apply it to the state-of-the-art containerization-based edge-cloud computing infrastructures. The influences of the message sending interval, payload, network bandwidth and concurrent devices on full stack latency are measured, and the processing capability of executing machine learning tasks are benchmarked. The results show that containerization on the edge does not introduce noticeable performance degradation in terms of communication, computing and intelligence capabilities, making it a promising technology for the edge-cloud computing paradigm. However, there is a large room for performance improvement between current implementation of the edge-cloud infrastructure and the demanding requirements anticipated by time-critical industrial applications. We also emphasize and showcase that partitioning of an industrial application into microservices throughout the whole stack can be considered during solution design. The proposed evaluation methodology can be a reference to users of edge-cloud computing as well as developers to get a client perspective overview of system performance.","2644-1284","","10.1109/OJIES.2021.3055901","Swedish Innovation Agency, Vinnova; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9343777","Performance evaluation;edge-cloud computing;containerization;partitioning","Cloud computing;Performance evaluation;Computational modeling;Containers;Protocols;Industrial electronics;Computer architecture","","20","","57","CCBY","1 Feb 2021","","","IEEE","IEEE Journals"
"Micro-Safe: Microservices- and Deep Learning-Based Safety-as-a-Service Architecture for 6G-Enabled Intelligent Transportation System","C. Roy; R. Saha; S. Misra; K. Dev","Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Institute of Intelligent Systems, University of Johannesburg, Auckland Park, South Africa",IEEE Transactions on Intelligent Transportation Systems,"11 Jul 2022","2022","23","7","9765","9774","In this paper, we propose a microservices and deep learning-based scheme, termed as Micro-Safe, for provisioning Safety-as-a-Service (Safe-aaS) in a 6G environment. A Safe-aaS infrastructure provides customized safety-related decisions dynamically to the registered end-users. As the decisions are time-sensitive in nature, the generation of these decisions should incur minimum latency and high accuracy. Further, scalability and extension of the coverage of the entire Safe-aaS platform are also necessary. Considering road transportation as the application scenario, we propose Safe-aaS, which is a microservices- and deep learning-based platform for provisioning ultra-low latency safety services to the end-users in a 6G scenario. We design the proposed solution in two stages. In the first stage, we develop the microservices-enabled application layer to improve the scalability and adaptability of the traditional Safe-aaS platform. Moreover, we apply the state space model to represent the decision parameters requested and the decision delivered to the end-users. During the second stage, we use deep learning models to improve the accuracy in the decisions delivered to the end-users. Additionally, we apply an assortment of activation functions to analyze and compare the accuracy of the decisions generated in the proposed scheme. Extensive simulation of our proposed scheme, Micro-Safe, demonstrates that latency is improved by 26.1 – 31.2%, energy consumption is reduced by 22.1 – 29.9%, throughput is increased by 26.1 – 31.7%, compared to the existing schemes.","1558-0016","","10.1109/TITS.2021.3110725","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9548710","6G;microservices;deep learning;state space model;safety-as-a-service (safe-aas);road transportation;decision parameters","6G mobile communication;Safety;Deep learning;Scalability;Delays;Cloud computing;Throughput","","20","","28","IEEE","27 Sep 2021","","","IEEE","IEEE Journals"
"Federated Learning-Empowered Mobile Network Management for 5G and Beyond Networks: From Access to Core","J. Lee; F. Solat; T. Y. Kim; H. V. Poor","Department of Computing, Gachon University, Seongnam, Republic of Korea; Department of Computing, Gachon University, Seongnam, Republic of Korea; Network Intelligence Research Section, Electronics and Telecommunications Research Institute, Daejeon, Republic of Korea; Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ, USA",IEEE Communications Surveys & Tutorials,"22 Aug 2024","2024","26","3","2176","2212","The fifth generation (5G) and beyond wireless networks are envisioned to provide an integrated communication and computing platform that will enable multipurpose and intelligent networks driven by a growing demand for both traditional end users and industry verticals. This evolution will be realized by innovations in both core and access capabilities, mainly from virtualization technologies and ultra-dense networks, e.g., software-defined networking (SDN), network slicing, network function virtualization (NFV), multi-access edge computing (MEC), terahertz (THz) communications, etc. However, those technologies require increased complexity of resource management and large configurations of network slices. In this new milieu, with the help of artificial intelligence (AI), network operators will strive to enable AI-empowered network management by automating radio and computing resource management and orchestration processes in a data-driven manner. In this regard, most of the previous AI-empowered network management approaches adopt a traditional centralized training paradigm where diverse training data generated at network functions over distributed base stations associated with MEC servers are transferred to a central training server. On the other hand, to exploit distributed and parallel processing capabilities of distributed network entities in a fast and secure manner, federated learning (FL) has emerged as a distributed AI approach that can enable many AI-empowered network management approaches by allowing for AI training at distributed network entities without the need for data transmission to a centralized server. This article comprehensively surveys the field of FL-empowered mobile network management for 5G and beyond networks from access to the core. Specifically, we begin with an introduction to the state-of-the-art of FL by exploring and analyzing recent advances in FL in general. Then, we provide an extensive survey of AI-empowered network management, including background on 5G network functions, mobile traffic prediction, and core/access network management regarding standardization and research activities. We then present an extensive survey of FL-empowered network management by highlighting how FL is adopted in AI-empowered network management. Important lessons learned from this review of AI and FL-empowered network management are also provided. Finally, we complement this survey by discussing open issues and possible directions for future research in this important emerging area.","1553-877X","","10.1109/COMST.2024.3352910","National Research Foundation of Korea (NRF); Korea Government (MSIT)(grant numbers:2021R1F1A1048098); ICT Research and Development Program of MSICT/IITP (Development of Intelligent 6G Mobile Core Network Technologies)(grant numbers:2022-0-00862); U.S. National Science Foundation(grant numbers:CNS-2128448,ECCS-2335876); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10400810","Federated learning;5G;6G;network management;artificial intelligence;machine learning","Surveys;6G mobile communication;Servers;Artificial intelligence;Internet of Things;5G mobile communication;Training","","20","","222","IEEE","16 Jan 2024","","","IEEE","IEEE Journals"
"Service Management for IoT: Requirements, Taxonomy, Recent Advances and Open Research Challenges","A. I. A. Ahmed; A. Gani; S. H. A. Hamid; A. Abdelmaboud; H. J. Syed; R. A. A. Habeeb Mohamed; I. Ali","Center for Mobile Cloud Computing Research, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia; Center for Mobile Cloud Computing Research, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia; Center for Mobile Cloud Computing Research, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia; Department of Information Systems, King Khalid University, Muhayil Asir, Saudi Arabia; Department of Computer Science, National University of Computer and Emerging Sciences, Karachi, Pakistan; Center for Mobile Cloud Computing Research, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia; Center for Mobile Cloud Computing Research, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia",IEEE Access,"1 Nov 2019","2019","7","","155472","155488","The exponential growth in the number of smart devices connected to the Internet of Things (IoT), and associated with various IoT-based smart applications and services, raises interoperability challenges which could affect the sustainability of IoT services. IoT software applications are built using different software platforms and embedded in diverse types of terminals and sensing devices. Aiming to offer smart services over a range of network technologies that use different communication protocols. The concept of Web service with service-oriented solutions was introduced to cope with the heterogeneity of hardware and software, and to tackle issues of interoperability, flexibility and scalability. The main step of this solution was the integration of Web of Things technologies into smart device networks, with the utilization of IoT gateways. Service management is a crucial factor in sustaining service-oriented solutions in dynamic and highly scalable IoT systems, and is concerned with several issues associated with service provisioning, orchestration, composition and adaption. This work was motivated by the need for robust and flexible service management systems that can meet the requirements for the rapid scalability and heterogeneity associated with the exponential growth of IoT systems. In the literature there is no survey of service management issues and associated research efforts in the field of IoT. In this article, we identify the key requirements for managing IoT services as well as common service management platforms for IoT. We provide a thematic taxonomy based on the important factors, and investigate recent advances in service management for IoT systems. Finally, the major challenges that remain open are presented as a guide for future research directions.","2169-3536","","10.1109/ACCESS.2019.2948027","King Khalid University(grant numbers:R.G.P-319-2019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8874971","Internet of Things;web service;web of things;SOA;microservice;service composition;service orchestration","Internet of Things;Protocols;Interoperability;Web services;Security;Metadata","","20","","80","CCBY","17 Oct 2019","","","IEEE","IEEE Journals"
"Organic 6G Networks: Vision, Requirements, and Research Approaches","M. -I. Corici; F. Eichhorn; R. Bless; M. Gundall; D. Lindenschmitt; B. Bloessl; M. Petrova; L. Wimmer; R. Kreuch; T. Magedanz; H. D. Schotten","Fraunhofer Institute for Open Communication Systems (FOKUS), Berlin, Germany; Fraunhofer Institute for Open Communication Systems (FOKUS), Berlin, Germany; Institute of Telematics (TM), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; German Research Center for Artificial Intelligence GmbH (DFKI), Kaiserslautern, Germany; Institute for Wireless Communication and Navigation (WICON), RPTU Kaiserslautern-Landau, Kaiserslautern, Germany; Secure Mobile Networking Lab (SEEMOO) Technische Universität Darmstadt (TUDa), Darmstadt, Germany; Chair and Institute of Networked Systems (iNETS), RWTH Aachen University, Aachen, Germany; Leibniz Institute for High Performance Microelectronics (IHP), Frankfurt (Oder), Germany; Architekturen der Vermittlungsknoten (AV), Technische Universität Berlin (TUB), Berlin, Germany; Fraunhofer Institute for Open Communication Systems (FOKUS), Berlin, Germany; German Research Center for Artificial Intelligence GmbH (DFKI), Kaiserslautern, Germany",IEEE Access,"19 Jul 2023","2023","11","","70698","70715","Building upon the significant number of already published 6G position papers, we are concentrating on the immediate next steps toward turning the research vision of software-centric networks into reality. This is accomplished, by summarizing and assessing the various requirements documents and providing a significant number of specific research directions and approaches in order to fulfill them. This article complements the existing body of work, by focusing on future core networks and their infrastructures, yet maintaining a system-level perspective and progressing in the direction of scoping key technology elements and providing high-potential research approaches for them. Additionally, we rigorously discuss the impact that different technological advancements have on the other parts of the system, to provide a coherent, end-to-end network understanding. This is in strong contrast to current approaches, where from the challenges, each research direction becomes independent and, thus, its advances are potentially cancelled out by the next technology in the chain. By maintaining this system perspective, the adoption of the different technologies becomes easier, as they are developed in unison. To address the requirements in a coherent, holistic, and unified way, we extend our high-level architecture concept named “Organic 6G Networks” towards a comprehensive end-to-end system. A holistic software-centric system, adapting the latest software development advancements from the IT industry. The Organic 6G network provides support for building a streamlined software network architecture and offers the next step on the path towards the development and specification of future mobile networks.","2169-3536","","10.1109/ACCESS.2023.3293055","German Federal Ministry for Education and Research (BMBF) through the Project Open6GHub(grant numbers:16KISK003K,16KISK004,16KISK006,16KISK010,16KISK013,16KISK014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10175535","Mobile networks;6G;core networks;organic core networks;organic networks","6G mobile communication;5G mobile communication;Computer architecture;Software engineering;Network architecture;Wireless communication;Complexity theory;Organic electronics","","20","","45","CCBY","7 Jul 2023","","","IEEE","IEEE Journals"
"Mobility-Aware IoT Application Placement in the Cloud – Edge Continuum","D. Kimovski; N. Mehran; C. E. Kerth; R. Prodan","Institute of Information Technology, University of Klagenfurt, Klagenfurt am Wörthersee, Austria; Institute of Information Technology, University of Klagenfurt, Klagenfurt am Wörthersee, Austria; Institute of Information Technology, University of Klagenfurt, Klagenfurt am Wörthersee, Austria; Institute of Information Technology, University of Klagenfurt, Klagenfurt am Wörthersee, Austria",IEEE Transactions on Services Computing,"14 Dec 2022","2022","15","6","3358","3371","The Edge computing extension of the Cloud services towards the network boundaries raises important placement challenges for IoT applications running in a heterogeneous environment with limited computing capacities. Unfortunately, existing works only partially address this challenge by optimizing a single or aggregate objective (e.g., response time), and not considering the edge devices’ mobility and resource constraints. To address this gap, we propose a novel mobility-aware multi-objective IoT application placement (mMAPO) method in the Cloud – Edge Continuum that optimizes completion time, energy consumption, and economic cost as conflicting objectives. mMAPO utilizes a Markov model for predictive analysis of the Edge device mobility and constrains the optimization to devices that do not frequently move through the network. We evaluate the quality of the mMAPO placements using simulation and real-world experimentation on two IoT applications. Compared to related work, mMAPO reduces the economic cost by 28 percent and decreases the completion time by 80 percent while maintaining a stable energy consumption.","1939-1374","","10.1109/TSC.2021.3094322","European Commission(grant numbers:101016835); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9473013","Cloud;edge continuum;mobility;application placement;multi-objective optimization;energy consumption;cost","Internet of Things;Cloud computing;Energy consumption;Optimization;Predictive models;Markov processes;Economics","","19","","46","CCBY","2 Jul 2021","","","IEEE","IEEE Journals"
"ChainFaaS: An Open Blockchain-Based Serverless Platform","S. Ghaemi; H. Khazaei; P. Musilek","Department of Electrical and Computer Engineering, University of Alberta, Edmonton, Alberta, Canada; Department of Electrical Engineering and Computer Science, York University, Toronto, Canada; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, Alberta, Canada",IEEE Access,"24 Jul 2020","2020","8","","131760","131778","Due to the rapid increase in the total amount of data generated in the world, the need for more computational resources is also increasing dramatically. This trend results in huge data centers and massive server farms being built around the world, which have a negative impact on global carbon emissions. On the other hand, there are many underutilized personal computers around the world that can be used towards distributed computing. To better understand the capacity of personal computers, we have conducted a survey that aims to find their unused computational power. The results indicate that the typical CPU utilization of a personal computer is only 24.5% and, on average, a personal computer is only used 4.5 hours per day. This shows a significant computational potential that can be used towards distributed computing. In this paper, we introduce ChainFaaS with the motivation to use the computational capacity of personal computers as well as to improve developers' experience of internet-based computing services by reducing their costs, enabling transparency, and providing reliability. ChainFaaS is an open, public, blockchain-based serverless platform that takes advantage of personal computers' computational capacity to run serverless tasks. If a substantial number of personal computers were connected to this platform, some tasks could be offloaded from data centers. As a result, the need for building new data centers would be reduced with a positive impact on the environment. We have proposed the design of ChainFaaS, and then implemented and evaluated a prototype of this platform to show its feasibility.","2169-3536","","10.1109/ACCESS.2020.3010119","Future Energy Systems Research Initiative under the Canada First Research Excellence Fund (CFREF) at the University of Alberta, Canada; Compute Canada; SAVI Testbed Cloud; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9143110","Blockchain;distributed ledger technology;serverless computing;green computing;distributed computing","Microcomputers;Cloud computing;Task analysis;Data centers;Servers;Carbon dioxide","","19","","58","CCBY","17 Jul 2020","","","IEEE","IEEE Journals"
"Performance Evaluation of a Satellite Communication-Based MEC Architecture for IoT Applications","M. Luglio; M. Marchese; F. Patrone; C. Roseti; F. Zampognaro","Electronics Department, University of Rome “Tor Vergata,”, Rome, Italy; Electrical, Electronics and Telecommunication Engineering and Naval Architecture Department (DITEN), University of Genoa, Genoa, Italy; Electrical, Electronics and Telecommunication Engineering and Naval Architecture Department (DITEN), University of Genoa, Genoa, Italy; Electronics Department, University of Rome “Tor Vergata,”, Rome, Italy; Electronics Department, University of Rome “Tor Vergata,”, Rome, Italy",IEEE Transactions on Aerospace and Electronic Systems,"10 Oct 2022","2022","58","5","3775","3785","New scenarios and use cases are raising following the birth of the fifth generation of mobile communications. The Internet of Things (IoT) is one of the main use cases which are growing, leading to a massive amount of data that need to be exchanged throughout the Internet. Satellite communication networks are essential in remote and isolated environments and can support fully connected environments by offloading the terrestrial infrastructure concerning delay–tolerant traffic flows. However, satellite network resources are limited and expensive, so they need to be carefully used in order to avoid waste and satisfy the required user performance. The multi-access edge computing (MEC) concept can be exploited in this context to allow data preprocessing at the edge, i.e., close to the users, so reducing the amount of data that has to traverse the backhaul satellite link and, in some cases, reducing data delivery times. This article analyses the performance of a satellite architecture in the IoT framework highlighting the advantages brought by MEC, also including data aggregation and compression techniques.","1557-9603","","10.1109/TAES.2022.3199330","ESA; VIBeS project(grant numbers:4000122991/18/U.K./ND); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858607","Internet of Things (IoT);Message Queuing Telemetry Transport (MQTT);multi-access edge computing (MEC);satellite communications (SatCom)","Satellites;Internet of Things;Protocols;Satellite broadcasting;Computer architecture;Performance evaluation;Bandwidth","","19","","42","CCBY","17 Aug 2022","","","IEEE","IEEE Journals"
"A Collaborative Optimization Strategy for Energy Reduction in Ironmaking Digital Twin","H. Zhou; C. Yang; Y. Sun","College of Control Science and Engineering, Zhejiang University, Hangzhou, China; College of Control Science and Engineering, Zhejiang University, Hangzhou, China; College of Control Science and Engineering, Zhejiang University, Hangzhou, China",IEEE Access,"6 Oct 2020","2020","8","","177570","177579","The expenses of pig iron manufacturing stay in a relatively high level due to the large consumption of coke and ore. There are a variety of methods to optimize the ironmaking process, including multi-indexes method, production-oriented approach, and cost control strategy. In order to reduce the production cost, we optimize the raw materials supply schedule of an ironmaking plant with five sintering machines and seven blast furnaces. The ironmaking plant used to fix the feeding system, leading to an inefficient operation and mismatched connection between these two procedures. At first, the physical characteristics of each blast furnace is captured by linear regression model. Then, a self-adaptive genetic algorithm with variable population size is constructed under distribution system. Finally, the improved genetic algorithm is applied to optimize the total production cost represented by the aggregation of seven coke ratios on a cloud computing platform. With the application of this cloud collaborative optimization framework, the mean coke ratio of the ironmaking factory has decreased 13.96kg/t Fe.","2169-3536","","10.1109/ACCESS.2020.3027544","National Natural Science Foundation of China(grant numbers:61933015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9208798","Multiple devices;cost control;collaborative optimization","Sociology;Statistics;Cloud computing;Optimization;Genetic algorithms;Production facilities;Genetics","","19","","35","CCBYNCND","29 Sep 2020","","","IEEE","IEEE Journals"
"Statistical Analysis of Remote Health Monitoring Based IoT Security Models & Deployments From a Pragmatic Perspective","K. Ashok; S. Gopikrishnan","School of Computer Science and Engineering, VIT-AP University, Amaravati, Andhra Pradesh, India; School of Computer Science and Engineering, VIT-AP University, Amaravati, Andhra Pradesh, India",IEEE Access,"11 Jan 2023","2023","11","","2621","2651","Remote health monitoring-based Internet of Things (IoT) network security is a multi-domain task, that involves identification of network attack, evaluation of mitigation strategies, design of performance aware data security models, integration of privacy models, and modeling of device-level security methods. Internal designs for each of these models is highly complex, and varies in terms of quantitative & qualitative performance measures. This is due to their variation in terms of design nuances, functional advantages, context-based limitations, and possible deployment-specific future scopes. Due to this variation, it is highly ambiguous to select these models for performance-specific IoT deployments. Moreover, these models also vary in terms of security level, Quality of Service (QoS) parameters, scalability performance, computational complexity, deployment costs, and other performance metrics. Thus, to identify optimum models, researchers & network designers are required to test & validate multiple security models for their deployments. Due to which, the cost & time to market for IoT devices is increased, thereby affecting viability of IoT products. To overcome these selection issues, an empirical survey of different IoT security models including block-chains, encryption techniques, hashing models, privacy preservation techniques, machine learning based security methods, etc. are discussed in this text. This text also discusses various attack mitigation models that provide node-level security, network-level security, physical security, & route-level security. This discussion will assist in initially evaluating different operating characteristics of these models, which will allow readers to identify most suited models for their application-specific use cases. This article also assesses the models’ performance in terms of computational latency, energy consumption, security levels, deployment complexity, and scalability measures. These metrics are compared between different security models, which will further assist readers to identify optimum models for their performance-specific use cases. To further assist in model selection, this text proposes evaluation of a novel IoT Security Performance Rank (ISRP), that combines various performance metrics to form a singular rank which can be used to describe overall performance of these models. Readers will be able to consider optimal security approaches for new and current IoT installations based on this ranking.","2169-3536","","10.1109/ACCESS.2023.3234632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10007827","IoT;security;blockchain;QoS;medical signal detection;energy;attacks;data;route;physical;privacy","Security;Medical services;Internet of Things;Computational modeling;Blockchains;Data models;Monitoring;Quality of service;Signal detection;Remote monitoring","","19","","137","CCBYNCND","5 Jan 2023","","","IEEE","IEEE Journals"
"A Systematic Literature Review Toward a Blockchain Benchmarking Framework","M. Touloupou; M. Themistocleous; E. Iosif; K. Christodoulou","Department of Digital Innovation, University of Nicosia, Nicosia, Cyprus; Department of Digital Innovation, University of Nicosia, Nicosia, Cyprus; Department of Digital Innovation, University of Nicosia, Nicosia, Cyprus; Department of Digital Innovation, University of Nicosia, Nicosia, Cyprus",IEEE Access,"12 Jul 2022","2022","10","","70630","70644","Blockchain is a disruptive technology that focuses on the safe exchange of data between several distributed applications. Despite its widespread adoption, there are areas that require further research towards the understanding of their performance characteristics. In addition, consensus algorithms, a vital part of blockchain, require a more comprehensive understanding of their technical principles and characteristics. Along with the design of different types of consensus algorithms, several challenges, such as system scalability and power consumption, have been raised. Therefore, more comprehensive research is needed to investigate the degree to which consensus algorithms are built and how they perform. To this end, this study extends the body of knowledge and contributes towards the assessment of blockchain protocol performance. We present a comprehensive taxonomy of selected studies on blockchain performance, identifying similarities and differences while attempting to identify existing work on simulators and benchmarking frameworks that aim to explore the performance of blockchain-enabled consensus algorithms.","2169-3536","","10.1109/ACCESS.2022.3188123","University Blockchain Research Initiative (UBRI) Project; Ripple’s Impact Fund, a fund of the Silicon Valley Community Foundation(grant numbers:2021-244121); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9813702","Blockchain;distributed ledgers;consensus;blockchain performance","Blockchains;Bibliographies;Systematics;Protocols;Consensus algorithm;Libraries;Planning","","19","","83","CCBY","4 Jul 2022","","","IEEE","IEEE Journals"
"Adoption of Blockchain With 5G Networks for Industrial IoT: Recent Advances, Challenges, and Potential Solutions","M. Kaur; M. Z. Khan; S. Gupta; A. Alsaeedi","Department of Computer Science and Engineering, University Institute of Engineering, Chandigarh University, Gharuan, Punjab, Mohali, India; Department of Computer Science and Information, Taibah University, Medina, Saudi Arabia; Department of Computer Science and Engineering, University Institute of Engineering, Chandigarh University, Gharuan, Punjab, Mohali, India; Department of Computer Science, College of Computer Science and Engineering, Taibah University, Medina, Saudi Arabia",IEEE Access,"6 Jan 2022","2022","10","","981","997","It has been proven that Internet of Things (IoT) platforms can improve the performance and efficiency of a wide range of processes. With the acceptance of IoT as a major part of the technology of Industry 4.0, the notion of leveraging the Internet in industries to enable automation and reconfigure existing industrial processes has greatly evolved. By introducing smart technology and intelligent processes, the Industrial Internet of Things (IIoT) is committed to bringing high operational efficiency, enhanced productivity, and effective management to industrial assets. Despite this, the reliance of IIoT on central architecture presents numerous challenges, including the security and maintenance of smart devices, privacy issues owing to third-party participation, and massive computations conducted by a central entity, all of which prevent its widespread adoption in businesses. Emerging blockchain technologies have the potential to transform IIoT platforms and applications. A distributed and decentralized approach followed by blockchain might offer interesting solutions to the challenges raised by IIoT. Furthermore, 5G networks are expected to deliver excellent solutions to meet the demands of decentralized systems, with a focus on application-specific vulnerabilities. Blockchain and IIoT, enabled by 5G, is a viable option to fully explore the potential of contemporary industry. In this context, this article analyzes and examines recent achievements to highlight the major obstacles in blockchain–IIoT convergence and presents a framework for potential solutions. A well-organized literature review by analyzing the existing work in three primary areas: blockchain consensus algorithms used in existing IoT and IIoT applications, blockchain for 5G-enabled IoT networks, and blockchain in industry have been performed, with major findings summarized in each area. Directions for the future are also provided and intend to assist researchers in understanding the full potential of these innovations.","2169-3536","","10.1109/ACCESS.2021.3138754","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9663288","Blockchain;IIoT;Industry 4.0;IoT;5G","Industrial Internet of Things;Blockchains;Fourth Industrial Revolution;Computer architecture;Internet;Computer science","","19","","52","CCBY","27 Dec 2021","","","IEEE","IEEE Journals"
"Refining Microservices Placement Employing Workload Profiling Over Multiple Kubernetes Clusters","J. Han; Y. Hong; J. Kim","School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology (GIST), Gwangju, South Korea; School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology (GIST), Gwangju, South Korea; AI Graduate School, Gwangju Institute of Science and Technology (GIST), Gwangju, South Korea",IEEE Access,"28 Oct 2020","2020","8","","192543","192556","As cloud-native computing is becoming the de-facto paradigm in the cloud field, Microservices Architecture has attracted attention from industries and researchers for agility and efficiency. Moreover, with the popularity of the IoT in the context of edge computing, cloud-native applications that utilize geographically-distributed multiple resources are emerging. In line with this trend, there is an increasing demand for microservices placement that selectively use optimal resources. However, optimal microservices placement is a significant challenge because microservices are dynamic and complex, depending on diversified workloads. Besides, generalizing workloads' characteristics consisting of complex microservices is realistically challenging. Thus, microservices deployment with mathematically structured algorithms based on simulation is less practical. As an alternative, a microservices placement framework is required that can reflect the characteristics of workloads derived from empirical profiling. Therefore, in this research work, we propose a refinement framework for profiling-based microservices placement to identify and respond to workload characteristics in a practical way. To achieve this goal, we perform profiling experiments with selected workloads to derive delicate resource requirements. Then, we perform microservices placement with a greedy-based heuristic algorithm that considers application performance by using resource requirements derived from the profiled results. Finally, we verify the proposed concept by comparing the experimental results that use our work and those that don't.","2169-3536","","10.1109/ACCESS.2020.3033019","Vehicles AI Convergence Research and Development Program through the National IT Industry Promotion Agency of Korea (NIPA); Ministry of Science and ICT (MSIT)(grant numbers:S1605-20-1002); Institute of Information and Communications Technology Planning and Evaluation (IITP); Korean Government, MSIT, Artificial Intelligence Graduate School Program (GIST)(grant numbers:2019-0-01842); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9235316","Cloud-native computing;microservices placement;workload profiling;container orchestration;resource monitoring","Cloud computing;Containers;Virtual machining;Resource management;Monitoring;Optimization;Refining","","19","","37","CCBY","22 Oct 2020","","","IEEE","IEEE Journals"
"PP5GS—An Efficient Procedure-Based and Stateless Architecture for Next-Generation Core Networks","E. Goshi; R. Stahl; H. Harkous; M. He; R. Pries; W. Kellerer","Chair of Communication Networks, Technical University of Munich, Munich, Germany; Chair of Communication Networks, Technical University of Munich, Munich, Germany; Nokia Standards Department, Nokia, Munich, Germany; Nokia Standards Department, Nokia, Munich, Germany; Nokia Standards Department, Nokia, Munich, Germany; Chair of Communication Networks, Technical University of Munich, Munich, Germany",IEEE Transactions on Network and Service Management,"9 Oct 2023","2023","20","3","3318","3333","The introduction of the Service-Based Architecture (SBA) for the 5G Core Networks has drastically changed the way these networks are designed and operated. Aiming for higher flexibility and agility, the adoption of SBA is the first step towards cloud-native deployments of 5G Core. However, the high degree of functional decomposition in SBA has implications in terms of increased inter-NF signaling traffic during the execution of control plane procedures, as well as an increased complexity in orchestrating a system with tight inter-NF dependencies. In this work, we introduce PP5GS as a stateless 5G Core architecture that implements a procedure-based functional decomposition of the 5G Core NFs. We develop Per-Procedure NFs for four different control plane procedures and perform extensive evaluations in a private cloud environment orchestrated with Kubernetes. The results show that PP5GS requires up to 34% and 55% less computing resources compared to the baseline stateful and stateless systems, respectively, while generating at least 40% less signaling traffic. Moreover, complex control plane procedures can complete up to 50% faster. Lastly, the results show that PP5GS is a more feasible architecture in leveraging edge-offloading of 5G Core NFs.","1932-4537","","10.1109/TNSM.2022.3230206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9991849","5G core;SBA;control plane;performance measurements","5G mobile communication;Cloud computing;Scalability;Computer architecture;Noise measurement;Task analysis;Protocols","","19","","42","IEEE","19 Dec 2022","","","IEEE","IEEE Journals"
"Towards a Service-Oriented Architecture for the Energy Efficiency of Buildings: A Systematic Review","L. Mendoza-Pitti; H. Calderón-Gómez; M. Vargas-Lombardo; J. M. Gómez-Pulido; J. L. Castillo-Sequera","Department of Computer Science, University of Alcala, Alcala de Henares, Spain; Department of Computer Science, University of Alcala, Alcala de Henares, Spain; E-health and Supercomputing Research Group (GISES), Technological University of Panama, Panama, Panama; Department of Computer Science, University of Alcala, Alcala de Henares, Spain; Department of Computer Science, University of Alcala, Alcala de Henares, Spain",IEEE Access,"15 Feb 2021","2021","9","","26119","26137","Currently, smart buildings generate large amounts of data due to the many devices and equipment available. Hence, buildings implement building management systems (BMSs), which monitor, control, manage and analyze each of these components. However, current BMSs are incapable of managing a massive amount of data (big data) and therefore cannot extract knowledge or make intelligent decisions in quasi real time. In addition, there are serious limitations to integrating BMSs with other services since they generally use proprietary software. In this sense, service-oriented architecture (SOA) is an architectural style that allows one to build distributed systems and provide functionalities such as services to end users or other types of services. Therefore, an SOA has the great advantage of allowing the expansion of the functionalities of BMSs. In fact, there are several studies that address SOAs for building management. However, we have not found any description or systematic analysis in the literature that allows the development of a versatile and interoperable SOA focused on the energy efficiency of buildings and that can integrate massive data analysis features. For these reasons, this study seeks to fill this knowledge gap and, more specifically, to identify and analyze the various software requirements proposed in the literature and the characteristics of big data that allow for improving the energy efficiency of buildings. To this end, we performed an in-depth review of the literature according to the methodology proposed by Kitchenham. As a result of this review, we provide researchers with a specific vision of the requirements and characteristics to consider for software development aimed at the energy efficiency of unique or historic buildings.","2169-3536","","10.1109/ACCESS.2021.3057543","National Secretariat for Science, Technology and Innovation (SENACYT, Panama) and the Institute for the Training and Use of Human Resources (IFARHU); SENACYT through the National Research System (SNI-SENACYT) National Research System; ‘‘Smart Energy’’ Campus of International Excellence; Universidad de Alcalá and the Universidad Rey Juan Carlos (Spain); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9348934","Big data applications;buildings;energy efficiency;energy management;management information systems;service-oriented systems engineering;software architecture","Buildings;Energy efficiency;Service-oriented architecture;Software;Interoperability;Energy consumption;Big Data","","18","","138","CCBY","5 Feb 2021","","","IEEE","IEEE Journals"
"Diktyo: Network-Aware Scheduling in Container-Based Clouds","J. Santos; C. Wang; T. Wauters; F. De Turck","Department of Information Technology, IDLab, Ghent University-imec, Gent, Belgium; AI Cloud Platform Department, IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; Department of Information Technology, IDLab, Ghent University-imec, Gent, Belgium; Department of Information Technology, IDLab, Ghent University-imec, Gent, Belgium",IEEE Transactions on Network and Service Management,"12 Dec 2023","2023","20","4","4461","4477","Containers have revolutionized application deployment and life-cycle management in current cloud platforms. Applications have evolved from single monoliths to complex graphs of loosely-coupled microservices. However, the efficient allocation of microservice-based applications is challenging due to their complex inter-dependencies. Further, recent applications are becoming even more delay-sensitive, demanding lower latency between dependent microservices. Scheduling policies in popular container orchestration platforms mainly aim to increase the resource efficiency of the infrastructure, insufficient for latency-sensitive applications. Application domains such as the Internet of Things and multi-tier Web services would benefit from network-aware policies that consider network latency and bandwidth in the scheduling process. Previous works have studied network-aware scheduling via theoretical formulations or heuristic-based methods evaluated via simulations or small testbeds, making their full applicability in popular platforms difficult. This paper proposes a novel network-aware framework for the popular Kubernetes (K8s) platform named Diktyo that determines the placement of dependent microservices in long-running applications focused on reducing the application’s end-to-end latency and guaranteeing bandwidth reservations. Simulations show that Diktyo can significantly reduce the network latency for various applications across different infrastructure topologies compared to default K8s scheduling plugins. Also, experiments in a K8s cluster with microservice benchmark applications show that Diktyo can increase database throughput by 22% and reduce application response time by 45%.","1932-4537","","10.1109/TNSM.2023.3271415","Research Foundation Flanders (FWO)(grant numbers:1299323N); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10111024","Microservices;container scheduling;Kubernetes;network-aware","Microservice architectures;Containers;Network topology;Topology;Bandwidth;Resource management;Cloud computing","","18","","99","IEEE","28 Apr 2023","","","IEEE","IEEE Journals"
"Modeling Analysis and Cost-Performance Ratio Optimization of Virtual Machine Scheduling in Cloud Computing","B. Wan; J. Dang; Z. Li; H. Gong; F. Zhang; S. Oh","Department of Computer Science and Technology, Xidian University, Xi'an, China; Department of Computer Science and Technology, Xidian University, Xi'an, China; Key Laboratory of Hunan Province for Internet of Things and Information Security and College of Information Engineering, Xiangtan University, Xiangtan, China; College of Mathematics and Statistics, Changsha University of Science and Technology, Changsha, China; DEKE Lab, School of Information, Renmin University of China, Beijing, China; Department of Computer and Information Engineering, Ajou University, Suwon, South Korea",IEEE Transactions on Parallel and Distributed Systems,"20 Feb 2020","2020","31","7","1518","1532","As an essential feature of cloud computing, dynamic scalability enables the cloud system to dynamically expand or shrink resources according to user needs at runtime. Effectively predicting and optimizing the cost and performance of cloud computing platforms have become one of the key research challenges in the field of cloud computing. In this article, to quantitatively predict the cost and performance of cloud computing platforms, we propose a cloud computing resource analysis model considering both hot/cold startup and hot/cold shutdown of virtual machines (VMs), and use the M/M/N/oo queuing model to analyze cloud computing platform and acquire accurate performance indicators, such as elasticity indicators, cost indicators, performance indicators, cost-performance ratios, etc. In addition, we establish a multi-objective optimization model to optimize both performance and cost of cloud computing platform. Then the optimal stopping and cost-performance optimization algorithm are applied to obtain the optimal configurations, including the number of hot startup VMs, the system service rate, the hot/cold startup rate of VMs, and the hot/cold shutdown rate. By comparing with existing optimization methods, we demonstrate the superiority of our cost-performance ratio optimization method.","1558-2183","","10.1109/TPDS.2020.2968913","Science and Technology Projects of Xi'an, China(grant numbers:201809170CX11JC12); National Natural Science Foundation of China(grant numbers:61972302); Key Research and Development Program of Shanxi Provence(grant numbers:2017ZDXM-GY-002); Hunan Provincial Natural Science Foundation of China for Distinguished Young Scholars(grant numbers:2018JJ1025); Hunan Science and Technology Planning(grant numbers:2019RS3019); National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2018YFB1003702); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8967018","Cloud computing;queuing system;cost-performance ratio optimization","Cloud computing;Optimization;Computational modeling;Analytical models;Task analysis;Queueing analysis;Elasticity","","18","","43","IEEE","23 Jan 2020","","","IEEE","IEEE Journals"
"A Comprehensive Survey on Auction Mechanism Design for Cloud/Edge Resource Management and Pricing","N. Sharghivand; F. Derakhshan; N. Siasi","Department of Computer Engineering, University of Tabriz, Tabriz, Iran; Department of Computer Engineering, University of Tabriz, Tabriz, Iran; Department of Physics, Computer Science and Engineering, Christopher Newport University, Newport News, VA, USA",IEEE Access,"17 Sep 2021","2021","9","","126502","126529","Driven by the vision of cloud and edge computing as the 5th utility, a major challenge is the efficient resource management and pricing taking into account incentives and preferences of cloud/edge providers, users, and the system. In recent years, auction mechanism design has gained wide attention as a tool for addressing this challenge. Here existing surveys lack detailed studies in this area, hence this work presents a comprehensive survey on the state-of-the-art auction-based mechanisms in the field of cloud/edge computing. First, the problem statements and background are presented for cloud/edge computing and auction theory. This is followed by a collaboration model on the benefits of auction mechanism designs to cloud/edge computing. Next, various auction mechanisms in cloud computing are reviewed based upon the following aspects: (1) direction of bids-forward, reverse, and two-sided auctions, (2) the heterogeneity of resources-combinatorial auctions, (3) the number and types of adopted attributes in winner determination-multi-attribute auctions, (4) the setting of auction-offline, online, or sequential auctions, and (5) the interoperability among cloud providers-inter-clouds. For each aspect, existing studies are summarized along with their saliencies and drawbacks. Moreover, existing auction mechanisms in the context of edge computing are also presented. Our comprehensive survey shows that although significant progress has been achieved in this field, there still exist key challenges that need to be investigated, as discussed here to provide future research directions.","2169-3536","","10.1109/ACCESS.2021.3110914","Iran National Science Foundation (INSF); University of Tabriz(grant numbers:97013453); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9530537","Auction mechanism design;cloud computing;edge computing;resource management;resource pricing","Cloud computing;Resource management;Pricing;Computational modeling;Edge computing;Servers;Software as a service","","18","","162","CCBY","7 Sep 2021","","","IEEE","IEEE Journals"
"When Backscatter Communication Meets Vehicular Networks: Boosting Crosswalk Awareness","F. Pereira; H. Sampaio; R. Chaves; R. Correia; M. Luís; S. Sargento; M. Jordão; L. Almeida; C. Senna; A. S. R. Oliveira; N. Borges Carvalho","Instituto de Telecomunicações, Aveiro, Portugal; Instituto de Telecomunicações, Aveiro, Portugal; Instituto de Telecomunicações, Aveiro, Portugal; Instituto de Telecomunicações, Aveiro, Portugal; Instituto de Telecomunicações, Aveiro, Portugal; Instituto de Telecomunicações, Aveiro, Portugal; Instituto de Telecomunicações, Aveiro, Portugal; Instituto de Telecomunicações, Aveiro, Portugal; Instituto de Telecomunicações, Aveiro, Portugal; Instituto de Telecomunicações, Aveiro, Portugal; Instituto de Telecomunicações, Aveiro, Portugal",IEEE Access,"25 Feb 2020","2020","8","","34507","34521","The research of safety applications in vehicular networks has been a popular research topic in an effort to reduce the number of road victims. Advances on vehicular communications are facilitating information sharing through real time communications, critical for the development of driving assistance systems. However, the communication by itself is not enough to reach the most desired target as we need to know which safety-related information should be disseminated. In this work, we bring passive sensors and backscatter communication to the vehicular network world. The idea is to increase the driver (or vehicle) awareness regarding the presence of pedestrians in a crosswalk. Passive sensors and backscatter communication technologies are used for the pedestrians’ detection phase, while the vehicular network is used during the dissemination of the detection information to surrounding vehicles. The proposed solution was validated through end-to-end experimentation, with real hardware and in a real crosswalk with real pedestrians and vehicles, demonstrating its applicability.","2169-3536","","10.1109/ACCESS.2020.2974214","European Regional Development Fund; National Public (FCT) (OE) through Project MobiWise(grant numbers:POCI-01-0145-FEDER-016426); Urban Innovative Actions(grant numbers:UIA03-084); Fundação para a Ciência e a Tecnologia(grant numbers:SFRH/BD/143204/2019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9000613","Vulnerable road users;driver assistance systems;connected vehicles;passive sensors;energy harvesting;backscatter communication;vehicular networks;decentralized message dissemination","Backscatter;Sensors;Vehicles;Safety;Roads;Batteries;Modulation","","18","","40","CCBY","17 Feb 2020","","","IEEE","IEEE Journals"
"Enabling All In-Edge Deep Learning: A Literature Review","P. Joshi; M. Hasanuzzaman; C. Thapa; H. Afli; T. Scully","Computer Science Department, Munster Technological University, Cork, Ireland; Computer Science Department, Munster Technological University, Cork, Ireland; CSIRO Data61, Marsfield, NSW, Australia; Computer Science Department, Munster Technological University, Cork, Ireland; Computer Science Department, Munster Technological University, Cork, Ireland",IEEE Access,"12 Jan 2023","2023","11","","3431","3460","In recent years, deep learning (DL) models have demonstrated remarkable achievements on non-trivial tasks such as speech recognition, image processing, and natural language understanding. One of the significant contributors to the success of DL is the proliferation of end devices that act as a catalyst to provide data for data-hungry DL models. However, computing DL training and inference still remains the biggest challenge. Moreover, most of the time central cloud servers are used for such computation, thus opening up other significant challenges, such as high latency, increased communication costs, and privacy concerns. To mitigate these drawbacks, considerable efforts have been made to push the processing of DL models to edge servers (a mesh of computing devices near end devices). Recently, the confluence point of DL and edge has given rise to edge intelligence (EI), defined by the International Electrotechnical Commission (IEC) as the concept where the data is acquired, stored, and processed utilizing edge computing with DL and advanced networking capabilities. Broadly, EI has six levels of categories based on the three locations where the training and inference of DL take place, e.g., cloud server, edge server, and end devices. This survey paper focuses primarily on the fifth level of EI, called all in-edge level, where DL training and inference (deployment) are performed solely by edge servers. All in-edge is suitable when the end devices have low computing resources, e.g., Internet-of-Things, and other requirements such as latency and communication cost are important such as in mission-critical applications (e.g., health care). Besides, 5G/6G networks are envisioned to use all in-edge. Firstly, this paper presents all in-edge computing architectures, including centralized, decentralized, and distributed. Secondly, this paper presents enabling technologies, such as model parallelism, data parallelism, and split learning, which facilitates DL training and deployment at edge servers. Thirdly, model adaptation techniques based on model compression and conditional computation are described because the standard cloud-based DL deployment cannot be directly applied to all in-edge due to its limited computational resources. Fourthly, this paper discusses eleven key performance metrics to evaluate the performance of DL at all in-edge efficiently. Finally, several open research challenges in the area of all in-edge are presented.","2169-3536","","10.1109/ACCESS.2023.3234761","ADVANCE CRT Ph.D. Cohort(grant numbers:18/CRT/6222); ADAPT SFI Research Centre at Munster Technological University; Science Foundation Ireland through the SFI Research Centres Programme; European Regional Development Fund (ERDF)(grant numbers:13/RC/2106\_P2); Horizon 2020 projects STOP Obesity Platform(grant numbers:823978); ITFLOWS(grant numbers:882986); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10007810","Artificial intelligence;all in-edge;deep learning;distributed systems;decentralized systems;edge intelligence","Cloud computing;Servers;Computational modeling;Training data;Edge computing;Adaptation models;Deep learning;Artificial intelligence;Edge computing;Distributed processing","","18","","287","CCBY","5 Jan 2023","","","IEEE","IEEE Journals"
"An Adaptive Mechanism for Dynamically Collaborative Computing Power and Task Scheduling in Edge Environment","Y. Xu; L. Chen; Z. Lu; X. Du; J. Wu; P. C. K. Hung","School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; Faulty of Business and Information Technology, Ontario Tech University, Oshawa, ON, Canada",IEEE Internet of Things Journal,"6 Feb 2023","2023","10","4","3118","3129","Edge computing can provide high bandwidth and low-latency service for big data tasks by leveraging the edge side’s computing, storage, and network resources. With the development of microservice and docker technology, service providers can flexibly and dynamically cache microservice at the edge side to respond efficiently with limited resources. Automatically caching needed services on the nearest edge nodes and dynamically scheduling users’ requests can realize that computing power and software services flow with the users to provide continuous services. However, achieving the goal needs to overcome many challenges, such as the significant fluctuation of user devices’ requests at the edge side and the lack of collaboration among edge nodes. In this article, dynamic computing power scheduling and collaborative task scheduling among edge nodes are comprehensively developed. The problem is considered a multiobjective optimization problem, including sequentially minimizing the deadline missing rate of requests and the average task completion time. We propose an adaptive mechanism for dynamically collaborative computing power and task scheduling (ADCS) in the edge environment to solve this problem. It adopts the greedy decision method to schedule computing tasks to meet their deadline requirements. At the same time, it uses the best-fit method to adjust the computing resources according to the changes of users’ requests. The simulation results show that ADCS can decrease the deadline missing rate and reduce the average completion time. Compared with DSR and CoDSR, the deadline missing rate is reduced by 59.91% and 19.95%, respectively. The average completion time is decreased by 37.87% and 6.71%.","2327-4662","","10.1109/JIOT.2021.3119181","National Key Research and Development Program of China(grant numbers:2019YFB1405000); National Natural Science Foundation of China(grant numbers:61873309,92046024); Shanghai Science and Technology Innovation Action Plan Project(grant numbers:19510710500,18510732000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9566207","Collaborative task scheduling;edge computing;heterogeneous resources;Internet of Everything (IoE);microservice caching","Task analysis;Processor scheduling;Dynamic scheduling;Collaboration;Scheduling;Cloud computing;Servers","","18","","34","IEEE","11 Oct 2021","","","IEEE","IEEE Journals"
"Resource-Aware Dynamic Service Deployment for Local IoT Edge Computing: Healthcare Use Case","J. Islam; T. Kumar; I. Kovacevic; E. Harjula","Centre for Wireless Communication, University of Oulu, Oulu, Finland; Centre for Wireless Communication, University of Oulu, Oulu, Finland; Centre for Wireless Communication, University of Oulu, Oulu, Finland; Centre for Wireless Communication, University of Oulu, Oulu, Finland",IEEE Access,"26 Aug 2021","2021","9","","115868","115884","Edge Computing is a novel computing paradigm moving server resources closer to end-devices. In the context of IoT, Edge Computing is a centric technology for enabling reliable, context-aware and low-latency services for several application areas such as smart healthcare, smart industry and smart cities. In our previous work, we have proposed a three-tier IoT Edge architecture and a virtual decentralized service platform based on lightweight microservices, called nanoservices, running on it. Together, these proposals form a basis for virtualizing the available local computational capacity and utilizing it to provide localized resource-efficient IoT services based on the applications’ need. Furthermore, locally-deployed functions are resilient to access network problems and can limit the propagation of sensitive user data for improved privacy. In this paper, we propose an automatic service and resource discovery mechanism for efficient on-the-fly deployment of nanoservices on local IoT nodes. As use case, we have selected a healthcare remote monitoring scenario, which requires high service reliability and availability in a highly dynamic environment. Based on the selected use case, we propose a real-world prototype implementation of the proposed mechanism on Raspberry Pi platform. We evaluate the performance and resource-efficiency of the proposed resource matching function with two alternative deployment approaches: containerized and non-containerized deployment. The results show that the containerized deployment is more resource-efficient, while the resource discovery and matching process takes approximately 6–17 seconds, where containerization adds only 1–1.5 seconds. This can be considered a feasible price for streamlined service management, scalability, resource-efficiency and fault-tolerance.","2169-3536","","10.1109/ACCESS.2021.3102867","Academy of Finland, under the projects: DigiHealth and 6G Flagship Projects(grant numbers:326291,318927); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9507488","IoT;edge computing;distributed computing;virtualization;resource discovery;microservices;nanoservices","Cloud computing;Computer architecture;Internet of Things;Edge computing;Dynamic scheduling;Computational modeling;Medical services","","18","","38","CCBY","5 Aug 2021","","","IEEE","IEEE Journals"
"Explainable AI for 6G Use Cases: Technical Aspects and Research Challenges","S. Wang; M. A. Qureshi; L. Miralles-Pechuán; T. Huynh-The; T. R. Gadekallu; M. Liyanage","School of Computer Science, University College Dublin, Dublin 4, Ireland; ADAPT Centre, Explainable Analytics Group, Faculty of Business, Technological University Dublin, Dublin 2, Ireland; School of Computing, Technological University Dublin, Dublin 7, Ireland; Department of Computer and Communications Engineering, Ho Chi Minh City University of Technology and Education, Ho Chi Minh City, Vietnam; Division of Research and Development, Lovely Professional University, Phagwara, India; School of Computer Science, University College Dublin, Dublin 4, Ireland",IEEE Open Journal of the Communications Society,"1 May 2024","2024","5","","2490","2540","Around 2020, 5G began its commercialization journey, and discussions about the next-generation networks (such as 6G) emerged. Researchers predict that 6G networks will have higher bandwidth, coverage, reliability, energy efficiency, and lower latency, and will be an integrated “human-centric” network system powered by artificial intelligence (AI). This 6G network will lead to many real-time automated decisions, ranging from network resource allocation to collision avoidance for self-driving cars. However, there is a risk of losing control over decision-making due to the high-speed, data-intensive AI decision-making that may go beyond designers’ and users’ comprehension. To mitigate this risk, explainable AI (XAI) methods can be used to enhance the transparency of the black-box AI decision-making process. This paper surveys the application of XAI towards the upcoming 6G age, including 6G technologies (such as intelligent radio and zero-touch network management) and 6G use cases (such as industry 5.0). Additionally, the paper summarizes the lessons learned from recent attempts and outlines important research challenges in applying XAI for 6G use cases soon.","2644-125X","","10.1109/OJCOMS.2024.3386872","European Commission in SPATIAL(grant numbers:101021808); Academy of Finland in 6Genesis(grant numbers:318927); Science Foundation Ireland through CONNECT Phase 2 Project(grant numbers:13/RC/2077_P2); ADAPT Centre Phase 2 Project(grant numbers:13/RC/2106_P2); Industry Fellowship(grant numbers:21/IRDIF/9839); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10499970","B5G;6G;AI;XAI;explainability","6G mobile communication;Artificial intelligence;5G mobile communication;Explainable AI;Resource management;Security;Closed box","","18","","303","CCBYNCND","16 Apr 2024","","","IEEE","IEEE Journals"
"A Service-Defined Approach for Orchestration of Heterogeneous Applications in Cloud/Edge Platforms","G. Castellano; F. Esposito; F. Risso","Department of Control and Computer Engineering, Politecnico di Torino, Turin, Italy; Computer Science Department, Saint Louis University, St. Louis, USA; Department of Control and Computer Engineering, Politecnico di Torino, Turin, Italy",IEEE Transactions on Network and Service Management,"10 Dec 2019","2019","16","4","1404","1418","Edge Computing is moving resources toward the network borders, thus enabling the deployment of a pool of new applications that benefit from the new distributed infrastructure. However, due to the heterogeneity of such applications, specific orchestration strategies need to be adopted for each deployment request. Each application can potentially require different optimization criteria and may prefer particular reactions upon the occurrence of the same event. This paper presents a Service-Defined approach for orchestrating cloud/edge services in a distributed fashion, where each application can define its own orchestration strategy by means of declarative statements, which are parsed into a Service-Defined Orchestrator (SDO). Moreover, to coordinate the coexistence of a variety of SDOs on the same infrastructure while preserving the resource assignment optimality, we present DRAGON, a Distributed Resource AssiGnment and OrchestratioN algorithm that seeks optimal partitioning of shared resources between different actors. We evaluate the advantages of our novel Service-Defined orchestration approach over some representative edge use cases, as well as measure convergence and performance of DRAGON on a prototype implementation, assessing the benefits compared to conventional orchestration approaches.","1932-4537","","10.1109/TNSM.2019.2941639","Division of Computer and Network Systems(grant numbers:CNS-1647084,CNS-1836906,CNS-1908574,NSF CNS-1647084); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8839604","Orchestration;mathematical optimization;distributed algorithms;distributed management","Optimization;Streaming media;Computer architecture;Resource management;Cloud computing;Partitioning algorithms;Convergence","","17","","33","IEEE","16 Sep 2019","","","IEEE","IEEE Journals"
"Split Federated Learning for 6G Enabled-Networks: Requirements, Challenges, and Future Directions","H. Hafi; B. Brik; P. A. Frangoudis; A. Ksentini; M. Bagaa","Faculty of New Information and Communication Technologies, Abdelhamid Mehri University, Constantine, Algeria; Computer Science Department, College of Computing and Informatics, Sharjah University, Sharjah, United Arab Emirates; Distributed Systems Group, TU Wien, Vienna, Austria; Communication Systems Department, EURECOM, Sophia-Antipolis, France; Department of Electrical and Computer Engineering, Université du Québec à Trois-Rivières, Trois-Rivières, Canada",IEEE Access,"22 Jan 2024","2024","12","","9890","9930","Sixth-generation (6G) networks anticipate intelligently supporting a wide range of smart services and innovative applications. Such a context urges a heavy usage of Machine Learning (ML) techniques, particularly Deep Learning (DL), to foster innovation and ease the deployment of intelligent network functions/operations, which are able to fulfill the various requirements of the envisioned 6G services. The revolution of 6G networks is driven by massive data availability, moving from centralized and big data towards small and distributed data. This trend has motivated the adoption of distributed and collaborative ML/DL techniques. Specifically, collaborative ML/DL consists of deploying a set of distributed agents that collaboratively train learning models without sharing their data, thus improving data privacy and reducing the time/communication overhead. This work provides a comprehensive study on how collaborative learning can be effectively deployed over 6G wireless networks. In particular, our study focuses on Split Federated Learning (SFL), a technique that recently emerged promising better performance compared with existing collaborative learning approaches. We first provide an overview of three emerging collaborative learning paradigms, including federated learning, split learning, and split federated learning, as well as of 6G networks along with their main vision and timeline of key developments. We then highlight the need for split federated learning towards the upcoming 6G networks in every aspect, including 6G technologies (e.g., intelligent physical layer, intelligent edge computing, zero-touch network management, intelligent resource management) and 6G use cases (e.g., smart grid 2.0, Industry 5.0, connected and autonomous systems). Furthermore, we review existing datasets along with frameworks that can help in implementing SFL for 6G networks. We finally identify key technical challenges, open issues, and future research directions related to SFL-enabled 6G networks.","2169-3536","","10.1109/ACCESS.2024.3351600","European Union’s Horizon 2020 Research and Innovation Program under the Agile and Cognitive Cloud edge Continuum management (AC3) Project(grant numbers:101093129); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385040","6G networks;wireless communication;federated deep learning;split deep learning;split federated learning","6G mobile communication;Federated learning;Artificial intelligence;Surveys;Training;Wireless networks;Market research;Smart devices","","17","","240","CCBYNCND","9 Jan 2024","","","IEEE","IEEE Journals"
"QoE-Driven IoT Architecture: A Comprehensive Review on System and Resource Management","B. Saovapakhiran; W. Naruephiphat; C. Charnsripinyo; S. Baydere; S. Özdemir","National Electronics and Computer Technology Center, NSTDA, Pathumthani, Thailand; National Electronics and Computer Technology Center, NSTDA, Pathumthani, Thailand; National Electronics and Computer Technology Center, NSTDA, Pathumthani, Thailand; Department of Computer Engineering, Yeditepe University, Istanbul, Turkey; Department of Computer Engineering, Hacettepe University, Ankara, Turkey",IEEE Access,"17 Aug 2022","2022","10","","84579","84621","Internet of Things (IoT) services have grown substantially in recent years. Consequently, IoT service providers (SPs) are emerging in the market and competing to offer their services. Many IoT applications utilize these services in an integrated manner with different Quality-of-Service (QoS) requirements. Thus, the provisioning of end-to-end QoS is getting more indispensable for IoT platforms. However, provisioning the system by using only QoS metrics without considering user experiences is not sufficient. Recently, Quality of Experience (QoE) model has become a promising approach to quantify actual user experiences of services. A holistic design approach that considers constraints of various QoS/QoE metrics together is needed to satisfy requirements of these applications and services. Besides, IoT services may operate in environments with limited resources. Therefore, effective management of services and system resources is essential for QoS/QoE support. This paper provides a comprehensive survey for the state-of-the-art studies on IoT services with QoS/QoE perspective. Our contributions are threefold: 1) QoE-driven architecture is demonstrated by classifying vital components according to QoE-related functions in prior studies; 2) QoE metrics and QoE optimization objectives are classified by corresponding system and resource control problems in the architecture; and 3) QoE-aware resource management e.g., QoE-aware offloading, placement and data caching policies with recent Machine Learning approaches are extensively reviewed.","2169-3536","","10.1109/ACCESS.2022.3197585","National Science and Technology Development Agency (NSTDA), Ministry of Higher Education, Science, Research and Innovation, Thailand(grant numbers:P1952667); The Scientific and Technological Research Council of Turkey (TUBITAK), Turkey(grant numbers:119N049); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9852447","Internet of Things;quality of service;quality of experience;IoT services;IoT applications;QoS for IoT services;QoS metrics;QoE metrics;IoT architecture","Quality of experience;Quality of service;Internet of Things;Optimization;Computer architecture;Resource management","","17","","166","CCBY","8 Aug 2022","","","IEEE","IEEE Journals"
"The Security and Privacy of Mobile-Edge Computing: An Artificial Intelligence Perspective","C. Wang; Z. Yuan; P. Zhou; Z. Xu; R. Li; D. O. Wu","Hubei Engineering Research Center on Big Data Security, Key Laboratory of Distributed System Security of Hubei Province, School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; Hubei Engineering Research Center on Big Data Security, Key Laboratory of Distributed System Security of Hubei Province, School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; Hubei Engineering Research Center on Big Data Security, Key Laboratory of Distributed System Security of Hubei Province, School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; School of Software and the Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, Dalian University of Technology, Dalian, China; Intelligent and Distributed Computing Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong",IEEE Internet of Things Journal,"11 Dec 2023","2023","10","24","22008","22032","Mobile-edge computing (MEC) is a new computing paradigm that enables cloud computing and information technology (IT) services to be delivered at the network’s edge. By shifting the load of cloud computing to individual local servers, MEC helps meet the requirements of ultralow latency, localized data processing, and extends the potential of the Internet of Things (IoT) for end-users. However, the crosscutting nature of MEC and the multidisciplinary components necessary for its deployment have presented additional security and privacy concerns. Fortunately, artificial intelligence (AI) algorithms can cope with excessively unpredictable and complex data, which offers a distinct advantage in dealing with sophisticated and developing adversaries in the security industry. Hence, in this article, we comprehensively provide a survey of security and privacy in MEC from the perspective of AI. On the one hand, we use European Telecommunications Standards Institute (ETSI) MEC reference architecture as our-based framework while merging the software-defined network (SDN) and network function virtualization (NFV) to better illustrate a serviceable platform of MEC. On the other hand, we focus on new security and privacy issues, as well as potential solutions from the viewpoints of AI. Finally, we comprehensively discuss the opportunities and challenges associated with applying AI to MEC security and privacy as possible future research directions.","2327-4662","","10.1109/JIOT.2023.3304318","National Natural Science Foundation of China(grant numbers:61972448,62172068,61802048); Hong Kong Research Grants Council, General Research Fund(grant numbers:11203523); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10214503","Artificial intelligence (AI);fifth generation (5G);Internet of Things (IoT);machine learning (ML);mobile-edge computing (MEC);security and privacy;software-defined network (SDN) security;virtual machine security","Security;Privacy;Artificial intelligence;Internet of Things;Cloud computing;Data privacy;Computer architecture","","17","","165","IEEE","11 Aug 2023","","","IEEE","IEEE Journals"
"Performance Modeling of Metric-Based Serverless Computing Platforms","N. Mahmoudi; H. Khazaei","Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada; Department of Electrical Engineering and Computer Science, York University, Toronto, ON, Canada",IEEE Transactions on Cloud Computing,"6 Jun 2023","2023","11","2","1899","1910","Analytical performance models are very effective in ensuring the quality of service and cost of service deployment remain desirable under different conditions and workloads. While various analytical performance models have been proposed for previous paradigms in cloud computing, serverless computing lacks such models that can provide developers with performance guarantees. Besides, most serverless computing platforms still require developers’ input to specify the configuration for their deployment that could affect both the performance and cost of their deployment, without providing them with any direct and immediate feedback. In previous studies, we built such performance models for steady-state and transient analysis of scale-per-request serverless computing platforms (e.g., AWS Lambda, Azure Functions, Google Cloud Functions) that could give developers immediate feedback about the quality of service and cost of their deployments. In this work, we aim to develop analytical performance models for latest trend in serverless computing platforms that use concurrency value and the rate of requests per second for autoscaling decisions. Examples of such serverless computing platforms are Knative and Google Cloud Run (a managed Knative service by Google). The proposed performance model can help developers and providers predict the performance and cost of deployments with different configurations which could help them tune the configuration toward the best outcome. We validate the applicability and accuracy of the proposed performance model by extensive real-world experimentation on Knative and show that our performance model is able to accurately predict the steady-state characteristics of a given workload with minimal amount of data collection.","2168-7161","","10.1109/TCC.2022.3169619","Shared Hierarchical Academic Research Computing Network; Compute Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9763051","Google cloud run;knative;metric-based autoscaling;optimization;performance modelling;serverless computing;stochastic processes","Measurement;Computational modeling;Serverless computing;Concurrent computing;Monitoring;Containers;Analytical models","","17","","50","IEEE","26 Apr 2022","","","IEEE","IEEE Journals"
"Task Scheduling Mechanisms for Fog Computing: A Systematic Survey","M. Hosseinzadeh; E. Azhir; J. Lansky; S. Mildeova; O. H. Ahmed; M. H. Malik; F. Khan","Institute of Research and Development, Duy Tan University, Da Nang, Vietnam; Research and Development Center, Mobile Telecommunication Company of Iran, Tehran, Iran; Department of Computer Science and Mathematics, Faculty of Economic Studies, University of Finance and Administration, Prague, Czech Republic; Department of Computer Science and Mathematics, Faculty of Economic Studies, University of Finance and Administration, Prague, Czech Republic; Department of Information Technology, University of Human Development, Sulaymaniyah, Iraq; School of Computing and Creative Technologies, College of Arts, Technology and Environment (CATE), University of the West of England, Bristol, U.K.; Department of Computer Engineering, Gachon University, Seongnam, South Korea",IEEE Access,"5 Jun 2023","2023","11","","50994","51017","In the Internet of Things (IoT) ecosystem, some processing is done near data production sites at higher speeds without the need for high bandwidth by combining Fog Computing (FC) and cloud computing. Fog computing offers advantages for real-time systems that require high speed internet connectivity. Due to the limited resources of fog nodes, one of the most important challenges of FC is to meet dynamic needs in real-time. Therefore, one of the issues in the fog environment is the optimal assignment of tasks to fog nodes. An efficient scheduling algorithm should reduce various qualitative parameters such as cost and energy consumption, taking into account the heterogeneity of fog nodes and the commitment to perform tasks within their deadlines. This study provides a detailed taxonomy to gain a better understanding of the research issues and distinguishes important challenges in existing work. Therefore, a systematic overview of existing task scheduling techniques for cloud-fog environment, as well as their benefits and drawbacks, is presented in this article. Four main categories are introduced to study these techniques, including machine learning-based, heuristic-based, metaheuristic-based, and deterministic mechanisms. A number of papers are studied in each category. This survey also compares different task scheduling techniques in terms of execution time, resource utilization, delay, network bandwidth, energy consumption, execution deadline, response time, cost, uncertainty, and complexity. The outcomes revealed that 38% of the scheduling algorithms use metaheuristic-based mechanisms, 30% use heuristic-based, 23% use machine learning algorithms, and the other 9% use deterministic methods. The energy consumption is the most significant parameter addressed in most articles with a share of 19%. Finally, a number of important areas for improving the task scheduling methods in the FC in the future are presented.","2169-3536","","10.1109/ACCESS.2023.3277826","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10129024","Fog computing;cloud computing;task scheduling;methods;quality of service","Task analysis;Processor scheduling;Surveys;Edge computing;Dynamic scheduling;Systematics;Costs;Quality of service","","16","","79","CCBYNCND","18 May 2023","","","IEEE","IEEE Journals"
"Dependence-Aware Edge Intelligent Function Offloading for 6G-Based IoV","L. Dong; H. Gao; W. Wu; Q. Gong; N. C. Dechasa; Y. Liu","School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Engineering and Science, Shanghai University, Shanghai, China; Department of Computer Science, The University of Texas at Dallas, Richardson, TX, USA; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; Xi’an High-Tech Research Institute, Xi’an, China",IEEE Transactions on Intelligent Transportation Systems,"8 Feb 2023","2023","24","2","2265","2274","Using the increasingly wireless communication capacity of 5G/6G technology, edge intelligence (EI) enables modern vehicles to leverage the powerful computing resources of edge servers scattered aside the roads to implement intelligent transportation applications (ITA). The running of these intelligent applications is always accompanied by the calculating and transmitting of massive amounts of road situation data. For the protection of drivers and passengers, these complex processes should be handled in an ultra-reliable and low latent manner. Intelligent processing function offloading from terminals to edge servers is a promising approach to address these issues. However, the runtime environment specification of each intelligent processing function and the interdependence between two consecutive functions pose a challenge for the assignment of the offloaded functions among edge servers. In this paper, we propose a dependence-aware edge intelligent function offloading scheme for 6G-based Internet of Vehicle (IoV). All traditional ITAs are split into different chains of standard intelligent functions. Each edge server can provide some specific intelligent functional services. These services can receive data from cars and serve as different intelligent functions. Then, an intelligent application offloading scheme is changed into an embedding scheme of a service chain. An NP-hard objective function is constructed using a multi-winner committee selection model for this offloading service chain embedding problem. We design two algorithms to get the optimal assignment of intelligent functions using a greedy strategy and dynamic programming strategy separately. Finally, experiments show that when the proportion of vehicles meeting the constraint conditions is not in [9%, 10%], our algorithms are fast.","1558-0016","","10.1109/TITS.2022.3148229","National Science Foundation(grant numbers:1747818); Fundamental Research Funds for Central Universities(grant numbers:JB161004); National Science and Technology Major Project of the Ministry of Science and Technology of China(grant numbers:2009ZX03001-003); Project of “Fake Transaction Identification Algorithm Based on Big Data”; Xi’an Yi Shu Technology Company Ltd; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9709183","IoV;edge intelligence;multi-winner committee selection;dynamic programming;offloading","Face recognition;Heuristic algorithms;Servers;Computer architecture;Automobiles;Edge computing;5G mobile communication","","16","","28","IEEE","9 Feb 2022","","","IEEE","IEEE Journals"
"SDN-Based Multi-Tier Computing and Communication Architecture for Pervasive Healthcare","A. C. Baktir; C. Tunca; A. Ozgovde; G. Salur; C. Ersoy","NETLAB, Computer Networks Research Laboratory, Bogazici University, Istanbul, Turkey; NETLAB, Computer Networks Research Laboratory, Bogazici University, Istanbul, Turkey; Department of Computer Engineering, Galatasaray University, Istanbul, Turkey; Movement Disorders & Dementia Clinic, Istanbul, Turkey; NETLAB, Computer Networks Research Laboratory, Bogazici University, Istanbul, Turkey",IEEE Access,"21 Oct 2018","2018","6","","56765","56781","A large spectrum of healthcare applications, ranging from continuous blood sugar level monitoring to sleep apnea detection are nowadays facilitated by modern mobile gadgets. Wearable and ambient sensors generate enormous amounts of physiological data that demand high computation power for real-time processing and large storage area for recording the personal data. In order to conserve the energy on the battery-limited mainstream mobile devices of the end-users, the execution of the healthcare applications may be offloaded to a remote server. While cloud computing provides unlimited pool of resources for latency-tolerant services such as training a machine learning model, the personalization of healthcare services and the delay sensitivity of continuous health assessment necessitate a computation infrastructure in the vicinity of the end-users. As a remedy to address various demands of a wide range of pervasive healthcare applications, we propose a multi-tier computing and communication architecture composed of end-user devices, edge servers, and legacy cloud data-centers. The dynamic management of this architecture, policies to be applied within the network and the orchestration of the healthcare services are carried out by the concept of programmable networks, in the form of software-defined networking (SDN). As a concrete demonstration of our ideas, a fall risk assessment service is implemented and an experimental study is conducted to evaluate its accuracy and the performance of the multi-tier architecture. The results indicate that the proposed architecture is feasible to enable real-time healthcare services and has significant performance advantages over traditional cloud-based approaches.","2169-3536","","10.1109/ACCESS.2018.2873907","Bogazici University Research Fund(grant numbers:12663); State Planning Organization of Turkey through the TAM Project(grant numbers:2007K120610); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8482108","Edge computing;fall risk assessment;pervasive healthcare;software-defined networking;wearable computing","Medical services;Computer architecture;Edge computing;Biomedical monitoring;Servers;Cloud computing;Monitoring","","16","","44","OAPA","5 Oct 2018","","","IEEE","IEEE Journals"
"Introducing Cloud-Assisted Micro-Service-Based Software Development Framework for Healthcare Systems","J. Zaki; S. M. R. Islam; N. S. Alghamdi; M. Abdullah-Al-Wadud; K. -S. Kwak","Department of Computer and Systems, Faculty of Engineering, Mansoura University, Mansoura, Egypt; Department of Computer Science and Engineering, Sejong University, Seoul, South Korea; Department of Computer Sciences, College of Computer and Information Sciences, Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia; Department of Software Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Department of Information and Communication Engineering, Inha University, Incheon, South Korea",IEEE Access,"31 Mar 2022","2022","10","","33332","33348","In healthcare services, application development is considered the most complex and time-consuming phase. As it is difficult to plan and time-intense, it requires high maintenance. Healthcare applications need strict compliance and the scope of application is immense along with associates, classes in services, and classified system. Application designing in healthcare with the help of traditional approaches such as monolithic and service-oriented architecture (SOA) generate problems in different areas like service availability, remote access to services, service provisioning, scalability, healthcare systems integration with each other. That is why there is a need for less sophisticated and user-friendly healthcare systems, which are easy to plan and develop, inexpensive requirement maintenance, and agile testing. To overcome the aforesaid issues in the domain of healthcare application development, this paper develops a framework of micro services for the development of healthcare services using cloud computing infrastructure. Micro-service-based techniques provide lightly coupled and fine-grained methodology. With the use of micro services technique presented in this work, the efficiency, scalability, and performance are improved. In this research, an approach for development and deployment properly in the cloud for healthcare applications is developed. Thus, it contributes to the system design approach and system analysis. Quantitative and qualitative results are reported showing the advantages of micro services approach used.","2169-3536","","10.1109/ACCESS.2022.3161455","National Research Foundation of Korea-Grant funded by the Korean Government (Ministry of Science)(grant numbers:ICT-NRF-2020R1A2B5B02002478); Sejong university through its faculty research program(grant numbers:20212023); Princess Nourah bint Abdulrahman University Researchers Supporting Project(grant numbers:PNURSP2022R40); Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9739683","Service oriented architecture;micro services;cloud computing;healthcare;application design;monolith approach;application software","Medical services;Microservice architectures;Monitoring;Cloud computing;Costs;Electrocardiography;Computer architecture","","16","","41","CCBY","22 Mar 2022","","","IEEE","IEEE Journals"
"Service Based Virtual RAN Architecture for Next Generation Cellular Systems","E. Zeydan; J. Mangues-Bafalluy; J. Baranda; M. Requena; Y. Turk","Centre Tecnològic de Telecomunicacions de Catalunya, Castelldefels, Barcelona, Spain; Centre Tecnològic de Telecomunicacions de Catalunya, Castelldefels, Barcelona, Spain; Centre Tecnològic de Telecomunicacions de Catalunya, Castelldefels, Barcelona, Spain; Centre Tecnològic de Telecomunicacions de Catalunya, Castelldefels, Barcelona, Spain; Aselsan Corporation, Istanbul, Turkey",IEEE Access,"27 Jan 2022","2022","10","","9455","9470","Service based architecture (SBA) is a paradigm shift from Service-Oriented Architecture (SOA) to microservices, combining their principles. Network virtualization enables the application of SBA in cellular systems. To better guide the software design of this virtualized cellular system with SBA, this paper presents a software perspective and a positional approach to using fundamental development principles for adapting SBA in virtualized Radio Access Networks (vRANs). First, we present the motivation for using an SBA in cellular radio systems. Then, we explore the critical requirements, key principles, and components for the software to provide radio services in SBA. We also explore the potential of applying SBA-based Radio Access Network (RAN) by comparing the functional split requirements of 5G RAN with existing open-source software and accelerated hardware implementations of service bus, and discuss the limitations of SBA. Finally, we present some discussions, future directions, and a roadmap of applying such a high-level design perspective of SBA to next-generation RAN infrastructure.","2169-3536","","10.1109/ACCESS.2022.3144534","European Union (EU) H2020 5GROWTH Project(grant numbers:856709); Generalitat de Catalunya(grant numbers:2017 SGR 1195); National Program on Equipment and Scientific and Technical Infrastructure under the European Regional Development Fund (FEDER)(grant numbers:EQC2018-005257-P); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9684900","Service-based architecture;network interfaces;radio access networks;software","Computer architecture;Software;Radio access networks;5G mobile communication;Cloud computing;Protocols;Hardware","","16","","51","CCBY","18 Jan 2022","","","IEEE","IEEE Journals"
"Inverse Queuing Model-Based Feedback Control for Elastic Container Provisioning of Web Systems in Kubernetes","Z. Cai; R. Buyya","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Cloud Computing and Distributed Systems (CLOUDS) Laboratory, School of Computing and Information Systems, University of Melbourne, Parkville, VIC, Australia",IEEE Transactions on Computers,"13 Jan 2022","2022","71","2","337","348","Container orchestration platforms such as Kubernetes and Kubernetes-derived KubeEdge (called Kubernetes-based systems collectively) have been gradually used to conduct unified management of Cloud, Fog, and Edge resources. Container provisioning algorithms are crucial to guaranteeing quality of services (QoS) of such Kubernetes-based systems. However, most existing algorithms focus on placement and migration of fixed number of containers without considering elastic provisioning of containers. Meanwhile, widely used linear-performance-model-based feedback control or fixed-processing-rate-based queuing model on diverse platforms cannot describe the performance of containerized Web systems accurately. Furthermore, a fixed reference point used by existing methods is likely to generate inaccurate output errors incurring great fluctuations encountered with large arrival-rate changes. In this article, a feedback control method is designed based on a combination of varying-processing-rate queuing model and linear-model to provision containers elastically which improves the accuracy of output errors by learning reference models for different arrival rates automatically and mapping output errors from reference models to the queuing model. Our approach is compared with several state-of-art algorithms on a real Kubernetes cluster. Experimental results illustrate that our approach obtains the lowest percentage of service level agreement (SLA) violation (decreasing no less than 8.44 percent) and the second lowest cost.","1557-9956","","10.1109/TC.2021.3049598","National Natural Science Foundation of China(grant numbers:61972202); Fundamental Research Funds for the Central Universities(grant numbers:30919011235); China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9314906","Cloud;fog and edge computing;kubernetes;container auto-scaling;Qos control;queuing theory;feedback control","Containers;Adaptation models;Computational modeling;Feedback control;Time factors;Quality of service;Cloud computing","","16","","39","IEEE","6 Jan 2021","","","IEEE","IEEE Journals"
"QoS-Aware Co-Scheduling for Distributed Long-Running Applications on Shared Clusters","J. Zhu; R. Yang; X. Sun; T. Wo; C. Hu; H. Peng; J. Xiao; A. Y. Zomaya; J. Xu","Beihang University, Beijing, China; School of Computing, University of Leeds, Leeds, U.K.; School of Computing, University of Leeds, Leeds, U.K.; Beihang University, Beijing, China; Beihang University, Beijing, China; Beihang University, Beijing, China; Alibaba Group, Hangzhou, Zhejiang, China; University of Sydney, Camperdown, NSW, Australia; School of Computing, University of Leeds, Leeds, U.K.",IEEE Transactions on Parallel and Distributed Systems,"29 Sep 2022","2022","33","12","4818","4834","To achieve a high degree of resource utilization, production clusters need to co-schedule diverse workloads – including both batch analytic jobs with short-lived tasks and long-running applications (LRAs) that execute for a long time frame from hours to months – onto the shared resources. Microservice architecture advances the manifestation of distributed LRAs (DLRAs), comprising multiple interconnected microservices that are executed in long-lived distributed containers and serve massive user requests. Detecting and mitigating QoS violation become even more intractable due to the network uncertainties and latency propagation across dependent microservices. However, current resource managers are only responsible for resource allocation among applications/jobs but agnostic to runtime QoS such as latency at application level. The state-of-the-art QoS-aware scheduling approaches are dedicated for monolithic applications, without considering the temporal-spatio performance variability across distributed microservices. In this paper, we present Toposch, a new scheduling and execution framework to prioritize the QoS of DLRAs whilst balancing the performance of batch jobs and maintaining high cluster utilization through harvesting idle resources. Toposch tracks footprints of every single request across microservices and uses critical path analysis, based on the end-to-end latency graph, to identify microservices that have high risk of QoS violation. Based on microservice and node level risk assessment, we intervene the batch scheduling by adaptively reducing the visible resources to batch tasks and thus delaying their execution to give way to DLRAs. We propose a prediction-based vertical resource auto-scaling mechanism, with the aid of resource-performance modeling and fine-grained resource inference and access control, for prompt recovery of QoS violation. A cost-effective task preemption is leveraged to ensure a low-cost task preemption and resource reclamation during the auto-scaling. Toposch is integrated with Apache YARN and experiments show that Toposch outperforms other baselines in terms of performance guarantee of DLRAs, at an acceptable cost of batch job slowdown. The tail latency of DLRAs is merely 1.12x of the case of executing alone on average in Toposch with a 26% JCT increase of Spark analytic jobs.","1558-2183","","10.1109/TPDS.2022.3202493","MIIT of China(grant numbers:2105-370171-07-02-860873); S&T Program of Hebei(grant numbers:20310101D); Fundamental Research Funds for the Central Universities(grant numbers:20226941); UK EPSRC(grant numbers:EP/T01461X/1); Alan Turing Pilot Project; Alan Turing PDEA Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9869329","Resource scheduling;cluster management;QoS;tail latency;datacenters","Quality of service;Microservice architectures;Task analysis;Runtime;Containers;Resource management;Databases","","15","","65","CCBY","29 Aug 2022","","","IEEE","IEEE Journals"
"Cost-Effective Web Application Replication and Deployment in Multi-Cloud Environment","T. Shi; H. Ma; G. Chen; S. Hartmann","School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; Department of Informatics, Clausthal University of Technology, Clausthal-Zellerfeld, Germany",IEEE Transactions on Parallel and Distributed Systems,"30 Dec 2021","2022","33","8","1982","1995","Multi-cloud is becoming a popular cloud ecosystem because it allows enterprise users to share the workload across multiple cloud service providers to achieve high-quality services with lower operation cost and higher application resilience. In multi-cloud, cloud services are widely distributed at different locations with differentiated prices. Therefore, Web application providers face the challenge to select proper cloud services for application replication and deployment with the aim of minimizing the deployment cost. Meanwhile, the deployed application replicas must satisfy the constraint on request response time to maintain the quality of user experience. To meet the two major requirements, this article studies a new problem of Web application replication and deployment in multi-cloud (WARDMC) that jointly considers both the cost minimization and constraints on average response time, including particularly request processing time and network latency. To address the problem, we develop a new approach named MCApp. MCApp combines iterative mixed integer linear programming with domain-tailored large neighborhood search to optimize both application replicas deployment and user requests dispatching. Extensive experiments using the real-world datasets demonstrate that MCApp significantly outperforms several recently proposed approaches.","1558-2183","","10.1109/TPDS.2021.3133884","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645306","Multi-cloud;Web application deployment;service replication;cost optimization;mixed integer linear programming;large neighborhood search","Cloud computing;Costs;Time factors;Optimization;Upper bound;Pricing;User experience","","15","","59","IEEE","10 Dec 2021","","","IEEE","IEEE Journals"
"Traffic Routing-Based Computation Offloading in Cybertwin-Driven Internet of Vehicles for V2X Applications","B. Ma; Z. Ren; W. Cheng","State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China",IEEE Transactions on Vehicular Technology,"20 May 2022","2022","71","5","4551","4560","With the rapid development of the Internet of Vehicles (IoV), a lot of Vehicle-to-Everything (V2X) applications have sprung up. To tackle the conflict between the resource-hungry V2X applications and the resource-constrained vehicles, most works focus on the computation offloading problem, which is significant to V2X applications by bringing computation tasks from the vehicles to the edge or cloud infrastructure. However, the dynamic network conditions caused by the mobility of vehicles will bring task migration and huge additional costs, resulting in poor latency performance. Motivated by the aforementioned problem, a traffic routing-based computation offloading scheme in cybertwin-driven IoV for V2X applications is proposed, in which cybertwin represents the network hardware devices and the network software functions. Moreover, according to the cybertwin-driven IoV network architecture, the traffic routing-based computation offloading problem is formulated. Finally, to avoid the inconsistency between the data transmission direction and the vehicle’s movement direction, the enhanced Heterogeneous Earliest Finish Time (eHEFT) algorithm is designed, which introduces the gradient routing into the traditional HEFT algorithm. Performance evaluation results validate that the proposed joint optimization scheme is indeed capable of reducing latency.","1939-9359","","10.1109/TVT.2021.3134715","National Key R&D Program of China(grant numbers:2019YFB1803301); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9647922","V2X applications;computation offloading;routing;latency","Task analysis;Vehicle-to-everything;Routing;Cloud computing;Computational modeling;Heuristic algorithms;Costs","","15","","39","IEEE","13 Dec 2021","","","IEEE","IEEE Journals"
"Development of a Novel IoT-Enabled Power- Monitoring Architecture With Real-Time Data Visualization for Use in Domestic and Industrial Scenarios","A. R. Jadhav; S. Kiran M. P. R.; R. Pachamuthu","Department of Electrical Engineering, IIT Hyderabad, Hyderabad, India; Department of Electrical Engineering, IIT Hyderabad, Hyderabad, India; Department of Electrical Engineering, IIT Hyderabad, Hyderabad, India",IEEE Transactions on Instrumentation and Measurement,"16 Nov 2020","2021","70","","1","14","With the increased interest in smart cities and smart infrastructures, the need for energy conservation is increasing. Especially with the current electrical energy production mainly relying on nonrenewable resources, conservation of electrical energy is one of the challenging aspects across the globe. However, one can only perform energy conservation optimally by identifying consumption patterns at a granular level, which requires accurate and ubiquitous monitoring infrastructure. Because the electrical energy wastage can occur at any granularity (from a small house-hold appliance to grid-level wastage), the development of a low-cost, easy-to-install, and accurate power-monitoring infrastructure is need of the hour. Hence, in this article, we propose the developed designs for IoT-enabled power monitoring. First is the noninvasive power monitor with voltage connection. The second design introduces a novel split architecture with centralized voltage measurement, which removes the need for local voltage measurements. We have proposed the third and final version of the IoT-enabled power monitor to fulfill the need for three-phase power monitoring. Unlike first and second designs, this design can be used with noninvasive and invasive current sensors. The proposed architecture also supports essential features, such as secure data transfer. Developed devices transmit real-time data to the cloud server, which makes the data ubiquitously available anywhere and anytime. For analyzing the performance of the proposed architecture, the developed devices are deployed in real industrial scenarios. As an example use case, the electrical anomaly detection framework using the data collected is also explained, and the corresponding results are discussed.","1557-9662","","10.1109/TIM.2020.3028437","Visvesvaraya Ph.D. Scheme, MeitY, Government of India(grant numbers:MEITY-PHD-1115); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9211481","Anomaly detection;fault detection;IoT;operation optimization;power monitoring;smart-buildings","Monitoring;Sensors;Voltage measurement;Real-time systems;Data visualization;Current measurement;Systems architecture","","15","","29","IEEE","2 Oct 2020","","","IEEE","IEEE Journals"
"A Scaling Mechanism for an Evolved Packet Core Based on Network Functions Virtualization","C. H. T. Arteaga; F. B. Anacona; K. T. T. Ortega; O. M. C. Rendon","Departamento de Telemática, Grupo de Ingenier??a Telem??tica, Universidad del Cauca, Popayán, Colombia; Departamento de Telemática, Grupo de Ingenier??a Telem??tica, Universidad del Cauca, Popayán, Colombia; Departamento de Telemática, Grupo de Ingenier??a Telem??tica, Universidad del Cauca, Popayán, Colombia; Departamento de Telemática, Grupo de Ingenier??a Telem??tica, Universidad del Cauca, Popayán, Colombia",IEEE Transactions on Network and Service Management,"10 Jun 2020","2020","17","2","779","792","The workload variations affect the performance of mobile networks. The scaling task is pivotal for addressing these variations. In the literature, research works have incorporated horizontal or vertical scaling in the virtualized network functions of the Evolved Packet Core (EPC) to improve its performance. However, up to now, these works exploit only horizontal or vertical scaling for achieving their aim. In this paper, we propose a scaling mechanism that utilizes horizontal and vertical scaling and considers workload variations for improving performance in EPC. This mechanism is threshold-based, straightforward, and implementable in real LTE-EPC scenarios. We also develop a mechanism prototype and deploy it in a real public cloud. In this cloud, we conduct a prototype evaluation, regarding registrations per second, latency, CPU, and RAM, and considering a varying workload. The evaluation results reveal that our mechanism increases the registrations per second about 308% and decreases the corresponding latency approximately 70% regarding an EPC without scaling while keeping the CPU usage lower than 90% and the used capacity of registrations per second between 65% and 90%. These results corroborate the importance of used both horizontal and vertical scaling to improve EPC performance, handle workload variations, and save resources.","1932-4537","","10.1109/TNSM.2019.2961988","University of Cauca and the Administrative Department of Science, Technology and Innovation (COLCIENCIAS)???Colombia through the Ph.D. Scholarship(grant numbers:527-2015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8941016","Scaling;evolved packet core;4G;network functions virtualization","Scalability;Cloud computing;Synchronization;Prototypes;Long Term Evolution;Network function virtualization;Authentication","","15","","36","IEEE","24 Dec 2019","","","IEEE","IEEE Journals"
"Proof of Sense: A Novel Consensus Mechanism for Spectrum Misuse Detection","P. Fernando; K. Dadallage; T. Gamage; C. Seneviratne; A. Madanayake; M. Liyanage","Faculty of Engineering, Vrije Universiteit Brussel, Brussels, Belgium; Department of Electrical and Information Engineering, University of Ruhuna, Galle, Sri Lanka; Department of Electrical and Information Engineering, University of Ruhuna, Galle, Sri Lanka; Department of Electrical and Information Engineering, University of Ruhuna, Galle, Sri Lanka; Department of Electrical and Computer Engineering, Florida International University (FIU), Miami, FL, USA; School of Computer Science, University College Dublin, Dublin 4, Ireland",IEEE Transactions on Industrial Informatics,"30 Sep 2022","2022","18","12","9206","9216","Optimal use of scarce radio spectrum is essential in the proliferation of beyond 5G networks, and promising blockchain technology offers various benefits for the spectrum management. However, existing blockchain-based solutions are expensive, nonoptimized, and lack spectrum fraud detection. This article proposes a novel consensus mechanism for a blockchain-based dynamic spectrum access (DSA) system. The proposed “Proof-of-Sense” consensus mechanism operates based on spectrum sensing procedures rather than cryptographic calculations. It is specially designed to address fraudulent/unauthorized access to the spectrum by analyzing the sensed spectrum data. The core of the consensus mechanism is a cryptographic key sharing mechanism inspired by Shamir’s secret sharing scheme. Moreover, the proposed DSA system can enable different microservices, such as automated spectrum auctions, payment and penalty handling, and spectrum fraud detection. A proof of concept based on experimental approaches coupled with Matlab simulations is presented to analyze the performance of the proposed consensus mechanism.","1941-0050","","10.1109/TII.2022.3169978","framework of 6Genesis Flagship(grant numbers:318927); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762480","Blockchain;consensus algorithm;dynamic spectrum access (DSA);spectrum management;spectrum misuse;spectrum sensing;smart contracts (SC)","Blockchains;Sensors;Regulators;Stakeholders;Informatics;Wireless sensor networks;Radio spectrum management","","15","","24","CCBYNCND","25 Apr 2022","","","IEEE","IEEE Journals"
"Lattice: A Vision for Machine Learning, Data Engineering, and Policy Considerations for Digital Agriculture at Scale","S. Chaterji; N. DeLay; J. Evans; N. Mosier; B. Engel; D. Buckmaster; M. R. Ladisch; R. Chandra","Department of Agricultural and Biological Engineering, Purdue University, West Lafayette, IN, USA; Department of Agricultural and Biological Engineering, Purdue University, West Lafayette, IN, USA; Department of Agricultural and Biological Engineering, Purdue University, West Lafayette, IN, USA; Department of Agricultural and Biological Engineering, Purdue University, West Lafayette, IN, USA; Department of Agricultural and Biological Engineering, Purdue University, West Lafayette, IN, USA; Department of Agricultural and Biological Engineering, Purdue University, West Lafayette, IN, USA; Department of Agricultural and Biological Engineering, Purdue University, West Lafayette, IN, USA; Microsoft Research, Microsoft Azure, Redmond, WA, USA",IEEE Open Journal of the Computer Society,"2 Jul 2021","2021","2","","227","240","Digital agriculture, with the incorporation of Internet-of-Things (IoT)-based technologies, presents the ability to control a system at multiple levels (individual, local, regional, and global) and generates tools that allow for improved decision making and higher productivity. Recent advances in IoT hardware, e.g., networks of heterogeneous embedded devices, and software, e.g., lightweight computer vision algorithms and cloud optimization solutions, make it possible to efficiently process data from diverse sources in a connected (smart) farm. By interconnecting these IoT devices, often across large geographical distances, it is possible to collect data at different time scales, including in near real-time (i.e., with delays of only a few tens of seconds). This data can then be used for actionable insights, e.g., precise applications of soil supplements and reduced environmental footprint. Through LATTICE, we present an integrated vision for IoT solutions, data processing, and actionable analytics for digital agriculture. We couple this with discussion of economics and policy considerations that will underlie adoption of such IoT and ML technologies. Our paper starts off with the types of datasets in typical field operations, followed by the lifecycle for the data and storage, cloud and edge analytics, and fast information-retrieval solutions. We discuss what algorithms are proving to be most impactful in this space, e.g., approximate data analytics and on-device/in-network processing. We conclude by discussing analytics for alternative agriculture for generation of biofuels and policy challenges in the implementation of digital agriculture in the wild.","2644-1268","","10.1109/OJCS.2021.3085846","Purdue University; Lilly Endowment; Microsoft Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9444818","Data integration;data analysis;internet of things;Sensor systems;cloud computing","Data integration;Soil;Digital agriculture;Fertilizers;Distributed databases;Cloud computing;Internet of Things;Data analysis;Intelligent sensors","","15","","72","CCBY","1 Jun 2021","","","IEEE","IEEE Journals"
"Applications of Blockchain in Business Processes: A Comprehensive Review","W. Viriyasitavat; L. D. Xu; D. Niyato; Z. Bi; D. Hoonsopon","Chulalongkorn Business School, Chulalongkorn University, Bangkok, Thailand; Department of Information Technology and Decision Sciences, Old Dominion University, Norfolk, VA, USA; School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; Department of Civil and Mechanical Engineering, Purdue University Fort Wayne, Fort Wayne, IN, USA; Chulalongkorn Business School, Chulalongkorn University, Bangkok, Thailand",IEEE Access,"16 Nov 2022","2022","10","","118900","118925","Blockchain (BC), as an emerging technology, is revolutionizing Business Process Management (BPM) in multiple ways. The main adoption is to serve as a trusted infrastructure to guarantee the trust of collaborations among multiple partners in trustless environments. Especially, BC enables trust of information by using Distributed Ledger Technology (DLT). With the power of smart contracts, BC enforces the obligations of counterparties that transact in a business process (BP) by programming the contracts as transactions. This paper aims to study the state-of-the-art of BC technologies by (1) exploring its applications in BPM with the focus on how BC provides the trust of BPs in their lifecycles; (2) identifying the relations of BPM as the need and BC as the solution with the assessment towards BPM characteristics; (3) discussing the up-to-date progresses of critical BC in BPM; (4) identifying the challenges and research directions for future advancement in the domain. The main conclusions of our comprehensive review are (1) the study of adopting BC in BPM has attracted a great deal of attention that has been evidenced by a rapidly growing number of relevant articles. (2) The paradigms of BPM over Internet of Things (IoT) have been shifted from persistent to transient, from static to dynamic, and from centralized to decentralized, and new enabling technologies are highly demanded to fulfill some emerging functional requirements (FRs) at the stages of design, configuration, diagnosis, and evaluation of BPs in their lifecycles. (3) BC has been intensively studied and proven as a promising solution to assure the trustiness for both of business processes and their executions in decentralized BPM. (4) Most of the reported BC applications are at their primary stages, future research efforts are needed to meet the technical challenges involved in interoperation, determination of trusted entities, confirmation of time-sensitive execution, and support of irreversibility.","2169-3536","","10.1109/ACCESS.2022.3217794","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9931722","Blockchain;business process management;smart contracts;trust assurance","Business process management;Smart contracts;Task analysis;Distributed ledger;Uncertainty;Internet of Things;Blockchains;Trust management","","15","","176","CCBY","28 Oct 2022","","","IEEE","IEEE Journals"
"Multimodal Event Processing: A Neural-Symbolic Paradigm for the Internet of Multimedia Things","E. Curry; D. Salwala; P. Dhingra; F. A. Pontes; P. Yadav","Insight SFI Research Centre for Data Analytics, Data Science Institute, National University of Ireland Galway, Galway, Ireland; Insight SFI Research Centre for Data Analytics, Data Science Institute, National University of Ireland Galway, Galway, Ireland; Insight SFI Research Centre for Data Analytics, Data Science Institute, National University of Ireland Galway, Galway, Ireland; Insight SFI Research Centre for Data Analytics, Data Science Institute, National University of Ireland Galway, Galway, Ireland; Insight SFI Research Centre for Data Analytics, Data Science Institute, National University of Ireland Galway, Galway, Ireland",IEEE Internet of Things Journal,"25 Jul 2022","2022","9","15","13705","13724","With the Internet of Multimedia Things (IoMT) becoming a reality, new approaches are needed to process real-time multimodal event streams. Existing approaches to event processing have limited consideration for the challenges of multimodal events, including the need for complex content extraction, and increased computational and memory costs. This article explores event processing as a basis for processing real-time IoMT data. This article introduces the multimodal event processing (MEP) paradigm, which provides a formal basis for native approaches to neural multimodal content analysis (i.e., computer vision, linguistics, and audio) with symbolic event processing rules to support real-time queries over multimodal data streams using the multimodal event processing language to express single, primitive multimodal, and complex multimodal event patterns. The content of multimodal streams is represented using multimodal event knowledge graphs to capture the semantic, spatial, and temporal content of the multimodal streams. The approach is implemented and evaluated within a MEP engine using single and multimodal queries achieving near real-time performance with a throughput of ~30 frames processed per second (fps) and subsecond latency of 0.075–0.30 s for video streams of 30 fps input rate. Support for high input stream rates (45 fps) is achieved through content-aware load-shedding techniques with a ~127X latency improvement resulting in only a minor decrease in accuracy.","2327-4662","","10.1109/JIOT.2022.3143171","Science Foundation Ireland; European Regional Development Fund(grant numbers:SFI/12/RC/2289_P2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9681901","Data management and analytics;event processing;Internet of Multimedia Things (IoMT);service middleware and platform","Streaming media;Internet of Things;Real-time systems;Data models;Engines;Data mining;Costs","","15","","61","CCBY","14 Jan 2022","","","IEEE","IEEE Journals"
"Tactile Based Intelligence Touch Technology in IoT Configured WCN in B5G/6G-A Survey","M. Gupta; R. K. Jha; S. Jain","School of Electronics and Communication Engineering (SoECE), Shri Mata Vaishno Devi University, Katra, Jammu, Jammu and Kashmir, India; Department of Electronics and Communication Engineering, Indian Institute of Information Technology, Design and Manufacturing, Jabalpur (IIITDM Jabalpur), Jabalpur, India; Computer Science Department, Central University of Jammu, Jammu, Jammu and Kashmir, India",IEEE Access,"30 Mar 2023","2023","11","","30639","30689","Touch-enabled sensation and actuation are expected to be the most promising, straightforward, and important uses of the B5G/6G communication networks. In light of the next generation (6G) systems’ prerequisite for low latency, the infrastructure should be reconfigurable, intelligent, and interoperable in the real-time existing wireless network. It has a drastic impact on society due to its high precision, accuracy, reliability, and efficiency, combined with the ability to connect a user from remote areas. Hence, the touch-enabled interaction is primarily concerned with the real-time transmission of tactile-based haptic information over the internet, in addition to the usual audio, visual, and data traffic, thus enabling a paradigm shift towards a real-time control and steering communication system. The existing system latency and overhead often have delays and limitations on the application’s usability. In light of the aforementioned concerns, the study proposes an intelligent touch-enabled system for B5G/6G and an IoT-based wireless communication network, incorporating AR/VR technologies. The tactile internet and network-slicing serve as the backbone of touch technology and incorporates intelligence from techniques such as artificial intelligence and machine/deep learning. The survey also introduces a layered and interfacing architecture with its E2E solution for the intelligent touch-based wireless communication system. It is anticipated for the upcoming 6G system to provide numerous opportunities for various sectors to utilize AR/VR technology in robotics and healthcare facilities to help in addressing several problems faced by society. Conclusively the article presents a few use cases concerning the deployment of touch infrastructure in automation, robotics, and intelligent healthcare systems, assisting in the diagnosis and treatment of the prevailing Covid-19 cases. The paper concludes with some considerable future research aspects of the proposed system with a few ongoing projects concerning the development and incorporation of the 6G wireless communication system.","2169-3536","","10.1109/ACCESS.2022.3148473","5G and Internet of Things (IoT) Laboratory, School of Electronics and Communication Engineering (SoECE), Technology Business Incubation Center (TBIC), Technical Education Quality Improvement Programme of Government of India-III (TEQIP-III) at Shri Mata Vaishno Devi University (SMVDU), Katra, Jammu; 6G and IoT Innovation Laboratory, Indian Institute of Information Technology, Design and Manufacturing (IIITDM), Jabalpur; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9701357","6G;AI;AR;intelligence;IoT;ML;network slicing;tactile internet;VR","Wireless communication;Real-time systems;Virtualization;Next generation networking;NOMA;Learning (artificial intelligence);6G mobile communication","","15","","201","CCBY","1 Feb 2022","","","IEEE","IEEE Journals"
"A Detailed Relevance Analysis of Enabling Technologies for 6G Architectures","D. G. S. Pivoto; T. T. Rezende; M. S. P. Facina; R. Moreira; F. de Oliveira Silva; K. V. Cardoso; S. L. Correa; A. V. D. Araujo; R. S. E. Silva; H. S. Neto; G. R. de Lima Tejerina; A. M. Alberti","Instituto Nacional de Telecomunicações, Santa Rita do Sapucaí, Brazil; Instituto Nacional de Telecomunicações, Santa Rita do Sapucaí, Brazil; Fundação Centro de Pesquisa e Desenvolvimento em Telecomunicações, Campinas, Brazil; Faculty of Computing, Federal University of Uberlândia, Uberlândia, Brazil; Faculty of Computing, Federal University of Uberlândia, Uberlândia, Brazil; Instituto de Informática (INF), Universidade Federal de Goiás, Goiânia, Brazil; Instituto de Informática (INF), Universidade Federal de Goiás, Goiânia, Brazil; Instituto de Informática (INF), Universidade Federal de Goiás, Goiânia, Brazil; Federal Institute of Goiás, Inhumas, Brazil; Federal Institute Catarinense, Concórdia, Brazil; Centre for Wireless Communications, Oulu, Finland; Instituto Nacional de Telecomunicações, Santa Rita do Sapucaí, Brazil",IEEE Access,"29 Aug 2023","2023","11","","89644","89684","As society evolves as a whole, new demands arise with increasingly demanding prerequisites, consequently requiring more significant effort to be met. Such demands cover emerging applications, such as remote surgeries in Smart Health use cases, whose latency and reliability network requirements cannot be met by current communication systems; or simply improving current applications with more challenging requirements to be achieved, such as increasing the transmission rate in a mobile network, offering Quality of Service (QoS), and consequently, better user experience. Therefore, enabling technologies must be chosen to design an appropriate 6G architecture to address such demands. However, the explosion of emerging applications focused on different scopes and requirements to be met makes choosing these enabling technologies extremely complex and unpredictable. Thus, this article aims to create a methodology for analyzing the relevance of enabling technologies and use it to design an optimal architecture capable of meeting the 6G demands. For this purpose, two methods named as Average (AVG) and Analytic Hierarchy Process (AHP) have been selected, whose objective is to determine the relevance of an enabler for the 6G architecture, taking into account different degrees of influencing variables for this analysis, such as adherence to a certain architectural model; popularity in the research area; degree of innovation; synergy with other enablers; and support for requirements. Each of these methods presents a particular result. In the case of the AVG method, the criteria and variables are evaluated independently, and the arithmetic mean is employed to combine the evaluations into a single measure of suitability. In contrast, the AHP method considers the relative importance of criteria and variables in order to classify an optimal set of enabling technologies capable of fulfilling the key roles to be performed by a 6G architecture, and consequently meeting the main 6G demands. Our evaluation provides a unique perspective on 6G enablers, identifying issues and fostering research for future mobile architectures. The results obtained also provide researchers with the necessary information to stay updated on emerging enabling technologies and their suitability for designing new optimized 6G architectures.","2169-3536","","10.1109/ACCESS.2023.3301811","Rede Nacional de Ensino e Pesquisa (RNP), with resources from Ministério da Ciência, Tecnologia e Inovações e Comunicações (MCTIC); Brazil 6G Project of the Radiocommunication Reference Center (Centro de Referência em Radiocomunicações—CRR) of the National Institute of Telecommunications (Instituto Nacional de Telecomunicações, Inatel), Brazil(grant numbers:01245.010604/2020-14); Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES)—Finance(grant numbers:001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10207841","6G architectures;6G enabling technologies;6G requirements;6G use cases;AHP;relevance analysis","6G mobile communication;Network architecture;User experience","","15","","239","CCBYNCND","3 Aug 2023","","","IEEE","IEEE Journals"
"Clustering-Based Emotion Recognition Micro-Service Cloud Framework for Mobile Computing","P. Wang; L. Dong; y. xu; W. Liu; N. Jing","National Key Laboratory of Science and Technology on ATR, College of Electronic Science, National University of Defense Technology, Changsha, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China",IEEE Access,"18 Mar 2020","2020","8","","49695","49704","In a situation where life becomes more stressful and challenging, people feel compelled to be more concerned about their mental situation. Different emotional statuses are external reactions to different mental states. Therefore, researchers always identify people's mental situation by monitoring their real-time emotions. At the same time, due to the availability of built-in sensors in a smartphone, applications that can identify real-time emotions of mobile users are constantly emerging. However, compared to most emotion recognition algorithms, computing resources and battery life in mobile phones are always limited. This makes accuracy and latency of these applications are unsatisfactory. In this paper, we propose a micro-service platform for mobile emotion recognition application developers (MSPMERAD) which can supply high performance. First, a classifier fusion emotion recognition algorithm is proposed by using a dynamic adaptive fusion strategy. Second, this new algorithm is encapsulated into a micro-service. With other affiliated micro-services such as data uploading, preprocessing, etc., developers can ignore the implementation of the emotion recognition algorithm and just focus on how to collect sensor data and interact with users. The accuracy and latency of one application based on the MSPMERAD are compared with another application that is implemented using a locale emotion recognition algorithm. Experiments based on the daily behavior data of 50 student volunteers show that the application based on our platform has higher recognition accuracy with a more reasonable time.","2169-3536","","10.1109/ACCESS.2020.2979898","Fundamental Research Funds for the Central Universities(grant numbers:JB161004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9031338","Classifier fusion method;dynamic adaptive fusion strategy;emotion recognition;micro-service;mobile users","Emotion recognition;Mobile handsets;Sensors;Heuristic algorithms;Cloud computing;Semantics;Electroencephalography","","15","","36","CCBY","10 Mar 2020","","","IEEE","IEEE Journals"
"Edge Intelligence for Internet of Vehicles: A Survey","G. Yan; K. Liu; C. Liu; J. Zhang","College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; CICT Connected and Intelligent Technologies Company Ltd., Chongqing, China",IEEE Transactions on Consumer Electronics,"29 Aug 2024","2024","70","2","4858","4877","The Internet of Vehicles (IoV) has become a fundamental platform for advancing Intelligent Transportation Systems (ITSs) and Intelligent Connected Vehicles (ICVs). However, the increasing volume of data generated by vehicle sensors and the computational demands of Artificial Intelligence (AI) algorithms present significant challenges for the platform. Edge Intelligence (EI), which brings intelligent computing and data processing closer to vehicles, has emerged as a potential solution. In this survey, we provide a comprehensive overview of Edge Intelligence for the Internet of Vehicles. We begin by discussing the motivations behind employing EI in the IoV for typical AI computations. To fully exploit the potential of EI in heterogeneous IoV environments, we present a layered vehicular EI architecture and discuss its benefits and challenges. Furthermore, we provide a taxonomy of EI approaches for vehicular networks, focusing on cooperative inference, distributed training, and collaborative sensing, in terms of their schemas and advanced frameworks. Finally, we explore emerging trends and research directions in this field, including vehicle-road-cloud integration, generative AI-driven IoV, and vehicular cyber-physical fusion. By offering insights into state-of-the-art techniques and trends, this survey aims to enable researchers to develop innovative solutions for transforming the intelligent IoV ecosystem.","1558-4127","","10.1109/TCE.2024.3378509","National Natural Science Foundation of China(grant numbers:62172064); Graduate Scientific Research and Innovation Foundation of Chongqing, China(grant numbers:CYB23044); Fundamental Research Funds for the Central Universities(grant numbers:2023CDJQCZX-001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10474509","Internet of Vehicles (IoV);edge intelligence (EI);inference;training;sensing","Sensors;Artificial intelligence;Training;Collaboration;Internet of Vehicles;Edge computing","","15","","113","IEEE","18 Mar 2024","","","IEEE","IEEE Journals"
"5G Multi-Access Edge Computing: A Survey on Security, Dependability, and Performance","G. Nencioni; R. G. Garroppo; R. F. Olimid","Department of Electrical Engineering and Computer Science, University of Stavanger, Stavanger, Norway; Department of Information Engineering, University of Pisa, Pisa, Italy; Department of Computer Science, University of Bucharest, Bucharest, Romania",IEEE Access,"30 Jun 2023","2023","11","","63496","63533","The Fifth Generation (5G) of mobile networks offers new and advanced services with stricter requirements. Multi-access Edge Computing (MEC) is a key technology that enables these new services by deploying multiple devices with computing and storage capabilities at the edge of the network, close to end-users. MEC enhances network efficiency by reducing latency, enabling real-time awareness of the local environment, allowing cloud offloading, and reducing traffic congestion. New mission-critical applications require high security and dependability, which are rarely addressed alongside performance. This survey paper fills this gap by presenting 5G MEC’s three aspects: security, dependability, and performance. The paper provides an overview of MEC, introduces taxonomy, state-of-the-art, and challenges related to each aspect. Finally, the paper presents the challenges of jointly addressing these three aspects.","2169-3536","","10.1109/ACCESS.2023.3288334","Norwegian Research Council through the Management and Orchestration for Data and Network Integration (5G-MODaNeI) Project(grant numbers:308909); Italian Ministry of Education and Research (MIUR) in the framework of the Future-Oriented Research Laboratory (FoReLab) Project (Departments of Excellence); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10158694","5G;MEC;security;dependability;performance","5G mobile communication;Security;Computer architecture;Cloud computing;Virtualization;Multi-access edge computing;Ultra reliable low latency communication","","14","","245","CCBYNCND","21 Jun 2023","","","IEEE","IEEE Journals"
"A Survey and Tutorial on “Connection Exploding Meets Efficient Communication” in the Internet of Things","H. Ning; F. Farha; Z. N. Mohammad; M. Daneshmand","School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; School of Business, Stevens Institute of Technology, Hoboken, NJ, USA",IEEE Internet of Things Journal,"12 Nov 2020","2020","7","11","10733","10744","Internet-of-Things (IoT)-enabled sensors and services have increased exponentially recently. Transmitting the massive generated data and control messages becomes an overhead on the communication system infrastructure. Many architectures and paradigms have been introduced to address the connection exploding, such as cloudlets, fog, and mist computing. Besides, software-related solutions, such as mobile Internet technologies and software-defined network also take part in mitigating the communication overhead. All of those new techniques have the same purposes summarized in achieving low latency, high throughput, and less storage and computing at the cloud level in addition to other objectives discussed through this survey. We list the proposed solutions, show their advantages and schemes, highlight some of the newest IoT-enabled applications, and show how they benefit from applying the new paradigms.","2327-4662","","10.1109/JIOT.2020.2996615","National Natural Science Foundation of China(grant numbers:61471035,61672131); Civil Aviation Joint Funds of the National Natural Science Foundation of China(grant numbers:U1633121); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9098906","Connection exploding;Internet-of-Things (IoT) communications","Cloud computing;Sensors;Edge computing;Security;Computer architecture;Internet of Things;Privacy;Surveys;Tutorials","","14","","101","IEEE","22 May 2020","","","IEEE","IEEE Journals"
"SC-TRUST: A Dynamic Model for Trustworthy Service Composition in the Internet of Things","A. A. Adewuyi; H. Cheng; Q. Shi; J. Cao; X. Wang; B. Zhou","Department of Computer Science, Liverpool John Moores University, Liverpool, U.K.; Department of Computer Science, University of Hertfordshire, Hatfield, U.K.; Department of Computer Science, Liverpool John Moores University, Liverpool, U.K.; Department of Computing, Hong Kong Polytechnic University, Hong Kong; State Key Laboratory of Synthetical Automation for Process Industries, College of Computer Science and Engineering, Northeastern University, Shenyang, China; Department of Computer Science, Liverpool John Moores University, Liverpool, U.K.",IEEE Internet of Things Journal,"21 Feb 2022","2022","9","5","3298","3312","A future Internet of Things (IoT) will feature a service-oriented architecture consisting of lightweight computing platforms offering individual, loosely coupled microservices. Often, an end-user will request a bespoke service that will require a composition of two or more microservices offered by different service providers. However, the underlying complexities of soft compositions and the increased security risks are inherent in such a massively decentralized and distributed architecture. The use of trust management to secure the IoT is well studied in the literature. However, there are limitations to its use in service compositions in the IoT. Specifically, transparent (agnostic) trust composition and decomposition remain key problems for this area. A novel model for trustworthy service compositions in the IoT, SC-TRUST, is therefore proposed to deal with these challenges. In this study, the trust properties of service compositions and the effect of service workflows on transparent trust composition and decomposition are investigated. Based on the findings, relevant trust evaluation functions are derived to guide the compositions. SC-TRUST was implemented in a suitable application and its performance, in terms of the utility derived and the trust accuracy, convergence, and resiliency, was evaluated. The results show that SC-TRUST improves the quality of service compositions and adequately mitigates trust-related attacks, thus increasing both efficiency and security.","2327-4662","","10.1109/JIOT.2021.3097980","National Natural Science Foundation of China(grant numbers:61872073); LiaoNing Revitalization Talents Program(grant numbers:XLYC1902010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9493744","Collaborative computing;distributed applications;Internet of Things (IoT);security and privacy protection;service composition;trust management","Internet of Things;Reliability;Collaboration;Trust management;Middleware;Computational modeling;Service-oriented architecture","","14","","52","IEEE","26 Jul 2021","","","IEEE","IEEE Journals"
"Network-Aware 5G Edge Computing for Object Detection: Augmenting Wearables to “See” More, Farther and Faster","Z. Yuan; T. Azzino; Y. Hao; Y. Lyu; H. Pei; A. Boldini; M. Mezzavilla; M. Beheshti; M. Porfiri; T. E. Hudson; W. Seiple; Y. Fang; S. Rangan; Y. Wang; J. -R. Rizzo","Department of Electrical and Computer Engineering, Tandon School of Engineering, New York University, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, Tandon School of Engineering, New York University, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, Tandon School of Engineering, New York University, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, Tandon School of Engineering, New York University, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, Tandon School of Engineering, New York University, Brooklyn, NY, USA; Department of Mechanical and Aerospace Engineering, Tandon School of Engineering, New York University, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, Tandon School of Engineering, New York University, Brooklyn, NY, USA; Department of Mechanical and Aerospace Engineering, Tandon School of Engineering, New York University, Brooklyn, NY, USA; Department of Mechanical and Aerospace Engineering, Tandon School of Engineering, New York University, Brooklyn, NY, USA; Department of Rehabilitation Medicine, NYU Langone Health, New York, NY, USA; Lighthouse Guild, New York, NY, USA; Department of Electrical and Computer Engineering, Tandon School of Engineering, New York University, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, Tandon School of Engineering, New York University, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, Tandon School of Engineering, New York University, Brooklyn, NY, USA; Department of Mechanical and Aerospace Engineering, Tandon School of Engineering, New York University, Brooklyn, NY, USA",IEEE Access,"21 Mar 2022","2022","10","","29612","29632","Advanced wearable devices are increasingly incorporating high-resolution multi-camera systems. As state-of-the-art neural networks for processing the resulting image data are computationally demanding, there has been a growing interest in leveraging fifth generation (5G) wireless connectivity and mobile edge computing for offloading this processing closer to end-users. To assess this possibility, this paper presents a detailed simulation and evaluation of 5G wireless offloading for object detection in the case of a powerful, new smart wearable called VIS4ION, for the Blind-and-Visually Impaired (BVI). The current VIS4ION system is an instrumented book-bag with high-resolution cameras, vision processing, and haptic and audio feedback. The paper considers uploading the camera data to a mobile edge server to perform real-time object detection and transmitting the detection results back to the wearable. To determine the video requirements, the paper evaluates the impact of video bit rate and resolution on object detection accuracy and range. A new street scene dataset with labeled objects relevant to BVI navigation is leveraged for analysis. The vision evaluation is combined with a full-stack wireless network simulation to determine the distribution of throughputs and delays with real navigation paths and ray-tracing from new high-resolution 3D models in an urban environment. For comparison, the wireless simulation considers both a standard 4G-Long Term Evolution (LTE) sub-6-GHz carrier and high-rate 5G millimeter-wave (mmWave) carrier. The work thus provides a thorough and detailed assessment of edge computing for object detection with mmWave and sub-6-GHz connectivity in an application with both high bandwidth and low latency requirements.","2169-3536","","10.1109/ACCESS.2022.3157876","NSF through the Smart and Connected Community Program(grant numbers:1952180); Industrial Affiliates of New York University (NYU) WIRELESS; NSF(grant numbers:1925079,1564142,1547332); Semiconductor Research Corporation (SRC); NSF(grant numbers:2003182); New York University Abu Dhabi (NYUAD) Institute (Research Enhancement Fund)(grant numbers:RE132); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9730919","Mobile edge computing;millimeter-wave;5G wireless;smart wearables;mobile machine vision;deep learning;object detection","Wireless communication;Cameras;5G mobile communication;Object detection;Wearable computers;Ions;Image edge detection","","14","","88","CCBY","8 Mar 2022","","","IEEE","IEEE Journals"
"Using the Cloud to Improve Sensor Availability and Reliability in Remote Monitoring","B. Y. Ooi; W. L. Beh; W. -K. Lee; S. Shirmohammadi","Faculty of Information and Communication Technology, Universiti Tunku Abdul Rahman, Kampar, Perak, Malaysia; Faculty of Science, Universiti Tunku Abdul Rahman, Kampar, Perak, Malaysia; Faculty of Information and Communication Technology, Universiti Tunku Abdul Rahman, Kampar, Perak, Malaysia; Distributed and Collaborative Virtual Environments Research Laboratory, University of Ottawa, Ottawa, ON, Canada",IEEE Transactions on Instrumentation and Measurement,"17 Apr 2019","2019","68","5","1522","1532","Although there have been significant advancements in low-power remote sensors in recent years, the challenge of sensor availability and data reliability in remote monitoring applications still persists. The fault and failure of sensors will affect the reliability of the monitored data and subsequently the adverse effect will inevitably propagate itself to the data analytics stage. There are many existing solutions focusing on improving sensor nodes to enhance data reliability and couple it with various energy harvesting techniques to prolong the availability of sensor nodes. This paper presents a complementary solution to these existing solutions by analyzing the correlation between data from different sensor nodes using cloud computing resources. The discovered relationship between the sensor nodes can then be used to improve data reliability and availability of sensor nodes. Performance evaluations using real data sets show that there are indeed relationships between the collected data, and through these discovered relationships the fault detection and fault masking methods outperform conventional approaches such as autoregressive-integrated moving average. In addition, this paper also proposes an approach to extend operation of sensor nodes duration through the discovered relationships, with experiments showing promising results.","1557-9662","","10.1109/TIM.2018.2882218","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8574916","Data reliability;remote monitoring;system availability;wireless sensor nodes","Reliability;Cloud computing;Wireless sensor networks;Temperature sensors;Remote monitoring;Batteries","","14","","38","IEEE","13 Dec 2018","","","IEEE","IEEE Journals"
"Blockchain-Based Management of Blood Donation","D. Hawashin; D. A. J. Mahboobeh; K. Salah; R. Jayaraman; I. Yaqoob; M. Debe; S. Ellahham","Department of Industrial and Systems Engineering, Khalifa University of Science and Technology, Abu Dhabi, United Arab Emirates; Department of Electrical Engineering and Computer Science, Khalifa University of Science and Technology, Abu Dhabi, United Arab Emirates; Department of Electrical Engineering and Computer Science, Khalifa University of Science and Technology, Abu Dhabi, United Arab Emirates; Department of Industrial and Systems Engineering, Khalifa University of Science and Technology, Abu Dhabi, United Arab Emirates; Department of Electrical Engineering and Computer Science, Khalifa University of Science and Technology, Abu Dhabi, United Arab Emirates; Heart & Vascular Institute, Cleveland Clinic Abu Dhabi, Abu Dhabi, United Arab Emirates; Heart & Vascular Institute, Cleveland Clinic Abu Dhabi, Abu Dhabi, United Arab Emirates",IEEE Access,"17 Dec 2021","2021","9","","163016","163032","Today’s a large number of blood donation management systems fall short in providing traceability, immutability, transparency, audit, privacy, and security features. Also, they are vulnerable to the single point of failure problem due to centralization. In this paper, we propose a private Ethereum blockchain-based solution to automate blood donation management in a manner that is decentralized, transparent, traceable, auditable, private, secure, and trustworthy. The proposed solution stores non-critical and large data off-chain using the decentralized storage of the InterPlanetary File System (IPFS). We present the system architecture, sequence diagrams, entity-relationship diagram, and algorithms to briefly explain the working principles of our blood donation management solution. We evaluate the performance of our solution in terms of efficiency and effectiveness through performing security analysis. We make our smart contract code publicly available on Github.","2169-3536","","10.1109/ACCESS.2021.3133953","Khalifa University of Science and Technology(grant numbers:CIRA-2019-001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9642983","Blockchain;private Ethereum;blood donation management;blood bank;traceability;security","Blood;Blockchains;Supply chains;Security;Medical services;Radiofrequency identification;Testing","","14","","34","CCBY","8 Dec 2021","","","IEEE","IEEE Journals"
"A Review on Attack Graph Analysis for IoT Vulnerability Assessment: Challenges, Open Issues, and Future Directions","O. S. M. B. H. Almazrouei; P. Magalingam; M. K. Hasan; M. Shanmugam","Advanced Informatics Department, Razak Faculty of Technology and Informatics, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; Advanced Informatics Department, Razak Faculty of Technology and Informatics, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; Center for Cyber Security, Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, Bangi, Malaysia; College of Computing and Informatics, Universiti Tenaga Nasional, Kajang, Malaysia",IEEE Access,"11 May 2023","2023","11","","44350","44376","Vulnerability assessment in industrial IoT networks is critical due to the evolving nature of the domain and the increasing complexity of security threats. This study aims to address the existing gaps in the literature by conducting a comprehensive survey on the use of attack graphs for vulnerability assessment in IoT networks. Attack graphs serve as a valuable cybersecurity tool for modeling and analyzing potential attack scenarios on systems, networks, or applications. The survey covers the research conducted between 2016 and 2021(34 peer-reviewed journal articles and 28 conference papers), identifying and categorizing the main methodologies and technologies employed in generating and analyzing attack graphs. In this review, core modeling techniques for IoT vulnerability assessment are highlighted, such as Markov Decision Processes (MDP), Feature Pyramid Networks (FPN), K-means clustering, and logistic regression models, along with other techniques involving genetic algorithms like fast-forward (FF), contingent fast-forwards (CFF), advanced reinforcement-learning algorithms, and HARMs models. The evaluation of the performance of these attack graph models using IoT networks or devices as case studies is also emphasized. This survey provides valuable insights into the state-of-the-art attack graph techniques for IoT network vulnerability assessment, identifying various applications, performances, research opportunities, and challenges. As a reference source, it serves to inform academicians and practitioners interested in leveraging attack graphs for IoT network vulnerability assessment and guides future research directions in this area.","2169-3536","","10.1109/ACCESS.2023.3272053","Ministry of Higher Education, Government of Malaysia, and the Research Management Centre, Universiti Teknologi Malaysia, through the Fundamental Research Grant Scheme (FRGS) Grant (vote number R.K130000.7856.5F016)(grant numbers:FRGS/1/2018/ICT04/UTM/02/8); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10113644","Attack graph;the Internet of Things;network vulnerabilities;vulnerability assessment","Internet of Things;Computer security;Systematics;Databases;Metadata;Wireless sensor networks;Terminology","","14","","74","CCBYNCND","1 May 2023","","","IEEE","IEEE Journals"
"SODALITE: SDN Wireless Backhauling for Dense 4G/5G Small Cell Networks","A. Betzler; D. Camps-Mur; E. Garcia-Villegas; I. Demirkol; J. J. Aleixendri","Mobile and Wireless Internet, I2CAT Foundation, Barcelona, Spain; Mobile and Wireless Internet, I2CAT Foundation, Barcelona, Spain; Department of Telematics Engineering, Universitat Politècnica de Catalunya, Barcelona, Spain; Department of Telematics Engineering, Universitat Politècnica de Catalunya, Barcelona, Spain; Mobile and Wireless Internet, I2CAT Foundation, Barcelona, Spain",IEEE Transactions on Network and Service Management,"10 Dec 2019","2019","16","4","1709","1723","Dense deployments of small cells (SCs) are key to fulfill the capacity requirements of future 5G networks. However, two roadblocks to the adoption of SCs are: 1) the limited availability and the cost of sites with wired backhaul resources and 2) the complexity to manage a dense deployment of wireless backhaul nodes. Towards these challenges we propose SODALITE, a novel system that applies software-defined networking (SDN) to a wireless backhaul network. We present, how SODALITE can be integrated to 3GPP's 4G and 5G architectures, and show the feasibility of SODALITE through LTE network testbed experiments. We substantiate the scalability of SODALITE through stochastic studies using real-life traffic traces from an LTE network and discuss the effects of cell densification and 5G system architecture on these studies. Further, a reliable backhauling solution for wireless links is introduced in SODALITE through SDN-enabled mechanisms that are capable of reconfiguring the data plane upon a link failure detection. Its reliability is shown through experiments on an LTE network testbed, and studied thoroughly via rigorous simulations and network emulator evaluations. As a result, we claim that SODALITE is a promising carrier-grade system to manage a wireless SC backhaul.","1932-4537","","10.1109/TNSM.2019.2930745","European Union under grant agreements 762057(grant numbers:H2020 5G-PICTURE); Spanish Ministry of Economy and Competitiveness (MINECO)(grant numbers:TEC2016-76795-C6-2-R,RYC-2013-13029,FEDER); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8771139","4G/5G small cells;wireless backhaul;SDN;fast re-route","Wireless communication;5G mobile communication;Computer architecture;Long Term Evolution;Scalability;Reliability;Switches","","14","","29","IEEE","24 Jul 2019","","","IEEE","IEEE Journals"
"Development of Mobile IoT Solutions: Approaches, Architectures, and Methodologies","N. Magaia; P. Gomes; L. Silva; B. Sousa; C. X. Mavromoustakis; G. Mastorakis","Department of Computer Science, Faculty of Sciences, LASIGE, University of Lisbon, Lisbon, Portugal; Faculty of Sciences, University of Lisbon, Lisbon, Portugal; Department of Computer Science, Faculty of Sciences, LASIGE, University of Lisbon, Lisbon, Portugal; Faculty of Sciences, University of Lisbon, Lisbon, Portugal; Department of Computer Science, University of Nicosia, Nicosia, Cyprus; Department of Management Science and Technology, Hellenic Mediterranean University, Agios Nikolaos, Greece",IEEE Internet of Things Journal,"4 Nov 2021","2021","8","22","16452","16472","Modern Living, as we know it, has been impacted meaningfully by the Internet of Things (IoT). IoT consists of a network of things that collect data from machines (e.g., mobile devices) and people. Mobile application development is a flourishing tendency, given the increasing popularity of smartphones. Nowadays, users are accessing their desired services on the smartphone by means of dedicated applications as the latter offers a more customized and prompt service. In addition, companies are also looking to persuade users by offering interactive and effective mobile applications. Mobile application developers are using IoT to develop better applications. However, there is no generalized consensus on the selection of best architecture or even the most suitable communications protocols to be used on an IoT application development. Therefore, this article aims at presenting approaches, architectures, and methodologies relevant to the development of mobile IoT solutions.","2327-4662","","10.1109/JIOT.2020.3046441","FCT through the LASIGE Research Unit(grant numbers:UIDB/00408/2020,UIDP/00408/2020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9302583","Architecture;Internet of Things (IoT);methodology;mobile development","Internet of Things;Computer architecture;Protocols;Cloud computing;Software;TCPIP;Security","","14","","198","IEEE","22 Dec 2020","","","IEEE","IEEE Journals"
"Swarm Economy: A Model for Transactions in a Distributed and Organic IoT Platform","L. C. C. De Biase; P. C. Calcina-Ccori; G. Fedrecheski; G. M. Duarte; P. S. S. Rangel; M. K. Zuffo","Engineering School, University of São Paulo (Escola Politecnica da USP), São Paulo, Brazil; Institute of Mathematics and Statistics, University of São Paulo, São Paulo, Brazil; Engineering School, University of São Paulo (Escola Politecnica da USP), São Paulo, Brazil; Engineering School, University of São Paulo (Escola Politecnica da USP), São Paulo, Brazil; Engineering School, University of São Paulo (Escola Politecnica da USP), São Paulo, Brazil; Engineering School, University of São Paulo (Escola Politecnica da USP), São Paulo, Brazil",IEEE Internet of Things Journal,"18 Jun 2019","2019","6","3","4561","4572","The Internet of Things (IoT) is steadily growing and migrating to decentralized architectures. In this context the Swarm approach, based on organic, edge-centric IoT networks, proposes one step further toward cooperation among devices, through the creation of a communication mediator called Swarm Broker. An important aspect in creating a cooperation network in the Swarm is the economical domain: the Swarm economy that guarantees fair rules to stimulate resource sharing among participants. This economic aspect is even more challenging in networks with distributed governance and motivates the adoption of a technology for a trustworthy and immutable public registry, such as blockchain, to store transaction information. In this paper, we aim to create an economic model that regulate the cooperation among the participants of the Swarm, by implementing strategies, to fulfill the restrictions posed by the decentralized network and the presence of resource-constrained devices that integrate the Swarm. We created a microeconomic model for resource sharing in the Swarm and a transaction model, based on the price and reputation of participants. We created a blockchain-based system for storing currency, contract, and reputation information in a distributed way. We included the resulting models and implementation into the Swarm network, by adding a module into the Swarm Broker.","2327-4662","","10.1109/JIOT.2018.2886069","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brazil (CAPES) - Finance Code 001, CAPES Pro-Defesa Auxílio 2993/2013, Pró-Reitoria de Pós-Graduação da Universidade de São Paulo and Laboratório de Sistemas Integráveis Tecnológico (LSI-TEC); National Secretary for Digital Policies (SEPIN) within the Ministry of Science, Technology, Innovation and Communications (MCTIC) of Brazil(grant numbers:01250.054503/2017-72); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8572758","Blockchain;economic model;edge computing;Internet of Things (IoT)","Internet of Things;Economics;Resource management;Cloud computing;Computational modeling;Computer architecture","","14","","31","IEEE","11 Dec 2018","","","IEEE","IEEE Journals"
"Survey on Delegated and Self-Contained Authorization Techniques in CPS and IoT","S. V. Sudarsan; O. Schelén; U. Bodin","EISLAB, Luleå University of Technology, Luleå, Sweden; EISLAB, Luleå University of Technology, Luleå, Sweden; EISLAB, Luleå University of Technology, Luleå, Sweden",IEEE Access,"15 Jul 2021","2021","9","","98169","98184","Authentication, authorization, and digital identity management are core features required by secure digital systems. In this, authorization is a key component for regulating the detailed access credentials with respect to required service resources. Authorization, therefore, plays a significant role in the trust management of autonomous devices and services. Due to the heterogeneous nature of cyber-physical systems and the Internet of Things, several authorization techniques using different access control models, accounts, groups, tokens, and delegations have both strengths and weaknesses. Many studies exist in the literature that focus on other main security requirements, such as authentication, identity management, and confidentiality. However, there is a need for a comprehensive review of different authorization techniques in cyber-physical systems and the Internet of Things. A specific target of this paper is authorization in the cyber-physical system and Internet of Things networks with non-constrained devices in an industrial context with mobility, subcontractors, and autonomous machines that are able to carry out advanced tasks on behalf of others. We study the different authorization techniques using our three-dimensional classification, including access control models, subgranting models, and authorization governance. We focus on the state of the art of authorization subgranting, including delegation techniques by access control/authorization server and self-contained authorization using a new concept of power of attorney. Comparisons are performed with respect to several parameters, such as type of communication, method of authorization, control of expiration, and use of techniques such as public key certificate, encryption techniques, and tokens. The results show the differences and similarities of server-based and power of attorney-based authorization subgranting. The most common standards are also analyzed in light of those classifications.","2169-3536","","10.1109/ACCESS.2021.3093327","Electronic Components and Systems for European Leadership (ECSEL) Joint Undertaking (JU) through the Arrowhead Tools Project(grant numbers:826452); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9467373","Authorization;access control models;cyber-physical systems (CPS);Internet of Things (IoT);subgranting;delegation;power of attorney (PoA);OAuth","Authorization;Internet of Things;Security;Authentication;Servers;Task analysis;Standards","","14","","105","CCBY","29 Jun 2021","","","IEEE","IEEE Journals"
"Optimization of Sports Training Systems Based on Wireless Sensor Networks Algorithms","J. Yang; W. Lv","Jiaozuo Normal College, Institute of Physical Education, Jiaozuo, China; Jiaozuo Normal College, Institute of Physical Education, Jiaozuo, China",IEEE Sensors Journal,"12 Nov 2021","2021","21","22","25075","25082","This paper aims to design and optimize a sports training system through wireless sensor network technology, which mainly includes a hardware system and software system. In the hardware system, the design of sensor nodes and the base station is realized, which can realize the real-time collection of movement parameters by the movement collectors. In the software system, the design of base station control, node control, and motion database software is realized, which can effectively collect, store, and analyze the motion parameters. Finally, the wireless sensor network-based sports training system is tested, and the test results show that the system designed in this paper can meet the needs of sports training use. The inertial measurement unit in the wearable device is used to collect the data generated by the human body during exercise, and low-power Bluetooth is used as the data transmission protocol between the wearable device and the smartphone. The server side provides computing resources in the form of cloud computing to achieve client-server interaction. In combination with the idea of service containerization, a deployment scheme for cloud services was proposed using container choreography, and compared to the pre-training period, the subjects’ trunk sway in the front and back directions decreased by 60.5% and 54.0%, and the trunk sway in the left and right directions decreased by 67.1% and 50.3% during and after training, respectively, and the ratio of time occupied by the equilibrium state increased by 63.5% and 47.4%, subjects’ balance was effectively improved.","1558-1748","","10.1109/JSEN.2020.3046290","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9301323","Wireless sensors algorithms;networks;athletic training;system optimization","Sensors;Wireless sensor networks;Interference;Sensor systems;Training;Cloud computing;Wireless communication","","14","","20","IEEE","21 Dec 2020","","","IEEE","IEEE Journals"
"Navigating Industry 5.0: A Survey of Key Enabling Technologies, Trends, Challenges, and Opportunities","R. Tallat; A. Hawbani; X. Wang; A. Al-Dubai; L. Zhao; Z. Liu; G. Min; A. Y. Zomaya; S. Hamood Alsamhi","School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; Computing School, Edinburgh Napier University, Edinburgh, U.K.; Computing School, Edinburgh Napier University, Edinburgh, U.K.; Department of Computer and Network Engineering, The University of Electro-Communications, Tokyo, Japan; Department of Computer Science, College of Engineering, Mathematics, and Physical Sciences, University of Exeter, Exeter, U.K.; Computing School of Computer Science, The University of Sydney, Sydney, NSW, Australia; Insight Centre for Data Analytics, University of Galway, Galway, Ireland",IEEE Communications Surveys & Tutorials,"22 May 2024","2024","26","2","1080","1126","This century has been a major avenue for revolutionary changes in technology and industry. Industries have transitioned towards intelligent automation, relying less on human intervention, resulting in the fourth industrial revolution, Industry 4.0. That is why IoT has been the researcher’s arena for quite some time. With Industry 4.0 still in motion, the world is on the verge of the  $5^{\textit {th}}$  industrial revolution, a relatively new concept with many unclear opinions regarding its potential benefits, challenges, opportunities, trends, and impact on society. There is a dire need for a broader and more critical perspective. This research paints a bigger picture of “What is happening?” and “What to expect?” during the transition phase of Industry 5.0. In this comprehensive review, we have addressed the state-of-the-art practices in Industry 4.0 and the transitional phase of Industry 5.0. We have highlighted the most promising key enabling technologies, trends, research topics, rising challenges, and unfolding opportunities that can help prepare society for this paradigm shift. The paper then surveys the work toward the outstanding key enablers, challenges, trends, and opportunities in the IoT evolution for Industry 5.0. To spur further avenues for researchers and industrialists, the paper offers conclusive insights at the end. In addition, the article has a precise set of research questions answered in consequent sections and subsections for the reader’s clarity.","1553-877X","","10.1109/COMST.2023.3329472","Innovation Team and Talents Cultivation Program of the National Administration of Traditional Chinese Medicine(grant numbers:ZYYCXTD-D-202208); National Natural Science Foundation of China(grant numbers:62372310); Liaoning Province Applied Basic Research Program(grant numbers:2023JH2/101300194); Science Foundation Ireland(grant numbers:SFI/12/RC/2289_P2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10305081","Industry 5.0;industry 4.0;digital twin;federated learning;Industrial Internet of Things (IIoT);industrial wireless sensor networks;Internet of Robotic Things (IoRT);blockchain;6G;intelligent sensing","Industries;Fourth Industrial Revolution;Service robots;Artificial intelligence;Surveys;Robot sensing systems;Industrial Internet of Things","","14","","358","IEEE","2 Nov 2023","","","IEEE","IEEE Journals"
"Edge on Wheels With OMNIBUS Networking for 6G Technology","M. Ergen; F. Inan; O. Ergen; I. Shayea; M. F. Tuysuz; A. Azizan; N. K. Ure; M. Nekovee","Electronics and Communication Engineering Department, Faculty of Electrical and Electronics Engineering, Istanbul Technical University (ITU), Istanbul, Turkey; Ambeent Inc., Yuba, CA, USA; Electronics and Communication Engineering Department, Faculty of Electrical and Electronics Engineering, Istanbul Technical University (ITU), Istanbul, Turkey; Electronics and Communication Engineering Department, Faculty of Electrical and Electronics Engineering, Istanbul Technical University (ITU), Istanbul, Turkey; Computer and Information Sciences Department, Northumbria University, Newcastle Upon Tyne, U.K.; Advanced Informatics Department, Razak Faculty of Technology and Informatics, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; Istanbul Technical University Artificial Intelligence and Data Science Application and Research Center, ITU Ayazaga Campus, Istanbul, Turkey; Centre for Advanced Communications, Mobile Technology and IoT, School of Engineering and Informatics, University of Sussex, Brighton, U.K.",IEEE Access,"8 Dec 2020","2020","8","","215928","215942","In recent years, both the scientific community and the industry have focused on moving computational resources with remote data centres from the centralized cloud to decentralised computing, making them closer to the source or the so called “edge” of the network. This is due to the fact that the cloud system alone cannot sufficiently support the huge demands of future networks with the massive growth of new, time-critical applications such as self-driving vehicles, Augmented Reality/Virtual Reality techniques, advanced robotics and critical remote control of smart Internet-of-Things applications. While decentralised edge computing will form the backbone of future heterogeneous networks, it still remains at its infancy stage. Currently, there is no comprehensive platform. In this article, we propose a novel decentralised edge architecture, a solution called OMNIBUS, which enables a continuous distribution of computational capacity for end-devices in different localities by exploiting moving vehicles as storage and computation resources. Scalability and adaptability are the main features that differentiate the proposed solution from existing edge computing models. The proposed solution has the potential to scale infinitely, which will lead to a significant increase in network speed. The OMNIBUS solution rests on developing two predictive models: (i) to learn timing and direction of vehicular movements to ascertain computational capacity for a given locale, and (ii) to introduce a theoretical framework for sequential to parallel conversion in learning, optimisation and caching under contingent circumstances due to vehicles in motion.","2169-3536","","10.1109/ACCESS.2020.3038233","Ambeent Inc.; Universiti Teknologi Malaysia, Research University Grant Scheme Tier 2(grant numbers:PY/2019/00325); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9260179","Edge computing;5G;6G;V2X;ubiquitous AI;distributed AI;multi-access edge computing (MEC)","Scalability;Wheels;Timing;Time factors;Robots;Remote control;Edge computing","","14","","96","CCBY","16 Nov 2020","","","IEEE","IEEE Journals"
"AoI-Aware User Service Satisfaction Enhancement in Digital Twin-Empowered Edge Computing","J. Li; S. Guo; W. Liang; J. Wang; Q. Chen; Z. Xu; W. Xu","Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Department of Computer Science, City University of Hong Kong, Hong Kong, China; Department of Computer Science, City University of Hong Kong, Hong Kong, China; School of Computers, Guangdong University of Technology, Guangzhou, China; School of Software, Dalian University of Technology, Dalian, China; College of Computer Science, Sichuan University, Chengdu, China",IEEE/ACM Transactions on Networking,"18 Apr 2024","2024","32","2","1677","1690","The emerging digital twin technique enhances the network management efficiency and provides comprehensive insights on network performance, through mapping physical objects to their digital twins. The user satisfaction on digital twin-enabled service relies on the freshness of digital twin data, which is measured by the Age of Information (AoI). Due to long service delays, the use of the remote cloud for delay-sensitive service provisioning faces serious challenges. Mobile Edge Computing (MEC), as an ideal paradigm for delay-sensitive services, is able to realize real-time data communication between physical objects and their digital twins at the network edge. However, the mobility of physical objects and dynamics of user query arrivals make seamless service provisioning in MEC become challenging. In this paper, we investigate dynamic digital twin placements for improving user service satisfaction in MEC environments, by introducing a novel metric to measure user service satisfaction based on the AoI concept and formulating two user service satisfaction enhancement problems: the static and dynamic utility maximization problems under static and dynamic digital twin placement schemes. To this end, we first formulate an Integer Linear Programming (ILP) solution to the static utility maximization problem when the problem size is small; otherwise, we propose a performance-guaranteed approximation algorithm. We then propose an online algorithm with a provable competitive ratio for the dynamic utility maximization problem, by considering dynamic user query services. Finally, we evaluate the performance of the proposed algorithms via simulations. Simulation results demonstrate that the proposed algorithms outperform the comparison baseline algorithms, improving the algorithm performance by at least 10.7%, compared to the baseline algorithms.","1558-2566","","10.1109/TNET.2023.3324704","Key-Area Research and Development Program of Guangdong Province(grant numbers:2021B0101400003); Hong Kong Research Grants Council (RGC) Research Impact Fund(grant numbers:R5060-19,R5034-18); Areas of Excellence Scheme(grant numbers:AoE/E-601/22-R); General Research Fund(grant numbers:152203/20E,152244/21E,152169/22E,152228/23E); Shenzhen Science and Technology Innovation Commission(grant numbers:JCYJ20200109142008673); City University of Hong Kong(grant numbers:9380137,7005845); Hong Kong Research Grant Council(grant numbers:NSFC/RGC N_CityU 140/20); NSFC(grant numbers:62372118); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2022A1515011032); National Natural Science Foundation of China(grant numbers:61802048); Xinghai Scholar Program in the Dalian University of Technology, China; NSFC(grant numbers:62272328); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10289645","Digital twin;mobile edge computing;age of information;approximation and online algorithms;digital twin placement","Digital twins;Heuristic algorithms;Approximation algorithms;Delays;Prediction algorithms;Cloud computing;Real-time systems","","13","","43","IEEE","20 Oct 2023","","","IEEE","IEEE Journals"
"Energy-Efficient Anomaly Detection With Primary and Secondary Attributes in Edge-Cloud Collaboration Networks","X. Li; Z. Zhou; Z. Shi; X. Xue; Y. Duan","School of Information Engineering, China University of Geosciences (Beijing), Beijing, China; School of Information Engineering, China University of Geosciences (Beijing), Beijing, China; Department of Shale Gas, Research Institute of Petroleum Exploration and Development, PetroChina, Beijing, China; School of Computer Software, College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Information Science and Technology, Hainan University, Haikou, China",IEEE Internet of Things Journal,"26 Jul 2021","2021","8","15","12176","12188","An energy-efficient anomaly detection is fundamental to maintain a healthy status of domain applications in edge-cloud collaboration networks. Generally, various kinds of multimodal sensory data capture heterogeneous attributes, where a certain attribute, called the primary one, may be more significant in detecting certain anomaly. This observation drives us to propose a novel energy-efficient anomaly detection mechanism, where attributes sensed by multimodal smart things (msts) are categorized as primary and secondary ones according to their relevance with the characteristic of this anomaly. This technique includes two steps: 1) an initial anomaly detection in single edge networks. Edge nodes associated with the primary attribute adopt a lightweight object detection model to initially detect the potential occurrence of this anomaly. Certain edge networks are determined where an anomaly is suspected and 2) an anomaly refinement with multimodal and multiattribute smart things in marginal edge networks. The cloud identifies and issues a specific query request to gather anomaly-aware sensory data from smart things with secondary attributes, for refining the detection accuracy of this anomaly, where an adaptive weighted fusion model is developed to analyze sensory data coupling of msts. The experimental results show that this technique performs better than the state of the art on the reduction of energy consumption and query time.","2327-4662","","10.1109/JIOT.2021.3062420","National Key Research and Development Program of China (Key Technologies and Demonstrations of the Internet of Things and Smart Cities)(grant numbers:2019YFB2101803); National Natural Science Foundation of China(grant numbers:61772479,42050103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9369387","Edge-cloud collaboration networks;energy-efficient anomaly detection;multimodal smart things (msts);primary and secondary attributes","Image edge detection;Anomaly detection;Cloud computing;Collaboration;Sensors;Internet of Things;Oils","","13","","34","IEEE","4 Mar 2021","","","IEEE","IEEE Journals"
"MAACO: A Dynamic Service Placement Model for Smart Cities","C. Cabrera; S. Svorobej; A. Palade; A. Kazmi; S. Clarke","Department of Computer Science and Technology, University of Cambridge, Cambridge, U.K; School of Computer Science and Statistics, Trinity College, Dublin, Ireland; School of Computer Science and Statistics, Trinity College, Dublin, Ireland; School of Computer Science and Statistics, Trinity College, Dublin, Ireland; School of Computer Science and Statistics, Trinity College, Dublin, Ireland",IEEE Transactions on Services Computing,"6 Feb 2023","2023","16","1","424","437","Smart cities generate huge volumes of data to be processed by applications with different criticality and requirements. For example, a healthcare application needs lower latency when requested from an ambulance travelling to a hospital during an emergency compared to applications in less-critical domains. Cities can use Multi-access Edge Computing to reduce latency by placing applications’ services closer to users. A service placement process selects the set of servers to run the services for deployment. Smart cities challenge this selection as a large number of servers and services generate a large number of potential solutions with different QoS properties. Additionally, placement approaches must consider applications’ criticality and users’ mobility to offer an appropriate overall latency. Current approaches have considered servers’ utilisation and users’ location to place services. However, they do not consider applications’ criticality and mobile users’ paths. This paper presents MAACO, a Mobility-Aware, priority-driven, ACO-based service placement model that prioritises applications according to their criticality and minimises critical applications’ latency, while considering predicted paths for mobile users. Evaluation results show that MAACO achieves lower latency and waiting time compared against baselines at the cost of reduced load balance between the network servers.","1939-1374","","10.1109/TSC.2022.3143029","Science Foundation Ireland(grant numbers:16/SP/3804); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9681330","Smart cities;multi-access edge computing;dynamic service placement","Servers;Random access memory;Costs;Approximation algorithms;Trajectory;Smart cities;Heuristic algorithms","","13","","41","IEEE","13 Jan 2022","","","IEEE","IEEE Journals"
"Dynamic Bandwidth Slicing for Time-Critical IoT Data Streams in the Edge-Cloud Continuum","F. Habeeb; K. Alwasel; A. Noor; D. N. Jha; D. AlQattan; Y. Li; G. S. Aujla; T. Szydlo; R. Ranjan","Newcastle University, Newcastle Upon Tyne, U.K.; Saudi Electronic University, Riyadh, Saudi Arabia; College of Computer Science and Engineering, Taibah University, Madinah, Saudi Arabia; University of Oxford, Oxford, U.K.; Newcastle University, Newcastle Upon Tyne, U.K.; School of Computing, Newcastle University, Newcastle Upon Tyne, U.K.; Durham University, Durham, U.K.; Institute of Computer Science, AGH University of Science and Technology, Kraków, Poland; School of Computing, Newcastle University, Newcastle Upon Tyne, U.K.",IEEE Transactions on Industrial Informatics,"19 Sep 2022","2022","18","11","8017","8026","Edge computing has gained momentum in recent years, as complementary to cloud computing, for supporting applications (e.g., industrial control systems) that require time-critical communication guarantees. While edge computing can provide immediate analysis of streaming data from Internet of Things devices, those devices lack computing capabilities to guarantee reasonable performance for time-critical applications. To alleviate this critical problem, the prevalent trend is to offload these data analytic tasks from the edge devices to the cloud. However, existing offloading approaches are static in nature as they are unable to adapt varying workload and network conditions. To handle these issues, we present a novel distributed and quality of services based multilevel queue traffic scheduling system that can undertake semiautomatic bandwidth slicing to process time-critical incoming traffic in the edge-cloud environments. Our developed system shows a great enhancement in latency and throughput as well as reduction in energy consumption for edge-cloud environments.","1941-0050","","10.1109/TII.2022.3169971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762563","Bandwidth slicing;cloud;data stream;edge;Internet of Things (IoT);multiqueues;software-defined networking (SDN);time critical","Internet of Things;Cloud computing;Bandwidth;Quality of service;Microservice architectures;Ecosystems;Time factors","","13","","29","IEEE","25 Apr 2022","","","IEEE","IEEE Journals"
"Internet of Underwater Things Infrastructure: A Shared Underwater Acoustic Communication Layer Scheme for Real-World Underwater Acoustic Experiments","Z. Zhu; Y. Zhou; R. Wang; F. Tong","College of Ocean and Earth Sciences and the National and Local Joint Engineering Research Center for Navigation and Location Service Technology, Xiamen University, Xiamen, China; College of Ocean and Earth Sciences and the National and Local Joint Engineering Research Center for Navigation and Location Service Technology, Xiamen University, Xiamen, China; College of Ocean and Earth Sciences and the National and Local Joint Engineering Research Center for Navigation and Location Service Technology, Xiamen University, Xiamen, China; College of Ocean and Earth Sciences and the National and Local Joint Engineering Research Center for Navigation and Location Service Technology, Xiamen University, Xiamen, China",IEEE Transactions on Aerospace and Electronic Systems,"10 Oct 2023","2023","59","5","6991","7003","The advancement of underwater acoustic communication (UAC) holds great promise for the Internet of Underwater Things (IoUT). However, a critical challenge to the development of UAC technology is the difficulty of obtaining sea trial data, such difficulty makes the emerging algorithms inconvenient to evaluate. In this article, we propose a shared UAC layer (SUACL) to collect underwater acoustic signals, the proposed SUACL is founded on the IoUT, and users are allowed to remotely access the system to arrange underwater acoustic data transmission and reception via Internet. The SUACL system aims to enable the remote operation of communication units that are deployed in designated sea areas. The merit of the proposed SUACL is that the sea trial data can be obtained without risky and high expense, and the emerging underwater acoustic technologies can be evaluated in time. The SUACL is composed of the nonunderwater part, which includes the application layer and fusion layer, and the above/underwater part including the communication layer on which we focus. We provide a detailed description of the communication layer of the proposed SUACL system, such as the preamplifier with reconfigurable gain, the analog-to-digital converter with reconfigurable sampling rates, the reconfigurable transducers with different frequency bands, and the reconfigurable number of hydrophones. The SUACL's reconfigurability options make it a versatile tool that can meet the diverse experimental needs for underwater communication research. In addition, we also describe the mechanisms of how experimental data are exchanged through the SUACL. Our experimental results demonstrate the feasibility and effectiveness of the proposed SUACL in a shallow-water environment.","1557-9603","","10.1109/TAES.2023.3281531","Fundamental Research Funds for the Central Universities(grant numbers:20720210078); National Key Research and Development Program of China(grant numbers:2018YFE0110000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10138644","Internet of Underwater Things (IoUT);microservice;shared underwater acoustic communication layer (SUACL);underwater acoustic communication (UAC)","Underwater acoustics;Internet;Sensors;Oceans;Internet of Things;Computer architecture;Software","","13","","43","IEEE","30 May 2023","","","IEEE","IEEE Journals"
"Request Scheduling Combined With Load Balancing in Mobile-Edge Computing","H. Liu; Y. Li; S. Wang","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China",IEEE Internet of Things Journal,"21 Oct 2022","2022","9","21","20841","20852","Edge servers can cache some delay-sensitive and resource-intensive applications to reduce the delay of user tasks. However, due to the limited resources of edge servers, they cannot cache all services and process all user requests like remote clouds. In order to take advantage of the low latency, we need to make full use of the limited resources of the edge servers and reasonably allocate user requests to edge servers for processing. Meanwhile, in order to maintain the efficient and long-term operation of the server cluster, we should also consider the load balancing of the cluster. How to balance the server load while making users have the best experience is an urgent problem to be solved. Solving the problem faces the challenge of the interaction between service placement and request scheduling, the tradeoff between communication and computation, and the consideration of response time and edge server load. We propose the service placement algorithm based on user visits and our request scheduling algorithm based on the simulated annealing algorithm. We verify the superiority of our algorithms in response time and server load balancing. The experimental results based on the real data sets prove our algorithm can cope with the actual situation and quickly converge to a favorable value.","2327-4662","","10.1109/JIOT.2022.3176631","National Key Research and Development Program of China(grant numbers:2020YFB1805502); NSFC(grant numbers:62032003,61922017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779350","Load balancing;mobile-edge computing (MEC);request scheduling;service placement","Servers;Optimization;Time factors;Delays;Load management;Base stations;Internet of Things","","13","","26","IEEE","20 May 2022","","","IEEE","IEEE Journals"
"Smart Meters for Smart Energy: A Review of Business Intelligence Applications","M. H. Raza; Y. M. Rind; I. Javed; M. Zubair; M. Q. Mehmood; Y. Massoud","Department of Electrical Engineering, MicroNano Laboratory, Information Technology University (ITU), Lahore, Pakistan; Department of Electrical Engineering, MicroNano Laboratory, Information Technology University (ITU), Lahore, Pakistan; Department of Electrical Engineering, MicroNano Laboratory, Information Technology University (ITU), Lahore, Pakistan; Innovative Technologies Laboratories (ITL), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Department of Electrical Engineering, MicroNano Laboratory, Information Technology University (ITU), Lahore, Pakistan; Innovative Technologies Laboratories (ITL), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia",IEEE Access,"2 Nov 2023","2023","11","","120001","120022","Smart Energy (SE) has emerged as a critical technology in tackling global challenges like climate change while addressing the rising energy demands driven by today’s data-intensive industrial revolution. SE integrates information and communication technologies into energy systems, optimizing them to meet these challenges effectively. At the core of SE operations are smart meters, playing a fundamental role in ensuring efficient functionality. These devices collect data, which is then leveraged to derive Business Intelligence (BI) for operations across the entire spectrum, from the sensing infrastructure to the cloud, primarily utilizing the Internet of Things (IoT) technology framework. With the increasing complexity of operations and the growing demand for optimization and enhanced functionality, the SE technology stack is evolving to integrate across all layers and domains. This integration has led to the stratification of computational load across IoT layers, intensifying the dependence on smart meter data for BI. Consequently, smart meters themselves have evolved to become more functional and complex. This paper’s novelty lies in its comprehensive exploration of the integration of BI with smart meter data. It delves into various aspects, including the different layers of intelligent operations within SE systems, the current state of the art, and diverse implementations of smart meters and their applications across operational locations, ranging from consumers to fog computing. The paper concludes by identifying research gaps and future directions, offering insights into the evolving requirements for the next generation of SE systems and the necessary adaptations in smart metering infrastructure to support these roles. This work contributes to a better understanding of the evolving landscape of data and computation in the context of SE, facilitating more efficient and effective energy management solutions.","2169-3536","","10.1109/ACCESS.2023.3326724","Innovative Technologies Laboratories (ITL), King Abdullah University of Science and Technology (KAUST), Saudi Arabia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290882","Smart grids;smart meters;AMI;cloud;business intelligence;artificial intelligence;smart energy","Smart meters;Sensors;Voltage measurement;Power measurement;Meters;Internet of Things;Business intelligence;Smart grids;Artificial intelligence;Energy management;Climate change","","13","","217","CCBYNCND","23 Oct 2023","","","IEEE","IEEE Journals"
"Intelligent VNF Placement to Mitigate DDoS Attacks on Industrial IoT","G. W. De Oliveira; M. Nogueira; A. L. d. Santos; D. M. Batista","Computer Science Department, University of São Paulo, São Paulo, Brazil; Computer Science Department, Federal University of Minas Gerais, Belo Horizonte, Brazil; Computer Science Department, Federal University of Minas Gerais, Belo Horizonte, Brazil; Computer Science Department, University of São Paulo, São Paulo, Brazil",IEEE Transactions on Network and Service Management,"29 Jun 2023","2023","20","2","1319","1331","The Internet of Things (IoT) has undergone rapid popularization, reaching a wide range of application domains, such as manufactures. Hence, more and more heterogeneous IoT devices have been deployed in a variety of industrial environments, progressively becoming common objects to the supply chain. The physical infrastructure of manufacturing systems has become complex and requires efficient and dynamic solutions for managing network performance and security. Network Function Virtualization (NFV) has attracted attention when the intention is to respond to security threats on Industrial IoT (IIoT). Few works use NFV to detect and mitigate security threats on IIoT networks, but even less consider performance indicators of the network context when placing the Virtual Network Functions (VNFs). Thus, this work introduces a Machine Learning (ML) approach to place security VNFs based on NFV performance to mitigate Distributed Denial of Service (DDoS) attacks on IIoT. Experiments considering a new composed data set and diverse ML techniques show ML classification as an alternative for IIoT scenarios, achieving, according to the best-performing technique, 99.40% of accuracy in relation to the ideal placement. To facilitate the reproduction of the work, all the code and data produced are publicly available.","1932-4537","","10.1109/TNSM.2023.3274364","Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq); INCT of the Future Internet for Smart Cities(grant numbers:465446/2014-0); Coordenação de Aperfeiçoamento de Pessoal de Nível Superior – Brasil (CAPES)(grant numbers:001); FAPESP(grant numbers:14/50937-1,15/24485-9); FAPESP(grant numbers:18/23098-0); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10121740","Industrial IoT;NFV;VNF placement;performance management;DDoS attacks;machine learning","Security;Performance evaluation;Industrial Internet of Things;Measurement;Denial-of-service attack;Time factors;Computer crime","","13","","39","IEEE","9 May 2023","","","IEEE","IEEE Journals"
"Toward the Observability of Cloud-Native Applications: The Overview of the State-of-the-Art","J. Kosińska; B. Baliś; M. Konieczny; M. Malawski; S. Zieliński","Institute of Computer Science, AGH University of Krakow, Kraków, Poland; Institute of Computer Science, AGH University of Krakow, Kraków, Poland; Institute of Computer Science, AGH University of Krakow, Kraków, Poland; Institute of Computer Science, AGH University of Krakow, Kraków, Poland; Institute of Computer Science, AGH University of Krakow, Kraków, Poland",IEEE Access,"24 Jul 2023","2023","11","","73036","73052","The Cloud-native model, established to enhance the Twelve-Factor patterns, is an approach to developing and deploying applications according to DevOps concepts, Continuous Integration/Continuous Delivery, containers, and microservices. The notion of observability can help us cope with the complexity of such applications. We present a Systematic Mapping Study (SMS) in the observability of Cloud-native applications. We have chosen 56 studies published between 2018 and 2022. The selected studies were thoroughly analyzed, compared, and classified according to the chosen comparative criteria. The presented SMS assesses engineering approaches, maturity, and efficiency of observability by deliberating around four research questions: 1) What provides the motivations for equipping Cloud-native applications with observability capabilities? 2) Which research areas are addressed in the related literature? 3) How are observability approaches implemented? 4) What are the future trends in the Cloud-native applications observability research?","2169-3536","","10.1109/ACCESS.2023.3281860","European Union’s Horizon 2020 Research and Innovation Program through Sano(grant numbers:857533); Sano Project carried out within the International Research Agendas Program of the Foundation for Polish Science, co-financed by the European Union under the European Regional Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10141603","Cloud-native;microservice architecture;observability;monitoring;logging;tracing;systematic mapping study","Observability;Cloud computing;Monitoring;Telemetry;Systematics;Search engines;Microservice architectures","","13","","91","CCBYNCND","1 Jun 2023","","","IEEE","IEEE Journals"
"Autoscaling Pods on an On-Premise Kubernetes Infrastructure QoS-Aware","L. M. Ruíz; P. P. Pueyo; J. Mateo-Fornés; J. V. Mayoral; F. S. Tehàs","Department of Computer Science and INSPIRES, University of Lleida, Lleida, Spain; Department of Computer Science and INSPIRES, University of Lleida, Lleida, Spain; Department of Computer Science and INSPIRES, University of Lleida, Lleida, Spain; Department of Computer Science and INSPIRES, University of Lleida, Lleida, Spain; Department of Computer Science and INSPIRES, University of Lleida, Lleida, Spain",IEEE Access,"30 Mar 2022","2022","10","","33083","33094","Cloud systems and microservices are becoming powerful tools for businesses. The evidence of the advantages of offering infrastructure, hardware or software as a service (IaaS, PaaS, SaaS) is overwhelming. Microservices and decoupled applications are increasingly popular. These architectures, based on containers, have facilitated the efficient development of complex SaaS applications. A big challenge is to manage and design microservices with a massive range of different facilities, from processing and data storage to computing predictive and prescriptive analytics. Computing providers are mainly based on data centers formed of massive and heterogeneous virtualized systems, which are continuously growing and diversifying over time. Moreover, these systems require integrating into current systems while meeting the Quality of Service (QoS) constraints. The primary purpose of this work is to present an on-premise architecture based on Kubernetes and Docker containers aimed at improving QoS regarding resource usage and service level objectives (SLOs). The main contribution of this proposal is its dynamic autoscaling capabilities to adjust system resources to the current workload while improving QoS.","2169-3536","","10.1109/ACCESS.2022.3158743","Intelligent Energy Europe (IEE) program and the Ministerio de Economía y Competitividad(grant numbers:PID2020-113614RB-C22); MCIN/AEI/10.1H6 network TIN2016-81840-REDT; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9732997","Cloud;microservices;Kubernetes;SLO;QoS","Quality of service;Containers;Cloud computing;Monitoring;Measurement;Microservice architectures;Computer architecture","","13","","36","CCBY","10 Mar 2022","","","IEEE","IEEE Journals"
"A Survey on Trusted Distributed Artificial Intelligence","M. A. AĞCA; S. Faye; D. Khadraoui","Luxembourg Institute of Science and Technology (LIST), Esch-Sur-Alzette, Luxembourg; Luxembourg Institute of Science and Technology (LIST), Esch-Sur-Alzette, Luxembourg; Luxembourg Institute of Science and Technology (LIST), Esch-Sur-Alzette, Luxembourg",IEEE Access,"30 May 2022","2022","10","","55308","55337","Emerging Artificial Intelligence (AI) systems are revolutionizing computing and data processing approaches with their strong impact on society. Data is processed with automated labelling pipelines rather than providing it as input to the system. The innovative nature increases the overall performance of monitoring/detection/reaction mechanisms for efficient system resource management. However, due to hardware-driven design limitations, networking and trust mechanisms are not flexible and adaptive enough to be able to interact and control the resources dynamically. Novel adaptive software-driven design approaches can enable us to build growing intelligent mechanisms with software-defined networking (SDN) features by virtualizing network functionalities with maximized features. These challenges and critical feature sets have been identified and introduced into this survey with their scientific background for AI systems and growing intelligent mechanisms. Furthermore, obstacles and research challenges between 1950–2021 are explored and discussed with a focus on recent years. The challenges are categorized according to three defined architectural perspectives (central, decentral/autonomous, distributed/hybrid) for emerging trusted distributed AI mechanisms. Therefore, resiliency and robustness can be assured in a dynamic context with an end-to-end Trusted Execution Environment (TEE) for growing intelligent mechanisms and systems. Furthermore, as presented in the paper, the trust measurement, quantification, and justification methodologies on top of Trusted Distributed AI (TDAI) can be applied in emerging distributed systems and their underlying diverse application domains, which will be explored and experimented in our future related works.","2169-3536","","10.1109/ACCESS.2022.3176385","Luxembourg Institute of Science and Technology (LIST) Ph.D. Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9777972","Trusted AI;distributed systems;software defined networking (SDN);trusted execution environment (TEE)","Artificial intelligence;Peer-to-peer computing;Security;Monitoring;Scalability;Throughput;Software defined networking","","13","","123","CCBY","18 May 2022","","","IEEE","IEEE Journals"
"Industrial Internet of Things Intelligence Empowering Smart Manufacturing: A Literature Review","Y. Hu; Q. Jia; Y. Yao; Y. Lee; M. Lee; C. Wang; X. Zhou; R. Xie; F. R. Yu","Future Network Research Center, Purple Mountain Laboratories, Nanjing, China; Future Network Research Center, Purple Mountain Laboratories, Nanjing, China; Department of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Mechanical and Electronic Engineering, Wuhan University of Technology, Wuhan, China; Department of Computer Science, Northwestern Polytechnical University, Xi’an, China; Department of Computer Science, Peking University, Beijing, China; Future Network Research Center, Purple Mountain Laboratories, Nanjing, China; State Key Laboratory of networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada",IEEE Internet of Things Journal,"22 May 2024","2024","11","11","19143","19167","The fiercely competitive business environment and increasingly personalized customization needs are driving the digital transformation and upgrading of the manufacturing industry. Industrial Internet of Things (IIoT) intelligence, which can provide innovative and efficient solutions for various aspects of the manufacturing value chain, illuminates the path of transformation for the manufacturing industry. It is time to provide a systematic vision of IIoT intelligence. However, existing surveys often focus on specific areas of IIoT intelligence, leading researchers and readers to have biases in their understanding of IIoT intelligence, that is, believing that research in one direction is the most important for the development of IIoT intelligence, while ignoring contributions from other directions. Therefore, this article provides a comprehensive overview of IIoT intelligence. We first conduct an in-depth analysis of the inevitability of manufacturing transformation and study the successful experiences from the practices of Chinese enterprises. Then, we give our definition of IIoT intelligence and demonstrate the value of IIoT intelligence for industries in functions, operations, deployments, and application. Afterward, we propose a hierarchical development architecture for IIoT intelligence, which consists of five layers. The practical values of technical upgrades at each layer are illustrated by a close look on lighthouse factories. Following that, we identify seven kinds of technologies that accelerate the transformation of manufacturing, and clarify their contributions. The ethical implications and environmental impacts of adopting IIoT intelligence in manufacturing are analyzed as well. Finally, we explore the open challenges and development trends from four aspects to inspire future researches.","2327-4662","","10.1109/JIOT.2024.3367692","National Natural Science Foundation of China(grant numbers:92367104,92267301); Purple Mountain Talents-Jiangning Baijia Lake Plan Program(grant numbers:74072203-3); Natural Science Foundation of Hubei Province(grant numbers:2023AFB128); Teaching Research Project of Wuhan University of Technology(grant numbers:W2022093); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10440434","Artificial intelligence (AI);Industrial Internet of Things (IIoT);IIoT intelligence;smart manufacturing","Industrial Internet of Things;Manufacturing;Computer architecture;Industries;Smart manufacturing;Market research;Costs","","13","","219","IEEE","20 Feb 2024","","","IEEE","IEEE Journals"
"AI-Enhanced Cloud-Edge-Terminal Collaborative Network: Survey, Applications, and Future Directions","H. Gu; L. Zhao; Z. Han; G. Zheng; S. Song","State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA; School of Engineering, University of Warwick, Coventry, U.K.; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China",IEEE Communications Surveys & Tutorials,"22 May 2024","2024","26","2","1322","1385","The cloud-edge-terminal collaborative network (CETCN) is considered as a novel paradigm for emerging applications owing to its huge potential in providing low-latency and ultra-reliable computing services. However, achieving such benefits is very challenging due to the heterogeneous computing power of terminal devices and the complex environment faced by the CETCN. In particular, the high-dimensional and dynamic environment states cause difficulties for the CETCN to make efficient decisions in terms of task offloading, collaborative caching and mobility management. To this end, artificial intelligence (AI), especially deep reinforcement learning (DRL) has been proven effective in solving sequential decision-making problems in various domains, and offers a promising solution for the above-mentioned issues due to several reasons. Firstly, accurate modelling of the CETCN, which is difficult to obtain for real-world applications, is not required for the DRL-based method. Secondly, DRL can effectively respond to high-dimensional and dynamic tasks through iterative interactions with the environment. Thirdly, due to the complexity of tasks and the differences in resource supply among different vendors, collaboration is required between different vendors to complete tasks. The multi-agent DRL (MADRL) methods are very effective in solving collaborative tasks, where the collaborative tasks can be jointly completed by cloud, edge and terminal devices which provided by different vendors. This survey provides a comprehensive overview regarding the applications of DRL and MADRL in the context of CETCN. The first part of this survey provides a depth overview of the key concepts of the CETCN and the mathematical underpinnings of both DRL and MADRL. Then, we highlight the applications of RL algorithms in solving various challenges within CETCN, such as task offloading, resource allocation, caching and mobility management. In addition, we extend discussion to explore how DRL and MADRL are making inroads into emerging CETCN scenarios like intelligent transportation system (ITS), the industrial Internet of Things (IIoT), smart health and digital agriculture. Furthermore, security considerations related to the application of DRL within CETCN are addressed, along with an overview of existing standards that pertain to edge intelligence. Finally, we list several lessons learned in this evolving field and outline future research opportunities and challenges that are critical for the development of the CETCN. We hope this survey will attract more researchers to investigate scalable and decentralized AI algorithms for the design of CETCN.","1553-877X","","10.1109/COMST.2023.3338153","National Key Research and Development Program of China(grant numbers:2020YFB1807700); Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101120003); Key Research and Development Program of Shaanxi(grant numbers:2022KWZ-09); Postdoctoral Research Program of Shaanxi Province; NSF(grant numbers:CNS-2107216,CNS-2128368,CMMI-2222810,ECCS-2302469); U.S. Department of Transportation; Toyota Motor Corporation; Amazon; National Natural Science Foundation of China(grant numbers:NSFC62071352); Royal Society International Exchanges(grant numbers:IEC/NSFC/181395); NSFC/RGC Joint Research Scheme sponsored by the Research Grants Council of the Hong Kong Special Administrative Region, China; National Natural Science Foundation of China(grant numbers:N_HKUST656/22); Higher Education Discipline Innovation Project(grant numbers:B08038); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10336879","Cloud-edge-terminal collaborative network (CETCN);single-agent reinforcement learning (SARL);multiagent reinforcement learning (MARL);network architecture;task offloading;resource allocation;collaborative caching;mobility management;applications;security;standards","Task analysis;Resource management;Surveys;Delays;Cloud computing;Servers;Federated learning","","12","","449","IEEE","1 Dec 2023","","","IEEE","IEEE Journals"
"Edge Video Analytics: A Survey on Applications, Systems and Enabling Techniques","R. Xu; S. Razavi; R. Zheng","Department of Computing and Software, McMaster University, Hamilton, ON, Canada; Department of Civil Engineering, McMaster University, Hamilton, ON, Canada; Department of Computing and Software, McMaster University, Hamilton, ON, Canada",IEEE Communications Surveys & Tutorials,"21 Nov 2023","2023","25","4","2951","2982","Video, as a key driver in the global explosion of digital information, can create tremendous benefits for human society. Governments and enterprises are deploying innumerable cameras for a variety of applications, e.g., law enforcement, emergency management, traffic control, and security surveillance, all facilitated by video analytics (VA). This trend is spurred by the rapid advancement of deep learning (DL), which enables more precise models for object classification, detection, and tracking. Meanwhile, with the proliferation of Internet-connected devices, massive amounts of data are generated daily, overwhelming the cloud. Edge computing, an emerging paradigm that moves workloads and services from the network core to the network edge, has been widely recognized as a promising solution. The resulting new intersection, edge video analytics (EVA), begins to attract widespread attention. Nevertheless, only a few loosely-related surveys exist on this topic. The basic concepts of EVA (e.g., definition, architectures) were not fully elucidated due to the rapid development of this domain. To fill these gaps, we provide a comprehensive survey of the recent efforts on EVA. In this paper, we first review the fundamentals of edge computing, followed by an overview of VA. EVA systems and their enabling techniques are discussed next. In addition, we introduce prevalent frameworks and datasets to aid future researchers in the development of EVA systems. Finally, we discuss existing challenges and foresee future research directions. We believe this survey will help readers comprehend the relationship between VA and edge computing, and spark new ideas on EVA.","1553-877X","","10.1109/COMST.2023.3323091","China Scholarship Council(grant numbers:202108320092); NSERC Discovery Grants; Canada Research Chair Program; McMaster Faculty of Engineering’s Multidisciplinary Ph.D. Research Support; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10274950","Video analytics;edge computing;computer vision;deep learning","Surveys;Edge computing;Visual analytics;Tutorials;Cloud computing;Real-time systems;Cameras","","12","","290","IEEE","10 Oct 2023","","","IEEE","IEEE Journals"
"Toward Design of Internet of Things and Machine Learning-Enabled Frameworks for Analysis and Prediction of Water Quality","M. A. Rahu; A. F. Chandio; K. Aurangzeb; S. Karim; M. Alhussein; M. S. Anwar","Department of Electronic Engineering, Quaid-e-Awam University of Engineering, Science and Technology, Nawabshah, Pakistan; Department of Electronic Engineering, Quaid-e-Awam University of Engineering, Science and Technology, Nawabshah, Pakistan; Department of Computer Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Department of Telecommunication Engineering, Quaid-e-Awam University of Engineering, Science and Technology, Nawabshah, Pakistan; Department of Computer Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Department of AI and Software, Gachon University, Seongnam-si, South Korea",IEEE Access,"22 Sep 2023","2023","11","","101055","101086","The degradation of water quality has become a critical concern worldwide, necessitating innovative approaches for monitoring and predicting water quality. This paper proposes an integrated framework that combines the Internet of Things (IoT) and machine learning paradigms for comprehensive water quality analysis and prediction. The IoT-enabled framework comprises four modules: sensing, coordinator, data processing, and decision. The IoT framework is equipped with temperature, pH, turbidity, and Total Dissolved Solids (TDS) sensors to collect the data from Rohri Canal, SBA, Pakistan. The acquired data is preprocessed and then analyzed using machine learning models to predict the Water Quality Index (WQI) and Water Quality Class (WQC). With this aim, we designed a machine learning-enabled framework for water quality analysis and prediction. Preprocessing steps such as data cleaning, normalization using the Z-score technique, correlation, and splitting are performed before applying machine learning models. Regression models: LSTM (Long Short-Term Memory), SVR (Support Vector Regression), MLP (Multilayer Perceptron) and NARNet (Nonlinear Autoregressive Network) are employed to predict the WQI, and classification models: SVM (Support Vector Machine), XGBoost (eXtreme Gradient Boosting), Decision Trees, and Random Forest are employed to predict the WQC. Before that, the Dataset used for evaluating machine learning models is split into two subsets: Dataset 1 and Dataset 2. Dataset 1 comprises 600 values for each parameter, while Dataset 2 includes the complete set of 6000 values for each parameter. This division enables comparison and evaluation of the models’ performance. The results indicate that the MLP regression model has strong predictive performance with the lowest Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE) values, along with the highest R-squared (0.93), indicating accurate and precise predictions. In contrast, the SVR model demonstrates weaker performance, evidenced by higher errors and a lower R-squared (0.73). Among classification algorithms, the Random Forest achieves the highest metrics: accuracy (0.91), precision (0.93), recall (0.92), and F1-score (0.91). It is also conceived that the machine learning models perform better when applied to datasets with smaller numbers of values compared to datasets with larger numbers of values. Moreover, comparisons with existing studies reveal this study’s improved regression performance, with consistently lower errors and higher R-squared values. For classification, the Random Forest model outperforms others, with exceptional accuracy, precision, recall, and F1-score metrics.","2169-3536","","10.1109/ACCESS.2023.3315649","King Saud University, Riyadh, Saudi Arabia, through the Researchers Supporting Project(grant numbers:RSPD2023R947); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10251529","Data collection;environmental monitoring;Internet of Things (IoT);machine learning;water quality analysis;water quality class (WQC);water quality index (WQI)","Water quality;Machine learning;Data models;Monitoring;Water resources;Sensors;Predictive models","","12","","99","CCBYNCND","14 Sep 2023","","","IEEE","IEEE Journals"
"6G Cloud-Native System: Vision, Challenges, Architecture Framework and Enabling Technologies","Q. Li; Z. Ding; X. Tong; G. Wu; S. Stojanovski; T. Luetzenkirchen; A. Kolekar; S. Bangolae; S. Palat","Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA",IEEE Access,"19 Sep 2022","2022","10","","96602","96625","Every generation of wireless technologies needs to bring a set of new system capabilities to enable future applications and services, the sixth generation mobile system (6G) is no exception. This paper provides an overview of the technology transformation from the communication-centric the forth generation mobile system (4G) and the fifth generation mobile system (5G) to the compute-centric 6G with the cloud-native system framework as the foundation of the next generation technologies. We explain what 6G plans to achieve, the fundamental reasons for this technology transformation, the architecture framework and enabling technologies to achieve 6G cloud-native technology objectives. This paper intends to provide a technical deep dive on the 6G cloud-native system to trigger more discussions, innovations and bring the technology transformation from concept to reality.","2169-3536","","10.1109/ACCESS.2022.3205341","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9882121","6G;cloud-native;cloud computing;communication-computing convergence;system architecture;ubiquitous computing","6G mobile communication;Cloud computing;5G mobile communication;Artificial intelligence;Ubiquitous computing;Systems architecture;Security","","12","","62","CCBY","8 Sep 2022","","","IEEE","IEEE Journals"
"A Federated Platform Enabling a Systematic Collaboration Among Devices, Data and Functions for Smart Mobility","L. You; M. Danaf; F. Zhao; J. Guan; C. L. Azevedo; B. Atasoy; M. Ben-Akiva","School of Intelligent Systems Engineering, Sun Yat-sen University, Shenzhen, China; Intelligent Transportation Systems Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Future Urban Mobility IRG, Singapore-MIT Alliance for Research and Technology Centre, 4 Engineering Drive 3, Singapore; School of Architecture, Harbin Institute of Technology, Shenzhen, China; Department of Management Engineering, Technical University of Denmark, Kongens Lyngby, Denmark; Intelligent Transportation Systems Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Intelligent Transportation Systems Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA",IEEE Transactions on Intelligent Transportation Systems,"29 Mar 2023","2023","24","4","4060","4074","Through the vast adoption and application of emerging technologies, the intelligence and autonomy of smart mobility can be substantially elevated to address more diversified demands and supplies. Along with this trend, a systematic collaboration among three essential elements of smart mobility services, namely devices, data and functions, is being studied to comprehensively break down the intrinsic barriers that existed in current solutions, to support the integration of connectable devices, the fusion of heterogeneous data, the composability of reusable functions, and the flexibility in their cooperations. To enable such a collaboration, this paper proposes a federated platform, called Future Mobility Sensing Advisor (FMSA), which can 1) manage the three elements through standardized interfaces separately and uniformly; 2) create a fully connected knowledge graph to orchestrate the three elements efficiently and effectively; 3) support the client-server interaction in centralized and federated modes to handle service requests and edge resources with various availability and accessibilities jointly and adaptively; and 4) accommodate various mobility services to foster harmonious and sustainable mobility tenderly and invisibly. Moreover, the efficiency and effectiveness of the platform are also tested through a performance evaluation, and a pilot supported at the Great Boston Area, respectively. As a result, it shows that FMSA can 1) achieve high performance by using the two interaction modes selectively, and 2) renovate smart mobility towards sustainability through personalized services that can measure user preferences and system objectives mutually.","1558-0016","","10.1109/TITS.2023.3236991","National Natural Science Foundation of China(grant numbers:62002398); Collaborative Innovation Center for Transportation of Guangzhou(grant numbers:202206010056); National Research Foundation Singapore through the Future Urban Mobility IRG Research Program of Singapore-MIT Alliance for Research and Technology; Tripod (Sustainable Travel Incentives with Prediction, Optimization and Personalization) Project under the U.S. Department of Energy Advanced Research Projects Agency-6 Energy (ARPA-E) Traveler Response Architecture using Novel Signaling for Network Efficiency in Transportation (TRANSNET) program(grant numbers:DE-AR0000611); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10024386","Smart mobility;systematic collaboration;federated platform;service orchestration;federated computing","Collaboration;Systematics;Sensors;Public transportation;Distributed databases;Data integration;Costs","","12","","34","IEEE","23 Jan 2023","","","IEEE","IEEE Journals"
"Hierarchical Modeling of Complex Internet of Things Systems Using Conceptual Modeling Approaches","K. M. Abbasi; T. A. Khan; I. U. Haq","Department of Software Engineering, Bahria University, Islamabad, Pakistan; Department of Software Engineering, Bahria University, Islamabad, Pakistan; Department of CIS, Pakistan Institute of Engineering and Applied Sciences (PIEAS), Islamabad, Pakistan",IEEE Access,"7 Aug 2019","2019","7","","102772","102791","Popularity of the Internet of things (IoT) is currently on the rise. Academia as well as industry is equally fascinated by this disruptive technology. The IoT research and implementation have succeeded securing their place in numerous domains including, but not limited to, business, transportation and logistics, health care, smart cities, and agriculture. Conceptual modeling is pivotal to the design and implementation of a system. Various modeling approaches have been used for a variety of the IoT applications and case studies. In this paper, we examine various innovative applications of the existing conceptual modeling approaches for the IoT. We analyze the use of these approaches against different concerns in isolation as well as in mutual combination for the IoT. We also define a set of terminologies that we frequently encounter in this manuscript and those likely to be used while adopting proposed modeling approaches for the IoT scenarios. This study also attempts to reconcile different modeling approaches toward a unified modeling system. For this purpose, we map various conceptual modeling approaches against different layers of functional model of the IoT reference architecture. There is also a scenario in the end of the paper which elaborates the use of different modeling approaches in hierarchy for a complex IoT system.","2169-3536","","10.1109/ACCESS.2019.2930933","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8771106","Agent-based modeling;ambient-oriented modeling;aspect-oriented modeling;conceptual modeling approaches;contract-based modeling;fuzzy-logic modeling;the Internet of Things;network-based modeling;object-based modeling;ontology;service-oriented modeling","Internet of Things;Computational modeling;Analytical models;Security;Software;Adaptation models;Computer architecture","","12","","131","CCBY","24 Jul 2019","","","IEEE","IEEE Journals"
"A Survey on the Bottleneck Between Applications Exploding and User Requirements in IoT","S. Cui; F. Farha; H. Ning; Z. Zhou; F. Shi; M. Daneshmand","School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; School of Information Engineering, China University of Geosciences (Beijing), Beijing, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; School of Business, Stevens Institute of Technology, Hoboken, NJ, USA",IEEE Internet of Things Journal,"22 Dec 2021","2022","9","1","261","273","The rapid growth of the Internet of Things (IoT) and the increasing number of connected devices have propelled the proliferation of offered applications, causing “applications exploding.” In the context of IoT, filtering and selecting the most relevant applications in a given situation is a challenging task. Thus, developing techniques that can alleviate applications exploding and meet users’ requirements is highly demanded for IoT development. This survey focuses on applications exploding in the IoT and reviews some of the existing techniques, such as intelligent sensing, content distribution network, microservices, and 5G, which help mitigate the effects of applications exploding. Furthermore, the survey discusses how to describe user requirements and offer application services to better match the two. In addition, this survey presents the smart home as an instance of typical IoT applications and explores how adaptive users’ requirements for food ordering can be better met when various food provider applications are available for choice. Finally, partially resolved and unresolved bottlenecks brought by applications exploding are put forward to be further researched by the technical and scientific community.","2327-4662","","10.1109/JIOT.2021.3097634","National Natural Science Foundation of China(grant numbers:61872038); Fundamental Research Funds for the Central Universities(grant numbers:FRF-BD-18-016A); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9486861","Applications exploding;matching applications bottleneck;smart home;user requirements","Sensors;Internet of Things;Security;Protocols;Cloud computing;Smart homes;Scalability","","12","","76","IEEE","15 Jul 2021","","","IEEE","IEEE Journals"
"A Generic Future Mobility Sensing System for Travel Data Collection, Management, Fusion, and Visualization","L. You; F. Zhao; L. Cheah; K. Jeong; P. C. Zegras; M. Ben-Akiva","Future Urban Mobility IRG, Singapore-MIT Alliance for Research and Technology Centre, Singapore; Future Urban Mobility IRG, Singapore-MIT Alliance for Research and Technology Centre, Singapore; Engineering Systems and Design, Singapore University of Technology and Design, Singapore; Intelligent Transportation Systems Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Urban Studies and Planning, Massachusetts Institute of Technology, Cambridge, MA, USA; Civil and Environmental Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA",IEEE Transactions on Intelligent Transportation Systems,"2 Oct 2020","2020","21","10","4149","4160","In studies of human mobility, there is a need for a holistic system for collection of sensing data, management of data flows, fusion of multiple data sources, and visualization of integrated data to better understand travel behavior. We have designed and implemented a generic Future Mobility Sensing (FMS) system to serve these purposes. FMS harnesses various sensing technologies, heterogeneous multi-source data and analytical functionalities with three dedicated platforms, namely 1) the FMS Data Collection Platform, which intertwines sensing objects, machine learning algorithms and user verifications to collect high resolution, multi-day travel data; 2) the FMS Data Management Platform, which provides standardized APIs to access data stored in an interconnected data model; and 3) the FMS Data Fusion and Visualization Platform, which consolidates multi-source data to be interpreted and presented. Together, the three platforms form a mobility sensing flow to facilitate data-driven analysis and decision-making. With FMS, heterogeneous multi-source data are suitably integrated for analysis, and multi-dimensional knowledge is extracted and presented in intuitive and interactive analytical dashboards. The system is intended to be generic to support different requirements of mobility studies, such as travel surveys, as its function modules are reusable and can be customized to support a unified data collection, management, fusion and visualization process. This paper introduces the overall architecture of the FMS system and summarizes various applications that it can support. Specifically, we present a study of commercial vehicle parking in Singapore to demonstrate the capability of the system.","1558-0016","","10.1109/TITS.2019.2938828","National Research Foundation Singapore through the Singapore-MIT Alliance for Research and Technology’s Future Urban Mobility IRG Research Program; Singapore Ministry of National Development and the National Research Foundation; Land and Liveability National Innovation Challenge(grant numbers:L2NICTDF1-2016-1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8833515","Mobility sensing flow;multi-source data collection;big data;travel survey","Frequency modulation;Sensors;Data visualization;Data collection;Data mining;Data integration;Urban areas","","12","","32","IEEE","11 Sep 2019","","","IEEE","IEEE Journals"
"eBPF: A New Approach to Cloud-Native Observability, Networking and Security for Current (5G) and Future Mobile Networks (6G and Beyond)","D. Soldani; P. Nahi; H. Bour; S. Jafarizadeh; M. F. Soliman; L. Di Giovanna; F. Monaco; G. Ognibene; F. Risso","Rakuten Mobile Inc, Tokyo, Setagaya-ku, Japan; Rakuten Mobile Inc, Tokyo, Setagaya-ku, Japan; Rakuten Mobile Inc, Tokyo, Setagaya-ku, Japan; Rakuten Mobile Inc, Tokyo, Setagaya-ku, Japan; Rakuten Mobile Inc, Tokyo, Setagaya-ku, Japan; DAUIN Department, Politecnico di Torino, Turin, Italy; DAUIN Department, Politecnico di Torino, Turin, Italy; DAUIN Department, Politecnico di Torino, Turin, Italy; DAUIN Department, Politecnico di Torino, Turin, Italy",IEEE Access,"14 Jun 2023","2023","11","","57174","57202","Modern mobile communication networks and new service applications are deployed on cloud-native platforms. Kubernetes (K8s) is the de facto distributed operating system for container orchestration, and the extended version of the Berkeley Packet Filter (eBPF)– in the Linux (and MS Windows) kernel– is fundamentally changing the approach to cloud-native networking, security, and observability. In this paper, we introduce what eBPF is, its potential for Telco cloud, and review some of the most promising pricing and billing models applied to this revolutionary operating system (OS) technology. These models include schemes based on a data source usage model or the number of eBPF agents deployed on the network, linked to specific eBPF modules. These modules encompass network observability, runtime security, and power dissipation monitoring. Next, we present our eBPF platform, named Sauron in this work, and demonstrate how eBPF allows us to write custom code and dynamically load eBPF programs into the kernel. These programs enable us to estimate the energy consumption of cloud-native functions, derive performance counters and gauges for transport networks, 5G applications, and non-access stratum protocols. Additionally, we can detect and respond to unauthorized access to cloud-native resources in real-time using eBPF. Our experimental results demonstrate the technical feasibility of eBPF in achieving highly performant monitoring, observability, and security tooling for current mobile networks (5G, 5G Advanced) as well as future networks (6G and beyond).","2169-3536","","10.1109/ACCESS.2023.3281480","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10138542","eBPF;extended Berkeley packet filter;cloud-native observability;cloud-native security;cloud-native networking;cloud-native monitoring;5G;5G Advanced;6G;Kubernetes;K8s","5G mobile communication;Cloud computing;Security;Observability;Linux;6G mobile communication;Band-pass filters","","12","","42","CCBYNCND","30 May 2023","","","IEEE","IEEE Journals"
"Implementation of Network-Coded Cooperation for Energy Efficient Content Distribution in 5G Mobile Small Cells","R. Torre; I. Leyva-Mayorga; S. Pandi; H. Salah; G. T. Nguyen; F. H. P. Fitzek","Deutsche Telekom Chair of Communication Networks, Technische Universität Dresden, Dresden, Germany; Department of Electronic Systems, Aalborg University, Aalborg, Denmark; Deutsche Telekom Chair of Communication Networks, Technische Universität Dresden, Dresden, Germany; Deutsche Telekom Chair of Communication Networks, Technische Universität Dresden, Dresden, Germany; Deutsche Telekom Chair of Communication Networks, Technische Universität Dresden, Dresden, Germany; Deutsche Telekom Chair of Communication Networks, Technische Universität Dresden, Dresden, Germany",IEEE Access,"19 Oct 2020","2020","8","","185964","185980","The continuous increase of mobile data traffic calls for the design of energy-efficient content distribution mechanisms, to be incorporated in the fifth generation of mobile networks, 5G. One of the biggest concerns of the companies and the research community is to reduce the energy consumption in both the user equipments (UEs) and the network equipment. In this article, we present a novel content distribution framework called Network-Coded Cooperative (NCC) Networks, which benefits from the interplay between mobile clouds (MC) and Random Linear Network Coding (RLNC) to reduce the overall energy consumption in the devices that take part in the communication. This novel framework leads to reduced energy consumption by offloading the cellular interface to a link with greater energy efficiency, for instance, WiFi, within the mobile small cell. We evaluate the performance of our framework analytically and in practical implementation (i.e., testbed) in terms of throughput, energy savings, packet decoding ratio, latency, and synchronicity. In comparison to the conventional content distribution system, for the case of four users, the analytical model and the testbed implementation show energy savings of more than 12% and 8%, respectively. Furthermore, network usage is reduced, losses are neutralized, and the content is synchronously distributed to all users.","2169-3536","","10.1109/ACCESS.2020.3029601","European Union’s H2020 Research and Innovation Program(grant numbers:H2020-MCSA-ITN-2016-SECRET 722424); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9217438","Network coding;random linear network coding (RLNC);cooperative communication;content distribution;cellular networks;mobile small cells;energy efficiency;traffic offload","Energy consumption;Analytical models;Cellular networks;Encoding;5G mobile communication;Throughput;Protocols","","12","","54","CCBY","8 Oct 2020","","","IEEE","IEEE Journals"
"An Investigation of Cyber-Attacks and Security Mechanisms for Connected and Autonomous Vehicles","S. Gupta; C. Maple; R. Passerone","Department of Information Engineering and Computer Science (DISI), University of Trento, Trento, Italy; Secure Cyber Systems Research Group (SCSRG), Warwick Manufacturing Group (WMG), The University of Warwick, Coventry, U.K.; Department of Information Engineering and Computer Science (DISI), University of Trento, Trento, Italy",IEEE Access,"29 Aug 2023","2023","11","","90641","90669","Connected and autonomous vehicles (CAVs) can fulfill the emerging demand for smart transportation on a global scale. Such innovations for transportation can bring manyfold benefits, from fully autonomous driving services to proactive vehicle monitoring and traffic management. However, given the complexity involved in the deployment of CAVs, zero-tolerance safety, and security measures must be incorporated to avert vehicle immobilization, road accidents, disclosure of sensitive data, or any potential threats. In this article, we conceive a reference architecture for a CAVs ecosystem to derive a common attack taxonomy for the investigation of existing and emerging cyber threats. Subsequently, we discuss security mechanisms for the CAVs ecosystem that can be useful for the safe and secure transportation of passengers from one destination to another based on comprehensive studies of academic literature and industry white papers. Our work can provide valuable insights to security engineers and system architects for investigating security problems using a top-to-bottom approach and can aid in envisioning robust security solutions to ensure seamless CAVs operations.","2169-3536","","10.1109/ACCESS.2023.3307473","Fondazione Bruno Kessler (FBK) AI@TN Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10226207","Connected and autonomous vehicles;edge computing;fog computing;cloud computing;cyber attacks;security mechanisms","Security;Ecosystems;Edge computing;Autonomous vehicles;Sensors;Connected vehicles;Vehicle-to-everything;Cloud computing;Cyberattack","","12","","207","CCBYNCND","22 Aug 2023","","","IEEE","IEEE Journals"
"Rate-Adaptive Fog Service Platform for Heterogeneous IoT Applications","T. Zhang; J. Jin; X. Zheng; Y. Yang","School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia; Department of Computing, Macquarie University, Sydney, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia",IEEE Internet of Things Journal,"10 Jan 2020","2020","7","1","176","188","With the advancement of the Internet of Things (IoT) technologies, the number of heterogeneous IoT applications requiring a variety of resources and services is increasing dramatically. Recently, the introduction of fog computing has further unlocked the potential of real-time services within the IoT context. On the basis of fog architecture, we herein propose a novel rate-adaptive fog service platform aiming at heterogeneous services provisioning and optimized service rate allocation. By forming several service groups in the fog network in which each service could be adequately provisioned, service consumers would always benefit from the fact that the majority of services produced by the IoT applications are in their proximity and thus are delivered to the destination promptly. Taking advantage of the well-known network utility maximization (NUM) approach, a service rate-adaptive algorithm is developed to empower fog nodes working together to adjust service delivery rate dynamically. Throughout this process, the algorithm takes the current network condition and constraint into account to ensure the rate is calibrated in favor of providing satisfactory quality of service (QoS) to each service receiver at the same time. Compared to other resource allocation strategies that mainly focus on allocating resources for a single network service, our proposed platform is capable of not only dealing with both the elastic and inelastic services but also handling the abrupt network changes and converging back to the global optimum rapidly.","2327-4662","","10.1109/JIOT.2019.2945328","Australian Research Council(grant numbers:DP190102828,DP180100212); Australian Government Research Training Program Scholarship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8856237","Fog computing;Internet of Things (IoT);network utility maximization (NUM);quality of service (QoS);service-oriented networking","Internet of Things;Cloud computing;Quality of service;Real-time systems;Edge computing;Computer architecture;Resource management","","12","","37","IEEE","3 Oct 2019","","","IEEE","IEEE Journals"
"Embracing Complexity: Agent-Based Modeling for HetNets Design and Optimization via Concurrent Reinforcement Learning Algorithms","M. Ibrahim; U. S. Hashmi; M. Nabeel; A. Imran; S. Ekin","School of Electrical and Computer Engineering, Oklahoma State University, Stillwater, OK, USA; School of Electrical Engineering and Computer Science, National University of Sciences and Technology, Islamabad, Pakistan; AI4Networks Research Center, School of Electrical and Computer Engineering, University of Oklahoma, Tulsa, OK, USA; AI4Networks Research Center, School of Electrical and Computer Engineering, University of Oklahoma, Tulsa, OK, USA; School of Electrical and Computer Engineering, Oklahoma State University, Stillwater, OK, USA",IEEE Transactions on Network and Service Management,"9 Dec 2021","2021","18","4","4042","4062","Complexity is an inherent property in wireless heterogeneous networks (HetNets). In this paper, we investigate the application of the agent-based modeling (ABM) tool for optimization of complex and dynamic HetNets. The proposed framework contains a diversity of game-theoretic, machine learning, and rule-based algorithms within the same model. We present and analyze a HetNet ABM model that runs parallel reinforcement learning (RL) algorithms for spectrum deployment, interference management, resource allocation, and load balancing at both micro and macrocell levels. In our proposed model, two RL-based algorithms work jointly to manage the co-tier and cross-tier interferences. The macrocell runs the first algorithm to control the transmission power of the small cells. The second RL algorithm is run by small cells to assign the users to the sub-bands with less interference levels. Simultaneously, the user association is decided by the users depending on the available resources at the cells and user preferences. The model is then evaluated under various network load conditions to deduce relationships between the cell loads, aggregate bit rate, latency, and user association. Moreover, the system is assessed in a dynamic network scenario with moving users and is confirmed to possess the ability to attain convergence with sufficient performance levels.","1932-4537","","10.1109/TNSM.2021.3121282","National Science Foundation(grant numbers:1923295,1923669); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9580469","HetNets;complexity;agent-based-modeling;multi-agent-systems;5G and beyond","Interference;Load modeling;Resource management;Macrocell networks;Analytical models;Adaptation models;Optimization","","12","","90","IEEE","19 Oct 2021","","","IEEE","IEEE Journals"
"Energy Efficient Placement of Workloads in Composable Data Center Networks","O. O. Ajibola; T. E. H. El-Gorashi; J. M. H. Elmirghani","School of Electronic and Electrical Engineering, University of Leeds, Leeds, U.K.; School of Electronic and Electrical Engineering, University of Leeds, Leeds, U.K.; School of Electronic and Electrical Engineering, University of Leeds, Leeds, U.K.",Journal of Lightwave Technology,"7 May 2021","2021","39","10","3037","3063","This paper studies the energy efficiency of composable data center (DC) infrastructures over network topologies. Using a mixed integer linear programming (MILP) model, we compare the performance of disaggregation at rack-scale and pod-scale over selected electrical, optical and hybrid network topologies relative to a traditional DC. Relative to a pod-scale DC, the results show that physical disaggregation at rack-scale is sufficient for optimal efficiency when the optical network topology is adopted, and resource components are allocated in a suitable manner. The optical network topology also enables optimal energy efficiency in composable DCs. The paper also studies logical disaggregation of traditional DC servers over an optical network topology. Relative to physical disaggregation at rack-scale, logical disaggregation of server resources within each rack enables marginal fall in the total DC power consumption (TDPC) due to improved resource demands placement. Hence, an adaptable composable infrastructure that can support both in memory (access) latency sensitive and insensitive workloads is enabled. We also conduct a study of the adoption of micro-service architecture in both traditional and composable DCs. Our results show that increasing the modularity of workloads improves the energy efficiency in traditional DCs, but disproportionate utilization of DC resources persists. A combination of disaggregation and micro-services achieved up to 23% reduction in the TDPC of the traditional DC by enabling optimal resources utilization and energy efficiencies. Finally, we propose a heuristic for energy efficient placement of workloads in composable DCs which replicates the trends produced by the MILP model formulated in this paper.","1558-2213","","10.1109/JLT.2021.3063325","Engineering and Physical Sciences Research Council; INTelligent Energy aware NETworks(grant numbers:EP/H040536/1); SwiTching And tRansmission(grant numbers:EP/K016873/1); Terabit Bidirectional Multi-user Optical Wireless System(grant numbers:EP/S016570/1); Petroleum Technology Trust Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9369047","Composable infrastructures;energy efficient data centers;micro-services;MILP;optical networks;rack-scale data center;software defined infrastructures","Servers;Software;Network topology;Energy efficiency;Hardware;Topology;Optical fiber networks","","12","","49","IEEE","3 Mar 2021","","","IEEE","IEEE Journals"
"Optimization of Business Process Execution in Services Architecture: A Systematic Literature Review","T. Górski; A. P. WOźniak","Department of Computer Science, Polish Naval Academy (PNA) of the Heroes of Westerplatte, Gdynia, Poland; Institute of Computer and Information Systems, Military University of Technology, Warsaw, Poland",IEEE Access,"13 Aug 2021","2021","9","","111833","111852","Web services have become a standard way to provide functions of information systems. The number of web services grows rapidly with the increasing popularity of microservices architecture. In consequence, many business processes are executed entirely through web services. Therefore, optimizing the performance of business process execution may bring many benefits. There are many optimization methods in this area. Our systematic literature review aims to introduce available methods to researchers interested in the optimization of business process execution. We queried four databases: ACM, IEEE Xplore, Science Direct, and Springer. Out of 12150 initially found papers, we have selected 128 for the review. We have grouped methods presented in those papers into three stages of business process optimization: Resource Allocation, Service Composition, and Service Scheduling. Service Composition attracts the largest group of researchers with a vast majority of 119 articles in it. Moreover, the most popular are genetic algorithms. In general, researchers mainly propose heuristic methods that optimize business processes during run-time. We see the potential for further exploration at both Resource Allocation and Service Scheduling stages.","2169-3536","","10.1109/ACCESS.2021.3102668","Statutory Funds of the Department of Computer Science, Polish Naval Academy (PNA) of the Heroes of Westerplatte, through the Project Architectural views model of cooperating information technology (IT) systems; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9507503","Service-oriented architecture;business process;optimization;reliability;micro-services","Business;Service-oriented architecture;Optimization;Reliability;Resource management;Computer architecture;Software","","12","","136","CCBY","5 Aug 2021","","","IEEE","IEEE Journals"
"Opportunistic UAV Deployment for Intelligent On-Demand IoV Service Management","H. Sami; R. Saado; A. E. Saoudi; A. Mourad; H. Otrok; J. Bentahar","Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada; Department of CSM, Cyber Security Systems and Applied AI Research Center, Lebanese American University, Beirut, Lebanon; Department of CSM, Cyber Security Systems and Applied AI Research Center, Lebanese American University, Beirut, Lebanon; Department of CSM, Cyber Security Systems and Applied AI Research Center, Lebanese American University, Beirut, Lebanon; Department of EECS, Center of Cyber-Physical Systems, Khalifa University, Abu Dhabi, UAE; Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada",IEEE Transactions on Network and Service Management,"9 Oct 2023","2023","20","3","3428","3442","Due to the current improvement in self-driving cars and the extensive focus and research on the topic of the Internet of Vehicles (IoV), the near future may behold a great revolution in the automotive industry as cars become fully autonomous. This change entails a considerable amount of data to be transferred from Internet of Things (IoT) devices, such as radars, sensors, and actuators. Consequently, overwhelming the existing infrastructure, namely cloud, and Road Side Units (RSU), reduces the quality of service (QoS) experienced by vehicular users. Accordingly, this paper contributes in proposing a new architecture for using Unmanned Ariel Vehicles (UAVs) and On-Boarding Units (OBUs) working in collaboration to achieve a significantly improved QoS. The proposed framework offers an end-to-end solution for master election, cluster management and recovery, vehicle selection, service placement, and accurate localization of vehicles. A QoS improvement is possible through an efficient cluster formation and placement solution that assigns lightweight services, as containers, to OBUs and UAVs while meeting various objectives. The efficiency of the proposed scheme originates from the use of the evolutionary Memetic Algorithm that 1) respects the mobility and energy constraints of UAVs and OBUs, 2) meets the user demands, and 3) uses machine learning for the accurate localization of vehicles. Our experiments using the Mininet-WiFi and SUMO simulators show at least 30% improvement in terms of QoS compared to a state-of-the-art solution.","1932-4537","","10.1109/TNSM.2023.3242205","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10036008","UAV;OBU;IoV;container placement;memetic algorithm;localization;machine learning","Quality of service;Microservice architectures;Containers;Computer architecture;Stability analysis;Optimization;Location awareness","","11","","50","IEEE","3 Feb 2023","","","IEEE","IEEE Journals"
"Design of Industrial Edge Applications Based on IEC 61499 Microservices and Containers","W. Dai; Y. Zhang; L. Kong; J. H. Christensen; D. Huang","Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Huawei Technology Ltd., Shenzhen, China; Holobloc Inc., Cleveland, OH, USA; Department of Automation, Shanghai Jiao Tong University Shanghai, Shanghai, China",IEEE Transactions on Industrial Informatics,"21 Jun 2023","2023","19","7","7925","7935","Industrial automation is entering a new era of the Industrial Internet with enhanced computing, communication, and storage capabilities provided by cloud computing and field devices. The paradigm of automation systems is shifting from the ISA-95 pyramid to the two-layers architecture: industrial cloud and edge computing. Industrial software is also evolving under the new architecture in ways for which dedicated software applications are no longer suited. Service-based industrial cloud and edge applications provide maximum flexibility, interoperability, and efficiency by combining the IEC 61499 standard, microservice architecture, and container technology. This article provides orchestration methods and deployment procedures for the OT-IT hybrid industrial edge applications. The feasibility of the proposed approach is demonstrated by an industrial case study with accompanying performance analysis.","1941-0050","","10.1109/TII.2022.3214199","National R&D Key Program of China(grant numbers:2021YFB1714000); National Natural Science Foundation of China(grant numbers:61973216,62122048); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9918040","Industrial edge computing;IEC 61499;microservices;linux container;virtualization","Microservice architectures;IEC Standards;Cloud computing;Computer architecture;Containers;Automation;Virtualization","","11","","30","IEEE","13 Oct 2022","","","IEEE","IEEE Journals"
"SimulateIoT-FIWARE: Domain Specific Language to Design, Code Generation and Execute IoT Simulation Environments on FIWARE","J. A. Barriga; P. J. Clemente; J. Hernández; M. A. Pérez-Toledano","Department of Computer Science, Quercus Software Engineering Group, Universidad de Extremadura, Cáceres, Spain; Department of Computer Science, Quercus Software Engineering Group, Universidad de Extremadura, Cáceres, Spain; Department of Computer Science, Quercus Software Engineering Group, Universidad de Extremadura, Cáceres, Spain; Department of Computer Science, Quercus Software Engineering Group, Universidad de Extremadura, Cáceres, Spain",IEEE Access,"24 Jan 2022","2022","10","","7800","7822","Systems based on the Internet of Things (IoT) are continuously growing in many areas such as smart cities, home environments, buildings, agriculture, industry, etc. This system integrates heterogeneous technologies into a complex architecture of interconnected devices capable of communicating, processing, analysing or storing data. There are several IoT platforms that offer several capabilities for the development of these systems. Some of these platforms are Google Cloud’s IoT Platform, Microsoft Azure IoT suite, ThingSpeak IoT Platform, Thingworx 8 IoT Platform or FIWARE. However, they are complex IoT platforms where each IoT solution has to be developed ad-hoc and implemented by developers by hand. Consequently, developing IoT solutions is a hard, error-prone and tedious task. Thus, increase the abstraction level from which the IoT systems are designed helps to tackle the underlying technology complexity. In this sense, model-driven development approaches can help to both reduce the IoT application time to market and tackle the technological complexity to develop IoT applications. In this paper, we propose a Domain-Specific Language based on SimulateIoT for the design, code generation and simulation of IoT systems which could be deployed on FIWARE infrastructure (an open-source IoT platform). This implies not only designing the IoT system for a high abstraction level and later on code generation, but also designing and deploying an additional simulation layer to simulate the system on the FIWARE infrastructure before final deployment. The FIWARE IoT environment generated includes the sensors, actuators, fog nodes, cloud nodes and analytical characteristics, which are deployed as microservices on Docker containers and composed suitability to obtain a service-oriented architecture. Finally, two case studies focused on a smart building and an agricultural IoT environment are presented to show the IoT solutions deployed using FIWARE.","2169-3536","","10.1109/ACCESS.2022.3142894","Ministerio de Ciencia e Innovación (MCI) through the Agencia Estatal de Investigación (AEI)(grant numbers:RTI2018-098652-B-I00); Government of Extremadura, Council for Economy, Science and Digital Agenda(grant numbers:GR18112,IB20058); European Regional Development Fund (ERDF); Cátedra Telefónica de la Universidad de Extremadura (Red de Cátedras Telefónica); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9680737","Model-driven development;Internet of Things;IoT simulation;services-oriented;FIWARE","Internet of Things;Codes;Protocols;Actuators;Computer architecture;Open source software;Model-driven development","","11","","51","CCBY","13 Jan 2022","","","IEEE","IEEE Journals"
"Optimal Selection Techniques for Cloud Service Providers","G. Tricomi; G. Merlino; A. Panarello; A. Puliafito","Dipartimento di Ingegneria, Università degli Studi di Messina, Messina, Italy; Dipartimento di Ingegneria, Università degli Studi di Messina, Messina, Italy; SmartMe.IO S.r.l., Messina, Italy; Dipartimento di Ingegneria, Università degli Studi di Messina, Messina, Italy",IEEE Access,"17 Nov 2020","2020","8","","203591","203618","Nowadays Cloud computing permeates almost every domain in Information and Communications Technology (ICT) and, increasingly, most of the action is shifting from large, dominant players toward independent, heterogeneous, private/hybrid deployments, in line with an ever wider range of business models and stakeholders. The rapid growth in the numbers and diversity of small and medium Cloud providers is bringing new challenges in the as-a-Services space. Indeed, significant hurdles for smaller Cloud service providers in being competitive with the incumbent market leaders induce some innovative players to “federate” deployments in order to pool a larger, virtually limitless, set of resources across the federation, and stand to gain in terms of economies of scale and resource usage efficiency. Several are the challenges that need to be addressed in building and managing a federated environment, that may go under the “Security”, “Interoperability”, “Versatility”, “Automatic Selection” and “Scalability” labels. The aim of this paper is to present a survey about the approaches and challenges belonging to the “Automatic Selection” category. This work provides a literature review of different approaches adopted in the “Automatic and Optimal Cloud Service Provider Selection”, also covering “Federated and Multi-Cloud” environments.","2169-3536","","10.1109/ACCESS.2020.3035816","Italian Project TOOLSMART financed by Open Community(grant numbers:PA 2020); Pon Governance(grant numbers:2014–2020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9248017","Algorithms;cloud federation;matchmaking;multi-cloud;optimal selection;survey","Cloud computing;Computational modeling;Buildings;Economies of scale;Information and communication technology;Stakeholders;Business","","11","","104","CCBY","4 Nov 2020","","","IEEE","IEEE Journals"
"Information-Centric Mobile Networks: A Survey, Discussion, and Future Research Directions","S. Fayyaz; M. Atif Ur Rehman; M. Salah Ud Din; M. I. Biswas; A. K. Bashir; B. -S. Kim","Department of Electronics and Computer Engineering, Hongik University, Sejong, South Korea; Department of Computing and Mathematics, Manchester Metropolitan University, Manchester, U.K; Department of Electronics and Computer Engineering, Hongik University, Sejong, South Korea; Department of Computing and Mathematics, Manchester Metropolitan University, Manchester, U.K; Department of Computing and Mathematics, Manchester Metropolitan University, Manchester, U.K; Department of Software and Communications Engineering, Hongik University, Sejong, South Korea",IEEE Access,"1 May 2023","2023","11","","40328","40372","Information-centric networking (ICN) and its fruition, the named data networking (NDN) is a paradigm shift from host-centric address-based communication architecture to the content-centric name-based one. ICN intends to resolve various major issues faced by today’s internet architecture such as privacy, security, consistent routing, and mobility, to name a few. With the massive increase of mobile data traffic in today’s era, mobility is one of the major concerns in networking. On the one hand, ICN realization i.e., the NDN follows a pull-based communication model and natively supports the consumer (end-user) mobility in wired networks by maintaining the forwarding states on intermediate nodes. Nevertheless, the mobile consumer nodes confront issues in wireless networking environments such as excessive energy consumption as a result of request flooding, content retrieval delays due to intermittent connectivity, and bandwidth consumption due to the broadcasting nature of the wireless medium, among others. The producer (content-generator) mobility, on the other hand, was not initially supported in the original architectural design of NDN for both wired and wireless networks. Therefore, to efficiently address the degradation issues incurred by mobile consumer/producer nodes, a plethora of mobility management schemes have been proposed over the recent few years. In this paper, we provided a detailed survey on the existing research efforts—in the context of producer, consumer, and hybrid mobility, that have been proposed in the literature. Moreover, we outlined various research directions considering the role of mobility in futuristic technologies such as artificial intelligence-enabled smart networks, software-defined networking, edge computing, vehicular-fog computing, autonomous driving, semantic communication, and resource-constrained Internet of Things.","2169-3536","","10.1109/ACCESS.2023.3268775","National Research Foundation of Korea (NRF); Korean Government (MSIT)(grant numbers:2022R1A2C1003549); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10105953","Named data networking;mobility management;producer mobility;wireless networking;artificial intelligence;edge computing;vehicular fog computing;autonomous driving","IP networks;Wireless communication;Delays;Wireless sensor networks;Servers;Broadcasting;Sockets","","11","","138","CCBYNCND","20 Apr 2023","","","IEEE","IEEE Journals"
"The Metaverse: Survey, Trends, Novel Pipeline Ecosystem & Future Directions","H. Sami; A. Hammoud; M. Arafeh; M. Wazzeh; S. Arisdakessian; M. Chahoud; O. Wehbi; M. Ajaj; A. Mourad; H. Otrok; O. Abdel Wahab; R. Mizouni; J. Bentahar; C. Talhi; Z. Dziong; E. Damiani; M. Guizani","Department of CSM, Artificial Intelligence and Cyber Systems Research Center, Lebanese American University, Beirut, Lebanon; Department of CSM, Artificial Intelligence and Cyber Systems Research Center, Lebanese American University, Beirut, Lebanon; Department of CSM, Artificial Intelligence and Cyber Systems Research Center, Lebanese American University, Beirut, Lebanon; Department of CSM, Artificial Intelligence and Cyber Systems Research Center, Lebanese American University, Beirut, Lebanon; Department of CSM, Artificial Intelligence and Cyber Systems Research Center, Lebanese American University, Beirut, Lebanon; Department of CSM, Artificial Intelligence and Cyber Systems Research Center, Lebanese American University, Beirut, Lebanon; Department of CSM, Artificial Intelligence and Cyber Systems Research Center, Lebanese American University, Beirut, Lebanon; Department of CSM, Artificial Intelligence and Cyber Systems Research Center, Lebanese American University, Beirut, Lebanon; Department of CSM, Artificial Intelligence and Cyber Systems Research Center, Lebanese American University, Beirut, Lebanon; Department of CS, Center of Cyber-Physical Systems, Khalifa University, Abu Dhabi, UAE; Department of Computer and Software Engineering, Polytechnique Montréal, Montreal, QC, Canada; Department of CS, Center of Cyber-Physical Systems, Khalifa University, Abu Dhabi, UAE; Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada; Department of Software and IT Engineering, École de Technologie Supérieure, Montreal, QC, Canada; Department of Electrical Engineering, École de Technologie Supérieure, Montreal, QC, Canada; Department of CS, Center of Cyber-Physical Systems, Khalifa University, Abu Dhabi, UAE; Department of ML, Mohamed Bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE",IEEE Communications Surveys & Tutorials,"21 Nov 2024","2024","26","4","2914","2960","The Metaverse offers a second world beyond reality, where boundaries are non-existent, and possibilities are endless through engagement and immersive experiences using the virtual reality (VR) technology. Many disciplines can benefit from the advancement of the Metaverse when accurately developed, including the fields of technology, gaming, education, art & culture, socialization, commerce, and businesses. Nevertheless, developing the Metaverse environment to its full potential is an ambiguous task that needs proper guidance and directions. Existing surveys on the Metaverse focus only on a specific aspect and discipline of the Metaverse and lack a holistic view of the entire process. Moreover, most surveys refrain from providing detailed guidance about the development process of the metaverse, including its impact on technologies, businesses, existing challenges, and potential research directions due to their lack of a macro and micro perception of such a topic. To this end, a more holistic, multi-disciplinary, in-depth, and academic and industry-oriented review is required to provide a thorough study of the Metaverse development pipeline and fill the gap in existing Metaverse surveys. To address these issues, we present in this survey a novel multi-layered pipeline ecosystem composed of (1) the Metaverse computing, networking, communications and hardware infrastructure, (2) environment digitization, and (3) user interactions. For every layer, we discuss the components that detail the steps of its development. Also, for each of these components, we examine the impact of a set of enabling technologies and empowering domains (e.g., Artificial Intelligence, Security & Privacy, Blockchain, Business, Ethics, and Social) on its advancement. In addition, we explain the importance of these technologies to support decentralization, interoperability, user experiences, interactions, and monetization. Our presented study highlights the existing challenges for each component, followed by research directions and potential solutions. To the best of our knowledge, this survey is the most comprehensive and allows users, scholars, and entrepreneurs to get an in-depth understanding of the Metaverse ecosystem to find their opportunities and potentials for contribution.","1553-877X","","10.1109/COMST.2024.3392642","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10507201","Metaverse;augmented reality;virtual reality;mixed reality;AI;networking;communications;edge computing;security;privacy;blockchain;digital twins;avatars;rendering;3D modeling;user-to-user and user-to-business interactions","Metaverse;Surveys;Pipelines;Ecosystems;Business;Artificial intelligence;Blockchains","","11","","248","IEEE","23 Apr 2024","","","IEEE","IEEE Journals"
"Extended Berkeley Packet Filter: An Application Perspective","H. Sharaf; I. Ahmad; T. Dimitriou","Department of Computer Engineering, College of Engineering and Petroleum, Kuwait University, Kuwait, Kuwait; Department of Computer Engineering, College of Engineering and Petroleum, Kuwait University, Kuwait, Kuwait; Department of Computer Engineering, College of Engineering and Petroleum, Kuwait University, Kuwait, Kuwait",IEEE Access,"8 Dec 2022","2022","10","","126370","126393","The extended Berkeley Packet Filter (eBPF) is a lightweight and fast 64-bit RISC-like virtual machine (VM) inside the Linux kernel. eBPF has emerged as the most promising and de facto standard of executing untrusted, user-defined specialized code at run-time inside the kernel with strong performance, portability, flexibility, and safety guarantees. Due to these key benefits and availability of a rich ecosystem of compilers and tools within the Linux kernel, eBPF has received widespread adoption by both industry and academia for a wide range of application domains. The most important include enhancing performance of monitoring tools and providing a variety of new security mechanisms, data collection tools and data screening applications. In this review, we investigate the landscape of existing eBPF use-cases and trends with aim to provide a clear roadmap for researchers and developers. We first introduce the necessary background knowledge for eBPF before delving into its applications. Although, the potential use-cases of eBPF are vast, we restrict our focus on four key application domains related to networking, security, storage, and sandboxing. Then for each application domain, we analyze and summarize solution techniques along with their working principles in an effort to provide an insightful discussion that will enable researchers and practitioners to easily adopt eBPF into their designs. Finally, we delineate several exciting research avenues to fully exploit the revolutionary eBPF technology.","2169-3536","","10.1109/ACCESS.2022.3226269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9968265","BPF;eBPF;XDP;Linux kernel;security;network;sandboxing;storage;containers","Storage management;Band-pass filters;Filtering;Security;Cloud computing;Monitoring;Linux;Containers","","11","","109","CCBYNCND","1 Dec 2022","","","IEEE","IEEE Journals"
"I-ViSE: Interactive Video Surveillance as an Edge Service Using Unsupervised Feature Queries","S. Y. Nikouei; Y. Chen; A. J. Aved; E. Blasch","Department of Electrical and Computer Engineering, Binghamton University—SUNY, Binghamton, NY, USA; Department of Electrical and Computer Engineering, Binghamton University—SUNY, Binghamton, NY, USA; Department of Applied Analytics Branch, U.S. Air Force Research Laboratory, Rome, NY, USA; Department of Applied Analytics Branch, U.S. Air Force Research Laboratory, Rome, NY, USA",IEEE Internet of Things Journal,"25 Oct 2021","2021","8","21","16181","16190","Situation awareness (SAW) is essential for many mission-critical applications. However, SAW is challenging when trying to immediately identify objects of interest or focus on suspicious activities from thousands of video frames. This article develops a queryable system to instantly select interesting content. While face recognition technology is mature, in many scenarios, such as public safety monitoring, the features of objects of interest may be much more complicated than face features. In addition, human operators may not be always able to provide a descriptive, simple, and accurate query. Actually, it is more often that there are only rough, general descriptions of certain suspicious objects or accidents. This article proposes interactive video surveillance as an edge service (I-ViSE) based on unsupervised feature queries. Adopting unsupervised methods that do not reveal any private information, the I-ViSE scheme utilizes general features of a human body and color of clothes. An I-ViSE prototype is built following the edge-fog computing paradigm and the experimental results verified the I-ViSE scheme meets the design goal of scene recognition and target analysis in less than 2 s.","2327-4662","","10.1109/JIOT.2020.3016825","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9167223","Decentralization;online query;privacy preserving;smart surveillance;video feature extraction","Streaming media;Cameras;Computer architecture;Security;Surveillance;Image edge detection;Privacy","","11","","36","IEEE","14 Aug 2020","","","IEEE","IEEE Journals"
"The Road to Trustworthy 6G: A Survey on Trust Anchor Technologies","B. Veith; D. Krummacker; H. D. Schotten","Intelligent Networks Research Group, German Research Center for Artificial Intelligence (DFKI GmbH), Kaiserslautern, Germany; Intelligent Networks Research Group, German Research Center for Artificial Intelligence (DFKI GmbH), Kaiserslautern, Germany; Intelligent Networks Research Group, German Research Center for Artificial Intelligence (DFKI GmbH), Kaiserslautern, Germany",IEEE Open Journal of the Communications Society,"27 Feb 2023","2023","4","","581","595","While 5G networks are driving a growing number of use cases in the fields of iot and industrial applications, the vision of the next generation of mobile communications systems already includes concepts massively transforming the way people will interact with the digital world through the network, as humans are shifting into the center of diverse network driven applications. Envisaged use cases and possibilities to provide services and resources in a distributed manner render an architectural solution for trust establishment a critical component of 6G networks. This survey provides an overview of terms and visions related to the topic of trust in general and in mobile communications systems. Requirements for an end-to-end trust building framework are derived, in order to give a starting point for the design process of a trust anchor service as a component of 6G networks.","2644-125X","","10.1109/OJCOMS.2023.3244274","German Federal Ministry for Education and Research (BMBF) through the Project »Open6GHub«(grant numbers:16KISK003K); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10042484","6G;AI;DLT;trust anchors;trust as a service","6G mobile communication;Security;Peer-to-peer computing;Buildings;Quality of service;Distributed ledger;Biological system modeling","","11","","84","CCBYNCND","13 Feb 2023","","","IEEE","IEEE Journals"
"CBOSS: bringing traffic engineering inside data center networks","N. Benzaoui; J. M. Estarán; E. Dutisseuil; H. Mardoyan; G. D. Valicourt; A. Dupas; Q. P. Van; D. Verchere; B. Ušćumlić; M. S. Gonzalez; P. Dong; Y. . -K. Chen; S. Bigo; Y. Pointurier","Nokia Bell Labs, Route de Villejust, 91620 Nozay, France; Nokia Bell Labs, Route de Villejust, 91620 Nozay, France; Nokia Bell Labs, Route de Villejust, 91620 Nozay, France; Nokia Bell Labs, Route de Villejust, 91620 Nozay, France; Nokia Bell Labs, 791 Holmdel Road, Holmdel, New Jersey 07733, USA; Nokia Bell Labs, Route de Villejust, 91620 Nozay, France; Nokia Bell Labs, Route de Villejust, 91620 Nozay, France; Nokia Bell Labs, Route de Villejust, 91620 Nozay, France; Nokia Bell Labs, Route de Villejust, 91620 Nozay, France; Nokia Bell Labs, Route de Villejust, 91620 Nozay, France; Nokia Bell Labs, 791 Holmdel Road, Holmdel, New Jersey 07733, USA.; Nokia Bell Labs, 791 Holmdel Road, Holmdel, New Jersey 07733, USA; Nokia Bell Labs, Route de Villejust, 91620 Nozay, France; Nokia Bell Labs, Route de Villejust, 91620 Nozay, France",Journal of Optical Communications and Networking,"13 Jul 2018","2018","10","7","117","125","Today’s traditional, hierarchical, fullelectronic data centers cannot guarantee quality of service (QoS) for latency-sensitive applications or carrier-grade multitenancy.We propose cloud burst optical-slot switching (CBOSS), an intra-data center network architecture leveraging the original optical components and custom software- defined network (SDN) control to enable traffic engineering. In particular, we demonstrate network slicing on demand with subwavelength granularity and deterministic per-flowQoSguarantees in a 3-top-of-rack(ToR)CBOSSnetwork prototype. We rely on an integrated silicon photonics (SiP) wavelength dropper, a fast-tunable laser, and custom SDN-based controller.","1943-0639","","10.1364/JOCN.10.00B117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8410221","Capacity guarantee;Data center;Jitter;Latency;Optical slot switching network;Quality of Service;Tunable laser;Wavelength ropper.","Optical switches;Quality of service;Data centers;Optical amplifiers;Optical receivers;Optical filters;Optical fiber networks","","11","","","","13 Jul 2018","","","IEEE","IEEE Journals"
"Classifying Model-View-Controller Software Applications Using Self-Organizing Maps","D. Guamán; S. Delgado; J. Pérez","Departamento de Sistemas Informáticos (DSI), ETS de Ingeniería de Sistemas Informáticos (ETSISI), Universidad Politécnica de Madrid, Madrid, Spain; Departamento de Sistemas Informáticos (DSI), ETS de Ingeniería de Sistemas Informáticos (ETSISI), Universidad Politécnica de Madrid, Madrid, Spain; Departamento de Sistemas Informáticos (DSI), ETS de Ingeniería de Sistemas Informáticos (ETSISI), Universidad Politécnica de Madrid, Madrid, Spain",IEEE Access,"25 Mar 2021","2021","9","","45201","45229","The new era of information and the needs of our society require continuous change in software and technology. Changes are produced very quickly and software systems require evolving at the same velocity, which implies that the decision-making process of software architectures should be (semi-)automated to satisfy changing needs and to avoid wrong decisions. This issue is critical since suboptimal architecture design decisions may lead to high cost and poor software quality. Therefore, systematic and (semi-)automated mechanisms that help software architects during the decision-making process are required. Architectural patterns are one of the most important features of software applications, but the same pattern can be implemented in different ways, leaving to results of different quality. When an application requires to evolve, knowledge extracted from similar applications is useful for driving decisions, since quality pattern implementations can be reproduced in similar applications to improve specific quality attributes. Therefore, clustering methods are especially suitable for classifying similar pattern implementations. In this paper, we apply a novel unsupervised clustering technique, based on the well-known artificial neural network model Self-Organizing Maps, to classify Model-View-Controller (MVC) pattern from a quality point of view. Software quality is analyzed by 24 metrics organized into the categories of Count/Size, Maintainability, Duplications, Complexity, and Design Quality. The main goal of this work is twofold: to identify the quality features that establish the similarity of MVC applications without software architect bias, and to classify MVC applications by means of Self-Organizing Maps based on quality metrics. To that end, this work performs an exploratory study by conducting two analyses with a dataset of 87 Java MVC applications characterized by the 24 metrics and two attributes that describe the technology dimension of the application. The stated findings provide a knowledge base that can help in the decision-making process for the architecture of Java MVC applications.","2169-3536","","10.1109/ACCESS.2021.3066348","Computer Science Department, Universidad Técnica Particular de Loja; Spanish Ministry of Economy and Competitiveness (MINECO) through the Project CROWDSAVING(grant numbers:TIN2016-79726-C2-1-R); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380344","Model-view-controller;software architectures;artificial neural networks;self-organizing maps (SOM);unsupervised clustering techniques;machine learning;software quality","Software;Software architecture;Computer architecture;Decision making;Clustering methods;Clustering algorithms;Self-organizing feature maps","","11","","110","CCBY","17 Mar 2021","","","IEEE","IEEE Journals"
"Monitoring-Aware Optimal Deployment for Applications Based on Microservices","E. Fadda; P. Plebani; M. Vitali","DAUIN Department, Corso Duca degli Abruzzi 24, Turin, Italy; DEIB Department, Politecnico di Milano, Milan, Italy; DEIB Department, Politecnico di Milano, Milan, Italy",IEEE Transactions on Services Computing,"8 Dec 2021","2021","14","6","1849","1863","Modern cloud applications are required to be distributed, scalable, and reliable. The microservice architectural style enables developers to achieve this goal with reduced effort. Nonetheless, microservices deployment is not trivial due to the heterogeneity of the microservices in terms of both functional and non-functional requirements. This is also true when considering the monitoring requirements that are specific to each microservice and must be satisfied in order to enable the verification of the application objectives satisfaction. However, not all providers offer the same set of metrics with the same quality. The goal of this paper is to provide an approach for supporting the deployment of microservices in multi-cloud environments focusing on the Quality of Monitoring. Adopting a multi-objective mixed integer linear optimisation problem, our approach supports the application owner in finding the optimal deployment for satisfying all the constraints and maximising the quality of monitored data, while minimising the costs. To this end, a knowledge base is introduced to mediate between the perspectives of the cloud provider and the application owner, while a Bayesian Network is adopted to enhance the provider’s monitoring capabilities by estimating metrics requested by the application owners that the cloud provider is not able to monitor.","1939-1374","","10.1109/TSC.2019.2910069","European Union's Horizon 2020(grant numbers:RIA 731945); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8685146","Monitoring system;microservice;multi-cloud;cloud computing;deployment optimisation","Monitoring;Measurement;Cloud computing;Optimization;Knowledge based systems;Scalability;Trajectory","","11","","35","IEEE","11 Apr 2019","","","IEEE","IEEE Journals"
"Towards Cost-Efficient Edge Intelligent Computing With Elastic Deployment of Container-Based Microservices","P. Zhao; P. Wang; X. Yang; J. Lin","School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China; School of Software Engineering, Xi’an Jiaotong University, Xi’an, China; School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China; School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China",IEEE Access,"9 Jun 2020","2020","8","","102947","102957","With the tremendous growth of the Internet of Things (IoT), big data, and artificial intelligence (AI), the edge computing-based service paradigm has been introduced to meet the increasing demand of applications. To provide efficient computing services at the network edge, the algorithms and applications are generally deployed based on the container-based microservice strategy, which significantly impacts the system efficiency and QoS. Considering the fundamental system uncertainties, including the dynamic workload and service rate, we investigate how to minimize the long-term system cost through the elastic microservice deployment in this paper. To this end, we formulate the container-based microservice deployment as a stochastic optimization problem to minimize the system cost while maintaining the system QoS and stability. We develop a cost-aware elastic microservice deployment algorithm to solve the formulated problem, which balances the tradeoff between system cost and QoS. Our algorithm makes the real-time decisions based on current queue backlogs and system states without predicting the future knowledge. Finally, we conduct the theoretical analysis and extensive simulations based on data traces from the ResNet-50 model-based visual recognition application. The results demonstrate that our algorithm outperforms the baseline strategies with respect to the system cost, queue backlogs, and the number of Pod replicas.","2169-3536","","10.1109/ACCESS.2020.2998767","National Basic Research Program of China (973 Program)(grant numbers:2017YFB1010004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9104693","Edge computing system;microservices;elastic deployment;system cost;container","Containers;Heuristic algorithms;Edge computing;Computational modeling;Virtualization;Artificial intelligence;Quality of service","","11","","22","CCBY","1 Jun 2020","","","IEEE","IEEE Journals"
"When Clones Flock Near the Fog","S. Abdelwahab; S. Zhang; A. Greenacre; K. Ovesen; K. Bergman; B. Hamdaoui","School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR, USA",IEEE Internet of Things Journal,"8 Jun 2018","2018","5","3","1914","1923","FogMQ is a message brokering and device cloning service in fog and edge computing. Excessive tail end-to-end latency occurs with conventional message brokers when a massive number of geographically distributed devices communicate through a message broker. Latency of broker-less messaging is highly dependent on computational resources of devices. Deviceto-device messaging does not necessarily ensure low messaging latency and cannot scale well for a large number of resourcelimited and geographically distributed devices. For each device, FogMQ provides a high capacity device cloning service that subscribes to device messages. The clones facilitate near-theedge data analytics in resourceful cloud compute nodes. Clones in FogMQ apply Flock; an algorithm mimicking flocking-like behavior and allows the clones to autonomously migrate between heterogeneous cloud platforms. Flock controls and minimizes the weighted tail end-to-end latency. We have implemented FogMQ and evaluated it in a geographically distributed testbed. In our functional evaluation, we show that FogMQ is stable and achieves a bounded tail end-to-end latency that is up to 34% less than existing brokering methods.","2327-4662","","10.1109/JIOT.2018.2817392","NPRP grant from the Qatar National Research Fund (a member of Qatar Foundation)(grant numbers:NPRP 5-319-2-121); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8320280","Game theory;messages brokering;resource management;ubiquitous computing","Cloning;Cloud computing;Servers;Social network services;Internet of Things;Device-to-device communication;Delays","","11","","54","IEEE","20 Mar 2018","","","IEEE","IEEE Journals"
"An Energy-Aware Host Resource Management Framework for Two-Tier Virtualized Cloud Data Centers","C. Zhang; Y. Wang; H. Wu; H. Guo","School of Software Technology, Dalian University of Technology, Dalian, China; School of Computer Science and Technology, Dalian University of Technology, Dalian, China; School of Computer Science and Information Engineering, Anyang Institute of Technology, Anyang, China; School of Software Technology, Dalian University of Technology, Dalian, China",IEEE Access,"6 Jan 2021","2021","9","","3526","3544","The high energy consumption of cloud data centers is one of the key issues restricting the future development of the cloud computing industry. For two-tier virtualized data centers in which containers are deployed on VMs, this paper conducts an in-depth study of this problem from the aspect of resource management. First, the working model of two-tier virtualized data center is defined. By mapping tasks to containers and using VMs to isolate different jobs, this model improves the resource utilization of hosts while ensuring the isolation and security of jobs. Then, an energy-aware host resource management framework is constructed, which includes two algorithms. The initial static placement is a two-tier scheduling algorithm, including the load balancing alternate placement and the two-sided matching methods to complete the placement of containers to VMs and VMs to hosts respectively. The runtime dynamic consolidation algorithm takes the initial placement scheme as input, and utilizes the dynamic consolidation method to use the least active hosts to meet the real-time resource requirements of containers. Finally, simulation experiments compared with related algorithms are conducted using real workload traces. The results show that the proposed two algorithms have better performance in host resource utilization, number of active hosts, number of container migrations and SLA metric. And the entire framework achieves an energy-saving effect of 13.8% at least.","2169-3536","","10.1109/ACCESS.2020.3047803","National Natural Science Foundation of China Grants (NSFC)(grant numbers:61772112); Innovation Foundation of Science and Technology of Dalian(grant numbers:2018J11CY010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9309376","Cloud data center;container;virtual machine;energy consumption;resource management","Containers;Cloud computing;Heuristic algorithms;Resource management;Data centers;Virtualization;Energy consumption","","11","","41","CCBYNCND","28 Dec 2020","","","IEEE","IEEE Journals"
"Evaluation Framework for Electric Vehicle Security Risk Assessment","S. Shirvani; Y. Baseri; A. Ghorbani","Canadian Institute for Cybersecurity (CIC), University of New Brunswick (UNB), Fredericton, Canada; Canadian Institute for Cybersecurity (CIC), University of New Brunswick (UNB), Fredericton, Canada; Canadian Institute for Cybersecurity (CIC), University of New Brunswick (UNB), Fredericton, Canada",IEEE Transactions on Intelligent Transportation Systems,"17 Jan 2024","2024","25","1","33","56","Electric Vehicles (EVs) seem promising for future transportation to solve environmental concerns and energy management problems. According to Reuters, global car makers plan to invest over half a billion in more efficient and intelligent EVs and batteries. However, there are several challenges in EV mass production, including cybersecurity. Due to the cyber-physical nature of EVs and charging stations, their security and trustworthiness are ongoing challenges. In this study, we identify gaps in the security profiling of EVs and categorize them into five components: 1) charging station security, 2) information privacy, 3) software security, 4) connected vehicle security, and 5) autonomous driving security. Our study provides a comprehensive analysis of identified vulnerabilities, threats, challenges and attacks for different EV security aspects, along with their possible surface/subsurface and countermeasures. We develop a comprehensive security risk assessment framework by first using EV security profiles and mapping identified vulnerabilities to a well-known threat model, STRIDE. Then, we classify the risk levels associated with each vulnerability by setting ground criteria for the impact and likelihood of the threats. Finally, we validate our risk assessment framework by applying the same criteria to eight real-world EV attack scenarios. As a result, researchers can adapt the proposed risk assessment framework to discover threats and assess their risks in EVs and charging station ecosystems.","1558-0016","","10.1109/TITS.2023.3307660","NSERC through the Discovery Grant and Canada Research Chair; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247152","Security profiling;risk assessment;electric vehicle;autonomous vehicle;cybersecurity attacks","Security;Risk management;Privacy;Electric vehicles;Autonomous vehicles;Vehicle-to-everything;Transportation;Autonomous driving","","11","","299","IEEE","11 Sep 2023","","","IEEE","IEEE Journals"
"Self-Aware Neural Network Systems: A Survey and New Perspective","B. Z. Du; Q. Guo; Y. Zhao; T. Zhi; Y. Chen; Z. Xu","State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China",Proceedings of the IEEE,"18 Jun 2020","2020","108","7","1047","1067","Neural network (NN) processors are specially designed to handle deep learning tasks by utilizing multilayer artificial NNs. They have been demonstrated to be useful in broad application fields such as image recognition, speech processing, machine translation, and scientific computing. Meanwhile, innovative self-aware techniques, whereby a system can dynamically react based on continuously sensed information from the execution environment, have attracted attention from both academia and industry. Actually, various self-aware techniques have been applied to NN systems to significantly improve the computational speed and energy efficiency. This article surveys state-of-the-art self-aware NN systems (SaNNSs), which can be achieved at different layers, that is, the architectural layer, the physical layer, and the circuit layer. At the architectural layer, SaNNS can be characterized from a data-centric perspective where different data properties (i.e., data value, data precision, dataflow, and data distribution) are exploited. At the physical layer, various parameters of physical implementation are considered. At the circuit layer, different logics and devices can be used for high efficiency. In fact, the self-awareness of existing SaNNS is still in a preliminary form. We propose a comprehensive SaNNS from a new perspective, that is, the model layer, to exploit more opportunities for high efficiency. The proposed system is called as MinMaxNN, which features model switching and elastic sparsity based on monitored information from the execution environment. The model switching mechanism implies that models (i.e., min and max model) dynamically switch given different inputs for both efficiency and accuracy. The elastic sparsity mechanism indicates that the sparsity of NNs can be dynamically adjusted in each layer for efficiency. The experimental results show that compared with traditional SaNNS, MinMaxNN can achieve 5.64× and 19.66% performance improvement and energy reduction, respectively, without notable loss of accuracy and negative effects on developers' productivity.","1558-2256","","10.1109/JPROC.2020.2977722","National Basic Research Program of China (973 Program)(grant numbers:2017YFB1003101,2018AAA0103300,2017YFA0700900,2017YFA0700902,2017YFA0700901); NSF of China(grant numbers:61732007,61432016,61532016,61672491,61602441,61602446,61732002,61702478,61732020); Beijing Natural Science Foundation(grant numbers:JQ18013); National Science and Technology Major Project(grant numbers:2018ZX01031102); Transformation and Transfer of Scientific and Technological Achievements of Chinese Academy of Sciences(grant numbers:KFJ-HGZX-013); Key Research Projects in Frontier Science of Chinese Academy of Sciences(grant numbers:QYZDB-SSW-JSC001); Strategic Priority Research Program of Chinese Academy of Science(grant numbers:XDB32050200,XDC01020000); Standardization Research Project of Chinese Academy of Sciences(grant numbers:BZ201800001); Beijing Academy of Artificial Intelligence (BAAI) through the Beijing Nova Program of Science and Technology(grant numbers:Z191100001119093); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9045930","Self-aware neural network (NN) processors","Artificial neural networks;Neural networks;Self-aware;Monitoring;Sensors;Program processors;Logic gates","","11","","106","IEEE","24 Mar 2020","","","IEEE","IEEE Journals"
"A Taxonomy of Quality Metrics for Cloud Services","X. Guerron; S. Abrahão; E. Insfran; M. Fernández-Diego; F. González-Ladrón-De-Guevara","Instituto Universitario Mixto de Tecnología Informática, Universitat Politècnica de València (UPV), Valencia, Spain; Instituto Universitario Mixto de Tecnología Informática, Universitat Politècnica de València (UPV), Valencia, Spain; Instituto Universitario Mixto de Tecnología Informática, Universitat Politècnica de València (UPV), Valencia, Spain; Departamento de Organización de Empresas, Universitat Politècnica de València (UPV), Valencia, Spain; Departamento de Organización de Empresas, Universitat Politècnica de València (UPV), Valencia, Spain",IEEE Access,"24 Jul 2020","2020","8","","131461","131498","A large number of metrics with which to assess the quality of cloud services have been proposed over the last years. However, this knowledge is still dispersed, and stakeholders have little or no guidance when choosing metrics that will be suitable to evaluate their cloud services. The objective of this paper is, therefore, to systematically identify, taxonomically classify, and compare existing quality of service (QoS) metrics in the cloud computing domain. We conducted a systematic literature review of 84 studies selected from a set of 4333 studies that were published from 2006 to November 2018. We specifically identified 470 metric operationalizations that were then classified using a taxonomy, which is also introduced in this paper. The data extracted from the metrics were subsequently analyzed using thematic analysis. The findings indicated that most metrics evaluate quality attributes related to performance efficiency (64%) and that there is a need for metrics that evaluate other characteristics, such as security and compatibility. The majority of the metrics are used during the Operation phase of the cloud services and are applied to the running service. Our results also revealed that metrics for cloud services are still in the early stages of maturity - only 10% of the metrics had been empirically validated. The proposed taxonomy can be used by practitioners as a guideline when specifying service level objectives or deciding which metric is best suited to the evaluation of their cloud services, and by researchers as a comprehensive quality framework in which to evaluate their approaches.","2169-3536","","10.1109/ACCESS.2020.3009079","Spanish Ministry of Science, Innovation and Universities through the Adapt@Cloud Project(grant numbers:TIN2017-84550-R); Universidad Central del Ecuador (UCE); Banco Central del Ecuador; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139920","Software quality;metrics;cloud services;systematic literature review","Measurement;Cloud computing;Taxonomy;Quality of service;Systematics;NIST;Elasticity","","11","","78","CCBY","14 Jul 2020","","","IEEE","IEEE Journals"
"LiDiTE: A Full-Fledged and Featherweight Digital Twin Framework","E. Russo; G. Costa; G. Longo; A. Armando; A. Merlo","Department of Informatics, Bioengineering, Robotics, and Systems Engineering (DIBRIS), University of Genoa, Genova, Italy; Institute for Advanced Studies (IMT), Lucca, Italy; Department of Informatics, Bioengineering, Robotics, and Systems Engineering (DIBRIS), University of Genoa, Genova, Italy; Department of Informatics, Bioengineering, Robotics, and Systems Engineering (DIBRIS), University of Genoa, Genova, Italy; Department of Informatics, Bioengineering, Robotics, and Systems Engineering (DIBRIS), University of Genoa, Genova, Italy",IEEE Transactions on Dependable and Secure Computing,"10 Nov 2023","2023","20","6","4899","4912","The rising of the Cyber-Physical System (CPS) and the Industry 4.0 paradigms demands the design and implementation of Digital Twin Frameworks (DTFs) that may support the quick build of reliable Digital Twins (DTs) for experimental and testing purposes. Most of the current DTF proposals allow the generation of DTs at a good pace but affect generality, scalability, portability, and completeness. As a consequence, current DTF are mostly domain-specific and hardly span several application domains (e.g., from simple IoT deployments to the modeling of complex critical infrastructures). Furthermore, the generated DTs often requires a high amount of computational resource to run. In this paper, we present LiDiTE, a solution based on a novel reference model for general-purpose DTFs. LiDiTE overcomes the limitations of state-of-the-art tools by supporting the fine-grained development of real-world complexity scenarios. To achieve that, LiDiTE builds on technologies that favor scalability, reuse, and extensibility of scenarios. We show such features by building the DT of real critical infrastructure and evaluating the performance of our DT against those of the real system. Further contributions of this paper include open access to the source code of LiDiTE and the experimental dataset.","1941-0018","","10.1109/TDSC.2023.3236798","Horizon 2020 project; Strategic Programs for Advanced Research and Technology in Europe; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10016764","Cyber-physical systems;digital twin;digital twin framework;digital simulation;reference model","Computational modeling;Digital twins;Behavioral sciences;Business;Actuators;Valves;Sensors","","10","","52","IEEE","13 Jan 2023","","","IEEE","IEEE Journals"
"Continuous Delivery of Customized SaaS Edge Applications in Highly Distributed IoT Systems","R. López-Viana; J. Díaz; V. H. Díaz; J. -F. Martínez","Universidad Politécnica de Madrid, Madrid, Spain; Universidad Politécnica de Madrid, Madrid, Spain; Universidad Politécnica de Madrid, Madrid, Spain; Universidad Politécnica de Madrid, Madrid, Spain",IEEE Internet of Things Journal,"9 Oct 2020","2020","7","10","10189","10199","Edge computing is a reality for the current IoT systems that need fast processing and quick response time to make real-time decisions and IoT systems without permanent connectivity to the cloud (e.g., car manufacturing, precision agriculture, or cattle raising). Additionally, these industries are facing the need for rapid and continuous innovation by accelerating the delivery of over-the-air (OTA) software updates in edge devices. DevOps promotes collaboration between development and operation teams and automation at all steps of software construction to achieve continuous delivery (CD) of business value. Although DevOps has demonstrated numerous successful cases in the Web domain, in the IoT domain and, more specifically, at the edge, there are few reported cases. This work presents a success case of CD of customized software as a service software as a service (SaaS) updates at the IoT Edge. This may enable new business models at the IoT Edge. This article presents an architectural model of a highly distributed (cloud and edge) IoT system and a CD process flow for customized SaaS applications in edge nodes. Both the architectural model and the CD process flow are instantiated in a case study for precision agriculture.","2327-4662","","10.1109/JIOT.2020.3009633","AFARCLOUD European Project (Aggregate Farming in the Cloud); ECSEL JU(grant numbers:783221-AFarCloud-H2020-ECSEL-2017-2); Spanish Ministry of Science, Innovation and Universities(grant numbers:PCI2018-092965); crowdsaving(grant numbers:TIN2016-79726-C2-1-R); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9142209","Continuous delivery (CD);DevOps;edge computing;IoT systems;software as a service (SaaS)","Cloud computing;Software as a service;Edge computing;Internet of Things;Business;Agriculture","","10","","25","IEEE","16 Jul 2020","","","IEEE","IEEE Journals"
"Lifecycle Management of Automotive Safety-Critical Over the Air Updates: A Systems Approach","H. Guissouma; C. P. Hohl; F. Lesniak; M. Schindewolf; J. Becker; E. Sax","Institute of Information Processing Technologies, Karlsruhe Institute of Technology, Karlsruhe, Germany; FZI, Karlsruhe, Germany; Institute of Information Processing Technologies, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute of Information Processing Technologies, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute of Information Processing Technologies, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute of Information Processing Technologies, Karlsruhe Institute of Technology, Karlsruhe, Germany",IEEE Access,"7 Jun 2022","2022","10","","57696","57717","With the increasing importance of Over The Air (OTA) updates in the automotive field, maintaining safety standards becomes more challenging as frequent incremental changes of embedded software are regularly integrated into a wide range of vehicle variants. This necessitates new processes and methodologies with a holistic view on the backend, where the updates are developed and released, and the frontend (vehicle), to which the updates are deployed. In this paper, we introduce an approach, including a process and a methodology, for continuous contract-based design, validation and deployment of modular updates for variant-rich automotive systems. The approach considers the vehicle as part of its connected environment enclosing a backend and concentrates on safety-critical applications. In addition, we present the UPDateable Automotive Test dEmonstratoR (UPDATER), which is a mock-up for modern Electric/Electronic architectures including a backend and a frontend part. It serves as a prototype for developing, deploying and monitoring automotive OTA updates. In a case study based on UPDATER, we apply the approach to three exemplary updates of a variable Advanced Driver Assistance System (ADAS). We show how the updates development and management may be achieved in an efficient and agile way.","2169-3536","","10.1109/ACCESS.2022.3176879","German Federal Ministry of Education and Research through the German Federal Ministry of Education and Research (BMBF) in the Project Step-Up!CPS (Förderkennzeichen)(grant numbers:01IS18080D); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779735","OTA updates;contract-based design;variant and configuration management;safety-critical systems;middleware;DevOps;advanced driver assistance systems;monitoring","Computer architecture;Automotive engineering;Software;Safety;Monitoring;Conferences;Adaptation models","","10","","67","CCBY","23 May 2022","","","IEEE","IEEE Journals"
"Encryption as a Service for IoT: Opportunities, Challenges, and Solutions","A. Javadpour; F. Ja’fari; T. Taleb; Y. Zhao; B. Yang; C. Benzaïd","Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, Finland; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Faculty of Information Technology and Electrical Engineering, Oulu University, Oulu, Finland; Science and Technology on Communication Security Laboratory, Chengdu, China; ICTFICIAL Oy, Espoo, Finland; Faculty of Information Technology and Electrical Engineering, Oulu University, Oulu, Finland",IEEE Internet of Things Journal,"21 Feb 2024","2024","11","5","7525","7558","The widespread adoption of Internet of Things (IoT) technology has introduced new cybersecurity challenges. Encryption services are being offloaded to cloud and fog platforms to mitigate these risks. Encryption as a Service (EaaS) emerges as a remedy, offering cryptographic solutions tailored to the resource constraints of IoT devices. This study thoroughly examines existing EaaS platforms, categorizing them based on encryption algorithms and service offerings. Additionally, we outline various EaaS architecture types depending on the placement of key components. Practical implementations of these platforms are explored through different testbeds. A key focus lies in dissecting the challenges that EaaS faces, particularly in the context of IoT, while suggesting potential remedies. This work stands out as an all-encompassing exploration, bridging the gap left by previous surveys.","2327-4662","","10.1109/JIOT.2023.3341875","European Union’s Horizon Europe Research and Innovation Program(grant numbers:HORIZON-JU-SNS-2022); RIGOUROUS Project(grant numbers:101095933); Research Council of Finland (former Academy of Finland) 6G Flagship Program(grant numbers:346208); National Science Foundation of China(grant numbers:U20B2049,U20B2048,U20B2046); National Natural Science Foundation of China(grant numbers:62372076); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10363120","Cloud computing;Encryption as a Service (EaaS);fog computing;Internet of Things (IoT)","Encryption;Internet of Things;Security;Cloud computing;Surveys;Heuristic algorithms;Performance evaluation","","10","","66","IEEE","15 Dec 2023","","","IEEE","IEEE Journals"
"Resource Provisioning Using Meta-Heuristic Methods for IoT Microservices With Mobility Management","S. M. Rajagopal; M. Supriya; R. Buyya","Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Bengaluru, India; Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Bengaluru, India; CLOUDS Laboratory, School of Computing and Information Systems, The University of Melbourne, Melbourne, VIC, Australia",IEEE Access,"22 Jun 2023","2023","11","","60915","60938","The fog and edge computing paradigm provide a distributed architecture of nodes with processing capability for smart healthcare systems driven by Internet of Thing (IoT) applications. It also provides a method to reduce big data transmissions that cause latency and enhance the system’s efficiency. Resource provisioning and scheduling in edge and fog systems is a significant problem due to heterogeneity and dispersion of edge/fog/cloud resources. The goal of scheduling is to map tasks to appropriate resources, which belong to NP-hard problems, and it takes much time to find an optimal solution. Meta-heuristic methods achieve near-optimal solutions within a reasonable time. Current edge/fog resource allocation research does not sufficiently address resource allocation problems in mobility-aware microservice-based IoT applications. This paper proposes a meta-heuristic-based micro-service resource provisioning model with mobility management for smart healthcare systems. The proposed approach has been tested on an experimental set-up with a simulation of a critical real-time smart healthcare application with and without considering the mobility of the devices. It applies meta-heuristic methods such as modified genetic and flower pollination algorithms for resource management. The proposed method outperforms the existing solutions in energy consumption, network usage, cost, execution time, and latency by 17%, 20%, 22%, 17%, and 63%, respectively.","2169-3536","","10.1109/ACCESS.2023.3281348","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10138589","Edge computing;fog computing;Internet of Things;meta-heuristic;microservices;mobility;smart healthcare;time critical applications","Edge computing;Internet of Things;Smart healthcare;Metaheuristics;Microservice architectures","","10","","60","CCBY","30 May 2023","","","IEEE","IEEE Journals"
"Intrinsic Security and Self-Adaptive Cooperative Protection Enabling Cloud Native Network Slicing","W. Qiang; W. Chunming; Y. Xincheng; C. Qiumei","College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Future Network Research Center, State Key Laboratory of Mobile Networks and Mobile Multimedia Technology, Nanjing, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China",IEEE Transactions on Network and Service Management,"10 Jun 2021","2021","18","2","1287","1304","With the emergence of cloud native technology, the network slicing enables automatic service orchestration, flexible network scheduling and scalable network resource allocation, which profoundly affects the traditional security solution. Security is regarded as a technology independent of the cloud native architecture in the initial design, traditional passive defense such as “reinforced” and “stacked” is relied on to achieve system security protection. The lack of intrinsic security mechanisms makes the system capability insufficient when faces the uncertain threat brought by vulnerabilities and backdoors under the ecosystem of opening-up and sharing. The static nature of existing networks and computing systems makes them easy to be compromised and hard to defend, and thus it is urgent to provide intrinsic security and proactive protection against the unpredictable attacks. To this end, this paper proposes a novel paradigm named intrinsic cloud security (iCS) from the perspective of dynamic defense. The dynamic defense provides component-level security, and has complementary and consistency with the cloud native environment. In particular, iCS introduces mimic defense and moving target defense (MTD), and makes full use of the new features introduced by cloud native to implement an intrinsic and proactive defense mechanism with acceptable costs and efficiency. The iCS paradigm achieves seamless integration and symbiosis evolution between security and cloud native. We implement a trial of iCS based on 5GC commercial system and evaluate its performance on costs, efficiency and attack success. The result shows that the iCS enhanced mode always can provide a better and more stable defense effects.","1932-4537","","10.1109/TNSM.2021.3071774","National Key R&D Program of China(grant numbers:2020YFB1804705); Key R&D Program of Zhejiang Province(grant numbers:2020C01077,2021C01036,2020C01021); Major Scientific Project of Zhejiang Lab(grant numbers:2018FD0ZX01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9399165","Cyber security;MTD;mimic defense;cloud native;network functions virtualization (NFV);software defined network (SDN);network slicing","Security;Cloud computing;Network slicing;5G mobile communication;Ecosystems;Symbiosis;Computer science","","10","","46","IEEE","8 Apr 2021","","","IEEE","IEEE Journals"
"Guest Editorial: AI-Enabled Threat Intelligence and Hunting Microservices for Distributed Industrial IoT System","N. Moustafa; K. -K. R. Choo; A. M. Abu-Mahfouz","School of Engineering and Information Technology, University of New South Wales (UNSW Canberra), Campbell, ACT, Australia; Department of Information Systems and Cyber Security, University of Texas at San Antonio, San Antonio, TX, USA; Council for Scientific and Industrial Research, Department of Electrical, Electronic and Computer Engineering, University of Pretoria, Pretoria, South Africa",IEEE Transactions on Industrial Informatics,"7 Dec 2021","2022","18","3","1892","1895","Industrial Internet of Things (IIoT) systems are increasingly found in settings such as factories, smart cities/nations, and healthcare institutions. These systems facilitate the interconnection of automation and data analytics across different industrial technologies, such as cyber-physical systems, Internet of Things (IoT), and cloud and edge computing devices and systems. However, IIoT systems also generate significant volume of data, which can incur significant overheads in processing such data at cloud centers [A1]. Existing IIoT systems may be developed as monolithic architecture, where such a system is deployed as a single solution. In this architectural design, few programming languages can be used to create a single application or process composed of several classes, methods, and packages, in which the entire application is executed in one server irrespective of the application requirements.","1941-0050","","10.1109/TII.2021.3111028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9536391","","Special issues and sections;Security;Industrial Internet of Things;Microservice architecture;Edge computing;Privacy;Blockchains","","10","","0","IEEE","13 Sep 2021","","","IEEE","IEEE Journals"
"Efficient Resource Management of Micro-Services in VANETs","M. Bany Taha; S. Alrabaee; K. -K. R. Choo","Data Science and Artificial Intelligent Department, American University of Madaba, Amman, Jordan; Information Systems and Security Department, United Arab Emirates University, Al Ain, Abu Dhabi, United Arab Emirates; Department of Information System and Cybersecurity, The University of Texas at San Antonio, San Antonio, TX, USA",IEEE Transactions on Intelligent Transportation Systems,"7 Jul 2023","2023","24","7","6820","6835","While vehicular ad hoc Networks (VANETs) are relatively well-studied, a number of challenges remain, particularly as autonomous vehicles become more commonplace. For example, devices on a vehicle such as, On-Board Units (OBUs) may have resource constraints which render them incapable of supporting computationally expensive cryptography operations required to achieve various security features. One potential solution is to offload computationally expensive tasks to other nodes in the VANET; however, the dynamic nature of the setup (such as the, mobility of the requesting vehicles and other nodes) compounds the challenge of resource management. In this context, we propose a scheme that uses Ciphertext-Policy Attribute-Based Encryption (CP-ABE) to achieve data confidentiality in VANETs despite such challenges. Specifically, we build a cluster of vehicles to perform CP-ABE operations without relying on other nodes in the VANET. We use Kubernetes, an open-source container orchestration system, to build vehicle cluster(s) to handle distributed micro-tasks. In this scheme, we use a set of factors that impact the computation operations in cluster vehicle components (i.e., the OBU). Each factor, including the distance between the data owner vehicle and the target vehicle, the duration of each target vehicle in the cluster, and the resource of each vehicle in the cluster, has a weight based on its influence in computational operations. The Euclidean method is used to calculate the weight value for each factor. Based on the final total weight for each vehicle, our approach distributes the tasks between vehicles. We evaluate our results by comparing our approach with the mechanism of Kubernetes for task distribution, which only considers the resources in each vehicle. We also consider several scenarios with varying factors to evaluate their impact on the execution time of CP-ABE on OBUs in addition to using simulations to evaluate the performance of our approach in terms of transmission and propagation overheads for vehicles in the cluster.","1558-0016","","10.1109/TITS.2023.3255921","UAE University and Zayed University joint grant(grant numbers:12R143); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10077450","Resource management;SLNN;MLP;Euclidean algorithm;CP-ABE;VANET","Cryptography;Vehicular ad hoc networks;Task analysis;Peer-to-peer computing;Resource management;Clustering algorithms;Vehicle dynamics","","9","","38","IEEE","20 Mar 2023","","","IEEE","IEEE Journals"
"Towards Edge Computing as a Service: Dynamic Formation of the Micro Data-Centers","M. Simić; I. Prokić; J. Dedeić; G. Sladić; B. Milosavljević","Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia",IEEE Access,"23 Aug 2021","2021","9","","114468","114484","Edge computing brings cloud services closer to the edge of the network, where data originates, and dramatically reduces the network latency of the cloud. It is a bridge linking clouds and users making the foundation for novel interconnected applications. However, edge computing still faces many challenges like remote configuration, well-defined native applications model, and limited node capacity. It lacks geo-organization and a clear separation of concerns. As such edge computing is hard to be offered as a service for future real-time user-centric applications. This paper presents the dynamic organization of geo-distributed edge nodes into micro data-centers to cover any arbitrary area and expand capacity, availability, and reliability. A cloud organization is used as an influence with adaptations for a different environment, and a model for edge applications utilizing these adaptations is presented. It is argued that the presented model can be integrated into existing solutions or used as a base for the development of future systems. Furthermore, a clear separation of concerns is given for the proposed model. With the separation of concerns setup, edge-native applications model, and a unified node organization, we are moving towards the idea of edge computing as a service, like any other utility in cloud computing.","2169-3536","","10.1109/ACCESS.2021.3104475","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9512038","Cloud computing;distributed systems;edge computing;formal specifications;infrastructure as software;platform","Cloud computing;Computational modeling;Adaptation models;Peer-to-peer computing;Edge computing;Organizations;Servers","","9","","65","CCBY","12 Aug 2021","","","IEEE","IEEE Journals"
"A Hybrid Task Crash Recovery Solution for Edge Computing in IoT-Based Manufacturing","R. Xiao; Y. Zhang; X. H. Cui; F. Zhang; H. H. Wang","School of Cyber Science and Engineering, Wuhan University, Wuhan, China; School of Computer and Information Engineering, Hubei University, Wuhan, China; School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Department of Finance, McQuay Air Conditioning and Refrigeration (Wuhan) Company, Wuhan, China; School of Computer and Information Engineering, Hubei University, Wuhan, China",IEEE Access,"9 Aug 2021","2021","9","","106220","106231","Smart factory is the deep integration of technology, business, products, and industry. The R&D of products, the construction of factory information, customer orders, and supply chain data are combined to act on the production. This paper proposes a Hybrid Internet of Things (H-IoT) platform framework, which consists of five layers: intelligent device layer, communication protocol layer, edge computing layer, IoT control layer, and application layer. Object Linking and Embedding (OLE) for Process Control (OPC) transmits the received device data to the database of the edge server through the TCP/IP protocol. After the edge server performs preprocessing such as data cleaning, the device data are uniformly collected and distributed through the IoT layer. The various subsystems of the smart factory obtain the required resource data through IoT. To run tasks uninterruptedly and efficiently, we propose a Container-Managed Task Crash Recovery Mechanism for the Edge Computing (EC) layer. We design a reasonable EC hierarchy, then propose a task migration scheduling strategy. Our method ensures the reliability and stability of manufacturing with mass data transmission even in the case of a random crash of partial edge servers.","2169-3536","","10.1109/ACCESS.2021.3068471","National Key Research and Development Program of China(grant numbers:2018YFC1604000); Fundamental Research Funds for the Central Universities of China(grant numbers:2042017gf0035,2042019kf0268); National Natural Science Foundation of China(grant numbers:61572374,U163620068,U1135005,61572371); Natural Science Foundation of Hubei Province(grant numbers:2017CFB663); Academic Team Building Plan for Young Scholars from Wuhan University(grant numbers:WHU2016012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9385091","H-IoT framework;EC;container-managed task crash recovery mechanism","Edge computing;Production;Task analysis;Manufacturing;Computer architecture;Job shop scheduling;Processor scheduling","","9","","40","CCBYNCND","24 Mar 2021","","","IEEE","IEEE Journals"
"Smart City Middleware: A Survey and a Conceptual Framework","C. Goumopoulos","Department of Information and Communications Systems Engineering, University of the Aegean, Samos, Greece",IEEE Access,"9 Jan 2024","2024","12","","4015","4047","Smart city middleware serves as a foundational tool in the evolution of urban digitalization, acting as an intermediary software layer that simplifies the development, deployment, and management of applications tailored for smart urban environments. However, the development of effective middleware for smart cities is challenging. The present research embarks on a comprehensive exploration of the smart city middleware landscape, unraveling the intricacies of its development and the challenges faced therein. Rooted in the assessment of 20 distinct middleware solutions, our study highlights the pivotal technologies, features and functionalities that are imperative for a middleware to effectively support a city’s digital transformation. The functional and non-functional requirements form the nucleus of our evaluation. We also explore the architectural styles pivotal to middleware development and the programming paradigms shaping smart city application development. Our study highlights challenges in using middleware for smart city applications, such as interoperability, scalability, security amidst big data, context management, reliability, quality of service, energy efficiency, and compliance with technological standards and regulations. Based on the detailed analysis, we propose a conceptual framework for smart city middleware, shaped by the challenges and requirements identified in existing literature and middleware solutions. This framework is designed to reflect the diverse demands and complexities of urban digital transformation, and guide smart city middleware development accordingly. As a result, this research stands as a reference study for software developers, urban planners, and researchers, outlining the current state and future directions in the domain of smart city middleware.","2169-3536","","10.1109/ACCESS.2023.3349376","University of the Aegean Research Unit through the “Internet of Things-Intelligent Environments in Next-Generation Networks” Project(grant numbers:70477); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10379798","Smart cities;middleware;conceptual framework;surveys;functional and non-functional requirements;enabling technologies;architectural styles;programming paradigms;challenges","Smart cities;Middleware;Software;Surveys;Interoperability;Security;Computer architecture","","9","","123","CCBY","3 Jan 2024","","","IEEE","IEEE Journals"
"ML4IoT: A Framework to Orchestrate Machine Learning Workflows on Internet of Things Data","J. M. Alves; L. M. Honório; M. A. M. Capretz","Department of Electrical and Computer Engineering, Western University, London, ON, Canada; Department of Electrical Energy, Federal University of Juiz de Fora, Juiz de Fora, Brazil; Department of Electrical and Computer Engineering, Western University, London, ON, Canada",IEEE Access,"28 Oct 2019","2019","7","","152953","152967","Internet of Things (IoT) applications generate vast amounts of real-time data. Temporal analysis of these data series to discover behavioural patterns may lead to qualified knowledge affecting a broad range of industries. Hence, the use of machine learning (ML) algorithms over IoT data has the potential to improve safety, economy, and performance in critical processes. However, creating ML workflows at scale is a challenging task that depends upon both production and specialized skills. Such tasks require investigation, understanding, selection, and implementation of specific ML workflows, which often lead to bottlenecks, production issues, and code management complexity and even then may not have a final desirable outcome. This paper proposes the Machine Learning Framework for IoT data (ML4IoT), which is designed to orchestrate ML workflows, particularly on large volumes of data series. The ML4IoT framework enables the implementation of several types of ML models, each one with a different workflow. These models can be easily configured and used through a simple pipeline. ML4IoT has been designed to use container-based components to enable training and deployment of various ML models in parallel. The results obtained suggest that the proposed framework can manage real-world IoT heterogeneous data by providing elasticity, robustness, and performance.","2169-3536","","10.1109/ACCESS.2019.2948160","Natural Sciences and Engineering Research Council of Canada(grant numbers:STPGP 506840-17); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8876834","Big data;container-based virtualization;IoT;machine learning;machine learning workflow;microservices","Machine learning;Internet of Things;Data models;Training;Big Data;Task analysis;Tools","","9","","41","CCBY","21 Oct 2019","","","IEEE","IEEE Journals"
"Online Workload Burst Detection for Efficient Predictive Autoscaling of Applications","F. Tahir; M. Abdullah; F. Bukhari; K. M. Almustafa; W. Iqbal","Punjab University College of Information Technology (PUCIT), University of the Punjab, Lahore, Pakistan; Punjab University College of Information Technology (PUCIT), University of the Punjab, Lahore, Pakistan; Punjab University College of Information Technology (PUCIT), University of the Punjab, Lahore, Pakistan; College of Computer and Information Sciences, Prince Sultan University, Riyadh, Saudi Arabia; Punjab University College of Information Technology (PUCIT), University of the Punjab, Lahore, Pakistan",IEEE Access,"28 Apr 2020","2020","8","","73730","73745","Autoscaling methods are employed to ensure the scalability of cloud-hosted applications. The public-facing applications are prone to receive sudden workload bursts, and the existing autoscaling methods do not handle the bursty workloads gracefully. It is challenging to detect the burst online from the incoming dynamic workload traffic, and then identifying appropriate resources to address the burst without overprovisioning is even harder. In this paper, we address this challenge by investigating the appropriate method for online burst detection and then proposed a novel predictive autoscaling method to use burst detection for satisfying specific response time requirements. We compared the proposed method with multiple state-of-the-art baseline autoscaling methods under multiple realistic and synthetic bursty workloads for a benchmark application. Our experimental results show a 60.8% average decrease in response time violations as compared to the baseline method.","2169-3536","","10.1109/ACCESS.2020.2988207","Prince Sultan University, Riyadh, Saudi Arabia, and Higher Education Commission, Pakistan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9069259","Autoscaling;predictive;SLO violations;response time;workload;burstiness;online burst detection","Time factors;Resource management;Cloud computing;Computational modeling;Entropy;Predictive models;Load modeling","","9","","56","CCBY","16 Apr 2020","","","IEEE","IEEE Journals"
"Partition Placement and Resource Allocation for Multiple DNN-Based Applications in Heterogeneous IoT Environments","T. Kim; H. Park; Y. Jin; S. -S. Lee; S. Lee","Department of Computer Science, Yonsei University, Seoul, South Korea; Department of Computer Science, Yonsei University, Seoul, South Korea; Department of Computer Science, Yonsei University, Seoul, South Korea; Department of Computer Science, Yale University, New Haven, CT, USA; Department of Computer Science, Yonsei University, Seoul, South Korea",IEEE Internet of Things Journal,"17 May 2023","2023","10","11","9836","9848","The evolution of the Internet of Things (IoT) has been driving the explosive growth of deep neural network (DNN)-based applications and processing demands. Hence, edge computing has emerged as a potential solution to meet these processing requirements. However, emerging IoT applications have increasingly demanded to run multiple DNNs to extract multifaceted knowledge, requiring more computational resources and increasing response time. Consequently, edge nodes cannot act as a complete substitute for the previous cloud paradigm, owing to their relatively limited resources. To address this problem, we propose to incorporate nearby IoT devices when allocating resources to multiple DNN models. Furthermore, the optimization of resource allocation can be hindered by the heterogeneity of IoT devices, which affects the delay performance of DNN-based computing. In this context, we propose a DNN partition placement and resource allocation strategy that considers different processing powers, memory, and battery levels for heterogeneous IoT devices. We evaluate the performance of the proposed strategy through extensive simulations. Simulation results reveal that the proposed strategy outperforms other existing solutions in terms of end-to-end delay, service probability, and energy consumption. The proposed solution was further simulated in a Kubernetes testbed consisting of actual devices to assess its feasibility.","2327-4662","","10.1109/JIOT.2023.3235993","National Research Foundation of Korea (NRF); Korea Government (MSIT)(grant numbers:2022R1A2B5B01001683); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10014999","Heterogeneous Internet of Things (IoT);multiple deep neural networks (DNNs);partition placement;resource allocation;utility","Internet of Things;Computational modeling;Resource management;Performance evaluation;Cloud computing;Edge computing;Delays","","9","","50","IEEE","11 Jan 2023","","","IEEE","IEEE Journals"
"Transforming the 5G RAN With Innovation: The Confluence of Cloud Native and Intelligence","N. Li; X. Xu; Q. Sun; J. Wu; Q. Zhang; G. Chi; C. -L. Ⅰ; N. Sprecher","China Mobile Research Institute, Beijing, China; China Mobile Research Institute, Beijing, China; China Mobile Research Institute, Beijing, China; China Mobile Research Institute, Beijing, China; China Mobile Research Institute, Beijing, China; China Mobile Research Institute, Beijing, China; China Mobile Research Institute, Beijing, China; Nokia, Kfar-Saba, Israel",IEEE Access,"18 Jan 2023","2023","11","","4443","4454","Intelligence and cloudification are widely recognized as key driving forces in the evolution of 5G radio access network (RAN). This paper presents a promising architecture framework for the evolution of 5G radio access network, enabled by a deep integration with cloudification and artificial intelligence/machine learning (AI/ML) technologies. To accommodate the diversified scenarios and services and handle the complexity of the 5G network in a flexible and efficient manner, the architecture framework highlights three concepts: convergence of RAN and cloud, RAN empowered by hierarchical AI capabilities, and mutual awareness between RAN and services. The key design aspects and technologies that realize those concepts are discussed systematically. Two typical use cases including the RAN slice resource allocation optimization and RAN-aware video service assurance, are demonstrated along with the simulation or lab test results to validate the potential of the architecture framework.","2169-3536","","10.1109/ACCESS.2023.3234493","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10006796","5G;AI/ML;cloud-native;RAN;service-awareness","Cloud computing;5G mobile communication;Computer architecture;Optimization;Radio access networks;Cloud computing","","9","","14","CCBY","5 Jan 2023","","","IEEE","IEEE Journals"
"Toward Transportation Digital Twin Systems for Traffic Safety and Mobility: A Review","M. S. Irfan; S. Dasgupta; M. Rahman","Department of Civil, Construction, and Environmental Engineering, University of Alabama, Tuscaloosa, AL, USA; Department of Civil, Construction, and Environmental Engineering, University of Alabama, Tuscaloosa, AL, USA; Department of Civil, Construction, and Environmental Engineering, University of Alabama, Tuscaloosa, AL, USA",IEEE Internet of Things Journal,"8 Jul 2024","2024","11","14","24581","24603","Digital twin (DT) systems aim to create virtual replicas of physical objects that are updated in real-time with their physical counterparts and evolve alongside the physical assets throughout their lifecycle. DT technology promises to significantly enhance the capabilities of intelligent transportation systems (ITSs) through improving safety and mobility. The objectives of this review article are to: 1) conduct an in-depth review of DT technology applications in the transportation domain, i.e., safety and mobility applications; 2) develop a reference architecture for a transportation DT (TDT) system based on a hierarchical concept to guide in developing TDT systems at any scale; 3) identify research challenges for each component, i.e., the physical space, the communication gateway and the digital space of the TDT system, to realize safe and operationally efficient ITS; and 4) provide future research perspectives to develop and deploy networkwide transportation DT systems considering the scale and dynamic nature of future connected and automated transportation systems. Thus, this article focuses on providing a comprehensive understanding of the requirements, reference architecture, challenges, and future research opportunities for a TDT system, starting from individual transportation assets and building up to the entire networkwide TDT.","2327-4662","","10.1109/JIOT.2024.3395186","Alabama Transportation Institute (ATI) and Federal Motor Carrier Safety Administration (FMCSA)(grant numbers:25-1121-0006-004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10530969","Automation;connected transportation system;digital twin (DT);mobility;safety","Transportation;Digital twins;Safety;Reviews;Internet of Things;Predictive models;Surveys","","9","","147","IEEE","15 May 2024","","","IEEE","IEEE Journals"
"CLB: Coarse-Grained Precision Traffic-Aware Weighted Cost Multipath Load Balancing on PISA","D. D. Robin; J. I. Khan","Computer Science Department, Kent State University, Kent, OH, USA; Computer Science Department, Kent State University, Kent, OH, USA",IEEE Transactions on Network and Service Management,"9 Jun 2022","2022","19","2","784","803","This paper presents CLB, a programmable switch-based general-purpose in-network load balancer that can adapt to traffic changes at a very high speed. It uses Weighted-Cost Multipath (WCMP) mechanism for traffic-aware load balancing over many paths at a coarse-grained precision. CLB can be configured to match the load balancing requirements of a wide range of applications at line rate. We have analytically shown that CLB can achieve a bounded response time to traffic changes in the data plane. We implement CLB using the P4 programming language. Our experimental evaluation shows CLB can successfully distribute the incoming load over multiple paths for a given path-weight distribution and leads to performance improvement compared to other load balancing schemes.","1932-4537","","10.1109/TNSM.2022.3142106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9676704","Load-balancer;Programmable Switch;P4;SDN;BMV2;Traffic-aware;In-network","Load management;Switches;Servers;Throughput;Hardware;Costs;Random access memory","","9","","61","IEEE","11 Jan 2022","","","IEEE","IEEE Journals"
"A Comprehensive Review of Convolutional Neural Networks for Defect Detection in Industrial Applications","R. Khanam; M. Hussain; R. Hill; P. Allen","Department of Computer Science, School of Computing and Engineering, University of Huddersfield, Huddersfield, U.K.; Department of Computer Science, School of Computing and Engineering, University of Huddersfield, Huddersfield, U.K.; Department of Computer Science, School of Computing and Engineering, University of Huddersfield, Huddersfield, U.K.; Department of Computer Science, School of Computing and Engineering, University of Huddersfield, Huddersfield, U.K.",IEEE Access,"15 Jul 2024","2024","12","","94250","94295","Quality inspection and defect detection remain critical challenges across diverse industrial applications. Driven by advancements in Deep Learning, Convolutional Neural Networks (CNNs) have revolutionized Computer Vision, enabling breakthroughs in image analysis tasks like classification and object detection. CNNs’ feature learning and classification capabilities have made industrial defect detection through Machine Vision one of their most impactful applications. This article aims to showcase practical applications of CNN models for surface defect detection across various industrial scenarios, from pallet racks to display screens. The review explores object detection methodologies and suitable hardware platforms for deploying CNN-based architectures. The growing Industry 4.0 adoption necessitates enhancing quality inspection processes. The main results demonstrate CNNs’ efficacy in automating defect detection, achieving high accuracy and real-time performance across different surfaces. However, challenges like limited datasets, computational complexity, and domain-specific nuances require further research. Overall, this review acknowledges CNNs’ potential as a transformative technology for industrial vision applications, with practical implications ranging from quality control enhancement to cost reductions and process optimization.","2169-3536","","10.1109/ACCESS.2024.3425166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10589380","Computer vision;convolutional neural network;deep learning;industrial defect detection;object detection;quality inspection: manufacturing","Defect detection;Hardware;Convolutional neural networks;Computer architecture;Reviews;Artificial intelligence;Inspection;Computer vision;Deep learning;Quality assessment;Manufacturing processes","","9","","387","CCBYNCND","8 Jul 2024","","","IEEE","IEEE Journals"
"A Machine Learning Solution for Video Delivery to Mitigate Co-Tier Interference in 5G HetNets","D. Anand; M. A. Togou; G. -M. Muntean","School of Electronic Engineering, Dublin City University, Dublin, Ireland; School of Electronic Engineering, Dublin City University, Dublin, Ireland; School of Electronic Engineering, Dublin City University, Dublin, Ireland",IEEE Transactions on Multimedia,"31 Oct 2023","2023","25","","5117","5129","The exponential demand for multimedia services is one reason behind the substantial growth of mobile data traffic. Video traffic patterns have significantly changed in the past two years due to the coronavirus disease (COVID-19). The worldwide pandemic has caused many individuals to work from home and use various online video platforms (e.g., Zoom, Google Meet, and Microsoft Teams). As a result, overloaded macrocells are unable to ensure high Quality of Experience (QoE) to all users. Heterogeneous Networks (HetNets) consisting of small cells (femtocells) and macrocells are a promising solution to mitigate this problem. A critical challenge with the deployment of femtocells in HetNets is the interference management between Macro Base Stations (MBSs), Femto Base Stations (FBSs), and between FBS and FBS. Indeed, the dynamic deployment of femtocells can lead to co-tier interference. With the rolling out of the 5G mobile network, it becomes imperative for mobile operators to maintain network capacity and manage different types of interference. Machine Learning (ML) is considered a promising solution to many challenges in 5G HetNets. In this paper, we propose a Machine Learning Interference Classification and Offloading Scheme (MLICOS) to address the problem of co-tier interference between femtocells for video delivery. Two versions of MLICOS, namely, MLICOS1 and MLICOS2, are proposed. The former uses conventional ML classifiers while the latter employs advanced ML algorithms. Both versions of MLICOS are compared with the classic Proportional Fair (PF) scheduling algorithm, Variable Radius and Proportional Fair scheduling (VR + PF) algorithm, and a Cognitive Approach (CA). The ML models are assessed based on the prediction accuracy, precision, recall and F-measure. Simulation results show that MLICOS outperforms the other schemes by providing the highest throughput and the lowest delay and packet loss ratio. A statistical analysis was also carried out to depict the degree of interference faced by users when different schemes are employed.","1941-0077","","10.1109/TMM.2022.3187607","Science Foundation Ireland(grant numbers:18/CRT/6183); ML-LABS Centre for Research Training(grant numbers:12/RC/2289_P2); Insight SFI Research Centre for Data Analytics; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811394","HetNets;COVID-19;interference;machine learning;QoS;statistical visualization","Interference;Femtocells;Streaming media;Resource management;Quality of service;Clustering algorithms;Heuristic algorithms","","9","","50","CCBY","30 Jun 2022","","","IEEE","IEEE Journals"
"End-Edge-Cloud Collaborative Computing for Deep Learning: A Comprehensive Survey","Y. Wang; C. Yang; S. Lan; L. Zhu; Y. Zhang","School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; School of Economics and Management, University of the Chinese Academy of Sciences, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; Department of Informatics, University of Oslo, Oslo, Norway",IEEE Communications Surveys & Tutorials,"21 Nov 2024","2024","26","4","2647","2683","The booming development of deep learning applications and services heavily relies on large deep learning models and massive data in the cloud. However, cloud-based deep learning encounters challenges in meeting the application requirements of responsiveness, adaptability, and reliability. Edge-based and end-based deep learning enables rapid, near real-time analysis and response, but edge nodes and end devices usually have limited resources to support large models. This necessitates the integration of end, edge, and cloud computing technologies to combine their different advantages. Despite the existence of numerous studies on edge-cloud collaboration, a comprehensive survey for end-edge-cloud computing-enabled deep learning is needed to review the current status and point out future directions. Therefore, this paper: 1) analyzes the collaborative elements within the end-edge-cloud computing system for deep learning, and proposes collaborative training, inference, and updating methods and mechanisms for deep learning models under the end-edge-cloud collaboration framework. 2) provides a systematic investigation of the key enabling technologies for end-edge-cloud collaborative deep learning, including model compression, model partition, and knowledge transfer. 3) highlights six open issues to stimulate continuous research efforts in the field of end-edge-cloud deep learning.","1553-877X","","10.1109/COMST.2024.3393230","National Key Research and Development Program of China(grant numbers:2021YFB1715700); National Natural Science Foundation of China(grant numbers:62103046,72201266,72192843,72192844); Fundamental Research Funds for the Central Universities(grant numbers:E1E40805X2,2023CX01020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10508191","Deep learning;deep neural networks;edge computing;cloud computing;end-edge-cloud collaboration;end-edge-cloud computing","Computational modeling;Artificial intelligence;Collaboration;Surveys;Training;Cloud computing;Deep learning","","9","","252","IEEE","24 Apr 2024","","","IEEE","IEEE Journals"
"Open RAN xApps Design and Evaluation: Lessons Learnt and Identified Challenges","M. Hoffmann; S. Janji; A. Samorzewski; Ł. Kułacz; C. Adamczyk; M. Dryjański; P. Kryszkiewicz; A. Kliks; H. Bogucka","Rimedo Labs, Poznań, Poland; Rimedo Labs, Poznań, Poland; Rimedo Labs, Poznań, Poland; Rimedo Labs, Poznań, Poland; Institute of Radio-communications, Poznan University of Technology, Poznań, Poland; Rimedo Labs, Poznań, Poland; Rimedo Labs, Poznań, Poland; Rimedo Labs, Poznań, Poland; Rimedo Labs, Poznań, Poland",IEEE Journal on Selected Areas in Communications,"16 Jan 2024","2024","42","2","473","486","The concept of open radio access networks (RAN) creates numerous opportunities for developing new technology and economy branches. At the same time, a flexible and modular approach in the disaggregated RAN entails the need for careful design of the overall RAN architecture and the implementation and deployment process of new applications. It is assumed that dedicated and specialized software companies may deliver the latter. A joint effort must be guaranteed among different sectors (industry, academia, and standardization bodies) to make the whole process efficient, safe, and reliable. Here, one of the critical driving forces origins from the open-source community that often stimulates the development of a specific technology. In this paper, we address the challenges that have to be faced by third-party application developers in the context of Open RAN. Based on many implemented applications (called xApps or rApps), we compare various available solutions. We pose the most critical issues that must be tackled in the near future to stimulate the progress in open RAN development further. In particular, we compare available open platforms for xApp development and testing. We present the details of implementing four selected applications describing the problems encountered. The paper is split into two logical parts - first, we identify the key ambiguities related to the development of new xApps, which address more complicated use cases like beam management. In the second part, we present the challenges associated with detailed software implementation in existing open platforms. In the first case, we show that dedicated beam mobility management xApp can reduce beam switches and keep beam failures low. However, it requires access to detailed localization information. Similarly, the signaling storm detection xApp provides expected performance under the assumption that there is access to detailed information on, e.g., time advance resolution parameter. We conclude here that several aspects still need to be well-defined to allow smooth software implementation; these include the rules for data reporting in time, parameters available in service models, and localization features. Concerning the second logical part, related to low-level implementation, we compare the numerical results of the traffic steering and quality-of-service-based resource allocation xApps and draw conclusions related to implementation and testing. In particular, we point out problems associated with the simulator, the software, and conflicts inside. Finally, we identify the key challenges which should be treated as incentives for joint academia-industry cooperation in the field of Open RAN. Thus, the paper presents the lesson learned during the first years of xApp development.","1558-0008","","10.1109/JSAC.2023.3336190","National Centre for Research and Development in Poland within the 5GStar Project on “Advanced Methods and Techniques for Identification and Counteracting Cyberat-tacks on 5G Access Network and Applications(grant numbers:CYBERSECIDENT/487845/IV/NCBR/2021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10329915","Open RAN;5G;6G;xApp;ML","Computer architecture;Optimization;Radio access networks;Resource management;Security;5G mobile communication;6G mobile communication","","9","","43","CCBYNCND","28 Nov 2023","","","IEEE","IEEE Journals"
"Survey of Automated Fare Collection Solutions in Public Transportation","M. Bieler; A. Skretting; P. Büdinger; T. -M. Grønli","Mobile Technology Laboratory, School of Economics, Innovation and Technology, Kristiania University College, Oslo, Norway; Mobile Technology Laboratory, School of Economics, Innovation and Technology, Kristiania University College, Oslo, Norway; Mobile Technology Laboratory, School of Economics, Innovation and Technology, Kristiania University College, Oslo, Norway; Mobile Technology Laboratory, School of Economics, Innovation and Technology, Kristiania University College, Oslo, Norway",IEEE Transactions on Intelligent Transportation Systems,"14 Sep 2022","2022","23","9","14248","14266","Public transportation is expensive to operate and maintain and is often unsatisfactory. The attractiveness of public transportation can be enhanced by making it more seamless, which, in turn, would reduce financial constraints and inefficiencies. The adoption of mobile devices for ticketing solutions is promising. However, current solutions are often inflexible and require manual interactions that produce evanescent data. Therefore, using leading-edge technologies and infrastructure, it is desirable to develop a solution to fully automate fare collection. In this paper, we provide a comprehensive literature review to understand the state of public transportation and to facilitate the development and implementation of automated fare collection solutions. First, we discuss existing mobile technologies and their common ticketing implementations. Second, we provide a predictive behavior model with sensor analytics to better understand customer needs. Finally, we highlight how machine learning can harness transactional ticketing data to create valuable business intelligence. Overall, developing and implementing automated fare collection solutions in urban transportation is expected to have a significant positive impact on customer experiences, the emergence of new business models and the reduction of pollutant emissions.","1558-0016","","10.1109/TITS.2022.3161606","Pilot-T Program of the Norwegian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9756882","Behavior analysis;context awareness;intelligent sensors;intelligent transportation systems;machine learning;mobile applications","Public transportation;Machine learning;Mobile applications;Predictive models;Analytical models;Data models;Technological innovation","","9","","205","IEEE","13 Apr 2022","","","IEEE","IEEE Journals"
"Machine Learning for Service Migration: A Survey","N. Toumi; M. Bagaa; A. Ksentini","Communication Systems Department, EURECOM, Sophia-Antipolis, France; Department of Electrical and Computer Engineering, Université du Québec à Trois-Rivières, Trois-Rivières, Canada; Communication Systems Department, EURECOM, Sophia-Antipolis, France",IEEE Communications Surveys & Tutorials,"22 Aug 2023","2023","25","3","1991","2020","Future communication networks are envisioned to satisfy increasingly granular and dynamic requirements to accommodate the application and user demands. Indeed, novel immersive and mission-critical services necessitate increased computing and network resources, reduced communication latency, and guaranteed reliability. Thus, efficient and adaptive resource management schemes are required to provide and maintain sufficient levels of Quality of Experience (QoE) during the service life-cycle. Service migration is considered a key enabler of dynamic service orchestration. Indeed, moving services on demand is an efficient mechanism for user mobility support, load balancing in case of fluctuations in service demands, and hardware failure mitigation. However, service migration requires planning, as multiple parameters must be optimized to reduce service disruption to a minimum. Recent breakthroughs in computational capabilities allowed the emergence of Machine Learning as a tool for decision making that is expected to enable seamless automation of network resource management by predicting events and learning optimal decision policies. This paper surveys contributions applying Machine Learning (ML) methods to optimize service migration, providing a detailed literature review on recent advances in the field and establishing a classification of current research efforts with an analysis of their strengths and limitations. Finally, the paper provides insights on the main directions for future research.","1553-877X","","10.1109/COMST.2023.3273121","European Union’s H2020 5G!Drones Project(grant numbers:857031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10120920","Machine learning;service migration;management and orchestration;deep learning;reinforcement learning;prediction;placement","Surveys;Cloud computing;Planning;Optimization;Containers;Virtualization;Quality of experience","","9","","252","IEEE","8 May 2023","","","IEEE","IEEE Journals"
"Cloud-Native Computing: A Survey From the Perspective of Services","S. Deng; H. Zhao; B. Huang; C. Zhang; F. Chen; Y. Deng; J. Yin; S. Dustdar; A. Y. Zomaya","First Affiliated Hospital, Zhejiang University School of Medicine, Hangzhou, China; First Affiliated Hospital, Zhejiang University School of Medicine, Hangzhou, China; College of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Distributed Systems Group, Technische Universität Wien, Vienna, Austria; School of Computer Science, The University of Sydney, Sydney, NSW, Australia",Proceedings of the IEEE,"4 Mar 2024","2024","112","1","12","46","The development of cloud computing delivery models inspires the emergence of cloud-native computing. Cloud-native computing, as the most influential development principle for web applications, has already attracted increasingly more attention in both industry and academia. Despite the momentum in the cloud-native industrial community, a clear research roadmap on this topic is still missing. As a contribution to this knowledge, this article surveys key issues during the life cycle of cloud-native applications, from the perspective of services. Specifically, we elaborate on the research domains by decoupling the life cycle of cloud-native applications into four states: building, orchestration, operation, and maintenance. We also discuss the fundamental necessities and summarize the key performance metrics that play critical roles during the development and management of cloud-native applications. We highlight the key implications and limitations of existing works in each state. The challenges, future directions, and research opportunities are also discussed.","1558-2256","","10.1109/JPROC.2024.3353855","National Science Foundation of China(grant numbers:62125206,U20A20173,62202133); National Key Research and Development Program of China(grant numbers:2022YFB4500100); Key Research Project of Zhejiang Province(grant numbers:2022C01145); Zhejiang Provincial National Science Foundation of China(grant numbers:LY23F020015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10433234","Cloud-native applications;microservice;research roadmap;service life-cycle management;survey","Cloud computing;Microservice architectures;Surveys;Software engineering;Computer architecture;Service level agreements;Research initiatives;Life cycle assessment;Modeling","","9","","299","IEEE","12 Feb 2024","","","IEEE","IEEE Journals"
"NeutRAN: An Open RAN Neutral Host Architecture for Zero-Touch RAN and Spectrum Sharing","L. Bonati; M. Polese; S. D’Oro; S. Basagni; T. Melodia","Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA",IEEE Transactions on Mobile Computing,"4 Apr 2024","2024","23","5","5786","5798","Obtaining access to exclusive spectrum, cell sites, Radio Access Network (RAN) equipment, and edge infrastructure imposes major capital expenses to mobile network operators. A neutral host infrastructure, by which a third-party company provides RAN services to mobile operators through network virtualization and slicing techniques, is seen as a promising solution to decrease these costs. Currently, however, neutral host providers lack automated and virtualized pipelines for onboarding new tenants and to provide elastic and on-demand allocation of resources matching operators’ requirements. To address this gap, this paper presents NeutRAN, a zero-touch framework based on the O-RAN architecture to support applications on neutral hosts and automatic operator onboarding. NeutRAN builds upon two key components: (i) an optimization engine to guarantee coverage and to meet quality of service requirements while accounting for the limited amount of shared spectrum and RAN nodes, and (ii) a fully virtualized and automated infrastructure that converts the output of the optimization engine into deployable micro-services to be executed at RAN nodes and cell sites. NeutRAN was prototyped on an OpenShift cluster and on a programmable testbed with 4 base stations and 10 users from 3 different tenants. We evaluate its benefits, comparing it to a traditional license-based RAN where each tenant has dedicated physical and spectrum resources. We show that NeutRAN can deploy a fully operational neutral host-based cellular network in around 10 seconds. Experimental results show that it increases the cumulative network throughput by 2.18× and the per-user average throughput by 1.73× in networks with shared spectrum blocks of 30 MHz. NeutRAN provides a 1.77× cumulative throughput gain even when it can only operate on a shared spectrum block of 10 MHz (one third of the spectrum used in license-based RANs).","1558-0660","","10.1109/TMC.2023.3311728","National Science Foundation(grant numbers:CNS-1925601,CNS-2112471,CNS-1923789); Office of Naval Research(grant numbers:N00014-20-1-2132); Army Research Laboratory Cooperative(grant numbers:W911NF-19-2-0221); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10238780","O-RAN;open RAN;neutral host;automation;5G;6G","Computer architecture;Optimization;Engines;Resource management;Microprocessors;Throughput;Pipelines","","9","","63","IEEE","4 Sep 2023","","","IEEE","IEEE Journals"
"Opportunistic Federation of CubeSat Constellations: A Game-Changing Paradigm Enabling Enhanced IoT Services in the Sky","G. Araniti; A. Iera; A. Molinaro; S. Pizzi; F. Rinaldi","Department DIIES, University Mediterranea, Reggio Calabria, Italy; National Inter-University Consortium for Telecommunications (CNIT), Napoli, Italy; Department DIIES, University Mediterranea, Reggio Calabria, Italy; Department DIIES, University Mediterranea, Reggio Calabria, Italy; Department DIIES, University Mediterranea, Reggio Calabria, Italy",IEEE Internet of Things Journal,"8 Aug 2022","2022","9","16","14876","14890","Internet of Space Things (IoST) is a challenging paradigm, which is currently attracting great interest from the scientific and industrial communities. IoST is based on the integration of the space segment into the global Internet-of-Things (IoT) infrastructure. In the relevant literature, reference is generally made to multiple constellations of nanosatellite platforms, used to enable IoT services on a global scale, including also disadvantaged and poorly infrastructured areas. In this article, we focus on multitenant IoT scenarios, wherein multiple CubeSats constellations are enabled to offer services by exploiting a dynamic federation model. The objective is to efficiently provide services in an IoST scenario by leveraging an effective cooperation strategy originally designed for terrestrial IoT networks, the Mobile-IoT-Federation-as-a-Service (MIFaaS) paradigm. We extend this vision to IoT satellite networks in order to allow a constellation of satellites to effectively execute tasks through a tight cooperative behavior with other CubeSats constellations. The reported performance evaluation studies show that better performance, in terms of percentage of tasks successfully completed, can be achieved through the implementation of the proposed cooperation paradigm.","2327-4662","","10.1109/JIOT.2021.3115160","RUDN University Strategic Academic Leadership Program and PM3 Project (Italian Ministry of University and Research PON Grant)(grant numbers:ARS01_01181); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9547348","CubeSats;federation;Internet of Space Things (IoST);Internet of Things (IoT);multitenancy;virtualization","Internet of Things;CubeSat;Satellite broadcasting;5G mobile communication;Space vehicles;Low earth orbit satellites;Orbits","","8","","61","IEEE","24 Sep 2021","","","IEEE","IEEE Journals"
"Toward Network-Slicing-Enabled Edge Computing: A Cloud-Native Approach for Slice Mobility","S. D. A. Shah; M. A. Gregory; S. Li","School of Engineering and Information Technology, The University of New South Wales Canberra, Australian Defence Force Academy, Canberra, ACT, Australia; School of Engineering, RMIT University, Melbourne, VIC, Australia; School of Engineering, RMIT University, Melbourne, VIC, Australia",IEEE Internet of Things Journal,"8 Jan 2024","2024","11","2","2684","2700","Network slicing is a key enabler for 5G and beyond networks that permits operators to provide scalable, flexible, and dedicated networks over a common physical infrastructure. To cope with the rising demand for ultrareliable and low-latency communication (URLLC) in beyond 5G networks, the provision of dedicated secure networks closer to the users is essential. Multiaccess edge computing (MEC) is a promising technology that provides data and computational resources closer to mobile users. However, MEC servers are resource-constrained, and offering dedicated service-specific network slices at the edge in a highly dynamic and mobile environment is challenging. Network slicing and MEC are being evolved by two different standardization bodies that limit their integration and raise mobility challenges that deserve more attention. We propose a cloud-native microservices architecture for network slice mobility management in MEC that permits each MEC slice to be distributed as stateless and independently deployable microservices. The proposal separates the MEC slice operational data and the user context, as each network function in a MEC slice stores the context in a separate shared database. The proposed architecture leverages new SDN extended federation modules in compliance with the ETSI requirements for inter-MEC system coordination. The federation modules support a more flexible and scalable creation of network slices at MEC servers, efficient resource utilization, and mobility of network slices across MEC servers. The simulation results show that our proposed architecture outperforms the existing SDN-based approaches for network slicing in MEC by achieving high slice acceptance rates and reduced slice migration delay.","2327-4662","","10.1109/JIOT.2023.3292520","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10173679","5G;cloud-native;microservices;multiaccess edge computing (MEC);network slice mobility (NSM);ultrareliable and low-latency communication (URLLC);vehicle-toeverything (V2X)","Network slicing;Servers;Microservice architectures;Cloud computing;Vehicle dynamics;Internet of Things;Vehicle-to-everything","","8","","50","IEEE","5 Jul 2023","","","IEEE","IEEE Journals"
"A Survey on Security, Privacy, Trust, and Architectural Challenges in IoT Systems","M. Adam; M. Hammoudeh; R. Alrawashdeh; B. Alsulaimy","Computer Engineering Department, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia; Information and Computer Science Department, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia; Information and Computer Science Department, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia; Information and Computer Science Department, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia",IEEE Access,"25 Apr 2024","2024","12","","57128","57149","The Internet of Things (IoT) emerged as a pervasive technology, facilitating the seamless interaction of devices, individuals, and services, enabling data exchange and task execution across various domains. While the impact of IoT is undeniably transformative, its extensive proliferation raised significant concerns surrounding security, privacy, and trust, which stand as critical barriers to the widespread adoption and advancement of IoT technology. This review article explores IoT security, privacy, and trust research using a 3-layer IoT architecture. After introducing the fundamental tenets of IoT security, privacy, and trust, it proceeds to examine the prevalent security requirements within IoT architectures and their associated challenges. Then, the survey investigates the recent trends in research dedicated to addressing security, privacy, and trust issues within IoT systems. Furthermore, this article reviews the latest advancements and methodologies designed to secure IoT systems against security breaches and protect the privacy of sensitive data. Finally, the survey outlines unresolved challenges within the IoT security landscape and potential solutions. By offering this consolidated insight, this article offers a bridge between foundational and advanced IoT security topics, providing researchers with an in-depth understanding of current IoT security, privacy, and trust challenges, as well as cutting-edge solutions tailored to address the security and trust-related obstacles faced by IoT applications. In addition, it provides the IoT community with the knowledge necessary to navigate the complex terrain of security, privacy, and trust in IoT systems.","2169-3536","","10.1109/ACCESS.2024.3382709","Deanship of Scientific Research (DSR) at the King Fahd University of Petroleum and Minerals; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10483048","Countermeasures;IDS;IoT architecture;IoT attacks;mitigation;privacy;security;security requirements;trust","Internet of Things;Security;Privacy;Surveys;Computer architecture;Reviews;Blockchains;Trust management","","8","","124","CCBY","28 Mar 2024","","","IEEE","IEEE Journals"
"Predictive Cyber Foraging for Visual Cloud Computing in Large-Scale IoT Systems","J. Patman; D. Chemodanov; P. Calyam; K. Palaniappan; C. Sterle; M. Boccia","Department of Electrical Engineering and Computer Science, University of Missouri, Columbia, MO, USA; Department of Electrical Engineering and Computer Science, University of Missouri, Columbia, MO, USA; Department of Electrical Engineering and Computer Science, University of Missouri, Columbia, MO, USA; Department of Electrical Engineering and Computer Science, University of Missouri, Columbia, MO, USA; Department of Electrical Engineering and Information Technology, University of Naples Federico II, Naples, Italy; Department of Electrical Engineering and Information Technology, University of Naples Federico II, Naples, Italy",IEEE Transactions on Network and Service Management,"9 Dec 2020","2020","17","4","2380","2395","Cyber foraging has been shown to be especially effective for augmenting low-power Internet-of-Thing (IoT) devices by offloading video processing tasks to nearby edge/cloud computing servers. Factors such as dynamic network conditions, concurrent user access, and limited resource availability, cause offloading decisions that negatively impact overall processing throughput and end-user delays. Moreover, edge/cloud platforms currently offer both Virtual Machine (VM) and serverless computing pricing models, but many existing edge offloading approaches only investigate single VM-based offloading performance. In this paper, we propose a predictive (NP-complete) scheduling-based offloading framework and a heuristic-based counterpart that use machine learning to dynamically decide what combinations of functions or single VM needs to be deployed so that tasks can be efficiently scheduled. We collected over 10,000 network and device traces in a series of realistic experiments relating to a protest crowds incident management application. We then evaluated the practicality of our predictive cyber foraging approach using trace-driven simulations for up to 1000 devices. Our results indicate that predicting single VM offloading costs: (a) leads to near-optimal scheduling in 70% of the cases for service function chaining, and (b) offers a 40% gain in performance over traditional baseline estimation techniques that rely on simple statistics for estimations in the case of single VM-offloading. Considering a series of visual computing offloading scenarios, we also validate our approach benefits of using online versus offline machine learning models for predicting offloading delays.","1932-4537","","10.1109/TNSM.2020.3010497","National Science Foundation(grant numbers:CNS-1647182,CNS-1359125); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9144518","Computation offloading;cyber foraging;edge/cloud computing;machine learning;online job scheduling","Computational modeling;Edge computing;Predictive models;Performance evaluation;Servers;Task analysis;Estimation","","8","","60","IEEE","20 Jul 2020","","","IEEE","IEEE Journals"
"Techniques to Improve Reliability in an IoT Architecture Framework for Intelligent Products","C. M. Coman; G. D’amico; A. V. Coman; A. Florescu","Tesagon International SRL, Ploiesti, Romania; Focus Innovazione srl, Fasano, Italy; RID International Center SRL, Targu Mures, Romania; Faculty of Electronics, Telecommunications and Information Technology, Politehnica University of Bucharest, Bucuresti, Romania",IEEE Access,"16 Apr 2021","2021","9","","56940","56954","Sensor-Cloud Systems (SCS) are designed to link sensor networks and cloud applications, in order to gather and process information. While this is a hot topic for both academia and industry, with a large number of implementations, not much effort was put towards analysing the reliability of these systems. This article presents an experimental implementation of a system in the field of intelligent products and explores reliability improving techniques in five main areas of the SCS: network communication performance, auto recovery, local backup, automated software testing and system security. They all play an important role in determining the level of reliability of the novel Internet of Things (IoT) architecture framework for intelligent products presented in the article. A new formula is proposed for assessing the reliability of a SCS based on metrics from each of the five areas. Metrics used to assess the system reliability are presented, along with comparisons between operating with improvement techniques and without them. The results show that the reliability of the implemented SCS is improved considerably by implementing a deliberate reliability policy using an original five level tiered approach.","2169-3536","","10.1109/ACCESS.2021.3072168","MANUNET through the Executive Unit for Financing Higher Education, Research, Development and Innovation (UEFISCDI)(grant numbers:COFUND-ERANET MANUNET III-POKET); Apulia Region POR FESR-FSE 2014-2020 - Priority Axis 1 - Research, Technological Development, Innovation - Action 1.6 (Interventions for Strengthening the Regional and National Innovation System and Increasing Collaboration Between Companies and Research Structures and Their Enhancement)(grant numbers:MNET18/ICT-3415); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9399420","Knowledge based systems;Internet of Things;reliability;sensor systems;sustainable development","Reliability;Cloud computing;Wireless sensor networks;Security;Software reliability;Monitoring;Media","","8","","49","CCBYNCND","9 Apr 2021","","","IEEE","IEEE Journals"
"Toward Beneficial Transformation of Enterprise Workloads to Hybrid Clouds","J. Hwang","IBM T. J. Watson Research Center, Yorktown Heights, NY, USA",IEEE Transactions on Network and Service Management,"9 Jun 2016","2016","13","2","295","307","With the promise of providing flexible and elastic computing resources on demand, the cloud computing has been attracting enterprises and individuals to migrate workloads in the legacy environment to the public/private/hybrid clouds. However, the workload migration is often interpreted as an image migration or re-installation/data copying as the exact snapshot of the source machine, and the various cloud platforms and service models are rarely taken into consideration during migration planning. Thus, the cloud migration techniques have not provided enough options that can satisfy the various migration requirements. In this paper, we propose a model to tackle the migration challenges that transforms one resource into the same or another resource in hybrid clouds. We formulate the problem as a constraint satisfaction problem, and iteratively decompose the server components and consolidate the servers. Furthermore, we propose a compute-network mapping algorithm to match computing resources with network resources to guarantee network affinity. The ultimate goal is to recommend the optimal target cloud platform with network affinity and the minimum cost. Through the evaluation of the proposed model using real enterprise datasets (up to 2012 machines), we prove that the proposed model satisfies the goal. We show that when migrating into virtualized cloud environments, thorough resource planning can reduce 16% of current resources, 5%-10% servers can be consolidated, and more than 60% servers are possible candidates for server decomposition.","1932-4537","","10.1109/TNSM.2016.2541120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7430340","Cloud Transformation;Cloud Migration;Cloud Analytics;Hybrid Cloud;Cloud Transformation;Cloud Migration;Cloud Analytics;Hybrid Cloud","Servers;Cloud computing;Planning;Measurement;Monitoring;Computational modeling","","8","","50","IEEE","10 Mar 2016","","","IEEE","IEEE Journals"
"X-MAN: A Non-Intrusive Power Manager for Energy-Adaptive Cloud-Native Network Functions","Z. Xiang; M. Höweler; D. You; M. Reisslein; F. H. P. Fitzek","Deutsche Telecom Chair of Communication Networks, Technische Universität Dresden, Dresden, Germany; Deutsche Telecom Chair of Communication Networks, Technische Universität Dresden, Dresden, Germany; Deutsche Telecom Chair of Communication Networks, Technische Universität Dresden, Dresden, Germany; School of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ, USA; Deutsche Telecom Chair of Communication Networks, Technische Universität Dresden, Dresden, Germany",IEEE Transactions on Network and Service Management,"9 Jun 2022","2022","19","2","1017","1035","Emerging microservices demand flexible low-latency processing of network functions in virtualized environments, e.g., as containerized network functions (CNFs). While ensuring highly responsive low-latency CNF processing, the computing environments should conserve energy to reduce costs. In this systems integration study, we develop and evaluate the novel XDP-Monitoring Energy-Adaptive Network Functions (X-MAN) framework for managing the CPU operational states (P-states) so as to reduce the power consumption while prioritizing low-latency service. Architecturally, X-MAN consists of lightweight traffic monitors that are attached to the virtual network interfaces in the kernel space for per-CNF traffic monitoring and a power manager in user space with a global view of the CNFs on a CPU core. Algorithmically, X-MAN monitors the CPU core utilization via hybrid simple and weighted moving average prediction fed by the traffic monitors and a power management based on step-based CPU core frequency (P-state) adjustments. We evaluate X-MAN through extensive measurements in a real physical testbed operating at up to 10 Gbps. We find that X-MAN incurs significantly shorter and more consistent monitoring latencies for the CPU utilization than a state-of-the-art CPU hardware counter approach. Also, X-MAN achieves more responsive CPU core frequency adjustments and more pronounced reductions of the CPU power consumption than a state-of-the-art code instrumentation approach. We make the X-MAN source code publicly available.","1932-4537","","10.1109/TNSM.2021.3126822","German Research Foundation (DFG, Deutsche Forschungsgemeinschaft) as part of Germany’s Excellence Strategy – EXC 2050/1 – Cluster of Excellence “Centre for Tactile Internet with Human-in-the-Loop” (CeTI) of Technische Universität Dresden(grant numbers:390696704); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9610120","Containerized network function (CNF);CPU P-state;express data path (XDP);load monitoring;microservice;network testbed;power management","Monitoring;Power system management;Codes;Central Processing Unit;Hardware;Instruments;Kernel","","8","","112","IEEE","10 Nov 2021","","","IEEE","IEEE Journals"
"Secure IoT Search Engine: Survey, Challenges Issues, Case Study, and Future Research Direction","W. G. Hatcher; C. Qian; F. Liang; W. Liao; E. P. Blasch; W. Yu","Department of Computer and Information Science, Towson University, Towson, MD, USA; Department of Computer and Information Science, Towson University, Towson, MD, USA; Department of Computer Science, Sam Huston State University, Huntsville, TX, USA; Department of Computer and Information Science, Towson University, Towson, MD, USA; Air Force Research Laboratory, Arlington, DC, USA; Department of Computer and Information Science, Towson University, Towson, MD, USA",IEEE Internet of Things Journal,"7 Sep 2022","2022","9","18","16807","16823","The Internet of Things (IoT) encompasses a near-incalculable collection of dispersed and embedded computing devices acting as sensors and actuators, generating data at an incredible scale. However, a lack of coherency and cross-compatibility in IoT deployments has lead to increasing redundancy and waste of resources. To combat this, various concepts have been proposed for an open IoT search engine (IoT-SE) that serves human and machine users. Invariably, the IoT-SE envisions distributed query retrieval to handle massive volumes of devices and data. Incorporating the massively heterogeneous protocols and properties of devices deployed, the search of such a system for timely and pertinent data is massively challenging, to provide useful knowledge and service for IoT systems. Moreover, enabling and maintaining security and privacy in an IoT-SE is likewise a prodigious task, as end users, IoT devices, and the search system itself, have different protocols and requirements. To this end, a study of security issues in IoT search is conducted to outline the challenges ahead, and a case study to resolve practical security vulnerabilities in an IoT-SE system is carried out. The pertinent issues of security in an IoT-SE system are reviewed. Particularly: 1) a taxonomy is detailed for IoT-SE security issues; 2) the vulnerabilities of machine learning (ML) models in the IoT-SE are considered; and 3) defensive mechanisms are presented for securing IoT Search. A case study is carried out to implement basic security features in the IoT search, addressing the risk of false queries through the design of ML-based solutions. Finally, a roadmap for future research is provided, including the security and privacy for IoT systems connected to the IoT-SE, distributed edge computing in IoT-SE, privacy-preserving data markets in IoT-SE, and distributed ML in IoT-SE.","2327-4662","","10.1109/JIOT.2022.3153171","Air Force Office of Scientific Research(grant numbers:FA9550-20-1-0418); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9718521","Internet of Things (IoT);knowledge and service;search engine;security","Security;Internet of Things;Logic gates;Search engines;Smart homes;Taxonomy;Real-time systems","","8","","94","IEEE","23 Feb 2022","","","IEEE","IEEE Journals"
"An End-to-End Implementation of a Service-Oriented Architecture for Data-Driven Smart Buildings","L. Chamari; E. Petrova; P. Pauwels","Department of the Built Environment, Eindhoven University of Technology, Eindhoven, The Netherlands; Department of the Built Environment, Eindhoven University of Technology, Eindhoven, The Netherlands; Department of the Built Environment, Eindhoven University of Technology, Eindhoven, The Netherlands",IEEE Access,"30 Oct 2023","2023","11","","117261","117281","Buildings connect with multiple information systems like Building Management Systems (BMS), Energy Management Systems (EMS), IoT devices, Building Information Models (BIM), the electricity grid, weather services, etc. Data-driven smart building software demands seamless integration of the above systems and their data. The lack of a system architecture with well-defined Application Programming Interfaces (APIs) poses a significant challenge for developing reusable, modular and scalable applications. This article presents a service-oriented system architecture designed with data-driven smart buildings in mind. The architecture relies on the Zachman framework and consists of seven service categories: 1) existing business applications, 2) new microservice-based applications, 3) databases, 4) integration software, 5) infrastructure services, 6) shared services, and 7) user interfaces. It closely resembles the MACH architectural principles: Microservices, API-first, Cloud-based components, and Headless principles. This architecture is implemented as a proof-of-concept, including three smart building applications. These include a Digital Twin application integrating sensor data with a BIM model, a web application merging real-time sensor data with semantic building graphs, and a data exploration tool using sensor data, the Brick ontology, and Grafana dashboards. Future implementations include real-time control applications such as Model Predictive Control (MPC). The proposed architecture and its implementations provide a blueprint for a reusable, modular, and scalable architecture in the smart building domain.","2169-3536","","10.1109/ACCESS.2023.3325767","Dutch Ministry of Economic Affairs and Climate Policy and the Ministry of the Interior and Kingdom Relations; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10287934","Building Information Modeling;data-driven buildings;data integration;IoT;linked building data;microservices;reference architecture;REST API;real-time data acquisition;solution architecture","Computer architecture;Architecture;Buildings;Smart buildings;Business;Data models;Systems architecture;Data integration;Internet of Things;Real-time systems","","8","","102","CCBY","19 Oct 2023","","","IEEE","IEEE Journals"
"Provisioning Computational Resources for Cloud-Based e-Learning Platforms Using Deep Learning Techniques","J. Ariza; M. Jimeno; R. Villanueva-Polanco; J. Capacho","Department of Computer Science and Engineering, Universidad del Norte, Barranquilla, Colombia; Department of Computer Science and Engineering, Universidad del Norte, Barranquilla, Colombia; Department of Computer Science and Engineering, Universidad del Norte, Barranquilla, Colombia; Department of Computer Science and Engineering, Universidad del Norte, Barranquilla, Colombia",IEEE Access,"29 Jun 2021","2021","9","","89798","89811","The use of e-learning technologies is growing even faster due to the existing conditions where virtual setups temporarily replace traditional classroom environments. Service infrastructure support for e-learning has moved to the cloud. For this reason, the efficient provisioning of resources for such platforms, which is achieved through prediction, is very relevant. The existing techniques for predicting the use of resources in the cloud are not designed with e-learning’s specific requirements. This paper presents a neural network-based model for predicting the usage of computational resources for e-learning platforms. This model consists of a series of interconnected neural networks used to predict values for variables of interest, such as Random Access Memory (RAM) usage and Central Processing Unit (CPU) usage. Using data collected from a high school real scenario, we analyzed and used it to train and validate our neural network-based model. This scenario consisted of a Moodle server deployed in a Google Virtual Machine with a configured course and its contents. Each student performed a series of activities while connected to it. Our proposed model achieves high accuracy. The obtained results are promising, paving the way towards constructing software tools for provisioning computational resources on demand for e-learning platforms.","2169-3536","","10.1109/ACCESS.2021.3090366","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9459736","Availability;cloud-based e-learning platforms;resource consumption;prediction;deep learning","Cloud computing;Electronic learning;Monitoring;Prediction algorithms;Computational modeling;Virtual machining;Predictive models","","8","","58","CCBY","18 Jun 2021","","","IEEE","IEEE Journals"
"Design Methodology for 6G End-to-End System: Hexa-X-II Perspective","S. Kerboeuf; P. Porambage; A. Jain; P. Rugeland; G. Wikström; M. Ericson; D. Thai Bui; A. Outtagarts; H. Karvonen; P. Alemany; R. Muñoz; R. Vilalta; P. Botsinis; A. Ramos; J. Castaneda Cisneros; M. Karaca; C. Karousatou; S. Barmpounakis; P. Demestichas; A. Zafeiropoulos; I. Tzanettis; S. Papavassiliou; P. G. Giardina; G. Landi; B. Han; A. Nimr; M. A. Uusitalo","Nokia Bell Labs, Nokia Networks France, Massy, France; VTT Technical Research Centre of Finland, Espoo, Finland; Nokia Bell Labs, Espoo, Finland; Ericsson Research, Stockholm, Sweden; Ericsson Research, Stockholm, Sweden; Ericsson Research, Stockholm, Sweden; Nokia Bell Labs, Nokia Networks France, Massy, France; Nokia Bell Labs, Nokia Networks France, Massy, France; Siemens Industry Software Oy, Siemens EDA, Oulu, Finland; Centre Tecnològic de Telecomunicacions de Catalunya (CTTC/CERCA), Barcelona, Spain; Centre Tecnològic de Telecomunicacions de Catalunya (CTTC/CERCA), Barcelona, Spain; Centre Tecnològic de Telecomunicacions de Catalunya (CTTC/CERCA), Barcelona, Spain; Apple Technology Engineering, Munich, Germany; ATOS-EVIDEN, Madrid, Spain; Ericsson Research, Stockholm, Sweden; Ericsson Research, Istanbul, Turkey; WINGS ICT Solutions, Athens, Greece; WINGS ICT Solutions, Athens, Greece; WINGS ICT Solutions, Athens, Greece; Institute of Communication and Computer Systems, Athens, Greece; Institute of Communication and Computer Systems, Athens, Greece; Institute of Communication and Computer Systems, Athens, Greece; Research & Developmen, Nextworks, Pisa, Italy; Research & Developmen, Nextworks, Pisa, Italy; Division of Wireless Communications and Radio Positioning (WICON), University of Kaiserslautern (RPTU), Kaiserslautern, Germany; Vodafone Chair for Mobile Communications Systems, Technische Universität Dresden, Dresden, Germany; Nokia Bell Labs, Espoo, Finland",IEEE Open Journal of the Communications Society,"11 Jun 2024","2024","5","","3368","3394","As the fifth generation (5G) mobile communication systems are commercially deployed, they bring new services, enhance user experiences, and offer various opportunities to different industries. Despite its advancements, 5G encounters several challenges. To tackle these issues, global industrial, academic, and standards organizations are actively researching on sixth generation (6G) wireless communication systems. 6G networks are envisioned as a transformative shift in the interactions between the physical, digital, and human realms, paving the way for a pervasive human-centered cyber-physical world. 6G is positioned to be a platform that offers communication and beyond communication services considering both performance and value centric technological development approaches. In this paper, we present the way forward towards the design of 6G endto-end (E2E) system as a consolidated view of leading industry stakeholders and academia in the European level 6G flagship project Hexa-X-II. We discuss the key challenges with 5G and how 6G is expected to tackle those by introducing new technological innovations and supporting novel use case requirements. We provide a comprehensive methodology for the design of a 6G E2E system including ten principles, a blueprint, and a structured design process. The architecture design principles prioritize environmental sustainability, digital inclusiveness, and trustworthiness, considering their impact on the 6G E2E system. The blueprint is described corresponding to the infrastructure, network centric application, and application layers, as well as the pervasive functionalities and the relevant technological innovations. Following the design principles and the system blueprint, the design process is demonstrated as two-way approaches (i.e., 1) key performance and value indicators based design process. 2) top-down versus bottom-up alignment process) through the lens of a collaborative robot use case. Through this use case, a special attention is given to the technological enablers that cover management and orchestration functionalities and their 6G enhancement to go beyond the limitations characterizing the previous network generation. In addition, virtual modelling aspects related to digital twining and simulations for 6G E2E system design are also discussed.","2644-125X","","10.1109/OJCOMS.2024.3398504","Hexa-X-II Project; Smart Networks and Services Joint Undertaking (SNS JU) through the European Union’s Horizon Europe Research and Innovation Programme(grant numbers:101095759); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10525242","Keywords6G;end-to-end;platform;system;blueprint;network architecture;design process;use case;key value indicators;key performance indicators;collaborative robots","6G mobile communication;5G mobile communication;Sustainable development;Industries;Ecosystems;Collaborative robots;Europe","","8","","103","CCBYNCND","8 May 2024","","","IEEE","IEEE Journals"
"Edge Computing and Microservices Middleware for Home Energy Management Systems","L. C. B. C. Ferreira; A. D. R. Borchardt; G. D. S. Cardoso; D. A. Mendes Lemes; G. R. D. R. d. Sousa; F. B. Neto; E. R. de Lima; G. Fraidenraich; P. Cardieri; L. G. P. Meloni","Department of Communications, School of Electrical and Computer Engineering, University of Campinas (FEEC), São Paulo, Campinas, Brazil; Department of Communications, School of Electrical and Computer Engineering, University of Campinas (FEEC), São Paulo, Campinas, Brazil; Department of Communications, School of Electrical and Computer Engineering, University of Campinas (FEEC), São Paulo, Campinas, Brazil; Department of Communications, School of Electrical and Computer Engineering, University of Campinas (FEEC), São Paulo, Campinas, Brazil; Department of Communications, School of Electrical and Computer Engineering, University of Campinas (FEEC), São Paulo, Campinas, Brazil; Companhia Paranaense de Energia, Curitiba, Parana, Brazil; Department of Hardware Design, Instituto de Pesquisa Eldorado, São Paulo, Campinas, Brazil; Department of Communications, School of Electrical and Computer Engineering, University of Campinas (FEEC), São Paulo, Campinas, Brazil; Department of Communications, School of Electrical and Computer Engineering, University of Campinas (FEEC), São Paulo, Campinas, Brazil; Department of Communications, School of Electrical and Computer Engineering, University of Campinas (FEEC), São Paulo, Campinas, Brazil",IEEE Access,"21 Oct 2022","2022","10","","109663","109676","A middleware software can be seem as an abstraction layer between hardware and user applications, that facilitates the development and deployment of services in various scenarios, such as those found in Home Energy Management Systems (HEMS). There are several middleware proposals for HEMS, with most of them taking the cloud computing approach. This approach is unconcerned about computing resources but raises a dependency on external connections. This paper presents a middleware for energy management systems, based on the concept of edge computing for smart homes. The paper presents a reference model for the proposed architecture, considering specific requirements for this type of application. The proposed architecture employs the concept of microservices for data access and system configuration. The proposed middleware is designed to work with embedded systems under computational constraints, such as processing capability and storage, to reduce costs and allow its application closer to the user. The middleware is open and customizable to meet the developer’s needs. The proposed solution was implemented and tested in a university laboratory, as well as at the Eldorado Research Institute to confirm the effectiveness of the middleware. The proposal stands out from others found in the literature as it can be implemented using low cost hardware. In addition to using microservices concepts, the proposed middleware is a valuable option for applications that need an edge computing approach. A performance analysis was carried out, using low cost hardware with limited resources. The results show that the proposal can handle a significant number of devices, offering low latency and low error rate, and consuming few processing resources and memory.","2169-3536","","10.1109/ACCESS.2022.3214229","Companhia Paranaense de Energia (COPEL) through the Research and Development Project ANEEL PD-02866-0508/2019; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9917529","Home energy management systems;middleware;Internet of Things","Middleware;Internet of Things;Hardware;Edge computing;Computer architecture;Microservice architectures;Energy management systems;Computer applications","","8","","33","CCBY","13 Oct 2022","","","IEEE","IEEE Journals"
"Bayesian Inference-Assisted Machine Learning for Near Real-Time Jamming Detection and Classification in 5G New Radio (NR)","S. Jere; Y. Wang; I. Aryendu; S. Dayekh; L. Liu","Bradley Department of ECE, Wireless@Virgnia Tech, Virginia Tech, Blacksburg, VA, USA; School of Systems and Enterprises, Stevens Institute of Technology, Hoboken, NJ, USA; School of Systems and Enterprises, Stevens Institute of Technology, Hoboken, NJ, USA; 5G, Advanced Connectivity and Edge Cyber, Deloitte & Touche LLP, Dallas, TX, USA; Bradley Department of ECE, Wireless@Virgnia Tech, Virginia Tech, Blacksburg, VA, USA",IEEE Transactions on Wireless Communications,"11 Jul 2024","2024","23","7","7043","7059","The increased flexibility and density of spectrum access in 5G New Radio (NR) has made jamming detection and classification a critical research area. To detect coexisting jamming and subtle interference, we introduce a Bayesian Inference-assisted machine learning (ML) methodology. Our methodology uses cross-layer Key Performance Indicator data collected on a Non-Standalone (NSA) 5G NR testbed to leverage supervised learning models, and is further assessed, calibrated, and revealed using Bayesian Network Model (BNM)-based inference. The models can operate on both instantaneous and sequential time-series data samples, achieving an Area under Curve above 0.954 for instantaneous models and above 0.988 for sequential models including the echo state network (ESN) from the Reservoir Computing (RC) family, across various jamming scenarios. The 180 ms instantaneous detection time allows for continuous tracking of the dynamic jamming condition due to UE mobility. Our approach serves as a validation method and a resilience enhancement tool for ML-based jamming detection while also enabling root cause identification for observed performance degradation. The introduced BNM-based inference proof-of-concept is successful in addressing 72.2% of the erroneous predictions of the RC-based sequential detection model caused by insufficient training data samples, thereby demonstrating its near real-time applicability in 5G NR and Beyond-5G networks.","1558-2248","","10.1109/TWC.2023.3337058","Cyber 5G Strategic Growth Offering of Deloitte & Touche LLP, Commonwealth Cyber Initiative (CCI) Southwest Node, and US National Science Foundation(grant numbers:CCF-1937487); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10352089","Jamming;interference;network intrusion;5G NR;O-RAN;near real-time;machine learning;reservoir computing;Bayesian network model;causal analysis and inference","Jamming;5G mobile communication;Interference;Real-time systems;Bayes methods;Computational modeling;Wireless communication","","8","","53","IEEE","8 Dec 2023","","","IEEE","IEEE Journals"
"I2BN: Intelligent Intent Based Networks","P. Szilágyi","Nokia Bell Labs, Budapest, Hungary",Journal of ICT Standardization,"22 Sep 2023","2021","9","2","159","200","Intent based network management reduces the complexity of network programming from a growing set of deeply technical APIs to context-free high-level objectives that the network should autonomously achieve and keep. The practical implementation of an intent based network requires substantial automation technology embedded in the network. Automation should cover the entire lifecycle of intents, from their ingestion to fulfillment and assurance. This article investigates the feasibility of automatically assembling interworking implementation units into intent specific automation pipelines, where units are reusable self-learning closed loop micro-services with self-declared capabilities. Each closed loop may gain knowledge and respond to dynamically changing network conditions, thereby enabling network autonomy in reaching the declared intent objectives. The human-network intent interface for expressing intents is proposed to be based on the aggregation of the deployed network and service automation capabilities, rather than a formalism decoupled from the actual network implementation. This principle removes the ambiguity and compatibility gap between human intent definition and machine intent fulfillment, while retaining the flexibility and extendibility of the intents offered by any specific system via onboarding additional micro-services with novel capabilities. The concepts discussed by the article fit into the architecture and closed loop work items already defined by ETSI ZSM and provides considerations towards new areas such as intent driven autonomous networks and enablers for automation.","2246-0853","","10.13052/jicts2245-800X.926","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255479","Intent;I2BN;network and service automation;network intelligence;ZSM","Autonomous networks;Pipelines;Memory management;Standardization;Machine learning;Programming;Data models","","8","","24","","22 Sep 2023","","","River Publishers","River Publishers Journals"
"Knowledge-Driven Industrial Intelligent System: Concept, Reference Model, and Application Direction","P. Z. H. Sun; Y. Bao; X. Ming; T. Zhou","Department of Industrial Engineering, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Industrial Engineering, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Industrial Engineering, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Industrial Engineering, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China",IEEE Transactions on Computational Social Systems,"2 Aug 2023","2023","10","4","1465","1478","The application of automation technology and artificial intelligence technology has promoted the improvement of the business capabilities of enterprises in industrial scenarios. Compared with the improvement or innovation of the business process, in recent years, part of academic research and practical applications has also shifted their attention from a single point of business intelligence perspective to a comprehensive intelligent upgrade of the industrial system. To the best of our knowledge, however, there is little research on the concept and model of the industrial intelligent system (IIS). To make up for the lack, this article presents the concept and reference model of IIS by analyzing the intelligentization requirement of the industrial systems. Different from academic research on general intelligent system capabilities, the reference model given emphasizes factors that need to be considered when implementing IIS in the industry. By analyzing the reference model, knowledge as the core driving force of IIS is recognized. Then, the four main forms of knowledge in IIS, as well as the role and key technologies of knowledge in different stages of IIS, are discussed in detail. In addition, several important potential applications of IIS are pointed out in this article.","2329-924X","","10.1109/TCSS.2022.3188295","National Natural Science Foundation of China(grant numbers:71632008); National Natural Science Foundation of China through the Main Research Project on Machine Behavior and Human–Machine Collaborated Decision Making Methodology(grant numbers:72192820); Third Research Project on Human Behavior in Human–Machine Collaboration(grant numbers:72192822); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9829421","Industrial intelligent system (IIS);industry intelligentization;knowledge flow;reference model","Business;Information systems;Knowledge based systems;Knowledge acquisition;Intelligent systems;Industries;Decision making","","8","","152","IEEE","13 Jul 2022","","","IEEE","IEEE Journals"
"Joint Resource Dimensioning and Placement for Dependable Virtualized Services in Mobile Edge Clouds","P. Zhao; G. Dán","Division of Network and Systems Engineering, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; Division of Network and Systems Engineering, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden",IEEE Transactions on Mobile Computing,"31 Aug 2022","2022","21","10","3656","3669","Mobile edge computing (MEC) is an emerging architecture for accommodating latency sensitive virtualized services (VSs). Many of these VSs are expected to be safety critical, and will have some form of reliability requirements. In order to support provisioning reliability to such VSs in MEC in an efficient and confidentiality preserving manner, in this paper we consider the joint resource dimensioning and placement problem for VSs with diverse reliability requirements, with the objective of minimizing the energy consumption. We formulate the problem as an integer programming problem, and prove that it is NP-hard. We propose a two-step approximation algorithm with bounded approximation ratio based on Lagrangian relaxation. We benchmark our algorithm against two greedy algorithms in realistic scenarios. The results show that the proposed solution is computationally efficient, scalable and can provide up to 30 percent reduction in energy consumption compared to greedy algorithms.","1558-0660","","10.1109/TMC.2021.3060118","Vetenskapsrådet(grant numbers:2020-03860); Swedish Civil Contingencies Agency; Sweden's Innovation Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9358007","Resource allocation;service placement;Lagrangian relaxation;mobile edge computing;availability","Approximation algorithms;Resilience;Process control;Hardware;Energy consumption;Edge computing;Cloud computing","","7","","29","IEEE","18 Feb 2021","","","IEEE","IEEE Journals"
"HYDRA: Decentralized Location-Aware Orchestration of Containerized Applications","L. L. Jiménez; O. Schelén","Department of Computer Science, Electrical and Space Engineering, Luleȧ University of Technology, Luleȧ, Sweden; Department of Computer Science, Electrical and Space Engineering, Luleȧ University of Technology, Luleȧ, Sweden",IEEE Transactions on Cloud Computing,"5 Dec 2022","2022","10","4","2664","2678","The edge computing paradigm, spurred by the Internet-of-Things, poses new requirements and challenges for distributed application deployment. There is a need for an orchestrator design that leverages characteristics that enable this new paradigm. We present HYDRA, a decentralized and distributed orchestrator for containerized microservice applications. This orchestrator focuses on scalability and resiliency to enable the global manageability of cloud and edge environments. It can manage heterogeneous resources across geographical locations and provide robust application control. Further, HYDRA enables the location-aware deployment of microservice applications via containerization. Thus, an application's services may be deployed to separate locations according to expected needs. In this article, the experiments show the orchestrator scaling to 20000 nodes and simultaneously deploying 30000 applications. Further, empirical results show that location-aware application deployment does not hinder HYDRA's performance, and the random resource search algorithm currently being employed may be used as a baseline to find resources in this decentralized orchestrator. Therefore, we conclude that HYDRA is a viable orchestrator design for the new computing paradigm.","2168-7161","","10.1109/TCC.2020.3041465","Energimyndigheten(grant numbers:43090-2); Cloudberry Datacenters; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9274526","Edge computing;fog computing;orchestration;microservices;deployment;containers;Docker;Kubernetes;scalability;decentralized networks","Cloud computing;Peer-to-peer computing;Scalability;Containers;Edge computing;Voting;Virtualization","","7","","31","IEEE","1 Dec 2020","","","IEEE","IEEE Journals"
"Edge-Based Runtime Verification for the Internet of Things","C. Tsigkanos; M. M. Bersani; P. A. Frangoudis; S. Dustdar","TU Wien, Vienna, Austria; Politecnico di Milano, Milano, Italy; TU Wien, Vienna, Austria; TU Wien, Vienna, Austria",IEEE Transactions on Services Computing,"6 Oct 2022","2022","15","5","2713","2727","Complex distributed systems such as the ones induced by Internet of Things (IoT) deployments, are expected to operate in compliance to their requirements. This can be checked by inspecting events flowing throughout the system, typically originating from end-devices and reflecting arbitrary actions, changes in state or sensing. Such events typically reflect the behavior of the overall IoT system – they may indicate executions which satisfy or violate its requirements. This article presents a service-based software architecture and technical framework supporting runtime verification for widely deployed, volatile IoT systems. At the lowest level, systems we consider are comprised of resource-constrained devices connected over wide area networks generating events. In our approach, monitors are deployed on edge components, receiving events originating from end-devices or other edge nodes. Temporal logic properties expressing desired requirements are then evaluated on each edge monitor in a runtime fashion. The system exhibits decentralization since evaluation occurs locally on edge nodes, and verdicts possibly affecting satisfaction of properties on other edge nodes are propagated accordingly. This reduces dependence on cloud infrastructures for IoT data collection and centralized processing. We illustrate how specification and runtime verification can be achieved in practice on a characteristic case study of smart parking. Finally, we demonstrate the feasibility of our design over a testbed instantiation, whereupon we evaluate performance and capacity limits of different hardware classes under monitoring workloads of varying intensity using state-of-the-art LPWAN technology.","1939-1374","","10.1109/TSC.2021.3074956","Austrian Science Foundation(grant numbers:M 2778-N); EDENSPACE; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9411895","","Monitoring;Sensors;Runtime;Computer architecture;Temperature sensors;Internet of Things;Software architecture","","7","","48","CCBY","23 Apr 2021","","","IEEE","IEEE Journals"
"Wireless and Low-Power System for Synchronous and Real-Time Structural-Damage Assessment","E. Hidalgo Fort; P. Blanco-Carmona; J. R. Garcia-Oya; F. Muñoz-Chavero; R. Gonzalez-Carvajal; A. R. Serrano-Chacon; E. J. Mascort-Albea","Electronic Engineering Department, University of Seville, Seville, Spain; Electronic Engineering Department, University of Seville, Seville, Spain; Electronic Engineering Department, University of Seville, Seville, Spain; Electronic Engineering Department, University of Seville, Seville, Spain; Electronic Engineering Department, University of Seville, Seville, Spain; Department of Building Structures and Geotechnical Engineering, University of Seville, Seville, Spain; Department of Building Structures and Geotechnical Engineering, University of Seville, Seville, Spain",IEEE Sensors Journal,"14 Jun 2023","2023","23","12","13648","13658","The rise of the Internet of Things (IoT) systems and the evolution of low-power wide-area networks have directly contributed to the emergence of a new generation of structural health monitoring (SHM) systems based on the nondestructive test (NDT) approach. Consequently, this article presents the design and development of a synchronous, low-cost, real-time, wireless, and low-power consumption SHM system for pre-existing buildings and infrastructures, which has been validated on a structure built in the High Middle Ages forming part of the historical heritage of the city of Seville (Spain). The system proposes a modular and scalable design with the capacity for synchronous monitoring of the accelerations of a structure due to the deployment of several reduced-size nodes that acquire and transmit the accelerations of the structures through a secure sockets layer (SSL) narrowband (NB)-IoT connection. This aspect and firmware over the air (FOTA) capability enables the permanent deployment of sensor nodes, thereby not only obviating the use of costly traditional devices but also granting access to the structures to be monitored for each test and maintenance task and hence supporting the tasks related to the preventive conservation of heritage. The experimental tests carried out demonstrate the low impact of global positioning system (GPS) time synchronization and FOTA on the autonomy of the system. The precision of the system is also validated by comparing the results with a precision system in a real field test. Furthermore, continuous monitoring is guaranteed through the graphical interface developed as a composition of microservices, which enables management of the deployed networks.","1558-1748","","10.1109/JSEN.2023.3270961","PID2019-107258RB-C31; Ministerio de Ciencia e Innovación/Agencia Estatal de Innovacion (MCIN/AEI)(grant numbers:10.13039/501100011033); Sevilla City Council’s Urban Planning Department; “Specific collaboration agreement between the University of Sevilla and the Urban Planning Management of the Sevilla City Council, for the development of strategies aimed at the restoration and subsequent preventive conservation of the Medieval wall of Sevilla.”; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10124661","Built heritage;Internet of Things (IoT);low-cost structural health monitoring (SHM) nodes;narrowband (NB)-IoT;nondestructive test (NDT);SHM;synchronous SHM;wireless sensor networks","Monitoring;Wireless sensor networks;Wireless communication;Internet of Things;Sensors;Pollution measurement;Microprogramming","","7","","28","IEEE","15 May 2023","","","IEEE","IEEE Journals"
"Multisensory Metaverse-6G: A New Paradigm of Commerce and Education","M. H. ALSAMH; A. Hawbani; S. Kumar; S. Hamood Alsamhi","Department of Accounting, Faculty of Administrative Science, IBB University, Ibb, Yemen; School of Computer Science, Shenyang Aerospace University, Shenyang, China; Department of CSE, IIIT Naya Raipur, Atal Nagar-Nava Raipur, Chhattisgarh, India; Insight Centre for Data Analytics, University of Galway, Galway, Ireland",IEEE Access,"31 May 2024","2024","12","","75657","75677","Multisensory Metaverse applications ignite a revolution across industries, captivating audiences with immersive experiences. With the impending release of 6G technology, the stage is set for an extraordinary advancement toward an even higher level of engagement and immersion. This paper examines how combining multimodal Metaverse apps with 6G technology might change the game, with an emphasis on business and education. We examine how multisensory Metaverse-6G signifies and reshapes a paradigm change in digital connection and immersion in light of the impending advent of 6G networks, distinguished by ultra-high-speed connectivity, low latency, and immense interconnectedness. We investigate how immersive Metaverse experiences improve virtual purchasing, targeted marketing campaigns, and customer engagement from a business perspective. Within the field of education, we investigate how Metaverse technology might transform conventional pedagogical techniques through interactive learning, skill development, and distant collaboration. We aim to build a more connected, immersive, and inclusive digital world by imagining a day when multimodal Metaverse-6G experiences drive innovation across commerce and education. We foresee a future where Metaverse-6G convergence transforms communication and cooperation, and we also highlight future research areas and potential developments. By embracing innovation and tackling challenges, we can fully utilize this convergence to build a more inclusive, immersive, and networked digital future.","2169-3536","","10.1109/ACCESS.2024.3392838","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10506898","Multisensory;metaverse;extended reality;6G;immersive communication;holographic communication;metaverse-6G immersive experiences;connectivity;interaction;commerce;education","Metaverse;6G mobile communication;Surveys;Computer architecture;Collaboration;Technological innovation;Biological system modeling;Multisensory integration;Immersive experience;Education;Commerce and trade","","7","","107","CCBYNCND","23 Apr 2024","","","IEEE","IEEE Journals"
"Validation of Current O-RAN Technologies and Insights on the Future Evolution","Y. Huang; Q. Sun; N. Li; Z. Chen; J. Huang; H. Ding; C. -L. Ⅰ","China Mobile Research Institute, Beijing, China; Wireless and Terminal Technology Department, China Mobile Research Institute, Beijing, China; Wireless and Terminal Technology Department, China Mobile Research Institute, Beijing, China; Wireless and Terminal Technology Department, China Mobile Research Institute, Beijing, China; Wireless and Terminal Technology Department, China Mobile Research Institute, Beijing, China; China Mobile Research Institute, Beijing, China; China Mobile Research Institute, Beijing, China",IEEE Journal on Selected Areas in Communications,"16 Jan 2024","2024","42","2","487","505","Entering the 5G era, the mobile network operators (MNO) are facing greater challenges in providing services cost effectively than any other previous generations. The potential solutions to this are lying on the emerging trend of deep convergence of information technology (IT), communication technology (CT) and data technology (DT). In particular, the O-RAN technology, the representation of such ICDT convergence and proposed by the O-RAN ALLIANCE in 2018, is transforming Radio Access Networks towards a new paradigm featuring openness, cloudification and intelligence. O-RAN has gained huge attention from both industry and academia since its inception. In this paper, we presented the recent endeavors from China Mobile, including our deployment scenarios, various test results from open fronthaul, cloud platform to the intelligent controller. Our rich and comprehensive tests have demonstrated the viability and superiority of current O-RAN technologies. Furthermore, we also provide our deep thinking on the O-RAN future evolution in order to better serve the emerging applications such as Metaverse, cloud extended-reality (XR), extensive enterprise private 5G verticals and so on.","1558-0008","","10.1109/JSAC.2023.3336180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10330580","O-RAN;cloudification;RAN intelligent controller;prototype;field trial","Computer architecture;Hardware;5G mobile communication;Synchronization;Cloud computing;Testing;Microprocessors;Open RAN","","7","","47","IEEE","28 Nov 2023","","","IEEE","IEEE Journals"
"Visible Light Positioning as a Next-Generation Indoor Positioning Technology: A Tutorial","S. Bastiaens; M. Alijani; W. Joseph; D. Plets","Department of Information Technology, Research Group imec-WAVES, Ghent University, Ghent, Belgium; Department of Information Technology, Research Group imec-WAVES, Ghent University, Ghent, Belgium; Department of Information Technology, Research Group imec-WAVES, Ghent University, Ghent, Belgium; Department of Information Technology, Research Group imec-WAVES, Ghent University, Ghent, Belgium",IEEE Communications Surveys & Tutorials,"21 Nov 2024","2024","26","4","2867","2913","Indoor positioning technology has become undisputedly pervasive in modern society. It can be used for numerous applications in various environments, ranging from agriculture to smart industry. Among the different indoor positioning technologies, the development of the indoor visible light positioning (VLP) system has gained momentum recently thanks to its low cost and centimetre-order accuracy. Moreover, VLP is a promising candidate technology for integration with Light Fidelity (Li-Fi), which relies on the optical spectrum to build future wireless networks and plays a significant role in sixth-generation (6G) wireless networks. This tutorial investigates VLP as a novel positioning technology in depth. The inherent characteristics and fundamentals of this technology are extensively described with the components of a VLP system and possible localization approaches being discussed. Furthermore, a mathematical and technical discussion of a simple minimum retrofit of the single photodiode (PD) received signal strength (RSS) VLP systems is provided. Finally, the paper concludes with open challenges and a future roadmap for VLP.","1553-877X","","10.1109/COMST.2024.3372153","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10457033","Visible light positioning (VLP);indoor positioning systems (IPSs);tutorial","Tutorials;Global navigation satellite system;Surveys;IP networks;Lighting;Costs;Receivers","","7","","469","CCBY","1 Mar 2024","","","IEEE","IEEE Journals"
"A Fuzzy-Based Clinical Decision Support System for Coeliac Disease","M. E. Tabacchi; D. Tegolo; D. Cascio; C. Valenti; S. Sorce; V. Gentile; V. Taormina; I. Brusca; G. Magazzù; A. Giuliano; G. Raso","Dipartimento di Matematica e Informatica, Università degli Studi di Palermo, Palermo, Italy; Dipartimento di Matematica e Informatica, Università degli Studi di Palermo, Palermo, Italy; Dipartimento di Fisica e Chimica, Università degli Studi di Palermo, Palermo, Italy; Dipartimento di Matematica e Informatica, Università degli Studi di Palermo, Palermo, Italy; Facoltà di Ingegneria e Architettura, Università degli Studi di Enna ‘‘Kore,’’, Enna, Italy; Dipartimento di Fisica e Chimica, Università degli Studi di Palermo, Palermo, Italy; Dipartimento di Matematica e Informatica, Università degli Studi di Palermo, Palermo, Italy; Ospedale Fatebenefratelli, Buccheri La Ferla, Palermo, Italy; Dipartimento di Patologia Umana dell’adulto e dell’età evolutiva, Università di Messina, Messina, Italy; AcrossLimits Ltd., Valletta, Malta; Dipartimento di Fisica e Chimica, Università degli Studi di Palermo, Palermo, Italy",IEEE Access,"3 Oct 2022","2022","10","","102223","102236","Coeliac disease (CD) is a permanent inflammatory disease of the small intestine characterized by the destruction of the mucous membrane of this intestinal tract. Coeliac disease represents the most frequent food intolerance and affects about 1% of the population, but it is severely underdiagnosed. Currently available guidelines require CD-specific serology and atrophic histology in duodenal biopsy samples to diagnose CD in adults. In paediatric CD, but recently in adults also, non-invasive diagnostic strategies have become increasingly popular. In order to increase the rates of correct diagnosis of the disease without the use of biopsy, researchers have recently been using approaches based on artificial intelligence techniques. In this work, we present a Clinical Decision Support System (CDSS)system for supporting CD diagnosis, developed in the context of the Italy-Malta cross-border project ITAMA. The implemented CDSS has been based on a neural-network-based fuzzy classifier. The system was developed and tested using a Virtual Database and a Real Database acquired during the ITAMA project. Analysis on 10,000 virtual patients shows that the system achieved an accuracy of 99% and a sensitivity of 99%. On 19,415 real patients, of which 109 with a confirmed diagnosis of coeliac disease, the system achieved 99.6% accuracy, 85.7% sensitivity, 99.6% specificity and 96% precision. Such results show that the developed system can be used effectively to support the diagnosis of the CD by reducing the appeal to invasive techniques such as biopsy.","2169-3536","","10.1109/ACCESS.2022.3208903","ITAMA Project under the International Cooperation Program Interreg V-A 1(grant numbers:JS-Code C1-1.1-18); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9900343","Coeliac disease;computer aided diagnosis;artificial intelligence;endoscopy;neural network;fuzzy classifier;CDSS","Diseases;Biopsy;Decision support systems;Pediatrics;Databases;Medical diagnostic imaging;Social factors;Artificial intelligence;Fuzzy systems;Endoscopy;Clinical diagnosis;Computer aided diagnosis","","7","","28","CCBY","22 Sep 2022","","","IEEE","IEEE Journals"
"Band-Area Application Container and Artificial Fish Swarm Algorithm for Multi-Objective Optimization in Internet-of-Things Cloud","M. Ouyang; J. Xi; W. Bai; K. Li","School of Software Engineering, South China University of Technology, Guangzhou, China; School of Software Engineering, South China University of Technology, Guangzhou, China; School of Computer Science, Zhaoqing University, Zhaoqing, China; Department of Computer Science, State University of New York, New Paltz, NY, USA",IEEE Access,"15 Feb 2022","2022","10","","16408","16423","Container virtualization methods based on application deployment levels have been widely adopted in cloud-computing environments to implement application construction, deployment, and migration. However, most application containers focus on the interface between the applications and hosts and lack collaboration between application containers. This study proposes a new application container model that contains users, application services, documents, and messages, called Band-area Application Container. A salient feature of the Band-area is that it can express a variety of things in reality, such as organizations or individuals. End users can build a complex and changeable application system through cooperation between the Band-areas. However, the resource allocation of non Internet-of-Thing and Internet-of-Thing tasks from the application container is an open issue. The resource allocation method of tasks should not only improve the quality of the user experience, but also reduce energy consumption by improving the resource utilization of the server. To solve this problem, an artificial fish swarm algorithm is proposed to optimize container-based task scheduling. The algorithm considers not only the reliability, processing time overhead, and energy consumption of the task, but also the resource utilization of the servers. Experimental evaluation shows that, compared with the existing three algorithms, the algorithm obtains a better improvement rate in task processing time overhead, energy consumption, reliability, and cluster load balancing.","2169-3536","","10.1109/ACCESS.2022.3150326","Science and Technology Plan Project of Guangdong Province, China(grant numbers:2014B010112007,2016B010124010); Guangdong Province Educational Science Planning Project(grant numbers:2019KTSCX199); Zhaoqing Science and Technology Special Fund Project(grant numbers:2020G1004); Zhaoqing University Science and Technology Projects(grant numbers:zlgc201933); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9709286","Application container;artificial fish swarm algorithm;Internet-of-Things;multi-objective optimization;task scheduling","Containers;Task analysis;Resource management;Optimization;Computational modeling;Scheduling;Cloud computing","","7","","37","CCBY","9 Feb 2022","","","IEEE","IEEE Journals"
"PerfSim: A Performance Simulator for Cloud Native Microservice Chains","M. G. Khan; J. Taheri; A. Al-Dulaimy; A. Kassler","Department of Mathematics and Computer Science, Karlstad University, Karlstad, Sweden; Department of Mathematics and Computer Science, Karlstad University, Karlstad, Sweden; School of Innovation, Design and Engineering, Mälardalen University, Västerȧs, Sweden; Department of Mathematics and Computer Science, Karlstad University, Karlstad, Sweden",IEEE Transactions on Cloud Computing,"6 Jun 2023","2023","11","2","1395","1413","Cloud native computing paradigm allows microservice-based applications to take advantage of cloud infrastructure in a scalable, reusable, and interoperable way. However, in a cloud native system, the vast number of configuration parameters and highly granular resource allocation policies can significantly impact the performance and deployment cost. For understanding and analyzing these implications in an easy, quick, and cost-effective way, we present PerfSim, a discrete-event simulator for approximating and predicting the performance of cloud native service chains in user-defined scenarios. To this end, we proposed a systematic approach for modeling the performance of microservices endpoint functions by collecting and analyzing their performance and network traces. With a combination of the extracted models and user-defined scenarios, PerfSim can then simulate the performance behavior of all services over a given period and provide an approximation for system KPIs, such as requests’ average response time. Using the processing power of a single laptop, we evaluated both simulation accuracy and speed of PerfSim in 104 prevalent scenarios and compared the simulation results with the identical deployment in a real Kubernetes cluster. We achieved $\scriptstyle \mathtt {\sim }$∼81-99% simulation accuracy in approximating the average response time of incoming requests and $\scriptstyle \mathtt {\sim }$∼16-1200 times speed-up factor for the simulation.","2168-7161","","10.1109/TCC.2021.3135757","Knowledge Foundation of Sweden(grant numbers:20200067); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9652084","Performance simulator;performance modeling;cloud native computing;service chains;simulation platform","Cloud computing;Computational modeling;Microservice architectures;Resource management;Emulation;Containers;Testing","","7","","55","CCBY","15 Dec 2021","","","IEEE","IEEE Journals"
"FIWARE-Based Telemedicine Apps Modeling for Patients’ Data Management","X. Aizaga-Villon; K. Alarcon-Ballesteros; J. Cordova-Garcia; V. S. Padilla; W. Velasquez","Escuela Superior Politécnica del Litoral, Faculty of Electrical and Computer Engineering, ESPOL Polytechnic University, Guayaquil, Ecuador; Escuela Superior Politécnica del Litoral, Faculty of Electrical and Computer Engineering, ESPOL Polytechnic University, Guayaquil, Ecuador; Escuela Superior Politécnica del Litoral, Faculty of Electrical and Computer Engineering, ESPOL Polytechnic University, Guayaquil, Ecuador; Escuela Superior Politécnica del Litoral, Faculty of Electrical and Computer Engineering, ESPOL Polytechnic University, Guayaquil, Ecuador; Escuela Superior Politécnica del Litoral, Faculty of Electrical and Computer Engineering, ESPOL Polytechnic University, Guayaquil, Ecuador",IEEE Engineering Management Review,"26 Jul 2022","2022","50","2","173","188","Open-source technologies enable communication channels between web platforms and innovative architectures to provide reliable data distribution, in which healthcare applications can particularly benefit from them. This work presents a communication channel design to improve the user experience about telemedicine apps, especially when patients are in remote locations while assuring their information using an innovative approach. The general purpose is to avoid users having to physically go to medical facilities by the correct data management related to their appointments and medical history. By preventing the attendance to healthcare facilities, patients do not expose themselves unnecessarily to viruses and bacteria. Therefore, this research includes a data communications model based on the FIWARE platform and cloud technologies for reliable user medical information distribution. The prototype is developed based on open-source technologies and registered the evaluation of different performance metrics that included cases scenarios in which administrators of healthcare centers configured options according to the availability of assets and informatics resources. The results show the effectiveness of the communication model under realistic conditions for encouraging the acceptance of telemedicine alternatives, especially when patients and medical staff present limitations regarding mobility.","1937-4178","","10.1109/EMR.2022.3169991","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762509","Cloud system;distributed database;FIWARE;remote patient monitoring;telemedicine","Medical services;Telemedicine;Medical diagnostic imaging;Cloud computing;Computer architecture;Hospitals;Proposals","","7","","88","IEEE","25 Apr 2022","","","IEEE","IEEE Journals"
"Delay-Aware Container Scheduling in Kubernetes","W. -K. Lai; Y. -C. Wang; S. -C. Wei","Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan; Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan; Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan",IEEE Internet of Things Journal,"22 Jun 2023","2023","10","13","11813","11824","Kubernetes is a powerful tool to manage containerized applications, which is also regarded as one promising platform to support microservices in edge computing. The scheduler is a key component of Kubernetes. It allocates each pod (i.e., a set of running containers) to one worker node (i.e., a machine). The default scheduler in Kubernetes is designed for the cloud environment containing homogeneous nodes. However, IoT edge nodes usually have various computing power and network bandwidth. This article proposes a delay-aware container scheduling (DACS) algorithm to address the issue of node heterogeneity in edge computing. To efficiently assign pods to worker nodes, DACS takes account of not only residual resources of worker nodes but also potential delays caused by the pod assignment. We build a Kubernetes cluster by VMware to evaluate system performance. Experimental results reveal that DACS can significantly reduce both processing and network delays, thereby helping Kubernetes perform more efficiently in an edge environment.","2327-4662","","10.1109/JIOT.2023.3244545","National Science and Technology Council (NSTC), Taiwan,(grant numbers:111-2221-E-110-025); NSTC(grant numbers:111-2221-E-110-023-MY2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10044213","Container;delay;edge;kubernetes;scheduling","Containers;Delays;Processor scheduling;Task analysis;Micromechanical devices;Microservice architectures;Internet of Things","","7","","45","IEEE","14 Feb 2023","","","IEEE","IEEE Journals"
"A Survey of Edge Caching: Key Issues and Challenges","H. Li; M. Sun; F. Xia; X. Xu; M. Bilal","School of Computer Science, Nanjing University of Information Science and Technology, Nanjing, China; Shandong Provincial University Laboratory for Protected Horticulture, Weifang University of Science and Technology, Weifang, China; Reading Academy, Nanjing University of Information Science and Technology, Nanjing, China; School of Software, Nanjing University of Information Science and Technology, Nanjing, China; Department of Computer Engineering, Hankuk University of Foreign Studies, Yongin-si, Republic of Korea",Tsinghua Science and Technology,"4 Dec 2023","2024","29","3","818","842","With the rapid development of mobile communication technology and intelligent applications, the quantity of mobile devices and data traffic in networks have been growing exponentially, which poses a great burden to networks and brings huge challenge to servicing user demand. Edge caching, which utilizes the storage and computation resources of the edge to bring resources closer to end users, is a promising way to relieve network burden and enhance user experience. In this paper, we aim to survey the edge caching techniques from a comprehensive and systematic perspective. We first present an overview of edge caching, summarizing the three key issues regarding edge caching, i.e., where, what, and how to cache, and then introducing several significant caching metrics. We then carry out a detailed and in-depth elaboration on these three issues, which correspond to caching locations, caching objects, and caching strategies, respectively. In particular, we innovate on the issue “what to cache”, interpreting it as the classification of the “caching objects”, which can be further classified into content cache, data cache, and service cache. Finally, we discuss several open issues and challenges of edge caching to inspire future investigations in this research area.","1007-0214","","10.26599/TST.2023.9010051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10339809","edge caching;edge computing;caching location;caching object;caching strategy;5G network architecture;Internet of Things (IoT)","Surveys;Measurement;Systematics;5G mobile communication;User experience;Mobile handsets","","7","","134","","4 Dec 2023","","","TUP","TUP Journals"
"Toward a Live BBU Container Migration in Wireless Networks","E. Schiller; J. Ajayi; S. Weber; T. Braun; B. Stiller","Department of Informatics IfI, Communication Systems Group CSG, Universit&#x00E4;t Z&#x00FC;rich, Z&#x00FC;rich, Switzerland; Department of Computer Science INF, Communication and Distributed Systems Group CDS, Universit&#x00E4;t Bern, Bern, Switzerland; Faculty of Law IUS, Universit&#x00E4;t Z&#x00FC;rich, Z&#x00FC;rich, Switzerland; Department of Computer Science INF, Communication and Distributed Systems Group CDS, Universit&#x00E4;t Bern, Bern, Switzerland; Department of Informatics IfI, Communication Systems Group CSG, Universit&#x00E4;t Z&#x00FC;rich, Z&#x00FC;rich, Switzerland",IEEE Open Journal of the Communications Society,"28 Feb 2022","2022","3","","301","321","Cloud Radio Access Networks (Cloud-RANs) have recently emerged as a promising architecture to meet the increasing demands and expectations of future wireless networks. Such an architecture can enable dynamic and flexible network operations to address significant challenges, such as higher mobile traffic volumes and increasing network operation costs. However, the implementation of compute-intensive signal processing Network Functions (NFs) on the General Purpose Processors (General Purpose Processor) that are typically found in data centers could lead to performance complications, such as in the case of overloaded servers. There is therefore a need for methods that ensure the availability and continuity of critical wireless network functionality in such circumstances. Motivated by the goal of providing highly available and fault-tolerant functionality in Cloud-RAN-based networks, this paper proposes the design, specification, and implementation of live migration of containerized Baseband Units (BBUs) in two wireless network settings, namely Long Range Wide Area Network (LoRaWAN) and Long Term Evolution (LTE) networks. Driven by the requirements and critical challenges of live migration, the approach shows that in the case of LoRaWAN networks, the migration of BBUs is currently possible with relatively low downtimes to support network continuity. The analysis and comparison of the performance of functional splits and cell configurations in both networks were performed in terms of fronthaul throughput requirements. The results obtained from such an analysis can be used by both service providers and network operators in the deployment and optimization of Cloud-RANs services, in order to ensure network reliability and continuity in cloud environments.","2644-125X","","10.1109/OJCOMS.2022.3149965","University of Zürich, Switzerland; University of Bern; European Union H2020 Research and Innovation Program(grant numbers:830927); H2020 Concordia Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9709593","Cloud-RAN;live migration;LoRaWAN;LTE;network function virtualization (NFV)","Long Term Evolution;Baseband;Wireless networks;Computer architecture;Wide area networks;Cloud computing;Time-frequency analysis","","7","","54","CCBY","10 Feb 2022","","","IEEE","IEEE Journals"
"Network Slicing-Based Learning Techniques for IoV in 5G and Beyond Networks","W. Hamdi; C. Ksouri; H. Bulut; M. Mosbah","Department of Computer Engineering, Ege University, Izmir, Turkey; Automated and Connected Mobility, Vedecom Institute, Versailles, France; Department of Computer Engineering, Ege University, Izmir, Turkey; CNRS, Bordeaux INP, LaBRI, UMR 5800, Univ. Bordeaux, Talence, France",IEEE Communications Surveys & Tutorials,"22 Aug 2024","2024","26","3","1989","2047","The effects of transport development on people’s lives are diverse, ranging from economy to tourism, health care, etc. Great progress has been made in this area, which has led to the emergence of the Internet of Vehicles (IoV) concept. The main objective of this concept is to offer a safer and more comfortable travel experience through making available a vast array of applications, by relying on a range of communication technologies including the fifth-generation mobile networks. The proposed applications have personalized Quality of Service (QoS) requirements, which raise new challenging issues for the management and allocation of resources. Currently, this interest has been doubled with the start of the discussion of the sixth-generation mobile networks. In this context, Network Slicing (NS) was presented as one of the key technologies in the 5G architecture to address these challenges. In this article, we try to bring together the effects of NS implications in the Internet of Vehicles field and show the impact on transport development. We begin by reviewing the state of the art of NS in IoV in terms of architecture, types, life cycle, enabling technologies, network parts, and evolution within cellular networks. Then, we discuss the benefits brought by the use of NS in such a dynamic environment, along with the technical challenges. Moreover, we provide a comprehensive review of NS deploying various aspects of Learning Techniques for the Internet of Vehicles. Afterwards, we present Network Slicing utilization in different IoV application scenarios through different domains; terrestrial, aerial, and marine. In addition, we review Vehicle-to-Everything (V2X) datasets as well as existing implementation tools; besides presenting a concise summary of the Network Slicing-related projects that have an impact on IoV. Finally, in order to promote the deployment of Network Slicing in IoV, we provide some directions for future research work. We believe that the survey will be useful for researchers from academia and industry. First, to acquire a holistic vision regarding IoV-based NS realization and identify the challenges hindering it. Second, to understand the progression of IoV powered NS applications in the different fields (terrestrial, aerial, and marine). Finally, to determine the opportunities for using Machine Learning Techniques (MLT), in order to propose their own solutions to foster NS-IoV integration.","1553-877X","","10.1109/COMST.2024.3372083","French National Agency for Research under the Future Investments Program(grant numbers:ANR-20-IDES-0001); Industrial Research Chair “Mobility and Intelligent Transports” (Foundation of Bordeaux University, 2019-2023); European Union’s Horizon 2020 Research and Innovation Programme(grant numbers:951947 (5GMED)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10457570","Internet of Vehicles;network slicing;machine learning techniques","Surveys;Network slicing;5G mobile communication;Resource management;Quality of service;Reviews;Marine vehicles","","7","","407","IEEE","1 Mar 2024","","","IEEE","IEEE Journals"
"Ecosystem of Things: Hardware, Software, and Architecture","L. Chao; X. Peng; Z. Xu; L. Zhang","State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China",Proceedings of the IEEE,"6 Aug 2019","2019","107","8","1563","1583","Edge computing is a continuum that includes the computing resources from cloud to things. Ecosystem of things (EoT) is a subsystem of the ecosystem of edge computing, which potentially contains trillions of devices of things and directly interacts with the physical world. This paper surveys the state of the art of EoT by focusing on the computing infrastructure aspect with a forward-looking perspective. We point out a trend of smart edge computing with four types of smartness and intelligence. We address three fundamental questions. 1) What capabilities and how much energy efficiency are the hardware providing? What is the future growth potential? 2) What abstractions are provided by the system software? Are they adequate to support smart edge computing? 3) What ecosystem architectures have been proposed for the coordination of things, the edge, and the cloud? Are they meeting the needs to encourage innovation but avoid unnecessary ecosystem fragmentation? We examine advances from both industry and academia, including research results, visions, and project concepts. We also point out future research directions.","1558-2256","","10.1109/JPROC.2019.2925526","National Natural Science Foundation of China(grant numbers:61532016); CAS Pioneer Hundred Talents Program(grant numbers:Y704061000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8768345","Architecture;ecosystem of things (EoT);edge computing;hardware;Internet of Things (IoT);software","Edge computing;Ecosystems;Computer architecture;Cloud computing;Software engineering;Internet of Things;Smart phones","","7","","35","IEEE","22 Jul 2019","","","IEEE","IEEE Journals"
"On the Feasibility of Using Hierarchical Task Networks and Network Functions Virtualization for Managing Software-Defined Networks","W. Villota; M. Gironza; A. Ordoñez; O. M. Caicedo Rendon","Instituto de Computação, Universidade Estadual de Campinas, Campinas, Brazil; Telematics Engineering Group, Universidad del Cauca, Popayán, Colombia; Intelligent Management Systems Group, Foundation University of Popayán, Popayán, Colombia; Telematics Engineering Group, Universidad del Cauca, Popayán, Colombia",IEEE Access,"30 Jul 2018","2018","6","","38026","38040","Management is an essential process to ensure the proper operation of computer networks. There are a lot of proposals to manage software-defined networks (SDN) from their application plane. However, such proposals share some shortcomings related to low automation of network management tasks, long time needed to handle network situations, and the lack of flexibility and workability. In this paper, we introduce JANO to overcome these shortcomings and investigate the feasibility of using automated planning and network functions' virtualization to manage SDN from a vertical management plane. JANO uses hierarchical task networks' planning to generate automatically plans that automate management tasks and reduce the time required by administrators to face network situations. Also, JANO performs an instantiation of the management orchestrator of network functions virtualization to provide flexibility and workability in the generation and execution of plans targeted to addressing network situations. We evaluate JANO in a proof of concept. The evaluation results corroborated that JANO is a feasible solution to manage SDN, since the planning time slightly and linearly increased with the number of primitive tasks that form a plan, the time-consuming needed by administrators to addressing a network situation was short, and the additional traffic was low.","2169-3536","","10.1109/ACCESS.2018.2852649","Universidad del Cauca; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8408461","Automated planning;hierarchical task network;network functions virtualization;software-defined networks","Task analysis;Planning;Proposals;Monitoring;Network function virtualization;Workability","","6","","54","OAPA","9 Jul 2018","","","IEEE","IEEE Journals"
"Hardware Acceleration for Container Migration on Resource-Constrained Platforms","P. Shantharama; A. S. Thyagaturu; A. Yatavelli; P. Lalwaney; M. Reisslein; G. Tkachuk; E. J. Pullin","School of Electrical, Computer, and Energy Engineering, Arizona State University (ASU), Tempe, AZ, USA; Programmable Solutions Group, Intel Corporation, Chandler, AZ, USA; Design Engineering Group, Intel Corporation, San Diego, CA, USA; IP Engineering Group, Intel Corporation, San Diego, CA, USA; School of Electrical, Computer, and Energy Engineering, Arizona State University (ASU), Tempe, AZ, USA; Network Platforms Group, Intel Corporation, Chandler, AZ, USA; Network Platforms Group, Intel Corporation, Chandler, AZ, USA",IEEE Access,"2 Oct 2020","2020","8","","175070","175085","The computing capabilities of client devices are continuously increasing; at the same time, demands for ultra-low latency (ULL) services are increasing. These ULL services can be provided by migrating some micro-service container computations from the cloud and multi-access edge computing (MEC) to the client devices. The migration of a container image requires compression and decompression, which are computationally demanding. We quantitatively examine the hardware acceleration of container image compression and decompression on a client device. Specifically, we compare the Intel® Quick Assist Technology (QAT) hardware acceleration with software compression/decompression. For scenarios with a local container image registry (i.e., without network bandwidth constraints), we find that Intel QAT speeds up compression by a factor of over 7 compared to the single-core GZIP software and reduces the CPU core utilization by over 15% for large container images. These Intel QAT benefits come at the expense of Input/Output (IO) memory access bitrates of up to 900 Mbyte/s (while the software compression/decompression does not require IO memory access). For scenarios with a remote container image registry, we find that the container push (compression) time savings increase with the network bandwidth, while the container pull (decompression) time savings level out for moderately high network bandwidths and slightly decrease for a very high network bandwidth. Furthermore, Intel QAT acceleration achieves substantial power consumption reductions for container push compression for low to moderately high network bandwidths. Our evaluation results give reference performance benchmarks of the achievable latencies for container image instantiation and migration with and without hardware acceleration of the compression and decompression of container images.","2169-3536","","10.1109/ACCESS.2020.3024675","National Science Foundation(grant numbers:1716121); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200377","Compression;container migration;Docker;hardware acceleration;Intel QuickAssist Technology (QAT)","Containers;Image coding;Cloud computing;Bandwidth;Acceleration;Servers","","6","","58","CCBY","18 Sep 2020","","","IEEE","IEEE Journals"
"Application Areas of Information-Centric Networking: State-of-the-Art and Challenges","L. C. M. Hurali; A. P. Patil","Department of Computer Science and Engineering, Ramaiah Institute of Technology, Bengaluru, Karnataka, India; Department of Computer Science and Engineering, Ramaiah Institute of Technology, Bengaluru, Karnataka, India",IEEE Access,"29 Nov 2022","2022","10","","122431","122446","The Information-Centric Network (ICN) paradigm has gained popularity since its inception. The host-based IP networks were not primarily designed to handle scenarios that it is exposed to on the current Internet. In that direction lot of research has been happening to develop applications such as web applications, multimedia streaming, the Internet of Things, Wireless Sensor Networks and Vehicular networks. In addition, new ICN application areas, such as social networks, Industrial IoTs, etc., are emerging. This review investigates the possible application areas and their deficiencies evenly, broadly and at a certain level of depth with focus on security, scalability, IP interoperability, modularity and other application specific aspects. We discuss the current state-of-the-art in these ICN-based applications and the existing limitations. A comparative analysis of the literary works available is performed to understand the research gaps available, and a detailed discussion of the challenges in each area is provided. We conclude the review with future challenges in the application development with the ICN paradigm to reap its architectural benefits.","2169-3536","","10.1109/ACCESS.2022.3223667","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9956802","Content-centric networks;information-centric networks;named data networking;network security","Security;Routing;Internet;Information-centric networking;IP networks;Authentication;Registers","","6","","97","CCBY","21 Nov 2022","","","IEEE","IEEE Journals"
"Harnessing the Potential of Function-Reuse in Multimedia Cloud Systems","C. Denninnart; M. A. Salehi","Center of Advanced Computer Study, University of Louisiana at Lafayette, Lafayette, LA, USA; Center of Advanced Computer Study, University of Louisiana at Lafayette, Lafayette, LA, USA",IEEE Transactions on Parallel and Distributed Systems,"12 Aug 2021","2022","33","3","617","629","Cloud-based computing systems can get oversubscribed due to the budget constraints of their users or limitations in certain resource types. The oversubscription can, in turn, degrade the users perceived Quality of Service (QoS). The approach we investigate to mitigate both the oversubscription and the incurred cost is based on smart reusing of the computation needed to process the service requests (i.e., tasks). We propose a reusing paradigm for the tasks that are waiting for execution. This paradigm can be particularly impactful in serverless platforms where multiple users can request similar services simultaneously. Our motivation is a multimedia streaming engine that processes the media segments in an on-demand manner. We propose a mechanism to identify various types of “mergeable” tasks and aggregate them to improve the QoS and mitigate the incurred cost. We develop novel approaches to determine when and how to perform task aggregation such that the QoS of other tasks is not affected. Evaluation results show that the proposed mechanism can improve the QoS by significantly reducing the percentage of tasks missing their deadlines and reduce the overall time (and subsequently the incurred cost) of utilizing cloud services by more than 9 percent.","1558-2183","","10.1109/TPDS.2021.3097911","National Science Foundation(grant numbers:CNS-2007209,CNS-2047144); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9490337","Task aggregation;over-subscription;serverless computing;cloud computing;video stream processing","Streaming media;Task analysis;Cloud computing;Containers;Engines;Delays;Admission control","","6","","41","IEEE","19 Jul 2021","","","IEEE","IEEE Journals"
"Forging the Industrial Metaverse for Industry 5.0: Where Extended Reality, IIoT, Opportunistic Edge Computing, and Digital Twins Meet","T. M. Fernández-Caramés; P. Fraga-Lamas","Department of Computer Engineering, Faculty of Computer Science, A Coruña, Spain; Department of Computer Engineering, Faculty of Computer Science, A Coruña, Spain",IEEE Access,"19 Jul 2024","2024","12","","95778","95819","The Industrial Metaverse can benefit from the concepts fostered by Industry 5.0, since it implies making use of dynamic and up-to-date content, as well as fast human-to-machine interactions. To enable such enhancements, this article proposes the concept of Meta-Operator, which is essentially an industrial worker that follows the principles of Industry 5.0 and interacts with Industrial Metaverse applications and with his/her surroundings through advanced Extended Reality (XR) devices. In order to build the foundations of future Meta-Operators, this article provides a thorough description of the main technologies that support such a concept: the main components of the Industrial Metaverse, the latest XR technologies and accessories and the use of Opportunistic Edge Computing (OEC) communications (to detect and interact with the surrounding Internet of Things (IoT) and Industrial IoT (IIoT) devices). Moreover, this paper analyzes how to create the next generation of Industrial Metaverse applications based on the Industry 5.0 concepts, including the most relevant standardization initiatives, the integration of AR/MR devices with IoT/IIoT solutions, the development of advanced communications and software architectures and the creation of shared experiences and opportunistic collaborative protocols. Finally, this article provides an extensive list of potential Industry 5.0 applications for the Industrial Metaverse and analyzes thoroughly the main challenges and research lines. Thus, this article provides a holistic view and useful guidelines for the future developers and researchers that will create the next generation of applications for the Industrial Metaverse.","2169-3536","","10.1109/ACCESS.2024.3422109","PID2020-118857RA-100 (ORBALLO, Opportunistic Edge Computing Based on Mobile and Low-Power IoT Devices); MCIN/AEI/10.13039/501100011033; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10596039","Industrial metaverse;Industry 5.0;Meta-Operator;Augmented Reality;Mixed Reality;Opportunistic Edge Computing;Digital Twins;Metaverse;IIoT","Metaverse;Fifth Industrial Revolution;Fourth Industrial Revolution;Industries;Industrial Internet of Things;Extended reality;Object recognition;Mixed reality;Edge computing","","6","","276","CCBY","11 Jul 2024","","","IEEE","IEEE Journals"
"Anomaly Detection in Social-Aware IoT Networks","J. Tang; T. Qin; D. Kong; Z. Zhou; X. Li; Y. Wu; J. Gu","School of Artificial Intelligence, Hebei University of Technology, Tianjin, China; School of Artificial Intelligence, Hebei University of Technology, Tianjin, China; School of Artificial Intelligence, Hebei University of Technology, Tianjin, China; School of Information Engineering, China University of Geosciences, Beijing, China; School of Information Engineering, China University of Geosciences, Beijing, China; School of Information Science and Technology, Jinan University, Guangzhou, China; School of Artificial Intelligence and the Hebei Province Key Laboratory of Big Data Calculation, Hebei University of Technology, Tianjin, China",IEEE Transactions on Network and Service Management,"9 Oct 2023","2023","20","3","3162","3176","Anomaly event coverage is usually related to several attributes, among which the primary attribute dominates at the time of improving detection efficiency. In the case of Internet of Things (IoT) devices with complex social-aware relationships, IoT nodes with primary attributes should cooperate with each other through their social-aware interactions, to detect potential event anomalies and further determine the coverage of such anomalies. Existing research has put a lot of effort into designing IoT detection frameworks to discover anomalous sensor data, rarely caring about the social-aware interactions. This paper targets this important efficiency problem, and develops a novel anomaly detection mechanism in collaborative social-edge-cloud architecture. The focus of it is to first construct a vector space based Aggregation Behavior Comparison Detection Model, and quantify the change of monitoring behavior by defining the clustering threshold of vector space. This can quickly judge whether a local social network is abnormal and speed up the abnormal detection rate. If it is, a Social Behavior Correlation Detection Model is further designed based on the correlation of primary attributes derived from the dominating social-aware interaction behavior captured by (primary) edge nodes. This strategy can help detect specific “abnormal” areas managed by one or more edge devices with higher accuracy. In the process of anomaly detection, we also propose a spatial index tree to store the information of IoT nodes, so as to effectively collect and route the perceived data of IoT nodes for anomaly analysis. Experimental results demonstrate that our anomaly detection method promotes the detection efficiency and accuracy in comparison with the state of art’s techniques.","1932-4537","","10.1109/TNSM.2023.3242320","Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2019B1515120010); National Natural Science Foundation of China(grant numbers:61932011); Guangdong KeyR&D Plan2020(grant numbers:2020B0101090002); National KeyR&D Plan2020(grant numbers:2020YFB1005600); Open Project Program of Tianjin Key Laboratory of Aerospace Intelligent Equipment Technology, Tianjin Institute of Aerospace Mechanical and Electrical Equipment(grant numbers:ZNZB-2021-01); Supplementary Funds for Performance of Hebei Province Key Laboratory of Big Data Calculation(grant numbers:22567603H); National Natural Science Foundation of China(grant numbers:61702232); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10037228","Anomaly detection;Internet of Things;aggregation behavior comparison detection;social behavior correlation detection;spatial index tree","Internet of Things;Image edge detection;Anomaly detection;Behavioral sciences;Cloud computing;Monitoring;Indexes","","6","","43","IEEE","6 Feb 2023","","","IEEE","IEEE Journals"
"Network Selection Over 5G-Advanced Heterogeneous Networks Based on Federated Learning and Cooperative Game Theory","C. C. González; E. F. Pupo; E. Iradier; P. Angueira; M. Murroni; J. Montalban","Department of Electrical and Electronic Engineering (DIEE/UdR CNIT), University of Cagliari, Sardinia, Italy; Department of Electrical and Electronic Engineering (DIEE/UdR CNIT), University of Cagliari, Sardinia, Italy; Department of Computer Languages and System, University of the Basque Country (UPV/EHU), Bilbao, Spain; Department of Communications Engineering, University of the Basque Country (UPV/EHU), Bilbao, Spain; Department of Electrical and Electronic Engineering (DIEE/UdR CNIT), University of Cagliari, Sardinia, Italy; Department of Electronic Technology, University of the Basque Country (UPV/EHU), Bilbao, Spain",IEEE Transactions on Vehicular Technology,"15 Aug 2024","2024","73","8","11862","11877","5G-Advanced and Beyond claims a 3D ecosystem with cooperation between terrestrial and non-terrestrial networks to achieve seamless coverage, improve capacity, and enable advanced applications with strict quality of service (QoS) requirements. This complex environment requires a disaggregated Radio Access Network (RAN) deployment with open interfaces, such as the architecture promoted by the O-RAN Alliance. This architecture, supporting the slicing paradigm, is a prominent solution to guarantee dynamism and differentiated traffic management. Furthermore, intelligence is critical for future wireless networks to enable Machine Learning (ML)-based optimization for autonomous RANs, handling ultra-dense heterogeneous environments, and adapting to numerous scenarios. This paper presents an enhanced Dynamic Radio Access Network Selection (eDRANS) algorithm based on Federated Double Deep Q-Network (F-DDQN) and inserted in the novel O-RAN architecture. The proposal selects the most suitable base station (BS) to satisfy multiple service requests, optimizing QoS and slicing resource utilization. Moreover, the solution employs a Cooperative Game Theory (CGT) approach to manage resources in overload situations. This load-balancing process enables the acceptance of new clients without abruptly degrading the active users' perception. eDRANS is adapted to diverse network conditions, multiple service constraints, and several user types with different priorities and mobility behaviors. The proposal is validated through network-level simulations, recreating a heterogeneous environment composed of terrestrial-airborne nodes and using the Max-SINR criterion, a heuristic algorithm, and centralized and distributed ML solutions as benchmarks. Results show that eDRANS correctly learns during multiple trial-and-error interactions with the environment, fulfilling the Service Level Agreement (SLA) and maximizing user satisfaction.","1939-9359","","10.1109/TVT.2024.3373638","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10461128","5G-advanced and beyond;federated deep reinforcement learning;game theory;network slicing;QoS","Heuristic algorithms;Resource management;Quality of service;Vehicle dynamics;Proposals;Handover;Training","","6","","50","IEEE","6 Mar 2024","","","IEEE","IEEE Journals"
"Toward 6G Optical Fronthaul: A Survey on Enabling Technologies and Research Perspectives","A. Fayad; T. Cinkler; J. Rak","Department of Telecommunications and Media Informatics, Budapest University of Technology and Economics, Budapest, Hungary; Department of Telecommunications and Media Informatics, Budapest University of Technology and Economics, Budapest, Hungary; Department of Computer Communications, Gdańsk University of Technology, Gdańsk, Poland",IEEE Communications Surveys & Tutorials,"14 Feb 2025","2025","27","1","629","666","The anticipated launch of the Sixth Generation (6G) of mobile technology by 2030 will mark a significant milestone in the evolution of wireless communication, ushering in a new era with advancements in technology and applications. 6G is expected to deliver ultra-high data rates and almost instantaneous communications, with three-dimensional coverage for everything, everywhere, and at any time. In the 6G Radio Access Networks (RANs) architecture, the Fronthaul connects geographically distributed Remote Units (RUs) to Distributed/Digital Units (DUs) pool. Among all possible solutions for implementing 6G fronthaul, optical technologies will remain crucial in supporting the 6G fronthaul, as they offer high-speed, low-latency, and reliable transmission capabilities to meet the 6G strict requirements. This survey provides an explanation of the 5G and future 6G optical fronthaul concept and presents a comprehensive overview of the current state of the art and future research directions in 6G optical fronthaul, highlighting the key technologies and research perspectives fundamental in designing fronthaul networks for 5G and future 6G. Additionally, it examines the benefits and drawbacks of each optical technology and its potential applications in 6G fronthaul networks. This paper aims to serve as a comprehensive resource for researchers and industry professionals about the current state and future prospects of 6G optical fronthaul technologies, facilitating the development of robust and efficient wireless networks of the future.","1553-877X","","10.1109/COMST.2024.3408090","CHISTERA Sustainable and Adaptive Ultra-High-Capacity Micro Base Stations (SAMBAS) Grant funded by Research Foundation–Flanders (FWO), Agence nationale de la Recherche (ANR), National Research, Development and Innovation Office (NKFIH), and U.K. Research and Innovation (UKRI)(grant numbers:CHIST-ERA-20-SICT-003); Passau International Centre for Advanced Interdisciplinary Studies (PICAIS) of the University of Passau, Germany; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10546919","5G;6G;optical fronthaul;point-to-point (P2P);passive optical networks (PON);free space optics (FSO);radio access network (RAN);optical communication technologies","6G mobile communication;Optical fiber networks;Passive optical networks;Optical network units;Surveys;Optical fibers;5G mobile communication","","6","","279","IEEE","3 Jun 2024","","","IEEE","IEEE Journals"
"Toward Online Reliability-Enhanced Microservice Deployment With Layer Sharing in Edge Computing","Y. Shi; Y. Yang; C. Yi; B. Chen; J. Cai","College of Computer Science and Technology and the Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China; College of Computer Science and Technology and the Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China; College of Computer Science and Technology and the Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China; College of Computer Science and Technology and the Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China; Department of Electrical and Computer Engineering, Concordia University, Montréal, QC, Canada",IEEE Internet of Things Journal,"25 Jun 2024","2024","11","13","23370","23383","Container–based microservice provisioning, with its elasticity in terms of the layered structure, enables the sharing of common layers among different edge computing tasks, both within and across edge servers (ESs). However, due to the potential hardware breakdowns, each ES may prone to failures, affecting its lifetime (i.e., the time-length that an ES works continuously without interruptions), and in turn leading to the collapse of their hosted/provided microservices or the other ESs’ microservices requesting common layers from it. To address such an issue, in this article, we study the microservice deployment optimization with layer sharing for maximizing the system-wide reliability while satisfying all tasks’ delay requirements. Considering dynamic task generations and the asynchronization of various decision variables with different triggers, we design an online optimization algorithm by leveraging an improved Lyapunov technique integrating randomized rounding, Lagrangian method and convex optimization, which iteratively solves the problem over different timescales. Theoretical analyses and simulations evaluate the performance of the proposed solution, showing that it can achieve an increase of 12.4% in reliability and a reduction of 28.57% in total delay, compared to the counterparts.","2327-4662","","10.1109/JIOT.2024.3385816","State Key Laboratory of Massive Personalized Customization System and Technology(grant numbers:H&C-MPC-2023-04-01); National Natural Science Foundation of China (NSFC)(grant numbers:62176122); A3 Foresight Program of NSFC(grant numbers:62061146002); Postgraduate Research and Practice Innovation Program of Jiangsu Province(grant numbers:KYCX22_0372); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10494347","Edge computing;layer sharing;microservice deployment;online optimization;reliability enhancement","Microservice architectures;Task analysis;Reliability;Optimization;Loading;Delays;Containers","","6","","41","IEEE","8 Apr 2024","","","IEEE","IEEE Journals"
"Blockchain and Reinforcement Neural Network for Trusted Cloud-Enabled IoT Network","J. K. Samriya; S. Kumar; M. Kumar; M. Xu; H. Wu; S. S. Gill","Department of CSE, Indian Institute of Information Technology Sonepat, Sonepat, India; Department of Computer Engineering and Applications, GLA University, Mathura, India; Department of IT, Dr. B. R. Ambedkar National Institute of Technology, Jalandhar, India; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Center for Applied Mathematics, Tianjin University, Tianjin, China; School of Electronic Engineering and Computer Science, Queen Mary University of London, London, U.K.",IEEE Transactions on Consumer Electronics,"29 Apr 2024","2024","70","1","2311","2322","The rapid integration of Internet of Things (IoT) services and applications across various sectors is primarily driven by their ability to process real-time data and create intelligent environments through artificial intelligence for service consumers. However, the security and privacy of data have emerged as significant threats to consumers within IoT networks. Issues such as node tampering, phishing attacks, malicious code injection, malware threats, and the potential for Denial of Service (DoS) attacks pose serious risks to the safety and confidentiality of information. To solve this problem, we propose an integrated autonomous IoT network within a cloud architecture, employing Blockchain technology to heighten network security. The primary goal of this approach is to establish a Heterogeneous Autonomous Network (HAN), wherein data is processed and transmitted through cloud architecture. This network is integrated with a Reinforced Neural Network (RNN) called ClouD_RNN, specifically designed to classify the data perceived and collected by sensors. Further, the collected data is continuously monitored by an autonomous network and classified for fault detection and malicious activity. In addition, network security is enhanced by the Blockchain Adaptive Windowing Meta Optimization Protocol (BAW_MOP). Extensive experimental results validate that our proposed approach significantly outperforms state-of-the-art approaches in terms of throughput, accuracy, end-to-end delay, data delivery ratio, network security, and energy efficiency.","1558-4127","","10.1109/TCE.2023.3347690","National Natural Science Foundation of China(grant numbers:62071327,62102408); Tianjin Municipal Science and Technology Program(grant numbers:22ZYYYJC00020); Shenzhen Science and Technology Program(grant numbers:RCBS20210609104609044); Chinese Academy of Sciences President’s International Fellowship Initiative(grant numbers:2023VTC0006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10375765","Internet of Things;reinforcement neural network;heterogeneous autonomous network;cloud computing;control system","Internet of Things;Cloud computing;Blockchains;Security;5G mobile communication;Network security;Deep learning;Quality of service","","6","","40","IEEE","28 Dec 2023","","","IEEE","IEEE Journals"
"Testing the Resilience of MEC-Based IoT Applications Against Resource Exhaustion Attacks","R. Pietrantuono; M. Ficco; F. Palmieri","Università degli Studi di Napoli Federico II, Naples, Italy; Department of Computer Science, University of Salerno, Fisciano, SA, Italy; Department of Computer Science, University of Salerno, Fisciano, SA, Italy",IEEE Transactions on Dependable and Secure Computing,"13 Mar 2024","2024","21","2","804","818","Multi-access Edge Computing (MEC) is an emerging computing model that provides the necessary on-demand resources and services to the edge of the network, ensuring powerful computing, storage capacity, mobility, location, and context awareness support to emerging Internet of Things (IoT) applications. Nonetheless, its complex hierarchical model introduces new architectural interdependencies, which can influence the resilience of IoT applications against cyber attacks. Although application resilience has been investigated in the context of cloud computing, existing studies are not directly applicable to such an extended edge-cloud paradigm. The use of different enabling technologies at the edge of the network, such as various wireless access technologies and virtualization, implies several threats and challenges that make the analysis and deployment of resilience mechanisms a technically challenging problem. In this article, we first present an overview of the threat model, describing the threats for the different layers of this paradigm. We then study the impact of resource-exhausting attacks – a particularly relevant class for this paradigm - on three different IoT applications exploiting the services offered by the MEC-based architecture. We adopt a testing-based methodology conceived to characterize the resilience of such applications under attack. A set of most important resilience-related indicators are also identified. The characterization's results are useful to support the analyst in planning proper protection means at individual architectural layers.","1941-0018","","10.1109/TDSC.2023.3263137","NRRP MUR program; EU - NGEU(grant numbers:PE00000014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10086638","Internet of Things;multi-access edge computing;resilience;threat model;resource-exhausting attacks","Internet of Things;Resilience;Security;Cloud computing;Computational modeling;Computer architecture;Virtualization","","6","","53","IEEE","29 Mar 2023","","","IEEE","IEEE Journals"
"Follow the User: A Framework for Dynamically Placing Content Using 5G-Enablers","D. Santos; R. Silva; D. Corujo; R. L. Aguiar; B. Parreira","Instituto de Telecomunicações and Universidade de Aveiro, Campus Universitário de Santiago, Aveiro, Portugal; Instituto de Telecomunicações and Universidade de Aveiro, Campus Universitário de Santiago, Aveiro, Portugal; Instituto de Telecomunicações and Universidade de Aveiro, Campus Universitário de Santiago, Aveiro, Portugal; Instituto de Telecomunicações and Universidade de Aveiro, Campus Universitário de Santiago, Aveiro, Portugal; R&D Department, Altran Portugal, Vila Nova de Gaia, Portugal",IEEE Access,"25 Jan 2021","2021","9","","14688","14709","This article presents a framework for improved and efficient video delivery in scenarios featuring users moving at high speed (e.g., trains), leveraging on dynamic Multi-access Edge Computing (MEC) enabled 5G network capabilities. The framework is location-aware and it allows the content to efficiently follow the users, conserving load usage on network and computational resources, by placing virtualized Content Delivery Network (vCDN) nodes at edge sites. The nodes are controlled by the framework's centralized control unit, which is able to dynamically and preemptively deploy virtulized resources, as the train moves. The framework is capable of segmenting video content and placing the specific portion of content that a user is likely to consume across a set of dynamically deployed vCDN nodes, associated to the coverage section the train is currently passing. A proof of concept was implemented and evaluated, where the benefits of using this framework are assessed. Results showed that the proposed system was able to reduce the load on the core network by 10.9 percent and maximize the cache hit ratio to a value of 99.8 percent.","2169-3536","","10.1109/ACCESS.2021.3051570","Fundação para a Ciência e Tecnologia (FCT) / Ministério para a Educação e Ciência through the National Funds(grant numbers:PTDC/EEI-TEL/30685/2017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324846","vCDN;5G;MEC;location-awareness;video delivery optimization;NFV;SDN","5G mobile communication;Peer-to-peer computing;Servers;Monitoring;Quality of experience;Content distribution networks;Optimization","","6","","64","CCBYNCND","14 Jan 2021","","","IEEE","IEEE Journals"
"Next Generation Platform as a Service: Toward Virtualized DVB-RCS2 Decoding System","R. Kerherve; J. Lallet; L. Beaulieu; I. Fajjari; P. Veitch; J. Dion; B. Sayadi; L. Roullet","b<>com, Rennes, France; Nokia Bell Labs, Nozay, France; b<>com, Rennes, France; Orange Labs, Châtillon, France; British Telecom, Ipswich, U.K.; b<>com, Rennes, France; Nokia Bell Labs, Nozay, France; Nokia Bell Labs, Nozay, France",IEEE Transactions on Broadcasting,"6 Jun 2019","2019","65","2","425","433","Platform-as-a-Service (PaaS) cloud computing services have emerged in the last few years, offering an abstraction framework to enable the development, execution, and management of applications, which is de-coupled from complex infrastructure considerations. The next generation of PaaS solutions (NGPaaS) will have to address more stringent performance requirements compared to traditional PaaSes. They will have to support Telco-specific requirements in terms of resource efficiency, availability, and resilience. According to the 5G promise of convergence between vertical and Telco markets, they will also have to be able to address new use case scenarios. One of the possible scenarios to address could be the combining of Telco and vertical components as digital video broadcasting (DVB). This paper demonstrates how NGPaaS features enable the implementation of 5G-oriented connectivity services in cloud data centers. Regarding the convergence aspect, it aims to demonstrate how NGPaaS features facilitate the build of a combined Telco-broadcasting PaaS system on the cloud where a DVB-RCS2 system is able to bring its own components and integrate them with the connectivity services deployed on the PaaS by a Telco operator. This is achieved by applying specific customizations and enhancements to Kubernetes.","1557-9611","","10.1109/TBC.2019.2901392","H2020 European Institute of Innovation and Technology(grant numbers:761557); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8666151","Broadcasting;DVB-S;FPGA;cloud;microservice;kubernetes;acceleration","Cloud computing;Digital video broadcasting;5G mobile communication;Containers;Virtualization;Decoding;Field programmable gate arrays","","6","","26","IEEE","12 Mar 2019","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Containerized Edge Intelligence Inference Request Processing in IoT Edge Computing","L. Nkenyereye; K. -J. Baeg; W. -Y. Chung","AI Convergence Education & Research Group, Pukyong National University, Busan, South Korea; Department of Nanotechnology Engineering, Pukyong National University, Busan, South Korea; Department of AI Convergence & Department of Electronic Engineering, Pukyong National University, Busan, South Korea",IEEE Transactions on Services Computing,"14 Dec 2023","2023","16","6","4328","4344","Edge intelligence (EI) refers to a set of connected systems and devices for artificial intelligence (AI) data collected and learned near the data collection site. The EI model inference phase has been improved through edge caching technologies such as intelligent models (IMs). IM inference across heterogeneously distributed edge nodes is worthy of discussion. The present focuses on software-defined infrastructure (SDI) and introduces a containerized EI framework for a mobile wearable Internet-of-Things (IoT) system. This framework, called the containerized edge intelligence framework (CEIF), is an inter-working architecture that allows the provisioning of containerized EI processing intelligent services related to mobile wearable IoT systems. CEIF enables dynamic instantiation of the inference services of AI models that have been pre-trained on clouds. It also accommodates edge computing devices (ECDs) running the container virtualization technique. Dynamic AI learning policies can also help with workload optimization, thereby reducing the response time of the requests of the EI inference. To stall the rapid increase in user workload when inferring the collected data for analysis, we then propose a deep q-learning algorithm in which the container cluster platform learns the varying user workload at the location of each ECD. The requests of the EI inference are scaled with the learned value and are processed successfully without overloading the ECD. When evaluated in a case study, the proposed algorithm enabled scaling of the processing requests of the EI inference in a containerized EI system while minimizing the number of instantiated container EI instances. The EI inference's requests are completed in an under-loaded container EI cluster system.","1939-1374","","10.1109/TSC.2023.3320752","Ministry of Science and Technology; National Research Foundation of Korea(grant numbers:NRF-2019R1A2C1089139); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10268016","Internet of Things;IoT service layer;IoT edge computing;deep reinforcement learning;edge intelligence;artificial intelligence;AI model inference","Artificial intelligence;Containers;Computational modeling;Inference algorithms;Edge computing;Data models;Clustering algorithms","","6","","44","IEEE","29 Sep 2023","","","IEEE","IEEE Journals"
"Band-Area Resource Management Platform and Accelerated Particle Swarm Optimization Algorithm for Container Deployment in Internet-of-Things Cloud","M. Ouyang; J. Xi; W. Bai; K. Li","School of Software Engineering, South China University of Technology, Guangzhou, China; School of Software Engineering, South China University of Technology, Guangzhou, China; School of Computer Science, Zhaoqing University, Zhaoqing, China; Department of Computer Science, State University of New York, New Paltz, NY, USA",IEEE Access,"24 Aug 2022","2022","10","","86844","86863","The method of building and deploying applications through the combination of container virtualization technology and a microservices framework has been widely used in Internet-of-Things clouds. However, there are gaps and a lack of coordination mechanisms between the Internet-of-Things and cloud computing. This study constructs a resource management platform, which is based on application container virtualization technology and combined with the microservices framework. The platform provide a support environment for the construction and deployment of Internet-of-Things cloud applications. However, there is no unified specification for the microservices templates. Therefore, a new service model called tool service was designed. The invocation relationship between services is studied, and developers can combine services through the invocation relationship between services to form a service function chain. However, container-based service deployment remains an unresolved issue. The deployment method of a container involves the quality of service of end users and the profit of cloud providers. To balance the profits of both parties, it is necessary to minimize the service response time and improve the resource utilization of the cloud data center. To address this problem, an accelerated particle swarm optimization strategy is proposed to realize service deployment. Through the invocation relationship between services, the execution containers are aggregated, so as to reduce the service transmission overhead and improve resource utilization. Compared with the experimental results of existing deployment strategies, the proposed optimization strategy has significantly improved performance parameters such as service transmission overhead, container aggregation, and resource utilization.","2169-3536","","10.1109/ACCESS.2022.3198971","Science and Technology Plan Project of Guangdong Province, China(grant numbers:2014B010112007,2016B010124010); Guangdong Province Educational Science Planning Project(grant numbers:2019KTSCX199); Zhaoqing Science and Technology Special Fund Project(grant numbers:2020G1004); Zhaoqing University Science and Technology Projects(grant numbers:zlgc201933); Teaching Reform Project of University Public Computer Course of Guangdong Province(grant numbers:2021-GGJGJ-012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857907","Accelerated particle swarm optimization;cloud computing;container;Internet-of-things;microservices;multi-objective optimization","Containers;Cloud computing;Internet of Things;Microservice architectures;Resource management;Optimization;Virtualization;Particle swarm optimization","","6","","46","CCBY","16 Aug 2022","","","IEEE","IEEE Journals"
"Service-Oriented Resource Allocation for Blockchain-Empowered Mobile Edge Computing","A. Zhou; S. Li; X. Ma; S. Wang","State Key Laboratory of Network and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Network and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Network and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Network and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China",IEEE Journal on Selected Areas in Communications,"23 Nov 2022","2022","40","12","3391","3404","Integrating dense small cell (DSC) networks with mobile edge computing is employed by 5G to tackle the contradiction between the computation limitations of user equipment (UE) and the stringent latency requirement of services. This paper investigates the service-oriented edge resource allocation problem in DSC networks, determining where to deploy the service entity, how many service entities should be deployed at each edge cloud, and how to assign the UEs to service entities. The problem is challenging for the following three aspects: 1) Service entity deployment and UE assignment are highly coupled. 2) Due to the overlap of coverage regions of densely deployed small cells, the allocation mechanism of different base stations has mutual effects on the overall service performance. 3) Considering the limited resources of edge clouds, it is a thorny problem to encourage edge clouds to cache and share service startup images. We devote the following efforts to tackle the problem under these challenges. First, we explore blockchain’s decentralized, traceable, and secure characteristics, and propose a scheme to encourage image sharing in mobile edge computing. Second, we formulate the service-oriented edge resource allocation as mixed integer non-linear programming. Third, towards the target of reducing the computational complexity, we decouple UE assignment from service entity deployment and solve it through Gibbs sampling. Moreover, the power of Lyapunov optimization and convex optimization is incorporated to reduce the long-term power consumption and budget. Experiment results demonstrate the superiority of our approach over current notable solutions.","1558-0008","","10.1109/JSAC.2022.3213343","National Key Research and Development Program of China(grant numbers:2020YFB1805502); National Natural Science Foundation of China (NSFC)(grant numbers:U21B2016,61922017,62032003,61921003,61902036); Beijing University of Posts and Telecommunications (BUPT) Excellent Ph.D. Students Foundation(grant numbers:CX2022226); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9915530","Mobile edge computing;blockchain;image sharing;user equipment assignment;service entity deployment","Image edge detection;Resource management;Blockchains;Task analysis;5G mobile communication;Optimization;Peer-to-peer computing;Microcell networks","","6","","36","IEEE","10 Oct 2022","","","IEEE","IEEE Journals"
"Energy-Aware Self-Adaptation for Application Execution on Heterogeneous Parallel Architectures","R. Kavanagh; K. Djemame; J. Ejarque; R. M. Badia; D. Garcia-Perez","School of Computing, University of Leeds, Leeds, United Kingdom; School of Computing, University of Leeds, Leeds, United Kingdom; Barcelona Supercomputing Center (BSC), Barcelona, Spain; Barcelona Supercomputing Center (BSC), Barcelona, Spain; Atos, Madrid, Spain",IEEE Transactions on Sustainable Computing,"6 Mar 2020","2020","5","1","81","94","Hardware in High Performance Computing environments in recent years have increasingly become more heterogeneous in order to improve computational performance. An additional aspect of such systems is the management of power and energy consumption. The increase in heterogeneity requires middleware and programming model abstractions to eliminate additional complexities that it brings, while also offering opportunities such as improved power management. In this paper, we explore application level self-adaptation including aspects such as automated configuration and deployment of applications to different heterogeneous infrastructure and for their redeployment. This therefore not only mitigates complexities associated with heterogeneous devices but aims to take advantage of the heterogeneity. The overall result of this paper is a self-adaptive framework that manages application Quality of Service (QoS) at runtime, which includes the automatic migration of applications between different accelerated infrastructures. Discussion covers when this migration is appropriate and quantifies the likely benefits.","2377-3782","","10.1109/TSUSC.2019.2912000","European Commission(grant numbers:687584); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8693866","Self-adaptation;energy modelling;programming model;heterogeneous hardware architectures;application deployment","Task analysis;Runtime;Computer architecture;Quality of service;Hardware;Monitoring;Power demand","","6","","31","Crown","18 Apr 2019","","","IEEE","IEEE Journals"
"An IoT Toolchain Architecture for Planning, Running and Managing a Complete Condition Monitoring Scenario","F. Montori; I. Zyrianoff; L. Gigli; A. Calvio; R. Venanzi; S. Sindaco; L. Sciullo; F. Zonzini; M. Zauli; N. Testoni; A. Bertacchini; E. Londero; E. Alessi; M. D. Felice; L. Bononi; P. Bellavista; L. De Marchi; A. Marzani; P. Azzoni; T. S. Cinotti","Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Advanced Research Center on Electronic Systems “Ercole De Castro”, University of Bologna, Bologna, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Advanced Research Center on Electronic Systems “Ercole De Castro”, University of Bologna, Bologna, Italy; Advanced Research Center on Electronic Systems “Ercole De Castro”, University of Bologna, Bologna, Italy; Advanced Research Center on Electronic Systems “Ercole De Castro”, University of Bologna, Bologna, Italy; Department of Science and Engineering Methods, University of Modena and Reggio Emilia, Modena, Italy; Eurotech S.p.A., Udine, Italy; Advanced Research and System Platforms RND, Analog, MEMS and Sensors Group, STMicroelectronics, Catania, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Advanced Research Center on Electronic Systems “Ercole De Castro”, University of Bologna, Bologna, Italy; Advanced Research Center on Electronic Systems “Ercole De Castro”, University of Bologna, Bologna, Italy; Eurotech S.p.A., Udine, Italy; Advanced Research Center on Electronic Systems “Ercole De Castro”, University of Bologna, Bologna, Italy",IEEE Access,"24 Jan 2023","2023","11","","6837","6856","Condition Monitoring (CM) is an extremely critical application of the Internet of Things (IoT) within Industry 4.0 and Smart City scenarios, especially following the recent energy crisis. CM aims to monitor the status of a physical appliance over time and in real time in order to react promptly when anomalies are detected, as well as perform predictive maintenance tasks. Current deployments suffer from both interoperability and management issues within their engineering process at all phases– from their design to their deployment, to their management–, often requiring human intervention. Furthermore, the fragmentation of the IoT landscape and the heterogeneity of IoT solutions hinder a seamless onboarding process of legacy devices and systems. In this paper, we tackle these problems by first proposing an architecture for CM based on both abstraction layers and toolchains, i.e., automated pipelines of engineering tools aimed at supporting the engineering process. In particular, we introduce four different toolchains, each of them dedicated to a well-defined task (e.g., energy monitoring). This orthogonal separation of concerns aims to simplify both the understanding of a complex ecosystem and the accomplishment of independent tasks. We then illustrate our implementation of a complete CM system that follows said architecture as a real Structural Health Monitoring (SHM) pilot of the Arrowhead Tools project, by describing in detail every single tool that we developed. We finally show how our pilot achieves the main objectives of the project: the reduction of engineering costs, the integration of legacy systems, and the interoperability with IoT frameworks.","2169-3536","","10.1109/ACCESS.2023.3237971","Electronic Components and Systems for European Leadership, ECSEL, Joint Undertaking(grant numbers:826452); Arrowhead Tools European Union Horizon 2020 Research and Innovation Program; Istituto nazionale Assicurazione Infortuni sul Lavoro (INAIL)(grant numbers:BRIC/2018); ID=11 Framework, under Project MAC4PRO; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10019255","IoT;WoT;condition monitoring;SHM;arrowhead;toolchain","Computer architecture;Sensors;Monitoring;Interoperability;Condition monitoring;Costs;Internet of Things","","6","","45","CCBY","18 Jan 2023","","","IEEE","IEEE Journals"
"A Comprehensive Survey on Software as a Service (SaaS) Transformation for the Automotive Systems","D. F. Blanco; F. Le Mouël; T. Lin; M. -P. Escudié","Univ Lyon, INSA Lyon, Inria, CITI, EA3720, Villeurbanne, France; Univ Lyon, INSA Lyon, Inria, CITI, EA3720, Villeurbanne, France; STELLANTIS, Velizy-Villacoublay, France; Univ Lyon, INSA Lyon, Institut Gaston Berger, Villeurbanne, France",IEEE Access,"25 Jul 2023","2023","11","","73688","73753","Over the last few decades, automotive embedded Information and Communication Technology (ICT) systems have been used to enhance vehicle performance and enrich peopleâ’s driving experience, increasing the panel of software features within them. However, even though until now automakers have kept up with the innovation pace in terms of the functionalities that have been offered to passengers, the majority of automakers’ efforts have concentrated on bringing in these new functionalities by adding an unceasingly larger set of ECUs. All of this has been done without evolving any of the embedded software architecture consequently, due to budgetary constraints, legislative limitations, retro-compatibility problems, and a lack of awareness of the trending IT innovation. This unbalanced progress has then led to a substantial increase in in-vehicle architectural complexity, which has become a major concern for automakers nowadays as it makes the vehicle repairing process more complex, decreases software traceability and clashes with the objective of having higher business flexibility, modularity, and dynamicity within the vehicles. In this paper, we are going to go through literature, both academic and industrial, and propose a comprehensive study into automotive system transformation. We begin by giving a detailed analysis of the causes of evolution under five axes - i.e., society, business, industry, application, and technical. Then, we discuss the convergence of cars and software life cycles and propose a three-layered analysis of automotive ICT systems consisting of architecture design, software pipelines, and run-time management. Finally, we are going to propose certain detailed guidelines on the evolution perspectives for automotive systems through deriving from the convergence of advances in IT, as well as current and future automotive environmental constraints.","2169-3536","","10.1109/ACCESS.2023.3294256","Stellantis under the collaborative framework OpenLab VAT@Lyon, involving STELLANTIS and CITI Laboratory (Association Nationale Recherche Technologie (ANRT)(grant numbers:n°2020/1415); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10177956","Automotive ICT system;software defined vehicles;system architecture;vehicle-to-cloud (V2C)/vehicle-to-anything (V2X);software pipelines;E/E architecture","Automotive engineering;Computer architecture;Business;Technological innovation;Pipelines;Convergence;Information and communication technology;Software defined networking;System analysis and design;Vehicle-to-everything;Cloud computing","","6","","710","CCBY","11 Jul 2023","","","IEEE","IEEE Journals"
"Privacy-Preserving and Secure Industrial Big Data Analytics: A Survey and the Research Framework","L. Liu; J. Li; J. Lv; J. Wang; S. Zhao; Q. Lu","Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Institute of Computing Technology Chinese Academy of Sciences, South China University of Technology, Guangzhou, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China",IEEE Internet of Things Journal,"22 May 2024","2024","11","11","18976","18999","The development of the Industrial Internet will generate a large amount of valuable data, known as industrial big data (IBD). By mining and utilizing IBD, enterprises can improve production efficiency, reduce costs and risks, optimize management processes, and innovate services and business models. However, IBD comes from various institutions in all walks of life and has features such as multisource, heterogeneity, and multimodality. And data sharing and trading (DS&T) occur in the Industrial Internet environment without mutual trust. These characteristics pose new challenges to analytics methods and privacy and security protection technologies. Therefore, this article aims to provide references for privacy-preserving and secure industrial big data analytics (IBDA) from three perspectives: 1) research framework; 2) platform architecture; and 3) key technologies. First, we review the current state of research on theories and technologies related to IBDA. Then, we reveal three challenges to secure and efficient IBDA. We take the analytics and utilization of IBD as systematic engineering, propose the research framework for privacy-preserving and secure IBDA, and point out the specific content to be studied. Further, we design the architecture of the IBDA platform with the idea of layering, including a function model, security architecture, and system architecture. Finally, detailed research proposals and potential technologies for IBD analytics and utilization are presented from three aspects: 1) data fusion and analytics; 2) data privacy and security protection; and 3) blockchain.","2327-4662","","10.1109/JIOT.2024.3353727","National Science Foundation of China(grant numbers:51977155); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10399957","Blockchain;data analytics;data sharing and trading (DS&T);federated learning (FL);industrial big data (IBD);privacy and security","Big Data;Security;Data privacy;Internet;Blockchains;Computer architecture;Privacy","","5","","124","IEEE","15 Jan 2024","","","IEEE","IEEE Journals"
"Applying a Consumer-Centric Framework for Trust Assessment of Cloud Computing Service Providers","A. Balcão-Filho; N. Ruiz; F. d. F. Rosa; R. Bonacin; M. Jino","The CTI Renato Archer, Campinas, Brazil; The CTI Renato Archer, Campinas, Brazil; The CTI Renato Archer, Campinas, Brazil; The CTI Renato Archer, Campinas, Brazil; UNICAMP, Campinas, Brazil",IEEE Transactions on Services Computing,"6 Feb 2023","2023","16","1","95","107","Cloud computing services consumers do not have enough reliable information about critical characteristics of their providers, such as performance, security, trust and privacy, compliance with laws and regulations, among others. Our proposal addresses these problems presenting a trust assessment framework that integrates three domains: Governance, Transparency, and Security Information. Our approach is consumer-centric and deals with trust aspects from the end-user’s perspective. We use Indicators to communicate the outcomes, which aim to represent the expression of cybersecurity, manageability, and transparency of services under assessment. This paper includes an implementation proposal, prototype, and proof of concept, in which the framework was applied in a real scenario and executed over a long-term (18 months) usage simulation to verify its applicability, sensitivity, and robustness. Our study is intended for use by consumers of cloud computing who seek to know and measure levels of cybersecurity, protection of privacy, transparency of security, and high levels of quality in their services and infrastructure.","1939-1374","","10.1109/TSC.2021.3134125","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645168","Cloud computing;trust assessment;metrics;consumer-centric;security;privacy;framework","Cloud computing;Security;Privacy;Computational modeling;Measurement;Bibliographies;Taxonomy","","5","","53","IEEE","9 Dec 2021","","","IEEE","IEEE Journals"
"From 5G-Advanced to 6G in 2030: New Services, 3GPP Advances, and Enabling Technologies","R. Giuliano","Department of Engineering Science, Guglielmo Marconi University, Rome, Italy",IEEE Access,"8 May 2024","2024","12","","63238","63270","The telecommunications systems are in continuous evolution. After voice, video, mobile internet, and Internet of Things, what services will be supported in the near future? In the paper, three envisioned services are highlighted, which will be provided in the coming years by new telecommunication systems: immersive communications, everything connected, and high-positioning. The author provides a comprehensive description of their characteristics and investigates the developments that will be implemented in 3GPP Releases 17, Release 18, and Release 19, including technologies that could be integrated for supporting the three new services. In order to evaluate the performance of the new technologies and services, it is important to define appropriate Key Performance Indicators (KPIs). The paper reports and proposes new KPIs for network evaluation to support specific new services such as virtual/mixed reality, smart sensors, and gesture recognition, then facilitating the effective design of the next-generation network and its performance assessment optimally. Requirements of the major application fields that will see widespread adoption in the next 3–8 years due to these developments are also investigated. Finally, the paper further outlines the most promising enabling technologies, supporting the three bearer services.","2169-3536","","10.1109/ACCESS.2024.3396361","Guglielmo Marconi University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10517597","5G-advanced;6G;immersive communications;everything connected;high-positioning;3GPP enhancements;Release 18;Release 19","Telecommunications;6G mobile communication;3GPP;5G mobile communication;X reality;Smart phones;Next generation networking","","5","","174","CCBYNCND","2 May 2024","","","IEEE","IEEE Journals"
"Digital Twins for Smart Spaces—Beyond IoT Analytics","N. Hossein Motlagh; M. A. Zaidan; L. Lovén; P. L. Fung; T. Hänninen; R. Morabito; P. Nurmi; S. Tarkoma","Department of Computer Science, University of Helsinki, Helsinki, Finland; Department of Computer Science, University of Helsinki, Helsinki, Finland; Center for Ubiquitous Computing, University of Oulu, Oulu, Finland; Department of Computer Science, University of Helsinki, Helsinki, Finland; Centre for Wireless Communications, University of Oulu, Oulu, Finland; Department of Computer Science, University of Helsinki, Helsinki, Finland; Department of Computer Science, University of Helsinki, Helsinki, Finland; Department of Computer Science, University of Helsinki, Helsinki, Finland",IEEE Internet of Things Journal,"27 Dec 2023","2024","11","1","573","583","Smart spaces, physical spaces that are integrated with sensor-enabled Internet of Things devices, are a powerful paradigm for optimizing the operations of the space and improving its quality for the occupants. Managing the applications and services running in the space is a complex task as the operations of the devices and services are dependent on the physical characteristics of the space, the occupants of the space, and the technologies that are being integrated. Digital twinning, the combination of physical representations with a virtual counterpart, is a potential technology for facilitating the management of smart space devices and services. While digital twins are increasingly adopted in industry, their use in everyday environments remains low due to difficulties in creating and linking the virtual representation with the physical environment. In this article, we propose our vision for the adoption of digital twinning as a pathway to improve the functions of smart spaces. We derive a generic reference architecture that comprises four layers, covering the physical space, the sensing infrastructure, the network interfaces, and the underlying computational infrastructure. Next, we identify and address key requirements for the uptake of digital twins in smart spaces and assess their benefits using the ascendancy model of business analytics. Finally, to demonstrate the practicality of digital twinning, we present a proof-of-concept digital twin for the TellUs smart space at the University of Oulu in Finland and use it to highlight the potential benefits of different ascendancy levels.","2327-4662","","10.1109/JIOT.2023.3287032","Nokia Center for Advanced Research (NCAR); Business Finland Project(grant numbers:8782/31/2022,8754/31/2022); Academy of Finland(grant numbers:335934,345008,339614,355330,346208); Helsinki Institute for Information Technology (HIIT)(grant numbers:75233229); European Union’s Horizon 2020 Research and Innovation Programme the EMME-CARE Project(grant numbers:856612); ECSEL JU FRACTAL Project(grant numbers:877056); Finland Centennial Foundation to Urban Air Quality 2.0 Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10153976","Digital twin;Internet of Things (IoT);long range (LoRa) networks;sensor networks;smart spaces","Sensors;Intelligent sensors;Smart spaces;Digital twins;Wireless communication;Internet of Things;Wireless sensor networks","","5","","26","CCBY","16 Jun 2023","","","IEEE","IEEE Journals"
"Metarobotics for Industry and Society: Vision, Technologies, and Opportunities","E. G. Kaigom","Faculty of Computer Science and Engineering, Frankfurt University of Applied Sciences, Frankfurt, Germany",IEEE Transactions on Industrial Informatics,"4 Apr 2024","2024","20","4","5725","5736","Metarobotics aims to combine next generation wireless communication, multisense immersion, and collective intelligence to provide a pervasive, itinerant, and noninvasive access and interaction with distant robotized applications. Industry and society are expected to benefit from these functionalities. For instance, robot programmers will no longer travel worldwide to plan and test robot motions, even collaboratively. Instead, they will have a personalized access to robots and their environments from anywhere, thus spending more time with family and friends. Students enrolled in robotics courses will be taught under authentic industrial conditions in real-time. This article describes objectives of metarobotics in society, industry, and in-between. It identifies and surveys technologies likely to enable their completion and provides an architecture to put forward the interplay of key components of metarobotics. Potentials for self-determination, self-efficacy, and work-life-flexibility in robotics-related applications in Society 5.0, Industry 4.0, and Industry 5.0 are outlined.","1941-0050","","10.1109/TII.2023.3337380","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10361602","6G;collective intelligence;digital twins (DT);holoportation;Industry 4.0/5.0;metaverse;robotics;Society 5.0","Service robots;Industries;Metaverse;Surveys;Robot sensing systems;Collaboration;6G mobile communication;Fifth Industrial Revolution","","5","","80","IEEE","15 Dec 2023","","","IEEE","IEEE Journals"
"Reference Architecture for Running Computationally Intensive Physics-Based Digital Twins of Heavy Equipment in a Heterogeneous Execution Environment","V. Zhidchenko; E. Startcev; H. Handroos","Mechanical Engineering Department, LUT University, Lappeenranta, Finland; Mechanical Engineering Department, LUT University, Lappeenranta, Finland; Mechanical Engineering Department, LUT University, Lappeenranta, Finland",IEEE Access,"1 Jun 2022","2022","10","","54164","54184","Physics-based digital twins for heavy equipment provide a powerful tool for improving operation and maintenance activities. In contrast to data-driven models, they present more explainable and confident results but require more computational power. Besides the problem of physics-based digital twins creation, there is a task of managing their lifecycle, including their execution, maintenance, storage, and updating. The features distinguishing this kind of digital twins are the mobility of the real counterpart, operation in remote locations, long lifecycle, information sensitivity, and gaps in information technology awareness among the equipment owners and users. This paper presents a methodology and reference architecture for a set of interconnected systems capable of running digital twins of heavy equipment in such conditions. A data model for preserving digital twin-related information for decades of machine operation is described. Operating-system-level virtualization technologies are used to run digital twins in a heterogeneous execution environment. An example of the reference architecture implementation is presented for the physics-based digital twins of a mobile log crane. The experimental part of the paper includes a comparison of computing time for different types of digital twins in different execution environments. It highlights the peculiarities related to running physics-based digital twins in containers. Experiments were performed using the Amazon cloud platform, an edge computing system represented by a single-board microcomputer based on ARM architecture, and a virtual machine on a desktop personal computer. Experimental results show that physics-based digital twins for the analysis of the multi-body dynamics can be run within the proposed architecture with real-time performance in all three types of execution environments. The paper demonstrates the practical implementation of physics-based digital twins for heavy equipment and defines directions for future research in this field.","2169-3536","","10.1109/ACCESS.2022.3176645","Business Finland through the Project “Service Business from Physics-Based Digital Twins—DigiBuzz; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779257","Cloud computing;data model;digital twin;Internet of Things;simulation;virtualization","Digital twins;Data models;Maintenance engineering;Computer architecture;Computational modeling;Internet of Things;Software","","5","","77","CCBY","20 May 2022","","","IEEE","IEEE Journals"
"A Hybrid Latency- and Power-Aware Approach for Beyond Fifth-Generation Internet-of-Things Edge Systems","A. Kaushik; H. S. Al-Raweshidy","Department of Computer Science and Engineering, SRM University, Sonepat, Haryana, India; Department of Electronic and Electrical Engineering, Brunel University London, Uxbridge, London, U.K.",IEEE Access,"29 Aug 2022","2022","10","","87974","87989","Fifth-generation (5G) empowered internet of things (IoT) edge networks suffer from latency in delay-sensitive applications. To fulfil the low latency requirements of beyond fifth-generation (B5G)-IoT applications and provide quality of service (QoS) to IoT-edge communication, it is important to minimize server delay. Furthermore, 5G-IoT systems consume more power than their predecessors, which is a concern given the growing size of future IoT networks. This research presents a hybrid latency and power-aware approach for B5G-IoT networks (HLPA B5G-IoT) that minimizes latency with minimum overhead on battery-constrained IoT nodes while simultaneously providing a power-efficient solution for B5G-IoT-edge networks. HLPA B5G-IoT has a novel algorithm classifier tool (ACT) for selecting appropriate optimization algorithms based on the characteristics and requirements of B5G-IoT systems. The ACT matrix not only parametrically compares HLPA B5G-IoT with existing approaches but also identifies crucial parameters that enable algorithm selection for load balancing and energy efficiency. In this paper, metaheuristic algorithms, i.e., biogeography-based optimization (BBO) and grey wolf optimization (GWO), are tailored to meet the requirements of load balancing and power efficiency in IoT-edge systems. The proposed load-balancing algorithm reduces latency and improves overall network performance by 33.33%, 27.45%, 23.52%, 21.56%, 13.72%, 11.76%, and 7.84% compared with simulated annealing (SA), genetic algorithm (GA), particle swarm optimization (PSO), bacteria foraging algorithm (BFA), ant colony optimization (ACO), bat algorithm (BA), and genetic SA PSO (GSP), respectively. The power-efficiency algorithm consumes 46.6%, 40%, 32.2%, 27.7%, 15.5%, 11.1%, and 6.6% less energy compared with SA, GA, PSO, BFA, ACO, BA, and GSP, respectively.","2169-3536","","10.1109/ACCESS.2022.3200035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9862983","Beyond fifth-generation;edge computing;Internet of Things;latency;load balancing;power consumption;workload allocation","Internet of Things;Task analysis;Resource management;Cloud computing;Metaheuristics;5G mobile communication;Servers;Edge computing;6G mobile communication;Energy consumption","","5","","68","CCBY","18 Aug 2022","","","IEEE","IEEE Journals"
"Enabling Kubernetes Orchestration of Mixed-Criticality Software for Autonomous Mobile Robots","F. Lumpp; F. Fummi; H. D. Patel; N. Bombieri","Department of Engineering for Innovation Medicine, University of Verona, Verona, Italy; Department of Engineering for Innovation Medicine, University of Verona, Verona, Italy; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada; Department of Engineering for Innovation Medicine, University of Verona, Verona, Italy",IEEE Transactions on Robotics,"18 Dec 2023","2024","40","","540","553","Containerization and orchestration have become two key requirements in software development best practices. Containerization allows for better resource utilization, platform-independent development, and secure deployment of software. Orchestration automates the deployment, networking, scaling, and availability of containerized workloads and services. While containerization is increasingly being adopted in the robotic community, the use of task orchestration platforms (e.g., Kubernetes) is still an open challenge. The biggest limitation is due to the fact that state-of-the-art orchestrators do not support real-time (RT) containers, while advanced robotic software often consists of a mix of heterogeneous tasks (i.e., ROS nodes) with different levels of temporal constraints (i.e., mixed-criticality systems). This work addresses this challenge by presenting RT-Kube, a platform that extends the de-facto reference standard for container orchestration, Kubernetes, to schedule tasks with mixed-criticality requirements. It implements monitoring of tasks and detects missed deadlines for those with RT constraints. It selects low-priority tasks to be migrated at runtime to different units of the computing cluster to free resources and recover from temporal violations. We present quantitative experimental results on the software implementing the mission of a Robotnik RB-Kairos mobile robot to demonstrate the effectiveness of the proposed approach. The source code is publicly available on GitHub.","1941-0468","","10.1109/TRO.2023.3334642","European Union Next-GenerationEU; Piano Nazionale di Ripresa e Resilienza; Missione 4 Componente 2, Investimento 1.5 – D.D.(grant numbers:1058 23/06/2022); PNRR; iNEST; Interconnected North-Est Innovation Ecosystem(grant numbers:ECS_00000043); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10323202","Autonomous mobile robots;containers;edge cloud;Kubernetes;mixed-criticality systems (MCSs);orche- stration;real time (RT)","Real-time systems;Software;Containers;Standards;Autonomous robots;Mobile robots;Edge computing","","5","","50","CCBY","20 Nov 2023","","","IEEE","IEEE Journals"
"An Efficient and Autonomous Planning Scheme for Deploying IoT Services in Fog Computing: A Metaheuristic-Based Approach","Z. Lin; L. Lu; J. Shuai; H. Zhao; A. Shahidinejad","School of Computer Science and Information Security, Guilin University of Electronic Technology, Guilin, China; Library, Guilin University of Electronic Technology, Guilin, China; School of Computer Science and Information Security, Guilin University of Electronic Technology, Guilin, China; School of Information and Communication, Guilin University of Electronic Technology, Guilin, China; School of Information Technology, Deakin University, Geelong, VIC, Australia",IEEE Transactions on Computational Social Systems,"25 Jan 2024","2024","11","1","1415","1429","The fog computing paradigm is a promising concept to overcome the exponential increase in data volume in Internet of Things (IoT) applications. This paradigm can support delay-sensitive IoT applications by extending cloud services to the network edge. However, fog computing faces challenges such as resource allocation for applications at the network edge due to limited resources as well as its heterogeneous and distributed nature. This is in line with the goals of microservice architecture and develops the placement of microservice-based IoT applications. The IoT service placement problem (SPP) on fog nodes is known as non-deterministic polynomial-time (NP)-hard. In this study, we introduce a meta-heuristic approach named SPP-differential evolution algorithm (DEA) to handle SPP, which originates from the DEA with a shared parallel architecture. The proposed method takes advantage of the scalable and deployable nature of microservices to minimize the resource utilization and delay as much as possible. SPP-DEA is developed based on monitoring, analysis, decision-making, and execution with knowledge bas (MADE-k) autonomous planning model with the aim of compromise between service cost, response time, resource utilization, and throughput. In order to address the computational complexity of the problem, we consider the resource consumption distribution and service deployment priority in the placement process. In order to evaluate the quality of placement in SPP-DEA, extensive experiments have been performed on a synthetic fog environment. The simulation results show that compared to the state-of-the-art approaches, SPP-DEA reduces the service cost and waiting time by 16% and 11%, respectively.","2329-924X","","10.1109/TCSS.2023.3254922","Natural Science Foundation of Jiangsu Province(grant numbers:BK20180209); Natural Science Foundation of the Jiangsu Higher Education Institutions of China(grant numbers:18KJD520004); Suzhou Vocational Institute of Industrial Technology(grant numbers:2019kyqd018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10097451","Differential evolution algorithm (DEA);fog computing;Internet of Things (IoT);meta-heuristics;microservice architecture;service placement","Internet of Things;Edge computing;Computational modeling;Cloud computing;Metaheuristics;Quality of service;Quality of experience","","5","","36","IEEE","7 Apr 2023","","","IEEE","IEEE Journals"
"Next Generation Edge-Cloud Continuum Architecture for Structural Health Monitoring","L. Gigli; I. Zyrianoff; F. Zonzini; D. Bogomolov; N. Testoni; M. D. Felice; L. De Marchi; G. Augugliaro; C. Mennuti; A. Marzani","Advanced Research Center on Electronic Systems for Information and Communication Technologies “Ercole De Castro, University of Bologna, Bologna, Italy; Advanced Research Center on Electronic Systems for Information and Communication Technologies “Ercole De Castro, University of Bologna, Bologna, Italy; Advanced Research Center on Electronic Systems for Information and Communication Technologies “Ercole De Castro, University of Bologna, Bologna, Italy; Advanced Research Center on Electronic Systems for Information and Communication Technologies “Ercole De Castro, University of Bologna, Bologna, Italy; Advanced Research Center on Electronic Systems for Information and Communication Technologies “Ercole De Castro, University of Bologna, Bologna, Italy; Advanced Research Center on Electronic Systems for Information and Communication Technologies “Ercole De Castro, University of Bologna, Bologna, Italy; Advanced Research Center on Electronic Systems for Information and Communication Technologies “Ercole De Castro, University of Bologna, Bologna, Italy; INAIL Technological Innovations Department, Rome, Italy; INAIL Technological Innovations Department, Rome, Italy; Department of Civil, Chemical, Environmental, and Materials Engineering, University of Bologna, Bologna, Italy",IEEE Transactions on Industrial Informatics,"4 Apr 2024","2024","20","4","5874","5887","Assessing the integrity of industrial and civil appliances has become a priority worldwide. Noteworthy, this goal requires a strong synergy between multiple tools, disciplines, and approaches to be attained via a joint hardware-software co-design of the different Structural Health Monitoring (SHM) system components. This work proposes the $\sf{MAC4PRO}$ architecture, a sensor-to-cloud monitoring platform that seamlessly integrates sensing and software technologies for accurate data measurement, transmission, and analysis. The developed solution stands out for its interoperability and versatility, making it a promising candidate for integration in the next generation of smart structures. Our platform was validated during extensive experimental campaigns targeted at various industrial scenarios. The results show that the $\sf{MAC4PRO}$ architecture can identify subtle changes, such as 1mm size leakage events in pipeline circuits, or less than 1% frequency drifts in civil buildings after seismic excitation, while ensuring more than 90% reduction in the edge-to-cloud data transfer process.","1941-0050","","10.1109/TII.2023.3337391","Istituto Nazionale per l'Assicurazione Contro Gli Infortuni sul Lavoro; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10373409","Edge-cloud continuum;Internet of Things;interoperability;structural health monitoring","Computer architecture;Monitoring;Sensors;Cloud computing;Software;Vibrations;Next generation networking;Edge computing;Internet of Things;Interoperability;Medical services;Biomedical monitoring","","5","","45","CCBYNCND","25 Dec 2023","","","IEEE","IEEE Journals"
"Bioinspired Multiagent Embryonic Architecture for Resilient Edge Networks","T. Welsh; E. Benkhelifa","Cloud Computing and Applications Research Lab, Staffordshire University, Stoke on Trent, U.K.; Cloud Computing and Applications Research Lab, Staffordshire University, Stoke on Trent, U.K.",IEEE Transactions on Industrial Informatics,"5 Dec 2019","2019","15","12","6344","6353","With the pervasive introduction of Internet of Everything (IoE) technologies, use cases are frequently being found operating within harsh environmental conditions. This decreases the need for solutions that permit service delivery to operate in a highly resilient manner. This paper presents an architecture for a novel cloud platform designed for resilient service delivery. It supports networks where poor communication links or high node failure will cause services to be delivered in an nonresilient manner. This could be the result of factors such as high node mobility, poor environmental conditions, and unreliable infrastructure from environment disaster or cyber-attack. This biologically inspired architecture uses a purely distributed multiagent approach to provide self-healing and self-organizing properties, modeled on the characteristics of embryonic development and biological cell communication. To permit high levels of a node churn, this multiagent approach uses local-only communication. Probabilistic cellular automata are used to simulate this architecture and evaluate the efficacy of this approach.","1941-0050","","10.1109/TII.2019.2916094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8712435","Artificial intelligence;artificial life;bioinspired;cellular automata (CA);cloud;cybersecurity;edge;embryonic;Internet of Everything (IoE);multiagent;resilience","Computer architecture;Microprocessors;Resilience;Automata;Biological system modeling;Data processing;Cloud computing","","5","","32","IEEE","10 May 2019","","","IEEE","IEEE Journals"
"Scheduling and Predictive Maintenance for Smart Toilet","A. Lokman; R. K. Ramasamy; C. -Y. Ting","Faculty of Computing and Informatics, Multimedia University, Cyberjaya, Malaysia; Faculty of Computing and Informatics, Multimedia University, Cyberjaya, Malaysia; Faculty of Computing and Informatics, Multimedia University, Cyberjaya, Malaysia",IEEE Access,"24 Feb 2023","2023","11","","17983","17999","Modern society needs bathrooms. Poor sanitation is caused by worn-out appliances and expensive cleaning. The technique also requires an inexpensive, dependable sensor. This study had three goals. Creating an IoT administration platform is the main goal. Literature evaluations assess the merits and downsides of existing systems. Second, we suggest predictive maintenance to assist predict bathroom equipment breakdowns. Finally, a scheduling algorithm was used to determine how many janitors to hire. We’ll measure the model’s effectiveness and make future recommendations. Infrared, temperature and humidity sensors create an IoT bathroom. Sensors have been studied to understand how to adapt them to the hygienic and private toilet environment. Sensor accuracy and cost-effectiveness could be enhanced with more development and testing. The Auto-Regressive Integrated Moving Average (ARIMA) model accurately predicts time series lags, making it a good candidate for predictive maintenance. Long Short-Term Memory (LSTM) is good in time series predictions, therefore it’s fair to compare the two. We use the ARIMA model to handle Remaining Useful Life (RUL) prediction techniques by altering Moving Average (MA) and Auto-Regressive (AR). A genetic algorithm is used to create a janitorial cleaning schedule. The genetic algorithm was proposed to schedule cleaning workers. This approach improves the genetic algorithm by studying soft and hard scheduling restrictions. The Greedy algorithm is used to compare. Experimental evaluations reveal that the suggested model ARIGA meets both goals.","2169-3536","","10.1109/ACCESS.2023.3241942","TM Research and Development from Telekom Malaysia, Malaysia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10035972","Scheduling;predictive maintenance;IoT","Internet of Things;Scheduling;Monitoring;Temperature sensors;Costs;Temperature measurement;Predictive maintenance","","5","","59","CCBY","3 Feb 2023","","","IEEE","IEEE Journals"
"IoT Capabilities Composition and Decomposition: A Systematic Review","K. Halba; E. Griffor; A. Lbath; A. Dahbura","École doctorale Mathématiques, Sciences et technologies de l’information, Informatique (ED MSTII), Grenoble Alpes University, Saint-Martin-d’Hères, France; Department of Computer Science, National Institute of Standards and Technology, Gaithersburg, MD, USA; École doctorale Mathématiques, Sciences et technologies de l’information, Informatique (ED MSTII), Grenoble Alpes University, Saint-Martin-d’Hères, France; Johns Hopkins University, Baltimore, MD, USA",IEEE Access,"30 Mar 2023","2023","11","","29959","30007","As billions of IoT devices join the Internet, researchers and innovators increasingly explore IoT capabilities achieved via service composition or reuse of existing capabilities via service decomposition. Many systematic literature reviews (SLRs) were produced on this subject; however, two issues remain to be addressed: i) a reference taxonomy of the different aspects of IoT capabilities composition and decomposition is needed, and ii) many formal questions (e.g., standards role, formal representations applications, state-space explosion countermeasures, etc.), technical questions (e.g., composition process types and automation levels synergies, service decomposition categories, the role of AI/ML, etc.), and QoS questions (e.g., privacy, interoperability, and scalability challenges and solutions, etc.) remain unanswered. We introduce this work by discussing notions of IoT capabilities composition and decomposition in a layered IoT architecture while highlighting the strengths and weaknesses of existing SLRs. We identify unanswered questions through gaps in related work and motivate these questions using the PICOC methodology. We explain the search methodology and organize the topic questions using the proposed reference taxonomy. The identified research questions are answered, and trends and gaps that need additional attention from the research community are highlighted. This effort benefits city planners and end-users of IoT systems as it contributes to a better understanding of the role of composition and decomposition of IoT capabilities in building value-added services or reusing existing ones for resource optimization. For researchers, this effort contributes a reference taxonomy for the topic and sheds light on important questions while highlighting corresponding trends and gaps requiring further attention.","2169-3536","","10.1109/ACCESS.2023.3260182","National Institute of Standards and Technology (NIST), Gaithersburg, Maryland, USA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10078256","Capability;composition;decomposition;Internet of Things (IoT);cyber-physical systems (CPS);systematic literature review (SLR)","Internet of Things;Atomic layer deposition;Data models;Cyber-physical systems;Atomic measurements;Temperature measurement","","5","","256","CCBY","22 Mar 2023","","","IEEE","IEEE Journals"
"Novel Industry Architectures for Connectivity Solutions in the Smart Distribution Grids","S. Borenius; P. Kekolahti; H. Hämmäinen; M. Lehtonen; P. Mähönen","School of Electrical Engineering, Aalto University, Espoo, Finland; School of Electrical Engineering, Aalto University, Espoo, Finland; School of Electrical Engineering, Aalto University, Espoo, Finland; School of Electrical Engineering, Aalto University, Espoo, Finland; School of Electrical Engineering, Aalto University, Espoo, Finland",IEEE Access,"11 Jul 2023","2023","11","","68093","68112","The electric energy system is undergoing a major change due to the increasing requirements of dynamic performance. In distribution grids, this evolution will necessitate expanded automation, which in turn will require enhanced connectivity solutions. Strongly evolving communications technologies and architectures, particularly mobile communications as well as cloud and edge computing, will provide new opportunities and alternatives for connectivity solutions. This paper contributes by identifying potential technical and industry architectures for the connectivity solutions required to manage distribution grids in the early 2030s. The study utilizes a senior expert panel and a Delphi survey. Industry architectures are modelled as value networks. The paper uses the Finnish distribution grids as a case example. Regarding technical architectures, the results reveal skepticism concerning those emerging 5G mobile network features that target industrial applications and about the need for extensive distributed computing in the proximity of consumers and prosumers. The most probable industry architectures are found to be those that enable the Distribution System Operators (DSOs) to maintain direct control of critical technical components, or that enable Communications Service Providers (CSPs) to handle the operations of both communications solutions and distributed computing. CSPs are seen as well positioned for this task due to their existing networking and computing infrastructure. However, this may also involve business risks for both DSOs and CSPs.","2169-3536","","10.1109/ACCESS.2023.3291745","Finnish Public Funding Agency for Research, Business Finland through the Project “IFORGE,”(grant numbers:7127/31/2021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10171976","Cloud computing;communications;distributed computing;edge computing;industry architectures;mobile communications;overlay networks;power grids;smart grids;value networks","Computer architecture;Business;Surveys;5G mobile communication;Cloud computing;Information and communication technology;Cloud computing security;Distributed computing;Edge computing;Mobile communication;Power grids;Smart grids;Overlay networks","","5","","74","CCBY","3 Jul 2023","","","IEEE","IEEE Journals"
"Deadline and Energy-Aware Application Module Placement in Fog-Cloud Systems","A. Alwabel; C. K. Swain","Department of Computer Sciences, College of Computer Engineering and Sciences, Prince Sattam bin Abdulaziz University, Al-Kharj, Saudi Arabia; Department of Computer Science and Engineering, SRM University, Amaravathi, Andhra Pradesh, India",IEEE Access,"11 Jan 2024","2024","12","","5284","5294","Fog computing has emerged as a promising augmentation of cloud computing, positioned at the network’s edge, and it is poised to enhance a wide range of Internet of Things (IoT) driven applications. Although fog computing promises to reduce the response time of applications, its omnipresence is subject to the availability and capabilities of the resources in the fog infrastructure. Hence, there is a need of efficiently harness fog infrastructure to execute different IoT applications while meeting their quality of service (QoS) requirements. However, this objective becomes challenging when the applications are decomposed into multiple modules with diverse latency sensitivities. The scatter placement of application modules over distributed fog nodes further intensifies the problem by increasing the overall energy consumption of the fog environment. Therefore, this study proposes a deadline and energy-aware modular application placement policy for fog computing environments. The proposed policy simultaneously prioritizes the placement of critical applications in the fog infrastructure and consolidates the number of active fog nodes for energy management. The performance of the proposed policy was evaluated using iFogSim and compared with several contemporary solutions. The experimental results demonstrate that the proposed policy outperforms others in increasing the percentage of QoS-satisfied applications and reducing energy usage in fog computing.","2169-3536","","10.1109/ACCESS.2024.3350171","Prince Sattam bin Abdulaziz University(grant numbers:2023/01/26767); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10381709","Application module placement;placement policy;latency-aware placement;energy-aware placement;task scheduling;resource management;fog computing;cloud computing","Internet of Things;Cloud computing;Edge computing;Resource management;Task analysis;Quality of service;Costs","","5","","42","CCBYNCND","5 Jan 2024","","","IEEE","IEEE Journals"
"FLAG: Federated Learning for Sustainable Irrigation in Agriculture 5.0","S. Bera; T. Dey; A. Mukherjee; D. De","CSE Department, Maulana Abul Kalam Azad University of Technology, West Bengal, Kolkata, India; CSE Department, Maulana Abul Kalam Azad University of Technology, West Bengal, Kolkata, India; Computer Science Department, Mahishadal Raj College, Mahishadal, India; CSE Department, Maulana Abul Kalam Azad University of Technology, West Bengal, Kolkata, India",IEEE Transactions on Consumer Electronics,"29 Apr 2024","2024","70","1","2303","2310","This paper proposes a federated learning-based decision making framework for sustainable irrigation using IoT and dew-edge-cloud paradigm. The federated learning is used to prevent the sharing of user identities and raw data for data privacy protection. Further, gradient encryption is used to prevent the leakage of gradient information. Long short-term memory (LSTM) network and deep neural network (DNN) are used for data analysis in local and global models. Edge computing is used to reduce energy consumption and latency. The cache-based dew computing is used to provide temporary holding of the data when network connectivity is not available. The results present that the proposed framework achieves ~99% prediction accuracy at ~50% lower latency and energy consumption than the conventional edge-cloud framework.","1558-4127","","10.1109/TCE.2024.3370373","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10445480","Dew computing;federated learning;energy-efficient;latency;sustainability","Servers;Cloud computing;Irrigation;Computational modeling;Data models;Sensors;Soil moisture","","5","","23","IEEE","26 Feb 2024","","","IEEE","IEEE Journals"
"Toward Reference Architectures: A Cloud-Agnostic Data Analytics Platform Empowering Autonomous Systems","A. C. Marosi; M. Emődi; A. Farkas; R. Lovas; R. Beregi; G. Pedone; B. Németh; P. Gáspár","Laboratory of Parallel and Distributed Systems, Institute for Computer Science and Control, Eötvös Lóránd Research Network, Budapest, Hungary; Laboratory of Parallel and Distributed Systems, Institute for Computer Science and Control, Eötvös Lóránd Research Network, Budapest, Hungary; Laboratory of Parallel and Distributed Systems, Institute for Computer Science and Control, Eötvös Lóránd Research Network, Budapest, Hungary; Laboratory of Parallel and Distributed Systems, Institute for Computer Science and Control, Eötvös Lóránd Research Network, Budapest, Hungary; Centre of Excellence in Production Informatics and Control, Institute for Computer Science and Control, Budapest, Hungary; Centre of Excellence in Production Informatics and Control, Institute for Computer Science and Control, Budapest, Hungary; Systems and Control Laboratory, Institute for Computer Science and Control, Eötvös Lóránd Research Network, Budapest, Hungary; Laboratory of Parallel and Distributed Systems, Institute for Computer Science and Control, Eötvös Lóránd Research Network, Budapest, Hungary",IEEE Access,"13 Jun 2022","2022","10","","60658","60673","This work introduces a scalable, cloud-agnostic and fault-tolerant data analytics platform for state-of-the-art autonomous systems that is built from open-source, reusable building blocks. As the baseline for further new reference architectures, it represents an architecture blueprint for processing, enriching and analyzing various feeds of structured and non-structured input data from advanced Internet-of-Things (IoT) based use cases. The platform builds on industry best practices, leverages on solid open-source components in a reusable fashion, and is based on our experience gathered from numerous IoT and Big Data research projects. The platform is currently used in the framework of the National Laboratory for Autonomous Systems in Hungary (abbreviated as ARNL). The platform is demonstrated through selected use cases from ARNL including the areas of smart/autonomous production systems (collaborative robotic assembly) and autonomous vehicles (mobile robots with smart vehicle control). Finally, we validate the platform through the evaluation of its streaming ingestion capabilities.","2169-3536","","10.1109/ACCESS.2022.3180365","National Research, Development and Innovation (NRDI) Office—NKFIH, Hungary(grant numbers:ED_18-2-2018-0006,SNN 129178); Ministry of Innovation and Technology NRDI Office within the framework of the Autonomous Systems National Laboratory Program; Janos Bolyai Research Scholarship of the Hungarian Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9789158","Reference architecture;blueprint;data analytics;autonomous systems;IoT;IIoT;big data;mobile robots;collaborative robots;smart control","Computer architecture;Data analysis;Cloud computing;Architecture;Open source software;Autonomous systems;Measurement","","5","","63","CCBYNCND","6 Jun 2022","","","IEEE","IEEE Journals"
"Digital Transformation in the Public Administrations: A Guided Tour for Computer Scientists","P. Ciancarini; R. Giancarlo; G. Grimaudo","Department of Computer Science–Science and Engineering, University of Bologna, Bologna, Italy; Department of Mathematics and Computer Science, University of Palermo, Palermo, Italy; Department of Engineering, University of Palermo, Palermo, Italy",IEEE Access,"15 Feb 2024","2024","12","","22841","22865","The goal of Digital Transformation of the Public Sector is the achievement of a better quality of life for citizens, via a more responsive and transparent administration and governance. By now it is clear that technological innovation, both in terms of computer architectures and software systems, is a crucial component of it, yet not sufficient. Indeed, a cultural, organizational and legal shift in how public organizations operate and relate to the citizens is also required. Nevertheless, computer scientists can play a key role in such a transformation and, given its impact on Society, it is essential to achieve a broader level of awareness of it and involvement in it of those scientific and professional figures. To this end, a technical map specifically designed for computer scientists, but properly placed in the context of the cultural, organizational and legal changes mentioned above, would be highly beneficial. To date, such a map is missing, to the best of our knowledge. The main contribution of this Tutorial is to provide it, together with a guided tour describing which key technological aspects enable and drive such a transformation. More specifically, based on a careful analysis of the available scholarly literature, that does not seem to include any Computer Science textbook material, a model of such a transformation is proposed, together with carefully selected examples incarnating it to show its validity: the cities of Barcelona and Chicago. Finally, a look at the future of this area is also provided.","2169-3536","","10.1109/ACCESS.2024.3363075","CINI Grant(grant numbers:PNRM-AMINSEP); Italian MUR PRIN Project “Multicriteria data structures and algorithms: from compressed to learned indexes, and beyond”(grant numbers:2017WR7SHH); Project INdAM—GNCS “Analysis and Processing of Big Data Based on Graph Models.”; Italian MUR PRIN Project “Multicriteria data structures and algorithms: from compressed to learned indexes, and beyond”(grant numbers:2017WR7SHH); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10423006","Agile software development;computer systems organization;cloud computing;computing methodologies;machine learning;data knowledge and engineering;design and engineering of services for e-citizens;digital government;digital transformation","Tutorials;Digital transformation;Ecosystems;Computer science;Smart cities;Regulation;Open data;Cloud computing;Data engineering;Knowledge management;Electronic government","","5","","237","CCBYNCND","6 Feb 2024","","","IEEE","IEEE Journals"
"Cybersecurity of Satellite Communications Systems: A Comprehensive Survey of the Space, Ground, and Links Segments","S. Salim; N. Moustafa; M. Reisslein","School of Engineering and Information Technology, University of New South Wales, Canberra, ACT, Australia; School of Engineering and Information Technology, University of New South Wales, Canberra, ACT, Australia; School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA",IEEE Communications Surveys & Tutorials,"14 Feb 2025","2025","27","1","372","425","Satellite communications (Satcoms) systems have become an integral part of modern society, providing critical infrastructure for a wide range of applications. However, as the reliance on Satcoms has increased, cyberattacks on Satcoms systems have emerged as a severe concern, with the potential to cause significant disruption, economic losses, and even loss of life. We first give a tutorial-style overview of the architecture of a Satcoms system, which typically consists of a space segment, a ground segment (encompassing the terrestrial ground stations and users), and a links segment. Following the taxonomy provided by this segment structure, we provide—to the best of our knowledge—the first comprehensive survey of the state-of-the-art cyberattacks (cyberthreats) on all three segments of Satcoms systems. For each Satcoms system segment, we organize the cyberattacks according to categories of Satcoms-specific cyberattacks, which we relate to the threat classifications in the general STRIDE cyberthreat model. Also, for all three segments of Satcoms systems, we comprehensively survey the general cybersecurity strategies and the specific cybersecurity mechanisms (techniques) that defend Satcoms systems against cyberattacks. We distill the critical learned lessons associated with Satcoms cybersecurity strategies, such as the need to balance security with cost-effectiveness. Finally, we outline the open challenges and future research directions in Satcoms systems cybersecurity.","1553-877X","","10.1109/COMST.2024.3408277","SmartSat CRC(grant numbers:RG214317); Australian Government’s CRC Program; Australian Research Council/Discovery Early Career Researcher Award(grant numbers:DE23010011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10546924","Cyberattack;cybersecurity;ground segment;links segment;space segment;satellite communications","Computer security;Computer crime;Surveys;Space vehicles;Satellites;Satellite broadcasting;Media","","5","","388","IEEE","3 Jun 2024","","","IEEE","IEEE Journals"
"WaterEdge: Edge–Cloud Collaborative Intelligent Coagulation System for Group-Level Water Treatment Plants","Y. Wang; S. Yang; X. Ren; S. Guo; C. Zhao; Q. Han","School of Computer Science and Technology, Xi'an University of Posts and Telecommunications, Xi'an, China; National Engineering Laboratory for Big Data Analytics (NEL-BDA) and the Ministry of Education Key Lab for Intelligent Networks and Network Security (MOE KLINNS Lab), Xi'an Jiaotong University, Xi'an, China; National Engineering Laboratory for Big Data Analytics (NEL-BDA), Xi'an Jiaotong University, Xi'an, China; National Engineering Laboratory for Big Data Analytics (NEL-BDA), Xi'an Jiaotong University, Xi'an, China; National Engineering Laboratory for Big Data Analytics (NEL-BDA), Xi'an Jiaotong University, Xi'an, China; National Engineering Laboratory for Big Data Analytics (NEL-BDA), Xi'an Jiaotong University, Xi'an, China",IEEE Systems Journal,"12 Dec 2023","2023","17","4","5346","5356","Providing an intelligent coagulation solution for group-level water treatment plants (WTPs) is of great significance to improve coagulation efficiency. Existing solutions are devoted to intelligent models based on operators' experience, but ignore the dangers of too much residual aluminum to human health and plant cost. To solve this problem, we propose an edge–cloud collaborative intelligent (ECCI) coagulation system, WaterEdge, to minimize the residual aluminum, while ensuring water quality. To support this, a collaborative architecture of “multiPool-multiWTP-cloud” is developed for fast computation, continuous service maintenance, and upgradation of all WTPs. An ECCI dosage prediction algorithm based on double incremental learning is proposed to determine optimal dosage. To reduce the response time, an ECCI online scheduling algorithm is presented. Furthermore, we build an ECCI platform to provide lifecycle management of applications and verify the effectiveness of WaterEdge in real-world WTPs. Extensive experiments demonstrate that WaterEdge can significantly outperform existing schemes.","1937-9234","","10.1109/JSYST.2023.3321734","National Key Research and Development Program of China(grant numbers:2021YFB2401300,2022YFA1004100,2020YFA0713900); National Natural Science Foundation of China(grant numbers:62172329,U1811461,U21A6005,11690011,62306234); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10288578","Edge–cloud collaborative intelligence (ECCI);industrial intelligence;optimal dosage prediction;smart water treatment plants (WTPs);task scheduling","Predictive models;Coagulation;Task analysis;Scheduling;Collaboration;Aluminum;Water resources","","5","","33","IEEE","20 Oct 2023","","","IEEE","IEEE Journals"
"Research Trends, Detection Methods, Practices, and Challenges in Code Smell: SLR","M. A. A. Hilmi; A. Puspaningrum; Darsih; D. O. Siahaan; H. S. Samosir; A. S. Rahma","Department of Informatics, Politeknik Negeri Indramayu, Indramayu, Indonesia; Department of Informatics, Politeknik Negeri Indramayu, Indramayu, Indonesia; Department of Informatics, Politeknik Negeri Indramayu, Indramayu, Indonesia; Department of Informatics, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Faculty of Vocational Studies, Institut Teknologi Del, Toba, Indonesia; Department of Informatics, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia",IEEE Access,"23 Nov 2023","2023","11","","129536","129551","Context: A code smell indicates a flaw in the design, implementation, or maintenance process that could degrade the software’s quality and potentially cause future disruptions. Since being introduced by Beck and Fowler, the term code smell has attracted several studies from researchers and practitioners. However, over time, studies are needed to discuss whether this issue is still interesting and relevant. Objective: Conduct a thorough systematic literature review to learn the most recent state of the art for studying code smells, including detection methods, practices, and challenges. Also, an overview of trends and future relevance of the topic of code smell, whether it is still developing, or if there has been a shift in the discussion. Method: The search methodology was employed to identify pertinent scholarly articles from reputable databases such as ScienceDirect, IEEE Xplore, ACM Digital Library, SpringerLink, ProQuest, and CiteSeerX. The application of inclusion and exclusion criteria serves to filter the search results. In addition, forward and backward snowballing techniques are employed to enhance the comprehensiveness of the results. Results: The inquiry yielded 354 scholarly articles published over the timeframe spanning from January 2013 to July 2022. After inclusion, exclusion, and snowballing techniques were applied, 69 main studies regarding code smells were identified. Many researchers focus on detecting code smells, primarily via machine learning techniques and, to a lesser extent, deep learning methods. Additional subjects encompass the ramifications of code smells; code smells within specific contexts, the correlation between code smells and software metrics, and facets about security, refactoring, and development habits. Contexts and types of code smells vary in the focus of the study. Some tools used are Jspirit, aDoctor, CAME, and SonarQube. The study also explores the concept of design smells and anti-pattern detection. While a singular dominating technique to code smell detection has yet to be thoroughly investigated, other aspects of code smell detection remain that still need to be examined. Conclusion: The findings underscore scholarly attention’s evolution towards code smells over the years. This study identified significant journals and conferences and influential researchers in this field. The detection methods used include empirical, machine learning, and deep learning. However, challenges include subjective interpretation and limited contextual applicability.","2169-3536","","10.1109/ACCESS.2023.3334258","Politeknik Negeri Indramayu on the 2022–2023 Inter-Higher Education Cooperation Research (PKPT) Scheme in Collaboration with Institut Teknologi Sepuluh Nopember (ITS) Surabaya; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10322720","Code smell;detection;systematic review;software quality;bad smell","Codes;Systematics;Surveys;Software quality;Object recognition;Correlation;Taxonomy;Machine learning;Computer security;Performance evaluation","","5","","95","CCBYNCND","17 Nov 2023","","","IEEE","IEEE Journals"
"A Framework for Cognitive, Decentralized Container Orchestration","R. C. Sofia; J. Salomon; S. Ferlin-Reiter; L. Garcés-Erice; P. Urbanetz; H. Mueller; R. Touma; A. Espinosa; L. M. Contreras; V. Theodorou; N. Psaromanolakis; L. Mamatas; V. Tsaoussidis; X. Fu; T. Yuan; A. del Rio; D. Jiménez; A. Stam; E. Paraskevoulakou; P. Karamolegkos; V. Vieira; J. Martrat; I. M. Prusiel; D. Matzakou; J. Soldatos; D. Remon; M. Jahn","fortiss GmbH, Munich, Germany; RedHat, Ra’anana, Israel; RedHat, Ra’anana, Israel; IBM Research Europe, Zürich, Switzerland; IBM Research Europe, Zürich, Switzerland; Siemens AG, Munich, Germany; i2CAT Foundation, Barcelona, Spain; i2CAT Foundation, Barcelona, Spain; Telefónica, Madrid, Spain; Intracom Telecom, Attica, Greece; Intracom Telecom, Attica, Greece; Athena Research Center, University of Macedonia, Thessaloniki, Greece; Athena Research Center, Democritus University of Thrace, Xanthi, Greece; Institute of Computer Science, University of Göttingen, Göttingen, Germany; Institute of Computer Science, University of Göttingen, Göttingen, Germany; Electrónica Física, Ingeniería Eléctrica y Física Aplicada, Universidad Politécnica de Madrid, Madrid, Spain; Electrónica Física, Ingeniería Eléctrica y Física Aplicada, Universidad Politécnica de Madrid, Madrid, Spain; Almende, Rotterdam, The Netherlands; Data & Cloud Research Group, University of Piraeus Research Centre, Piraeus, Greece; Data & Cloud Research Group, University of Piraeus Research Centre, Piraeus, Greece; INOVA+, Matosinhos, Portugal; ATOS, Madrid, Spain; ATOS, Madrid, Spain; Netcompany-Intrasoft, Luxembourg City, Luxembourg; Netcompany-Intrasoft, Luxembourg City, Luxembourg; Eclipse Foundation, Darmstadt, Germany; Eclipse Foundation, Darmstadt, Germany",IEEE Access,"11 Jun 2024","2024","12","","79978","80008","This perspective paper introduces a novel framework for container orchestration called CODECO. The CODECO orchestration framework relies on a data-network-computing approach to define the best infrastructure that can support the operation of next-generation Internet applications across a mobile, heterogeneous Edge-Cloud continuum. The selection of such an infrastructure is aligned with target performance profiles defined by the user, such as resilience or greenness. CODECO proposes to rely on decentralized Artificial Intelligence approaches to provide the most suitable infrastructure to an application deployment, considering infrastructural challenges, such as intermittent connectivity and node failure. This paper explains the current CODECO framework and gives insight into operational use-cases where CODECO is being deployed, as relevant examples of application for such a framework. Recent developments in the creation of the open-source CODECO framework are described and explained, allowing the use of the framework by the research community. The paper then provides a thorough analysis of CODECO’s features in comparison with existing orchestration frameworks, explaining the benefits introduced with this dynamic orchestration approach.","2169-3536","","10.1109/ACCESS.2024.3406861","European Commission in the Context of the Horizon Europe Cognitive Decentralised Edge-Cloud Orchestration (CODECO)(grant numbers:101092696); Swiss State Secretariat for Education, Research and Innovation (SERI)(grant numbers:23.00028); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10540390","Edge-cloud;orchestration;Kubernetes;AI/ML;heterogeneous networks;data observability","Containers;Europe;Computer architecture;Servers;Internet of Things;Real-time systems;Cloud computing;Edge computing;Heterogeneous networks;Observability;Data models;Internet;Computer applications;Open source software;Automation","","5","","73","CCBYNCND","29 May 2024","","","IEEE","IEEE Journals"
"Airborne Computing: A Toolkit for UAV-Assisted Federated Computing for Sustainable Smart Cities","K. Hayawi; Z. Anwar; A. W. Malik; Z. Trabelsi","College of Interdisciplinary Studies, Zayed University, Abu Dhabi, UAE; Department of Computer Science, North Dakota State University, Fargo, ND, USA; Department of Computing, National University of Sciences and Technology, Islamabad, Pakistan; College of Information Technology, United Arab Emirates University, Abu Dhabi, UAE",IEEE Internet of Things Journal,"19 Oct 2023","2023","10","21","18941","18950","Smart vehicles are equipped with onboard computing units designed to run in-vehicle applications. However, due to limited computing power, the onboard units are unable to execute compute-intensive tasks and those that require near real-time processing. Therefore tasks are offloaded to nearby fog/edge devices that have more powerful processors. However, the fog devices are static, placed at fixed locations such as intersections, and have a limited communication range. Therefore, they can only facilitate vehicles in their immediate vicinity and only limited areas of the city can be covered to provide services on demand. In this article, we propose an unmanned aerial vehicle (UAV)-based computing framework design termed Skywalker to provide computing in regions where there are no static fog units thereby extending coverage. Skywalker’s contributions are threefold: 1) it allows for load-aware UAV placement and provisions a swarm of UAVs to fly to areas experiencing a gap in service where the size of the swarm is proportional to the demand; 2) it implements multiple scheduling algorithms that the UAVs swarm employs to divide up the task processing responsibility for individual UAVs within the swarm; and 3) a zone-based delivery mechanism is being proposed to facilitate the return of completed tasks, either through direct delivery or relay-based methods. The choice between these options depends on the distance covered by the requesting vehicle from the UAV swarm. The efficiency of the framework is compared with existing techniques and it is found that it can greatly extend coverage during peak traffic hours while providing low communication delay and consuming minimum energy.","2327-4662","","10.1109/JIOT.2023.3292308","Zayed University through the Cluster Research(grant numbers:R20140); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10172226","Fog computing;smart cities;task offloading;unmanned aerial vehicle (UAV) swarm","Task analysis;Smart cities;Edge computing;Autonomous aerial vehicles;Servers;Relays;Real-time systems","","5","","21","IEEE","4 Jul 2023","","","IEEE","IEEE Journals"
"Early Evaluation of Mobile Applications’ Resource Consumption and Operating Costs","J. Berrocal; J. García-Alonso; P. Fernandez; A. Pérez-vereda; J. Hernandez; C. Canal; J. M. Murillo; A. Ruiz-Cortes","Department of Computer and Telematic Systems Engineering, University of Extremadura, Badajoz, Spain; Department of Computer and Telematic Systems Engineering, University of Extremadura, Badajoz, Spain; Department of Computer Languages and Systems, University of Sevilla, Sevilla, Spain; Department of Computer Science, University of Malaga, Málaga, Spain; Department of Computer and Telematic Systems Engineering, University of Extremadura, Badajoz, Spain; Department of Computer Science, University of Malaga, Málaga, Spain; Department of Computer and Telematic Systems Engineering, University of Extremadura, Badajoz, Spain; Department of Computer Languages and Systems, University of Sevilla, Sevilla, Spain",IEEE Access,"17 Aug 2020","2020","8","","146648","146665","The explosive growth of the mobile application market in recent years has led to a large concomitant mobile software industry whose components are, in many cases, startups and small-size software providers. The success of these applications and the firms behind them depends on a subtle balance between different dimensions mainly affected by their architectural design, such as user satisfaction, resource consumption, operating costs, and timing. The present communication describes a framework with a specific set of practices for identifying the boundaries of different architectural designs —in this article we apply it to estimate both the smartphone’s resource consumption and the operating costs in the cloud— and thus help in the architectural decision-making process. This will enable mobile software developers to predict at early stages which architectural design best suits their business model in accordance with the number of users and the expected use of the application and even provide an advance alert of when architectural choices will need to be reviewed, obviating the need for costly architectural re-design in further phases.","2169-3536","","10.1109/ACCESS.2020.3015082","MCI/AEI/FEDER,UE, through Projects(grant numbers:RTI2018-094591-B-I00,PGC2018-094905-B-I00); APOLO(grant numbers:U.S.–1264651); HORATIO(grant numbers:RTI2018-101204–B–C21); RCIS Research Network(grant numbers:RED2018-102654-T); 4IE+ Project; Interreg V-A España-Portugal (POCTEP) 2014–2020 Program(grant numbers:0499-4IE-PLUS-4-E); FEDER/Junta de Andalucia(grant numbers:UMA18-FEDERJA-180); Department of Economy, Science, and Digital Agenda of the Government of Extremadura(grant numbers:GR18112,IB18030); European Regional Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9162092","J9 mobile applications;D211 software architectures;D218 software engineering process;M2 services lifecycle","Software;Mobile applications;Investment;Industries;Mobile handsets;Computer architecture","","5","","67","CCBY","7 Aug 2020","","","IEEE","IEEE Journals"
"Cloud-Based Implementation of an Automatic Coverage Estimation Methodology for Self-Organising Network","D. Fernandes; D. Clemente; G. Soares; P. Sebastião; F. Cercas; R. Dinis; L. S. Ferreira","Instituto Universitário de Lisboa (ISCTE-IUL)/IT—Instituto de Telecomunicações, Av. das Forças Armadas, Lisbon, Portugal; Instituto Universitário de Lisboa (ISCTE-IUL)/IT—Instituto de Telecomunicações, Av. das Forças Armadas, Lisbon, Portugal; Multivision—Consultoria, Rua Soeiro Pereira Gomes, Lote N°1, 3°C, Lisbon, Portugal; Instituto Universitário de Lisboa (ISCTE-IUL)/IT—Instituto de Telecomunicações, Av. das Forças Armadas, Lisbon, Portugal; Instituto Universitário de Lisboa (ISCTE-IUL)/IT—Instituto de Telecomunicações, Av. das Forças Armadas, Lisbon, Portugal; FCT—Universidade Nova de Lisboa, Monte da Caparica, Caparica, Portugal; Multivision—Consultoria, Rua Soeiro Pereira Gomes, Lote N°1, 3°C, Lisbon, Portugal",IEEE Access,"16 Apr 2020","2020","8","","66456","66474","One of the main concerns of telecommunications operators is related to network coverage. A weak coverage can lead to a performance decrease, not only in the user experience, when using the operators' services, such as multimedia streaming, but also in the overall Quality of Service. This paper presents a novel cloud-based framework of a semi-empirical propagation model that estimates the coverage in a precise way. The novelty of this model is that it is automatically calibrated by using drive test measurements, terrain morphology, buildings in the area, configurations of the network itself and key performance indicators, automatically extracted from the operator's network. Requirements and use cases are presented as motivations for this methodology. The results achieve an accuracy of about 5 dB, allowing operators to obtain accurate neighbour lists, optimise network planning and automate certain actions on the network by enabling the Self-Organising Network concept. The cloud implementation enables a fast and easy integration with other network management and monitoring tools, such as the Metric platform, optimising operators' resource usage recurring to elastic resources on-demand when needed. This implementation was integrated into the Metric platform, which is currently available to be used by several operators.","2169-3536","","10.1109/ACCESS.2020.2986437","OptiNET-5G project and co-funded by the Centro2020, Portugal2020; European Union(grant numbers:023304); Instituto de Telecomunicações, FCT/MCTES through national funds and co-funded by the EU funds(grant numbers:UIDB/EEA/50008/2020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9060956","Cloud implementation;coverage estimation;drive tests;measurements;propagation model","Estimation;Telecommunications;Planning;Tools;Measurement;Optimization;Cloud computing","","5","","51","CCBY","8 Apr 2020","","","IEEE","IEEE Journals"
"Service Placement for Latency Reduction in the Fog Using Application Profiles","K. Velasquez; D. P. Abreu; M. Curado; E. Monteiro","Department of Informatics Engineering, Centre for Informatics and Systems, University of Coimbra, Coimbra, Portugal; Department of Informatics Engineering, Centre for Informatics and Systems, University of Coimbra, Coimbra, Portugal; Department of Informatics Engineering, Centre for Informatics and Systems, University of Coimbra, Coimbra, Portugal; Department of Informatics Engineering, Centre for Informatics and Systems, University of Coimbra, Coimbra, Portugal",IEEE Access,"7 Jun 2021","2021","9","","80821","80834","The Cloud-Fog-Internet of Things continuum combines different paradigms to provide connectivity and ubiquity for end-users, while also granting low latency and low jitter to cope with different challenges, including the requirements of latency-sensitive applications, such as virtual/augmented reality and online gaming. This constitutes a complex and dynamic environment with heterogeneous resources that need to be managed or orchestrated, in order to accomplish application requirements for low latency. Common orchestration solutions make placement decisions based only on the resources of the underlying network and the application resource requests; however, using the profiles of applications to make placement decisions has the potential to enhance the final performance perceived by the end-users. This paper proposes the use of application profiles according to their popularity to guide their placement. To corroborate the effectiveness of the use of the profiles, two placement mechanisms are presented, one based on Genetic Algorithm and the other inspired on graph partitions. Simulation results show that it is possible to reduce the latency and jitter of applications via a service placement guided by the profiles. The mechanism based on graph partitions showed better results for all scenarios, followed closely by the Genetic Algorithm in the scenarios with lower load.","2169-3536","","10.1109/ACCESS.2021.3085370","European Regional Development Fund (FEDER), through the Regional Operational Programme of Lisbon (POR LISBOA 2020); Competitiveness and Internationalization Operational Programme (COMPETE 2020) of the Portugal 2020 Framework through the Project 5G with Nr. 024539(grant numbers:POCI-01-0247-FEDER-024539); National Funds through the Foundation for Science and Technology (FCT), I.P., through the Project Centre for Informatics and Systems of the University of Coimbra (CISUC)(grant numbers:UID/CEC/00326/2020); European Social Fund through the Regional Operational Program Centro 2020; Portuguese Funding Institution, Foundation for Science and Technology (FCT)(grant numbers:SFRH/BD/119392/2016,SFRH/BD/117538/2016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9445114","Popularity;profiles;latency;placement;fog","Cloud computing;Measurement;Proposals;Mathematical model;Genetic algorithms;Quality of service;Quality of experience","","4","","40","CCBY","2 Jun 2021","","","IEEE","IEEE Journals"
"Energy Consumption of Machine Learning Enhanced Open RAN: A Comprehensive Review","X. Liang; Q. Wang; A. Al-Tahmeesschi; S. B. Chetty; D. Grace; H. Ahmadi","School of Physics, Engineering and Technology, University of York, York, U.K; School of Physics, Engineering and Technology, University of York, York, U.K; School of Physics, Engineering and Technology, University of York, York, U.K; School of Physics, Engineering and Technology, University of York, York, U.K; School of Physics, Engineering and Technology, University of York, York, U.K; School of Physics, Engineering and Technology, University of York, York, U.K",IEEE Access,"14 Jun 2024","2024","12","","81889","81910","The Open Radio Access Network (RAN) emerges as a revolutionary architecture promising unprecedented levels of openness, flexibility, and intelligence within radio access networks. Central to this innovation is the integration of Machine Learning (ML) and Artificial Intelligence (AI) within the RAN Intelligent Controller (RIC), aimed at optimizing network operations and enhancing control mechanisms. This paper undertakes a thorough examination of Open RAN, particularly focusing on its energy consumption aspects, which are pivotal for ensuring the sustainability of future wireless networks. In this paper, we review and compare Open RAN architecture with previous network architectures. In particular we focus on O-RAN Alliance specifications. Additionally, we explore the deployment of ML across various facets of Open RAN and highlights how to estimate the energy consumption of ML models. Through constructing explicit energy consumption models for key O-RAN components, we provide a granular analysis of their energy profiles. Finally we compare the energy dynamics of O-RAN against traditional RAN architectures, delineating the impact of virtualization and disaggregation on energy efficiency.","2169-3536","","10.1109/ACCESS.2024.3412758","Engineering and Physical Sciences Research Council United Kingdom, Impact Acceleration Accounts(grant numbers:EP/X525856/1); Department of Science, Innovation and Technology, U.K., through Yorkshire Open-RAN(grant numbers:TS/X013758/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10552840","Open radio access network (Open RAN);energy efficiency;machine learning;disaggregation","Energy consumption;Computer architecture;Energy efficiency;Resource management;Virtualization;Reviews;5G mobile communication;Radio access networks;Open systems","","4","","94","CCBY","11 Jun 2024","","","IEEE","IEEE Journals"
"Computing Power Networking Meets Blockchain: A Reputation-Enhanced Trading Framework for Decentralized IoT Cloud Services","L. Lin; J. Wu; Z. Zhou; J. Zhao; P. Li; J. Xiong","College of Computer and Cyber Security, Fujian Normal University, Fuzhou, China; College of Computer and Cyber Security, Fujian Normal University, Fuzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Zhejiang-HUST Joint Research Center for Graph Processing, Zhejiang Lab, Hangzhou, China; School of Computer Science and Engineering, University of Aizu, Aizuwakamatsu, Japan; College of Computer and Cyber Security, Fujian Normal University, Fuzhou, China",IEEE Internet of Things Journal,"9 May 2024","2024","11","10","17082","17096","Computing power networking (CPN) represents a transformative paradigm in distributed computing, harnessing the collective capabilities of edge servers dispersed across diverse geographical locations. CPN’s core strengths lie in its ability to accelerate data processing, diminish latency, and scale efficiently, rendering it particularly apt for real-time applications and the Internet of Things. When coupled with blockchain technology, CPN extends its potential by facilitating secure and transparent allocation and trading of computing resources, bolstering data integrity and reliability. However, current research at the intersection of CPN and blockchain primarily focuses on framework development and technology integration, often overlooking the challenge of delivering dependable computing services, especially in the presence of potentially unreliable nodes. To tackle this issue, we introduce a reputation-enhanced resource trading framework, designed to ensure equitable and trustworthy computing power transactions. We establish a decentralized reputation model, capable of accurately assessing node behavior over extended periods. Additionally, we present three optimization mechanisms for reputation updates, accounting for transaction history, quality of service, and transaction amount. Furthermore, our work introduces a reputation-enhanced consensus mechanism within the trading system, strategically employing incentives to motivate participants to deliver high-quality services, thereby increasing their rewards. Simultaneously, it effectively mitigates wealth inequality among resource providers of varying sizes. To validate our approach, we develop a prototype system and conduct performance evaluations, which affirm the superiority of our system in enhancing reputation and delivering robust economic features.","2327-4662","","10.1109/JIOT.2024.3358379","National Natural Science Foundation of China(grant numbers:62272102,62172454,61502103); Natural Science Foundation of Fujian Province(grant numbers:2021J01167,2023J02014); Japan Society for the Promotion of Science (JSPS) KAKENHI(grant numbers:21H03424); Japan Science and Technology Agency (JST) PRESTO(grant numbers:23828673); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10414371","Blockchain;computing power networking (CPN);consensus mechanism;decentralized cloud computing (DCC);reputation","Internet of Things;Servers;Consensus protocol;Cloud computing;Peer-to-peer computing;Computational modeling;Resource management","","4","","47","IEEE","25 Jan 2024","","","IEEE","IEEE Journals"
"Exploiting the Solar Energy Surplus for Edge Computing","B. Martinez; X. Vilajosana","Internet Interdisciplinary Institute IN3, Universitat Oberta de Catalunya, Barcelona, Spain; IN3, Universitat Oberta de Catalunya, Barcelona, Spain",IEEE Transactions on Sustainable Computing,"8 Mar 2022","2022","7","1","135","143","In the context of the global energy ecosystem transformation, we introduce a new approach to reduce the carbon emissions of the cloud-computing sector and, at the same time, foster the deployment of small-scale private photovoltaic plants. We consider the opportunity cost of moving some cloud services to private, distributed, solar-powered computing facilities. To this end, we compare the potential revenue of leasing computing resources to a cloud pool with the revenue obtained by selling the surplus energy to the grid. We first estimate the consumption of virtualized cloud computing instances, establishing a metric of computational efficiency per nominal photovoltaic power installed. Based on this metric and characterizing the site’s annual solar production, we estimate the total return and payback. The results show that the model is economically viable and technically feasible. We finally depict the still many questions open, such as security, and the fundamental barriers to address, mainly related with a cloud model ruled by a few big players.","2377-3782","","10.1109/TSUSC.2021.3058588","SPOTS(grant numbers:RTI2018-095438-A-I00); Spanish Ministry of Science, Innovation and Universities; Generalitat de Catalunya(grant numbers:2017-SGR-60); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9352548","Green computing;green cloud;sustainable edge computing;energy surplus","Computational modeling;Cloud computing;Production;Monitoring;Computer architecture;Companies;Batteries","","4","","32","IEEE","10 Feb 2021","","","IEEE","IEEE Journals"
"Specification and Unattended Deployment of Home Networks at the Edge of the Network","I. Bernabé-Sánchez; D. Díaz-Sánchez; M. Muñoz-Organero","Telematic Engineering Department, Carlos III University of Madrid, Leganés, Spain; Telematic Engineering Department, Carlos III University of Madrid, Leganés, Spain; Telematic Engineering Department, Carlos III University of Madrid, Leganés, Spain",IEEE Transactions on Consumer Electronics,"25 Nov 2020","2020","66","4","279","288","Consumer devices continue to expand their capabilities by connecting to digital services and other devices to form information-sharing ecosystems. This is complex and requires meeting connection requirements and minimal processing capabilities to ensure communication. The emergence of new services, and the evolution of current technologies, constantly redefine the rules of the game by opening up new possibilities and increasing competition among service providers. Paradigms such as edge computing, softwarization of physical devices, self-configuration mechanisms, definition of software as a code and interoperability between devices, define design principles to be taken into account in future service infrastructures. This work analyzes these principles and presents a programmable architecture in which services and virtual devices are instantiated in any computing infrastructure, as cloud or edge computing, upon request according to the needs specified by service providers or users. Considering that the target computing infrastructures are heterogeneous, the solution defines network elements and provides network templates to ensure it can be deployed on different infrastructures irrespectively of the vendor. A prototype has been developed and tested on a virtualized cloud-based home network relying on open source solutions.","1558-4127","","10.1109/TCE.2020.3018543","Project MAGOS(grant numbers:TEC2017-84197-C4-1-R); Comunidad de Madrid (Spain) through the Project CYNAMON(grant numbers:P2018/TCS-4566); European Structural Funds (ESF and FEDER); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9173527","Connected consumer devices;fog computing;orchestrator","Cloud computing;Software;Ecosystems;Computer architecture;Edge computing","","4","","33","CCBY","21 Aug 2020","","","IEEE","IEEE Journals"
"Dynamic Software Containers Workload Balancing via Many-Objective Search","A. Ghammam; T. Ferreira; W. Aljedaani; M. Kessentini; A. Husain","Department of Computer Science and Engineering, Oakland University, Rochester, MI, USA; College of Innovation and Technology, University of Michigan-Flint, Flint, MI, USA; Department of Computer Science and Engineering, University of North Texas, Denton, TX, USA; Department of Computer Science and Engineering, Oakland University, Rochester, MI, USA; Ford Motor Company, Detroit, MI, USA",IEEE Transactions on Services Computing,"8 Aug 2023","2023","16","4","2575","2591","Software containers are becoming the new state of the art in the industry as they are extensively used to deploy systems. Indeed, the use of containers enables better modularity, reusability, and portability compared to other technologies. As the complexity of software systems is dramatically increasing, it is critical to enable optimal usage of the needed resources to execute them such as memory and CPU. Thus, different scheduling strategies are proposed to select the most suitable nodes to execute a set of containers. For instance, the default strategy in the Docker Swarm kit scheduling framework is based on an equal distribution of the containers between nodes independent of their sizes and consumed resources. However, balancing the containers’ workload is a complex problem due to the conflicting objectives of minimizing the number of selected nodes, minimizing the number of containers per node, the number of changes compared to the original schedule, and the coupling between containers allocated to different nodes. To deal with those conflicting scheduling objectives, we propose a scheduler based on a many-objective optimization approach for scheduling the execution of containers between multiple nodes. The proposed approach aims at finding the best allocation for containers in nodes that leads to efficient utilization of resources. To evaluate our approach, we compared the performance of multiple many and multi-objective techniques based on NSGA-II, NSGA-III, and IBEA algorithms using 48 Docker-related systems and the results show that NSGA-III outperforms the other algorithms in quality attributes as well as in CPU, Memory and Network usage.","1939-1374","","10.1109/TSC.2023.3237755","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10048583","Container scheduling;docker;many-objective optimization","Containers;Job shop scheduling;Optimization;Statistics;Software;Sociology;Processor scheduling","","4","","56","IEEE","17 Feb 2023","","","IEEE","IEEE Journals"
"Empowering Cloud Computing With Network Acceleration: A Survey","L. Rosa; L. Foschini; A. Corradi","Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy",IEEE Communications Surveys & Tutorials,"21 Nov 2024","2024","26","4","2729","2768","Modern interactive and data-intensive applications must operate under demanding time constraints, prompting a shift toward the adoption of specialized software and hardware network acceleration technologies. This specialization, however, poses significant scalability, flexibility, security, and economic sustainability challenges for application developers. Cloud computing holds the potential to overcome these obstacles by offering the cost-effective option to access specialized acceleration technologies through standard cloud interfaces. Nevertheless, that integration is still challenging for cloud providers. In the cloud, physical resources are hidden behind a virtualization layer, whereas acceleration technologies make applications directly interact with the hardware. To bridge this gap, recent literature explores the possibility of empowering cloud platforms with accelerated networking as a commodity, thus offering the innovative option of Network Acceleration as a Service. This survey reviews these recent research efforts by adopting popular technologies like XDP, DPDK, and RDMA as a reference. To organize the surveyed research in a comprehensive framework, we identify four key aspects that pose critical problems to the integration of acceleration options in cloud computing - access interfaces, virtualization techniques, serviceability, and security - and systematically discuss the associated challenges. Then, we present the issues to be further addressed and outline the most promising research directions for the full integration of network acceleration within next-generation cloud computing platforms.","1553-877X","","10.1109/COMST.2024.3377531","European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU, partnership on “Telecommunications of the Future”(grant numbers:PE00000001 (Program “RESTART”)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10472517","Cloud computing;network acceleration;next-generation networks;RDMA;XDP;DPDK","Cloud computing;Surveys;Virtualization;Tutorials;Data centers;Software;Reviews","","4","","168","CCBYNCND","14 Mar 2024","","","IEEE","IEEE Journals"
"Collaborative Deployment and Routing of Industrial Microservices in Smart Factories","M. Hu; Z. Guo; H. Wen; Z. Wang; B. Xu; J. Xu; K. Peng","Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Three Gorges Hi-Tech Information Technology Company, Ltd., Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China",IEEE Transactions on Industrial Informatics,"5 Nov 2024","2024","20","11","12758","12770","In large smart factories, massive microservices compose complicated modular IT systems, providing various service functions. However, large microservices-based IT systems incur sophisticated communications and invocations among the massive microservices, which calls for efficient orchestration techniques to meet the high requirements in smart factories. Also, complex data interdependencies among microservices tightly couple deployment with routing, further intensifying the difficulties in orchestration. Such challenges demand delicate joint optimization of service deployment and request routing, which however, are neglected by previous work. In this case, this article investigates the collaborative optimization of microservice deployment and routing in smart factories. First, we construct a communication queuing network model to analyze service performance under dynamic load. Second, two heuristics are proposed to provide differentiated deployment and routing schemes for various demands. Finally, rigorous experiments validate that our approach significantly enhances network efficiency across various production scenarios in smart factories.","1941-0050","","10.1109/TII.2024.3424347","National Science and Technology Major Project of China(grant numbers:2022ZD0117104); National Natural Science Foundation of China(grant numbers:62171189,62272183); Key Research and Development Program of Hubei Province China(grant numbers:2022BAA038,2023BAB074); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10621029","Edge computing;queuing networks;request routing;service deployment;smart factory","Microservice architectures;Routing;Smart manufacturing;Optimization;Task analysis;Collaboration;Production facilities","","4","","38","IEEE","2 Aug 2024","","","IEEE","IEEE Journals"
"A Novel 5G-NR Resources Partitioning Framework Through Real-Time User-Provider Traffic Demand Analysis","M. Maule; J. S. Vardakas; C. Verikoukis","Iquadrat Informatica S.L., Barcelona, Spain; Iquadrat Informatica S.L., Barcelona, Spain; the University of Patras, Patras, Greece",IEEE Systems Journal,"9 Dec 2022","2022","16","4","5317","5328","Network slicing (NS) is a key enabler of the 5G and beyond network architectures, allowing multiple dedicated logical networks with selected functionality to be executed on top of a common infrastructure. At the radio access network, novel flexibility paradigms and dynamic response to wireless channel variations are necessary for any NS solution. In this framework, this article proposes a novel real-time NS management framework for the 5G New-Radio, where the slice resource management is achieved through a joint dynamic evaluation of served users’ quality of service and tenants’ service level agreement. The design of the proposed framework within the standardized 5G architecture is discussed, together with the integration of the dynamic resource assignation procedure within the slice life cycle workflow. A novel mathematical model is proposed for the dynamic resource assignation within the slice life cycle workflow. The solution’s accuracy is tested by means of computer simulations, and found to be satisfactory according to the proposed evaluation metrics. Finally, the practicality of the optimal radio slicing configuration extracted from the proposed model is applied to our experimental platform, and the performance of a real case scenario is comprehensively evaluated.","1937-9234","","10.1109/JSYST.2021.3115896","EU H2020 5GSTEP-FWD(grant numbers:722429); EU H2020 5G-ERA(grant numbers:101016681); EU H2020 5G-ROUTES(grant numbers:951867); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9585085","5G;5G New-Radio (5G-NR);cloud radio access network (C-RAN);network slicing (NS);resources optimization;service-based architecture;testbed","5G mobile communication;Computer architecture;Real-time systems;Dynamic scheduling;Protocols;Cloud computing;3GPP","","4","","36","IEEE","25 Oct 2021","","","IEEE","IEEE Journals"
"Re-Scheduling IoT Services in Edge Networks","X. Li; Z. Zhou; Q. He; Z. Shi; W. Gaaloul; S. Yangui","School of Information Engineering, China University of Geosciences (Beijing), Beijing, China; School of Information Engineering, China University of Geosciences (Beijing), Beijing, China; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, VIC, Australia; Department of Shale Gas, Research Institute of Petroleum Exploration and Development, PetroChina, Beijing, China; LAAS-CNRS, University of Toulouse, Toulouse, France; Computer Science Department, TELECOM SudParis, Évry, France",IEEE Transactions on Network and Service Management,"9 Oct 2023","2023","20","3","3233","3246","With the explosive growth of the Internet of Things (IoT) devices deployed in edge networks, the functionalities of IoT devices are typically encapsulated as IoT services, and user requests can be achieved through the composition of data and/or computation-intensive IoT services. Considering the prediction-uncertainty of forthcoming requests, certain IoT services may (i) not be hosted currently by appropriate IoT devices, or (ii) such an IoT service exists, but its non-functional properties may hardly be satisfied with respect to certain constraints prescribed by requests. To address this challenge, this paper proposes an efficiency-aware service Migration Scheduling (denoted eMS) mechanism in edge networks, in order to migrate IoT services on-demand, and thus, to optimally settle non-satisfiable constraints. Specifically, IoT services are re-scheduled, such that certain IoT services are migrated from their hosting IoT devices to neighboring ones, while minimizing the energy consumption and average delay caused by this service re-scheduling operation. We formulate this service re-scheduling as a multi-objective and multi-constraint optimization problem, which is solved through integrating the greedy algorithm into the fast non-dominated sorting and crowded-comparison operators as the hybrid genetic algorithm (G-NSGA-II). Based on real-life datasets provided by an oil pipeline monitoring project, extensive experiments are conducted, and evaluation results show that our eMS is promising in reducing the energy consumption and average delay of service re-scheduling in comparison with the state-of-art’s techniques.","1932-4537","","10.1109/TNSM.2023.3242937","National Key Research and Development Program of China (Key Technologies and Demonstrations of the Internet of Things and Smart Cities)(grant numbers:2019YFB2101803); National Natural Science Foundation of China(grant numbers:42050103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10039683","IoT service re-scheduling;data and/or computation-intensive services;service migration;edge networks","Internet of Things;Energy consumption;Delays;Energy management;Task analysis;Processor scheduling;Performance evaluation","","4","","40","IEEE","7 Feb 2023","","","IEEE","IEEE Journals"
"Convergence of AI and MEC for Autonomous IoT Service Provisioning and Assurance in B5G","K. Abbas; Y. Cho; A. Nauman; P. W. Khan; T. A. Khan; K. Kondepu","Department of Computer Science, Hanyang University, Seoul, South Korea; Department of Computer Science, Hanyang University, Seoul, South Korea; Department of Information and Communication Engineering, Yeungnam University, Gyeongsan, South Korea; Department of Industrial and Management Systems Engineering, West Virginia University, Morgantown, USA; Institute for Communication Systems, University of Surrey, Guildford, U.K.; Department of Computer Science and Engineering, Indian Institute of Technology Dharwad, Dharwad, India",IEEE Open Journal of the Communications Society,"22 Nov 2023","2023","4","","2913","2929","With the exponential growth of Internet of Things (IoT) devices, IoT has become a transformative technology with applications spanning various domains. It encompasses a wide range of public and industrial vertical services that come with diverse and stringent Quality of Service (QoS) requirements. Traditional networks often struggle to meet the demands of these diverse IoT services. As a result, the introduction of 5G and Beyond 5G (B5G) networks holds promise in accommodating these diverse IoT services through network slicing technology. Network slicing involves partitioning a single physical network infrastructure into multiple logically isolated networks and ensures dedicated resources to each service as per QoS requirements. Additionally, Multi-Access Edge Computing (MEC) in B5G networks presents an innovative solution to facilitate low-latency communication for IoT services. However, the automatic provisioning and management of end-to-end (e2e) network slicing for IoT services across multi-domain infrastructures pose significant challenges, including manual error-prone resource configuration, network slice template preparation, and human intervention. This paper proposes an automated Artificial Intelligence (AI) and MEC-enabled solution for provisioning and managing network slice resources across multiple domains specifically tailored for IoT services. Our solution provides an abstraction layer that generates slice templates for each domain and automates the deployment of resources based on the specified QoS requirements. It automates the slice resource configuration process, reduces human intervention, and manages the complete lifecycle of IoT slices. We have conducted several tests with our system, creating multiple IoT slices, and have observed stable performance in slice design, resource provisioning, slice isolation, and management.","2644-125X","","10.1109/OJCOMS.2023.3329420","National Research Foundation of Korea (NRF); Korea Government (MSIT)(grant numbers:NRF-2022R1A4A1032361); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10304258","IoT;beyond 5G networks;MEC;SDN;network slicing;AI for 5G;service automation and management","Internet of Things;5G mobile communication;Quality of service;Network slicing;Artificial intelligence;Resource management;Computer architecture","","4","","61","CCBYNCND","1 Nov 2023","","","IEEE","IEEE Journals"
"Shadows: Blockchain Virtualization for Interoperable Computations in IIoT Environments","R. Tapwal; P. K. Deb; S. Misra; S. K. Pal","Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, West Bengal, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, West Bengal, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, West Bengal, India; Department of Mechanical Engineering, Indian Institute of Technology Kharagpur, Kharagpur, West Bengal, India",IEEE Transactions on Computers,"9 Feb 2023","2023","72","3","868","879","In this work, we propose Shadows, a virtual blockchain (VC) for achieving parallel consensus and efficient management of data in industries by utilizing BC. Typically, industrial processes involve heterogeneous activities which require real-time consensus, managed execution, isolation, data sharing, accelerated computation, and efficient utilization of various computational resources such as CPU, RAM, and storage. Achieving these in real-time using a single conventional blockchain (BC) leads to the exertion of computational power. To achieve resource-efficient real-time consensus, we virtualize the nodes of the BC network and create different BC for various activities. Further, to virtualize BC and provide better access to data, we propose smart contracts liable for providing a unified view of a single BC, dynamically creating BCs, allocating resources to these, and making communication between the same. Through lab-scale experiments, we demonstrate that Shadows is capable of utilizing the resources efficiently and achieving real-time consensus. In particular, Shadows uses 18% CPU and 92% memory while reducing consensus time by 56%, compared to a single conventional BC. Shadows also accesses the data efficiently by utilizing smart contracts and dynamically balances the load by migrating the virtual nodes. Further, Shadows reduces the number of migrations to make the balance system by 67%.","1557-9956","","10.1109/TC.2022.3184271","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9802863","Blockchain;industrial Internet of Things;osmotic computing;parallel consensus;resource allocation;smart contracts;virtualization","Smart contracts;Blockchains;Security;Resource management;Industries;Virtualization;Real-time systems","","4","","27","IEEE","21 Jun 2022","","","IEEE","IEEE Journals"
"Tactful Networking: Humans in the Communication Loop","R. L. Costa; A. C. Viana; A. Ziviani; L. N. Sampaio","Ècole Polytechnique/IPP and Inria, Palaiseau, France; Inria, Palaiseau, France; National Laboratory for Scientific Computing (LNCC), Petropolis, Brazil; Federal University of Bahia, Salvador, Brazil",IEEE Transactions on Emerging Topics in Computational Intelligence,"20 Jan 2021","2021","5","1","92","107","This survey discusses the human-perspective into networking through the Tactful Networking paradigm, whose goal is to add perceptive senses to the network by assigning it with human-like capabilities of observation, interpretation, and reaction to daily-life features and associated entities. To achieve this, knowledge extracted from inherent human behavior in terms of routines, personality, interactions, and others is leveraged, empowering the learning and prediction of user needs to improve QoE and system performance while respecting privacy and fostering new applications and services. Tactful Networking groups solutions from literature and innovative interdisciplinary human aspects studied in other areas. The paradigm is motivated by mobile devices' pervasiveness and increasing presence as a sensor in our daily social activities. With the human element in the foreground, it is essential: (i) to center big data analytics around individuals; (ii) to create suitable incentive mechanisms for user participation; (iii) to design and evaluate both human-aware and system-aware networking solutions; and (iv) to apply prior and innovative techniques to deal with human-behavior sensing and learning. This survey reviews the human aspect in networking solutions through over a decade, followed by discussing the tactful networking impact through literature in behavior analysis and representative examples. This paper also discusses a framework comprising data management, analytics, and privacy for enhancing human raw-data to assist Tactful Networking solutions. Finally, challenges and opportunities for future research are presented.","2471-285X","","10.1109/TETCI.2020.3039520","FAPERJ, FAPESB, CNPq, CAPES, INCT-CiD, and EMBRACE associate team Inria project(grant numbers:AAPG2019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9285188","Human behavioral perception;human-aware;next-generation networks","Social networking (online);Routing;Quality of service;Privacy;Sensors;Data privacy;Quality of experience","","4","","181","IEEE","7 Dec 2020","","","IEEE","IEEE Journals"
"Efficient Large-Scale Multiple Migration Planning and Scheduling in SDN-Enabled Edge Computing","T. He; A. N. Toosi; R. Buyya","CLOUDS Lab, School of Computing and Information Systems, University of Melbourne, Parkville, VIC, Australia; Department of Software Systems and Cybersecurity, Faculty of Information Technology, Monash University, Clayton, VIC, Australia; CLOUDS Lab, School of Computing and Information Systems, University of Melbourne, Parkville, VIC, Australia",IEEE Transactions on Mobile Computing,"7 May 2024","2024","23","6","6667","6680","Services provided by mobile edge clouds offer low-latency responses for large-scale and real-time applications. Dynamic service management algorithms generate live service migration requests to support user mobility and ensure service latency in mobile edge clouds. To handle these migration requests, multiple migration planning and scheduling algorithms are necessary to calculate the migration order and optimize the performance and overhead of multiple migrations. However, current planning and scheduling algorithms in cloud data centers are not suitable for dynamic and large-scale scenarios in edge computing, as the network topology expands and the number of migration requests increases. Edge computing requires near real-time scheduling to handle user mobility-induced live migrations. To address this issue, this paper presents an efficient multiple migration planning and scheduling framework for edge computing. The framework includes a lifecycle management framework and innovative iterative Maximal Independent Set-based scheduling algorithms based on the resource dependency graph of multiple migrations. Our solution is shown to efficiently schedule live migrations at scale using real-world taxi traces and telecom base station coordinates. It can achieve significant processing speedups over existing migration planning algorithms in clouds, up to 3,000 times, while ensuring multiple and individual migration performance for time-critical services.","1558-0660","","10.1109/TMC.2023.3326610","Australian Research Council(grant numbers:DP160102414,DP230100081); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290920","Mobile-edge computing;quality of service;service migration;multiple migration scheduling","Planning;Edge computing;Heuristic algorithms;Scheduling;Quality of service;Data centers;Cloud computing","","4","","47","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"QoS-Aware Online Service Provisioning and Updating in Cost-Efficient Multi-Tenant Mobile Edge Computing","S. Lu; J. Wu; P. Lu; N. Wang; H. Liu; J. Fang","Faculty of Information Technology, Beijing University of Technology, Beijing, China; Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA; Faculty of Information Technology, Beijing University of Technology, Beijing, China; Computer Science at Rowan University, Glassboro, NJ, USA; School of Software Engineering, Beijing Jiaotong University, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China",IEEE Transactions on Services Computing,"5 Feb 2024","2024","17","1","113","126","The vigorous development of IoT technology has spawned a series of applications that are delay-sensitive or resource-intensive. Mobile edge computing is an emerging paradigm that provides services between end devices and traditional cloud data centers to users. However, with the continuously increasing investment of demands, it is nontrivial to maintain a higher quality-of-service (QoS) under the erratic activities of mobile users. In this paper, we investigate the service provisioning and updating problem under the multiple-users scenario by improving the performance of services with long-term cost constraints. We first decouple the original long-term optimization problem into a per-slot deterministic one by using Lyapunov optimization. Then, we propose two service updating decision strategies by considering the trajectory prediction conditions of users. Based on that, we design an online strategy by utilizing the committed horizon control method looking forward to multiple slots predictions. We prove the performance bound of our online strategy theoretically in terms of the trade-off between delay and cost. Extensive experiments demonstrate the superior performance of the proposed algorithm.","1939-1374","","10.1109/TSC.2023.3335412","Fundamental Research Funds for the Central Universities(grant numbers:2021RC258); China Postdoctoral Science Foundation(grant numbers:2021M700366); National Natural Science Foundation of China(grant numbers:92267107); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10325614","Cost-efficient;mobile edge computing;online service provisioning;quality -of-service (QoS)","Costs;Delays;Quality of service;Optimization;Servers;Cloud computing;Trajectory","","4","","45","IEEE","22 Nov 2023","","","IEEE","IEEE Journals"
"Selected Code-Quality Characteristics and Metrics for Internet of Things Systems","M. Klima; M. Bures; K. Frajtak; V. Rechtberger; M. Trnka; X. Bellekens; T. Cerny; B. S. Ahmed","Department of Computer Science, Faculty of Electrical Engineering, Czech Technical University in Prague, Prague, Czech Republic; Department of Computer Science, Faculty of Electrical Engineering, Czech Technical University in Prague, Prague, Czech Republic; Department of Computer Science, Faculty of Electrical Engineering, Czech Technical University in Prague, Prague, Czech Republic; Department of Computer Science, Faculty of Electrical Engineering, Czech Technical University in Prague, Prague, Czech Republic; Department of Computer Science, Faculty of Electrical Engineering, Czech Technical University in Prague, Prague, Czech Republic; Lupovis.io, Glasgow, U.K.; Department of Computer Science, Baylor University, Waco, TX, USA; Department of Computer Science, Faculty of Electrical Engineering, Czech Technical University in Prague, Prague, Czech Republic",IEEE Access,"4 May 2022","2022","10","","46144","46161","Software code is present on multiple levels within current Internet of Things (IoT) systems. The quality of this code impacts system reliability, safety, maintainability, and other quality aspects. In this paper, we provide a comprehensive overview of code quality-related metrics, specifically revised for the context of IoT systems. These metrics are divided into main code quality categories: Size, redundancy, complexity, coupling, unit test coverage and effectiveness, cohesion, code readability, security, and code heterogeneity. The metrics are then linked to selected general quality characteristics from the ISO/IEC 25010:2011 standard by their possible impact on the quality and reliability of an IoT system, the principal layer of the system, the code levels and the main phases of the project to which they are relevant. This analysis is followed by a discussion of code smells and their relation to the presented metrics. The overview presented in the paper is the result of a thorough analysis and discussion of the author’s team with the involvement of external subject-matter experts in which a defined decision algorithm was followed. The primary result of the paper is an overview of the metrics accompanied by applicability notes related to the quality characteristics, the system layer, the level of the code, and the phase of the IoT project.","2169-3536","","10.1109/ACCESS.2022.3170475","Operational Programme (OP) Výzkum, vývoj a vzdělávání (VVV) Funded Project “Research Center for Informatics”(grant numbers:CZ.02.1.01/0.0/0.0/16_019/0000765); Knowledge Foundation of Sweden (KKS) through the Synergi Project AIDA-A Holistic Artificial Intelligence (AI)-Driven Networking and Processing Framework for Industrial Internet of Things (IoT)(grant numbers:Rek: 20200067); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762941","Code quality;code smells;Internet of Things;IoT;maintainability;metrics;quality characteristics;reliability;security","Codes;Measurement;Couplings;Internet of Things;Complexity theory;Security;Software reliability","","4","","92","CCBY","25 Apr 2022","","","IEEE","IEEE Journals"
"Adaptive Cruise Control Based on Real-Time DDS Middleware","B. Almadani; N. Alshammari; A. Al-Roubaiey","Department of Computer Engineering, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia; Department of Computer Engineering, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia; Department of Computer Engineering, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia",IEEE Access,"26 Jul 2023","2023","11","","75407","75423","This paper discusses the Adaptive Cruise Control (ACC) system, which is a solution that allows drivers to minimize the amount of time spent driving. It supports four different driving modes and regulates the acceleration and deceleration of the car in order to maintain a fixed speed or avoid a collision with another vehicle. Real-Time Publish-Subscribe (RTPS) middleware has emerged as one of the most efficient and practical options for the real solutions to the difficulties listed above. This paper proposes to develop a real-time system for integrating the various components of ACC, such as Information Cluster, Radar, Brake Switches, Cruise Switches, ACC Controller, Engine/Throttle Controller, Brake Controller, Brake Actuators, Speed Sensors, and Back Brake Lights. The exchange of data is through a RTPS Data Distribution Service (DDS) middleware. The design of the publish/subscribe model was explained in detail in this paper along with the proper Quality of Service (QoS) policies suggested to govern the behavior of the model.","2169-3536","","10.1109/ACCESS.2023.3296317","King Fahd University of Petroleum and Minerals; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10185018","Adaptive cruise control;DDS middleware;leading vehicle;quality of service policies;ACC controller;engine controller;brake controller","Quality of service;Brakes;Middleware;Control systems;Real-time systems;Cruise control;Vehicles","","4","","37","CCBYNCND","17 Jul 2023","","","IEEE","IEEE Journals"
"Flexible and scalable ML-based diagnosis module for optical networks: a security use case [Invited]","C. Natalino; L. Gifre; F. -J. Moreno-Muro; S. Gonzalez-Diaz; R. Vilalta; R. Munoz; P. Monti; M. Furdek","Electrical Engineering Department, Chalmers University of Technology, Gothenburg, Sweden; Centre Tecnològic de Telecomunicacions de Catalunya (CTTC/CERCA), Castelldefels, Spain; ATOS, Madrid, Spain; ATOS, Madrid, Spain; Centre Tecnològic de Telecomunicacions de Catalunya (CTTC/CERCA), Castelldefels, Spain; Centre Tecnològic de Telecomunicacions de Catalunya (CTTC/CERCA), Castelldefels, Spain; Electrical Engineering Department, Chalmers University of Technology, Gothenburg, Sweden; Electrical Engineering Department, Chalmers University of Technology, Gothenburg, Sweden",Journal of Optical Communications and Networking,"22 Jun 2023","2023","15","8","C155","C165","To support the pervasive digital evolution, optical network infrastructures must be able to quickly and effectively adapt to changes arising from traffic dynamicity or external factors such as faults and attacks. Network automation is crucial for enabling dynamic, scalable, resource-efficient, and trustworthy network operations. Novel telemetry solutions enable optical network management systems to obtain fine-grained monitoring data from devices and channels as the first step toward the near-real-time diagnosis of anomalies such as security threats and soft failures. However, the collection of large amounts of data creates a scalability challenge related to processing the data within the desired monitoring cycle regardless of the number of optical services being analyzed. This paper proposes a module that leverages the cloud native software deployment approach to achieve near-real-time machine learning (ML)-assisted diagnosis of optical channels. The results obtained over an emulated physical-layer security scenario demonstrate that the architecture successfully scales the necessary components according to the computational load and consistently achieves the desired monitoring cycle duration over a varying number of monitored optical channels.","1943-0639","","10.1364/JOCN.482932","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10159444","","Monitoring;Optical fiber networks;Optical feedback;Task analysis;Optical devices;Security;Scalability","","4","","","","22 Jun 2023","","","IEEE","IEEE Journals"
"A Comprehensive Survey on Resource Management in 6G Network Based on Internet of Things","S. S. Sefati; A. U. Haq; Nidhi; R. Craciunescu; S. Halunga; A. Mihovska; O. Fratu","Telecommunications Department, Faculty of Electronics, Telecommunications and Information Technology, National University of Science and Technology Politehnica Bucharest, Bucharest, Romania; Department of Business Development and Technology, Aarhus University, Aarhus, Denmark; Department of Business Development and Technology, Aarhus University, Aarhus, Denmark; Telecommunications Department, Faculty of Electronics, Telecommunications and Information Technology, National University of Science and Technology Politehnica Bucharest, Bucharest, Romania; Telecommunications Department, Faculty of Electronics, Telecommunications and Information Technology, National University of Science and Technology Politehnica Bucharest, Bucharest, Romania; SmartAvatar B.V. Amsterdam, North Holland, Netherlands; Telecommunications Department, Faculty of Electronics, Telecommunications and Information Technology, National University of Science and Technology Politehnica Bucharest, Bucharest, Romania",IEEE Access,"21 Aug 2024","2024","12","","113741","113784","The transition to 6th Generation (6G) cellular networks offers significant improvements over 5th Generation (5G), enhancing data transfer, reducing latency, and improving network reliability. Advanced Multiple-Input Multiple-Output (MIMO) technology in 6G boosts network efficiency, particularly benefiting Ultra-Reliable Low-Latency Communications (URLLC). This paper reviews literature on resource management in the Internet of Things (IoT) within the 6G context. We categorize the study into four segments: network-aware resource management, dynamic resource allocation, predictive resource distribution based on traffic and architecture, and energy-centric resource allocation considering IoT device mobility and location. We provide a detailed perspective on current research and highlight future research avenues. Key contributions include a comparative study of IoT resource management techniques, an overview of resource management across LTE, 5G, and 6G networks, insights into applications like Intelligent Transportation Systems (ITS), Industrial IoT (IIoT), and Mobile CrowdSensing (MCS), and an emphasis on upcoming challenges. We emphasize the crucial role of efficient resource management in IoT, particularly in the 6G landscape.","2169-3536","","10.1109/ACCESS.2024.3444313","Project “Mobility and Training foR beyond 5G ecosystems (MOTOR5G)”; European Union’s Horizon 2020 Program under the Marie Skłodowska Curie Actions (MSCA) Innovative Training Network (ITN)(grant numbers:861219); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10637461","6G network;Internet of Things;resource management;survey;quality of service (QoS)","6G mobile communication;Resource management;Internet of Things;5G mobile communication;Surveys;Quality of service;Routing protocols","","4","","210","CCBYNCND","15 Aug 2024","","","IEEE","IEEE Journals"
"Distributed Cost-Aware Fault-Tolerant Load Balancing in Geo-Distributed Data Centers","R. Tripathi; V. Sivaraman; V. Tamarapalli","Department of Information Technology, National Institute of Technology Raipur, Raipur, India; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Computer Science and Engineering, Indian Institute of Technology Guwahati, Guwahati, India",IEEE Transactions on Green Communications and Networking,"16 Feb 2022","2022","6","1","472","483","Many critical e-commerce and financial services running on distributed data centers require high availability. Recent surveys show that the frequency and duration of failures (either partial or complete) at a data center are increasing over the years. However, most of the literature does not address fault-tolerance in data centers. In this paper, we address the problem of load balancing in fault-tolerant data centers while minimizing the operating cost (due to energy consumption). We formally model the problem using linear optimization and propose a distributed two-stage algorithm to solve it. In the first stage, the proposed workload shifting algorithm distributes the load on a failed data center across the remaining ones with a marginal increase in the operating cost. Subsequently, the min-cost network flow model is used to derive a request mapping policy, where the quality of service (QoS) requirements are considered using a set of constraints on the delay. We evaluate the proposed algorithms using real-world data for the demand and the energy consumption cost (both brown and green energy). Results show that the proposed algorithm has low computational complexity, yet exactly gives the cost obtained by the global optimal solution.","2473-2400","","10.1109/TGCN.2021.3107915","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9523583","Optimization;data centre;renewable energies;fault tolerance;fault tolerant systems","Data centers;Load management;Servers;Fault tolerant systems;Fault tolerance;Delays;Optimization","","4","","38","IEEE","26 Aug 2021","","","IEEE","IEEE Journals"
"A Systematic Literature Review of Offensive and Defensive Security Solutions With Software Defined Network","A. Melis; A. A. Sadi; D. Berardi; F. Callegati; M. Prandini","Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy",IEEE Access,"4 Sep 2023","2023","11","","93431","93463","Software Defined Networking (SDN) is one of the most significant innovations in telecommunication systems in the past two decades. From the very beginning, the scientific community understood the importance of investigating the possible usages of SDN as a means to increase network security, but also their potential to be exploited as an attack device. For this reason, there has been a massive production of research works, which, however, do not form a well-defined corpus. The literature is spread over many venues and composed of contributions with very different flavors. Though some review works already exist, in this work we conduct a systematic literature review of the field, gathering 466 relevant publications— the largest curated dataset on the topic to the best of our knowledge. In our work, the dataset undergoes a twofold analysis: (a) quantitative, through publication metadata, which allows us to chart publication outlets, approaches, and tackled issues; (b) qualitative, through 14 research questions that provide an aggregated overview of the literature contributions to the key issues, also to spot gaps left open. From these analyses, we derive a call for action to address the main open challenges.","2169-3536","","10.1109/ACCESS.2023.3276238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10124203","Attack;defense;mitigation;security;SDN;threat","Security;Software defined networking;Systematics;Surveys;Bibliographies;Computer crime;Control systems","","4","","27","CCBYNCND","15 May 2023","","","IEEE","IEEE Journals"
"Efficient Edge Data Management Framework for IIoT via Prediction-Based Data Reduction","L. Yang; Y. Liao; X. Cheng; M. Xia; G. Xie","Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China",IEEE Transactions on Parallel and Distributed Systems,"7 Nov 2023","2023","34","12","3309","3322","Large amounts of time series data are required to support data analysis at the edge in the end-edge-cloud Industrial Internet of Things (IIoT) architecture. Reducing the storage cost is one of the main challenges in edge data management due to the limited storage resource of edge nodes. The state-of-the-art data reduction method has a high time overhead and poor reduction efficiency for unstable data sets. To solve this problem, this study proposes a time-series data management framework that combines data partition and data compression techniques. For the data partition technique, we propose an adaptive selection strategy to integrate the access pattern of the application and the characteristics of the time series data, thereby improving the partition accuracy. For the data compression technique, we propose a compression scheme based on time series data segmentation by using the idea of divide and conquer; we further introduce a change point detection technique to improve the compression efficiency for unstable data sets. Experimental results obtained with three types of real industrial data sets show that our framework is significantly better than the state-of-the-art method in terms of compression ratio and time overhead.","1558-2183","","10.1109/TPDS.2023.3327750","National Natural Science Foundation of China(grant numbers:62372167,61972139,62141212,62133014); Natural Science Foundation of Hunan Province(grant numbers:2021JJ30153,2022JJ10021); Natural Science Foundation of Chongqing(grant numbers:cstc2021jcyj-msxmX0817,cstc2021jcyj-msxmX0461,CSTB2022NSCQ-MSX1393); Chongqing Science and Technology Innovation Project(grant numbers:CQYC20220511500); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10297424","Data reduction;edge data management framework;industrial internet of things (IIoT)","Industrial Internet of Things;Image edge detection;Data compression;Computer architecture;Time series analysis;Real-time systems;Streams","","4","","36","IEEE","26 Oct 2023","","","IEEE","IEEE Journals"
"Enabling and Leveraging AI in the Intelligent Edge: A Review of Current Trends and Future Directions","T. Goethals; B. Volckaert; F. D. Turck","Department of Information Technology, IDLab, Ghent University—imec, Ghent, Belgium; Department of Information Technology, IDLab, Ghent University—imec, Ghent, Belgium; Department of Information Technology, IDLab, Ghent University—imec, Ghent, Belgium",IEEE Open Journal of the Communications Society,"14 Oct 2021","2021","2","","2311","2341","The use of AI on Smart applications and in the organization of the network edge presents a rapidly advancing research field, with a great variety of challenges and opportunities. This article aims to provide a holistic review of studies from 2019 to 2021 related to the Intelligent Edge, a concept comprising both the use of AI to organize edge networks (Edge Intelligence) and Smart applications in the edge. An introduction is given to the technologies required to understand the state of the art of AI in edge networks, and a taxonomy is provided with “Enabling Technology” for Edge Intelligence, “Organization” of the edge using AI, and AI “Applications” in the edge as its main topics. Research trend data from 2015 to 2020 is presented for various subdivisions of these topics, showing both absolute and relative research interest in each subtopic. The “Organization” aspect, being the main focus of this article, has a more fine-grained subdivision, explaining all contributing factors in detail. The trends indicate an exponential increase in research interest in nearly all subtopics, but significant differences between them. For each subdivision of the taxonomy a number of selected studies from 2019 to 2021 are gathered to form a high-level illustration of the state of the art of Edge Intelligence. From these selected studies and the trend data, a number of short-term challenges and high-level visions for Edge Intelligence are formulated, providing a basis for future work.","2644-125X","","10.1109/OJCOMS.2021.3116437","Vlaio by means of the FlexNet Research Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555813","Fog computing;fog networks;edge networks;edge computing;artificial intelligence;review;trends","Artificial intelligence;Software;Market research;Genomics;Taxonomy;Logistics;Training","","4","","163","CCBY","1 Oct 2021","","","IEEE","IEEE Journals"
"An IoT Edge Computing Framework Using Cordova Accessor Host","A. H. H. Ngu; J. S. Eyitayo; G. Yang; C. Campbell; Q. Z. Sheng; J. Ni","Department of Computer Science, Texas State University, San Marcos, TX, USA; Department of Computer Science, Texas State University, San Marcos, TX, USA; Department of Computer Science, Texas State University, San Marcos, TX, USA; Department of Computer Science, Texas State University, San Marcos, TX, USA; Department of Computing, Macquarie University, Sydney, NSW, Australia; Department of Computer Science, Texas State University, San Marcos, TX, USA",IEEE Internet of Things Journal,"22 Dec 2021","2022","9","1","671","683","The Internet of Things (IoT) is a rapidly growing system of physical sensors and connected devices, enabling advanced information gathering, interpretation, and monitoring. The realization of a versatile IoT edge computing framework will accelerate seamless integration of the cyber-world with new physical IoT devices, and will fundamentally change and empower the way humans interact with the world. While there are many cloud-based IoT computing frameworks, they cannot support the needs of IoT applications that require local processing and guarantee of consumer’s privacy. This article presents experimentation with the opensource plug-and-play IoT middleware, called Cordova Accesor Host. We demonstrated that Cordova Accessor Host supports the essential ingredients of the composition and reusability of IoT services using the accessor as the basic building block and adopting an accessor-module-plugin design pattern. The portability is demonstrated by using the same accessor for collecting sensor data from radically different IoT devices such as, wearables (e.g., smartwatches) and microcontrollers (e.g., Arduino). Our energy profiling experiments show that IoT services deployed using the Cordova Accessor Host consume around 35% less battery power than the same IoT services deployed in the native Android operating system.","2327-4662","","10.1109/JIOT.2021.3086043","National Science Foundation under the Research Experiences for Undergraduates site(grant numbers:CCF-1659807,CNS-1757893); Texas State University; NSF-CRI(grant numbers:1305302); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9446337","Edge computing;open service platform;service middleware and platform","Internet of Things;Sensors;Cloud computing;Edge computing;Servers;Programming;Operating systems","","4","","29","IEEE","3 Jun 2021","","","IEEE","IEEE Journals"
"Proactive Auto-Scaling Approach of Production Applications Using an Ensemble Model","M. Samir; K. T. Wassif; S. H. Makady","Faculty of Computers and Artificial Intelligence, Cairo University, Giza, Egypt; Faculty of Computers and Artificial Intelligence, Cairo University, Giza, Egypt; Faculty of Computers and Artificial Intelligence, Cairo University, Giza, Egypt",IEEE Access,"17 Mar 2023","2023","11","","25008","25019","The resource usage behaviors of application workloads are currently the primary concern of cloud providers offering hosting services. These services should be able to adapt to workload changes by automatically provisioning and de-provisioning resources so that, at all times, the existing resources in a system match the current service demand. Such behavior can be achieved manually by hiring a DevOps team to manage the application’s resources. Another option would be automating the resource provisioning processing using automated rules. Once such rules are met, the hosting environment will scale the resources accordingly. However, managing a DevOps team or creating flaky rules can lead to over-scaling application resources. This work proposes a new approach: a proactive auto-scaling framework built on an ensemble model. Such a model utilizes several machine learning techniques to scale application resources to match resource demand before the need arises. We evaluated our solution against three real production applications hosted on Cegedim Cloud Hosting Environment, an industrial environment serving several cloud applications from various domains, and against other machine learning models used in similar proactive auto-scaling experiments mentioned in past work. The experimentation results show that predicting application resources like CPU or RAM is feasible. Moreover, even in production environments, our ensemble model performs optimally in the CPU case and is near the optimal model when predicting RAM resources.","2169-3536","","10.1109/ACCESS.2023.3256302","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10066275","Auto-scaling;resource allocation;dynamic resource provisioning;resource management on clouds","Predictive models;Cloud computing;Machine learning;Prediction algorithms;Load modeling;Data models;Time series analysis","","3","","23","CCBYNCND","10 Mar 2023","","","IEEE","IEEE Journals"
"SatEdge: Platform of Edge Cloud at Satellite and Scheduling Mechanism for Microservice Modules","Y. Huang; X. Zhang; Z. Xu","School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China",IEEE Access,"14 Nov 2023","2023","11","","126283","126294","Edge cloud at satellite (ECS) is a newly developed edge computing (EC) technology that uses EC services offered by satellites to support high reliability and seamless global coverage. Satellites assume the role of computing and storage nodes for edge clouds, while terrestrial control centers function as cloud centers. In this paper, we propose a novel system and software architecture for the ECS to improve the cloud management of satellite networks and increase the flexibility of satellite service provision at the edge. Then, we propose a platform for the ECS based on KubeEdge called SatEdge. SatEdge has many function modules to meet the needs of the satellite-terrestrial network (STN) such as high reliability, high flexibility, and low latency. On this platform, we designed a microservice scheduling algorithm called optimal microservice scheduling with adaptivity and mobility (OMS-AM). OMS-AM can schedule a globally optimal workflow for microservice modules on the satellites to minimize task processing latency, failed task rate, and energy consumption. Compared with our last work, OMS-AM reduces the task processing latency by 14% at most. Additionally, OMS-AM improves the mobility of the current scheduling method put forth in our previous study, which may help lower the task failure rate. Energy usage and the total normalized costs are additional indicators of the efficiency of the microservice architecture.","2169-3536","","10.1109/ACCESS.2023.3330649","National Key Research and Development Program of China(grant numbers:2021YFB2900504,2020YFB1807900); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10309921","Edge cloud at satellite;satellite-terrestrial network;edge computing;microservice scheduling","Satellites;Microservice architectures;Cloud computing;Computer architecture;Containers;Edge computing;Task analysis","","3","","29","CCBYNCND","6 Nov 2023","","","IEEE","IEEE Journals"
"BayesTuner: Leveraging Bayesian Optimization For DNN Inference Configuration Selection","S. M. Nabavinejad; S. Reda","School of Computer Science, Institute for Research in Fundamental Sciences (IPM), Tehran, Iran; School of Engineering, Brown University, Providence, RI, USA",IEEE Computer Architecture Letters,"1 Dec 2021","2021","20","2","166","170","Deep learning sits at the core of many applications and products deployed on large-scale infrastructures such as data centers. Since the power consumption of data centers contributes significantly to operational costs and carbon footprint, it is essential to improve their power efficiency. To this end, both the hardware platform and application should be configured properly. However, identifying the best configuration automatically for a wide range of available options with affordable search cost is challenging (e.g., DNN batch size, number of cores, and amount of memory allocated to the application). Employing an exhaustive approach to test all the possible configurations is unfeasible. To tackle this challenge, we introduce BayesTuner that employs Bayesian Optimization to estimate the performance model of deep neural network inference applications under different configurations with a few test runs. Having these models, BayesTuner is able to differentiate the optimal or near-optimal configurations from the rest of options. Using a realistic setup with various DNNs, we show how efficiently BayesTuner can explore the huge state space of possible control configurations, and minimize the power consumption of the system, while meeting the throughput constraint of different DNNs.","1556-6064","","10.1109/LCA.2021.3123695","National Science Foundation(grant numbers:1814920); DoD ARO(grant numbers:W911NF-19-1-0484); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9612061","Deep neural network;power;throughput;Bayesian optimization","Graphics processing units;Power demand;Time factors;Runtime;Time-frequency analysis;Monitoring;Deep learning;Neural networks","","3","","24","IEEE","11 Nov 2021","","","IEEE","IEEE Journals"
"A Micro-Service Approach to Cloud Native RAN for 5G and Beyond","P. Song; H. Peng; X. Zhang","Networks and Edge Computing, Intel Beijing Research and Development Center, Beijing, China; Networks and Edge Computing, Intel Beijing Research and Development Center, Beijing, China; Networks and Edge Computing, Intel Shanghai Research and Development Center, Shanghai, China",IEEE Access,"23 Nov 2023","2023","11","","130257","130271","5G aims to support diverse applications with programmable infrastructure. Traditional RAN based on purpose-built-in hardware and monolithic software lack resiliency, programmability, and business agility. Cloud native virtualized RAN (vRAN) solves the issues by designing telecom applications into micro-service in cloud environment. This enables flexible virtualized network function deployment, efficient service provisioning, and on-demand resource usage. However, designing telecom RAN applications as micro-services has no guidelines but faces challenges such as high-precision synchronization and real-time processing requirements. This paper first introduces cloud native tenets and the current state of RAN cloudification and then evaluates the micro-service design of RAN software components. A micro-service approach for vDU user plane was proposed in this study, and a vDU prototype was developed based on the Intel x86 computing platform. System validation results proved the feasibility of inter-Pod communication processing latency, and capacity analytics predicted substantial capacity improvement for the proposed vDU scheme. The paper concludes with a summary and open points on the way forward for cloud native vRAN transformation.","2169-3536","","10.1109/ACCESS.2023.3332964","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10318161","5G NR;vRAN;vDU;cloud native;micro-service","Cloud computing;Software engineering;Telecommunications;5G mobile communication;Hardware;Computer architecture;Business;Microservice architectures","","3","","33","CCBYNCND","15 Nov 2023","","","IEEE","IEEE Journals"
"A Survey on the Use of Container Technologies in Autonomous Driving and the Case of BeIntelli","B. Acar; M. G. Augusto; M. Sterling; F. Sivrikaya; S. Albayrak","Chair of Agent Technology, Technische Universität Berlin, Berlin, Germany; Chair of Agent Technology, Technische Universität Berlin, Berlin, Germany; Chair of Agent Technology, Technische Universität Berlin, Berlin, Germany; Chair of Agent Technology, Technische Universität Berlin, Berlin, Germany; Chair of Agent Technology, Technische Universität Berlin, Berlin, Germany",IEEE Open Journal of Intelligent Transportation Systems,"28 Nov 2023","2023","4","","800","814","The application of containerization technology has seen a significant increase in popularity in recent years, both in the business and scientific sectors. In particular, the ability to create portable applications that can be deployed on different machines has become a valuable asset. Autonomous driving has embraced this technology, as it offers a wide range of potential applications, including the operation of autonomous vehicles and the digitization of infrastructure for the development of Cooperative, Connected, and Automated Mobility (CCAM) services. This paper provides a comprehensive analysis of containerization in autonomous driving, emphasizing its application, utility, benefits, and limitations.","2687-7813","","10.1109/OJITS.2023.3331449","German Federal Ministry for Digital and Transport (BMDV) through BeIntelli Project(grant numbers:01MM20004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10314411","Docker;containerization;automotive;CCAM;autonomous driving","Containers;Autonomous vehicles;Virtualization;Virtual machining;Operating systems;Microservice architectures;Industries","","3","","97","CCBYNCND","9 Nov 2023","","","IEEE","IEEE Journals"
"Network traffic characteristics of hyperscale data centers in the era of cloud applications","F. Yan; C. Xie; J. Zhang; Y. Xi; Z. Yao; Y. Liu; X. Lin; J. Huang; Y. Ce; X. Zhang; N. Calabretta","Alibaba Group, Alibaba Cloud, Beijing, China; Alibaba Group, Alibaba Cloud, New York, NY, USA; Beijing University of Posts and Telecommunications, Beijing, China; Alibaba Group, Alibaba Cloud, Beijing, China; Alibaba Group, Alibaba Cloud, New York, NY, USA; Alibaba Group, Alibaba Cloud, Beijing, China; Alibaba Group, Alibaba Cloud, Beijing, China; Alibaba Group, Alibaba Cloud, Beijing, China; Alibaba Group, Alibaba Cloud, Beijing, China; Alibaba Group, Alibaba Cloud, Beijing, China; Eindhoven University of Technology, Eindhoven, Netherlands",Journal of Optical Communications and Networking,"20 Sep 2023","2023","15","10","736","749","We present the network architecture of Alibaba Cloud DCs and investigate their traffic characteristics based on statistical data and captured traces. The statistical coarse-grained data are in the granularity of one minute, while the captured traces are fine-grained data that are in the granularity of one packet. We study the traffic features from the perspective of a macroscopic view, network performance, and microscopic view. The results report that the average utilization ratio of spine switches is stable when the observation time period reaches one day and the intra-ToR traffic ratio is in the range of 2%–10%. By mapping the folded-Clos topology to a tree topology and considering logical switching planes, we obtain the traffic matrix among pods from the average port utilization ratio. As we further investigate the perspective of network performance and the microscopic view, we find that there is no cell loss happening as the normalized queue speed ${Q_s}$ is lower than 0.4. The normalized queue speed ${Q_s}$ is defined as the total bytes of a queue sent in 1 s divided by 100 Gb, which reflects the packet sending speed of the queue. The observed maximum buffer size for one port conforms with the calculated maximum buffer occupation of 2.8 MB. By analyzing the captured traces, we find that the packet length is subject to a trimodal distribution. Under a time granularity of 10 ms, the instant bandwidth of one ToR port could reach 96 Gb/s at an average load of around 0.2 under a maximum link bandwidth of 100 Gb/s.","1943-0639","","10.1364/JOCN.494291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10258089","","Optical switches;Cloud computing;Bandwidth;Servers;Topology;Data centers;Optical fiber networks","","3","","","","20 Sep 2023","","","IEEE","IEEE Journals"
"A Flexible and Reliable Internet-of-Things Solution for Real-Time Production Tracking With High Performance and Secure Communication","B. -Y. Ooi; W. -K. Lee; M. J. W. Shubert; Y. -W. Ooi; C. -Y. Chin; W. -H. Woo","Faculty of Information and Communication Technology, Universiti Tunku Abdul Rahman, Kampar, Malaysia; Department of Computer Engineering, Gachon University, Seongnam-si, South Korea; Department of Electronics and Information, Ostbayerische Technische Hochschule Regensburg, Regensburg, Germany; Starfive Technology International Sdn Bhd, Penang, Malaysia; W Tech Marketing Sdn. Bhd., Ipoh, Malaysia; Arrow Systems Sdn Bhd, Lahat, Malaysia",IEEE Transactions on Industry Applications,"19 May 2023","2023","59","3","3121","3132","The concept of Industrial Revolution 4.0 (IR4.0) has attracted a lot of attention from academia and industry in recent years. Many existing manufacturers are forced to relook into their exiting production processes, exploring modern ways to improve the yields. Unfortunately, this is difficult to achieve when the data is limited. The situation is particularly serious for many SMEs in developing countries, wherein the manufacturing machines are not up-to-date and lack computational and connectivity capabilities. As an initiative to tackle this issue, we present an end-to-end Internet-of-Things (IoT) solution in this paper, aiming at tracking the production performance of old manufacturing machines reliably. This paper goes over the designs and reasoning behind the proposed solution. We also demonstrated that with careful optimization, high-performance secure encryption key encapsulation and decapsulation are achievable, which is critical for secure communication in IoT systems. As of the time of writing, our IoT system had been deployed in a real manufacturing environment and had been running continuously for approximately 365 days without data loss over the wide area network (WAN).","1939-9367","","10.1109/TIA.2023.3255214","Arrow Systems Sdn. Bhd.; Universiti Tunku Abdul Rahman, Malaysia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10065559","Industry 4.0;Internet-of-Things;production tracking;smart manufacturing","Manufacturing;Production;Reliability;Graphics processing units;Internet of Things;Productivity;Real-time systems","","3","","36","IEEE","10 Mar 2023","","","IEEE","IEEE Journals"
"A Secure and Distributed Placement for Quality of Service-Aware IoT Requests in Fog-Cloud of Things: A Novel Joint Algorithmic Approach","S. K. Srichandan; S. K. Majhi; S. Jena; K. Mishra; R. Bhat","Sambalpur University Institute of Information Technology, Sambalpur University, Burla, Odisha, India; Department of Computer Science and Information Technology, Guru Ghasidas Vishwavidyalaya, Bilaspur, Chhattisgarh, India; Sambalpur University Institute of Information Technology, Sambalpur University, Burla, Odisha, India; Gandhi Institute of Technology and Management, Visakhapatnam, Andhra Pradesh, India; Department of Computer Science and Engineering, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, India",IEEE Access,"25 Apr 2024","2024","12","","56730","56748","The fast proliferation of internet-enabled devices generates massive amounts of data every day from every aspect of life. These internet-enabled devices lack the storage, processing power, and capacity necessary to handle and store this massive amount of accurate and volumetric data. Cloud computing has been proposed as a compelling substitute to process these requests. However, the ingress traffic to the Cloud is huge which causes latency overhead due to the gap that exists between end devices and the Cloud datacentre. Additionally, processing these dynamic and heterogeneous requests with disparate requirements is a computationally NP-hard problem. In this regard, Fog computing appears to be an appealing solution to surpass the aforementioned challenges as a complementary to Cloud. Therefore, this research integrated a Fog layer between the end devices and Cloud datacenters enabling a collaborative computation framework. For the classification of requests and determining the target layers for processing, this research suggests an Adaptive Neuro-Fuzzy Inference System (ANFIS). Furthermore, an improved Honey Badger Algorithm (IHBA) is implemented for scheduling those requests at the target layer. To improve the convergence rate of the traditional HBA, a chaos mapping function has been implemented with an Opposition-based Learning (OBL) method. The proposed task consolidation approach has been validated using extensive simulations for QoS parameters on real-world benchmark datasets. With percentage improvements of 13.69%, 28.4%, 33.85%, 7.66%, 26.48%, and 5.63% for makespan, resource utilization, energy consumption, service delay, service cost, and delay violation, respectively, the obtained simulation results show that the suggested algorithm works better than the current state-of-the-art.","2169-3536","","10.1109/ACCESS.2024.3390723","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10504827","Adaptive neuro-fuzzy inference system (ANFIS);cloud computing;improved honey badger algorithm;resource management;tasks scheduling and offloading","Internet of Things;Cloud computing;Quality of service;Costs;Edge computing;Computational modeling;Biological system modeling;Fuzzy neural networks;Inference algorithms;Resource management;Task analysis;Scheduling","","3","","43","CCBYNCND","18 Apr 2024","","","IEEE","IEEE Journals"
"Solving Task Scheduling Problem in Mobile Cloud Computing Using the Hybrid Multi-Objective Harris Hawks Optimization Algorithm","B. Saemi; A. A. R. Hosseinabadi; A. Khodadadi; S. Mirkamali; A. Abraham","Department of Computer, Kavosh Institute of Higher Education, Mahmood Abad, Mazandaran, Iran; Department of Computer Science, University of Regina, Regina, Canada; Department of Computer Engineering, Islamic Azad University, Qazvin Branch, Qazvin, Iran; Department of Computer Engineering and IT, Payame Noor University, Tehran, Iran; School of Computer Science Engineering and Technology, Bennett University, Greater Noida, Uttar Pradesh, India",IEEE Access,"10 Nov 2023","2023","11","","125033","125054","Nowadays, mobile devices can run a wide range of programs, and they all require more and more processing power. Due to their limited resources, mobile devices often make use of cloud computing $'\text{s}$  offloading features to do more complex tasks. The offloading problem in Mobile Cloud Computing (MCC) is the task scheduling problem, which entails deciding where to dump work to maximize its value. The task scheduling problem in MCC is an NP-hard problem because of the difficulty in moving resources and the size of the search space required to find the ideal scheduler, making the use of extensive search techniques impractical. For this reason, metaheuristic search strategies are provided, to yield a best-case or near-best-case scenario in terms of job completion time and energy savings. This work provides a non-dominated multi-objective strategy based on the Harris Hawks Optimization (HHO) technique called Hybrid Multi-objective Harris Hawks Optimization (HMHHO) to handle the described issue in MCC. The objectives of this research were allocating jobs from mobile source nodes to processors in the public cloud, cloud patches, and processors in mobile resources. In comparison to the other four algorithms—the Genetic Algorithm (GA), the Ant Colony Optimization (ACO), the Particle Swarm Optimization (PSO), and the Cuckoo Search Algorithm (CSA) the proposed method completes jobs faster and uses less energy on average.","2169-3536","","10.1109/ACCESS.2023.3329069","Analytical Center for the Government of the Russian Federation, in November 2021(grant numbers:70-2021-00143,IGK 000000D730321P5Q0002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308570","Task scheduling;multi-objective;mobile cloud computing;optimization;metaheuristic algorithm;Harris Hawks optimization","Task analysis;Cloud computing;Optimization;Mobile handsets;Processor scheduling;Dynamic scheduling;Costs;Mobile computing;Metaheuristics","","3","","38","CCBYNCND","3 Nov 2023","","","IEEE","IEEE Journals"
"Container Power Consumption Prediction Based on GBRT-PL for Edge Servers in Smart City","D. Ou; C. Jiang; M. Zheng; Y. Ren","School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; Zhejiang Hithink RoyalFlush Artificial Intelligence Research Institute, Hangzhou, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China",IEEE Internet of Things Journal,"19 Oct 2023","2023","10","21","18799","18807","Edge computing and Internet of Things (IoT) devices have been widely deployed in smart city applications due to the rapid promotion and implementation of 5G communication technology. Limited by power supply and hardware computing capability, applications on edge servers are mostly deployed and run in the form of container microservices to improve resource utilization. Therefore, identifying the power consumption at the container granularity is of great importance for the load scheduling and service quality assurance of edge servers. In this article, a gradient boosting piecewise linear regression tree (GBRT-PL)-based container power prediction method is proposed. Performance metrics with a strong correlation between container and server power consumption are selected for power consumption modeling. To effectively suit the nonlinear relationship between container performance metrics and server power consumption, the integrated predictive capabilities of multiple regression trees (RTs) and the segmented linear model of single RT leaf nodes are applied. The data from the experiments prove that the GBRT-PL model predicts power consumption more accurately than other models for single and multiple container groups. The highest average relative error rate in the four multicontainer group tests is 6.72%, whereas the highest relative error rate in the 90% quantile is 11.66%. In addition, it can accurately predict the majority of power consumption peaks, which contributes to the precise detection of power consumption anomalies.","2327-4662","","10.1109/JIOT.2023.3281368","National Natural Science Foundation of China(grant numbers:61972118); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10143546","Container;edge computing;power consumption prediction;smart city","Containers;Power demand;Servers;Cloud computing;Predictive models;Data models;Virtual machining","","3","","27","IEEE","5 Jun 2023","","","IEEE","IEEE Journals"
"Comparing the Scalability of Communication Networks and Systems","T. Hossfeld; P. E. Heegaard; W. Kellerer","Chair of Communication Networks, University of Würzburg, Würzburg, Germany; Department of Information Security and Communication Technology, Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Chair of Communication Networks, TU Munich, Munich, Germany",IEEE Access,"22 Sep 2023","2023","11","","101474","101497","Scalability is often mentioned in literature, but a stringent definition is missing. In particular, there is no general scalability assessment which clearly indicates whether a system scales or not or whether a system scales better than another. The key contribution of this article is the definition of a scalability index (SI) which quantifies if a system scales in comparison to another system, a hypothetical system, e.g., linear system, or the theoretically optimal system. The suggested SI generalizes different metrics from literature, which are specialized cases of our SI. The primary target of our scalability framework is, however, benchmarking of two systems, which does not require any reference system. The SI is demonstrated and evaluated for different use cases, that are (1) the performance of an IoT load balancer depending on the system load, (2) the availability of a communication system depending on the size and structure of the network, (3) scalability comparison of different location selection mechanisms in fog computing with respect to delays and energy consumption; (4) comparison of time-sensitive networking (TSN) mechanisms in terms of efficiency and utilization. Finally, we discuss how to use and how not to use the SI and give recommendations and guidelines in practice. To the best of our knowledge, this is the first work which provides a general SI for the comparison and benchmarking of systems, which is the primary target of our scalability analysis.","2169-3536","","10.1109/ACCESS.2023.3314201","German Research Foundation [Deutsche Forschungsgemeinschaft (DFG)](grant numbers:316878574 (DFG SDN-App 2)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247010","Communication networks;performance;availability;scalability","Scalability;Elasticity;Communication networks;Time factors;Measurement;Time measurement;Indexes;Performance evaluation","","3","","32","CCBY","11 Sep 2023","","","IEEE","IEEE Journals"
"Multi-Microservice Migration Modeling, Comparison, and Potential in 5G/6G Mobile Edge Computing: A Non-Average Parameter Values Approach","A. Rezazadeh; H. Lutfiyya","Department of Computer Science, Western University, London, Canada; Department of Computer Science, Western University, London, Canada",IEEE Access,"19 Dec 2023","2023","11","","140464","140480","Cloud, fog, and edge computing integration with future mobile Internet-of-Things (IoT) devices and related applications in 5G/6G networks will become more practical in the coming years. Containers became the de facto virtualization technique that replaced Virtual Memory (VM). Mobile IoT applications, e.g., intelligent transportation and augmented reality, incorporating fog-edge, have increased the demand for a millisecond-scale response and processing time. Edge Computing reduces remote network traffic and latency. These services must run on edge nodes that are physically close to devices. However, classical migration techniques may not meet the requirements of future mission-critical IoT applications. IoT mobile devices have limited resources for running multiple services, and client-server latency worsens when fog-edge services must migrate to maintain proximity in light of device mobility. This study analyzes the performance of the MiGrror migration method and the pre-copy live migration method when the migration of heterogeneous multiple VMs/containers is considered. This paper presents mathematical models for the stated methods and provides migration guidelines and comparisons for services to be implemented as multiple containers, as in microservice-based environments. Experiments demonstrate that MiGrror outperforms the pre-copy technique and, unlike conventional live migrations, can maintain less than 10 milliseconds of downtime and reduce migration time with a minimal bandwidth overhead. The results show that MiGrror can improve service continuity and availability for users. Most significant is that the model can use average and non-average values for different parameters during migration to achieve improved and more accurate results, while other research typically only uses average values. This paper shows that using only average parameter values in migration can lead to inaccurate results.","2169-3536","","10.1109/ACCESS.2023.3342190","Natural Sciences and Engineering Research Council of Canada and Bell Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10356059","Downtime model;migration model;MiGrror;service continuity;service availability;microservices;heterogeneous multiple containers;multi-access/mobile edge computing (MEC);fog computing;live migration","Microservice architectures;Internet of Things;Containers;Edge computing;Cloud computing;Bandwidth;Computational modeling","","3","","54","CCBYNCND","13 Dec 2023","","","IEEE","IEEE Journals"
"A Privacy-Aware Approach for Managing the Energy of Cloud-Based IoT Resources Using an Improved Optimization Algorithm","Y. Chen; S. Hao; H. Nazif","School of Economics and Management, Harbin Institute of Technology, Harbin, China; School of Economics and Management, Harbin Institute of Technology, Harbin, China; Department of Mathematics, Payame Noor University, Tehran, Iran",IEEE Internet of Things Journal,"6 May 2022","2022","9","10","7362","7374","Cloud-based Internet of Things (IoT) is a network of physical things controlled and monitored via the Internet to develop numerous smart systems. In this network, numerous physical things with different types of sensors are created that can be linked to the Internet. IoT devices need to work with limited energy sources for a long time, and wireless communications are the primary source of energy usage. As a result, the lifespan of IoT devices can be extended by minimizing radio communications. Because of the great capability of the IoT, the issue of resource energy consumption is raised. On the other hand, privacy concerns must be considered when designing cloud-based IoT applications. The current article introduces programming formulations of the mentioned issues using an improved ant colony optimization (IACO) algorithm. The method is combined with a new kind of local search algorithm to obtain higher-quality responses. Also, for responding to privacy concerns, a privacy-preserving method has been suggested. The suggested approach has been tested widely, and the outcomes have been compared to the genetic and particle swarm optimization (PSO) algorithms. The results indicated the good ability of the proposed method in terms of network lifespan and energy consumption.","2327-4662","","10.1109/JIOT.2021.3112474","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9536866","ACO;cloud computing;clustering;energy;Internet of Things (IoT);network lifetime;privacy;routing","Internet of Things;Cloud computing;Energy consumption;Privacy;Routing;Energy efficiency;Clustering algorithms","","3","","53","IEEE","14 Sep 2021","","","IEEE","IEEE Journals"
"Assessment of Industrial Internet Platform Application in Manufacturing Enterprises: System Construction and Industrial Practice","J. Li; Y. Zhou; X. Zhang; S. Liu; Q. Li","Department of Automation, Tsinghua University, Beijing, China; China Industrial Control Systems Cyber Emergency Response Team, Beijing, China; China Industrial Control Systems Cyber Emergency Response Team, Beijing, China; China Industrial Control Systems Cyber Emergency Response Team, Beijing, China; Department of Automation, Tsinghua University, Beijing, China",IEEE Access,"28 Jul 2021","2021","9","","103709","103727","To evaluate the effectiveness of the industrial Internet Platform (IIP) applied by manufacturing enterprises, this study proposes a set of assessment systems for guiding and standardizing the application of IIP. Our assessment framework is established based on five dimensions: strategy and organization; basic conditions; platform application; business innovation; and efficiency and benefit. We collect IIP application data from more than 1400 manufacturing enterprises in China. The application performance of the IIP in manufacturing enterprises were comprehensively analyzed from different perspectives such as equipment on the cloud, industrial knowledge accumulation and reuse, and industrial APP innovative application. The research results show that the overall score of IIP application in China is 31.76 according to our assessment criterion. The breadth and depth of platform application need to be improved. Furthermore, this study provides guidance to a wide range of potential market participants, such as manufacturing enterprises, service providers and relevant policy makers, to assess the effectiveness of platform application.","2169-3536","","10.1109/ACCESS.2021.3098940","National Natural Science Foundation of China(grant numbers:61771281); National Social Science Fund of China(grant numbers:15ZDB151); National Key Research and Development Program of China(grant numbers:2018YFF0213700); 2018 Industrial Internet Platform Innovation and Development Project of China; 2018 Integrated Standardization of Intelligent Manufacturing and Application of New Models; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9492081","Industrial internet platform;assessment system;manufacturing enterprise","Manufacturing;Internet;Security;Computer architecture;Cloud computing;Technological innovation;Collaboration","","3","","47","CCBY","20 Jul 2021","","","IEEE","IEEE Journals"
"Distributed Digital Twins as Proxies-Unlocking Composability and Flexibility for Purpose-Oriented Digital Twins","A. Aziz; S. S. Chouhan; O. Schelén; U. Bodin","Department of Computer Science, Electrical and Space Engineering, Luleå University of Technology, Luleå, Sweden; Department of Computer Science, Electrical and Space Engineering, Luleå University of Technology, Luleå, Sweden; Department of Computer Science, Electrical and Space Engineering, Luleå University of Technology, Luleå, Sweden; Department of Computer Science, Electrical and Space Engineering, Luleå University of Technology, Luleå, Sweden",IEEE Access,"15 Dec 2023","2023","11","","137577","137593","In the realm of the Industrial Internet of Things (IoT) and Industrial Cyber-Physical Systems (ICPS), Digital Twins (DTs) have revolutionized the management of physical entities. However, existing implementations often face constraints due to hardware-centric approaches and limited flexibility. This article introduces a transformative paradigm that harnesses the potential of distributed digital twins as proxies, enabling software-centricity and unlocking composability and flexibility for purpose-oriented digital twin development and deployment. The proposed microservices-based architecture, rooted in service-oriented architecture (SOA) and microservices principles, emphasizes reusability, modularity, and scalability. Leveraging the Lean Digital Twin Methodology and packaged business capabilities expedites digital twin creation and deployment, facilitating dynamic responses to evolving industrial demands. This architecture segments the industrial realm into physical and virtual spaces, where core components are responsible for digital twin management, deployment, and secure interactions. By abstracting and virtualizing physical entities into individual digital twins, this approach lays the groundwork for purpose-oriented composite digital twin creation. Our key contributions involve a comprehensive exposition of the architecture, a practical proof-of-concept (PoC) implementation, and the application of the architecture in a use-case scenario. Additionally, we provide an analysis, including a quantitative evaluation of the proxy aspect and a qualitative comparison with traditional approaches. This assessment emphasizes key properties such as reusability, modularity, abstraction, discoverability, and security, transcending the limitations of contemporary industrial systems and enabling agile, adaptable digital proxies to meet modern industrial demands.","2169-3536","","10.1109/ACCESS.2023.3340132","projects Arrowhead flexible Production Value Network (fPVN) [EU Key Digital Technologies Joint Undertaking (KDT JU)](grant numbers:101111977); Mine.io (Horizon Europe)(grant numbers:101091885); AIMS5.0 (EU KDT JU)(grant numbers:10111208); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10345582","OT virtualization;distributed digital twin as proxy;edge;cloud;composition;flexibility","Digital twins;Industrial Internet of Things;Interoperability;Computer architecture;Security;Virtualization;Microservice architectures","","3","","66","CCBYNCND","5 Dec 2023","","","IEEE","IEEE Journals"
"A Comprehensive Survey on SmartNICs: Architectures, Development Models, Applications, and Research Directions","E. F. Kfoury; S. Choueiri; A. Mazloum; A. AlSabeh; J. Gomez; J. Crichigno","College of Engineering and Computing, University of South Carolina, Columbia, SC, USA; College of Engineering and Computing, University of South Carolina, Columbia, SC, USA; College of Engineering and Computing, University of South Carolina, Columbia, SC, USA; College of Engineering and Computing, University of South Carolina, Columbia, SC, USA; College of Engineering and Computing, University of South Carolina, Columbia, SC, USA; College of Engineering and Computing, University of South Carolina, Columbia, SC, USA",IEEE Access,"9 Aug 2024","2024","12","","107297","107336","The end of Moore’s Law and Dennard Scaling has slowed processor improvements in the past decade. While multi-core processors have improved performance, they are limited by the application’s level of parallelism, as prescribed by Amdahl’s Law. This has led to the emergence of domain-specific processors that specialize in a narrow range of functions. Smart Network Interface Cards (SmartNICs) can be seen as a revolutionary technology that combines heterogeneous domain-specific processors and general-purpose cores to offload infrastructure tasks. Despite the impressive advantages of SmartNICs and their importance in modern networks, the literature has been missing a comprehensive survey. To this end, this paper provides a background encompassing an overview of the evolution of NICs from basic to SmartNICs, describing their architectures, development environments, and advantages over legacy NICs. The paper then presents a comprehensive taxonomy of applications offloaded to SmartNICs, covering network, security, storage, and compute functions. Challenges associated with SmartNIC development and deployment are discussed, along with current initiatives and open research issues.","2169-3536","","10.1109/ACCESS.2024.3437203","National Science Foundation (NSF), Office of Advanced Cyberinfrastructure (OAC)(grant numbers:2118311,2403360,2346726); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10620201","SmartNIC;data processing unit (DPU);infrastructure processing unit (IPU);Moore’s law;application offloading;P4;application specific integrated circuit (ASIC);field programmable gate array (FPGA)","Surveys;Program processors;Computer architecture;Hardware;Software;Engines;Switches;Data processing;Moore's Law;Application specific integrated circuits;Field programmable gate arrays","","3","","222","CCBY","1 Aug 2024","","","IEEE","IEEE Journals"
"Resource Management From Single-Domain 5G to End-to-End 6G Network Slicing: A Survey","S. Ebrahimi; F. Bouali; O. C. L. Haas","Centre for Future Transport and Cities, Coventry University, Coventry, U.K.; Centre for Future Transport and Cities, Coventry University, Coventry, U.K.; Centre for Future Transport and Cities, Coventry University, Coventry, U.K.",IEEE Communications Surveys & Tutorials,"21 Nov 2024","2024","26","4","2836","2866","Network Slicing (NS) is one of the pillars of the fifth/sixth generation (5G/6G) of mobile networks. It provides the means for Mobile Network Operators (MNOs) to leverage physical infrastructure across different technological domains to support different applications. This survey analyzes the progress made on NS resource management across these domains, with a focus on the interdependence between domains and unique issues that arise in cross-domain and End-to-End (E2E) settings. Based on a generic problem formulation, NS resource management functionalities (e.g., resource allocation and orchestration) are examined across domains, revealing their limits when applied separately per domain. The appropriateness of different problem-solving methodologies is critically analyzed, and practical insights are provided, explaining how resource management should be rethought in cross-domain and E2E contexts. Furthermore, the latest advancements are reported through a detailed analysis of the most relevant research projects and experimental testbeds. Finally, the core issues facing NS resource management are dissected, and the most pertinent research directions are identified, providing practical guidelines for new researchers.","1553-877X","","10.1109/COMST.2024.3390613","Centre for Future Transport and Cities (CFTC), Coventry University, Coventry, U.K.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10504277","Network slicing;end-to-end (E2E);resource management;technological domains;radio access networks (RANs);transport networks (TNs);core networks (CNs);5G/6G networks;orchestration;resource allocation (RA)","Resource management;Surveys;5G mobile communication;Reviews;Research and development;Tutorials;Optimization","","3","","276","IEEE","17 Apr 2024","","","IEEE","IEEE Journals"
"Bringing Energy Efficiency Closer to Application Developers: An Extensible Software Analysis Framework","C. Marantos; L. Papadopoulos; C. P. Lamprakos; K. Salapas; D. Soudris","School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece",IEEE Transactions on Sustainable Computing,"8 Jun 2023","2023","8","2","180","193","Green, sustainable and energy-aware computing terms are gaining more and more attention during the last years. The increasing complexity of Internet of Things (IoT) applications makes energy efficiency an important requirement, imposing new challenges to software developers. Software tools capable of providing energy consumption estimations and identifying optimization opportunities are critical during all the phases of application development. This work proposes a novel framework that targets the energy efficiency at application development level. The proposed framework is implemented as a single user-friendly tool-flow, providing a variety of useful features, such as the estimation of the energy consumption without the need of executing the application on the targeted IoT devices and the estimation of potential gains by GPU acceleration on modern heterogeneous IoT architectures. The proposed methodology provides several novel contributions, such as the combination of static analysis and dynamic instrumentation approaches in order to exploit the advantages of both. The framework is evaluated on widely used benchmarks, achieving increased estimation accuracy (more than 90% for similar architectures and more than 72% for the potential use of the GPU). The effectiveness of the framework is further demonstrated using two industrial use-cases achieving an energy reduction from 91% up to 98%.","2377-3782","","10.1109/TSUSC.2022.3222409","EU's Horizon 2020 Research and Innovation Programme(grant numbers:780572); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9950623","Green computing;energy consumption;static analysis;dynamic instrumentation;software design","Software;Energy consumption;Internet of Things;Hardware;Graphics processing units;Estimation;Codes","","3","","60","IEEE","15 Nov 2022","","","IEEE","IEEE Journals"
"A Survey on Spatio-Temporal Big Data Analytics Ecosystem: Resource Management, Processing Platform, and Applications","H. Liang; Z. Zhang; C. Hu; Y. Gong; D. Cheng","School of Computer Science, Wuhan University, Wuhan, Hubei, China; School of Computer Science, Wuhan University, Wuhan, Hubei, China; School of Computer Science, Wuhan University, Wuhan, Hubei, China; School of Computer Science, Wuhan University, Wuhan, Hubei, China; School of Computer Science, Wuhan University, Wuhan, Hubei, China",IEEE Transactions on Big Data,"13 Mar 2024","2024","10","2","174","193","With the rapid evolution of the Internet, Internet of Things (IoT), and geographic information systems (GIS), spatio-temporal Big Data (STBD) is experiencing exponential growth, marking the onset of the STBD era. Recent studies have concentrated on developing algorithms and techniques for the collection, management, storage, processing, analysis, and visualization of STBD. Researchers have made significant advancements by enhancing STBD handling techniques, creating novel systems, and integrating spatio-temporal support into existing systems. However, these studies often neglect resource management and system optimization, crucial factors for enhancing the efficiency of STBD processing and applications. Additionally, the transition of STBD to the innovative Cloud-Edge-End unified computing system needs to be noticed. In this survey, we comprehensively explore the entire ecosystem of STBD analytics systems. We delineate the STBD analytics ecosystem and categorize the technologies used to process GIS data into five modules: STBD, computation resources, processing platform, resource management, and applications. Specifically, we subdivide STBD and its applications into geoscience-oriented and human-social activity-oriented. Within the processing platform module, we further categorize it into the data management layer (DBMS-GIS), data processing layer (BigData-GIS), data analysis layer (AI-GIS), and cloud native layer (Cloud-GIS). The resource management module and each layer in the processing platform are classified into three categories: task-oriented, resource-oriented, and cloud-based. Finally, we propose research agendas for potential future developments.","2332-7790","","10.1109/TBDATA.2023.3342619","National Key Research and Development Program of China(grant numbers:2023YFE0205700); National Natural Science Foundation of China(grant numbers:62302348); Fundamental Research Funds for the Central Universities(grant numbers:2042023kf0132); Hubei Provincial Natural Science Foundation of Chinan(grant numbers:2023AFB831); Hubei Luojia Laboratory(grant numbers:220100016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10356753","Artificial intelligence framework;Big Data system;cloud platform;database management system;geographic information system;resource management;spatio-temporal Big Data","Resource management;Big Data;Cloud computing;Geographic information systems;Artificial intelligence;Ecosystems;Data processing","","3","","130","IEEE","13 Dec 2023","","","IEEE","IEEE Journals"
"Optimized Task Scheduling and Virtual Object Management Based on Digital Twin for Distributed Edge Computing Networks","R. Xu; C. -W. Park; S. Khan; W. Jin; S. J. S. Moe; D. H. Kim","Big Data Research Center, Jeju National University, Jeju, Republic of Korea; Autonomous IoT Research Section, Electronics and Telecommunications Research Institute, Daejeon, South Korea; Big Data Research Center, Jeju National University, Jeju, Republic of Korea; Department of Electronic and Communication Engineering, Yanbian University, Jilin, Yanji, China; Department of Computer Engineering, Jeju National University, Jeju, South Korea; Department of Computer Engineering, Jeju National University, Jeju, South Korea",IEEE Access,"25 Oct 2023","2023","11","","114790","114810","In this paper, we address the challenge of limited resources in Internet of Things (IoT) devices by proposing a solution based on digital twin in distributed edge computing networks. Edge computing is a promising approach that moves computing resources closer to the network’s edge to reduce response times in IoT applications. However, simply offloading tasks from IoT devices to edge computing does not accelerate user control. To enhance task performance and improve user management experience, we introduce optimized task scheduling and virtual object management based on a digital twin concept. Our system incorporates virtualization, synchronization, visualization, and simulation functionalities to provide digital twin capabilities. Additionally, we develop a user-friendly web application with a graphical user interface (GUI) for intuitive management of edge computing services. To support our approach, we implement an edge computing supervisor that generates virtualized objects such as edge gateways, IoT devices, and services. These virtual objects serve as resources for creating tasks. Using our proposed digital twin platform, users can dynamically create new tasks based on demand, easily deploy and execute tasks in specific locations, and dynamically allocate edge network resources according to task requirements. An optimized task scheduling mathematical model is presented to compare task scheduling done with and without optimization. Further, the edge computing and digital twin based optimized task scheduling method is integrated with Federated Learning for collaborative learning and privacy preserved computation of sensors sensitive data. We demonstrate the effectiveness of our system by generating tasks for data collection related to indoor environment for prediction of Predicted Mean Vote (PMV) for thermal comfort index of smart homes occupants using HTTP and IoTivity-based devices in distributed edge computing networks. These tasks are properly delivered and executed on the expected edge gateways, showcasing the successful integration of our digital twin platform with edge computing networks. Further, the optimized task scheduling has improved the overall performance of the proposed system, keeping in view latency and processing time.","2169-3536","","10.1109/ACCESS.2023.3325475","Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT)(grant numbers:2022-0-00438); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10287327","Internet of Things;edge computing;federated learning;digital twin;task management;hyper-parameter optimization","Digital twins;Task analysis;Edge computing;Internet of Things;Logic gates;Federated learning;Processor scheduling","","3","","36","CCBYNCND","18 Oct 2023","","","IEEE","IEEE Journals"
"Edge-Cloud Architectures for Hybrid Energy Management Systems: A Comprehensive Review","O. Boiko; A. Komin; R. Malekian; P. Davidsson","Department of Information Technologies, Sumy State University, Sumy, Ukraine; Department of Information Technologies, Sumy State University, Sumy, Ukraine; Department of Computer Science and Media Technology and the Internet of Things and People Research Centre, Malmö University, Malmö, Sweden; Department of Computer Science and Media Technology and the Internet of Things and People Research Centre, Malmö University, Malmö, Sweden",IEEE Sensors Journal,"15 May 2024","2024","24","10","15748","15772","This article provides an overview of recent research on edge-cloud architectures in hybrid energy management systems (HEMSs). It delves into the typical structure of an IoT system, consisting of three key layers: the perception layer, the network layer, and the application layer. The edge-cloud architecture adds two more layers: the middleware layer and the business layer. This article also addresses challenges in the proposed architecture, including standardization, scalability, security, privacy, regulatory compliance, and infrastructure maintenance. Privacy concerns can hinder the adoption of HEMS. Therefore, we also provide an overview of these concerns and recent research on edge-cloud solutions for HEMS that addresses them. This article concludes by discussing the future trends of edge-cloud architectures for HEMS. These trends include increased use of artificial intelligence on an edge level to improve the performance and reliability of HEMS and the use of blockchain to improve the security and privacy of edge-cloud computing systems.","1558-1748","","10.1109/JSEN.2024.3382390","J. Gust. Richert stiftelse (Sweco Sverige) for the project Multi-Objective Optimization and Optimal Integration of Hybrid Energy Systems(grant numbers:01003438636); Kamprad Family Foundation (Familjen Kamprads Stiftelse) for the project Models of Distributed Information Processing in Smart Grid Systems; Knowledge Foundation (Stiftelsen för kunskapsoch kompetensutveckling) for the project Intelligent and Trustworthy IoT Systems(grant numbers:20220087-H-01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10492690","Distributed computing;distributed energy;domestic energy consumption;edge computing;edge intelligence;hybrid renewable energy systems;Internet of Energy;power systems;renewable energy sources;residential energy consumption;sustainable development;systems architecture;trustworthiness;user data privacy","Computer architecture;Cloud computing;Security;Renewable energy sources;Edge computing;Smart grids;Distributed computing;Sustainable development;Systems architecture;Data privacy","","3","","146","IEEE","4 Apr 2024","","","IEEE","IEEE Journals"
"Configurable Harris Hawks Optimisation for Application Placement in Space-Air-Ground Integrated Networks","N. Akhter; R. Mahmud; J. Jin; J. But; I. Ahmad; Y. Xiang","School of Science, Computing and Engineering Technologies, Swinburne University of Technology, Melbourne, VIC, Australia; School of Electrical Engineering, Computing and Mathematical Sciences, Curtin University, Perth, WA, Australia; School of Science, Computing and Engineering Technologies, Swinburne University of Technology, Melbourne, VIC, Australia; School of Science, Computing and Engineering Technologies, Swinburne University of Technology, Melbourne, VIC, Australia; School of Engineering, Edith Cowan University, Perth, WA, Australia; School of Information Technology, Deakin University, Geelong, VIC, Australia",IEEE Transactions on Network and Service Management,"15 Apr 2024","2024","21","2","1724","1736","Space-Air-Ground Integrated Network (SAGIN) has recently emerged as a viable solution for reliable transmission, high data rates, and seamless connectivity with extensive coverage. However, the characteristics of the computation and communication devices located at various levels of SAGIN make application placement within such environments a challenging task. Real-time service expectations and resource requirements of applications further intensify this issue, and push the domain to operate beyond its capacity, resulting in uneven delays and significant overhead. Taking these constraints into account, SAGIN’s application placement problem can be expressed as a multiobjective optimisation problem. This paper aims to solve such a problem using a Dynamic Weight-configurable Harris Hawks Optimisation (DW-HHO) algorithm, considering diverse application contexts such as deadlines, resource usage and the number of application activities. It simultaneously minimises application total service time and host resource overhead with a robust global search. The performance of the proposed solution is compared with benchmark metaheuristic solutions such as PSO, NSGA-II, Greedy and Random. Experimental results demonstrate that DW-HHO outperforms other benchmark metaheuristic solutions in optimising resource utilisation and service delivery time of applications in SAGIN environments. The proposed DW-HHO demonstrates notable improvements over existing methods. Specifically, when evaluating the total service time for PSO, NSGA-II, Greedy, and Random, DW-HHO outperforms these methods by 7.28%, 9.07%, 13.01%, and 14.97%, respectively.","1932-4537","","10.1109/TNSM.2024.3361176","Australian Research Council Linkage Project(grant numbers:LP190100594); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10418264","Application placement;Harris Hawks optimisation (HHO);space-air-ground integrated network (SAGIN)","Space-air-ground integrated networks;Resource management;Computer architecture;Satellites;Delays;Computational modeling;Australia","","3","","31","IEEE","1 Feb 2024","","","IEEE","IEEE Journals"
"TENET: Adaptive Service Chain Orchestrator for MEC-Enabled Low-Latency 6DoF Virtual Reality","A. Medeiros; A. D. Maio; T. Braun; A. Neto","Institute of Computer Science, University of Bern, Bern, Switzerland; Institute of Computer Science, University of Bern, Bern, Switzerland; Institute of Computer Science, University of Bern, Bern, Switzerland; Department of Informatics and Applied Mathematics, Federal University of Rio Grande do Norte, Natal, Brazil",IEEE Transactions on Network and Service Management,"15 Apr 2024","2024","21","2","1894","1911","The next generation of Virtual Reality (VR) applications is expected to provide advanced experiences through Six Degrees of Freedom (6DoF) content, which requires higher data rates and ultra-low latency. In this article, we refactor 6DoF VR applications into atomic services to increase the computing capacity of VR systems aiming to reduce the end-to-end (E2E) of 6DoF VR applications. Those services are chained and deployed across Head-Mounted Displays (HMDs) and Multi-access Edge Computing (MEC) servers in high mobility scenarios over real-edge network topologies. We investigate the Distributed Service Chain Problem (DSCP) to find the optimal service placement of services from a service chain such that its E2E latency does not exceed 5 ms. The DSCP problem is  $\mathcal {NP}$ -hard. We provide an integer linear program to model the system, along with a heuristic, namely disTributed sErvice chaiN orchEstraTor (TENET), which is one order of magnitude faster than optimally solving the DSCP problem. We compare TENET to DSCP implementation and well-known service migration algorithms in terms of E2E latency, power consumption, video resolution selection based on E2E latency, context migrations, and execution time. We observe a significant reduction of E2E latency and gains in more advanced video resolution selection and accepted context service migrations when using TENET’s deployment strategy on VR services.","1932-4537","","10.1109/TNSM.2023.3331755","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10314807","Mobile virtual reality;end-to-end latency;six degrees of freedom videos;multi-access edge computing;service function chaining;service offloading;service migration;quality of service","Streaming media;Resists;Servers;Quality of service;Low latency communication;Energy consumption;Computational modeling","","3","","37","CCBY","10 Nov 2023","","","IEEE","IEEE Journals"
"Enhancing Rescue Operations With Virtualized Mobile Multimedia Services in Scarce Resource Devices","A. Atutxa; J. Astorga; M. Huarte; E. Jacob; J. Unzilla","Department of Communications Engineering, Faculty of Engineering, University of the Basque Country, Bilbao, Spain; Department of Communications Engineering, Faculty of Engineering, University of the Basque Country, Bilbao, Spain; Department of Communications Engineering, Faculty of Engineering, University of the Basque Country, Bilbao, Spain; Department of Communications Engineering, Faculty of Engineering, University of the Basque Country, Bilbao, Spain; Department of Communications Engineering, Faculty of Engineering, University of the Basque Country, Bilbao, Spain",IEEE Access,"8 Dec 2020","2020","8","","216029","216042","The aim of this article is to present an architecture to support reconfigurable multimedia services for a practical emergency environment use case of rescue operations. Specifically, radio communication and video surveillance services are provided by means of a small device carried by a mobile vehicle. This solution is validated by implementing a digital mobile radio (DMR) standard radio hotspot and a video streaming server in an unmanned aerial vehicle (UAV) that conveys a device with scarce resources in order to minimize power consumption. To achieve a fast and flexible deployment of the envisioned services, a virtualization-based approach is proposed. Namely, a Kubernetes orchestrator is used to manage the life-cycle of services deployed on a small resource device, endowing the architecture with scalability and management flexibility. This article describes the executed performance tests that measured key parameters such as deployment time and recovery time after disconnection. Highly promising results were obtained, showing that the proposed architecture can be deployed in less than 4 minutes and can recover from network disconnections in less than 10 seconds. Thus, the performance, reliability and flexibility of the overall solution are demonstrated.","2169-3536","","10.1109/ACCESS.2020.3041394","Spanish Ministry of Economy, Industry and Competitiveness through the State Secretariat for Research, Development and Innovation under the Adaptive Management of 5G Services to Support Critical Events in Cities (5G-City) Project(grant numbers:TEC2016-76795-C6-5-R); Department of Economic Development and Competitiveness of the Basque Government through the 5G4BRIS Research Project(grant numbers:KK-2020/00031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272962","Application virtualization;emergency services;multimedia communication;radio communication","Streaming media;Containers;Virtualization;Performance evaluation;Time measurement;Reliability;Multimedia communication","","3","","33","CCBY","30 Nov 2020","","","IEEE","IEEE Journals"
"A Survey on Addressing IoT Security Issues by Embedding Blockchain Technology Solutions: Review, Attacks, Current Trends, and Applications","N. T. Y. Huan; Z. A. Zukarnain","Department of Computer Network, Faculty Science and Information Technology, University Putra Malaysia, Seri Kembangan, Selangor, Malaysia; Faculty of Computer Science and Information Technology, University Putra Malaysia, Seri Kembangan, Malaysia",IEEE Access,"21 May 2024","2024","12","","69765","69782","By 2025, the Internet of Things (IoT) infrastructure is projected to encompass over 75 billion devices, facilitated by the increasing proliferation of intelligent applications. The IoT ecosystem consists of sensors that function as data generators and applications that necessitate financial transactions to compensate the data producers. Security is a highly important concern. Typically, IoT gadgets are vulnerable to security breaches, and the advancement of industrial systems might introduce severe security weakness. The rapid evolution of IoT technologies and deployment scenarios makes the existing research challenging to keep pace with emerging threats and vulnerabilities. Employing blockchain technology makes it feasible to enhance security by maintaining payments in a ledger that is not just secure but also translucent, distributed, and immutable. This article provides an introductory overview of the IoT and subsequently delves into the many security threats and vulnerabilities arising within the IoT framework. This study provided an overview of the blockchain, focusing on its categorization and important properties. Moreover, this article examines the necessity of combining blockchain technology with the IoT, in addition to reviewing relevant literature and the studies conducted by other scholars. This article offers insight into the uses of blockchain on the Internet of Things (IoT).","2169-3536","","10.1109/ACCESS.2024.3378592","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10473992","Blockchain;integration with IoT;blockchain characteristics;IoT security issues","Internet of Things;Security;Blockchains;Sensors;Peer-to-peer computing;Routing;Malware","","3","","127","CCBYNCND","18 Mar 2024","","","IEEE","IEEE Journals"
"$\mu$μ-DDRL: A QoS-Aware Distributed Deep Reinforcement Learning Technique for Service Offloading in Fog Computing Environments","M. Goudarzi; M. A. Rodriguez; M. Sarvi; R. Buyya","Cloud Computing and Distributed Systems (CLOUDS) Laboratory, School of Computing and Information Systems, University of Melbourne, Parkville, VIC, Australia; School of Computing and Information Systems, University of Melbourne, Parkville, VIC, Australia; Department of Infrastructure Engineering, University of Melbourne, Parkville, VIC, Australia; Cloud Computing and Distributed Systems (CLOUDS) Laboratory, School of Computing and Information Systems, University of Melbourne, Parkville, VIC, Australia",IEEE Transactions on Services Computing,"5 Feb 2024","2024","17","1","47","59","Fog and Edge computing extend cloud services to the proximity of end users, allowing many Internet of Things (IoT) use cases, particularly latency-critical applications. Smart devices, such as traffic and surveillance cameras, often do not have sufficient resources to process computation-intensive and latency-critical services. Hence, the constituent parts of services can be offloaded to nearby Edge/Fog resources for processing and storage. However, making offloading decisions for complex services in highly stochastic and dynamic environments is an important, yet difficult task. Recently, Deep Reinforcement Learning (DRL) has been used in many complex service offloading problems; however, existing techniques are most suitable for centralized environments, and their convergence to the best-suitable solutions is slow. In addition, constituent parts of services often have predefined data dependencies and quality of service constraints, which further intensify the complexity of service offloading. To solve these issues, we propose a distributed DRL technique following the actor-critic architecture based on Asynchronous Proximal Policy Optimization (APPO) to achieve efficient and diverse distributed experience trajectory generation. Also, we employ PPO clipping and V-trace techniques for off-policy correction for faster convergence to the most suitable service offloading solutions. The results obtained demonstrate that our technique converges quickly, offers high scalability and adaptability, and outperforms its counterparts by improving the execution time of heterogeneous services.","1939-1374","","10.1109/TSC.2023.3332308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10316602","Deep Reinforcement Learning (DRL);Fog/Edge Computing;Internet of Things (IoT);QoS- aware service Offloading","Edge computing;Internet of Things;Quality of service;Trajectory;Servers;Stochastic processes;Costs","","3","","48","IEEE","13 Nov 2023","","","IEEE","IEEE Journals"
"Cybertwin Based Cloud Native Networks","Q. Yu; D. Liang; M. Qin; J. Chen; H. Zhou; J. Ren; Y. Li; J. Wu; Y. Gao; W. Zhang","Peng Cheng Laboratory, Shenzhen 518055, China; Peng Cheng Laboratory, Shenzhen 518055, China; Peng Cheng Laboratory, Shenzhen 518055, China; Peng Cheng Laboratory, Shenzhen 518055, China; School of Electronic Science and Engineering, Nanjing University, Nanjing 210008, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu 611730, China; Peng Cheng Laboratory, Shenzhen 518055, China; School of Computer Science, Fudan University, Shanghai 200433, China; Intelligent Networking and Computing Research Center and School of Computer Science, Fudan University, Shanghai 200433, China; School of Electrical Engineering and Telecommunications, University of New South Wales, Sydney 2052, Australia",Journal of Communications and Information Networks,"4 Oct 2023","2023","8","3","187","202","With the emerging applications of the Internet of things, artificial intelligence, and satellite communications, the future network will be featured as the Internet of everything around the globe. The network heterogeneity, applications cloudification, and personalized user services demand a revolutionary change in the network architecture. With the rapid development of cloud native technology, the new network should support heterogeneous networks and personalized quality of services for users. In this paper, we propose a Cybertwin-based cloud native network (CCNN) that merges the radio access network (RAN), the IP bearer network, and the data center network and is based on the cloud native data center network using Kubernetes as a network operating system for unified virtualization of computing, storage, and network resources, unified scheduling and allocation, and unified operation and management. Then, we propose a fully decoupled RAN architecture that can flexibly and efficiently utilize the resource for personlized user services. We also propose a Cybertwin-based management framework built on Kubernetes for integrated networking, computing and storage resource scheduling. Finally, we design an immunology-inspired intrinsic security architecture with zero trust security system and adaptive defense system. The proposed CCNN is a new network architecture expected to address future generation communications and networks challenges.","2509-3312","","10.23919/JCIN.2023.10272347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10272347","6G;network architecture;Cybertwin;cloud native;network security","Cloud computing;5G mobile communication;Wireless fidelity;Internet;Servers;Computer architecture;Network architecture","","3","","","","4 Oct 2023","","","PTP","PTP Journals"
"From Digital Twins to Digital Twin Prototypes: Concepts, Formalization, and Applications","A. Barbie; W. Hasselbring","Software Engineering Group, Kiel University, Kiel, Germany; Software Engineering Group, Kiel University, Kiel, Germany",IEEE Access,"31 May 2024","2024","12","","75337","75365","The transformation to Industry 4.0 also transforms the processes of developing intelligent manufacturing production systems. Digital twins may be employed to advance the development of these new (embedded) software systems. However, there is no consensual definition of what a digital twin is. In this paper, we provide an overview of the current state of the digital twin concept and formalize the digital twin concept using the Object-Z notation. This formalization includes the concepts of physical twins, digital models, digital templates, digital threads, digital shadows, digital twins, and digital twin prototypes. The relationships between all these concepts are visualized as class diagrams using the Unified Modeling Language. Our digital twin prototype approach supports engineers in the development and automated testing of complex embedded software systems. This approach enables engineers to test embedded software systems in a virtual context without the need of a connection to a physical object. In continuous integration/continuous deployment pipelines, such digital twin prototypes can be used for automated integration testing and, thus, allow for an agile verification and validation process. In this paper, we demonstrate and report on the application and implementation of a digital twin using the example of two real-world field studies (ocean observation systems and smart farming). For independent replication and extension of our approach by other researchers, we provide a laboratory study published open source on GitHub.","2169-3536","","10.1109/ACCESS.2024.3406510","Deutsche Forschungsgemeinschaft (DFG) through Open Access Publikationskosten; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10540125","Cyber-physical systems;embedded software systems;digital twin prototypes;automated testing;continuous integration","Digital twins;Unified modeling language;Embedded software;Testing;NASA;Mathematical models;Industrial Internet of Things;Cyber-physical systems;Testing;Automatic testing","","3","","68","CCBY","28 May 2024","","","IEEE","IEEE Journals"
"Principles to Design Smart Physical Objects as Adaptive Recommenders","F. Cena; L. Console; A. Matassa; I. Torre","Dipartimento di Informatica, University of Torino, Torino, Italy; Dipartimento di Informatica, University of Torino, Torino, Italy; Dipartimento di Informatica, University of Torino, Torino, Italy; Dip. di Informatica, Bioingegneria, Robotica, Ingegneria dei Sistemi, University of Genova, Genova, Italy",IEEE Access,"13 Nov 2017","2017","5","","23532","23549","Recommenders have proven to be useful means to support people in their activities and in making decisions. They evolved from online recommenders to context-aware and ubiquitous recommenders. Moving forward along this line, this paper introduces the new emerging class of smart physical recommenders: context-aware recommender systems that are embedded into physical everyday objects. This paper describes the features of these systems and presents a conceptual model to design them, by analyzing a number of issues that have to be addressed by a designer and discussing the consequences of different design choices with their impact on the smartness of the designed object. The model is structured in a number of layers corresponding to different conceptual design phases in which different requirements are analyzed. The contribution of this paper is to discuss and provide design guidelines for a new rising class of recommenders that combine the features of intelligent agents, cyber-physical objects, and recommender-support systems. The description of the model is complemented by an exemplary analysis of its application.","2169-3536","","10.1109/ACCESS.2017.2765746","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8086157","Adaptive systems;context-aware recommenders;multi-layered design;smart objects","Analytical models;Intelligent agents;Cognition;Computational modeling;Instruments;Tools","","3","","52","OAPA","27 Oct 2017","","","IEEE","IEEE Journals"
"Enabling Service-Oriented Manufacturing Through Architectures, Models, and Protocols","S. Gaiardelli; S. Spellini; M. Panato; C. Tadiello; M. Lora; D. S. Cheng; F. Fummi","Department of Engineering for Innovation Medicine (DIMI), University of Verona, Verona, Italy; FACTORYAL S.r.l., San Giovanni Lupatoto, Italy; FACTORYAL S.r.l., San Giovanni Lupatoto, Italy; FACTORYAL S.r.l., San Giovanni Lupatoto, Italy; Department of Engineering for Innovation Medicine (DIMI), University of Verona, Verona, Italy; Department of Engineering for Innovation Medicine (DIMI), University of Verona, Verona, Italy; Department of Engineering for Innovation Medicine (DIMI), University of Verona, Verona, Italy",IEEE Access,"21 Jun 2024","2024","12","","85259","85274","Modern production lines are often composed of machinery from different vendors that must be connected with each other to increase the overall interoperability and flexibility. A common problem that arises in such systems is the complexity of the configuration task: they usually require each component to be manually configured. Thus, machinery requires different configuration strategies, negatively impacting scalability and increasing the chance of human errors. Furthermore, each time the manufacturing system has to be updated, the entire procedure must be repeated. This paper proposes a software architecture abstracting the complexity of existing production lines by enabling the service-oriented manufacturing paradigm. Then, it presents a strategy to model manufacturing systems, covering the topology of the production plant, machinery, and production recipes. The paper also proposes a model-based methodology to automatically configure the reference software architecture and hence the machines in the system. The application of the contributions to a fully-fledged production line shows the effectiveness of relying on model-based automatic configuration.","2169-3536","","10.1109/ACCESS.2024.3385634","European Union’s Horizon 2020 Research and Innovation Program through the Marie Skłodowska-Curie(grant numbers:894237); European Union Next-GenerationEU (Piano Nazionale di Ripresa e Resilienza (PNRR)—Missione 4, Componente 2, and Investimento 1.5—D.D. 1058 23/06/2022) through the PNRR Research Activities of the Consortium Interconnected North-Est Innovation Ecosystem (iNEST)(grant numbers:ECS_00000043); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10493040","Smart manufacturing;computer aided manufacturing;flexible manufacturing systems;software architecture;systems modeling","Computer architecture;Manufacturing;Unified modeling language;Automation;Servers;Task analysis;Smart manufacturing;Computer aided manufacturing;Flexible manufacturing systems;Software architecture;Systems engineering and theory","","3","","34","CCBY","8 Apr 2024","","","IEEE","IEEE Journals"
"An Efficient Algorithm for Microservice Placement in Cloud-Edge Collaborative Computing Environment","X. He; H. Xu; X. Xu; Y. Chen; Z. Wang","Faculty of Computing, Harbin Institute of Technology, Harbin, China; Faculty of Computing, Harbin Institute of Technology, Harbin, China; Faculty of Computing, Harbin Institute of Technology, Harbin, China; Faculty of Computing, Harbin Institute of Technology, Harbin, China; Faculty of Computing, Harbin Institute of Technology, Harbin, China",IEEE Transactions on Services Computing,"8 Oct 2024","2024","17","5","1983","1997","Microservices along with cloud-edge computing technologies are widely adopted to take advantage of the abundant computing resources of the cloud and the low latency, high bandwidth capabilities of the edge. However, factors such as frequent user requirement changes have made the current deployment scheme not fully adaptable to new requirements, resulting in an increase in average response time. Therefore, the microservice system needs to adjust the microservice deployment scheme online in response to continuously changing user requirements to reduce the average response time, which is known as the cloud-edge collaborative microservice deployment problem. However, existing methods are not able to meet the efficiency requirements and do not fully consider the complex dependencies of microservices and budget constraints in the cloud. To address this problem, this paper proposes a solution to the cloud-edge collaborative microservice deployment problem by modeling it as an NP-hard integer nonlinear programming problem in a cloud-edge environment consisting of private edge clouds and public cloud. An efficient Two-stage Iterated Greedy Optimization (TIGO) algorithm is also proposed and its convergence is proven. Extensive experimental results show that this approach achieves better average response times in less time compared to existing methods.","1939-1374","","10.1109/TSC.2024.3399650","National Key Research and Development Program of China(grant numbers:2022ZD0115404); National Natural Science Foundation of China(grant numbers:62372140,61832014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10528881","Cloud-Edge computing;microservice system;service deployment;complex dependencies","Microservice architectures;Cloud computing;Time factors;Servers;Costs;Containers;Quality of service","","3","","48","IEEE","10 May 2024","","","IEEE","IEEE Journals"
"C4ISR Service Deployment Based on an Improved Quantum Evolutionary Algorithm","Z. Jiao; J. Zhang; P. Yao; L. Wan; X. Wang","Graduate College, Air Force Engineering University, Xi’an, China; Information and Navigation College, Air Force Engineering University, Xi’an, China; Information and Navigation College, Air Force Engineering University, Xi’an, China; ACT Navigation, Air Force Engineering University, Xi’an, China; Experimental Training Base of School of Information and Communication, University of National Defense Science and Technology, Xi’an, China",IEEE Transactions on Network and Service Management,"10 Jun 2021","2021","18","2","2405","2419","To overcome the limited communication bandwidth in the deployment of command, control, communications, computers, intelligence, surveillance, and reconnaissance (C4ISR) services, a service deployment platform was extended from the cloud to a terminal platform based on the concept of fog computing. Additionally, computing and memory resources on the terminal platform were used to alleviate the communication pressure due to the flow of combat information. A corresponding C4ISR service deployment model was constructed, and the quantum evolutionary algorithm, the shortest path algorithm, and the pairwise exchange method were combined to create a two-level search algorithm. This method can plan the operational information flow path and service deployment simultaneously, thereby achieving the optimal deployment of services even under the constraints of limited communication bandwidth.","1932-4537","","10.1109/TNSM.2021.3054752","National Natural Science Foundation of China(grant numbers:61403414,61573017); Natural Science Foundation of Shaanxi Province(grant numbers:2016JQ6062,2017JM6062); China Postdoctoral Science Foundation(grant numbers:2016M603042,2017T100817); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9336054","Bandwidth constraints C4ISR service deployment;pairwise exchange method;quantum evolutionary algorithm","Bandwidth;Cloud computing;Computational modeling;Task analysis;Software;Processor scheduling;Service-oriented architecture","","3","","30","IEEE","26 Jan 2021","","","IEEE","IEEE Journals"
"A Comparative Study in the Standardization of IoT Devices Using Geospatial Web Standards","D. Marsh-Hunn; S. Trilles; A. González-Pérez; J. Torres-Sospedra; F. Ramos","GeOps Spatial Web, Freiburg, Germany; Institute of New Imaging Technologies, Universitat Jaume I, Castellón de la Plana, Spain; Institute of New Imaging Technologies, Universitat Jaume I, Castellón de la Plana, Spain; UBIK Geospatial Solutions S.L., Castellón de la Plana, Spain; Institute of New Imaging Technologies, Universitat Jaume I, Castellón de la Plana, Spain",IEEE Sensors Journal,"18 Jan 2021","2021","21","4","5512","5528","Although billions of devices are embedded in the World Wide Web through the Internet of Things, there is still a lack of a common, interoperable way to connect them and make them interact seamlessly. IoT has also found its way into the spatial web. Environmental monitoring and sensing platforms connected over the web by wireless sensor networks are now a common way to monitor natural phenomena. This study compares two open Web Standards (OGC's Sensor Observation Service and SensorThings API) from the geospatial point of view. An IoT platform, called SEnviro, is used to integrate and evaluate implementations for each standard and contrast their qualitative and quantitative traits. The results of the study show that the SensorThings API proves to be the adequate Web Standard for IoT applications in terms of interoperability. It outperforms the contesting Web Standard in terms of flexibility and scalability, which strongly impacts on developer and user experience.","1558-1748","","10.1109/JSEN.2020.3031315","Postdoctoral Juan de la Cierva Fellowship Programme of the Spanish Ministry for Science and Innovation(grant numbers:IJC2018-035017-I); Subvenciones para la Realización de Proyectos de I+D+i Desarrollados por Grupos de Investigación Emergentes programme(grant numbers:GV/2020/035); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9224992","Internet of Things;interoperability;geospatial standards;sensors","Standards;Internet of Things;Geospatial analysis;Interoperability;Sensor phenomena and characterization","","3","","30","IEEE","15 Oct 2020","","","IEEE","IEEE Journals"
"Heterogeneous Workload-Based Consumer Resource Recommendation Model for Smart Cities: eHealth Edge–Cloud Connectivity Using Federated Split Learning","S. T. Ahmed; V. V. Kumar; J. Jeong","Department of Electrical Engineering, Indian Institute of Technology Hyderabad, Hyderabad, India; School of Computer Science Engineering and Information Systems, Vellore Institute of Technology, Vellore, India; Department of Computer Science and Engineering, Dongguk University, Seoul, Republic of Korea",IEEE Transactions on Consumer Electronics,"29 Apr 2024","2024","70","1","4187","4196","Over the past decade, there has been a significant surge in consumer application services and server connectivity, and this trend is expected to double in 2030. The primary contributors to the increased demand for network resources are devices connected through third-party service providers and mobile operators. Many prominent consumer services rely on a client-server architecture, which can introduce latency delays in the communication channel. Additionally, peer-to-peer (P2P) communication places a substantial load on eHealth servers, leading to service delays. In this research paper, we propose a model for scheduling heterogeneous workloads and recommending resources for eHealth edge-cloud connectivity using Federated Split Learning (FSL) model for smart cities. Distributed FSL offers a robust solution for handling both direct and indirect user requests through a distributed mobile core operator stack. This technique empowers eHealth administrators to locally learn optimal policies and make informed decisions by prioritizing resource allocation and scheduling. We demonstrate the effectiveness of this technique through an active simulation server designed for track-driven caching policy and local policy scheduling, ultimately enhancing resource recommendation in eHealth applications. The proposed technique is focused on the development of a heterogeneous workload recommendation system and obtained accuracy of 89.63% over 200 users trails.","1558-4127","","10.1109/TCE.2024.3374462","Dongguk University Research Fund of 2023(grant numbers:S-2023-G0001-00043); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10473185","Federated split learning;smart cities;eHealth server;distributed computing;edge computing;resource recommendation","Resource management;Servers;Task analysis;Computational modeling;Processor scheduling;Electronic healthcare;Recommender systems","","2","","19","IEEE","19 Mar 2024","","","IEEE","IEEE Journals"
"Review on Security Defense Technology Research in Edge Computing Environment","K. Shang; W. He; S. Zhang","National Digital Switching System Engineering and Technological R&D Center, Zhengzhou, China; National Digital Switching System Engineering and Technological R&D Center, Zhengzhou, China; National Digital Switching System Engineering and Technological R&D Center, Zhengzhou, China",Chinese Journal of Electronics,"22 Jan 2024","2024","33","1","1","18","Edge computing, which achieves quick data processing by sinking data computing and storage to the network edge, has grown rapidly along with the Internet of things. The new network architecture of edge computing brings new security challenges. Based on this, this paper investigates the edge computing security literature published in recent years and summarizes and analyzes research work on edge computing security from different attack surfaces. We start with the definition and architecture of edge computing. From the attack surface between device and edge server, as well as on edge servers, the research describes the security threats and defense methods of edge computing. In addition, the cause of the attack and the pros and cons of defense methods is introduced. The challenges and future research directions of edge computing are given.","2075-5597","","10.23919/cje.2022.00.170","National Natural Science Foundation of China(grant numbers:62072467); National Key R&D Program of China(grant numbers:2021YFB1006200,2021YFB1006201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10410603","Edge computing;Security;Threat;Defense;Architecture;Attack surface","Cloud computing;Systems architecture;Computer architecture;Network architecture;Data processing;Security;Servers","","2","","107","","22 Jan 2024","","","CIE","CIE Journals"
"Designing a 6G Testbed for Location: Use Cases, Challenges, Enablers and Requirements","E. J. Khatib; C. S. Álvarez-Merino; H. Q. Luo-Chen; R. B. Moreno","E.T.S.I. de Telecomunicación, Telecommunication Research Institute (TELMA), Universidad de Málaga, Málaga, Spain; E.T.S.I. de Telecomunicación, Telecommunication Research Institute (TELMA), Universidad de Málaga, Málaga, Spain; E.T.S.I. de Telecomunicación, Telecommunication Research Institute (TELMA), Universidad de Málaga, Málaga, Spain; E.T.S.I. de Telecomunicación, Telecommunication Research Institute (TELMA), Universidad de Málaga, Málaga, Spain",IEEE Access,"3 Feb 2023","2023","11","","10053","10091","Location will have a central role in Research and Development (R&D) towards 6G networks, both as a service offered by the network (improving the current offering of 5G) and as an input to increasingly location-aware services and network functions. To integrate location into 6G standards, it will be very important to design validation systems such as testbeds, even when the actual technology is not yet commercially available. This paper performs a review of the use cases and their requirements, enabling technologies in 6G, and challenges; and proposes a flexible testbed architecture for performing network location related R&D. This architecture will allow to deploy an evolving infrastructure which will allow early validation of 6G technologies.","2169-3536","","10.1109/ACCESS.2023.3240775","Horizon 2020 Project LOCalization and analytics on-demand embedded in the 5G ecosystem, for Ubiquitous vertical applicationS (LOCUS) from the European Union(grant numbers:871249); Junta de Andalucia(grant numbers:PY18-4647:PENTA); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10032155","Testbed;proof-of-concept;B5G;6G;location;positioning","6G mobile communication;5G mobile communication;Computer architecture;Research and development;Wireless fidelity;Global navigation satellite system","","2","","270","CCBY","30 Jan 2023","","","IEEE","IEEE Journals"
"Digital Thread for Smart Products: A Survey on Technologies, Challenges, and Opportunities in Service-Oriented Supply Chains","D. Bianchini; T. Fapanni; M. Garda; F. Leotta; M. Mecella; A. Rula; E. Sardini","Department of Information Engineering, University of Brescia, Brescia, Italy; Department of Information Engineering, University of Brescia, Brescia, Italy; Department of Information Engineering, University of Brescia, Brescia, Italy; Department of Computer, Control and Management Engineering, University of Rome “La Sapienza,”, Rome, Italy; Department of Computer, Control and Management Engineering, University of Rome “La Sapienza,”, Rome, Italy; Department of Information Engineering, University of Brescia, Brescia, Italy; Department of Information Engineering, University of Brescia, Brescia, Italy",IEEE Access,"17 Sep 2024","2024","12","","125284","125305","In Smart Manufacturing, the recent opportunities provided by the Information and Communication Technologies have paved the way to a seamless connection of the manufactured product throughout its entire lifecycle, leading to the diffusion of the concepts of Smart Product and Digital Thread, which leverage digital technologies to assure a continuous flow of data encompassing the design phase of a product, manufacturing, operation, maintenance and also its eventual disposal or recycling. This compelling need to obtain a unified view of information associated with Smart Products has stimulated the so-called Internet of Services (IoS) paradigm, allowing for the sharing of products data and the execution of functions among various participants in intertwined supply chains. In these contexts, service-oriented architectures are being more and more employed to meet the complex and ever-evolving data analysis requirements, particularly when implementing Digital Thread solutions for Smart Products, where several issues must be considered, ranging from the heterogeneity of (Big) data to data sovereignty and data access policies, as information may cross the borders of multiple actors participating in intertwined supply chains. This survey discusses about the technological solutions and challenges to implement Digital Threads for Smart Products in Smart Manufacturing contexts, providing insights on opportunities for future research directions. In addition, the survey proposes a comprehensive multi-tier service-oriented architectural model to jointly tackle (Big) data heterogeneity, data sovereignty and data access policies issues, as they are only partially addressed by the research efforts examined in the literature review.","2169-3536","","10.1109/ACCESS.2024.3454375","Next-GenerationEU (Italian Piano Nazionale di Ripresa e Resilienza (PNRR)—M4 C2, Invest 1.3—D.D. 1551.11-10-2022)(grant numbers:PE00000004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10664538","Service-oriented architectures;smart products;digital thread;Internet of Services;cyber-physical production network;smart factory","Surveys;Smart manufacturing;Computer architecture;Supply chains;Instruction sets;Distributed databases;Data models;Service-oriented architecture;Product delivery;Cyber-physical systems","","2","","141","CCBYNCND","4 Sep 2024","","","IEEE","IEEE Journals"
"Dependency-Aware Microservice Deployment for Edge Computing: A Deep Reinforcement Learning Approach With Network Representation","C. Wang; H. Yu; X. Li; F. Ma; X. Wang; T. Taleb; V. C. M. Leung","College of Computer Science and Software Engineering, Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen University, Shenzhen, China; ICTFicial Oy, Espoo, Finland; School of Big Data & Software Engineering, Chongqing University, Chongqing, China; Shenzhen Institute, Tsinghua University, Shenzhen, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; Faculty of Electrical Engineering and Information Technology, Ruhr University Bochum, Bochum, Germany; College of Computer Science & Software Engineering, Shenzhen University, Shenzhen, China",IEEE Transactions on Mobile Computing,"6 Nov 2024","2024","23","12","14737","14753","The popularity of microservices in industry has sparked much attention in the research community. Despite significant progress in microservice deployment for resource-intensive services and applications at the network edge, the intricate dependencies among microservices are often overlooked, and some studies underestimate the importance of system context extraction in deployment strategies. This paper addresses these issues by formulating the microservice deployment problem as a max-min problem, considering system cost and quality of service (QoS) jointly. We first study the attention-based microservice representation (AMR) method to achieve effective system context extraction. In this way, the contributions of different computing power providers (users, edge servers, or cloud servers) in the networks can be effectively paid attention to. Subsequently, we propose the attention-modified soft actor-critic (ASAC) algorithm to tackle the microservice deployment problem. ASAC leverages attention mechanisms to enhance decision-making and adapt to changing system dynamics. Our simulation results demonstrate ASAC's effectiveness, prioritizing average system cost and reward compared to the other state-of-the-art algorithms.","1558-0660","","10.1109/TMC.2024.3453069","Science and Technology Innovation Key R&D Program of Chongqing(grant numbers:CSTB2023TIAD-STX0035); National NSFC(grant numbers:62372072,62072332); Chongqing Research Program of Basic Research and Frontier Technology(grant numbers:cstc2022ycjhbgzxm0058); Regional Science and Technology Innovation Cooperation Project of Chengdu City(grant numbers:2023-YF11-00023-HZ); Tianjin Xinchuang Haihe Lab(grant numbers:22HHXCJC00002); European Union's(grant numbers:101139172); AerOS(grant numbers:101069732); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10663201","Attention mechanism;deep reinforcement learning;dependency-aware;edge computing;microservice deployment;network representation","Microservice architectures;Servers;Heuristic algorithms;Decision making;Computer architecture;Cloud computing;Quality of service","","2","","44","IEEE","2 Sep 2024","","","IEEE","IEEE Journals"
"RRFT: A Rank-Based Resource Aware Fault Tolerant Strategy for Cloud Platforms","C. K. Dehury; P. K. Sahoo; B. Veeravalli","Mobile & Cloud Lab, Institute of Computer Science, University of Tartu, Tartu, Estonia; Department of Computer Science and Information Engineering, Chang Gung University, Guishan, Taiwan; Department of Electrical and Computer Engineering, National University of Singapore, Singapore, Singapore",IEEE Transactions on Cloud Computing,"6 Jun 2023","2023","11","2","1257","1272","The applications that are deployed in the cloud to provide services to the users encompass a large number of interconnected dependent cloud components. Multiple identical components are scheduled to run concurrently in order to handle unexpected failures and provide uninterrupted service to the end user, which introduces resource overhead problem for the cloud service provider. Furthermore such resource-intensive fault tolerant strategies bring extra monetary overhead to the cloud service provider and eventually to the cloud users. In order to address these issues, a novel fault tolerant strategy based on the significance level of each component is developed. The communication topology among the application components, their historical performance, failure rate, failure impact on other components, dependencies among them, etc., are used to rank those application components to further decide on the importance of one component over others. Based on the rank, a Markov Decision Process (MDP) model is presented to determine the number of replicas that varies from one component to another. A rigorous performance evaluation is carried out using some of the most common practically useful metrics such as, recovery time upon a fault, average number of components needed, number of parallel components successfully executed, etc., to quote a few, with similar component ranking and fault tolerant strategies. Simulation results demonstrate that the proposed algorithm reduces the required number of virtual and physical machines by approximately 10% and 4.2%, respectively, compared to other similar algorithms.","2168-7161","","10.1109/TCC.2021.3126677","Ministry of Science and Technology, Taiwan(grant numbers:110-2221-E-182-008-MY3); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609649","Cloud computing;component ranking;fault tolerance;Markov decision process","Cloud computing;Fault tolerant systems;Fault tolerance;Servers;Software;Task analysis;Software as a service","","2","","34","IEEE","9 Nov 2021","","","IEEE","IEEE Journals"
"Fenrir: Blockchain-Based Inter-Company App-Store for the Automotive Industry","D. F. Blanco; F. le Mouël; T. Lin","Univ Lyon, INSA Lyon, Inria, CITI, EA3720, Villeurbanne, France; Univ Lyon, INSA Lyon, Inria, CITI, EA3720, Villeurbanne, France; Stellantis, Velizy-Villacoublay, France",IEEE Access,"29 Nov 2022","2022","10","","122933","122953","From a software evolution perspective, more actors are integrating the in-vehicle software development cycle. In this process, software deployment mechanisms must include more complex techniques to meet the software verification and traceability levels required by industry safety and security constraints. In this context, we propose Fenrir, a public inter-automaker blockchain-based application store framework in which each automaker retains software installability control. This application store also aims to ensure traceability and security, while also keeping the solution light in terms of both energy consumption and computing requirements, to be used in constrained environments.We implemented Fenrir in a heterogeneous architecture composed by both on-board (bearing an ARM Cortex-A53 chipset, already deployed in cars) and off-board (Amazon EC2) nodes for a realistic automotive use-case scenario, in which we evaluated the performance and energy consumption. We demonstrate that the overheads added by our solution for an entire software deployment pipeline—comprising both deployment and usage of already deployed software packages—depends mainly on the verification mechanism, whose impact is not significant, i.e., 3.8% for the worst-case scenario and 0.3% for a typical scenario.","2169-3536","","10.1109/ACCESS.2022.3223130","STELLANTIS under the collaborative framework OpenLab VAT@Lyon, involving STELLANTIS and CITI Laboratory (ANRT)(grant numbers:2020/1415); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9954374","Automotive;application store;blockchain;distributed systems;multi-provider;software dependency management;software deployment","Blockchains;Automotive engineering;Computer architecture;Standards;Sensors;Actuators;Security;Software engineering;Distributed processing","","2","","51","CCBY","17 Nov 2022","","","IEEE","IEEE Journals"
"Optimal Resource Allocation Using Genetic Algorithm in Container-Based Heterogeneous Cloud","Q. -H. Chen; C. -Y. Wen","Department of Electrical Engineering, National Chung Hsing University, Taichung, Taiwan; Department of Electrical Engineering, National Chung Hsing University, Taichung, Taiwan",IEEE Access,"17 Jan 2024","2024","12","","7413","7429","This paper tackles the complex problem of optimizing resource configuration for microservice management in heterogeneous cloud environments. To address this challenge, an enhanced framework, the multi-objective microservice allocation (MOMA) algorithm, is developed to formulate the efficient resource management of cloud microservice resources as a constrained optimization problem, guided by resource utilization and network communication overhead, which are two important factors in microservice resource allocation. The proposed framework simplifies the deployment of cloud services and streamlines workload monitoring and analysis within a diverse cloud system. A comprehensive comparison is made between the effectiveness of the proposed algorithm and existing algorithms on real-world datasets, with a focus on resource balancing, network overhead, and network reliability. Experimental results reveal that the proposed algorithm significantly enhances resource utilization, reduces network transmission overhead, and improves reliability.","2169-3536","","10.1109/ACCESS.2024.3351944","Smart Sustainable New Agriculture Research Center (SMARTer), National Science and Technology Council, Taiwan(grant numbers:111-2634-F-005-001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385169","Resource allocation;genetic algorithm;container-based heterogeneous cloud;multi-objective optimization;microservice","Resource management;Microservice architectures;Scheduling;Optimization;Monitoring;Genetic algorithms;Clustering algorithms","","2","","59","CCBY","9 Jan 2024","","","IEEE","IEEE Journals"
"A Hierarchical Vehicular-Based Architecture for Vehicular Networks: A Case Study on Computation Offloading","T. Liu; J. Wang; B. Kim; J. Xie; Z. Han","Nanjing Institute of Technology, Nanjing, China; School of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; InfoTech Labs, Toyota Motor North America, Mountain View, CA, USA; Department of Electrical and Computer Engineering, University of North Carolina at Charlotte, Charlotte, NC~, USA; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA",IEEE Access,"15 Oct 2020","2020","8","","184273","184283","In order to realize an intelligent transportation system (ITS) which will provide smooth urban traffic, autonomous driving, accurate route navigation, etc., enormous computations need to be migrated from cloud centers to edge nodes, especially for the services requiring stringent latency. In addition to base stations and road side units (RSUs), vehicles can be alteratively considered as a kind of computation resources. In this article, a hierarchical vehicular-based architecture which consists of cloud centers and vehicles is investigated. Computation offloading performance in the hierarchical architecture is also studied. In specific, the main components in vehicular networks and their characteristics on communication and computations are presented firstly. Several communication techniques that are essential in enabling computation offloading among these components are then discussed. Secondly, a hierarchical vehicular-based architecture, which integrates the main components, is constructed. Thirdly, a case study on computation offloading in the proposed architecture is conducted. In the concerned scenario, the computation offloading problem is modelled as a multi-dimensional multiple knapsack problem (MMKP). Two algorithms are investigated, among which, the first algorithm is a greedy heuristic method providing a sub-optimal solution with a low computational complexity. The second algorithm is a modified branch and bound (B&B) method, which can obtain the best solution with a high computational complexity. Numerical results are also presented to verify the performance of the two algorithms. It can be demonstrated that the proposed architecture can migrate more computations from cloud centers to vehicular nodes, when the computations require more communication resources.","2169-3536","","10.1109/ACCESS.2020.3029169","National Natural Science Foundation of China(grant numbers:61702258,62002166); Key Projects of Natural Science Research in Colleges and Universities of Jiangsu Province(grant numbers:19KJA410001); Foundation of Jiangsu Advanced Numerical Control Technology Key Laboratory(grant numbers:SYKJ201901,NSF EARS-1839818,CNS-1717454,CNS-1731424,CNS-1702850,CNS-1718666,CNS-1731675,CNS-1910667,CNS-1910891,CNS-2025284); China Postdoctoral Science Foundation(grant numbers:2020M671483); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9214822","Hierarchical vehicular architecture;intelligent transportation system;computation offloading;multi-dimensional multiple knapsack problem;branch and bound algorithm","Computer architecture;Task analysis;Edge computing;Cloud computing;Servers;Electronic mail;Computational modeling","","2","","53","CCBYNCND","6 Oct 2020","","","IEEE","IEEE Journals"
"Integrating Bayesian Optimization and Machine Learning for the Optimal Configuration of Cloud Systems","B. Guindani; D. Ardagna; A. Guglielmi; R. Rocco; G. Palermo","Department of Electronics, Information, and Bioengineering, Politecnico di Milano, Milano, Italy; Department of Electronics, Information, and Bioengineering, Politecnico di Milano, Milano, Italy; Department of Mathematics, Politecnico di Milano, Milano, Italy; Department of Electronics, Information, and Bioengineering, Politecnico di Milano, Milano, Italy; Department of Electronics, Information, and Bioengineering, Politecnico di Milano, Milano, Italy",IEEE Transactions on Cloud Computing,"7 Mar 2024","2024","12","1","277","294","Bayesian Optimization (BO) is an efficient method for finding optimal cloud configurations for several types of applications. On the other hand, Machine Learning (ML) can provide helpful knowledge about the application at hand thanks to its predicting capabilities. This work proposes a general approach based on BO, which integrates elements from ML techniques in multiple ways, to find an optimal configuration of recurring jobs running in public and private cloud environments, possibly subject to black-box constraints, e.g., application execution time or accuracy. We test our approach by considering several use cases, including edge computing, scientific computing, and Big Data applications. Results show that our solution outperforms other state-of-the-art black-box techniques, including classical autotuning and BO- and ML-based algorithms, reducing the number of unfeasible executions and corresponding costs up to 2–4 times.","2168-7161","","10.1109/TCC.2024.3361070","MUR(grant numbers:2023-2027); European Commission(grant numbers:956137); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10418550","Acquisition function;Bayesian optimization;black-box optimization;machine learning","Cloud computing;Optimization;Closed box;Computational modeling;Costs;Software algorithms;Bayes methods","","2","","48","CCBYNCND","1 Feb 2024","","","IEEE","IEEE Journals"
"Real-Time Adaptive Anomaly Detection in Industrial IoT Environments","M. Raeiszadeh; A. Ebrahimzadeh; R. H. Glitho; J. Eker; R. A. F. Mini","Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada; Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada; Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada; Ericsson Research, Lund, Sweden; Ericsson Research, Lund, Sweden",IEEE Transactions on Network and Service Management,"19 Dec 2024","2024","21","6","6839","6856","To ensure reliability and service availability, next-generation networks are expected to rely on automated anomaly detection systems powered by advanced machine learning methods with the capability of handling multi-dimensional data. Such multi-dimensional, heterogeneous data occurs mostly in today’s Industrial Internet of Things (IIoT), where real-time detection of anomalies is critical to prevent impending failures and resolve them in a timely manner. However, existing anomaly detection methods often fall short of effectively coping with the complexity and dynamism of multi-dimensional data streams in IIoT. In this paper, we propose an adaptive method for detecting anomalies in IIoT streaming data utilizing a multi-source prediction model and concept drift adaptation. The proposed anomaly detection algorithm merges a prediction model into a novel drift adaptation method resulting in accurate and efficient anomaly detection that exhibits improved scalability. Our trace-driven evaluations indicate that the proposed method outperforms the state-of-the-art anomaly detection methods by achieving up to an 89.71% accuracy (in terms of Area under the Curve (AUC)) while meeting the given efficiency and scalability requirements.","1932-4537","","10.1109/TNSM.2024.3447532","Ericsson/ENCQOR-5G Senior Industrial Research Chair on Cloud and Edge Computing for 5G and Beyond; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10643234","Anomaly detection;real-time analytics;concept drift;streaming data;Industrial Internet of Things (IIoT)","Anomaly detection;Industrial Internet of Things;Real-time systems;Concept drift;Predictive models;Autoregressive processes;Accuracy","","2","","41","IEEE","21 Aug 2024","","","IEEE","IEEE Journals"
"User Classification and Traffic Steering in O-RAN","R. Ntassah; G. Michele Dell’Area; F. Granelli","DISI, University of Trento, Trento, Italy; Telecom Italia, Turin, Italy; DISI, University of Trento, Trento, Italy",IEEE Open Journal of the Communications Society,"28 Jun 2024","2024","5","","3581","3594","The O-RAN architectural framework enables the application of AI/ML techniques for traffic steering and load balancing. Indeed, an effective steering technique is crucial to avoiding ping-pong and radio link failure. Limited observability and network complexity make it challenging to understand individual user needs. Consequently, traffic steering methods struggle to make optimal decisions, resulting in performance degradation due to unnecessary handovers. Motivated by this, we present an xApp for the RAN intelligence controller (RIC) for user equipment (UE) steering to ensure an even load distribution among cells while maintaining an acceptable throughput level. We propose an ML-aided traffic steering technique. The proposed method comprises three phases: UE classification, downlink (DL) throughput prediction, and a traffic steering (TS) technique. A support vector machine (SVM) is used for UE classification, followed by cell throughput prediction using ensemble Long Short-Term Memory (E-LSTM). The TS algorithm uses the information from the ML models to initiate handovers (HO). The SVM model identifies UEs with low throughput, while the E-LSTM predicts cell DL throughput to provide information about potential target cells for these UEs. Experimental results demonstrate that the proposed method achieves an even load distribution of UEs in 60.25% of the cells with few handovers, while also significantly improving UE throughput.","2644-125X","","10.1109/OJCOMS.2024.3413590","Telecom Italia in the framework of the UniversiTIM Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555357","Bagging;DL throughput;load balancing;LSTM;O-RAN;supervised learning;SVM;traffic steering;xApp","Throughput;Predictive models;Support vector machines;Load modeling;Radio frequency;Long short term memory;Handover","","2","","63","CCBY","12 Jun 2024","","","IEEE","IEEE Journals"
"Edge Intelligence for Rendering Green Camera-Network-as-a-Service","N. A. Singh; A. Roy; S. Misra","Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Advanced Technology Development Centre, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India",IEEE Transactions on Green Communications and Networking,"16 Feb 2022","2022","6","1","365","375","This work proposes the Green Camera-Network-as-a-Service (G-CNaaS) architecture, which provides on-demand camera networks to multiple end-users simultaneously while utilizing minimal energy. G-CNaaS simultaneously reduces the carbon footprint and eliminates the single application-centric approach of traditional camera networks (TCNs) by enabling each camera to participate in multiple Virtual-Camera-Networks (VCNs) and selecting an optimal set of cameras for each VCN. We couple each camera node in every VCNs with a learning model suitable for the requested application. We assign an intelligent edge device to each VCN to analyze time-sensitive events. We introduce the camera selection factor by leveraging the properties of cameras: 1) field-of-view (FoV); 2) angular-distance; 3) observation range; and 4) residual energy to select the optimal camera set. The results of the extensive simulation of the G-CNaaS architecture show that it excels in performance concerning attributes such as the average lifetime, fair distribution of the work among the camera owners, and cost-effectiveness compared to the TCNs. We observe that the expenditure of a user using the TCN varies by 88.7%, while in the case of G-CNaaS, the expenditure varies by 10.28% with the increase in time from 1–60 months. On the other hand, the average energy consumed increases by 59.88% and 99.5% in the presence of 10 and 20 camera sensor owners.","2473-2400","","10.1109/TGCN.2021.3126276","Department of Science and Technology—INSPIRE Fellowship (Sanction letter no. DST/INSPIRE Fellowship/2017/1F170547), Government of India; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9606213","Edge intelligence;green computing;virtual camera network;network services;virtualization;cost effectiveness","Cameras;Sensors;Streaming media;Carbon footprint;Surveillance;Virtualization;Real-time systems","","2","","25","IEEE","8 Nov 2021","","","IEEE","IEEE Journals"
"Resource Choreography in Cyber-Physical-Social Systems: Representation, Modeling and Execution","F. He; L. Shen; X. Peng","Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University, Shanghai, China; Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University, Shanghai, China; Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University, Shanghai, China",IEEE Transactions on Services Computing,"6 Feb 2023","2023","16","1","550","563","Myriad of heterogeneous resources are widely distributed in the cyber, physical and social spaces. These resources are integrated by software to form diverse Cyber-Physical-Social Systems (CPSSs). Among them, the CPSSs in the form of resource choreography is receiving more attention. Traditional software development methods may not be suitable for constructing and executing CPSS applications (CPSS-Apps) with the characteristics of loosely-coupled resource collaboration and spatial-temporal constraints sensitive. In this article we propose a comprehensive framework to support resource choreography from the perspectives of representation, modeling and execution. In the framework, a CPSS-App is represented by an application model conforming to a meta-model. An application model is generated by a multi-scene storyboard modeling tool. The model is further used to customize the capability units acting as the abstract unit of the resources providing the same service. An architecture following the microservice style is applied to achieve the choreography of the capability units by asynchronous message communication while a resource is determined by application-level service discovery. The framework is evaluated through a human experiment. The results show that the application construction and execution by the framework is feasible. The modeling tool is usable and the execution architecture is scalable in different environment settings.","1939-1374","","10.1109/TSC.2021.3138637","National Key R&D Program of China(grant numbers:2018YFB1004803); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664383","Cyber-physical-social;modeling;execution;microservice","Software;Collaboration;Computer architecture;Microservice architectures;Motion pictures;Medical services;Couplings","","2","","60","IEEE","28 Dec 2021","","","IEEE","IEEE Journals"
"Resilient Energy Efficient IoT Infrastructure With Server and Network Protection for Healthcare Monitoring Applications","I. S. M. Isa; T. E. H. El-Gorashi; M. O. I. Musa; J. M. H. Elmirghani","Faculty of Technology and Engineering Electronic and Computer, Universiti Teknikal Malaysia Melaka, Durian Tunggal, Melaka, Malaysia; Faculty of Natural, Mathematical and Engineering Sciences, King’s College London, London, U.K.; School of Electrical and Electronic Engineering, University of Leeds, Leeds, U.K.; Faculty of Natural, Mathematical and Engineering Sciences, King’s College London, London, U.K.",IEEE Access,"8 Apr 2024","2024","12","","48910","48940","Fog computing has been introduced to extend the cloud services by bringing the services near to the user’s proximity. However, the distributed location of the fog servers requires a proper management to ensure the network to provide a service resilience during disruption while preserving the energy consumption of the networking and processing equipment. In this paper, a 1+1 server protection scheme where a primary and a secondary processing server are used to serve Electrocardiogram (ECG) monitoring IoT applications concurrently has been considered at the fog networking infrastructure. The infrastructure is designed to be resilient against server failures related to the geographic location of primary and secondary servers and against both server and network failures. A Mixed Integer Linear Programming (MILP) model is developed to optimize the number and locations of the processing servers for energy-efficient resilient fog infrastructure. The results reveal that considering server protection without geographical constraints resulted in network and processing energy penalties as the traffic is doubled compared to the non-resilient scenario. Meanwhile, considering geographical constraints for server protection at low demands resulted in high network energy penalty as more nodes are used to host the processing servers. Interestingly, increasing the resilience level to consider network protection with link and node disjoints selection at high demand resulted in low network energy penalty due to the activation of a large part of the network in any case to serve the demands. The results also reveal that the network energy penalty was reduced when more processing servers are allowed at each fog node while the same processing energy is consumed regardless of the increased resilience level. A heuristic was developed for each resilience scenario for verification and to enable real-time operation of the network, servers and IoT devices, and the results of the heuristic approach those of the MILP.","2169-3536","","10.1109/ACCESS.2024.3352024","PJP through Universiti Teknikal Malaysia Melaka (UTeM), Engineering and Physical Sciences Research Council (EPSRC)(grant numbers:PJP/2022/FTKEE/S01883); INTelligent Energy aware NETworks (INTERNET)(grant numbers:EP/H040536/1); SwiTching And tRansmission (STAR)(grant numbers:EP/K016873/1); Terabit Bidirectional Multi-user Optical Wireless System (TOWS) Project(grant numbers:EP/S016570/1); Ministry of Education, Malaysia, and Universiti Teknikal Malaysia Melaka (UTeM), for his Ph.D. studies, and for the approved fund which makes this important research viable and effective; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10387434","ECG monitoring;energy consumption;fog computing;GPON;health monitoring;Internet of Things;machine-to-machine (M2M) communication;network protection;resilience;server protection","Servers;Resilience;Cloud computing;Internet of Things;Edge computing;Monitoring;Energy efficiency","","2","","44","CCBYNCND","10 Jan 2024","","","IEEE","IEEE Journals"
"DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems","Z. Wang; S. Zhu; J. Li; W. Jiang; K. K. Ramakrishnan; M. Yan; X. Zhang; A. X. Liu","Key Laboratory of Dependable Service Computing in Cyber Physical Society, Ministry of Education, and the School of Big Data and Software Engineering, Chongqing University, Chongqing, China; Ant Group, Hangzhou, China; Ant Group, Hangzhou, China; Ant Group, Hangzhou, China; Department of Computer Science and Engineering, University of California at Riverside, Riverside, CA, USA; School of Big Data and Software Engineering, Chongqing University, Chongqing, China; School of Big Data and Software Engineering, Chongqing University, Chongqing, China; Ant Group, Hangzhou, China",IEEE/ACM Transactions on Networking,"16 Oct 2024","2024","32","5","3961","3976","Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.","1558-2566","","10.1109/TNET.2024.3400953","National Key Research and Development Project(grant numbers:2021YFB1714200); Fundamental Research Funds for the Central Universities(grant numbers:2023CDJKYJH013); Chongqing Technology Innovation and Application Development Project(grant numbers:CSTB2022TIAD-STX0007,CSTB2022TIAD-KPX0067); Postdoctoral Foundation of Chongqing(grant numbers:2020LY13); Research Fund from the Ant Group; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10542703","Microservices autoscaling;cloud systems;horizontal autoscaling;service quality","Microservice architectures;Measurement;Production;Task analysis;Servers;Resource management;Predictive models","","2","","50","IEEE","31 May 2024","","","IEEE","IEEE Journals"
"Internet of Things in Smart Cities: Comprehensive Review, Open Issues, and Challenges","E. H. Houssein; M. A. Othman; W. M. Mohamed; M. Younan","Faculty of Computers and Information, Minia University, Minia, Egypt; Faculty of Computers and Information, Minia University, Minia, Egypt; Faculty of Computers and Information, Minia University, Minia, Egypt; Faculty of Computers and Information, Minia University, Minia, Egypt",IEEE Internet of Things Journal,"25 Oct 2024","2024","11","21","34941","34952","Smart cities rely mainly on the Internet of Things (IoT) to make an urban area smart to offer its citizens a high quality of life with optimal use of resources and preservation of the environment. IoT is the key component that collects raw data on the surrounding environment to be analyzed to extract information that supports decision making. The widespread use of IoT results in the emergence of smart homes, smart energy, smart transportation, and smart healthcare, which build a smart city. On the other hand, challenges, such as heterogeneity, scalability, security, and privacy, hinder the efficient functioning of the IoT in the construction of smart cities. This article presents a comprehensive overview on the concept of IoT moving forward to the concept of smart city, highlighting key elements and characteristics, studying and reviewing state-of-the-art research on this theme. Future directions are discussed to guide researchers, who focus on interoperability between IoT platforms in smart cities and on IoT architectures based on micro-services. Case studies of successful smart cities are presented for gaining learned lessons. The impact of integrating wireless networks (5G and 6G) in the IoT is also clarified in the future direction. The significance of this research is found in its comprehensive examination of various aspects of the smart city instead of concentrating on a singular facet.","2327-4662","","10.1109/JIOT.2024.3449753","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646337","Internet of Things (IoT);smart city;smart energy;smart healthcare;smart transportation","Smart cities;Internet of Things;Reviews;Security;Market research;Sustainable development;Smart transportation","","2","","135","IEEE","26 Aug 2024","","","IEEE","IEEE Journals"
"Validating the Blockchain Benchmarking Framework Through Controlled Deployments of XRPL and Ethereum","M. Touloupou; K. Christodoulou; M. Themistocleous","Department of Digital Innovation, University of Nicosia, Nicosia, Cyprus; Department of Digital Innovation, University of Nicosia, Nicosia, Cyprus; Department of Digital Innovation, University of Nicosia, Nicosia, Cyprus",IEEE Access,"15 Feb 2024","2024","12","","22264","22277","In the evolving domain of blockchain, a critical challenge lies in the performance analysis of blockchains under controlled test conditions. This paper focuses on validating the Blockchain Benchmarking Framework (BBF), developed for the evaluation of blockchain protocols in a controlled environment. The BBF’s robustness and versatility are demonstrated through its application to the official Docker clients of Ripple’s XRP Ledger (XRPL) and Ethereum, deployed in private, local and controlled environments. These deployments are utilized to simulate network dynamics, transaction throughput, and resilience in a variety of scenarios. Our methodology encompasses tests ranging from standard operational conditions to adverse scenarios, including node failures and simulated double-spend attacks. These controlled environments are essential for evaluating the BBF’s efficacy in stress testing blockchain protocols and assessing their stability and robustness. The BBF’s ability to accurately capture and analyze performance characteristics is highlighted, providing insights into the operational mechanics, scalability, and resilience of these blockchain clients. The findings emphasize the BBF’s adaptability and effectiveness in managing different blockchain protocols, reaffirming its potential for broader application in pre-launch testing and analysis of blockchain performance. This study contributes to the understanding of how blockchain clients can be preliminarily assessed before mainnet deployment as well as to validate all the design decisions made by the protocol under different settings and synthetic scenarios.","2169-3536","","10.1109/ACCESS.2024.3363833","University Blockchain Research Initiative (UBRI) Project; Ripple’s Impact Fund, a fund of the Silicon Valley Community Foundation(grant numbers:2021-244121); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10427547","Benchmarking framework;blockchain applications;blockchain resilience;blockchain technology;performance analysis","Blockchains;Protocols;Benchmark testing;Resilience;Computer crashes;Security;Robustness;Performance evaluation","","2","","28","CCBYNCND","8 Feb 2024","","","IEEE","IEEE Journals"
"UMBRELLA: A One-Stop Shop Bridging the Gap From Lab to Real-World IoT Experimentation","I. Mavromatis; Y. Jin; A. Stanoev; A. Portelli; I. Weeks; B. Holden; E. Glasspole; T. Farnham; A. Khan; U. Raza; A. Aijaz; T. Bierton; I. Seto; N. Patel; M. Sooriyabandara","Digital Catapult, London, U.K.; Bristol Research and Innovation Laboratory, Toshiba Europe Ltd., Avon, Bristol, U.K.; Nordic Semiconductor, Avon, Bristol, U.K.; Turtle Beach, Basingstoke, Hampshire, U.K.; Bristol Research and Innovation Laboratory, Toshiba Europe Ltd., Avon, Bristol, U.K.; Q5D Technologies Ltd., Portishead, North Somerset, U.K.; Amiosec Ltd., Tewkesbury, Gloucestershire, U.K.; Bristol Research and Innovation Laboratory, Toshiba Europe Ltd., Avon, Bristol, U.K.; Bristol Research and Innovation Laboratory, Toshiba Europe Ltd., Avon, Bristol, U.K.; Waymap, London, U.K.; Bristol Research and Innovation Laboratory, Toshiba Europe Ltd., Avon, Bristol, U.K.; Bristol Research and Innovation Laboratory, Toshiba Europe Ltd., Avon, Bristol, U.K.; Corporate Research and Development Centre, Toshiba Corporation, Saiwai-ku, Kawasaki-shi, Japan; South Gloucestershire Council, Yate, Gloucester, U.K.; Bristol Research and Innovation Laboratory, Toshiba Europe Ltd., Avon, Bristol, U.K.",IEEE Access,"27 Mar 2024","2024","12","","42181","42213","UMBRELLA (A Living Laboratory: https://www.umbrellaiot.com/) is an open, large-scale IoT ecosystem deployed across South Gloucestershire, UK. It is intended to accelerate innovation across multiple technology domains. UMBRELLA is built to bridge the gap between existing specialised testbeds and address holistically real-world technological challenges in a System-of-Systems (SoS) fashion. UMBRELLA provides open access to real-world devices and infrastructure, enabling researchers and the industry to evaluate solutions for Smart Cities, Robotics, Wireless Communications, Edge Intelligence, and more. Key features include over 200 multi-sensor nodes installed on public infrastructure, a robotics arena with 20 mobile robots, a 5G network-in-a-box solution, and a unified backend platform for management, control and secure user access. The heterogeneity of hardware components, including diverse sensors, communication interfaces, and GPU-enabled edge devices, coupled with tools like digital twins, allows for comprehensive experimentation and benchmarking of innovative solutions unviable in lab environments. This paper provides a comprehensive overview of UMBRELLA’s multi-domain architecture and capabilities, making it an ideal playground for Internet of Things (IoT) and Industrial IoT (IIoT) innovation. It discusses the challenges in designing, developing and operating UMBRELLA as an open, sustainable testbed and shares lessons learned to guide similar future initiatives. With its unique openness, heterogeneity, realism and tools, UMBRELLA aims to continue accelerating cutting-edge technology research, development and translation into real-world progress.","2169-3536","","10.1109/ACCESS.2024.3377662","West of England Local Enterprise Partnership (LEP) Local Growth Fund administered by the West of England Combined Authority (WECA) through UMBRELLA; South Gloucestershire Council; Bristol Research Innovation Laboratory (BRIL), Toshiba Europe Ltd; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10472978","IoT;IIoT;testbed;experimentation;system-of-systems;wireless;robots;smart cities","Robots;Wireless sensor networks;Wireless communication;Sensors;Temperature sensors;Robot sensing systems;Ecosystems;Internet of Things;System of systems;Smart cities","","2","","68","CCBYNCND","18 Mar 2024","","","IEEE","IEEE Journals"
"A Smart CIoT With Secure Healthcare Framework Using Optimized Deep Recuperator Neural Network Long Short-Term Memory","C. Narmatha; S. Manimurugan; P. Karthikeyan","Faculty of Computers and Information Technology, University of Tabuk, Tabuk, Saudi Arabia; Faculty of Computers and Information Technology, University of Tabuk, Tabuk, Saudi Arabia; Department of Computer Science and Information Engineering, National Chung Cheng University, Chiayi, Taiwan",IEEE Internet of Things Journal,"8 Mar 2024","2024","11","6","10551","10562","The healthcare system currently relies on the facility to store and process large amounts of health data, supported by efficient management. The Internet of Things (IoT) has driven the growth of Adroit Healthcare, which has vast data processing capabilities and extensive data collection. The consumer IoT (CIoT), also known as the IoT in the context of individual use cases, is dominated by personal healthcare applications. The remarkable expansion of the CIoT can be attributed to the extensive embrace of wearable technology, facilitating everyday monitoring of vital health indicators like blood pressure, heart rate, and respiration rate. However, the CIoT raises concerns about data security, confidentiality, and customer trust. This study proposes an integrating blockchain and deep learning (DL) approach to promote responsible use of CIoT. The study develops an Intelligent IoT (IIoT) and healthcare diagnostic model, named BT-PWO-DRNN-LSTM, using blockchain technology and Prairie Wolf optimization-based deep recuperator neural network (DRNN) and long short-term memory (LSTM). The BT-PWO-DRNN-LSTM approach includes three key processes: 1) encrypted transactions; 2) cryptographic hash feature; and 3) medical diagnosis. The PWO technique is used for secure communication of medical images, the neighborhood catalogue disposition (NCD) technique is used in the cryptographic hash feature operation (CHFO), and the DL approach is employed as a classification algorithm for diagnosing disorders. The use of PWO methodology for secure healthcare communication and optimal parameter fine-tuning highlights the originality of the study. The BT-PWO-DRNN-LSTM prototype showed positive results with high sensitivity, specificity, and accuracy during the diagnostic process.","2327-4662","","10.1109/JIOT.2023.3326547","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290899","Blockchain technology (BCT);consumer IoT (CIoT);cryptographic hash function;deep learning (DL);deep recuperator neural network (DRNN)-long short-term memory (LSTM);Internet of Things (IoT);Prairie Wolf (PW) optimization","Internet of Things;Medical services;Blockchains;Security;Protocols;Cryptography;Biomedical monitoring","","2","","31","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"AI/ML Enabled Automation System for Software Defined Disaggregated Open Radio Access Networks: Transforming Telecommunication Business","S. Kumar","Institute for Communication Systems, University of Surrey, Guildford, UK",Big Data Mining and Analytics,"22 Apr 2024","2024","7","2","271","293","Open Air Interface (OAI) alliance recently introduced a new disaggregated Open Radio Access Networks (O-RAN) framework for next generation telecommunications and networks. This disaggregated architecture is open, automated, software defined, virtual, and supports the latest advanced technologies like Artificial Intelligence (AI) Machine Learning (AI/ML). This novel intelligent architecture enables programmers to design and customize automated applications according to the business needs and to improve quality of service in fifth generation (5G) and Beyond 5G (B5G). Its disaggregated and multivendor nature gives the opportunity to new startups and small vendors to participate and provide cheap hardware software solutions to keep the market competitive. This paper presents the disaggregated and programmable O-RAN architecture focused on automation, AI/ML services, and applications with Flexible Radio access network Intelligent Controller (FRIC). We schematically demonstrate the reinforcement learning, external applications (xApps), and automation steps to implement this disaggregated O-RAN architecture. The idea of this research paper is to implement an AI/ML enabled automation system for software defined disaggregated O-RAN, which monitors, manages, and performs AI/ML-related services, including the model deployment, optimization, inference, and training.","2097-406X","","10.26599/BDMA.2023.9020033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10506767","Open Radio Access Networks (O-RAN);Flexible Radio access network Intelligent Controller (FRIC);Reinforcement Learning (RL);external Applications (xApps);Artificial Intelligence (AI);Machine Learning (ML);sixth generation (6G)","Training;Automation;5G mobile communication;Computer architecture;Reinforcement learning;Quality of service;Software","","2","","64","","22 Apr 2024","","","TUP","TUP Journals"
"Joint Optimization of Microservice Deployment and Routing in Edge via Multi-Objective Deep Reinforcement Learning","M. Hu; H. Wang; X. Xu; J. He; Y. Hu; T. Deng; K. Peng","Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China",IEEE Transactions on Network and Service Management,"19 Dec 2024","2024","21","6","6364","6381","Edge computing technologies with container-based microservice architectures promise to provide stable and low-latency services for large-scale and complex edge applications. However, due to the limited CPU and storage resources in edge computing scenarios, the coarse-grained service deployment on edge nodes causes performance bottlenecks. In addition, the effective deployment of microservices is tightly correlated with request routing, but the current research ignores the joint optimization of multi-instance deployment and routing. In this paper, we first model the problem of jointly optimizing service deployment and routing in a dynamically changing environment with multi-edge network collaboration based on a queuing network analysis. Secondly, we design heuristic algorithms to scale microservice instances horizontally in dynamic user request states. In addition, we propose a reinforcement learning algorithm based on reward shaping (RSPPO) to minimize user waiting delay and edge network resource consumption. We also solve the microservice deployment and request routing problem for multi-edge collaboration to achieve load balancing among edge nodes. Finally, extensive experiments verify the significant and extensive effectiveness of our algorithm.","1932-4537","","10.1109/TNSM.2024.3443872","National Science and Technology Major Project of China(grant numbers:2022ZD0117104); National Natural Science Foundation of China(grant numbers:62171189,62272183); Key Research and Development Program of Hubei Province, China(grant numbers:2022BAA038,2023BAB074); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10639475","Fog and edge networks;service deployment;request routing;deep reinforcement learning","Microservice architectures;Routing;Heuristic algorithms;Optimization;Collaboration;Edge computing;Delays","","2","","38","IEEE","19 Aug 2024","","","IEEE","IEEE Journals"
"Faashouse: Sustainable Serverless Edge Computing Through Energy-Aware Resource Scheduling","M. S. Aslanpour; A. N. Toosi; M. A. Cheema; M. B. Chhetri","Monash University, Clayton, VIC, Australia; Monash University, Clayton, VIC, Australia; Monash University, Clayton, VIC, Australia; CSIRO's DATA61, Eveleigh, NSW, Australia",IEEE Transactions on Services Computing,"7 Aug 2024","2024","17","4","1533","1547","Serverless edge computing is a specialized system design tailored for Internet of Things (IoT) applications. It leverages serverless computing to minimize operational management and enhance resource efficiency, and utilizes the concept of edge computing to allow code execution near the data sources. However, edge devices powered by renewable energy face challenges due to energy input variability, resulting in imbalances in their operational availability. As a result, high-powered nodes may waste excess energy, while low-powered nodes may frequently experience unavailability, impacting system sustainability. Addressing this issue requires energy-aware resource schedulers, but existing cloud-native serverless frameworks are energy-agnostic. To overcome this, we propose an energy-aware scheduler for sustainable serverless edge systems. We introduce a reference architecture for such systems and formally model energy-aware resource scheduling, treating the function-to-node assignment as an imbalanced energy-minimizing assignment problem. We then design an optimal offline algorithm and propose faasHouse, an online energy-aware scheduling algorithm that utilizes resource sharing through computation offloading. Lastly, we evaluate faasHouse against benchmark algorithms using real-world renewable energy traces and a practical cluster of single-board computers managed by Kubernetes. Our experimental results demonstrate significant improvements in balanced operational availability (by 46%) and throughput (by 44%) compared to the Kubernetes scheduler.","1939-1374","","10.1109/TSC.2024.3354296","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10400820","Edge computing;serverless;function-as-a-service;energy awareness;scheduling;sustainability","Edge computing;Internet of Things;Renewable energy sources;Logic gates;Containers;Batteries;Computers","","2","","34","IEEE","16 Jan 2024","","","IEEE","IEEE Journals"
"A Deep Reinforcement Learning Approach for Competitive Task Assignment in Enterprise Blockchain","G. Volpe; A. M. Mangini; M. P. Fanti","Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy",IEEE Access,"23 May 2023","2023","11","","48236","48247","With the advent of Industry 4.0, the demand of high computing power for tasks such as data mining, 3D rendering, file conversion and cryptography is continuously growing. To this extent, distributed and decentralized environments play a fundamental role by dramatically increasing the amount of available resources. However, there are still several issues in the existing resource sharing solutions, such as the uncertainty of task running time, the renting price and the security of transactions. In this work, we present a blockchain-enabled task assignment platform by performance prediction based on Hyperledger Fabric, an open-source solution for private and permissioned blockchains in enterprise contexts that outperforms other technologies in terms of modularity, security and performance. We propose a model-free deep reinforcement learning framework to predict task runtime in agents current load state while the agent is engaged in multiple concurrent tasks. In addition, we let clients choose between prediction accuracy and price saving on each request. This way, we implicitly give inaccurate agents a chance to get assignments by competing in price rather than in time, allowing them to collect new experiences and improve future predictions. We conduct extensive experiments to evaluate the performance of the proposed scheme.","2169-3536","","10.1109/ACCESS.2023.3276859","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10126111","Blockchain;cloud;deep reinforcement learning (DRL);resource sharing","Task analysis;Blockchains;Runtime;Peer-to-peer computing;Distributed ledger;Fabrics;Prediction algorithms","","2","","45","CCBYNCND","16 May 2023","","","IEEE","IEEE Journals"
"Handling Data Handoff of AI-Based Applications in Edge Computing Systems","D. Scotece; C. Fiandrino; L. Foschini","Department of Information Engineering and Computer Science, University of Bologna, Bologna, Italy; IMDEA Networks Institute, Madrid, Spain; Department of Information Engineering and Computer Science, University of Bologna, Bologna, Italy",IEEE Transactions on Network and Service Management,"12 Dec 2023","2023","20","4","4435","4447","Edge computing aims at better supporting low-latency applications. One of its key techniques is computation offloading, the process that outsources computing tasks from resourced-constrained mobile devices and moves them to edge data centers. In this paper, we tackle an emerging problem within the umbrella of computation offloading, i.e., migration of offloaded inference tasks of Artificial Intelligence (AI) trained models. Such context tailors migration aspects of data-sensitive services where i) the value of the updates is inversely proportional to the data age and ii) outage is highly detrimental to accuracy. To tackle this challenge, we propose Mobile Edge Data-handoff (MED) a framework able to relocate inference or online training tasks from one edge datacenter to another by moving only the necessary data to minimize any accuracy drop during the process. We implemented MED in a well-known edge computing emulator, openLEON, and experimentally verified its performance with an AI-based Industry 4.0 application that forecasts the gas flow in a chemical plant. For our experiments, we use a real, open-source dataset that contains sensors readings. Collected results show that MED, employing proactive data handoff algorithms, is able to minimize the packet loss during the handoff thereby providing guarantees on the inference accuracy.","1932-4537","","10.1109/TNSM.2023.3267942","Juan de la Cierva Incorporation; Spanish Ministry of Science and Innovation(grant numbers:IJC2019-039885-I); European Commission; Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU, partnership on “Telecommunications of the Future” (PE00000001 - program “RESTART”)(grant numbers:J33C22002880001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10104102","Multi-access edge computing;industry 4.0;data handoff;computation offloading","Edge computing;Task analysis;Computational modeling;Sensors;Industries;Data centers;Containers","","2","","67","IEEE","18 Apr 2023","","","IEEE","IEEE Journals"
"6G Enablers for Zero-Carbon Network Slices and Vertical Edge Services","R. Bolla; R. Bruschi; C. Lombardo; B. Siccardi","Department of Electrical, Electronic and Telecommunications Engineering, and Naval Architecture and the National Laboratory of Smart and Secure Networks, the Italian National Consortium for Telecommunications, University of Genoa, Genoa, Italy; Department of Electrical, Electronic and Telecommunications Engineering, and Naval Architecture and the National Laboratory of Smart and Secure Networks, the Italian National Consortium for Telecommunications, University of Genoa, Genoa, Italy; CNIT S2N National Laboratory, Genoa, Italy; Department of Electrical, Electronic and Telecommunications Engineering, and Naval Architecture and the National Laboratory of Smart and Secure Networks, the Italian National Consortium for Telecommunications, University of Genoa, Genoa, Italy",IEEE Networking Letters,"22 Sep 2023","2023","5","3","173","176","6G of radio mobile networks should become an essential enabler for any vertical sector to meet sustainable growth targets of both the United Nations (UN) 2030 Agenda and the European Green Deal. 6G might lead to the dematerialization of (most of) any physical appliances, moving their smart parts to the network edge as virtual software instances. Such software instances can leverage on novel carbon-neutral schemes for computing and communication. Owing this vision, this letter aims to analyze how energy- and carbon-awareness can be introduced into the complex architecture of 6G, and how the diverse stakeholders in the 6G value-chain can benefit from this awareness.","2576-3156","","10.1109/LNET.2023.3262861","HORIZON-JU-SNS-2022 Research and Innovation Action 6Green(grant numbers:101096925); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10086577","6G enablers;green networking;edge computing;carbon neutrality","6G mobile communication;Stakeholders;Green products;Ecosystems;Energy consumption;Carbon footprint;5G mobile communication","","2","","15","CCBY","29 Mar 2023","","","IEEE","IEEE Journals"
"JITA4DS: Disaggregated Execution of Data Science Pipelines Between the Edge and the Data Centre","G. Vargas-Solar; M. S. Hassan; A. Akoglu","French Council of Scientific Research (CNRS)-LIRIS, France; University of Arizona, USA; University of Arizona, USA",Journal of Web Engineering,"22 Sep 2023","2022","21","1","1","26","This paper targets the execution of data science (DS) pipelines supported by data processing, transmission and sharing across several resources executing greedy processes. Current data science pipelines environments provide various infrastructure services with computing resources such as general-purpose processors (GPP), Graphics Processing Units (GPUs), Field Programmable Gate Arrays (FPGAs) and Tensor Processing Unit (TPU) coupled with platform and software services to design, run and maintain DS pipelines. These one-fits-all solutions impose the complete externalization of data pipeline tasks. However, some tasks can be executed in the edge, and the backend can provide just in time resources to ensure ad-hoc and elastic execution environments. This paper introduces an innovative composable “Just in Time Architecture” for configuring DCs for Data Science Pipelines (JITA-4DS) and associated resource management techniques. JITA-4DS is a cross-layer man-agement system that is aware of both the application characteristics and the underlying infrastructures to break the barriers between applications, middleware/operating system, and hardware layers. Vertical integration of these layers is needed for building a customizable Virtual Data Center (VDC) to meet the dynamically changing data science pipelines' requirements such as performance, availability, and energy consumption. Accordingly, the paper shows an experimental simulation devoted to run data science workloads and determine the best strategies for scheduling the allocation of resources implemented by JITA-4DS.","1544-5976","","10.13052/jwe1540-9589.2111","National Science Foundation(grant numbers:NSF CNS-1624668); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10243631","Disaggregated data centers;data science pipelines;edge computing","Data centers;Tensors;Processor scheduling;Pipelines;Data science;Logic gates;Software","","2","","28","","22 Sep 2023","","","River Publishers","River Publishers Journals"
"A Task-Resource Mapping Algorithm for Large-Scale Batch-Mode Computational Marine Hydrodynamics Codes on Containerized Private Cloud","Y. Xu; P. Liu; I. Penesis; G. He","School of Computer Science and Communication Engineering, Guangxi University of Science and Technology, Liuzhou, China; Marine, Offshore and Subsea Technology, School of Engineering, Newcastle University, Newcastle Upon Tyne, U.K.; Australia Maritime College, University of Tasmania, Launceston, TAS, Australia; Harbin Institute of Technology, Weihai, China",IEEE Access,"18 Sep 2019","2019","7","","127943","127955","CPU time has long been a remaining problem for large-scale batch mode based scientific computing applications. To address this time-consuming problem, a container-based private cloud was employed, and a novel task-resource mapping algorithm was developed. Firstly, the execution features of typical batch mode codes were extracted and then computing jobs were formulated as a coarseness acyclic DAG. Secondly, to guarantee both job makespan and resource utilization, a novel task-resource mapping algorithm, along with container pre-planning and worst-case-first task placement phases, were developed. Finally, a typical Computational Marine Hydrodynamics software, Rotorysics, with a different scale of input data matrix was used as benchmark software. To manifest the effectiveness of the proposed method, a number of numerical examples were given via CloudSim and a small-medium containerized private cloud platform was adopted with three practical study cases. The computational results show that 1) compared with the traditional HPC workstation computing solution, container-based cloud solution shows significant savings in makespan by more than 6 times. 2) the new method is scalable to address bigger size batch computing problem up to a run matrix 108.","2169-3536","","10.1109/ACCESS.2019.2939903","University of Tasmania; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8827456","Computational marine hydrodynamics (CMH) codes;containerization;large-scale batch mode computing;private cloud;task-resource mapping algorithm","Task analysis;Cloud computing;Containers;Resource management;Software;Scheduling;Atmospheric modeling","","2","","47","CCBY","9 Sep 2019","","","IEEE","IEEE Journals"
"Automating the Deployment of Artificial Intelligence Services in Multiaccess Edge Computing Scenarios","D. C. G. Valadares; T. B. D. O. Filho; T. F. Meneses; D. F. S. Santos; A. Perkusich","Virtus RDI Center, Federal University of Campina Grande, Campina Grande, Paraíba, Brazil; Virtus RDI Center, Federal University of Campina Grande, Campina Grande, Paraíba, Brazil; Virtus RDI Center, Federal University of Campina Grande, Campina Grande, Paraíba, Brazil; Virtus RDI Center, Federal University of Campina Grande, Campina Grande, Paraíba, Brazil; Virtus RDI Center, Federal University of Campina Grande, Campina Grande, Paraíba, Brazil",IEEE Access,"28 Sep 2022","2022","10","","100736","100745","With the increasing adoption of the edge computing paradigm, including multi-access edge computing (MEC) in telecommunication scenarios, many works have explored the benefits of adopting it. Since MEC, in general, presents a reduction in latency and energy consumption compared to cloud computing, it has been applied to deploy artificial intelligence services. This kind of service can have distinct requirements, which involve different computational resource capabilities as well different data formats or communication protocols to collect data. In this sense, we propose the VEF Edge Framework, which aims at helping the development and deployment of artificial intelligence services for MEC scenarios considering requirements as low-latency and CPU/memory consumption. We explain the VEF architecture and present experimental results obtained with a base case’s implementation: an object detection inference service deployed with VEF. The experiments measured CPU and memory usage for the VEF’s main components and the processing time for two procedures (inference and video stream handling).","2169-3536","","10.1109/ACCESS.2022.3208118","Ministry of Science, Technology and Innovation of Brazil with Resources from Law No. 8,248, in October 1991 (Resources from the 3rd Softex/UFCG Partnership Agreement); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9895389","Future connected systems;fog computing;edge computing;intelligent services;services deployment","Artificial intelligence;Computer architecture;Virtual machining;Cloud computing;Servers;Intelligent systems;Image edge detection;Edge computing;Multi-access edge computing","","2","","25","CCBY","20 Sep 2022","","","IEEE","IEEE Journals"
"Resource Allocation Considering Impact of Network on Performance in a Disaggregated Data Center","A. Ikoma; Y. Ohsita; M. Murata","Graduate School of Information Science and Technology, Osaka University, Suita, Japan; Cybermedia Center, Osaka University, Toyonaka, Japan; Graduate School of Information Science and Technology, Osaka University, Suita, Japan",IEEE Access,"17 May 2024","2024","12","","67600","67618","A disaggregated data center (DDC) can efficiently use resources such as CPU and memory. In a DDC, because each resource is independent and connected by a network, communication between resources is required for task execution. Communication delays can be an overhead for task execution, causing performance degradation. Because communication delays depend on the correspondence between resources on the network and the paths over which they communicate, an efficient resource allocation method is required to determine this relationship. Herein, we propose a resource allocation method called RA-CNP to execute many tasks simultaneously while satisfying performance requirements. This method models the impact of the network on the performance of tasks for the provided service. Furthermore, this method defines a resource allocation problem to avoid the allocation of resources that will be requested in the future. We evaluated the effectiveness of our method by simulating various DDC networks, assuming a DDC at the edge. The results demonstrated that RA-CNP could execute more tasks than conventional methods could, without violating performance requirements, based only on current network information in both networks configured by circuit and packet switches. RA-CNP could allocate resources in less than 10 s, even in a relatively large network configured with 64 switches; this capability demonstrates its practicality.","2169-3536","","10.1109/ACCESS.2024.3399930","National Institute of Information and Communications Technology(grant numbers:JPJ012368C00101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10529277","Disaggregated data center;optical network;resource allocation;resource disaggregation","Task analysis;Resource management;Optical switches;Data centers;Optical packet switching;Delays;Optical fiber networks","","2","","29","CCBYNCND","13 May 2024","","","IEEE","IEEE Journals"
"Multimodal Renewable Energy Hybrid Supply Optimization Model Based on Heterogeneous Cloud Wireless Access","F. Tian; H. Wang; H. Jiang; B. Zhao","College of Information, Shenyang Institute of Engineering, Shenyang, China; College of Information, Shenyang Institute of Engineering, Shenyang, China; School of Renewable Energy, Shenyang Institute of Engineering, Shenyang, China; Innovation and Entrepreneurship Education Center, Shenyang Institute of Engineering, Shenyang, China",IEEE Access,"6 Jun 2024","2024","12","","78286","78303","With the increasing emphasis on environmental issues, the utilization of renewable energy has been recognized as a feasible solution to address the energy crisis and reduce environmental pollution. In view of this, this article proposes a multi-modal renewable energy hybrid power supply optimization model based on heterogeneous cloud wireless access. The model innovatively combines heterogeneous cloud wireless access technology and various intelligent optimization algorithms, including k-clustering algorithm, particle swarm optimization algorithm, and whale optimization algorithm, forming a hybrid optimization algorithm. In order to comprehensively evaluate the actual performance of the model, this study recruited 20 experts to provide detailed ratings on four core dimensions: cost-benefit ratio, reliability, robustness, and user satisfaction. The results showed that the model scored 95.1, 96.4, 95.6, and 96.2 in the four dimensions of cost-benefit ratio, reliability indicators, robustness, and user satisfaction, respectively. This series of significant data not only confirms the theoretical superiority of the model, but also demonstrates its strong potential and practical value in practical applications. In summary, this study provides a promising and innovative solution for the field of renewable energy supply.","2169-3536","","10.1109/ACCESS.2024.3407726","Department of Education of Liaoning Province 2023 Basic Scientific Research Projects of Universities: “Research on Key Technologies for Coordinated Regulation of Multi Load and Clean Energy Based on Deep Learning”(grant numbers:JYTMS20230313); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10542731","K-clustering algorithm;heterogeneous cloud radio access;particle swarm optimization algorithm;WOA algorithm;energy supply optimization","Optimization;Clustering algorithms;Heuristic algorithms;Renewable energy sources;Wireless communication;Cloud computing;Whale optimization algorithms","","2","","30","CCBYNCND","31 May 2024","","","IEEE","IEEE Journals"
"AI-Enhanced Digital Twin Framework for Cyber-Resilient 6G Internet of Vehicles Networks","Y. Yigit; L. A. Maglaras; W. J. Buchanan; B. Canberk; H. Shin; T. Q. Duong","School of Computing, Engineering and The Built Environment, Edinburgh Napier University, Edinburgh, U.K.; School of Computing, Engineering and The Built Environment, Edinburgh Napier University, Edinburgh, U.K.; Blockpass ID Lab, Edinburgh Napier University, Edinburgh, U.K.; School of Computing, Engineering and The Built Environment, Edinburgh Napier University, Edinburgh, U.K.; Department of Electronics and Information Convergence Engineering, Kyung Hee University, Yongin, South Korea; Faculty of Engineering and Applied Science, Memorial University, St. John’s, NL, Canada",IEEE Internet of Things Journal,"7 Nov 2024","2024","11","22","36168","36181","Digital twin technology is crucial to the development of the sixth-generation (6G) Internet of Vehicles (IoV) as it allows the monitoring and assessment of the dynamic and complicated vehicular environment. However, 6G IoV networks have critical challenges in network security and computational efficiency, which need to be addressed. Existing digital twin technologies in 6G IoV networks often suffer from limitations, such as reliance on static models and high computational demands, leading to unstable attack detection and inefficiencies. Their results for attack detection performance metrics, precision, detection rate, and F1-Score are insufficient for 6G IoV. Moreover, these systems concentrate all computational processes within the digital twin’s service layer, leading to inefficiencies. To address these challenges, we introduce a novel artificial intelligence (AI) enhanced digital twin framework designed to significantly improve 6G IoV network security and computational efficiency under dynamic conditions. Our framework employs an advanced feature engineering module that uses feature selection methods and stacked sparse autoencoders (ssAE) to reduce feature dimensions within the cyber twin layer, effectively distributing the overall computational load. It also utilizes an online learning module which enables a network-aware attack detection mechanism for precise attack detection. The proposed solution exhibits a stable performance of around 98% success rate regarding attack detection metrics against two data sets. Specifically, our solution reduces system latency by 12%, energy consumption by 15%, RAM usage by 20%, and improves packet delivery rates by 6.1%. These findings underscore the potential of our framework to enhance the robustness and responsiveness of 6G IoV systems, offering a significant contribution to vehicular network security and management.","2327-4662","","10.1109/JIOT.2024.3455089","Scientific and Technological Research Council of Turkey (TUBITAK) 1515 Frontier Research and Development Laboratories Support Program for BTS Advanced AI Hub: BTS Autonomous Networks and Data Innovation Lab Project(grant numbers:5239903); National Research Foundation of Korea (NRF) Grant funded by the Korean Government (MSIT)(grant numbers:NRF-2022R1A4A3033401); Canada Excellence Research Chair (CERC)(grant numbers:CERC-2022-00109); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10673900","Artificial intelligence (AI);digital twin;Internet of Vehicles (IoV);ITS;security;vehicular ad-hoc network (VANET)","6G mobile communication;Digital twins;Vehicle dynamics;Feature extraction;Artificial intelligence;Measurement;Heuristic algorithms","","2","","33","IEEE","10 Sep 2024","","","IEEE","IEEE Journals"
"Exploring the Synergy of Fog Computing, Blockchain, and Federated Learning for IoT Applications: A Systematic Literature Review","W. V. Solis; J. Marcelo Parra-Ullauri; A. Kertesz","Software Engineering Department, University of Szeged, Szeged, Hungary; Smart Internet Laboratory, University of Bristol, Bristol, U.K.; Software Engineering Department, University of Szeged, Szeged, Hungary",IEEE Access,"17 May 2024","2024","12","","68015","68060","The proliferation of Internet of Things (IoT) applications poses formidable challenges in managing data processing, privacy, and security. In response, technologies such as Fog Computing (FC), Blockchain (BC), and Federated Learning (FL) have emerged as promising solutions. Combining these technologies can broaden their scope, and impose novel challenges. This paper conducts a Systematic Literature Review (SLR) to investigate their integration within the IoT domain, systematically evaluating the current state-of-the-art by analyzing 40 papers against 38 extraction criteria, encompassing technical characteristics specific to FC, BC, FL, or their integration. The findings offer insights into the advantages, challenges, opportunities, and limitations of this integration, addressing data processing, privacy, and security concerns in IoT. By filling a research gap and directly examining FC, BC, and FL interoperability across architectural layers, this study contributes to knowledge expansion in the field. This paper proposes a novel framework for implementing FL and BC within FC environments for IoT applications, alongside a comprehensive synthesis of existing literature, distinguishing it from previous research efforts. Furthermore, it offers valuable insights into the current landscape, identifies research needs, and proposes future research directions. The framework and literature synthesis provided allow readers to access customized information on FC-BC-FL integration, aiding in designing and implementing robust IoT solutions.","2169-3536","","10.1109/ACCESS.2024.3398034","Ministry of Culture and Innovation of Hungary through the National Research, Development and Innovation Fund(grant numbers:TKP2021-NVA); National Project(grant numbers:TKP2021-NVA-09); Connecting Education and Research Communities for an Innovative Resource Aware Society (CERCIRAS) European Cooperation in Science and Technology (COST) Action(grant numbers:CA19135); COST Association, along with the University of Szeged Open Access Fund(grant numbers:6858); U.K. Government (GOV) Department of Science, Innovation, and Technology (DSIT) [Future Open Networks Research Challenge (FONRC)] Project REASON; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10522660","Blockchain;edge computing;federated learning;fog computing;Internet of Things;systematic literature review","Internet of Things;Blockchains;Bibliographies;Edge computing;Systematics;Computer architecture;Federated learning;Federated learning;Edge computing;Reviews;Database systems","","2","","112","CCBYNCND","8 May 2024","","","IEEE","IEEE Journals"
"Dynamic, Context-Aware Cross-Layer Orchestration of Containerized Applications","R. C. Sofia; D. Dykeman; P. Urbanetz; A. Galal; D. A. Dave","fortiss GmbH, Research Institute of the Free State of Bavaria for Software Intensive Systems and Services, Munich, Germany; Zurich Research Laboratory, IBM Research Europe, Zürich, Switzerland; Zurich Research Laboratory, IBM Research Europe, Zürich, Switzerland; fortiss GmbH, Research Institute of the Free State of Bavaria for Software Intensive Systems and Services, Munich, Germany; fortiss GmbH, Research Institute of the Free State of Bavaria for Software Intensive Systems and Services, Munich, Germany",IEEE Access,"4 Sep 2023","2023","11","","93129","93150","Container orchestration handles the semi-automated management of applications across Edge-Cloud, providing features such as autoscaling, high availability, and portability. Having been developed for Cloud-based applications, container orchestration faces challenges in the context of decentralized Edge-Cloud environments, requiring a higher degree of adaptability in the verge of mobility, heterogeneous networks, and constrained devices. In this context, this perspective paper aims at igniting discussion on the aspects that a dynamic orchestration approach should integrate to support an elastic orchestration of containerized applications. The motivation for the provided perspective focuses on proposing directions to better support challenges faced by next-generation IoT services, such as mobility or privacy preservation, advocating the use of context awareness and a cognitive, cross-layer approach to container orchestration to be able to provide adequate support to next-generation services. A proof of concept (available open source software) of the discussed concept has been implemented in a testbed composed of embedded devices.","2169-3536","","10.1109/ACCESS.2023.3307026","Horizon Europe Cognitive Decentralised Edge Cloud Orchestration (CODECO)(grant numbers:101092696); fortiss-IBM Center for Artificial Intelligence (C4AI) EDGE; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10225530","Context-awareness;IoT;edge computing;machine learning;data observability","Containers;Internet of Things;Next generation networking;Cloud computing;Virtual machine monitors;Dynamic scheduling;Delays;Context awareness;Edge computing;Machine learning;Observability","","2","","57","CCBYNCND","21 Aug 2023","","","IEEE","IEEE Journals"
"On-Board Computer for CubeSats: State-of-the-Art and Future Trends","A. Cratere; L. Gagliardi; G. A. Sanca; F. Golmar; F. Dell’Olio","Department of Electrical and Information Engineering, Micro Nano Sensor Group, Polytechnic University of Bari, Bari, Italy; Instituto de Ciencias Físicas, Universidad de San Martín-CONICET, Buenos Aires, Argentina; Instituto de Ciencias Físicas, Universidad de San Martín-CONICET, Buenos Aires, Argentina; Instituto de Ciencias Físicas, Universidad de San Martín-CONICET, Buenos Aires, Argentina; Department of Electrical and Information Engineering, Micro Nano Sensor Group, Polytechnic University of Bari, Bari, Italy",IEEE Access,"24 Jul 2024","2024","12","","99537","99569","Over the past three decades, the acceptance of higher risk thresholds within the space industry has facilitated the widespread integration of commercial off-the-shelf (COTS) components into avionics and payloads, leading to a remarkable transformation in the design of space missions. This transformation has led to the emergence of the New Space Economy and the widespread adoption of lean or small satellites in general, particularly CubeSats. CubeSats are now widely used in commercial, scientific, and research applications due to their versatility, affordability, simplicity of development, and accelerated development timelines. On-board computing plays a crucial role in the design of CubeSat missions, as increasingly high-performance computational requirements are needed to meet the challenges of future missions. This paper systematically reviews the state-of-the-art of CubeSat Command and Data Handling (C&DH) sub-system, covering both hardware components and flight software (FSW) development frameworks. It presents an analysis of the key features and recent developments of on-board computers (OBCs) in commercial and academic institutional projects funded by governments, agencies and public institutions. It further examines the effects of space radiation on avionics components and discusses the main fault-tolerance techniques used in CubeSat platforms. Finally, this paper highlights trends and hazards for future CubeSat avionics and identify potential directions for future developments in high-performance on-board computing. By synthesizing contemporary research and industry insights, this paper aims to shed light on CubeSat OBC design, providing an overview of the existing technology landscape and the challenges to be addressed for next-generation mission needs.","2169-3536","","10.1109/ACCESS.2024.3428388","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10597570","Command and data handling (C&DH);CubeSats;on-board computer (OBC);small satellite avionics","CubeSat;Payloads;Aerospace electronics;Computer architecture;Space vehicles;Distributed databases;On board unit;Small satellites","","2","","235","CCBYNCND","15 Jul 2024","","","IEEE","IEEE Journals"
"An Efficient Communication and Computation Resources Sharing in Information-Centric 6G Networks","M. Imran; M. N. Ali; M. S. U. Din; M. A. U. Rehman; B. -S. Kim","Department of Software and Communications Engineering, Hongik University, Sejong, South Korea; Department of Software and Communications Engineering, Hongik University, Sejong, South Korea; Department of Electronics and Computer Engineering, Hongik University, Sejong, South Korea; Department of Computing and Mathematics, Manchester Metropolitan University, Manchester, U.K.; Department of Software and Communications Engineering, Hongik University, Sejong, South Korea",IEEE Internet of Things Journal,"5 Aug 2024","2024","11","16","27275","27294","To efficiently allocate communication and computation resources among enhanced mobile broadband (eMBB) and ultrareliable low-latency communication (uRLLC) applications, this article proposes a joint communication and computation resource allocation scheme in sixth generation (6G) information-centric networks (ICNs) named: CCR-ICN. To achieve efficient communication and computation resources sharing, the CCR-ICN scheme designs a microservice-centric interest naming structure to enable named-based communication. Furthermore, the CCR-ICN scheme designs a robust forwarding strategy specifically tailored to efficiently allocate communication resources, such as bandwidth among eMBB and uRLLC application requests. In addition, for the allocation of computation resources, the proposed scheme designs a priority-driven computation resources allocation mechanism ensuring effective allocation of multiaccess edge computing computation resources. To evaluate the proposed scheme, we performed extensive simulations in a widely used NS-3-based ndnSIM simulator and the results reveal the CCR-ICN achieves 66.82% higher communication resource allocation, and a 17.51% lower communication overhead compared to the TS-6G scheme. Moreover, the CCR-ICN performs computation request satisfaction at 42.03% and 12.72% higher than the TS-6G and extended weighted fair queueing with latency constraint (EWFQ/LC) schemes, respectively. Furthermore, the CCR-ICN achieves computation resource utilization of 35.54% and 21.63% lower than the EWFQ/LC and TS-6G schemes, respectively.","2327-4662","","10.1109/JIOT.2024.3397674","National Research Foundation of Korea (NRF) Grant funded by the Korea Government (MSIT)(grant numbers:2022R1A2C1003549); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10522479","6G;information-centric networking;named data networking (NDN);networks slices;resource allocation","Resource management;Ultra reliable low latency communication;Microservice architectures;Computer architecture;Bandwidth;TCPIP;6G mobile communication","","2","","37","IEEE","7 May 2024","","","IEEE","IEEE Journals"
"Multi-Tree Genetic Programming Hyper-Heuristic for Dynamic Flexible Workflow Scheduling in Multi-Clouds","Z. Sun; Y. Mei; F. Zhang; H. Huang; C. Gu; M. Zhang","School of Computer Science and Technology, Harbin Institute of Technology (Shenzhen), Shenzhen, China; Evolutionary Computation Research Group, Centre for Data Science and Artificial Intelligence & School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; Evolutionary Computation Research Group, Centre for Data Science and Artificial Intelligence & School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; School of Computer Science and Technology, Harbin Institute of Technology (Shenzhen), Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology (Shenzhen), Shenzhen, China; Evolutionary Computation Research Group, Centre for Data Science and Artificial Intelligence & School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand",IEEE Transactions on Services Computing,"8 Oct 2024","2024","17","5","2687","2703","Multi-cloud is a promising paradigm due to its advantages such as avoiding vendor lock-in and optimising costs. This article focuses on dynamic flexible workflow scheduling with minimum total monetary cost in multi-clouds, considering multiple categories of services for each cloud with different configurations and billing methods. Existing studies generally ignore the characteristics and states of each individual cloud when making schedules, which may be ineffective regarding cost savings and quality of service. To address this issue, we propose to introduce a cloud selection decision on top of the existing task selection and resource selection decisions to help us select appropriate resource for task in an overall cost-effective cloud. To automatically learn the task, cloud and resource selection rules simultaneously, we propose a new genetic programming with multi-tree representation based on a customised discrete event-driven dynamic workflow scheduling simulator. Simulation results based on two real-world data traces show that the proposed algorithm performs significantly better than the state-of-the-art algorithms in terms of reducing the rental costs and deadline deviation, and improving the success rate. The results also show that the superiority of the proposed algorithm lies in the ability to select an appropriate cloud resource for a task.","1939-1374","","10.1109/TSC.2024.3394691","Shenzhen Science and Technology Program(grant numbers:GXWD20220817124827001,JCYJ20210324132406016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10509784","Deadline constraint;dynamic workflow scheduling;genetic programming;multi-clouds;hyper-heuristic","Dynamic scheduling;Cloud computing;Task analysis;Costs;Heuristic algorithms;Processor scheduling;Genetic programming","","2","","60","IEEE","29 Apr 2024","","","IEEE","IEEE Journals"
"BBAD: Blockchain-Backed Assault Detection for Cyber Physical Systems","M. Anwar; N. Tariq; M. Ashraf; S. A. Moqurrab; B. Alabdullah; H. S. Alsagri; A. Almjally","Department of Avionics Engineering, Air University, Islamabad, Pakistan; Department of Avionics Engineering, Air University, Islamabad, Pakistan; School of Electrical Engineering and Computer Science, National University of Sciences & Technology, Islamabad, Pakistan; School of Computing, Gachon University, Sujeong-gu, Seongnam-si, South Korea; Department of Information Systems, College of Computer and Information Sciences, Princess Nourah Bint Abdulrahman University, P. O. Box 88428, Riyadh, Saudi Arabia; Information Systems Department, College of Computer and Information Sciences, Imam Mohammad Ibn Saud Islamic University, Riyadh, Saudi Arabia; Information Technology, College of Computer and Information Sciences, Imam Mohammad Ibn Saud Islamic University, Riyadh, Saudi Arabia",IEEE Access,"29 Jul 2024","2024","12","","101878","101894","Cybersecurity challenges pose a significant threat to Healthcare Cyber Physical Systems (CPS) because they heavily rely on wireless communication. Particularly, jamming attacks can severely disrupt the integrity of these CPS networks. This research introduces a decentralized system to address this issue. Therefore, this paper suggested a system that leverages trust and blockchain technology to detect jamming attacks in healthcare CPS effectively. It proposes a layered model to improve CPS networks’ lifetime and performance. In smart healthcare environments, it ensures secure and reliable communication between sensor nodes, wearable sensors, medical devices, and monitoring systems. Results show that the suggested approach outperforms the baseline model in identifying and minimizing jamming assaults, with an average percentage difference of 15.71% more detection rate, 20.21% less packet loss rates, 16.65% less node-level energy consumption, reduced network latency of 8.29%, and 9.63% more network throughput.","2169-3536","","10.1109/ACCESS.2024.3404656","Princess Nourah Bint Abdulrahman University, Riyadh, Saudi Arabia, through the Researchers Supporting(grant numbers:PNURSP2024R440); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10537615","Smart healthcare;cyber-physical systems;jamming attacks;sensor nodes;trust","Jamming;Medical services;Security;Blockchains;Wireless communication;Reliability;Interference;Smart healthcare;Cyber-physical systems;Trust management","","2","","59","CCBYNCND","23 May 2024","","","IEEE","IEEE Journals"
"Open RAN: A Concise Overview","M. Wani; M. Kretschmer; B. Schröder; A. Grebe; M. Rademacher","Department of Cooperation Systems, Fraunhofer FIT, Sankt Augustin, Germany; Department of Cooperation Systems, Fraunhofer FIT, Sankt Augustin, Germany; Department of Industrial Communication & Data Research, brown-iposs GmbH, Bonn, Germany; Institute on Computer and Communication Technology, Technische Hochschule Köln, Cologne, Germany; Department of Computer Science, Hochschule Bonn-Rhein-Sieg, Sankt Augustin, Germany",IEEE Open Journal of the Communications Society,"8 Jan 2025","2025","6","","13","28","Open RAN has emerged as a transformative approach in the evolution of cellular networks, addressing challenges posed by modern applications and high network density. By leveraging disaggregated, virtualized, and software-based elements interconnected through open standardized interfaces, Open RAN introduces agility, cost-effectiveness, and enhanced competition in the Radio Access Network (RAN) domain. The Open RAN paradigm, driven by the O-RAN Alliance specifications, is set to transform the telecom ecosystem. Despite extensive technical literature, there is a lack of succinct summaries for industry professionals, researchers, and policymakers. This paper addresses this gap by providing a concise, yet comprehensive overview of Open RAN. Compared to previous work, our approach introduces Open RAN by gradually splitting up different components known from previous RAN architectures. We believe that this approach leads to a better understanding for people already familiar with the general concept of mobile communication networks. Building upon this general understanding of Open RAN, we introduce key architectural principles, interfaces, components and use-cases. Moreover, this work investigates potential security implications associated with adopting Open RAN architecture, emphasizing the necessity of robust network protection measures.","2644-125X","","10.1109/OJCOMS.2024.3430823","Federal Ministry for Digital and Transport of the Federal Republic of Germany(grant numbers:165GU054B); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10601697","Open RAN;5G;security;mobile networks","Open RAN;Security;Computer architecture;Hardware;5G mobile communication;Costs;Task analysis","","2","","117","CCBYNCND","18 Jul 2024","","","IEEE","IEEE Journals"
"Phoenix: Transformative Reconfigurability for Edge IoT Devices in Small-Scale IoT Systems","M. Moghaddassian; S. Shafaghi; P. Habibi; A. Leon-Garcia","The Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; The Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; The Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; The Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada",IEEE Access,"15 Dec 2023","2023","11","","137821","137836","Transformative reconfigurability refers to the ability of changing the current software stack of a configurable device by fully replacing its existing one. In the context of IoT systems, such major device reconfigurations can be used to change the role, adapt new functionality, and keep reconfigurable IoT devices compatible with the IoT systems requirements as the ambient technology around them evolve, thus fostering a thriving and continuously-connected IoT environment. In this paper, we introduce Phoenix, an IoT device configuration management system that is designed to automate transformative reconfigurability for edge IoT devices at small scales. Edge IoT devices are typically computationally capable and configurable devices that have enough processing power to run user programs and control sensors and embedded devices in an IoT environment. Enabling transformative reconfigurability for such devices at small scales can increase IoT systems flexibility, efficiency, and adaptability in small IoT environments, for example, agri-farms, smart homes, micro grids, and the like. Phoenix manages the life cycle of edge IoT devices configuration and uses bare-metal provisioning to provide unattended installation of new software stacks that are defined by user intents that instruct the reconfiguration process. We implemented a Phoenix proof-of-concept system and deployed it on the SAVI testbed where we evaluated its performance in reconfiguring a variety of edge IoT devices under different network conditions. Our results indicate that Phoenix can meet the requirements of small-scale heterogeneous IoT systems in various application environments.","2169-3536","","10.1109/ACCESS.2023.3339154","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341245","Bare metal provisioning;infrastructure automation;Internet of Things;reconfigurability","Internet of Things;Engines;Operating systems;Microservice architectures;Metals;Full stack;Fires;Reconfigurable architectures;Edge computing","","2","","66","CCBY","4 Dec 2023","","","IEEE","IEEE Journals"
"Containerized Microservices: A Survey of Resource Management Frameworks","L. M. Al Qassem; T. Stouraitis; E. Damiani; I. M. Elfadel","Center for Secure Cyber Physical Systems and the College of Computing and Mathematical Sciences, Khalifa University, Abu Dhabi, UAE; Center for Secure Cyber Physical Systems and the College of Computing and Mathematical Sciences, Khalifa University, Abu Dhabi, UAE; Center for Secure Cyber Physical Systems and the College of Computing and Mathematical Sciences, Khalifa University, Abu Dhabi, UAE; Center for Secure Cyber Physical Systems and the College of Computing and Mathematical Sciences, Khalifa University, Abu Dhabi, UAE",IEEE Transactions on Network and Service Management,"22 Aug 2024","2024","21","4","3775","3796","The growing adoption of microservice architectures (MSAs) has led to major research and development efforts to address their challenges and improve their performance, reliability, and robustness. Important aspects of MSA that are not sufficiently covered in the open literature include efficient cloud resource allocation and optimal power management. Other aspects of MSA remain widely scattered in the literature, including cost analysis, service level agreements (SLAs), and demand-driven scaling. In this article, we examine recent cloud frameworks for containerized microservices with a focus on efficient resource utilization using auto-scaling. We classify these frameworks on the basis of their resource allocation models and underlying hardware resources. We highlight current MSA trends and identify workload-driven resource sharing within microservice meshes and SLA streamlining as two key areas for future microservice research.","1932-4537","","10.1109/TNSM.2024.3388633","Khalifa University Center of Secure Cyberphysical System; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10499978","Microservices;containers;resource management;container orchestration;machine learning;workload forecasting;reactive allocation;predictive allocation","Microservice architectures;Resource management;Cloud computing;Containers;Service level agreements;Computer architecture;Surveys","","2","","120","IEEE","15 Apr 2024","","","IEEE","IEEE Journals"
"Application-Oriented Cloud Workload Prediction: A Survey and New Perspectives","B. Feng; Z. Ding","Department of Computer Science and Technology, Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, China; Department of Computer Science and Technology, Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, China",Tsinghua Science and Technology,"11 Sep 2024","2025","30","1","34","54","Workload prediction is critical in enabling proactive resource management of cloud applications. Accurate workload prediction is valuable for cloud users and providers as it can effectively guide many practices, such as performance assurance, cost reduction, and energy consumption optimization. However, cloud workload prediction is highly challenging due to the complexity and dynamics of workloads, and various solutions have been proposed to enhance the prediction behavior. This paper aims to provide an in-depth understanding and categorization of existing solutions through extensive literature reviews. Unlike existing surveys, for the first time, we comprehensively sort out and analyze the development landscape of workload prediction from a new perspective, i.e., application-oriented rather than prediction methodologies per se. Specifically, we first introduce the basic features of workload prediction, and then analyze and categorize existing efforts based on two significant characteristics of cloud applications: variability and heterogeneity. Furthermore, we also investigate how workload prediction is applied to resource management. Finally, open research opportunities in workload prediction are highlighted to foster further advancements.","1007-0214","","10.26599/TST.2024.9010024","National Natural Science Foundation of China(grant numbers:62372330); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10676402","cloud computing;workload prediction;resource management;artificial intelligence for IT operations (AIOps)","Surveys;Measurement;Energy consumption;Costs;Reviews;Taxonomy;Predictive models","","1","","117","","11 Sep 2024","","","TUP","TUP Journals"
"A pluggable module for enabling a trusted edge device management system based on microservice","S. -H. Lee; J. -Z. Liu","Department of Intelligent Commerce, National Kaohsiung University of Science and Technology, Kaohsiung, Taiwan; Department of Intelligent Commerce, National Kaohsiung University of Science and Technology, Kaohsiung, Taiwan",Journal of Communications and Networks,"21 Jul 2023","2023","25","3","381","391","In the development of the intelligent Internet of things, edge computing plays a key role. Not only that it has the characteristics of a quick response, but it can also effectively reduce the burden of cloud computing. In addition, it can also extend the application of network edge through the coexistence and collaboration with the cloud system. However, with a large number of edge devices being deployed, the previous remote device management system will face the challenges of resource constraints and software firmware compatibility. In addition, implementing remote fault management, configuration management, accounting management, performance management, and security management through edge device management is a key task. Therefore, this study has designed and implemented a trusted edge device management system based on a microservice and a plug-and-play hardware management device. The running state of the edge device, the functions of remote device control, and system restart are monitored remotely by in- and out-of-band management modes. In addition, in terms of data transmission security, the design concept of a trusted platform module is introduced to realize data encryption and authentication, and ensure security and reliability. In this paper, the operational feasibility of management services based on containerized microservice in edge devices is verified with system benchmarking tools without affecting system performance. According to the experimental results, the proposed architecture in this study can be effective in edge device management.","1976-5541","","10.23919/JCN.2023.000023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10146579","Edge computing;Internet of things;microservice;remote device management","Internet of Things;Microservice architectures;Cloud computing;Hardware;Computer architecture;Protocols;Software","","1","","","","8 Jun 2023","","","KICS","KICS Journals"
"ODTE: A Metric for Digital Twin Entanglement","P. Bellavista; N. Bicocchi; M. Fogli; C. Giannelli; M. Mamei; M. Picone","Department of Computer Science and Engineering (DISI), University of Bologna, Bologna, Italy; Department of Engineering Sciences and Methods, University of Modena and Reggio Emilia, Reggio Emilia, Italy; Department of Mathematics and Computer Science, University of Ferrara, Ferrara, Italy; Department of Mathematics and Computer Science, University of Ferrara, Ferrara, Italy; Department of Engineering Sciences and Methods, University of Modena and Reggio Emilia, Reggio Emilia, Italy; Department of Engineering Sciences and Methods, University of Modena and Reggio Emilia, Reggio Emilia, Italy",IEEE Open Journal of the Communications Society,"26 Apr 2024","2024","5","","2377","2390","Digital Twins (DTs) have recently emerged as a valuable approach for modeling, monitoring, and controlling physical objects in Industrial Internet of Things applications. Measuring the quality of entanglement between the digital and physical counterparts plays a crucial role in the adoption of DTs. In this context, entanglement denotes how well a DT mirrors its counterpart and the extent to which the behavior of the physical counterpart aligns with the commands issued by the DT. In this paper we propose a concise yet expressive and original metric for representing the quality of entanglement, namely Overall Digital Twin Entanglement (ODTE), based on two key factors: timeliness and completeness, i.e., the freshness of the collected data and the ratio between collected and total data, respectively. In addition, the paper describes how we have built our industrial testbed implemented on top of Kubernetes, where we show practical applications of the proposed ODTE metric by highlighting and discussing its benefits in realistic use cases.","2644-125X","","10.1109/OJCOMS.2024.3385659","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10492860","Digital twins;entanglement;industrial IoT","Measurement;Quality of experience;Quality of service;Digital twins;Synchronization;Task analysis;Industrial Internet of Things","","1","","39","CCBYNCND","4 Apr 2024","","","IEEE","IEEE Journals"
"Throughput Maximization for Result Multicasting by Admitting Delay-Aware Tasks in MEC Networks for High-Speed Railways","J. Xu; Z. Wei; X. Yuan; Y. Qiao; Z. Lyu; J. Han","School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; Department of Computer Science and Engineering, University of North Texas, Denton, TX, USA; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China",IEEE Transactions on Vehicular Technology,"19 Jun 2024","2024","73","6","8765","8781","The rapid expansion of high-speed railways (HSRs) and the growing demand for diverse data services during long journeys require efficient computing services. Mobile Edge Computing (MEC) emerged as a promising platform to fulfill this demand. We envision a scenario wherein passengers interact with each other on the same or different trains in real-time by offloading computationally intensive and delay-sensitive tasks to the track-side MEC networks for HSRs and computation results are multicast to the receivers. To improve the quality of data services, we propose a novel approach to optimize network throughput by admitting as many tasks as possible, subject to delay constraints, and multicasting the maximum number of results. The high mobility of trains and the frequent handovers during train-ground communication are factored into our scheme, which presents significant challenges to jointly consider the dynamic multicast grouping and admission/rejection policies for tasks/results. We introduce the multi-group-shared Group Steiner tree (GST) model and propose an efficient heuristic algorithm that reduces the multicast routing problem to finding a GST for each candidate cloudlet. The effectiveness of our proposed algorithm is demonstrated through simulations and the results are promising.","1939-9359","","10.1109/TVT.2024.3357769","Natural Science Foundation of Anhui Province(grant numbers:2108085MF202); National Natural Science Foundation of China(grant numbers:62002097); Fundamental Research Funds for the Central Universities(grant numbers:PA2023GDGP0044); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10412682","Edge computing;high-speed railways;multicasting;group Steiner tree problem;admission control","Cloud computing;Throughput;Delays;Routing;Multicast communication;Multicast algorithms;Edge computing","","1","","60","IEEE","23 Jan 2024","","","IEEE","IEEE Journals"
"VNF Placement and Dynamic NUMA Node Selection Through Core Consolidation at the Edge and Cloud","T. Ben Salah; M. Avgeris; A. Leivadeas; I. Lambadaris","Department of Software and Information Technology Engineering, École de Technologie Supérieure, University of Quebec, Montreal, QC, Canada; Department of Software and Information Technology Engineering, École de Technologie Supérieure, University of Quebec, Montreal, QC, Canada; Department of Software and Information Technology Engineering, École de Technologie Supérieure, University of Quebec, Montreal, QC, Canada; Department of Systems and Computer Engineering, Carleton University, Ottawa, QC, Canada",IEEE Transactions on Network and Service Management,"22 Aug 2024","2024","21","4","4655","4668","The recent networking trends driven primarily by the different virtualization technologies, such as Network Function Virtualization (NFV) and Service Function Chaining (SFC) pave the way for next-generation network services. In the 5G and beyond era, such services usually have strict delay requirements and the wider adoption of the distribution of their computational needs across the Edge-to-Cloud continuum is certainly a step in the right direction. However, the majority of the optimization solutions for placing the virtualized services so far focus on server selection, leaving other areas such as the impact of Non-Uniform Memory Access (NUMA) and CPU core selection underexplored. In this work, we herein formulate the problem of placing services as SFCs on an Edge/Cloud infrastructure, as a Mixed Integer Programming (MIP) problem. Then, we propose a heuristic algorithm called “Dynamic numa node Selection through Cores consolidation – DySCo” to solve it, which optimizes the placement in terms of server, NUMA and core selection. To the best of our knowledge, this is the first attempt to optimize network service placement in an Edge-Cloud interplay. Extensive simulation evaluation shows that DySCo is able to perform close to optimal while finding a solution in a real time fashion. Compared to a mix of baselines and modified solutions from the literature to treat this new problem, DySCo reduces on average the deployment cost by 17.53% and the delay by 28.88% for a given SFC.","1932-4537","","10.1109/TNSM.2024.3387275","Natural Sciences and Engineering Research Council of Canada (NSERC)(grant numbers:RGPIN-2019-05250); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10496505","Network function virtualization;service function chaining;edge computing;cloud computing;resource allocation;non-uniform memory access","Servers;Delays;Resource management;Costs;Random access memory;Quality of service;Throughput","","1","","28","IEEE","10 Apr 2024","","","IEEE","IEEE Journals"
"A Data Protection Focused Adaptation Engine for Distributed Video Analytics Pipelines","C. Lachner; J. Laufer; S. Dustdar; K. Pohl","Distributed Systems Group, TU Wien, Vienna, Austria; Paluno—The Ruhr Institute for Software Technology, University of Duisburg-Essen, Essen, Germany; Distributed Systems Group, TU Wien, Vienna, Austria; Paluno—The Ruhr Institute for Software Technology, University of Duisburg-Essen, Essen, Germany",IEEE Access,"4 Jul 2022","2022","10","","68669","68685","The design, development, deployment, and operation of a distributed Video Analytics Pipeline (VAP) at the edge of the network is highly complex. In the domain of adaptive systems, several solutions are proposed in literature to optimize either one particular performance aspect of a VAP, e.g., execution time or latency, or focus on minimal energy consumption, or calculate a trade-off including some of those aspects. However, nowadays, most systems utilizing a VAP that records personally identifiable data have to adhere to some form of data protection regulation, such as the GDPR. Still, adaptations to increase data protection requirements are often second to previously mentioned performance or energy consumption characteristics of a VAP. While there is state of the art literature dealing with data protection related adaptations, most of them solely focus on increasing certain security or privacy aspects of a system, leaving previously mentioned performance or energy consumption characteristics out of scope. To the best of our knowledge, there is no solution that covers all of these aspects. In this paper, we present a data protection focused adaptation engine that leverages the application- and infrastructure based adaptation space of a distributed VAP. The engine employs an extended system model and adaptation rules that are based on previous research. It features an optimization algorithm to improve data protection, performance and energy consumption characteristics of a distributed VAP.","2169-3536","","10.1109/ACCESS.2022.3185990","European Union’s Horizon 2020 Research and Innovation Program (FogProtect)(grant numbers:871525); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9805737","Adaptive systems;optimization;video analytics pipelines;data protection;edge computing","Data protection;Energy consumption;Task analysis;Pipelines;Visual analytics;Security;Engines","","1","","39","CCBY","24 Jun 2022","","","IEEE","IEEE Journals"
"Survivability Analysis of IoT Systems Under Resource Exhausting Attacks","R. Pietrantuono; M. Ficco; F. Palmieri","Department of Electrical Engineering and Information Technology, University of Napoli Federico II, Napoli, Italy; Department of Computer Science, University of Salerno, Fisciano, Italy; Department of Computer Science, University of Salerno, Fisciano, Italy",IEEE Transactions on Information Forensics and Security,"5 Jun 2023","2023","18","","3277","3288","Essential services in an Internet of Things (IoT)-based critical system should be continuously provided even when undesirable events like failures, attacks, and emergencies happen. In this work, we analyze the system’s ability to survive failures that are caused by resource exhaustion attacks. Such ability to survive means that the system’s services should be provided in compliance with the associated requirements also in presence of failures and other undesired events. Accordingly, we present a hybrid method (i.e., measurements- and model-based) to assess the expected survivability of an IoT system under resource-exhaustion attacks and, based on it, to optimize the preventive maintenance trigger period that maximizes survivability and minimizes the expected downtime cost. A realistic case study is implemented to emulate an IoT scenario and used to estimate the extent of resource consumption at each layer of the IoT stack when the system is subject to a resource-exhaustion attack. A semi-Markov process is then adopted to model the transient behavior of the system during an intrusion. The model is enriched with an additional state that represents a proactive recovery, in which the system is not available for a maintenance action aimed at preventing failure. The model solution gives the optimal maintenance triggering time.","1556-6021","","10.1109/TIFS.2023.3278449","SERICS through the NRRP MUR Program funded by the EU-NGEU(grant numbers:PE00000014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10130289","Survivability;recovery;Denial of Service;Internet of Things;Markov model","Internet of Things;Sensors;Wireless sensor networks;Logic gates;Degradation;Wireless communication;Communication system security","","1","","52","IEEE","22 May 2023","","","IEEE","IEEE Journals"
"Graph Neural Network Aided Deep Reinforcement Learning for Microservice Deployment in Cooperative Edge Computing","S. Chen; Q. Yuan; J. Li; H. He; S. Li; X. Jiang; J. Yang","University of Science and Technology of China, Hefei, Anhui, China; University of Science and Technology of China, Hefei, Anhui, China; University of Science and Technology of China, Hefei, Anhui, China; University of Science and Technology of China, Hefei, Anhui, China; Guangxi Key Laboratory of Digital Infrastructure, Nanning, Guangxi, China; University of Science and Technology of China, Hefei, Anhui, China; University of Science and Technology of China, Hefei, Anhui, China",IEEE Transactions on Services Computing,"31 Dec 2024","2024","17","6","3742","3757","Deploying microservices on the cooperative edge computing system greatly shortens the interaction delay between users and service and alleviates the traffic burden on the backbones, which has emerged as a new paradigm for service provision. However, it is challenging to embed microservices, having diverse resource demands and heterogeneous invocation relationships, into a distributed edge computing system with irregular network topology. In order to characterize the invocation relationship, we conceive a graph attention network based model to capture the structural features of microservices. Similarly, we propose a multi-channel directed graph convolutional network model to capture the spatial dynamic of edge resources distribution, which jointly considers the heterogeneity of the edge nodes and the links between them. Then, we develop a sequence-to-sequence based multi-step decision model, which maps the feature sequence of the current state to a sequence of deployment actions. Using this model, we further propose a microservice deployment algorithm based on graph neural network aided deep reinforcement learning, where a parallel asynchronous training process is used to accelerate convergence. The performance evaluation shows that the proposed algorithm can improve the deployment success ratio and resource utilization, while ensuring the load balance of edge nodes.","1939-1374","","10.1109/TSC.2024.3417241","National Natural Science Foundation of China(grant numbers:62341113,62101525,U23A20275); China Environment for Network Innovations (CENI)(grant numbers:2016-000052-73-01-000515); Guangxi Key Laboratory of Digital Infrastructure(grant numbers:GXDIOP2023001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10568381","Microservice deployment;collaborative edge computing;deep reinforcement learning;graph neural network","Microservice architectures;Edge computing;Computational modeling;Graph neural networks;Servers;Heuristic algorithms;Feature extraction","","1","","35","IEEE","21 Jun 2024","","","IEEE","IEEE Journals"
"Networked Metaverse Systems: Foundations, Gaps, Research Directions","Y. Zhang; D. Kutscher; Y. Cui","The Hong Kong University of Science and Technology, Guangzhou, China; The Hong Kong University of Science and Technology, Guangzhou, China; The Hong Kong University of Science and Technology, Guangzhou, China",IEEE Open Journal of the Communications Society,"10 Sep 2024","2024","5","","5488","5539","This article discusses ‘Metaverse’ from a technical perspective, focusing on networked systems aspects. Based on a technical definition of the ‘Metaverse,’ we examine the current state and challenges in communication and networking within Metaverse systems. We describe the state-of-the-art in different enabling Metaverse technologies and provide a technical analysis of current Metaverse system architectures. We then detail the state-of-the-art and the gaps in four areas: communication performance, mobility, large-scale operation, and end system architecture. Based on our analysis, we formulate a vision for future Metaverse infrastructure, outlining goals, design concepts, and suggested research directions.","2644-125X","","10.1109/OJCOMS.2024.3426098","Guangzhou Municipal Key Laboratory on Future Networked Systems(grant numbers:024A03J0623); National Natural Science Foundation of China(grant numbers:62371412); National Key Research and Development Program of China(grant numbers:2024YFE0200600); Guangdong Basic and Applied Basic Research Natural Science Funding Scheme(grant numbers:2024A1515011184); Guangzhou-HKUST (GZ) Joint Funding Scheme(grant numbers:2024A03J0539); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10592027","Metaverse;networking;Internet;information-centric networking","Metaverse;Surveys;Blockchains;Artificial intelligence;Computer architecture;Games;Focusing","","1","","431","CCBY","10 Jul 2024","","","IEEE","IEEE Journals"
"A Decentralized Solution for Epidemiological Surveillance in Campus Scenarios","A. Fornaia; G. Marotta; G. Pappalardo; E. Tramontana","Dipartimento di Matematica e Informatica, University of Catania, Catania, Italy; Dipartimento di Matematica e Informatica, University of Catania, Catania, Italy; Dipartimento di Matematica e Informatica, University of Catania, Catania, Italy; Dipartimento di Matematica e Informatica, University of Catania, Catania, Italy",IEEE Access,"6 Oct 2022","2022","10","","103806","103818","Throughout the various containment phases of a pandemic, such as Covid-19, digital tools and services have proven to be essential measures to counteract the ensuing disrupting effects in social and working interactions. In such scenarios, Nausica@DApp, the comprehensive solution proposed in this paper, eases compatibility of the in-presence activities of a campus-based corporation with the organizational constraints posed by the virus during the pandemic, or at a later endemic stage. This is accomplished throughout several intervention areas, such as personnel contact tracing, crowd gathering surveillance, and epidemiological monitoring. These operational requirements, in particular indirect contact tracing and overcrowd monitoring, call for the adoption of an absolute device localization paradigm, which, in the proposed solution, has been devised on top of the campus WiFi infrastructure, proving to be encouragingly accurate in most cases. Absolute localization, on the other hand, entails a certain amount of server-based centralized operations, which might affect the preservation of user data privacy. The novelty of the proposed solution consists in maximizing confidentiality and integrity in the handling of sensitive personal information, in spite of the centralized aspects of the localization system. This is accomplished by decentralizing contact tracing matching operations, which are entirely carried out locally, by apps running on the users’ mobile devices. Contact data are pseudonymized and their authenticity is guaranteed by a blockchain. Furthermore, the proposed novel solution improves privacy preservation by eschewing recourse to the Bluetooth app-to-app channel for user data exchange, in fact a typical choice of most current contract tracing solutions. Thanks to a sensible use of the blockchain features, integrated into Nausica@DApp’s microservice-based back-end, a higher degree of operation transparency can be relied upon, thus boosting the user’s level of trust and enhancing the availability and reliability of data about people gathering within the campus premises. Moreover, contact tracing only requires the mobile device WiFi interface to be on, so that users are neither forced to adopt new habits, nor to grant additional device access permissions to contact tracing apps (potentially undermining their own privacy). The overall system has been analysed in terms of performance and costs, and the experiments have shown that its adoption is viable and effective.","2169-3536","","10.1109/ACCESS.2022.3208167","Project TEAMS—TEchniques to Support the Analysis of Big Data in Medicine, Energy and Structures—Piano di incentivi per la ricerca di Ateneo 2020/2022; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9895418","Blockchain;contact tracing;data analysis;decentralized apps;distributed systems;epidemiological surveillance;localization;mobile applications;smart contracts","Blockchains;Smart contracts;Peer-to-peer computing;Surveillance;Location awareness;Pandemics;Data privacy;Data analysis;Epidemiology;Smart contracts","","1","","30","CCBY","20 Sep 2022","","","IEEE","IEEE Journals"
"Energy Consumption in Microservices Architectures: A Systematic Literature Review","G. Araújo; V. Barbosa; L. N. Lima; A. Sabino; C. Brito; I. Fé; P. Rego; E. Choi; D. Min; T. A. Nguyen; F. A. Silva","PASID Laboratory, Federal University of Piauí (UFPI), Picos, Piauí, Brazil; PASID Laboratory, Federal University of Piauí (UFPI), Picos, Piauí, Brazil; PASID Laboratory, Federal University of Piauí (UFPI), Picos, Piauí, Brazil; PASID Laboratory, Federal University of Piauí (UFPI), Picos, Piauí, Brazil; PASID Laboratory, Federal University of Piauí (UFPI), Picos, Piauí, Brazil; PASID Laboratory, Federal University of Piauí (UFPI), Picos, Piauí, Brazil; Federal University of Ceará, Fortaleza, Brazil; School of Software, College of Computer Science, Kookmin University, Seoul, South Korea; Department of Computer Science and Engineering, College of Engineering, Konkuk University, Seoul, South Korea; Konkuk Aerospace Design-Airworthiness Institute (KADA), Konkuk University, Seoul, South Korea; PASID Laboratory, Federal University of Piauí (UFPI), Picos, Piauí, Brazil",IEEE Access,"17 Dec 2024","2024","12","","186710","186729","Cloud computing emerges as a paradigm that facilitates on-demand access to technological resources through the mechanism of service virtualization. This virtualization enables the partitioning of hardware resources among applications that are organized into distinct independent modules. The concept of microservice architecture takes advantage of virtualization capabilities to embrace a software architecture strategy focused on the development of applications as assemblies of several interdependent but loosely coupled modules. Nonetheless, the adoption of microservices architecture is accompanied by substantial energy demands to meet the desired standards of performance and availability. Existing research within the domain of microservices has explored various topics pertinent to energy consumption, including elasticity, reliability, performance, and availability. Yet, the diversity of challenges and solutions presents a complex landscape for identifying prevailing research trends and unaddressed gaps in the context of microservices. This study aims to methodically discern, evaluate, and juxtapose the existing research trends and voids concerning energy consumption within microservices. It elucidates a systematic review on the subject of energy consumption in microservices architectures, offering a compilation of references to facilitate more directed future investigations. The initial selection encompassed 3625 articles, which were subsequently narrowed down through three stages of refinement, resulting in 37 articles chosen for an exhaustive review. These selected studies were cataloged and analyzed based on various criteria, including metrics, evaluation methodologies, and architectural typologies, thus uncovering research gaps and emerging trends related to energy consumption in microservice architectures. Furthermore, this inquiry delineates significant research challenges and prospective directions, structured around the key metrics that underpin the reviewed studies: performance, elasticity, scalability, reliability, sustainability, and availability.","2169-3536","","10.1109/ACCESS.2024.3389064","Basic Science Research Program through the National Research Foundation of Korea (NRF); Ministry of Education(grant numbers:2020R1A6A1A03046811,2021R1A2C2094943); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10500420","Microservices;systematic mapping;energy consumption;cloud computing;container","Microservice architectures;Systematics;Computer architecture;Cloud computing;Reviews;Energy consumption;Security","","1","","66","CCBYNCND","15 Apr 2024","","","IEEE","IEEE Journals"
"Optimizing Cloud Performance: A Microservice Scheduling Strategy for Enhanced Fault-Tolerance, Reduced Network Traffic, and Lower Latency","A. Alelyani; A. Datta; G. M. Hassan","Department of Computer Science and Software Engineering, The University of Western Australia, Perth, WA, Australia; Department of Computer Science and Software Engineering, The University of Western Australia, Perth, WA, Australia; Department of Computer Science and Software Engineering, The University of Western Australia, Perth, WA, Australia",IEEE Access,"8 Mar 2024","2024","12","","35135","35153","The emergence of microservice architecture has brought significant advancements in software development, offering improved scalability and availability of applications. Cloud computing benefits from microservice architecture by mitigating the risks of single failures and ensuring compliance with service-level agreements. However, using microservice architecture presents two challenges: 1) managing network traffic, which leads to latency and network congestion; and 2) inefficient resource allocation for microservices. Current approaches have limitations in addressing these challenges. To overcome these limitations, we propose a novel scheduling strategy that schedules microservice replicas using a modified particle swarm optimization algorithm to place them on the most suitable physical machine. Additionally, we balance the load across physical machines in the cluster using a simple round-robin algorithm. Furthermore, our scheduling strategy integrates with Kubernetes to tackle resource allocation and deployment challenges. The proposed strategy has been evaluated by simulating two scenarios using Alibaba and Google datasets. The experimental results demonstrate the effectiveness of our strategy in reducing traffic, balancing load, and utilizing CPU and memory efficiently.","2169-3536","","10.1109/ACCESS.2024.3373316","University of Western Australia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10459012","Microservice;network traffic cost;PSO;resource utilization;Alibaba;VM;container;scheduling strategy","Microservice architectures;Cloud computing;Telecommunication traffic;Resource management;Costs;Containers;Task analysis;Telecommunication network management;Scheduling;Software development management;Performance evaluation;Fault tolerance","","1","","54","CCBY","4 Mar 2024","","","IEEE","IEEE Journals"
"Architecture of an Artificial Intelligence Model Manager for Event-Driven Component-Based SCADA Systems","Z. Sičanica; S. Sučić; B. Milašinović","Končar—Digital, Zagreb, Croatia; Končar—Digital, Zagreb, Croatia; Faculty of Electrical Engineering and Computing, University of Zagreb, Zagreb, Croatia",IEEE Access,"23 Mar 2022","2022","10","","30414","30426","This paper analyzes Hat, an open-source framework for developing event-driven component-based SCADA applications, and discusses possibilities to add various analytical tools to such platforms. As a part of the contribution, an open-source component called Artificial Intelligence Model Manager (AIMM) has been developed and integrated into a Hat-based SCADA platform. AIMM is extensible through various plugins, allowing the addition of various models for advanced analytics e.g., machine learning tools, statistical tools, etc. The paper describes AIMM architecture and provides a use case in which state estimation was performed in a medium-voltage distribution grid. This case study demonstrates that it is possible to extend component-based SCADA systems with components for advanced analytics with minimal fundamental system changes.","2169-3536","","10.1109/ACCESS.2022.3159715","Advanced Tools Towards Cost-efficient Decarbonisation of Future Reliable Energy Systems (ATTEST) Project through the European Union’s Horizon 2020 Research and Innovation Program(grant numbers:864298); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9734070","Artificial intelligence;power system analysis computing;SCADA systems;software architecture","SCADA systems;Computer architecture;State estimation;Neural networks;Security;Analytical models;Monitoring","","1","","48","CCBY","14 Mar 2022","","","IEEE","IEEE Journals"
"CMXsafe: A Proxy Layer for Securing Internet-of-Things Communications","J. D. de Hoz Diego; T. Madi; C. Konstantinou","Computer, Electrical and Mathematical Science and Engineering Division, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia; Computer, Electrical and Mathematical Science and Engineering Division, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia; Computer, Electrical and Mathematical Science and Engineering Division, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia",IEEE Transactions on Information Forensics and Security,"29 May 2024","2024","19","","5767","5782","Security in Internet-of-Things (IoT) environments has become a major concern. This is partly due to a large number of remotely exploitable IoT vulnerabilities in service authentication and access control combined with the lack of timely technical support. To reduce the threat surface of remote vulnerability exploitation, we propose CMXsafe, a secure-by-design application-agnostic proxy layer that can be updated and managed independently of the IoT device application. CMXsafe places IoT devices behind gateways operating as 4th OSI transport layer relayers to offload security concerns of IoT network communications into the proxy layer. Specifically, the proxy layer produces secure communication paths between IoT applications and platforms while enforcing mutual authentication and access control to proxied services. We evaluate the performance of our architecture on the MQTT protocol used in a standard publisher-broker-subscriber configuration provided by Eclipse Mosquitto. We compare the performance penalty on the protocol when securing communications with TLS following a monolithic implementation and with CMXsafe. The experimental results suggest that CMXsafe outperforms integrated security by providing at least a 25% latency reduction and a 22% bandwidth improvement.","1556-6021","","10.1109/TIFS.2024.3404258","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10536903","Internet-of-Things;secure communications;socket proxy;secure proxy session;security context","Internet of Things;Security;Servers;Protocols;Sockets;Authentication;Microservice architectures","","1","","95","CCBY","22 May 2024","","","IEEE","IEEE Journals"
"Challenges of CPS/IoT Network Architecture in 6G Era","T. Kato; N. Fukumoto; C. Sasaki; A. Tagami; A. Nakao","KDDI Research, Inc., Saitama, Japan; KDDI Research, Inc., Saitama, Japan; KDDI Research, Inc., Saitama, Japan; KDDI Research, Inc., Saitama, Japan; Graduate School of Engineering, The University of Tokyo, Tokyo, Japan",IEEE Access,"7 May 2024","2024","12","","62804","62817","Cyber Physical System (CPS) and Internet of Things (IoT) have attracted attention as a promising technology for the future society. Although the 6th generation mobile system is expected to work as the CPS/IoT platform, the specific technical requirements are not studied thoroughly. In this paper, we take application driven approach focusing on three applications, eHealth, smart manufacturing, and smart city, to identify mandatory technical requirements for the 6G mobile core network architecture to realize CPS/IoT systems. Specifically, this paper gives the following three contributions. First, we investigate several use cases in the applications listed above and list up the requirements for the core network, i.e., the massive connectivity, the flexible SLA, the ambient IoT device accommodation, the wireless sensing function, and the AI-aware data sharing. Second, we identify the issues to satisfy those requirements in the current 5G technologies. Lastly, we pick up the 6G architectures proposed currently and discuss their pros and cons based on the requirements and issues pointed out.","2169-3536","","10.1109/ACCESS.2024.3395363","University of Tokyo; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10516287","6th generation communications system;cyber physical system;Internet of Things","6G mobile communication;Network architecture;Smart cities;Sensors;5G mobile communication;Middleware;Internet of Things;Cyber-physical systems","","1","","71","CCBYNCND","30 Apr 2024","","","IEEE","IEEE Journals"
"Vehicular Fog Resource Allocation Approach for VANETs Based on Deep Adaptive Reinforcement Learning Combined With Heuristic Information","Y. Cheng; A. Vijayaraj; K. Sree Pokkuluri; T. Salehnia; A. Montazerolghaem; R. Rateb","School of Information, Guangdong Polytechnic of Science and Trade, Guangzhou, Guangdong, China; Department of Information Technology, R.M.K. Engineering College, Chennai, Tamil Nadu, India; Department of Computer Science and Engineering, Shri Vishnu Engineering College for Women, Bhimavaram, India; Department of Computer Engineering and Information Technology, Razi University, Kermanshah, Iran; Faculty of Computer Engineering, University of Isfahan, Isfahan, Iran; Department of Computer Science, Faculty of Information Technology, Al-Ahliyya Amman University, Amman, Jordan",IEEE Access,"1 Oct 2024","2024","12","","139056","139075","Intelligent Transport Systems (ITS) are gradually progressing to practical application because of the rapid growth in network and information technology. Currently, the low-latency ITS requirements are hard to achieve in the conventional cloud-based Internet of Vehicles (IoV) infrastructure. In the context of IoV, Vehicular Fog Computing (VFC) has become recognized as an inventive and viable architecture that can effectively decrease the time required for the computation of diverse vehicular application activities. Vehicles receive rapid task execution services from VFC. The benefits of fog computing and vehicular cloud computing are combined in a novel concept called fog-based Vehicular Ad Hoc Networks (VANETs). These networks depend on a movable power source, so they have specific limitations. Cost-effective routing and load distribution in VANETs provide additional difficulties. In this work, a novel method is developed in vehicular applications to solve the difficulty of allocating limited fog resources and minimizing the service latency by using parked vehicles. Here, the improved heuristic algorithm called Revised Fitness-based Binary Battle Royale Optimizer (RF-BinBRO) is proposed to solve the problems of vehicular networks effectively. Additionally, the combination of Deep Adaptive Reinforcement Learning (DARL) and the improved BinBRO algorithm effectively analyzes resource allocation, vehicle parking, and movement status. Here, the parameters are tuned using the RF-BinBRO to achieve better transportation performance. To assess the performance of the proposed algorithm, simulations are carried out. The results defined that the developed VFC resource allocation model attains maximum service satisfaction compared to the traditional methods for resource allocation.","2169-3536","","10.1109/ACCESS.2024.3455168","Razi University of Kermanshah; Al-Ahliyya Amman University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10666661","Vehicular fog resource allocation;vehicular ad hoc networks;revised fitness-based binary battle royale optimizer;deep adaptive reinforcement learning;reward assessment;service satisfaction;service latency","Resource management;Vehicular ad hoc networks;Computational modeling;Cloud computing;Edge computing;Computer architecture;Optimization methods;Reinforcement learning","","1","","37","CCBYNCND","5 Sep 2024","","","IEEE","IEEE Journals"
"Artificial Intelligence Techniques for Securing Fog Computing Environments: Trends, Challenges, and Future Directions","D. Alsadie","Department of Computer Science and Artificial Intelligence, College of Computing, Umm Al-Qura University, Makkah, Saudi Arabia",IEEE Access,"22 Oct 2024","2024","12","","151598","151648","Fog computing, an extension of cloud computing, enhances capabilities by processing data closer to the source, thereby addressing latency and bandwidth issues inherent in traditional cloud models. However, the integration of Artificial Intelligence (AI) into fog computing introduces challenges, particularly in resource management, security, and privacy. This paper systematically reviews AI applications within fog computing environments, adhering to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology to ensure rigorous analysis. The studies were selected based on predefined inclusion criteria, including research published between 2010 and 2024 in peer-reviewed journals and conference papers, with searches conducted in databases like IEEE Xplore, ACM Digital Library, SpringerLink, and Scopus. The review identifies critical issues such as resource constraints, transparency in AI-driven security systems, and the need for adaptable AI models to address evolving security threats. In response, innovative solutions such as lightweight AI models (e.g., Pruned Neural Networks, Quantized Models, Knowledge Distillation), Explainable AI (XAI) (e.g., Model-Agnostic Methods, Feature Importance Analysis, Rule-Based Approaches), and federated learning are proposed. Additionally, a novel taxonomy is introduced, categorizing AI techniques into resource management, security enhancement, and privacy-preserving methods, offering a structured framework for researchers and practitioners. The paper concludes that effective AI integration in fog computing is essential for developing secure, efficient, and adaptable distributed systems, with significant implications for both academia and industry.","2169-3536","","10.1109/ACCESS.2024.3463791","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10684173","Fog computing;artificial intelligence;resource management;explainable AI;privacy and security","Artificial intelligence;Security;Edge computing;Resource management;Data models;Computational modeling;Authentication;Explainable AI;Privacy","","1","","153","CCBYNCND","19 Sep 2024","","","IEEE","IEEE Journals"
"A Security Services Management Architecture Toward Resilient 6G Wireless and Computing Ecosystems","E. Rodriguez; X. Masip-Bruin; J. Martrat; R. Diaz; A. Jukan; F. Granelli; P. Trakadas; G. Xilouris","Advanced Network Architectures Laboratory (CRAAX), Universitat Politècnica de Catalunya (UPC), Vilanova i la Geltru, Barcelona, Spain; Advanced Network Architectures Laboratory (CRAAX), Universitat Politècnica de Catalunya (UPC), Vilanova i la Geltru, Barcelona, Spain; Eviden Research and Innovation, Barcelona, Spain; Eviden Research and Innovation, Barcelona, Spain; Department of Communication Networks, Technische Universität Braunschweig, Braunschweig, Germany; CNIT, University of Trento, Trento, Italy; Department of Port Management and Shipping, National and Kapodistrian University of Athens, Athens, Greece; Department of Port Management and Shipping, National and Kapodistrian University of Athens, Athens, Greece",IEEE Access,"22 Jul 2024","2024","12","","98046","98058","The upcoming sixth-generation (6G) networks are expected to be more heterogeneous, scalable, reliable, secure and energy-efficient. 6G services and applications will benefit from the fast data transmission rates, global coverage, precise positioning, and widespread intelligence capabilities provided by 6G technology. In this complex scenario, new attack surfaces and vectors will emerge, due to the distributed, disaggregated, dynamic, programmable and open nature of the entire end-to-end 6G infrastructure along with the fragmentation of data, as well as the need for supporting cross-platforms interoperability, making the application of security and trust fundamentally challenging. To securely manage services in the future 6G mobile networks along with the set of functions ruling its management, conceptual work is needed to defining functional blocks supporting a secure end-to-end 6G systems management. This is especially critical in handling specific functionalities, such as network disaggregation, risk and threats business impact, energy-efficiency, self-configuration or dynamic discovery. This paper positions an innovative 6G security services management architecture, which builds on a set of innovative building blocks, leveraging key technologies, such as AI-assisted models and Digital Twin, while enabling a human-centric approach toward an end-to-end security solution. Two canonical workflows along with two illustrative application scenarios are proposed, Light Rail Transit and Extended Reality, to conceptually highlight the proposed architecture impact and its expected benefits when high bandwidth, ultra-low latency, and secure communications are required.","2169-3536","","10.1109/ACCESS.2024.3427661","European union’s Horizon Europe, under the project HORSE(grant numbers:101096342); Ministerio de Ciencia e Innovación(grant numbers:MCIN/AEI/10.13039/501100011033); ERDF: A way of making Europe(grant numbers:PID2021-124463OB-I00); Catalan Government(grant numbers:2021 SGR 00326); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10597424","6G;communication systems;wireless communication;network architecture;security","6G mobile communication;Security;Prevention and mitigation;Resilience;Privacy;5G mobile communication;Wireless communication;Network architecture","","1","","35","CCBYNCND","15 Jul 2024","","","IEEE","IEEE Journals"
"Efficient Microservice Deployment in the Edge-Cloud Networks With Policy-Gradient Reinforcement Learning","K. Afachao; A. M. Abu-Mahfouz; G. P. Hanke","Department of Electrical, Electronic and Computer Engineering, University of Pretoria, Pretoria, South Africa; Department of Electrical, Electronic and Computer Engineering, University of Pretoria, Pretoria, South Africa; Department of Computer Science, City University of Hong Kong, Hong Kong, SAR",IEEE Access,"30 Sep 2024","2024","12","","133110","133124","The rise of user-centric design demands ubiquitous access to infrastructure and applications, facilitated by the Edge-Cloud network and microservices. However, efficiently managing resource allocation while orchestrating microservice placement in such dynamic environments presents a significant challenge. These challenges stem from the limited resources of edge devices, the need for low latency responses, and the potential for performance degradation due to service failures or inefficient deployments. This paper addresses the challenge of microservice placement in Edge-Cloud environments by proposing a novel Reinforcement Learning algorithm called Bi-Generic Advantage Actor-Critic for Microservice Placement Policy. This algorithm’s ability to learn and adapt to the dynamic environment makes it well-suited for optimizing resource allocation and service placement decisions within the Edge-Cloud. We compare this algorithm against three baseline algorithms through simulations on a real-world dataset, evaluating performance metrics such as execution time, network usage, average migration delay, and energy consumption. The results demonstrate the superiority of the proposed method, with an 8% reduction in execution time, translating to faster response times for users. Additionally, it achieves a 4% decrease in network usage and a 2% decrease in energy consumption compared to the best-performing baseline. This research contributes by reproducing the Edge-Cloud environment, applying the novel Bi-Generic Advantage Actor-Critic technique, and demonstrating significant improvements over the state-of-the-art baseline algorithms in microservice placement and resource management within Edge-Cloud environments.","2169-3536","","10.1109/ACCESS.2024.3461149","Telkom, South Africa; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10680533","Edge computing;microservices;network optimization;online placement;scheduling algorithms;reinforcement learning","Microservice architectures;Heuristic algorithms;Reliability;Optimization methods;Resource management;Containers;Edge computing;Scheduling;User centered design;Reinforcement learning","","1","","32","CCBYNCND","16 Sep 2024","","","IEEE","IEEE Journals"
"A Novel Variability-Rich Scheme for Software Updates of Automotive Systems","A. Bazzi; A. Shaout; D. Ma","Department of Electrical and Computer Engineering, University of Michigan-Dearborn, Dearborn, MI, USA; Department of Electrical and Computer Engineering, University of Michigan-Dearborn, Dearborn, MI, USA; Department of Computer and Information Science, University of Michigan-Dearborn, Dearborn, MI, USA",IEEE Access,"10 Jun 2024","2024","12","","79530","79548","The automotive industry is experiencing a significant evolution from traditional hardware-defined to software-defined architecture, enabling higher levels of autonomy, connectivity, safety, and richer in-vehicle experiences. A service oriented architecture is essential for realizing Software-Defined Vehicles (SDVs) and fostering new business models for OEMs. However, this architecture evolution requires new development paradigms to address the increasing complexity of software, which is essential for seamless software development, integration, and deployment from cloud or backend repositories to the vehicle. Given the complexity of vehicular software updates, particularly when dealing with highly distributed embedded ECUs, a software-centric approach is more efficient and suitable to cover different architectures and configurations, ensuring consistency across all platforms. Therefore, we propose a variability-rich scheme for software updates based on a Merkle tree approach that can cope with the complexity of the new software architecture while addressing the safety and security requirements of real-time and resource-constrained embedded systems in the vehicle. The technical analysis and experiments conducted in this paper demonstrate how the proposed scheme, which combines a digital signature and a Merkle hash tree, achieves synergistic authentication and verification of multiple software variants. Our approach offers OEMs the ability to sign a software module once and verify it across multiple ECU variants, providing a more efficient alternative to the traditional method of creating a software update package for each variant. This approach not only adds flexibility to software updates and reduces the complexity of software variant management, but also maintains the security of the vehicle, ensuring that there is no compromise in the safety of the passengers.","2169-3536","","10.1109/ACCESS.2024.3409629","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10547264","Cyber physical systems;digital signatures;distributed software development;Merkle tree;safety-critical systems;service oriented architecture;software over the air updates;software-defined vehicle;variant management","Software;Computer architecture;Automotive engineering;Complexity theory;Industries;Cloud computing;Software algorithms;Cyber-physical systems;Software development management;Service-oriented architecture","","1","","24","CCBYNCND","4 Jun 2024","","","IEEE","IEEE Journals"
"Optimized Encryption-Integrated Strategy for Containers Scheduling and Secure Migration in Multi-Cloud Data Centers","M. A. Altahat; T. Daradkeh; A. Agarwal","Department of Electrical and Computer Engineering, Concordia University, Montreal, QC, Canada; Department of Computer Science, Yarmouk University, Irbid, Jordan; Department of Electrical and Computer Engineering, Concordia University, Montreal, QC, Canada",IEEE Access,"15 Apr 2024","2024","12","","51330","51345","Containers are recognized for their lightweight and virtualization efficiency, making them a vital element in modern application orchestration. In this context, the scheduler is crucial in strategically distributing containers across diverse computing nodes. This paper presents a novel two-stage container scheduling solution that addresses node imbalances and efficiently deploys containers. The proposed solution formulates the scheduling process as an optimization problem, integrating various objective functions and constraints to enhance server consolidation and minimize energy consumption. The confidentiality of migrated containers is ensured through encryption, and the associated costs are incorporated into the optimization constraints. This approach ensures security in container scheduling, considering container attributes as input features in our proposed attributes-based encryption model. By carefully selecting containers and destination nodes, this work seeks to establish balance within cloud-based clusters. This contributes to the improvement of container orchestration systems and their effectiveness in real-world scenarios. The proposed solution’s efficacy is demonstrated in its ability to efficiently deploy containers in multi-data center cloud environments and seamlessly migrate them between hosts within the same data center or across different data centers. Our results show optimal consolidation with a reduction in the number of running hosts, ranging from 4% to over 18%. Additionally, the solution promotes minimal total power consumption with savings ranging from 3.5 to 16.25 megawatts, while also ensuring balanced server loads.","2169-3536","","10.1109/ACCESS.2024.3386169","Natural Sciences and Engineering Research Council (NSERC)’s Discovery Grant, Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10494738","Containers;virtualization;scheduling;placement;optimization;encryption","Containers;Encryption;Cloud computing;Security;Cryptography;Servers;Optimization;Scheduling;Optimization methods","","1","","41","CCBYNCND","8 Apr 2024","","","IEEE","IEEE Journals"
"SIMA: Scalable IoT Microservice Architecture With Users and Things Joint Load Balancing","S. -C. Son; S. -K. Ko; H. O. Lee; S. Lee; B. -T. Lee","Honam Research Center, Electronics and Telecommunications Research Institute, Gwangju, Republic of Korea; Honam Research Center, Electronics and Telecommunications Research Institute, Gwangju, Republic of Korea; Honam Research Center, Electronics and Telecommunications Research Institute, Gwangju, Republic of Korea; Honam Research Center, Electronics and Telecommunications Research Institute, Gwangju, Republic of Korea; Honam Research Center, Electronics and Telecommunications Research Institute, Gwangju, Republic of Korea",IEEE Internet of Things Journal,"21 Feb 2024","2024","11","5","8170","8184","The Internet of Things (IoT) offers advantageous applications through interfacing between people and objects. Therefore, IoT service administrators should be able to partially manage both, as well as apply load-balancing policies jointly, according to their scale. Microservices (MSs) architecture is a futuristic and appropriate software development methodology for partial management. We break away from IoT server systems in monolithic architectures and focus on designing a scalable IoT MS architecture (SIMA) with user and object joint load balancing. The design also includes message exchange procedures for the initialization, discovery, and failure of MS replicas. Finally, we present the experimental results of the limitations of the monolithic platform and the maximum performance of the SIMA.","2327-4662","","10.1109/JIOT.2023.3317430","Electronics and Telecommunications Research Institute (ETRI); Korean Government (Honam Region Regional Industry-Based ICT Convergence Technology Advancement Support Project)(grant numbers:23ZK1100); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10256032","Internet of Things (IoT);IoT server platform;lightweight machine to machine (LwM2M);microservice architecture (MSA)","Internet of Things;Microservice architectures;Scalability;Protocols;Standards;Servers;Cloud computing","","1","","35","IEEE","20 Sep 2023","","","IEEE","IEEE Journals"
"A Systematic Literature Review of IoT System Architectural Styles and Their Quality Requirements","N. Khezemi; J. Baptiste Minani; F. Sabir; N. Moha; Y. -G. Guéhéneuc; G. El Boussaidi","Département de génie logiciel et des TI, École de technologie supérieure, Montreal, QC, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, QC, Canada; Department of Computer Sciences, University of the Punjab, Lahore, Pakistan; Département de génie logiciel et des TI, École de technologie supérieure, Montreal, QC, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, QC, Canada; Département de génie logiciel et des TI, École de technologie supérieure, Montreal, QC, Canada",IEEE Internet of Things Journal,"20 Nov 2024","2024","11","23","37599","37616","The Internet of Things (IoT) is increasingly prevalent, with systems developed across various domains. Choosing the right IoT architectural style is challenging due to the diversity of devices, dynamic environments, and real-time data needs. This choice significantly impacts system quality, requiring a careful balance of quality requirements and tradeoffs. Previous studies have not adequately identified the most suitable architectural styles for specific IoT quality needs. This study presents a systematic literature review of 103 primary studies (PSs) on IoT system quality requirements and architectural styles, assessing how each architectural style satisfies specific requirements. We followed the preferred reporting items for systematic review and meta-analysis (PRISMA) protocol to report our findings and answer three research questions (RQs). We selected PSs by applying inclusion and exclusion criteria to relevant papers published until the end of 2023. We analyzed data from PSs to understand IoT system quality requirements and architectural styles, assessing their alignment. The research revealed ten essential quality requirements for IoT systems and identified ten distinct architectural styles. Notably, each architectural style varies in its capacity to fulfill specific quality requirements, particularly regarding security, scalability, and performance. SOA, client-server, and REST architectural styles best fulfill many quality requirements. However, various architectural styles, such as Layered, Microservices, and Peer-to-Peer, show limited support for privacy requirements. Our findings can guide IoT systems practitioners in selecting an architectural style that aligns with their desired quality standards. Additionally, we recommend new research opportunities to deepen understanding of key architectural styles based on specific quality requirements.","2327-4662","","10.1109/JIOT.2024.3435496","Natural Sciences and Engineering Research Council of Canada (NSERC); Canada Research Chair program (CRCP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10759538","Architectural style;Internet of Things (IoT);IoT architecture;quality criteria;quality requirements","Privacy;Protocols;Scalability;Microservice architectures;Software;Real-time systems;Peer-to-peer computing;Internet of Things;Security;Standards","","1","","103","IEEE","20 Nov 2024","","","IEEE","IEEE Journals"
"Intelligent Control in 6G Open RAN: Security Risk or Opportunity?","S. Soltani; A. Amanloo; M. Shojafar; R. Tafazolli","5G/6GIC, Institute for Communication Systems, University of Surrey, Guildford, U.K.; 5G/6GIC, Institute for Communication Systems, University of Surrey, Guildford, U.K.; 5G/6GIC, Institute for Communication Systems, University of Surrey, Guildford, U.K.; 5G/6GIC, Institute for Communication Systems, University of Surrey, Guildford, U.K.",IEEE Open Journal of the Communications Society,"29 Jan 2025","2025","6","","840","880","The Open Radio Access Network (Open RAN) framework, emerging as the cornerstone for Artificial Intelligence (AI)-enabled Sixth-Generation (6G) mobile networks, heralds a transformative shift in radio access network architecture. As the adoption of Open RAN accelerates, ensuring its security becomes critical. The RAN Intelligent Controller (RIC) plays a central role in Open RAN by improving network efficiency and flexibility. Nevertheless, it also brings about potential security risks that need careful scrutiny. Therefore, it is imperative to evaluate the current state of RIC security comprehensively. This assessment is essential to gain a profound understanding of the security considerations associated with RIC. This survey combines a comprehensive analysis of RAN security, tracing its evolution from 2G to 5G, with an in-depth exploration of RIC security, marking the first comprehensive examination of its kind in the literature. Real-world security incidents involving RIC are vividly illustrated, providing practical insights. The study evaluates the security implications of the RIC within the 6G Open RAN context, addressing security vulnerabilities, mitigation strategies, and potential enhancements. It aims to guide stakeholders in the telecom industry toward a secure and dependable telecommunications infrastructure. The article serves as a valuable reference, shedding light on the RIC’s crucial role within the broader network infrastructure and emphasizing security’s paramount importance. This survey also explores the promising security opportunities that the RIC presents for enhancing network security and resilience in the context of 6G mobile networks. It outlines open issues, lessons learned, and future research directions in the domain of intelligent control in 6G Open RAN, facilitating a comprehensive understanding of this dynamic landscape.","2644-125X","","10.1109/OJCOMS.2025.3526215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10829659","RAN intelligent controller (RIC) security;open radio access network (Open RAN);6G;RAN security;O-RAN","Security;Open RAN;6G mobile communication;Surveys;Interoperability;Optimization;Protocols;Network security;5G mobile communication;Software","","1","","180","CCBY","6 Jan 2025","","","IEEE","IEEE Journals"
"Learning to Optimize Workflow Scheduling for an Edge–Cloud Computing Environment","K. Zhu; Z. Zhang; S. Zeadally; F. Sun","School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; College of Communication and Information, University of Kentucky, Lexington, KY, USA; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China",IEEE Transactions on Cloud Computing,"5 Sep 2024","2024","12","3","897","912","The widespread deployment of intelligent Internet of Things (IoT) devices brings tighter latency demands on complex workload patterns such as workflows. In such applications, tremendous dataflows are generated and processed in accordance with specific service chains. Edge computing has proven its feasibility in reducing the traffic in the core network and relieving cloud datacenters of fragmented computational demands. However, the efficient scheduling of workflows in hybrid edge–cloud networks is still challenging for the intelligent IoT paradigm. Existing works make dispatching decisions prior to real execution, making it difficult to cope with the dynamicity of the environment. Consequently, the schedulers are affected both by the scheduling strategy and by the mutual impact of dynamic workloads. We design an intelligent workflow scheduler for use in an edge–cloud network where workloads are generated with continuous steady arrivals. We develop new graph neural network (GNN)-based representations for task embedding and we design a proximal policy optimization (PPO)-based online learning scheduler. We further introduce an intrinsic reward to obtain an instantaneous evaluation of the dispatching decision and correct the scheduling policy on-the-fly. Numerical results validate the feasibility of our proposal as it outperforms existing works with an improved quality of service (QoS) level.","2168-7161","","10.1109/TCC.2024.3408006","Fundamental Research Funds for the Central Universities(grant numbers:2022JBZY002); National Natural Science Foundation of China(grant numbers:62173026); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10545338","Workflow scheduling;edge computing;Internet of Things;reinforcement learning","Task analysis;Processor scheduling;Dynamic scheduling;Cloud computing;Internet of Things;Edge computing;Job shop scheduling","","1","","41","IEEE","31 May 2024","","","IEEE","IEEE Journals"
"A Comprehensive Review of AI Techniques for Resource Management in Fog Computing: Trends, Challenges, and Future Directions","D. Alsadie","Department of Computer Science and Artificial Intelligence, College of Computing, Umm Al-Qura University, Makkah, Saudi Arabia",IEEE Access,"29 Aug 2024","2024","12","","118007","118059","Fog computing (FC), extending cloud services to the network edge, has emerged as a key paradigm for low-latency applications like the Internet of Things (IoT). However, efficient resource management, task scheduling, and load balancing pose challenges in fog environments. This review surveys recent research efforts aimed at addressing these challenges and optimizing FC performance. We conducted a systematic analysis of relevant research papers on FC published in reputable academic databases. The review focused on studies published between 2019 and 2024 and emphasized artificial intelligence based studies exploring resource management, task scheduling, and load balancing techniques within the FC domain. The review identifies a diverse range of techniques applied to optimize FC performance. These include machine learning (ML) and deep learning (DL) for resource allocation, heuristic algorithms for task scheduling, and nature-inspired meta-heuristics for load balancing. The review evaluates the strengths and limitations of these approaches, highlighting their impact on metrics like latency, energy consumption, and Quality of Service (QoS). This review demonstrates the significant progress made in optimizing FC through innovative techniques. ML and meta-heuristics have emerged as promising approaches for resource management, task scheduling, and load balancing, respectively. However, challenges persist in areas like real-world implementation complexities and ensuring service quality across geographically distributed fog networks. Future research directions are identified, emphasizing the need for further exploration of these challenges and the integration of emerging technologies like deep reinforcement learning for enhanced FC performance.","2169-3536","","10.1109/ACCESS.2024.3447097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10643044","Fog computing;artificial intelligence;resource management;optimization;challenges","Resource management;Task analysis;Artificial intelligence;Reviews;Load management;Internet of Things;Heuristic algorithms;Edge computing;Optimization methods","","1","","165","CCBYNCND","21 Aug 2024","","","IEEE","IEEE Journals"
"360-ADAPT: An Open-RAN-Based Adaptive Scheme for Quality Enhancement of Opera 360° Content Distribution","A. A. Simiscuka; M. A. Togou; M. Zorrilla; G. -M. Muntean","Insight SFI Centre for Data Analytics and the School of Electronic Engineering, Dublin City University, Dublin 9, Ireland; School of Computing, Dublin City University, Dublin 9, Ireland; Department of Digital Media, Vicomtech, Donostia-San Sebastián, Spain; Insight SFI Centre for Data Analytics and the School of Electronic Engineering, Dublin City University, Dublin 9, Ireland",IEEE Transactions on Green Communications and Networking,"28 Aug 2024","2024","8","3","924","938","There is increasing viewer interest and technological support for streaming immersive clips over the Internet. There are, however, challenges in supporting high quality of viewer experience, mostly due to the large amounts of the data associated with immersive video and spatial audio (Ambisonics). In situations where there are limited network resources, the streamed 360° content needs to be adjusted dynamically to meet the network constraints. Dynamic Adaptive Streaming over HTTP (DASH) adaptation is a key technology for delivering high-quality video over open radio access networks (RANs). DASH allows for efficient adaptation of video streams to the available network conditions. This paper introduces 360-ADAPT, a DASH-based adaptation solution on an Open-RAN architecture for increased quality remote 360° opera experiences. Unlike existing schemes, 360-ADAPT gives precedence to audio over the video when selecting bitrates, increasing the overall quality of the artistic act and improving use of resources and energy. The proposed 360-ADAPT was tested with real opera viewers in the context of an artistic-oriented platform for opera delivery, part of the Horizon2020 TRACTION project. Results indicate that 360-ADAPT achieves higher perceived quality levels than alternative solutions both in QoS and QoE metrics.","2473-2400","","10.1109/TGCN.2024.3418948","European Union Horizon 2020 Research and Innovation, TRACTION Project(grant numbers:870610); Science Foundation Ireland (SFI) through the Frontiers Projects FRADIS(grant numbers:21/FFP-P/10244); INSIGHT SFI Research Centre(grant numbers:12/RC/2289_P2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10571391","Adaptation;QoE;immersive media;360 video;ambisonics;OpenRAN","Streaming media;Media;Bit rate;Quality of experience;Quality of service;Headphones;Ambisonics;Open RAN","","1","","48","CCBY","25 Jun 2024","","","IEEE","IEEE Journals"
"SCROOGEVM: Boosting Cloud Resource Utilization With Dynamic Oversubscription","P. Jacquet; T. Ledoux; R. Rouvoy","Inria, Talence, France; Inria, Villeneuve d'Ascq, France; Inria, Talence, France",IEEE Transactions on Sustainable Computing,"9 Oct 2024","2024","9","5","754","765","Despite continuous improvements, cloud physical resources remain underused, hence severely impacting the efficiency of these infrastructures at large. To overcome this inefficiency, Infrastructure-as-a-Service (IaaS) providers usually compensate for oversized Virtual Machines (VMs) by offering more virtual resources than are physically available on a host. However, this technique—known as oversubscription—may hinder performances when a statically-defined oversubscription ratio results in resource contention of hosted VMs. Therefore, instead of setting a static and cluster-wide ratio, this article studies how a greedy increase of the oversubscription ratio per Physical Machine (PM) and resources type can preserve performance goals. Keeping performance unchanged allows our contribution to be more realistically adopted by production-scale IaaS infrastructures. This contribution, named ScroogeVM, leverages the detection of PM stability to carefully increase the associated oversubscription ratios. Based on metrics shared by public cloud providers, we investigate the impact of resource oversubscription on performance degradation. Subsequently, we conduct a comparative analysis of ScroogeVM with state-of-the-art oversubscription computations. The results demonstrate that our approach outperforms existing methods by leveraging the presence of long-lasting VMs, while avoiding live migration penalties and performance impacts for stakeholders.","2377-3782","","10.1109/TSUSC.2024.3369333","“FrugalCloud” Inria and OVHcloud partnership; Agence Nationale de la Recherche (ANR) under the France 2030 program; CARECloud(grant numbers:ANR-23-PECL-0003); DISTILLER(grant numbers:ANR-21-CE25-0022); SeMaFoR(grant numbers:ANR-20-CE25-0017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10444046","Cloud;IaaS;oversubscription","Cloud computing;Memory management;Resource management;Virtual machine monitors;Green computing;Optimization;Dynamic scheduling","","1","","53","IEEE","23 Feb 2024","","","IEEE","IEEE Journals"
"RLPRAF: Reinforcement Learning-Based Proactive Resource Allocation Framework for Resource Provisioning in Cloud Environment","R. Panwar; M. Supriya","Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Bengaluru, India; Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Bengaluru, India",IEEE Access,"16 Jul 2024","2024","12","","95986","96007","Recent developments in cloud technology enable one to dynamically deploy heterogeneous resources as and when needed. This dynamic nature of the incoming workload causes fluctuations in the cloud environment, which is currently addressed using traditional reactive scaling techniques. Simple reactive approaches affect elastic system performance either by over-provisioning resources which significantly increases the cost, or by under-provisioning, which leads to starvation. Hence automated resource provisioning becomes an effective method to deal with such workload fluctuations. The aforementioned problems can also be resolved by using intelligent resource provisioning techniques by dynamically assigning required resources while adapting to the environment. In this paper, a reinforcement learning-based proactive resource allocation framework (RLPRAF) is proposed. This framework simultaneously learns the environment and distributes the resources. The proposed work presents a paradigm for the optimal allocation of resources by merging the notions of automatic computation, linear regression, and reinforcement learning. When tested with real-time workloads, the proposed RLPRAF method surpasses previous auto-scaling algorithms considering CPU usage, response time, and throughput. Finally, a set of tests demonstrate that the suggested strategy lowers overall expense by 30% and SLA violation by 77.7%. Furthermore, it converges at an optimum timing and demonstrates that it is feasible for a wide range of real-world service-based cloud applications.","2169-3536","","10.1109/ACCESS.2024.3421956","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10579971","Resource allocation;resource provisioning;autonomic computing systems;machine learning;reinforcement learning;virtual machines","Cloud computing;Resource management;Dynamic scheduling;Virtual machining;Costs;Computational modeling;Time factors;Autonomous systems","","1","","51","CCBYNCND","2 Jul 2024","","","IEEE","IEEE Journals"
"Model-Driven Dependability and Power Consumption Quantification of Kubernetes-Based Cloud-Fog Continuum","I. Fé; T. A. Nguyen; A. B. Soares; S. Son; E. Choi; D. Min; J. -W. Lee; F. A. Silva","Laboratory of Applied Research to Distributed Systems (PASID), Federal University of Piauí (UFPI), Picos, Piauí, Brazil; Konkuk Aerospace Design-Airworthiness Research Institute (KADA), Konkuk University, Seoul, South Korea; Distributed Systems and Network Computer Laboratory (DisNeL), Federal University of Piauí (UFPI), Teresina, Piauí, Brazil; Electronics and Telecommunications Research Institute (ETRI), Daejeon, South Korea; School of Software, College of Computer Science, Kookmin University, Seoul, South Korea; Department of Computer Science and Engineering, College of Engineering, Konkuk University, Seoul, South Korea; Department of Aerospace Information Engineering, Konkuk University, Seoul, South Korea; Laboratory of Applied Research to Distributed Systems (PASID), Federal University of Piauí (UFPI), Picos, Piauí, Brazil",IEEE Access,"19 Dec 2023","2023","11","","140826","140852","System dependability is pivotal for the reliable execution of designated computing functions. With the emergence of cloud-fog computing and microservices architectures, new challenges and opportunities arise in evaluating system dependability. Enhancing dependability in microservices often involves component replication, potentially increasing energy costs. Thus, discerning optimal redundancy strategies and understanding their energy implications is crucial for both cost efficiency and ecological sustainability. This paper presents a model-driven approach to evaluate the dependability and energy consumption of cloud-fog systems, utilizing Kubernetes, a container application orchestration platform. The developed model considers various determinants affecting system dependability, including hardware and software reliability, resource accessibility, and support personnel availability. Empirical studies validate the model’s effectiveness, demonstrating a 22.33% increase in system availability with only a 1.33% rise in energy consumption. Moreover, this methodology provides a structured framework for understanding cloud-fog system dependability, serves as a reference for comparing dependability across different systems, and aids in resource allocation optimization. This research significantly contributes to the efforts to enhance cloud-fog system dependability.","2169-3536","","10.1109/ACCESS.2023.3340195","Basic Science Research Program through the National Research Foundation of Korea (NRF); Ministry of Education(grant numbers:2020R1A6A1A03046811,2021R1A2C2094943); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10347188","Cloud-fog continuum;dependability;Kubernetes;stochastic modeling","Modeling;Stochastic processes;Power demand;Energy consumption;Edge computing;Cloud computing","","1","","47","CCBYNCND","7 Dec 2023","","","IEEE","IEEE Journals"
"Visualization Approach for RAMI 4.0 Value Chain Analysis","S. Javed; J. van Deventer; C. Paniagua; J. Delsing","Department of Computer Science, Electrical and Space Engineering, Lulea University of Technology, Lulea, Sweden; Department of Computer Science, Electrical and Space Engineering, Lulea University of Technology, Lulea, Sweden; Department of Computer Science, Electrical and Space Engineering, Lulea University of Technology, Lulea, Sweden; Department of Computer Science, Electrical and Space Engineering, Lulea University of Technology, Lulea, Sweden",IEEE Open Journal of the Industrial Electronics Society,"9 Jan 2025","2025","6","","1","24","Industry 4.0 has revolutionized industrial automation, with models, such as Industry 4.0 Reference Architectural Model (RAMI 4.0), providing a structured framework for optimizing value chains and processes. However, the complexity and abstract nature of RAMI 4.0 have limited its practical application, especially due to the lack of clear visualization methods to understand industrial ecosystems. Effective visualization is essential to translate this framework into actionable insights, enabling stakeholders to grasp system interactions, dependencies, and value-creation processes. This article proposes a multidimensional visualization approach, illustrated through a smart heat pump example, to map information and operational technologies, their interactions, and value chains. Combining 3-D visualizations for integrated system overviews with 2-D visualizations for task-specific analysis, the approach provides a comprehensive understanding of RAMI 4.0 value chains, enabling stakeholders to address their analytical needs with clarity. It facilitates run-time value chain analysis, offering real-time insights for decision-making during operations. The approach maps industrial systems across RAMI 4.0 axes and aligns them with engineering processes and lifecycle phases, enabling the exploration of system interactions, dependencies, and stakeholder contributions. This supports the analysis of engineering and business processes, optimizes infrastructure, and facilitates smooth technological transitions. It enhances RAMI 4.0’s utility for real-time decision-making and operational efficiency, boosting competitiveness in industrial ecosystems.","2644-1284","","10.1109/OJIES.2024.3520410","European Commission and Arrowhead flexible Production Value Network (fPVN)(grant numbers:101111977,101092069); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10807841","2-D and 3-D visualizations;Industry 4.0 (I4.0);lifecycle management;microservice architecture;real-time decision making;run-time value chain analysis;smart industry ecosystems;stakeholder collaboration","Visualization;Stakeholders;Ecosystems;Real-time systems;Complexity theory;Decision making;Industrial electronics;Fourth Industrial Revolution;Biological system modeling;Three-dimensional displays","","1","","45","CCBY","19 Dec 2024","","","IEEE","IEEE Journals"
"AI-Driven Ground Robots: Mobile Edge Computing and mmWave Communications at Work","G. Baruffa; A. Detti; L. Rugini; F. Crocetti; P. Banelli; G. Costante; P. Valigi","Department of Engineering, University of Perugia, Perugia, Italy; Department of Electronic Engineering, University of Rome “Tor Vergata,”, Rome, Italy; Department of Engineering, University of Perugia, Perugia, Italy; Department of Engineering, University of Perugia, Perugia, Italy; Department of Engineering, University of Perugia, Perugia, Italy; Department of Engineering, University of Perugia, Perugia, Italy; Department of Engineering, University of Perugia, Perugia, Italy",IEEE Open Journal of the Communications Society,"22 May 2024","2024","5","","3104","3119","The seamless integration of multiple radio access technologies (multi-RAT) and cloud/edge resources is pivotal for advancing future networks, which seek to unify distributed and heterogeneous computing and communication resources into a cohesive continuum system, tailored for mobile applications. Many research projects and focused studies are proposing solutions in this area, the impact of which is undoubtedly increased by moving from theoretical and simulation studies to experimental validations. To this aim, this paper proposes a testbed architecture that combines contemporary communication and cloud technologies to provide microservice-based mobile applications with the ability to offload part of their tasks to cloud/edge data centers connected by multi-RAT cellular networks. The testbed leverages Kubernetes, Istio service mesh, OpenFlow, public 5G networks, and IEEE 802.11ad mmWave (60 GHz) Wi-Fi access points. The architecture is validated through a use case in which a ground robot autonomously follows a moving object by using an artificial intelligence-driven computer vision application. Computationally intensive navigation tasks are offloaded by the robot to microservice instances, which are executed on demand within cloud and edge data centers that the robot can exploit during its journey. The proposed testbed is flexible and can be reused to assess communication and cloud innovations focusing on multi-RAT cloud continuum scenarios.","2644-125X","","10.1109/OJCOMS.2024.3399015","University of Perugia through the Fondo di Ricerca di Base 2022 Project “Energy-Efficient Networking, Signal Processing and Communications (EFESO)”; Fondo di Ricerca di Base 2020 Project “Systems, Algorithms, and Architectures for Information Transmission and Processing in Future Telecommunications and Sensor Networks”; Ministero dell’Istruzione, dell’Università e della Ricerca (MIUR) through the PRIN 2017 Project “Liquid Edge Computing Based on Distributed Machine Learning and Millimeter-Wave Radio Access (LIQUID_EDGE);”; European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU, Partnership on “Telecommunications of the Future,” Program “RESTART”(grant numbers:PE00000001); “Netwin”(grant numbers:E83C22004640001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10527382","Containerization;ground robot;millimeter wave;mobile edge computing;mobile edge learning;object detection;orchestration;smart city;software defined networking","Robots;Microservice architectures;Task analysis;Cloud computing;Millimeter wave communication;5G mobile communication;Image edge detection","","1","","63","CCBYNCND","9 May 2024","","","IEEE","IEEE Journals"
"A Big Data Architecture for Digital Twin Creation of Railway Signals Based on Synthetic Data","G. Salierno; L. Leonardi; G. Cabri","Department of Engineering “Enzo Ferrari,”, University of Modena and Reggio Emilia, Modena, Italy; Department of Engineering “Enzo Ferrari,”, University of Modena and Reggio Emilia, Modena, Italy; Department of Physics, Informatics and Mathematics, University of Modena and Reggio Emilia, Modena, Italy",IEEE Open Journal of Intelligent Transportation Systems,"18 Jul 2024","2024","5","","342","359","Industry 5.0 has introduced new possibilities for defining key features of the factories of the future. This trend has transformed traditional industrial production by exploiting Digital Twin (DT) models as virtual representations of physical manufacturing assets. In the railway industry, Digital Twin models offer significant benefits by enabling anticipation of developments in rail systems and subsystems, providing insight into the future performance of physical assets, and allowing testing and prototyping solutions prior to implementation. This paper presents our approach for creating a Digital Twin model in the railway domain. We particularly emphasize the critical role of Big Data in supporting decision-making for railway companies and the importance of data in creating virtual representations of physical objects in railway systems. Our results show that the Digital Twin model of railway switch points, based on synthetic data, accurately represents the behavior of physical railway switches in terms of data points.","2687-7813","","10.1109/OJITS.2024.3412820","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554659","Big data;digital twin;machine learning;synthetic data;railway industry;artificial intelligence","Rail transportation;Switches;Digital twins;Big Data;Data models;Computer architecture;Synthetic data;Machine learning;Artificial intelligence;Computer architecture","","1","","30","CCBY","11 Jun 2024","","","IEEE","IEEE Journals"
"Mapping Technological Trajectories of Edge Computing: A Citation Graph Analysis","E. B. Wagiu; C. -M. Liu; Y. Palopak","Department of Computer Science and Information Engineering, National Taipei University of Technology, Taipei, Taiwan; Department of Computer Science and Information Engineering, National Taipei University of Technology, Taipei, Taiwan; Department of Information Management, National Taiwan University of Science and Technology, Taipei, Taiwan",IEEE Internet of Things Journal,"25 Apr 2024","2024","11","9","16545","16560","The notion of edge computing has recently arisen as a potentially helpful advancement in distributed computing and communication infrastructure. To better understand the evolution and diffusion of technological knowledge in this field, we conducted a citation graph analysis of scholarly publications related to edge computing. Our study aims to identify the critical technological trajectories and research themes that have shaped the development and adoption of edge computing over time. We employ main path analysis (MPA) to construct a citation graph using a comprehensive publications data set from Web of Science (WoS) databases. We applied keyword co-occurrence analysis techniques to identify clusters of research topics and their interconnections. Occurrence cluster analysis results indicate that current research in edge computing is primarily centered around the architecture of edge computing, high-speed networks, mobile edge computing, computing systems, and blockchain and security technologies. We also identified the significant papers and contributors in the edge computing field and their impact on the technological trajectory of edge computing. The results reveal 35 significant papers that shape the emergence of four phases of the technology trajectory, including edge computing transition, architecture, edge future, and edge optimization.","2327-4662","","10.1109/JIOT.2024.3355056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10402108","Edge computing;keyword co-occurrence;main path analysis (MPA);mapping trajectories","Edge computing;Trajectory;Internet of Things;Cloud computing;Systematics;Security;Computer architecture","","1","","84","IEEE","17 Jan 2024","","","IEEE","IEEE Journals"
"A Comprehensive Study of Load Balancing Approaches in Real-Time Multi-Core Systems for Mixed Real-Time Tasks","S. Jadon; P. K. Kannan; U. Kalaria; K. R. Varsha; K. Gupta; P. B. Honnavalli","Department of Computer Science Engineering, PES University, Bengaluru, India; Department of Computer Science Engineering, PES University, Bengaluru, India; Department of Computer Science Engineering, PES University, Bengaluru, India; Department of Computer Science Engineering, PES University, Bengaluru, India; Department of Computer Science Engineering, PES University, Bengaluru, India; Department of Computer Science Engineering, PES University, Bengaluru, India",IEEE Access,"18 Apr 2024","2024","12","","53373","53395","Real-time systems are becoming pervasive with the growing global connectivity and rising consumer demands. The need for real-time processing has become a crucial part of many business applications worldwide. A key factor that determines the time taken for an application to give out the result hinges on its ability to prioritize, manage, and execute real-time workloads. However, there are several difficulties and constraints connected with implementing tasks in a real-time context. This research study primarily focuses on load balancing for mixed real-time tasks on a multi-core system, one of the major challenges for executing real-time workloads. The purpose of load balancing is to distribute the load evenly among the processor(s) and maximize their utility while minimizing overall execution time. The goal of this paper is to present a critical analysis of existing load balancing techniques for both periodic and aperiodic tasks. The paper explores several factors including throughput, performance, migration time, response time, overhead, resource utilization, scalability, fault tolerance, power efficiency, and other variables that play a crucial role in assessing the efficacy of load balancing in real-time systems. The proposed has contributed in four folds. Firstly, the state-of-the-art of various load balancing algorithms are discussed followed by the architecture involved in real-time multi-core systems. Later, different load balancing based scheduling algorithms were compared on the basis of different schemas and metrics for algorithm evaluation is also provided. Finally, the paper also identifies areas that warrant further exploration or investigation, suggesting potential avenues for future research, and highlighting emerging trends or developments that may shape the field.","2169-3536","","10.1109/ACCESS.2024.3388291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10497602","Scheduling algorithms;real-time systems;priority-driven algorithms;EDF (earliest deadline first);RM (rate-monotonic);LBPSA (load balanced partitioning and scheduling algorithm);homogeneous multi-core;heterogeneous multi-core;static algorithms;dynamic algorithms","Task analysis;Real-time systems;Heuristic algorithms;Scheduling;Load management;Partitioning algorithms;Scheduling algorithms","","1","","64","CCBYNCND","12 Apr 2024","","","IEEE","IEEE Journals"
"LoRa Meets IP: A Container-Based Architecture to Virtualize LoRaWAN End Nodes","A. Cilfone; L. Davoli; G. Ferrari","Department of Engineering and Architecture, Internet of Things (IoT) Lab, University of Parma, Parma, Italy; Department of Engineering and Architecture, Internet of Things (IoT) Lab, University of Parma, Parma, Italy; Department of Engineering and Architecture, Internet of Things (IoT) Lab, University of Parma, Parma, Italy",IEEE Transactions on Mobile Computing,"3 Sep 2024","2024","23","10","9191","9207","In this work, a container-based architecture for the integration of Long Range Wide Area Network (LoRaWAN) end nodes—e.g., used to monitor industrial machines or mobile entities in specific environments—with Internet Protocol (IP)-based networks is proposed and its performance is investigated. To this end, we exploit the native service and resource discovery support of the Constrained Application Protocol (CoAP), as well as its light traffic requirements, owing to its use of User Datagram Protocol (UDP) rather than Transmission Control Protocol (TCP). This approach (i) adapts transparently (with no impact) to both private and public LoRaWAN networks, (ii) enables seamless interaction between LoRaWAN-based and CoAP-based nodes, through a logical “virtualization” of LoRaWAN nodes at server side, and (iii) enables routing among LoRaWAN end nodes, overcoming LoRaWAN's absence of inter-node communication and lack of compliance (at the end nodes’ side) with IP. Two virtualization approaches are proposed: (i) virtualization of a single end node (represented as a CoAP server) per container and (ii) virtualization of multiple end nodes (as CoAP servers) per container. Finally, deployments of the proposed virtualization architectures, using both a laptop and an Internet of Things (IoT) device (e.g., a Raspberry Pi), are considered, highlighting how the best solution relies on the use of several containers, with more than one CoAP server per container.","1558-0660","","10.1109/TMC.2024.3359150","European Union's Horizon 2020 research and innovation program ECSEL Joint Undertaking; Airborne Data Collection on Resilient System Architectures(grant numbers:876038); Intelligent Secure Trustable Things; European Union's Horizon Europe Research and Innovation Program Key Digital Technology (KDT) JU(grant numbers:101097267); OPtimization of Electric Vehicle Autonomy; National Research Centre for Agricultural Technologies(grant numbers:CN00000022); National Recovery and Resilience Plan; Italian Ministry of University and Research; European Union – NextGenerationEU; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10415221","Internet of Things;LoRaWAN;virtualization;Constrained Application Protocol (CoAP)","Internet of Things;Low-power wide area networks;Virtualization;Servers;Computer architecture;Mobile computing;Downlink","","1","","57","CCBY","26 Jan 2024","","","IEEE","IEEE Journals"
"Network Digital Twins: A Systematic Review","R. Verdecchia; L. Scommegna; B. Picano; M. Becattini; E. Vicario","Department of Information Engineering, Software Technologies Laboratory, University of Florence, Florence, Italy; Department of Information Engineering, Software Technologies Laboratory, University of Florence, Florence, Italy; Department of Information Engineering, Software Technologies Laboratory, University of Florence, Florence, Italy; Department of Information Engineering, Software Technologies Laboratory, University of Florence, Florence, Italy; Department of Information Engineering, Software Technologies Laboratory, University of Florence, Florence, Italy",IEEE Access,"10 Oct 2024","2024","12","","145400","145416","Network management is becoming more complex due to various factors. The growth of IoT increases the number of nodes to control. The combination of Edge and Fog Computing with distributed algorithms makes network synchronization challenging. Softwarized technologies simplify network management but create integration issues with legacy networks. Even in industrial settings where drones and mobile robots are used, proper network management is crucial yet challenging. In this context, digital twins can be used to replicate the structure and behavior of the physical network and at the same time can be used to successfully manage the complexity and heterogeneity of current networks. Despite the rapid growth of interest in the topic, a comprehensive overview of Network Digital Twin research is currently missing. To address this gap, in this paper, we present a systematic review of the Network Digital Twin literature. From the analysis of 138 primary studies, various insights emerge. Networking Digital Twin is a particularly recent concept that has been explored in the literature since 2017 and is experiencing a steady increase to this day. The vast majority of the studies propose solutions to optimize network performance, but there are also many oriented towards other goals such as security and functional suitability. The three most recurrent application domains, as self-reported in the primary studies, are those of smart industry, edge computing, and vehicular. The main research topics aim at network optimization, support for offloading, resource allocation, and floor monitoring, but also support in the implementation of machine learning algorithms such as federated learning. As a conclusion, Networking Digital Twin proves to be a promising emerging field both for academics and practitioners.","2169-3536","","10.1109/ACCESS.2024.3453034","European Union under Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU, Partnership on “Telecommunications of the Future” (Program “RESTART”)(grant numbers:PE0000001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10662957","Digital twins;computer networks;systematic literature review","Digital twins;Systematics;Monitoring;Data mining;Wireless communication;Synchronization;Surveys;Systematic literature review;Computer networks","","1","","174","CCBYNCND","2 Sep 2024","","","IEEE","IEEE Journals"
"Enabling Industrial Internet of Things by Leveraging Distributed Edge-to-Cloud Computing: Challenges and Opportunities","M. N. Jamil; O. Schelén; A. Afif Monrat; K. Andersson","Department of Computer Science, Electrical and Space Engineering, Luleå University of Technology, Skellefteå, Sweden; Department of Computer Science, Electrical and Space Engineering, Luleå University of Technology, Skellefteå, Sweden; Department of Computer Science, Electrical and Space Engineering, Luleå University of Technology, Skellefteå, Sweden; Department of Computer Science, Electrical and Space Engineering, Luleå University of Technology, Skellefteå, Sweden",IEEE Access,"17 Sep 2024","2024","12","","127294","127308","The Industrial Internet of Things (IIoT) promises automation, efficiency, and data-driven decision-making by real-time data collection and analysis. However, traditional IIoT architectures are cloud-centric and, therefore, struggle to handle large volumes of data, edge bandwidth constraints, and data confidentiality. Distributed edge-to-cloud computing emerges as a potential solution, also paving the ground for edge-to-cloud data analytics and distributed Artificial Intelligence (AI) to obtain insights for decision-making and predictive maintenance. Despite the potential, however, there is a lack of comprehensive studies identifying key requirements for distributed edge-to-cloud IIoT and analyzing to what extent emerging IoT platforms meet those requirements. The scope of this article is to survey existing literature to identify key requirements in IIoT from the perspective of distributed edge-to-cloud computing. We provide a comparative analysis of three prominent IoT platforms, namely ThingsBoard, Eclipse Ditto, and Microsoft Azure IoT, and assess how these platforms meet the key IIoT requirements. Finally, we identify open challenges and potential research opportunities based on the insights gained from the analysis of the three IoT platforms, thereby setting the stage for future work.","2169-3536","","10.1109/ACCESS.2024.3454812","Green Transition North project co-funded by European Union(grant numbers:20359796); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10666680","Industrial Internet of Things (IIoT);edge-to-cloud computing;data analytics;IoT platforms","Industrial Internet of Things;Cloud computing;Scalability;Data analysis;Real-time systems;Distributed databases;Big Data","","1","","90","CCBYNCND","5 Sep 2024","","","IEEE","IEEE Journals"
"Demystifying Usability of Open-Source Computational Offloading Simulators: Performance Evaluation Campaign","D. Alekseeva; A. Ometov; E. S. Lohan","Faculty of Information Technology and Communication Sciences, Tampere University, Tampere, Finland; Faculty of Information Technology and Communication Sciences, Tampere University, Tampere, Finland; Faculty of Information Technology and Communication Sciences, Tampere University, Tampere, Finland",IEEE Sensors Journal,"14 Dec 2023","2023","23","24","30522","30534","Along with analysis and practical implementation, simulations play a key role in wireless networks and computational offloading research for several reasons. First, the simulations provide the ability to easily obtain the data for a complex system’s model evaluation. Second, simulated data provide a controlled environment for experimentation, allowing models and algorithms to be tested for robustness and identifying potential limitations before deploying them in real-world applications. Choosing the most appropriate tool for simulation might be challenging and depends on several factors, such as the main purpose, complexity of data, researcher skills, community support, and available budget. As of the time of the present analysis, several system-level open-source tools for modeling computational offloading also cover the systems’ communications side, such as CloudSim, CloudSim Plus, IoTSim-Edge, EdgeCloudSim, iFogSim2, PureEdgeSim, and YAFS. This work presents an evaluation of those based on the unique features and performance results of intensive workload- and delay-tolerant scenarios: XR with an extremely high data rate and workload; remote monitoring with a low data rate with moderate delays and workload requirements; and data streaming as a general human traffic with a relatively high bit rate but moderate workload. The work concludes that CloudSim provides a reliable environment for virtualization on the host resources, while YAFS shows minimal hardware usage, while IoTSim-Edge, PureEdgeSim, and EdgeCloudSim have fewer implemented features.","1558-1748","","10.1109/JSEN.2023.3310669","Doctoral Training Network in ELectronics, Telecommunications and Automation (DELTA); Pekka Ahonen Fund; Doctoral Grant of the Information Technology and Communications Science Faculty at Tampere University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10251116","Cloud computing;edge computing;fog computing;modeling;simulation","Cloud computing;Computational modeling;Edge computing;Data models;Sensors;Servers;Wireless sensor networks","","1","","45","CCBY","13 Sep 2023","","","IEEE","IEEE Journals"
"SPACE4AI-D: A Design-Time Tool for AI Applications Resource Selection in Computing Continua","H. Sedghani; F. Filippini; D. Ardagna","Politecnico di Milano, Milan, Italy; Politecnico di Milano, Milan, Italy; Politecnico di Milano, Milan, Italy",IEEE Transactions on Services Computing,"31 Dec 2024","2024","17","6","4324","4339","Nowadays, Artificial Intelligence (AI) applications are becoming increasingly popular in a wide range of industries, mainly thanks to Deep Neural Networks (DNNs) that needs powerful resources. Cloud computing is a promising approach to serve AI applications thanks to its high processing power, but this sometimes results in an unacceptable latency because of long-distance communication. Vice versa, edge computing is close to where data are generated and therefore it is becoming crucial for their timely, flexible, and secure management. Given the more distributed nature of the edge and the heterogeneity of its resources, efficient component placement and resource allocation approaches become critical in orchestrating the application execution. In this paper, we formulate the resource selection and AI applications component placement problem in a computing continuum as a Mixed Integer Non-Linear Problem (MINLP), and we propose a design-time tool for its efficient solution. We first propose a Random Greedy algorithm to minimize the cost of the placement while guaranteeing some response time performance constraints. Then, we develop some heuristic methods such as Local Search, Tabu Search, Simulated Annealing and Genetic Algorithms, to improve the initial solutions provided by the Random Greedy. To evaluate our proposed approach, we designed an extensive experimental campaign, comparing the heuristics methods with one another and then the best heuristic against Best Cost Performance Constraint (BCPC) algorithm, a state-of-the-art approach. The results demonstrate that our proposed approach finds lower-cost solution than BCPC (27.6% on average) under the same time limit in large-scale systems. Finally, during the validation in a real edge system including FaaS resources our approach finds the globally optimal solution, suffering a deviation of around 12% between actual and predicted costs.","1939-1374","","10.1109/TSC.2024.3479935","European Commission(grant numbers:H2020,101016577); AI-SPRINT: AI in Secure Privacy pReserving computINg conTinuum; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10715700","AI services;component placement;cloud edge computing;resource selection","Artificial intelligence;Cloud computing;Time factors;Quality of service;Costs;Containers;Partitioning algorithms;Runtime;Heuristic algorithms;Delays","","1","","49","IEEE","14 Oct 2024","","","IEEE","IEEE Journals"
"An Endogenous Security Study of Telematics Box in Intelligent Connected Vehicles","Z. Zhang; Y. Zhang; J. Zhang; J. Xie; S. Liu","Electric and Computer Engineering Department, National Digital Switching System Engineering and Technological Research Center, Zhengzhou, China; Endogenous Security Research Center, Purple Mountain Laboratories, Nanjing, China; Endogenous Security Research Center, Purple Mountain Laboratories, Nanjing, China; Electric and Computer Engineering Department, National Digital Switching System Engineering and Technological Research Center, Zhengzhou, China; Endogenous Security Research Center, Purple Mountain Laboratories, Nanjing, China",IEEE Embedded Systems Letters,"5 Dec 2024","2024","16","4","501","504","Intelligent connected vehicles (ICVs) are the result of technological advancements in the new era, greatly enhancing the driving experience. However, due to the complex nature of the system, the intelligence of devices, and the connectivity of data, a complex physical fusion system has been created. The in-vehicle Telematics Box (T-Box), serving as the central communication and data hub, faces challenges, such as network vulnerabilities, data privacy, and malicious attacks through untrusted software updates. Therefore, an in-vehicle T-Box with high reliability, security, and performance is an urgent product in the era of the Internet of Vehicles. This article investigates the endogenous security of the T-Box in ICV, with a focus on the integration of dynamic heterogeneous redundancy (DHR) architecture. The underlying idea is to transform the original system into multiple heterogeneous systems, where some of them handle the same business functions. By applying a consensus mechanism to detect malicious nodes and dynamically scheduling healthy nodes into the working mode, a self-purifying defense system with intrinsic security is formed. This design approach endows the T-Box with inherent defense capabilities against unknown vulnerabilities. Meanwhile, the in-vehicle T-Box is redesigned in the software and hardware implementation scheme. Experimental results demonstrate that the new design notably enhances and ensures the robust stability and elevated reliability of the in-vehicle T-Box. Evidently, the upgraded T-Box ensures the safe processing of in-vehicle CAN bus data.","1943-0671","","10.1109/LES.2024.3432593","National Key Research and Development Program Project(grant numbers:2022YFB3102800); Major Science and Technology Projects in Henan Province(grant numbers:221100240100); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10606440","Dynamic heterogeneous redundancy (DHR) architecture;endogenous security;reliability;telematics box","Security;Reliability;Hardware;Computer architecture;Operating systems;Numerical models;Virtualization","","1","","14","IEEE","23 Jul 2024","","","IEEE","IEEE Journals"
"A Modular, End-to-End Next-Generation Network Testbed: Toward a Fully Automated Network Management Platform","A. Chouman; D. M. Manias; A. Shami","Department of Electrical and Computer Engineering, Western University, London, ON, Canada; Department of Electrical and Computer Engineering, Western University, London, ON, Canada; Department of Electrical and Computer Engineering, Western University, London, ON, Canada",IEEE Transactions on Network and Service Management,"11 Oct 2024","2024","21","5","5445","5463","Experimentation in practical, end-to-end (E2E) next-generation networks deployments is becoming increasingly prevalent and significant in the realm of modern networking and wireless communications research. The prevalence of fifth-generation technology (5G) testbeds and the emergence of developing networks systems, for the purposes of research and testing, focus on the capabilities and features of analytics, intelligence, and automated management using novel testbed designs and architectures, ranging from simple simulations and setups to complex networking systems; however, with the ever-demanding application requirements for modern and future networks, 5G-and-beyond (denoted as 5G+) testbed experimentation can be useful in assessing the creation of large-scale network infrastructures that are capable of supporting E2E virtualized mobile network services. To this end, this paper presents a functional, modular E2E 5G+ system, complete with the integration of a Radio Access Network (RAN) and handling the connection of User Equipment (UE) in real-world scenarios. As well, this paper assesses and evaluates the effectiveness of emulating full network functionalities and capabilities, including a complete description of user-plane data, from UE registrations to communications sequences, and leads to the presentation of a future outlook in powering new experimentation for 6G and next-generation networks.","1932-4537","","10.1109/TNSM.2024.3416031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10559989","Next-generation networks;end-to-end networks;5G and Beyond;network automation;network management platform","5G mobile communication;Network systems;Testing;Cloud computing;Next generation networking;Emulation;Software","","1","","79","IEEE","18 Jun 2024","","","IEEE","IEEE Journals"
"Evaluating Performance and Resource Consumption of REST Frameworks and Execution Environments: Insights and Guidelines for Developers and Companies","S. D. Meglio; L. Libero Lucio Starace","Department of Electrical Engineering and Information Technology, Università degli Studi di Napoli Federico II, Naples, Italy; Department of Electrical Engineering and Information Technology, Università degli Studi di Napoli Federico II, Naples, Italy",IEEE Access,"15 Nov 2024","2024","12","","161649","161669","The REST (REpresentational State Transfer) paradigm has become essential for designing distributed applications that leverage the HTTP protocol, enabling efficient data exchange and the development of scalable architectures such as microservices. However, selecting an appropriate framework among the myriad available options, especially given the diversity of emerging execution environments, presents a significant challenge. Often, this decision neglects crucial factors such as performance and energy efficiency, favoring instead developer familiarity and popularity within the industry. To address this, we conducted a comprehensive benchmark study using a prototype REST API application provided by an industry partner, which was implemented multiple times using different REST API frameworks. We evaluated five different REST API frameworks across three popular programming languages, incorporating both traditional and emerging execution environments, resulting in twelve distinct configurations. Our results reveal significant differences in performance and computational resource consumption across different frameworks and execution environments, highlighting the necessity of making informed technology choices based on thorough analysis rather than convenience or familiarity. In addition to our findings, we offer other contributions to the field: an automated pipeline that benchmarks different configurations with various frameworks and execution environments, and a reference benchmark REST API that can be used in other studies. This research provides valuable insights and tools for developers and organizations aiming to select high-performance, resource-efficient technologies that promote environmental sustainability and reduce operational costs.","2169-3536","","10.1109/ACCESS.2024.3489892","Piano Nazionale di Ripresa e Resilienza Ministero dell’Università e della Ricerca(grant numbers:PE0000013-FAIR); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10741246","Execution environments;green computing;performance benchmark;performance testing;REST API;REST API frameworks;sustainable computing;web applications","Java;Benchmark testing;Python;Energy consumption;Energy efficiency;Sports;Software development management;Prototypes;Costs;Codes;Green computing;Green computing;Web services","","","","64","CCBY","1 Nov 2024","","","IEEE","IEEE Journals"
"Enhancing Software Distribution and Collaboration in Manufacturing: Developing the IMOS Platform","A. Monte Pegado; A. Dionisio Rocha; J. Barata","NOVA School of Science and Technology, Center of Technology and Systems (UNINOVA-CTS) and Associated Lab of Intelligent Systems (LASI), NOVA University Lisbon, Lisbon, Portugal; NOVA School of Science and Technology, Center of Technology and Systems (UNINOVA-CTS) and Associated Lab of Intelligent Systems (LASI), NOVA University Lisbon, Lisbon, Portugal; NOVA School of Science and Technology, Center of Technology and Systems (UNINOVA-CTS) and Associated Lab of Intelligent Systems (LASI), NOVA University Lisbon, Lisbon, Portugal",IEEE Access,"2 Jan 2025","2025","13","","480","494","In today’s fast-evolving tech landscape, small and medium-sized businesses often struggle to keep pace with innovation due to financial constraints and the complexity of integrating new solutions. As the industrial world transitions from Industry 4.0 to Industry 5.0, both manufacturers and software providers are embracing a modular approach to software procurement and deployment. This strategy not only boosts flexibility and affordability but also prioritizes sustainability and the value of collaborative efforts. The Industry Modular Operating System (IMOS) is presented as a platform supporting software accessibility, execution, and integration, while encouraging partnerships between software developers and industrial site managers. IMOS is designed to address recent industry challenges, with a focus on building a solid framework that highlights modularity, facilitates distribution, and promotes cooperation. This system facilitates the distribution of industry-oriented tools by offering containerized software solutions through a unified marketplace. The platform enables industry professionals to efficiently manage diverse applications and services, both on-site and in the cloud, granting access to tailored data and resources. IMOS uniquely extends the concept of Industrial Symbiosis to digital assets, proposing the development of a collaborative forum that values co-creation, facilitates resource sharing, and fosters symbiotic relations. By creating this flexible ecosystem, IMOS closes the gap between development and production, fostering collaboration, innovation, and shared expertise within an active community of industry leaders.","2169-3536","","10.1109/ACCESS.2024.3521867","Fundação para a Ciência e Tecnologia through the program CTS/00066 and Center of Technology and Systems (CTS); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10812744","Application;cloud;collaborative manufacturing;community;containerization;IMOS;integration;marketplace;symbiosis","Software;Collaboration;Manufacturing;Industries;Ecosystems;Technological innovation;Symbiosis;Computer architecture;Production;Operating systems","","","","43","CCBY","23 Dec 2024","","","IEEE","IEEE Journals"
"Modular VNF Components Acceleration With FPGA Overlays","F. Bachini Lopes; A. E. Schaeffer-Filho; G. L. Nazar","Institute of Informatics, Federal University of Rio Grande do Sul, Porto Alegre, Brazil; Institute of Informatics, Federal University of Rio Grande do Sul, Porto Alegre, Brazil; Institute of Informatics, Federal University of Rio Grande do Sul, Porto Alegre, Brazil",IEEE Transactions on Network and Service Management,"10 Mar 2023","2023","20","1","846","857","Network Functions Virtualization (NFV) is a novel paradigm that aims to minimize operational and capital expenditures, by decoupling network functions from dedicated hardware and implementing them as Virtualized Network Functions (VNFs) instead. However, to fulfill such expectations, VNFs must be implemented efficiently, offering high performance and energy efficiency, which is not always feasible on General-Purpose Processors (GPPs). Thus, the use of reconfigurable accelerators, typically based on Field-Programmable Gate Arrays (FPGAs), has been proposed to offer higher efficiency whilst not forsaking the flexibility that is the core of the NFV paradigm. Not all VNFs or even VNF Components (VNFCs), however, are suitable for FPGA acceleration. This leads to new challenges related to identifying those VNFCs that should be deployed in FPGAs, maximizing the reuse of developed FPGA accelerators, and managing this heterogeneous infrastructure. To address these challenges, in this paper we present an enhanced design of VNFAccel, a platform to manage VNFCs in heterogeneous NFV infrastructures. We evaluate the performance and energy efficiency of the implemented functions in comparison to GPP-based solutions, showing that, when properly used, FPGAs can provide relevant benefits while maintaining the flexibility and reuse potential envisioned for NFV.","1932-4537","","10.1109/TNSM.2022.3211448","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) - Finance Code 001; Fundação de Amparo à Pesquisa do Estado do Rio Grande do Sul (FAPERGS)(grant numbers:#21/2551-0002089-0,#19/2551-0001645-0); Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq)(grant numbers:#311276/2021-0,#313124/2020-5); São Paulo Research Foundation (FAPESP)(grant numbers:#2021/06947-6); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910597","FPGA;network function virtualization;hardware acceleration","Field programmable gate arrays;Software;Network function virtualization;Guidelines;Firewalls (computing);Table lookup;Performance evaluation","","","","66","IEEE","4 Oct 2022","","","IEEE","IEEE Journals"
"Reducing Internal Collateral Damage From DDoS Attacks Through Micro-Service Cloud Architecture","A. Kumar; M. Agarwal","Department of Computer Science and Engineering, Indian Institute of Technology Patna, Patna, Bihar, India; Department of Computer Science and Engineering, Indian Institute of Technology Patna, Patna, Bihar, India",IEEE Transactions on Information Forensics and Security,"13 Jan 2025","2025","20","","1081","1091","Mitigating DDoS attacks poses a significant challenge for cyber security teams within victim organizations, as these attacks directly target service availability. Most DDoS mitigation solutions focus address the direct effects of DDoS attacks, such as service unavailability and network congestion, while the indirect effects, including collateral damage to legitimate users, receive substantially less attention in the present state-of-the-art. To address this gap, we propose a novel defense architecture designed to mitigate collateral damage and ensure service availability for legitimate users even under attack conditions. The proposed approach employs containerization, micro-services architecture, and traffic segmentation to enhance system resilience and fortify security. We send requests for two distinct services, namely an HTTP-based service and an SSH service, in order to analyze the collateral damage caused by the DDoS attack. The proposed architecture classifies incoming HTTP traffic into two categories: “benign traffic” and “suspicious traffic,” determined by the number of requests originating from the same source address. We tested this approach in three different scenarios (S-1, S-2, and S-3). Experimental results demonstrate that the proposed architecture effectively isolates suspicious traffic, mitigating its impact on benign services. This ensures the availability of critical services during a DDoS attack while minimizing collateral damage. In scenarios S-1, S-2, and S-3, it maintains service availability at 3%, 67%, and 98%, respectively, highlighting its efficacy in the face of varying levels of DDoS attack intensity. Furthermore, the architecture is extremely effective in reducing the collateral effects on SSH requests during a DDoS attack. In the S-1 scenario, SSH login time was reduced by 25%, 46%, and 27%, respectively. In the S-2 scenario, the reductions were 99%, 53%, and 29%. In the same vein, the system achieved reductions of 4%, 17%, and 99% in the S-3 scenario.","1556-6021","","10.1109/TIFS.2024.3516560","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10810456","Security and protection;cloud computing;DDoS attacks;containerization","Denial-of-service attack;Computer crime;Cloud computing;Prevention and mitigation;Containers;Virtual machines;Computer architecture;Computational modeling;Accuracy;Security","","","","31","IEEE","20 Dec 2024","","","IEEE","IEEE Journals"
"ACE-WARP: A Cost-Effective Approach to Proactive and Non-Disruptive Incident Response in Kubernetes Clusters","S. Bagheri; H. Kermabon-Bobinnec; M. E. Kabir; S. Majumdar; L. Wang; Y. Jarraya; B. Nour; M. Pourzandi","Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada; Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada; Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada; Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada; Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada; Ericsson Security Research, Montreal, QC, Canada; Ericsson Security Research, Montreal, QC, Canada; Ericsson Security Research, Montreal, QC, Canada",IEEE Transactions on Information Forensics and Security,"16 Sep 2024","2024","19","","8204","8219","A large-scale cluster of containers managed with an orchestrator like Kubernetes are behind many cloud-native applications today. However, the weaker isolation provided by containers means attackers can potentially exploit a vulnerable container and then escape its isolation to cause more severe damages to the underlying infrastructure and its hosted applications. Defending against such an attack using existing attack detection solutions can be challenging. Due to the well known high false positive rate of such solutions, taking aggressive actions upon every alert can lead to unacceptable service disruption. On the other hand, waiting for security administrators to perform in-depth analysis and validation could render the mitigation too late to prevent irreversible damages. In this paper, we propose ACE-WARP, a cost-effective proactive and non-disruptive incident response to address such security challenges for Kubernetes clusters. First, our approach is proactive in the sense that it performs mitigation based on predicted (instead of real) attacks, which prevents irreversible damages. Second, our approach is also non-disruptive since the mitigation is achieved through live migration of containers, which causes no service disruption even in the case of false positives. Finally, to realize the full potential of this approach in containers migration, we formulate the inherent trade-off between security and cost (delay) as a multi-objective optimization problem. Our evaluation results show that ACE-WARP can successfully mitigate up to 81% of the attacks, and our optimization algorithm achieves up to 30% more threat reduction and 7% less delay while being 37 times faster compared to a standard optimization solution.","1556-6021","","10.1109/TIFS.2024.3449038","Natural Sciences and Engineering Research Council of Canada and Ericsson Canada under the Industrial Research Chair in Software-Defined Networking/Network Functions Virtualization (SDN/NFV) Security; Canada Foundation for Innovation through the John R. Evans Leaders Fund (JELF) Project(grant numbers:38599); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646387","Cloud-native;incident response;proactive security;containers;Kubernetes;attack mitigation;optimization;advanced persistent threats (APT)","Prevention and mitigation;Containers;Security;Optimization;Delays;Costs;Wireless sensor networks","","","","83","IEEE","26 Aug 2024","","","IEEE","IEEE Journals"
"Voice Frequency-Based Gender Classification Using Convolutional Neural Network for Smart Home","N. Nasaruddin; M. A. P. Pratama Tresma; M. Khamdan Muchamad; Z. Fuadi","Department of Electrical and Computer Engineering, Universitas Syiah Kuala, Banda Aceh, Indonesia; Department of Electrical and Computer Engineering, Universitas Syiah Kuala, Banda Aceh, Indonesia; Department of Electrical and Computer Engineering, Universitas Syiah Kuala, Banda Aceh, Indonesia; Department of Mechanical and Industrial Engineering, Universitas Syiah Kuala, Banda Aceh, Indonesia",IEEE Access,"5 Aug 2024","2024","12","","104190","104203","The smart home’s functional requirements should include the capability to differentiate between various user categories, such as gender and voice recognition. The data-driven Internet of Things (IoT) can present challenges for the elderly and people with disabilities, but voice recognition technology could offer an effective solution. In addition, developing an accurate gender prediction model for voice recognition is still challenging due to the large time variation and randomness. Therefore, we propose gender classification and detection models based on voice frequency using Convolutional Neural Networks (CNN) with ResNet50 and ResNet101 architectures to enhance smart home functionality. We also introduce an algorithm for converting voice frequencies into images to speed up the recognition and detection processes. The research method involves converting voice frequencies into images to expedite the recognition and detection processes. The CNN models were trained and tested with various learning rates using audio datasets. Performance was evaluated through simulations that measured training accuracy, validation accuracy, recall, precision, and F1 scores. The simulation results show high training accuracy: ResNet50 achieved 99.67% and ResNet101 achieved 99.82%. The validation accuracy of the models also exceeded the accuracy of traditional CNN models in previous studies. The simulation results based on recall, precision, and F1 score for each proposed model are 99.3%, 100%, and 99.65%, respectively. Finally, we successfully used the ResNet50 model to create a low-latency smart home prototype. Thus, this paper significantly contributes to the practical applications of voice-based gender recognition in smart home environments with high accuracy and efficiency in detection.","2169-3536","","10.1109/ACCESS.2024.3434547","Ministry of Education, Culture, Research, and Technology, Indonesia(grant numbers:590/UN11.2.1/PT.01.03/DPRM/2023); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10613033","Voice frequency;convolutional neural networks (CNN);ResNet50;Resnet101;smart home","Smart homes;Speech recognition;Deep learning;Convolutional neural networks;Accuracy;Residual neural networks;Prototypes","","","","42","CCBYNCND","29 Jul 2024","","","IEEE","IEEE Journals"
"FACT: Facilitating Trustworthy Services in Mobile IoT Systems","P. Dass; S. Misra","Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India",IEEE Systems Journal,"12 Dec 2023","2023","17","4","5511","5518","Mobile Internet of Things (IoT) users possess dynamic service requirements in a real IoT system and always try to get services from trusted entities, which we term as facility nodes. However, the trustworthiness of the facility nodes varies from location to location, depending upon the service quality and user feedback. Therefore, a service provider may have different trust scores for its deployed facility nodes in different locations. It is always crucial for an IoT user to ensure that service has been taken from trustworthy nodes. In this article, we consider an IoT service provisioning architecture, where an IoT user requests services from nearby trusted nodes that are deployed by service providers other than the user's own registered service provider. To serve the requests from these unregistered users, the facility nodes incur additional costs. Furthermore, due to heterogeneous service requirements, IoT users prefer composite microservices from a set of facility nodes. In such cases, it is difficult to design a pricing scheme to incentivize the facility nodes. Therefore, we propose a pricing mechanism based on a two-stage Stackelberg game and analyze the interactions between the IoT users and the facility nodes, while considering the trust scores and service charges. In the game model, the facility nodes act as the leaders, and the IoT users act as the followers. Using backward induction, we analyze the existence and uniqueness of the Stackelberg equilibrium. Finally, the numerical results based on the mobility traces of a real map show that the proposed scheme incentivizes both the IoT users and the facility nodes, while ensuring trusted services.","1937-9234","","10.1109/JSYST.2023.3321665","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10287681","Game theory;mobile users;pricing;service provisioning;trustworthy Internet of Things (IoT)","Internet of Things;Costs;Pricing;Games;Cloud computing;Resource management;Delays","","","","31","IEEE","18 Oct 2023","","","IEEE","IEEE Journals"
"Adaptive Context Monitoring Framework for Enhancing Caching Efficiency in Context Management Platforms","A. Manchanda; P. P. Jayaraman; A. Banerjee; K. Fizza; A. Zaslavsky","SoSCET - Department of Computing Technologies, Swinburne University of Technology, Melbourne, VIC, Australia; SoSCET - Department of Computing Technologies, Swinburne University of Technology, Melbourne, VIC, Australia; SoSCET - Department of Computing Technologies, Swinburne University of Technology, Melbourne, VIC, Australia; SoSCET - Department of Computing Technologies, Swinburne University of Technology, Melbourne, VIC, Australia; School of Information Technology, Deakin University, Melbourne, VIC, Australia",IEEE Access,"1 Nov 2024","2024","12","","157612","157629","As the Internet of Things (IoT) continues to expand, the volume of data generated by IoT devices and the demand for IoT applications is increasing exponentially. These applications critically rely on real-time context reasoned from IoT data for effective decision-making and actuation, thereby making the accessibility of this context crucial. This study introduces a novel adaptive Context Monitoring Framework (CMF) for enhancing context caching efficiency in Context Management Platforms (CMPs) to better support the near real-time needs of IoT applications. Our proposed framework integrates two novel components: the Context Attributes Prioritisation Engine (CAPE), which prioritises and assigns weights to the context, and the Adaptive Context Management Engine (ACME), which dynamically adjusts thresholds for each context based on incoming query volumes and context cache performance. Combined, our hybrid approach ensures timely updates of context within the cache while also serving context in real-time (reducing any query response latency). Our approach is effective for dynamic changes in an IoT environment through the adaptive approach of continuously monitoring and updating the cached context. We implemented the proposed adaptive framework using a CMP namely the context-as-a-service (CoaaS) platform and evaluated it using real-world datasets obtained from a smart city application. A thorough experimental evaluation demonstrated a marked improvement in cache efficiency, achieving a 90% cache hit rate and reducing the cache expiry ratio to 5%.","2169-3536","","10.1109/ACCESS.2024.3486103","Australian Research Council (ARC) Discovery Project(grant numbers:DP200102299); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10734123","CAPE;CoaaS;context freshness;context caching;hybrid approach;IoT","Internet of Things;Monitoring;Real-time systems;Engines;Roads;Delays;Australia;Vehicle dynamics;Time factors;Smart cities","","","","49","CCBY","24 Oct 2024","","","IEEE","IEEE Journals"
"Advancing Interoperable IoT-Based Access Control Systems: A Unified Security Approach in Diverse Environments","M. Penica; M. Bhattacharya; W. O’Brien; S. McGrath; M. Hayes; E. O’Connell","Department of Electronic and Computer Engineering, University of Limerick, Limerick, Ireland; School of Information and Technology, Illinois State University, Normal, IL, USA; Department of Electronic and Computer Engineering, University of Limerick, Limerick, Ireland; Department of Electronic and Computer Engineering, University of Limerick, Limerick, Ireland; Department of Electronic and Computer Engineering, University of Limerick, Limerick, Ireland; Department of Electronic and Computer Engineering, University of Limerick, Limerick, Ireland",IEEE Access,"14 Feb 2025","2025","13","","27767","27782","The convergence of the Internet of Things (IoT) and the construction industry represents a potent opportunity to fundamentally redefine the processes of constructing and maintaining buildings. The integration of IoT technology has the potential to revolutionize building safety, efficiency, and sustainability, offering a promising avenue for innovation and advancement within the industry. In this work, we propose an innovative design and implementation of an interoperable physical access control IoT-based system that can be easily integrated with existing access control security systems and used in various environments such as offices, homes, and public buildings. Based on our developed and implemented physical access control IoT-based system, we identified several challenges in the current state of access control systems. These challenges include difficulties in integrating different security systems, the absence of interoperable standards, and the need for further research to enhance the scalability and performance of such systems.","2169-3536","","10.1109/ACCESS.2025.3538748","Science Foundation Ireland through the CONFIRM Platform(grant numbers:16/RC/3918); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10870056","Internet of Things (IoT);interoperability;physical access control;electronics;smart devices;wireless communication;software design","Access control;Security;Protocols;Interoperability;Hardware;Radiofrequency identification;QR codes;Service-oriented architecture;Buildings;Authentication","","","","33","CCBY","4 Feb 2025","","","IEEE","IEEE Journals"
"EdgeSim++: A Realistic, Versatile, and Easily Customizable Edge Computing Simulator","Q. Lu; G. Li; H. Ye","College of Computer Science and Engineering and the Guangxi Key Laboratory of Embedded Technology and Intelligent System, Guilin University of Technology, Guilin, China; College of Computer Science and Engineering, Guilin University of Technology, Guilin, China; College of Computer Science and Engineering and the Guangxi Key Laboratory of Embedded Technology and Intelligent System, Guilin University of Technology, Guilin, China",IEEE Internet of Things Journal,"25 Oct 2024","2024","11","21","35341","35360","In the edge computing environment, due to factors, such as expensive devices, complex scenarios, and fine control requirements, it is necessary to use simulators to construct heterogeneous devices, simulate task offloading scenarios, and evaluate and optimize decision-making algorithms. However, existing edge computing simulators overly simplify the simulation process, neglecting device interactions and task complexities, making it difficult to finely control the task offloading process and customize complex scenarios. We propose EdgeSim++, a realistic, versatile, and easily customizable edge computing simulator. EdgeSim++ has the capability to generate a batch of heterogeneous devices with various features, supporting the simulation of multilayer cloud-edge-end scenarios and various multiaccess edge computing architectures. It dynamically sets task and device resource attributes, enables devices to interact and schedule behaviors through the network, and facilitates easy customization of both machine learning and nonmachine learning offloading strategies. Additionally, EdgeSim++ supports network topology visualization and result visualization, providing intuitive displays of task transmission processes and offloading results, and real-time monitoring of device resource changes. To test the effectiveness of EdgeSim++, we provided default machine learning and nonmachine learning offloading strategies, detailed simulation steps, demonstrated scenario construction, showcased process control, monitored resource changes, and compared algorithm effects. The results illustrated that our simulator could realistically simulate complex edge computing scenarios, flexibly control decision processes, and customize complex offloading algorithms, exhibiting good scalability and reusability.","2327-4662","","10.1109/JIOT.2024.3434641","National Natural Science Foundation of China(grant numbers:62362018); Guangxi Key Research and Development Program(grant numbers:GUIKEAB23075116); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10612841","Artificial intelligence;cloud computing;edge computing;machine learning;modeling and simulation;resource management;task offloading","Edge computing;Task analysis;Internet of Things;Resource management;Machine learning;Computer architecture;Machine learning algorithms","","","","39","IEEE","29 Jul 2024","","","IEEE","IEEE Journals"
"Multi-Granularity Decomposition of Componentized Network Applications Based on Weighted Graph Clustering","Z. Wang; F. Zhou; L. Feng; W. Li; T. Zhang; S. Wang; Y. Li","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; China Mobile Research Institute, Beijing, China; China Mobile Research Institute, Beijing, China; China Mobile Research Institute, Beijing, China",Journal of Web Engineering,"22 Sep 2023","2022","21","3","815","844","With the development of mobile communication and network technology, smart network applications are experiencing explosive growth. These applications may consume different types of resources extensively, thus calling for the resource contribution from multiple nodes available in probably different network domains to meet the service quality requirements. Task decomposition is to set the functional components in an application in several groups to form subtasks, which can then be processed in different nodes. This paper focuses on the models and methods that decompose network applications composed of interdependent components into subtasks in different granularity. The proposed model characterizes factors that have important effects on the decomposition, such as dependency level, expected traffic, bandwidth, transmission delay between components, as well as node resources required by the components, and a density peak clustering (DPC) -based decomposition algorithm is proposed to achieve the multi-granularity decomposition. Simulation results validate the effect of the proposed approach on reducing the expected execution delay and balancing the computing resource demands of subtasks.","1544-5976","","10.13052/jwe1540-9589.21312","Beijing University of Posts and Telecommunications; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10243624","Componentized network application;weighted graph clustering;density peak clustering;multi-granularity task decomposition","Costs;Processor scheduling;Simulation;Clustering algorithms;Bandwidth;Mobile communication;Explosives","","","","25","","22 Sep 2023","","","River Publishers","River Publishers Journals"
"Simulating Fog of Medical Things: Research Challenges and Opportunities","A. Pati; A. Panigrahi; M. Parhi; B. Kumar Pattanayak; B. Sahu; S. Kant","Department of Computer Science and Engineering, Siksha ‘O’ Anusandhan (Deemed to be University), bhubaneswar, Odisha, India; Department of Computer Science and Engineering, Siksha ‘O’ Anusandhan (Deemed to be University), bhubaneswar, Odisha, India; Center for Data Science, Siksha ‘O’ Anusandhan (Deemed to be University), bhubaneswar, Odisha, India; Department of Computer Science and Engineering, Siksha ‘O’ Anusandhan (Deemed to be University), bhubaneswar, Odisha, India; Department of Information Technology, Vardhaman College of Engineering (Autonomous), Hyderabad, Telangana, India; Department of Management, College of Business and Economics, Bule Hora University, Ethiopia, Africa",IEEE Access,"17 Oct 2024","2024","12","","146527","146550","Technology has a significant impact on medical applications at the current moment. Contemporary computers are capable of processing a lot of patient medical records quickly. Due to recent advancements in the Internet-of-Things (IoT) and medical applications, patient data may be dispersed over several places. Worldwide, the IoT connects numerous devices for e-healthcare systems. The medical data monitoring and tracking field, exercise programs, and remote medical help are expanding within the e-healthcare systems. IoT-based technologies are now being used in e-healthcare systems, which can relieve pressure on e-healthcare systems, lower medical expenses, and speed up computing and processing. In the IoT setting, cloud computing, which contains centralized data centers, was developed to manage more extensive and sophisticated e-healthcare data. The central server governs the data for all IoT devices. Problems with IoT and Cloud integration only include latency, bandwidth overuse, delays in real-time responses, security, privacy, integrity, etc. The ideas of fog computing and edge computing were developed to solve the above-mentioned problems. A thorough literature overview on Fog-based medical applications using IoT is provided in this article, i.e., Fog of Medical Things (FoMT), that explores the simulators that may be employed to create and assess new Fog-related theories as well as the key attributes of Fog computing frameworks. This review also emphasizes the difficulties in the field and some unanswered questions. This study can serve as a crucial road map for the future creation of Fog-based e-healthcare IoT applications.","2169-3536","","10.1109/ACCESS.2024.3468015","Siksha ‘O’ Anusandhan, Bhubaneswar, Odisha, India; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10693431","FoMT;IoT;fog computing;cloud computing;simulation tools;e-healthcare systems","Cloud computing;Edge computing;Internet of Things;Bandwidth;Medical services;Computational modeling;Security;Electronic healthcare","","","","206","CCBYNCND","25 Sep 2024","","","IEEE","IEEE Journals"
"A Novel Space-Based Hosting Approach for Ultra Low Latency Web Services","D. Dalai; S. Babu; V. Bala Sukumaran; B. S. Manoj","Indian Institute of Space Science and Technology, Thiruvananthapuram, Kerala, India; Department of Electrical and Computer Engineering, Iowa State University, Ames, IA, USA; Indian Institute of Space Science and Technology, Thiruvananthapuram, Kerala, India; Indian Institute of Space Science and Technology, Thiruvananthapuram, Kerala, India",IEEE Access,"7 Oct 2024","2024","12","","142838","142862","The lack of terrestrial 4G and 5G communication infrastructure in rural and remote areas aggravates the digital divide. Satellite communication is important to achieve the universal goal of connecting the digitally unconnected population through global coverage. In this paper, first, we discuss the feasibility of hosting web services in satellites through several examples of radiation-hardened processors and computational boards. Next, we propose a Space-Based Hosting Service (SBHS) approach to deploy the content-servers or copies of terrestrial content-servers in satellites to achieve the low-latency service requirements. For SBHS, we develop mathematical models for communication, queuing, as well as computation, and use B+ tree to estimate computational delay and computational energy consumption of the content-server. Further, an optimization algorithm using Markov Decision Process (MDP) is designed to schedule computational cores in Low Earth Orbit (LEO) satellites to minimize queuing delay and computational energy consumption. The efficacy of our SBHS is evaluated using extensive simulations with the Wikipedia use-case in the Iridium-NEXT satellite constellation model under varying traffic conditions from different countries. The results show that SHBS is able to achieve less than 10 ms round trip latency worldwide and minimum end-to-end delays of 9.19 ms and 24.29 ms, respectively, for text-based and multimedia-based traffic. The 92.51% and 88.66% reduction in delay for text and multimedia traffic, respectively, compared to the benchmark schemes underlines the benefits of hosting content-servers in space for achieving ultra-low latency.","2169-3536","","10.1109/ACCESS.2024.3462252","Indian Institute of Technology (IIT) Palakkad Technology IHub Foundation Doctoral Fellowships(grant numbers:IPTIF/HRD/DF/032/,IPTIF/HRD/DF/032/SEP46); Department of Science and Technology-Science and Engineering Research Board (DST-SERB)(grant numbers:CGR/2021/009286); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10681066","LEO satellite;B+ tree;content server;Markov decision process (MDP);satellite networks;Wikipedia","Satellite broadcasting;Low earth orbit satellites;Internet;Servers;Space vehicles;Planetary orbits;Markov processes;Online services;Information resources;Encyclopedias;Web services;4G mobile communication;5G mobile communication","","","","77","CCBY","16 Sep 2024","","","IEEE","IEEE Journals"
"Multi-Access Edge Computing Resource Slice Allocation: A Review","F. Bahramisirat; M. A. Gregory; S. Li","School of Engineering, RMIT University, Melbourne, VIC, Australia; School of Engineering, RMIT University, Melbourne, VIC, Australia; School of Engineering, RMIT University, Melbourne, VIC, Australia",IEEE Access,"17 Dec 2024","2024","12","","188572","188589","The global deployment of fifth generation mobile cellular (5G) and beyond (5GB) communication systems has accelerated the growth of the Internet of Things (IoT) and the Internet of Everything (IoE). New applications and services that add complexity to the network, and require low latency, have necessitated new architectures that facilitate dynamic resource allocation. The network edge is changing, as Multi-Access Edge Computing (MEC) is deployed with new responsive designs that include dynamic resource slice allocation that supports the varying needs of new and innovative applications and services. Augmented reality, extended reality and virtual reality applications are key drivers for the changing network architecture. IoT and IoE data aggregation and analysis at the network edge has required new approaches to designing edge network architectures that achieve seamless access anywhere and anytime, ultra-low latency, edge computing and storage, and ultra-high bandwidth. MEC and Network Slicing (NS) are evolving to provide a key part of the solution to the new challenges. MEC allows operators to connect user devices to edge cloud computing and storage systems with low latency and high bandwidth. However, a practical configuration is required to facilitate cloud-based applications and services at the network edge and to address end-user demands. The MEC resources may be flexibly and dynamincally partitioned into separate slices. This paper will review how the integration of MEC and NS can be utilised for efficient resource allocation in edge networks. This article comprehensively reviewed the architecture of network slicing and edge computing. A review of MEC and NS in support of 5G and 5GB has been included and current challenges identified. This article aims to serve as an insightful guide to inspire further research into 5GB communication systems and edge computing.","2169-3536","","10.1109/ACCESS.2024.3515077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10792437","Network slicing;multi-access edge computing;5G;6G;resource allocation;radio access network","Resource management;5G mobile communication;Network slicing;Ultra reliable low latency communication;Radio access networks;Network function virtualization;Internet of Things;Cloud computing;Surveys;Reviews","","","","101","CCBY","11 Dec 2024","","","IEEE","IEEE Journals"
"From Technical Prerequisites to Improved Care: Distributed Edge AI for Tomographic Imaging","B. Akdemir; H. Faheem Shahid; M. A. K. Brix; J. Lääkkölä; J. Islam; T. Kumar; J. Reponen; M. T. Nieminen; E. Harjula","Centre for Wireless Communications—Networks and Systems (CWC-NS), University of Oulu, Oulu, Finland; Centre for Wireless Communications—Networks and Systems (CWC-NS), University of Oulu, Oulu, Finland; University of Oulu, Oulu, Finland; Centre for Wireless Communications—Networks and Systems (CWC-NS), University of Oulu, Oulu, Finland; Centre for Wireless Communications—Networks and Systems (CWC-NS), University of Oulu, Oulu, Finland; Department of Information and Communications Engineering, Aalto University, Espoo, Finland; University of Oulu, Oulu, Finland; University of Oulu, Oulu, Finland; Centre for Wireless Communications—Networks and Systems (CWC-NS), University of Oulu, Oulu, Finland",IEEE Access,"24 Jan 2025","2025","13","","14317","14343","Recent years have seen a surge in AI-driven medical image processing, leading to significant improvements in diagnostic performance. However, medical imaging technologies tend to create staggering volumes of medical data, necessitating high-performance computing. Cloud systems with robust GPUs and resource capacity are optimal choices for DL-based medical image processing. However, transferring data to the cloud for processing strains communication links, introduces high communication latency, and raises privacy and security concerns. Consequently, despite the undisputed benefits of cloud computing, dedicated standalone local computers are still used for image reconstruction in today’s systems. This localized strategy uses expensive hardware inefficiently and falls short of scalability and maintainability. Edge computing emerges as an innovative concept by bringing cloud processing capabilities closer to data sources. A continuum of computing including local, edge, and cloud tiers would offer a promising solution for medical image processing. According to literature survey, there are no significant works on utilizing edge cloud continuum for CBCT imaging. To fill this gap, we introduce novel 3-TECC architectural concept, specifically designed for CBCT data reconstruction in medical imaging. This article explores the evolving synergy among medical imaging, distributed AI, containerized solutions, and edge-cloud continuum technologies, highlighting their clinical implications and illuminating the potential for transformative patient care. We uncover challenges and opportunities this convergence provides with the CBCT image reconstruction use case, while aligning with regulatory compliance. The proposed 3-TECC architecture advocates a decentralized data processing paradigm, reducing reliance on the centralized approach and emphasizing the role of local-edge computing.","2169-3536","","10.1109/ACCESS.2025.3530297","Business Finland under Tomohead(grant numbers:8095/31/2022); Research Council of Finland DigiHealth(grant numbers:326291); Flagship of Advanced Mathematics for Sensing, Imaging, and Modelling(grant numbers:359186); 6G Flagship Programme(grant numbers:346208); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10843208","CBCT;distributed AI;edge computing;edge cloud continuum;GDPR;medical imaging","Imaging;Medical diagnostic imaging;Cloud computing;Image reconstruction;Image edge detection;Computer architecture;Computed tomography;Edge AI;Hospitals;Technical requirements","","","","162","CCBY","16 Jan 2025","","","IEEE","IEEE Journals"
"A Secure and Efficient BlockChain and Distributed Ledger Technology-Based Optimal Resource Management in Digital Twin Beyond 5G Networks Using Hybrid Energy Valley and Levy Flight Distributer Optimization Algorithm","K. Suresh Kumar; J. A. Alzubi; N. M. Sarhan; E. M. Awwad; V. Kandasamy; G. Ali","Department of Information Technology, Saveetha Engineering College, Chennai, Tamil Nadu, India; Faculty of Engineering, Al-Balqa Applied University, As-Salt, Jordan; Department of Quantitative Analysis, College of Business Administration, King Saud University, Riyadh, Saudi Arabia; Department of Electrical Engineering, College of Engineering, King Saud University, Riyadh, Saudi Arabia; Department of Information Technology, Panimalar Engineering College, Poonamallee, Chennai, Tamil Nadu, India; Department of Information Technology, Muni University, Arua, Uganda",IEEE Access,"14 Aug 2024","2024","12","","110331","110352","This paper aims to establish a virtual object management system, as well as optimal task scheduling using the foundation of Digital Twins (DT), to improve the user’s experience with management and to accomplish the task efficiently. On the other hand, offloading tasks using IoT gadgets to edge computing, fails to speed up control by users. The capabilities of the DT are provided by executing processes such as visualization, virtualization, synchronization, and simulation. The optimal selection of the virtual objects for the DT is done by utilizing the implemented Hybrid Energy Valley with Lévy Flight Distribution Optimization (HEV-LFDO) in order to optimally offload the task by the edge devices. The optimal selection of the virtual objects is done with the aid of the HEV-LFDO in the DT by considering the total cost of executing all tasks using the selected virtual objects and the decision variables to determine whether a virtual object is taken for executing a task or not as the constraint. The data for performing resource management is secured using the blockchain or distributed ledger technology. This accounts for the minimization of the local loss function. Finally, the secured data is considered for optimal resource management tasks. The optimal resource management is done using the same HEV-LFDO. This optimal resource management is carried out by considering the constraints like the cost of assigning a virtual object for the task to the edge device, and the cost of assigning the task to the edge device. These two costs are analyzed by taking the network’s bandwidth, energy consumption, and computational resources into consideration. Experimental verifications are conducted on the executed optimal resource management scheme to prove the ability of the implemented model to be integrated with the edge computing network. The overall processing time as well as the latency are also minimized by executing the optimal resource management scheme.","2169-3536","","10.1109/ACCESS.2024.3435847","King Saud University, Riyadh, Saudi Arabia, through Researchers Supporting Program(grant numbers:RSPD2024R1052); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10614449","Secure and efficient blockchain;distributed ledger technology;optimal resource management;digital twin beyond 5G networks;hybrid energy valley with lévy flight distribution optimization","Resource management;Task analysis;Blockchains;Costs;Optimization;Distributed ledger;5G mobile communication;Security;Digital twins;Object detection;Virtualization","","","","29","CCBYNCND","30 Jul 2024","","","IEEE","IEEE Journals"
"Empowering Healthcare With Cyber-Physical System—A Systematic Literature Review","H. M. Khater; F. Sallabi; M. A. Serhani; E. Barka; K. Shuaib; A. Tariq; M. Khayat","College of Information Technology, United Arab Emirates University, Al Ain, United Arab Emirates; College of Information Technology, United Arab Emirates University, Al Ain, United Arab Emirates; College of Computing and Informatics, University of Sharjah, Sharjah, United Arab Emirates; College of Information Technology, United Arab Emirates University, Al Ain, United Arab Emirates; College of Information Technology, United Arab Emirates University, Al Ain, United Arab Emirates; College of Information Technology, United Arab Emirates University, Al Ain, United Arab Emirates; College of Information Technology, United Arab Emirates University, Al Ain, United Arab Emirates",IEEE Access,"18 Jun 2024","2024","12","","83952","83993","Cyber-physical systems (CPS) are recognized for their intelligence as they seamlessly interact with humans, enhancing the physical world through computation, communication, and control. Over the last few years, the evolution of CPS, including IoT components, has significantly impacted many facets of people’s lifestyles. It has been immersed in a wide range of services and applications in various areas, including manufacturing, healthcare, and energy. However, the interrelationship between the cyber and physical worlds gives rise to a multitude of research problems and challenges. In the healthcare field, for instance, CPS introduces complexities related to interoperability, privacy, data security, and real-time data processing with critical implications for the reliability and safety of medical processes. To address these challenges and harness the full potential of CPS in healthcare, this paper, through a comprehensive literature review, aims to discuss cutting-edge CPS technologies and solutions that hold promise for healthcare applications. To this end, we propose a comprehensive architectural model that can serve as a benchmark for implementing CPS in healthcare applications. This model thoroughly details how services, components, and technologies can be integrated to transform massive raw data collected from the physical world into valuable information for an enhanced decision-making process. Finally, a use case on healthcare CPS is presented, outlining its characteristics, the role that different technologies have played in its development, and the major challenges in implementing such systems successfully. This study provides a cohesive understanding of the role CPS can play in empowering healthcare while offering insights into the challenges, and future research trends.","2169-3536","","10.1109/ACCESS.2024.3407376","United Arab Emirates University(grant numbers:31R227); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10542115","Cyber-physical systems;healthcare;real-time processing;IoT","Medical services;Sensors;Real-time systems;Monitoring;Intelligent sensors;Cloud computing;Cyber-physical systems;Internet of Things","","","","252","CCBYNCND","30 May 2024","","","IEEE","IEEE Journals"
"A Survey of Industrial AIoT: Opportunities, Challenges, and Directions","K. S. Awaisi; Q. Ye; S. Sampalli","Faculty of Computer Science, Dalhousie University, Halifax, NS, Canada; Faculty of Computer Science, Dalhousie University, Halifax, NS, Canada; Faculty of Computer Science, Dalhousie University, Halifax, NS, Canada",IEEE Access,"19 Jul 2024","2024","12","","96946","96996","Internet of Things (IoT) is an important technology employed in a variety of different applications, such as transportation, healthcare, and manufacturing. In recent years, the number of IoT devices deployed globally has been increasing at a rapid pace and is estimated to reach 20 billion by the end of 2025. In modern industry, IoT plays a pivotal role by monitoring the condition of industrial machines and, consequently, improving the efficiency of industrial processes. To optimize the efficiency of industrial IoT applications, various Artificial Intelligence (AI) techniques have been adopted, leading to a new computing paradigm, namely, Industrial Artificial Intelligence of Things (i.e. Industrial AIoT). In this paper, we describe the challenges to tackle and the opportunities to explore in Industrial AIoT. Specifically, we first review the use of state-of-the-art AI methods in Industrial AIoT applications, with a focus on Deep Learning (DL) and Machine Learning (ML) techniques. Thereafter, we present a series of important applications of Industrial AIoT. The key challenges associated with the implementation of Industrial AIoT applications are also discussed. In addition, the societal and economic impacts of Industrial AIoT are briefly described. Finally, we outline the future research directions in Industrial AIoT, which should be further investigated to fully utilize the potential of this innovative technology.","2169-3536","","10.1109/ACCESS.2024.3426279","Natural Sciences and Engineering Research Council of Canada(grant numbers:RGPIN-2022-04754); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10591982","Internet of Things (IoT);industrial internet of things (IIoT);artificial intelligence (AI);artificial intelligence of things (AIoT);industrial AIoT;machine learning (ML);deep learning (DL)","Artificial intelligence;Industrial Internet of Things;Sensors;Industries;Monitoring;Logic gates;Internet of Things;Machine learning;Deep learning","","","","371","CCBYNCND","10 Jul 2024","","","IEEE","IEEE Journals"
"A Distributed Intelligent Service Trusted Provision Approach for IoT","Y. Qi; S. Shao; S. Wu; X. Qiu; S. Guo; S. Guo","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Grid Ningxia Information and Telecommunication Company Ltd., Ningxia, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Department of Computing and the Research Institute for Sustainable Urban Development, Hong Kong Polytechnic University, Kowloon, Hong Kong; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China",IEEE Internet of Things Journal,"11 Dec 2023","2023","10","24","22341","22355","The traditional centralized resource scheduling method leads to trust issues among multiple subjects carrying microservices. At the same time, in the process of service provision, single-point failure problems also occur from time to time. In order to realize the trusted provision of services, we build a blockchain-based distributed intelligent service trusted provision architecture, which uses smart contracts to realize the on-chain registration of resource information and automatic orchestration of microservices. In order to break through the bottleneck of blockchain efficiency and improve scalability, we use sharding technology to expand the blockchain. And the Raft-practical Byzantine fault-tolerance two-level consensus mechanism combining the Boneh–Lynn–Sacham (BLS) threshold signature (B-RBFT) is designed for blockchain sharding, which greatly improves throughput and reduces consensus delay while taking security into account. To meet higher Quality-of-Service (QoS) requirements, we design the microservice orchestration algorithm based on the improved double deep  $Q$  network (DDQN) to support microservice deployment and migration. In particular, to make the neural network converge faster, we improve the traditional DDQN framework by using double replay buffers and weighted target values. Simulation results show that our proposed algorithm has advantages in convergence speed, resource usage cost, delay, and load balancing.","2327-4662","","10.1109/JIOT.2023.3303927","National Key Research and Development Program of China(grant numbers:2020YFB1005900); National Natural Science Foundation of China(grant numbers:62071070); BUPT Excellent Ph.D. Students Foundation(grant numbers:CX2022135); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10216783","Blockchain;consensus mechanism;Internet of Things (IoT);microservice orchestration;resource sharing","Microservice architectures;Blockchains;Internet of Things;Delays;Resource management;Costs;Computer architecture","","","","30","IEEE","14 Aug 2023","","","IEEE","IEEE Journals"
"Distributed Edge Cloud Proposal Based on VNF/SDN Environment","C. S. M. Babou; Y. Owada; M. Inoue; K. Takizawa; T. Kuri","National Institute of Information and Communications Technology, Koganei, Tokyo, Japan; National Institute of Information and Communications Technology, Koganei, Tokyo, Japan; National Institute of Information and Communications Technology, Koganei, Tokyo, Japan; National Institute of Information and Communications Technology, Sendai, Miyagi, Japan; National Institute of Information and Communications Technology, Koganei, Tokyo, Japan",IEEE Access,"17 Sep 2024","2024","12","","124619","124635","The integration of lightweight virtualization and distributed management techniques effectively addresses various challenges in edge cloud environments, including optimizing service provision and reducing energy consumption. As the ICT sector transitions from traditional cloud computing to edge cloud to enhance proximity, it has adopted hypervisor-based virtualization. The emergence of beyond 5G systems, characterized by the need for autonomous orchestration, decentralized processing, ultra-low latency, and high throughput, necessitates a shift towards a distributed cloud system with autonomous functionalities. This infrastructure enables proactive control and management of network functions, allowing the delivery of services and microservices through a preconfigured setup at the edge. In regions prone to geographical instability, where network integrity can be compromised by natural disasters like earthquakes and tsunamis, establishing a robust, self-managing network is crucial. Such a network should be capable of independently providing services using local resources, functioning autonomously without external reliance, and effectively managing itself. This paper introduces NerveNet-VNF, a concept focusing on container-based virtualization of network functions. This system is coupled with SDN technology and integrated with a Dynamic Load Balancing (D-LB) system within a distributed cloud architecture. This integrated solution enables the hosting, management, and provisioning of local microservice requirements. Additionally, it introduces a new concept for splitting network functionality based on containers compared to the traditional abstract splitting system. Evaluation of this system demonstrates improvements in latency and throughput. Notably, there is a 20% reduction in NerveNet-VNF request execution time compared to the pre-existing NerveNet network. Moreover, integrating the distributed SDN controller technique into the distributed cloud system (HEC) has reduced latency by 88.91% and improved throughput by 28.48%.","2169-3536","","10.1109/ACCESS.2024.3454357","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10664513","Edge cloud;NerveNet-VNF;autonomous orchestrator systems;D-LB;K3s;SDN;VNF;HEC;local data delivery;5G;OpenDayLight;mininet","Cloud computing;Load management;Computer architecture;Virtualization;Throughput;Containers;Edge computing;5G mobile communication;Virtualization;Software defined networking","","","","43","CCBYNCND","4 Sep 2024","","","IEEE","IEEE Journals"
"When Holographic Communication Meets Metaverse: Applications, Challenges, and Future Trends","A. V. Shvetsov; S. Hamood Alsamhi","Department of Road Transport and Car Service Operations, North-Eastern Federal University, Yakutsk, Russia; ICT, Bahrain Polytechnic, Isa Town, Bahrain",IEEE Access,"31 Dec 2024","2024","12","","197488","197515","Holographic communication represents a transformative technology for reshaping the digital interaction landscape by enabling the creation of realistic, immersive, and interactive 3D experiences. This survey overviews holographic communication and its integration with the Metaverse technologies’ concepts, advantages, uses, and many applications. Furthermore, we examine a new paradigm for integrating holographic communication with the Metaverse, emphasizing how holography enhances immersive quality of virtual environments within the Metaverse, making interactions more lifelike and engaging. Extending the integration of holographic communication and Metaverse, we examine this combination’s numerous uses in various applications across various industries, such as education where virtual classrooms and 3D simulations redefine remote learning, a business where virtual meetings and product demonstrations create more impactful customer engagements, entertainment where immersive gaming and 3D broadcasting transform user experiences, healthcare where remote consultations and surgical simulations enhance medical training and accessibility, and remote assistance where real-time holographic support improves technical troubleshooting. In addition, we discuss the challenges and prospects of integration of holographic communication into Metaverse ecosystems, emphasizing key approaches and technical developments emerging technologies such as AI-driven content optimization, advanced coding and compression techniques, and new paradigms like terahertz communication and quantum holography. This survey highlights the revolutionary potential of holographic communication and offers insightful information on how it will influence digital engagement and connectivity in the future, eventually opening up new avenues in the quickly changing Metaverse landscape.","2169-3536","","10.1109/ACCESS.2024.3514576","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10788695","Extended reality;digital interaction;immersive communications;holographic communication;Metaverse;connectivity;virtual environments;interaction;immersive experiences","Metaverse;Surveys;Three-dimensional displays;Medical services;Education;Entertainment industry;Ecosystems;Business;Avatars;6G mobile communication","","","","162","CCBY","11 Dec 2024","","","IEEE","IEEE Journals"
"An Open API Architecture to Discover the Trustworthy Explanation of Cloud AI Services","Z. Wang; Y. Liu; J. Huang","Department of Electrical, Computer Engineering, Concordia University, Montréal, QC, Canada; Department of Electrical, Computer Engineering, Concordia University, Montréal, QC, Canada; Department of Electrical, Computer Engineering, Concordia University, Montréal, QC, Canada",IEEE Transactions on Cloud Computing,"6 Jun 2024","2024","12","2","762","776","This article presents the design of an open-API-based explainable AI (XAI) service to provide feature contribution explanations for cloud AI services. Cloud AI services are widely used to develop domain-specific applications with precise learning metrics. However, the underlying cloud AI services remain opaque on how the model produces the prediction. We argue that XAI operations are accessible as open APIs to enable the consolidation of the XAI operations into the cloud AI services assessment. We propose a design using a microservice architecture that offers feature contribution explanations for cloud AI services without unfolding the network structure of the cloud models. We can also utilize this architecture to evaluate the model performance and XAI consistency metrics showing cloud AI services’ trustworthiness. We collect provenance data from operational pipelines to enable reproducibility within the XAI service. Furthermore, we present the discovery scenarios for the experimental tests regarding model performance and XAI consistency metrics for the leading cloud vision AI services. The results confirm that the architecture, based on open APIs, is cloud-agnostic. Additionally, data augmentations result in measurable improvements in XAI consistency metrics for cloud AI services.","2168-7161","","10.1109/TCC.2024.3398609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10529172","Explainable AI (XAI);microservices;cloud model service;software architecture;software quality","Artificial intelligence;Cloud computing;Computer architecture;Computational modeling;Data models;Measurement;Microservice architectures","","","","56","IEEE","10 May 2024","","","IEEE","IEEE Journals"
"COPA: Experimenter-Level Container Orchestration for Networking Testbeds","H. C. C. de Resende; M. A. K. Schimuneck; C. B. Both; J. A. Wickboldt; J. M. Marquez-Barja","Imec, University of Antwerp, Antwerp, Belgium; Institute of Informatics, Federal University of Rio Grande do Sul (UFRGS), Porto Alegre, Brazil; Applied Computing Graduate Program, University of Vale do Rio dos Sinos (UNISINOS), São Leopoldo, Brazil; Institute of Informatics, Federal University of Rio Grande do Sul (UFRGS), Porto Alegre, Brazil; Imec, University of Antwerp, Antwerp, Belgium",IEEE Access,"18 Nov 2020","2020","8","","201781","201798","As Cloud Computing (CC) branched areas such as Multi-access Edge Computing (MEC) and Fog computing are still on growing research interest. The creation of new tools to improve quality and speed the experimentation in such areas is a general interest. In this article, we propose COPA, an experimenter-level container orchestration tool for networking testbeds. This tool provides a friendly interface for the experimenter test container orchestration algorithms which can start, stop, copy, and even migrate a container from one host to another. COPA also includes network/resources monitoring to feed the experimenter's orchestration algorithm so that it can make decisions based on real-time environment information. Furthermore, the experimenter can automatize the experiment scenario setup and deployment by pre-configuring in COPA. This tool helps the experimenter in testing different scenarios and quickly changing experiment parameters. Considering these features, COPA aims to provide an experimentation architecture to deploy and test container orchestration algorithms. Furthermore, we provide a case study explaining how COPA can be a key tool in the MEC and Network Function Virtualization (NFV) experimentation environments. This tool was already deployed in Federated Union of Telecommunications Research Facilities for an EU-Brazil Open Laboratory (FUTEBOL) testbeds as part of the control framework and was well validated by the project reviewers and partners.","2169-3536","","10.1109/ACCESS.2020.3035619","European Union’s Horizon 2020 through the 5G for Connected and Automated Road Mobility in the European unioN (5 G-CARMEN) Project(grant numbers:825012); Federated Union of Telecommunications Research Facilities for an EU-Brazil Open Laboratory (FUTEBOL) Project(grant numbers:688941); Brazilian Ministry of Science, Technology, Innovation, and Communication (MCTIC) through Rede Nacional de Ensino e Pesquisa (RNP), Centro de Tecnologia da Informação e Comunicação (CTIC), and Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9247110","Testbed;multi-access edge computing;future networks;container;orchestration","Containers;Tools;Monitoring;Virtualization;Computer architecture;Graphical user interfaces;Edge computing","","","","45","CCBY","3 Nov 2020","","","IEEE","IEEE Journals"
"MTDA: Efficient and Fair DPU Offloading Method for Multiple Tenants","Z. Huang; Y. Tan; Y. Zhu; H. Tan; K. Li","College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China",IEEE Transactions on Services Computing,"31 Dec 2024","2024","17","6","3971","3984","In modern cloud computing environment, the offloading potential of DPU must be fully exploited for multiple tenants. Existing DPU offloading techniques lack the capability to perform the fair allocation of a DPU domain's internal resources among tenants with various performance requirements. In this article, we propose a virtual multi-channel DPU offloading architecture for multiple tenants (MTDA) and implement it on a BlueField-2 DPU platform to achieve stability and fairness in resource allocation for generic datacenter tasks. MTDA provides an independent virtual channel for each tenant before their requests are submitted to avoid competition among tenants. Considering the diverse requirements of tenants, MTDA constructs a credit-based resource allocation model and a traffic-aware scheduling algorithm to fully utilize the rich computing resources of DPU and improve the fairness of DPU resource allocation. Experimental results show that MTDA increases the throughput by up to 101.2%, 143.2%, 36.1%, and 41.7%, lowers the latency by up to 50.3%, 58.9%, 26.6%, and 29.4%, improves the fairness by up to 98.8%, 99.0%, 98.3%, and 98.4%, and provides more stable performance for multi-tenants, compared with DPDK, iPipe, FairNIC, and LogNIC.","1939-1374","","10.1109/TSC.2024.3433588","construction of innovative provinces in Hunan Province(grant numbers:2021GK4012); National Natural Science Foundation of China(grant numbers:62302158); Natural Science Foundation of Hunan Province(grant numbers:2023JJ40175); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10609547","Credit model;DPU;fair resource allocation;multi-tenant","Resource management;Cloud computing;Task analysis;Dynamic scheduling;Computational modeling;Throughput;Multicore processing","","","","52","IEEE","25 Jul 2024","","","IEEE","IEEE Journals"
"Edge Computing Management With Collaborative Lazy Pulling for Accelerated Container Startup","C. -C. Chen; Y. Chiang; Y. -C. Lee; H. -Y. Wei","Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan",IEEE Transactions on Network and Service Management,"19 Dec 2024","2024","21","6","6437","6450","With the growing demand for latency-sensitive applications in 5G networks, edge computing has emerged as a promising solution. It enables instant response and dynamic resource allocation based on real-time network information by moving resources from the cloud to the network edge. Containers, known for their lightweight nature and ease of deployment, have been recognized as a valuable virtualization technology for service deployment. However, the prolonged startup time of containers can lead to long response time, particularly in edge computing scenarios characterized by long propagation time, frequent deployment, and migration. In this paper, we comprehensively consider image caching, container assignment, and registry selection problem in an edge system. To our best effort, there is no existing work that has taken all the above aspects into account. To address the problem, we propose a novel image caching strategy that employs partial caching, allowing local registries to cache either the least functional or complete version of application images. In addition, a container assignment and registry selection problem is solved by using an edge-based collaborative lazy pulling algorithm. To evaluate the performance of our proposed algorithms, we conduct experiments with real-world app usage data and popular images in a testbed environment. The experimental results demonstrate that our algorithms outperform traditional greedy algorithms in terms of average user response time and cache hit rate.","1932-4537","","10.1109/TNSM.2024.3462408","National Science and Technology Council (NSTC) of Taiwan(grant numbers:113-2628-E-002-032-); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10681577","Edge computing;5G;container;caching;resource allocation","Containers;Image edge detection;Edge computing;Time factors;Delays;Costs;Collaboration","","","","46","IEEE","17 Sep 2024","","","IEEE","IEEE Journals"
"Security Risks and Designs in the Connected Vehicle Ecosystem: In-Vehicle and Edge Platforms","M. De Vincenzi; J. Moore; B. Smith; S. E. Sarma; I. Matteucci","Institute of Informatics and Telematics, Pisa, Italy; Ford Motor Company, Dearborn, MI, USA; Ford Motor Company, Dearborn, MI, USA; Department of Mechanical Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA; Institute of Informatics and Telematics, Pisa, Italy",IEEE Open Journal of Vehicular Technology,"4 Feb 2025","2025","6","","442","454","The evolution of Connected Vehicles (CVs) has introduced significant advancements in both in-vehicle and vehicle-edge platforms, creating a highly connected ecosystem. These advancements, however, have heightened exposure to cybersecurity risks. This work reviews emerging security challenges in the CV ecosystem from a new perspective, focusing on the integration of in-vehicle platforms such as the infotainment system and vehicle-edge platforms. By analyzing case studies such as Android Automotive, Message Queuing Telemetry Transport (MQTT), and the Robot Operating System (ROS), we identify the primary security threats, including malware attacks, data manipulation, and Denial of Service (DoS) attacks. The discussion extends to privacy concerns and the lack of trust-building mechanisms in CVs, highlighting how these gaps can be exploited. To mitigate these risks, we retrieve solutions drawn from the broader field of Internet of Things (IoT) security research, including Multi-Factor Authentication (MFA) and trust-based systems. The proposed framework aims to increase the trustworthiness of devices within the CV ecosystem. Finally, we identify future research directions in adaptive mechanisms and cross-domain security.","2644-1330","","10.1109/OJVT.2024.3524088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10818588","Connected vehicles;security;privacy;android automotive;message queuing telemetry transport;robot operating system","Vehicle-to-everything;Security;Ecosystems;Operating systems;Privacy;Malware;Prevention and mitigation;Image edge detection;Connected vehicles;Computer architecture","","","","118","CCBY","30 Dec 2024","","","IEEE","IEEE Journals"
"Enhancing Agriculture Through IoT and Blockchain: A VeChain-Enhanced Sustainable Development Approach for Small-Scale Agricultural Exploitations","R. L. P. Urtecho; J. Rodríguez-Molina; M. Martínez-Núñez; J. Garbajosa","Department of Telematics and Electronics Engineering, Universidad Politécnica de Madrid, Madrid, Spain; Department of Telematics and Electronics Engineering, Universidad Politécnica de Madrid, Madrid, Spain; Department of Organization Engineering, Business Administration and Statistics, Universidad Politécnica de Madrid, Madrid, Spain; Higher Technical School of Engineering of Information Systems, Universidad Politécnica de Madrid, Madrid, Spain",IEEE Access,"5 Dec 2024","2024","12","","179962","179980","Small and mid-sized exploitations in the agricultural sector face several challenges, including excessive use of environmental resources, limited product visibility, and insufficient automation in farming operations. These issues result in increased operational costs, reduced crop quality, and higher risks of harvest loss, especially when weather predictions are unreliable. Addressing these challenges is crucial to improving productivity and sustainability in farming. This research work introduces a cost-effective Software-as-a-Service (SaaS) solution that leverages Internet of Things (IoT) technology and blockchain -specifically, the VeChain network- to provide real-time data on crop conditions and weather forecasts. The proposed solution enables farmers to make informed decisions by offering actionable insights and monitoring crops growth and general progress over time, thus promoting more efficient and sustainable farming practices. Furthermore, the integration of blockchain ensures transparency and immutability of data, building trust between farmers, distributors, and end customers. This increased visibility and accountability can also enhance product traceability across supply chains. By tackling these issues faced by small and mid-sized agricultural exploitations, this project aims to modernize farming practices, optimize resource utilization, and contribute to long-term sustainability, helping to secure food production in the face of global environmental and economic turmoil.","2169-3536","","10.1109/ACCESS.2024.3506510","Agencia Estatal de Investigación/Ministerio de Ciencia e Innovación through the project “Sustainability-Aware Internet of Things Systems Driven by Social Communities”(grant numbers:PID2020-118969RB-I00); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10767676","Distributed information systems;Internet of Things;sensors;cloud computing","Blockchains;Internet of Things;Security;Crops;Real-time systems;Farming;Monitoring;Agriculture;Supply chains;Weather forecasting","","","","40","CCBY","26 Nov 2024","","","IEEE","IEEE Journals"
"An Evaluation of Unobtrusive Sensing in a Healthcare Case Study","J. E. Rivadeneira; J. M. Fernandes; A. Rodrigues; F. Boavida; J. S. Silva","Centre for Informatics and Systems of the University of Coimbra (CISUC), Coimbra, Portugal; Centre for Informatics and Systems of the University of Coimbra (CISUC), Coimbra, Portugal; Polytechnic University of Coimbra, Coimbra Business School | ISCAC, Bencanta, Coimbra, Portugal; Centre for Informatics and Systems of the University of Coimbra (CISUC), Coimbra, Portugal; Institute for Systems Engineering and Computers at Coimbra (INESC Coimbra), University of Coimbra, Coimbra, Portugal",IEEE Access,"2 Jul 2024","2024","12","","89405","89417","This paper examines the integration of Human-in-the-Loop Cyber-Physical Systems (HiTL-CPS) and Unobtrusive Sensing through a case study named iFriend. Our approach enhances the data acquisition phase of HiTLCPS by integrating unobtrusive sensing techniques to monitor real-time heart rate and breathing rate. This is achieved by leveraging Channel State Information (CSI) of Wi-Fi signals, specifically focusing on its amplitude information. This integration facilitates seamless interaction between humans and the cyber-physical environment. We detail the architecture of the iFriend system, comprising sensors, actuators, and computational units forming a closed-loop control mechanism. The unobtrusive sensing module is specifically designed to capture physiological changes without causing discomfort or interfering with daily activities, making it well-suited for healthcare applications and human-machine interaction. We assess iFriend in an experimental setting, demonstrating its feasibility, with between 80% and 90% of estimates hovering around 2.5 breaths per minute for BR or 10 beats per minute for HR, respectively.","2169-3536","","10.1109/ACCESS.2024.3419555","Fundação para a Ciência e a Tecnologia (FCT), I.P.(grant numbers:UIDB/00326/2020,UIDP/00326/2020,UIDB/00308/2020,DOI 10.54499/UIDB/00308/2020); European Social Fund through the Regional Operational Program Centro 2020; FCT through the Ph.D.(grant numbers:SFRH/BD/147371/2019); Secretaría de Educación Superior, Ciencia, Tecnología e Innovación del Ecuador (SENESCYT) under Ph.D.(grant numbers:CZ02-000903-2018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10571912","CSI;human-in-the-loop;IoT;unobtrusive sensing","Sensors;Biomedical monitoring;Monitoring;Machine learning;Wearable sensors;Medical services;Human in the loop","","","","38","CCBY","26 Jun 2024","","","IEEE","IEEE Journals"
"Multi-Layered Continuous Reasoning for Cloud-IoT Application Management","J. L. Herrera; J. Berrocal; S. Forti; A. Brogi; J. M. Murillo","Dipartimento di Informatica-Scienza e Ingegneria, University of Bologna, Bologna, Italy; Department of Computer Science and Communications Engineering, University of Extremadura, Badajoz, Spain; Department of Computer Science, University of Pisa, Pisa, Italy; Department of Computer Science, University of Pisa, Pisa, Italy; Department of Computer Science and Communications Engineering, University of Extremadura, Badajoz, Spain",IEEE Transactions on Services Computing,"31 Dec 2024","2024","17","6","3985","3998","The advent of the Internet of Things has increased the interest in automating mission-critical processes from domains such as smart cities. These applications’ stringent Quality of Service (QoS) requirements motivate their deployment through the Cloud-IoT Continuum, which requires solving the NP-hard problem of placing the application's services onto the infrastructure's devices. Moreover, as the infrastructure and application change over time, the placement needs to continuously adapt to these changes to maintain an acceptable QoS. While continuous reasoning techniques have enabled the creation of tools for these scenarios, they can have some trouble finding a feasible adaptation for abrupt and sharp changes, requiring non-adaptive techniques in those cases. Furthermore, for scenarios with smoother changes, it would be desirable to have faster algorithms to perform this placement. To explore the trade-off of effectiveness and execution times of different methods while ensuring that an application placement is found, we propose Multi-Layered Continuous Reasoning (MLCR) as an autonomic framework to adapt application placements through multiple continuous reasoning-based methods. We also present an MLCR prototype based on three methods: Faustum, MigDADO, and ConDADO. An evaluation in a realistic use case shows that MLCR is faster than traditional methods for application placement and maintains an acceptable QoS.","1939-1374","","10.1109/TSC.2024.3451239","CAS21/00057 (MCI/AEI/FEDER, UE)(grant numbers:PID2021-124054OB-C31,DIN2020-011586,PDC2022-133465-I00); MCIN/AEI/10.13039/50100011033(grant numbers:TED2021-130913B-I00); European Union NextGenerationEU/PRTR; Department of Economy, Science and Digital Agenda of the Government of Extremadura(grant numbers:GR21133); European Regional Development Fund; European Union; Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU; Telecommunications of the Future; PE00000001 - program “RESTART”; CUP(grant numbers:J33C22 002880001); Energy-aware management of software applications in Cloud-IoT ecosystems(grant numbers:RIC2021_ PON_A18); ESF REACT-EU resources; Italian Ministry of University and Research; PON Ricerca e Innovazione 2014–20; hOlistic Sustainable Management of distributed softWARE systems (OSMWARE)(grant numbers:UNIPI PRA_2022_64); University of Pisa, Italy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10654521","Internet of Things (IoT);fog computing;quality of service (QoS);continuous reasoning;service placement","Microservice architectures;Quality of service;Cognition;Cloud computing;Internet of Things;Costs;Smart cities","","","","45","IEEE","28 Aug 2024","","","IEEE","IEEE Journals"
"Dynamic Swarm Orchestration and Semantics in IoT Edge Devices: A Systematic Literature Review","B. Anuraj; D. Calvaresi; J. -M. Aerts; J. -P. Calbimonte","Institute of Informatics, University of Applied Sciences and Arts Western Switzerland (HES-SO), Sierre, Switzerland; Institute of Informatics, University of Applied Sciences and Arts Western Switzerland (HES-SO), Sierre, Switzerland; Department of Biosystems, Measure, Model & Manage Bioresponses Research Group, KU Leuven, Leuven, Belgium; Institute of Informatics, University of Applied Sciences and Arts Western Switzerland (HES-SO), Sierre, Switzerland",IEEE Access,"29 Aug 2024","2024","12","","116917","116938","In its dynamic evolution, the Internet of Things (IoT) has become increasingly pervasive in our daily lives, ranging from domestic appliances to industrial robots. This integration brings together people, processes, data, and devices, prompting new types of interactions among them. Besides acquiring data, these devices also have actuation and processing capabilities, making them susceptible of becoming autonomous entities with coordination potential. Given the inherent limitations of storage, power, or computation of IoT devices, delegation and cooperation strategies, including intermediary nodes in the network, can significantly optimize the usage of resources. Hence, this type of node can rely on swarm-inspired intelligence to orchestrate edge nodes, possibly with semantics-enabled behaviors. This study proposes a Systematic Literature Review (SLR) investigating different solutions and approaches for the orchestration of edge devices powered by declarative and semantic models of their affordances, goals, and capabilities. The SLR explores different aspects of the literature, including demographics, application domains, goals, requirements, scope, services, frameworks, and technologies, as well as challenges and future directions in the field. The purpose of this SLR is to provide software engineers, researchers, and innovators comprehensive insights into the present status of advancements in this area and a discussion of the unresolved issues and opportunities.","2169-3536","","10.1109/ACCESS.2024.3446876","Horizon Europe Project SmartEdge(grant numbers:101092908); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10643067","Internet of Things;swarm-based orchestration;dynamic orchestration;semantic sensor networks;edge devices;Web of Things","Semantics;Internet of Things;Task analysis;Protocols;Planning;Particle swarm optimization;Ontologies","","","","86","CCBY","21 Aug 2024","","","IEEE","IEEE Journals"
"Efficient task offloading and resource allocation in an intelligent UAV-MEC system","B. Picano; R. Fantacci","University of Florence, 50139 Firenze, Italy; University of Florence, 50139 Firenze, Italy",Journal of Communications and Networks,"8 Jan 2025","2024","26","6","666","678","Nowadays, the functional integration of digital twin (DT) technology and artificial intelligence (AI) methodologies has enabled reliable predictions of many random processes, supporting efficient control and optimization procedures. In line with this trend, this paper explores the joint use of these technologies in an AI-empowered DT framework for an unmanned aerial vehicle-aided multi-access edge computing (UAV-MEC) system. Specifically, this approach defines an intelligent UAV-MEC system capable of significantly improving service quality and deployment flexibility. The focus is on a UAV-MEC network consisting of multiple elementary service areas, where DTs efficiently orchestrate and reduce congestion levels by utilizing UAVs with onboard processing capabilities. A potential architecture for the DTs is outlined, conceptualizing each DT as a collection of basic cyber entities. Additionally, a suitable framework utilizing a matching game approach is proposed to effectively manage task offloading, channel allocation, and the dynamic assignment of UAV support to congested service zones within the same area. Finally, comprehensive simulation results validate the efficacy of the proposed intelligent UAV-MEC system, as indicated by metrics such as task completion delay and accuracy in congestion prediction.","1976-5541","","10.23919/JCN.2024.000050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10759571","Digital twin;intelligence system;machine learning;mobile edge computing;unmanned aerial vehicle","Autonomous aerial vehicles;Resource management;Artificial intelligence;Optimization;Drones;Delays;Decision making;Costs;6G mobile communication;Trajectory","","","","","","20 Nov 2024","","","KICS","KICS Journals"
"Delegating Data Plane With Cloud-Assisted Routing","P. K. Dey; M. Yuksel","Department of Electrical and Computer Engineering, University of Central Florida, Orlando, FL, USA; Department of Electrical and Computer Engineering, University of Central Florida, Orlando, FL, USA",IEEE Transactions on Network and Service Management,"9 Oct 2023","2023","20","3","3190","3204","Data centers are embracing the software-defined networking (SDN) as it is evident that this technology of completely separating the data plane from the control plane gives more flexibility for their internal routing management and provides better Quality-of-Service (QoS) to the users. Implementing a similar solution to serve the purpose of software-defined wide-area networking using public Internet routing is also gaining popularity. Although, instead of having a complete separation, a hybrid approach to keep most of the control plane along with the least of the data plane in the remote and vice-versa for the local platform may be more fitting. To this end, we propose a new hybrid SDN approach, Cloud-Assisted Routing (CAR), that utilizes the high computational services that cloud offers at a lower price by splitting both control and data plane functions between a local router and a remote cloud computing platform. Such delegation of data plane to a third-party authority requires proper control plane management policies and handling or avoiding possible loops and failures. We detail the architectural view of CAR, address its associated challenges, and present prototype-based evaluations of it for reducing routing table sizes.","1932-4537","","10.1109/TNSM.2023.3239802","U.S. National Science Foundation(grant numbers:1814086,1647189); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10025801","Routing scalability;software-defined net-working;cloud-assisted networking","Cloud computing;Routing;Automobiles;Computer architecture;Internet;Scalability;Switches","","","","67","IEEE","25 Jan 2023","","","IEEE","IEEE Journals"
"Integrated Cognitive Symbiotic Computing and Ambient Backscatter Communication Network","C. Ren; Y. Hu; L. Sun; H. Li; C. Sun; H. Zhang; A. Nallanathan; V. C. M. Leung","Beijing Engineering and Technology Research Center for Convergence Networks and Ubiquitous Services, University of Science and Technology Beijing, Beijing, China; Beijing Engineering and Technology Research Center for Convergence Networks and Ubiquitous Services, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Research and Development Center Sony (China) Ltd., Beijing, China; Research and Development Center Sony (China) Ltd., Beijing, China; Beijing Engineering and Technology Research Center for Convergence Networks and Ubiquitous Services, University of Science and Technology Beijing, Beijing, China; School of Electronic Engineering and Computer Science, Queen Mary University of London, London, U.K.; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada",IEEE Transactions on Cognitive Communications and Networking,"7 Oct 2024","2024","10","5","1635","1649","Ambient backscatter communication (AmBC) possesses signal reception and energy-harvesting capabilities, allowing providing wireless cognition through simple energy detection. In typical applications like industrial Internet of Things (IoT), cognitive AmBC (CAmBC) networks are required to offer passive communication, edge computing, and cognition capabilities. However, passive communication relies on the environment and has limited computing power, creating interdependencies among spectrum sensing, networking, and computational cognition. Moreover, the heterogeneous evaluation metrics for communication and computation make unified planning and management challenging. Therefore, this paper proposes the integrated cognitive symbiotic computing-AmBC (CSC-AmBC) based on symbiotic communication and cognitive radio. CSC-AmBC integrates AmBC communication and computational cognition capabilities in a task-oriented manner, sharing proximity and AmBC computing and communication (ACC) resources among primary and secondary tasks. Meta-Link with Tokens and two cognitive ACC reuse models is used to facilitate integration and enhance task execution efficiency, which introduces Places to accommodate the heterogeneous and variable ACC resources. Additionally, the task execution gain metric is introduced to evaluate the multi-task ACC resource utilization. Numerical results validate the cognition networking and the advantage of the proposed task execution gain of CSC-AmBC.","2332-7731","","10.1109/TCCN.2024.3439628","National Natural Science Foundation of China(grant numbers:62201034,62225103,U22B2003,62341103); Beijing Municipal Natural Science Foundation(grant numbers:L212004-03); Beijing Natural Science Foundation(grant numbers:L212004); Xiaomi Fund of Young Scholar, and National Key Laboratory of Wireless Communications Foundation(grant numbers:IFN20230201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10623808","Integrated computing and communication;ambient backscatter communication;cognitive networks","Task analysis;Cognition;Backscatter;Wireless sensor networks;Resource management;Measurement;Symbiosis","","","","34","IEEE","6 Aug 2024","","","IEEE","IEEE Journals"
"Efficient Container Scheduling With Hybrid Deep Learning Model for Improved Service Reliability in Cloud Computing","J. Jeon; S. Park; B. Jeong; Y. -S. Jeong","Department of AI⋅SW Convergence, Dongguk University, Seoul, Republic of Korea; Department of Multimedia Engineering, Dongguk University, Seoul, Republic of Korea; Department of Computer Science and Artificial Intelligence, Dongguk University, Seoul, Republic of Korea; Department of AI⋅SW Convergence, Dongguk University, Seoul, Republic of Korea",IEEE Access,"10 May 2024","2024","12","","65166","65177","In a cloud computing environment, the container scheduling technique ensures reliability for containerized applications by selecting nodes that satisfy various resource requirements and then deploying containers. If the initial resources of a container are over-allocated, resources may be wasted, or other containers that are waiting in a scheduling queue may not be allocated. However, if resources are under-allocated, service disruptions may occur due to node overbooking, and service reliability cannot be ensured. Therefore, in this study, a forecasted resource-evaluating scheduler (FoRES) is proposed as a container scheduling technique that ensures resource efficiency and service reliability. FoRES predicts future CPU and memory usage by using a time-series decomposition-based hybrid forecasting (DeHyFo) model that combines multiple linear regressions with the LightTS model. FoRES then calculates the optimal scheduling decisions that minimize idle resources and node overload by applying an efficient resource utilization (SERU) scoring function to the predicted resource usage. Evaluating the performance of FoRES based on various scenarios improved resource efficiency and service reliability by up to 2.07 and 2.32 times, respectively, compared with existing scheduling techniques, even if the initial resources of the container were inefficiently allocated.","2169-3536","","10.1109/ACCESS.2024.3396652","Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT)(grant numbers:2022-0-00047); Development of microservices development/operation platform technology that supports application service operation intelligence; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10518053","Cloud computing;container scheduling;deep learning;resource efficiency;service reliability;time-series forecasting","Containers;Optimal scheduling;Scheduling;Predictive models;Processor scheduling;Resource management;Measurement;Cloud computing;Deep learning;Time series analysis;Reliability","","","","32","CCBYNCND","3 May 2024","","","IEEE","IEEE Journals"
"Adaptive Security Framework for the Internet of Things: Improving Threat Detection and Energy Optimization in Distributed Environments","W. Villegas-Ch; R. Gutierrez; I. Sánchez-Salazar; A. Mera-Navarrete","Escuela de Ingeniería en Ciberseguridad, FICA, Universidad de Las Américas, Quito, Ecuador; Escuela de Ingeniería en Ciberseguridad, FICA, Universidad de Las Américas, Quito, Ecuador; Escuela de Ingeniería en Ciberseguridad, FICA, Universidad de Las Américas, Quito, Ecuador; Departamento de Sistemas, Universidad Internacional del Ecuador, Quito, Ecuador",IEEE Access,"1 Nov 2024","2024","12","","157924","157944","The increasing use of Internet of Things (IoT) devices in critical sectors has increased exposure to security threats, making protecting these systems a priority challenge. Based on static configurations, traditional security approaches have proven ineffective in the face of the dynamic nature of emerging threats, as they cannot adapt in real time to changes in the environment or new attack vectors. This work proposes an adaptive security framework for Internet of Things (IoT) systems capable of autonomously detecting, mitigating, and adapting to various threats, improving precision and response times, and optimizing energy consumption. The framework was implemented in a distributed Internet of Things environment, using adaptive architectures based on the Robot Operating System (ROS) and microservices orchestration with Kubernetes. The results showed a significant improvement in response time, with a reduction of 44%, reaching an average of 250 milliseconds, compared to 450 milliseconds for static approaches. Furthermore, a 92% precision in threat detection was achieved, reducing false positives to 4% and false negatives to 6%. Power consumption was controlled, reaching a maximum of 160 milliamp-hours after facing multiple threats, confirming the system’s efficiency in resource-limited environments. These results demonstrate that the proposed adaptive framework is a robust and efficient solution for security in Internet of Things environments, overcoming the limitations of traditional approaches and ensuring adequate protection without compromising energy efficiency.","2169-3536","","10.1109/ACCESS.2024.3486983","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10736580","Adaptive security framework;Internet of Things;threat detection;energy efficiency in Internet of Things","Security;Internet of Things;Adaptive systems;Real-time systems;Time factors;Threat assessment;Energy consumption;Energy efficiency;Sensors;Logic gates","","","","44","CCBYNCND","28 Oct 2024","","","IEEE","IEEE Journals"
"Decentralized Text-Based Person Re-Identification in Multi-Camera Networks","R. Agyeman; B. Rinner","Institute of Networked and Embedded Systems, University of Klagenfurt, Klagenfurt am Wörthersee, Austria; Institute of Networked and Embedded Systems, University of Klagenfurt, Klagenfurt am Wörthersee, Austria",IEEE Access,"22 Nov 2024","2024","12","","172125","172148","Text-based person re-identification (re-ID) is an emerging research domain in multi-camera surveillance systems. It involves identifying individuals across different camera views using textual descriptions as queries. Although text-based person re-ID holds significant potential for surveillance systems, its practical applications are limited by the high computational demands of existing algorithms. This is because most state-of-the-art algorithms prioritize identification accuracy over resource efficiency. Therefore, surveillance systems based on existing algorithms rely heavily on centralized architectures, where a central application server aggregates videos from camera nodes, generates galleries from the videos, and performs person identification. However, such centralized systems face challenges in large-scale deployments, including scalability issues, high bandwidth utilization, and processing bottlenecks. This paper presents a decentralized approach to text-based person re-ID to overcome these limitations. First, it introduces U-TextReIDNet, a resource-efficient model designed to identify persons in multi-person images on the NVIDIA Jetson Nano embedded board. U-TextReIDNet achieves Top-1 accuracies of 54.02% on the CUHK-PEDES dataset and 38.45% on the RSTPReid dataset. With only 38.77 million parameters, U-TextReIDNet is significantly smaller than most existing text-based person re-ID models. Using U-TextReIDNet, we implement a decentralized system that distributes the person identification task across camera nodes, transmitting only videos containing persons of interest to the command station. Additionally, we developed a prototype of this decentralized system, and conducted performance and usability tests using real human subjects. The prototype successfully performs real-time person re-ID, reduces bandwidth utilization, improves system scalability, and eliminates processing bottlenecks.","2169-3536","","10.1109/ACCESS.2024.3501382","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10756694","Decentralized algorithm;multi-camera networks;multi-modal re-identification model;real-time video analytics;text-based person re-identification","Cameras;Visualization;Identification of persons;Videos;Servers;Feature extraction;Computer architecture;Surveillance;Scalability;Privacy","","","","79","CCBY","18 Nov 2024","","","IEEE","IEEE Journals"
"Advancing 6G: Survey for Explainable AI on Communications and Network Slicing","H. Sun; Y. Liu; A. Al-Tahmeesschi; A. Nag; M. Soleimanpour; B. Canberk; H. Arslan; H. Ahmadi","Institute for Safe Autonomy and School of Physics, Engineering and Technology, York, U.K.; Institute for Safe Autonomy and School of Physics, Engineering and Technology, York, U.K.; Institute for Safe Autonomy and School of Physics, Engineering and Technology, York, U.K.; School of computer Science, University College Dublin, Dublin 4, Ireland; Texas Biomedical Research Institute, San Antonio, TX, USA; School of Computing, Engineering and The Built Environment, Edinburgh Napier University, Edinburgh, U.K.; School of Engineering and Natural Sciences, Istanbul Medipol University, Istanbul, Türkiye; Institute for Safe Autonomy and School of Physics, Engineering and Technology, York, U.K.",IEEE Open Journal of the Communications Society,"26 Feb 2025","2025","6","","1372","1412","The unprecedented advancement of Artificial Intelligence (AI) has positioned Explainable AI (XAI) as a critical enabler in addressing the complexities of next-generation wireless communications. With the evolution of the 6G networks, characterized by ultra-low latency, massive data rates, and intricate network structures, the need for transparency, interpretability, and fairness in AI-driven decision-making has become more urgent than ever. This survey provides a comprehensive review of the current state and future potential of XAI in communications, with a focus on network slicing, a fundamental technology for resource management in 6G. By systematically categorizing XAI methodologies–ranging from modelagnostic to model-specific approaches, and from pre-model to post-model strategies–this paper identifies their unique advantages, limitations, and applications in wireless communications. Moreover, the survey emphasizes the role of XAI in network slicing for vehicular network, highlighting its ability to enhance transparency and reliability in scenarios requiring real-time decision-making and high-stakes operational environments. Real-world use cases are examined to illustrate how XAI-driven systems can improve resource allocation, facilitate fault diagnosis, and meet regulatory requirements for ethical AI deployment. By addressing the inherent challenges of applying XAI in complex, dynamic networks, this survey offers critical insights into the convergence of XAI and 6G technologies. Future research directions, including scalability, real-time applicability, and interdisciplinary integration, are discussed, establishing a foundation for advancing transparent and trustworthy AI in 6G communications systems.","2644-125X","","10.1109/OJCOMS.2025.3534626","Engineering and Physical Sciences Research Council United Kingdom (EPSRC), Impact Acceleration Accounts (IAA) (Green Secure and Privacy Aware Wireless Networks for Sustainable Future Connected and Autonomous Systems)(grant numbers:EP/X525856/1); EPSRC CHEDDAR: Communications Hub for Empowering Distributed Cloud Computing Applications and Research(grant numbers:EP/Y037421/1,EP/X040518/1); Department of Science, Innovation and Technology, U.K.(grant numbers:Yorkshire Open-RAN (YORAN) TS/X013758/1,RIC Enabled (CF-)mMIMO for HDD (REACH) TS/Y008952/1); Scientific and Technological Research Council of Turkey (TUBITAK) 1515 Frontier RD Laboratories Support Program for BTS Advanced AI Hub: BTS Autonomous Networks and Data Innovation Lab(grant numbers:5239903); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10854503","Sixth Generation (6G);wireless communications;Explainable AI (XAI);Artificial Intelligent (AI);Machine Learning (ML);network slicing;vehicular networks","6G mobile communication;Artificial intelligence;Explainable AI;Surveys;Network slicing;Wireless communication;Decision making;Reviews;Security;Real-time systems","","","","237","CCBY","27 Jan 2025","","","IEEE","IEEE Journals"
"Optimizing UE Power Efficiency: AI/ML Approach for Upgrade Time Determination","K. Subramaniam; N. Kumar; S. Balusamy; C. Sahoo; G. Chandrasekaran","Management S/W Group, Samsung Research and Development Institute, Bengaluru, India; Bearer Software, Samsung Research and Development Institute, Bengaluru, India; SDN and Cloud Solution, Samsung Research and Development Institute, Bengaluru, India; Element Management System Software, Samsung Research and Development Institute, Bengaluru, India; Service Management Orchestrator and Slice Manager Part, Samsung Research and Development Institute, Bengaluru, India",IEEE Access,"9 Sep 2024","2024","12","","122878","122887","The transition of Control Plane (CP) block functions into software entities, as proposed by 3GPP, necessitates periodic downtime for maintenance activities such as software upgrades or failures. This downtime requires the disconnection of all User Equipment (UE) connections to the CP, triggering the UE reattach procedure and resulting in increased UE power consumption and spectrum wastage. To mitigate these challenges, optimal CP upgrade timings should align with periods of low traffic. In this paper, we propose an AI/ML-based procedure to autonomously determine the optimal time to upgrade CP block functions, eliminating the need for manual intervention by operators. Our approach involves analyzing traffic conditions using statistical data from several CP blocks managing base stations across various areas, including residential and non-residential zones like subways, shopping complex and hospitals. Leveraging Seasonal Auto-Regressive Integrated Moving Average (SARIMA) forecasting, we predict bearer statistical data to calculate the optimal CP software upgrade time, validated using Z-Score analysis at the same time. In addition to address the suboptimal upgrade timings, we also proposed CP Outage Handling Procedure (COHP) v2 by preserving UE contexts during CP upgrades. Our results demonstrate SARIMA’s high accuracy in predicting lean traffic conditions, with an R-Squared score of 0.99. Furthermore, upgrading CP software during predicted lean periods leads to substantial UE power savings ranging from 80% to 97% compared to manual upgrades.","2169-3536","","10.1109/ACCESS.2024.3439008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10623456","5G core;cloud native;control plane;user plane;CNF;AI/ML;SARIMA;power optimization","Time series analysis;Base stations;Software algorithms;Forecasting;5G mobile communication;3GPP;Energy efficiency","","","","23","CCBYNCND","5 Aug 2024","","","IEEE","IEEE Journals"
"MonitorChain: An Extensible Tool for Real-Time Monitoring of Blockchain-Based Software Applications","A. Araújo; R. Soares; W. Santos","Computing Institute, Federal University of Alagoas, Maceió, Brazil; Computing Institute, Federal University of Alagoas, Maceió, Brazil; Computing Institute, Federal University of Alagoas, Maceió, Brazil",IEEE Access,"18 Oct 2024","2024","12","","147784","147797","Support tools that monitor software application processes are required to identify, measure, and assess their performance and resolve any anomalies that impede their full functionality. Similarly, applications that use blockchain technology require analysis and monitoring tools for the resources and functionality provided by their smart contracts. This paper presents a computational tool that monitors hardware resources and evaluates transaction processing requests in blockchain networks. The solution presented here is called MonitorChain, and it has software components that allow the connection with different blockchain networks, the configuration of the workload, the monitoring of consumed hardware resources, and the evaluation of requests processed over blockchain networks. The effectiveness and adaptability of MonitorChain were validated through extensive evaluations across various blockchain environments, including public networks like Ethereum, Avalanche, and Fantom, as well as a private network using Hyperledger Fabric, with a consistent data load of 10,000 transactions. Additionally, through interviews with experienced software developers, we gathered feedback that led to further refinements and identified opportunities for future enhancements. The results demonstrated MonitorChain’s capability to provide critical insights into blockchain applications’ performance, scalability, and efficiency. MonitorChain advances the field compared to existing solutions by offering enhanced support for diverse networks, customizable workloads, and real-time data visualization, contributing significantly to the blockchain monitoring landscape.","2169-3536","","10.1109/ACCESS.2024.3475876","Alagoas Research Foundation (FAPEAL)(grant numbers:E:60030.0000002551/2022); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10706853","Blockchain technology;software development tools;real-time monitoring tools;smart contracts","Blockchains;Monitoring;Software;Measurement;Scalability;Real-time systems;Distributed ledger;Smart contracts;Security;Software development management;Real-time systems","","","","35","CCBYNCND","7 Oct 2024","","","IEEE","IEEE Journals"
"An Automated Edge Computing Approach for IoT Device Registration and Application Deployment","V. Mafeni; Y. Kim","School of Electronic Engineering, Soongsil University, Seoul, South Korea; School of Electronic Engineering, Soongsil University, Seoul, South Korea",IEEE Systems Journal,"20 Jun 2024","2024","18","2","1447","1458","As the Internet of Things (IoT) evolves rapidly across various industries, the number of IoT protocols and applications is growing with vast number of heterogeneous components and entities. In setups with thousands of IoT devices, manual deployment of applications and device registration become impractical due to their time-consuming and costly nature, as well as the requirement for background knowledge of IoT devices and protocols. Furthermore, IoT devices often have resource constraints that prevent them from running complex software. Therefore, there is a significant need to enhance and optimize edge computing systems for IoT, making them suitable and dynamic for automated IoT device registration and heterogeneous application deployment. In this article, we present an edge-based framework designed to facilitate the automated registration of diverse wireless IoT devices and the deployment of IoT applications. To validate our approach, we use a smart irrigation system enhanced with a containerized machine learning model as a proof of concept. Our evaluation of the implemented prototype demonstrates that our system is scalable and feasible.","1937-9234","","10.1109/JSYST.2024.3392290","Institute of Information and communications Technology Planning and Evaluation(grant numbers:2020-0-00946); Development of Fast and Automatic Service recovery and Transition software in Hybrid Cloud Environment; MSIT, Korea(grant numbers:IITP-2024-RS-2023-00258649); IITP; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10518055","Automation;edge computing;Internet of Things (IoT);service orchestration;wireless sensors","Internet of Things;Edge computing;Cloud computing;Scalability;Wireless communication;Manuals;Wireless sensor networks","","","","51","CCBYNCND","3 May 2024","","","IEEE","IEEE Journals"
"A Visionary Look at the Security of Reconfigurable Cloud Computing","M. Stojilović; K. Rasmussen; F. Regazzoni; M. B. Tahoori; R. Tessier","School of Computer and Communication Sciences, École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerlan; Department of Computer Science, University of Oxford, Oxford, U.K.; University of Amsterdam, Amsterdam, The Netherlands; Karlsruhe Institute of Technology, Karlsruhe, Germany; Department of Electrical and Computer Engineering, University of Massachusetts Amherst, Amherst, MA, USA",Proceedings of the IEEE,"18 Dec 2023","2023","111","12","1548","1571","Field-programmable gate arrays (FPGAs) have become critical components in many cloud computing platforms. These devices possess the fine-grained parallelism and specialization needed to accelerate applications ranging from machine learning to networking and signal processing, among many others. Unfortunately, fine-grained programmability also makes FPGAs a security risk. Here, we review the current scope of attacks on cloud FPGAs and their remediation. Many of the FPGA security limitations are enabled by the shared power distribution network in FPGA devices. The simultaneous sharing of FPGAs is a particular concern. Other attacks on the memory, host microprocessor, and input/output channels are also possible. After examining current attacks, we describe trends in cloud architecture and how they are likely to impact possible future attacks. FPGA integration into cloud hypervisors and system software will provide extensive computing opportunities but invite new avenues of attack. We identify a series of system, software, and FPGA architectural changes that will facilitate improved security for cloud FPGAs and the overall systems in which they are located.","1558-2256","","10.1109/JPROC.2023.3330729","German Research Foundation(grant numbers:DFG TA 782/42-1); NSF(grant numbers:CNS-1902532); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10323596","Cloud computing;denial-of-service (DoS) attack;fault injection;field-programmable gate array (FPGA);secure computing;side-channel attack","Field programmable gate arrays;Cloud computing;Servers;Security;Data centers;Central Processing Unit;Switches;Reconfigurable architectures;Denial-of-service attack;Side-channel attacks","","","","162","IEEE","21 Nov 2023","","","IEEE","IEEE Journals"
"Empowering Beyond 5G Networks: An Experimental Assessment of Zero-Touch Management and Orchestration","S. Barrachina-Muñoz; F. Rezazadeh; L. Blanco; S. Kukliński; E. Zeydan; A. Chawla; L. Zanzi; F. Devoti; V. Vlahodimitropoulou; I. Chochliouros; A. -M. Bosneag; S. Cherrared; L. Vettori; J. Mangues-Bafalluy","Centre Tecnològic de Telecomunicacions de Catalunya (CTTC), Barcelona, Spain; Centre Tecnològic de Telecomunicacions de Catalunya (CTTC), Barcelona, Spain; Centre Tecnològic de Telecomunicacions de Catalunya (CTTC), Barcelona, Spain; Orange Polska, Warsaw, Poland; Centre Tecnològic de Telecomunicacions de Catalunya (CTTC), Barcelona, Spain; Ericsson Ireland, Athlone, Ireland; NEC Laboratories Europe, Heidelberg, Germany; NEC Laboratories Europe, Heidelberg, Germany; Hellenic Telecommunications Organization S.A. (OTE), Athens, Greece; Hellenic Telecommunications Organization S.A. (OTE), Athens, Greece; Ericsson Ireland, Athlone, Ireland; Orange France, Châtillon, France; Centre Tecnològic de Telecomunicacions de Catalunya (CTTC), Barcelona, Spain; Centre Tecnològic de Telecomunicacions de Catalunya (CTTC), Barcelona, Spain",IEEE Access,"11 Dec 2024","2024","12","","182752","182762","Effective zero-touch management and orchestration (ZSM&O) is crucial for scaling network slicing, particularly transitioning toward Beyond 5G (B5G) and 6G networks. This paper empirically validates the network slicing framework developed under the European Union Horizon 2020 MonB5G project. Building on three years of academia-industry collaboration, MonB5G introduces a flexible slicing model featuring umbrella slices that orchestrate modular, specialized slices across multi-domain environments to address next-generation service demands. For the first time, we evaluate its practicality in a 5G cloud-native testbed through a virtual reality (VR) streaming use case, supported by solutions such as federated learning-based CPU forecasting, anomaly detection, and deep reinforcement learning (DRL) for radio access network (RAN) optimization. The paper offers insights from technically demanding experimental tests and highlights challenges and development paths for managing next-generation mobile networks.","2169-3536","","10.1109/ACCESS.2024.3510804","H2020 Distributed management of Network Slices in beyond 5G (MonB5G) Project(grant numbers:871780); Spanish Ministry of Economy and Competitiveness (MINECO)–Program Programa de Universalización de Infraestructuras Digitales para la Cohesión I+D (UNICO I+D)(grant numbers:TSI-063000-2021-54,TSI-063000-2021-55); ‘‘ERDF—A way of making Europe’’ project; Ministerio de Ciencia e Innovación/Agencia Estatal de Investigación (MCIN/AEI)/10.13039/501100011033(grant numbers:PID2021-126431OB-I00,MINECO TSI-064100-2022-16,TSI-064100-2023-26); Generalitat de Catalunya(grant numbers:2021 SGR 00770); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10776956","Network slicing;scalability;network management;orchestration;B5G;testbed","5G mobile communication;6G mobile communication;Scalability;Monitoring;Resource management;Network slicing;Federated learning;Cloud computing;Automation;Anomaly detection","","","","35","CCBY","4 Dec 2024","","","IEEE","IEEE Journals"
"Software Architecture for Agricultural Robots: Systems, Requirements, Challenges, Case Studies, and Future Perspectives","R. Raja","Donders Centre for Cognition, Department of Artificial Intelligence, Radboud University, Nijmegen, The Netherlands",IEEE Transactions on AgriFood Electronics,"10 Apr 2024","2024","2","1","125","137","Designing software architectures for autonomous robots for agricultural contexts is a demanding and difficult job due to the requirement to monitor numerous sensors and actuators, as well as autonomous decision-making in unpredictable, unexpected scenarios. Depending on the essential requirements of a robotic device for agricultural usage, robot software architecture is created differently. Since no single software architecture exists for all applications, extensive knowledge of the various software architectures for robots is needed when creating your own robotic architecture or selecting one from a number of existing architectures. As a result, this article provides a comprehensive history of software architecture and its application in the agricultural domain along with a chronology of how software design has evolved over time. We provide several case studies to understand the importance of application of software architecture in agriculture and food industry and how to choose the best architecture for agricultural tasks. Finally, this article discusses the open obstacles and difficulties that must be addressed in order to ensure more advancements in the development of robot architecture for agricultural applications.","2771-9529","","10.1109/TAFE.2024.3366335","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10459067","Agricultural robot;food processing;food packaging;harvesting robot;robot programming;software architecture","Robot sensing systems;Agricultural robots;Software architecture;Food packaging;Robot programming","","","","66","IEEE","5 Mar 2024","","","IEEE","IEEE Journals"
"Online Layer-Aware Joint Request Scheduling, Container Placement, and Resource Provision in Edge Computing","Z. Li; J. Lou; Z. Tang; J. Guo; T. Wang; W. Jia; W. Zhao","School of Artificial Intelligence, Beijing Normal University, Beijing, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Institute of Artificial Intelligence and Future Networks, Beijing Normal University, Zhuhai, China; Institute of Artificial Intelligence and Future Networks, Beijing Normal University, Zhuhai, China; Institute of Artificial Intelligence and Future Networks, Beijing Normal University, Zhuhai, China; Institute of Artificial Intelligence and Future Networks, Beijing Normal University, Zhuhai, China; Shenzhen University of Advanced Technology, Shenzhen, China",IEEE Transactions on Services Computing,"7 Feb 2025","2025","18","1","328","341","Containers have emerged as a pivotal tool for service deployment in edge computing. Before running the container, an image composed of several layers must exist locally. Recent strategies have utilized layer-sharing in images to reduce deployment delays. However, existing research only focuses on a single aspect of container orchestration, like container placement, neglecting the joint optimization of the entire orchestration process. To fill in such gaps, this article introduces an online strategy that considers layer-aware container orchestration, encompassing request scheduling, container placement, and resource provision. The goal is to reduce costs, adapt to evolving user demands, and adhere to system constraints. We present an online optimization problem that accounts for various real-world factors in orchestration, including container and server expenses. An online algorithm is proposed, integrating a regularization-based approach and stepwise rounding to address this optimization problem efficiently. The regularization approach separates time-dependent container placement and server wake-up costs, requiring only current information and past decisions. The stepwise rounding process generates feasible solutions that meet system constraints, reducing computational costs. Additionally, a competitive ratio proof is provided for the proposed algorithm. Extensive evaluations demonstrate that our approach achieves about 20% performance enhancement compared to baseline algorithms.","1939-1374","","10.1109/TSC.2024.3504237","National Natural Science Foundation of China(grant numbers:62272050,62302048); Guangdong Key Lab of AI and Multi-modal Data Processing, United International College(grant numbers:Zhuhai); Department of Education of Guangdong Province; Institute of Artificial Intelligence and Future Networks; Engineering Center of AI and Future Education; Guangdong Provincial Department of Science and Technology, China; Zhuhai Science-Tech Innovation Bureau(grant numbers:2320004002772); Interdisciplinary Intelligence Super Computer Center of Beijing Normal University at Zhuhai; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10759825","Container placement;request scheduling;resource provision;edge computing","Containers;Costs;Servers;Processor scheduling;Image edge detection;Edge computing;Scheduling;Optimization;Delays;Artificial intelligence","","","","53","IEEE","21 Nov 2024","","","IEEE","IEEE Journals"
"Dynamic Service Placement in Edge Computing: A Comparative Evaluation of Nature-Inspired Algorithms","A. H. Kazmi; A. Staffolani; T. Zhang; C. Cabrera; S. Clarke","School of Computer Science and Statistics, Trinity College Dublin, Dublin 2, Ireland; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; School of Computer Science and Statistics, Trinity College Dublin, Dublin 2, Ireland; Department of Computer Science and Technology, University of Cambridge, Cambridge, U.K.; School of Computer Science and Statistics, Trinity College Dublin, Dublin 2, Ireland",IEEE Access,"6 Jan 2025","2025","13","","2653","2670","Edge computing has emerged as a promising solution for delivering services that demand low latency, high bandwidth, and stringent privacy requirements in numerous data- and compute-intensive applications, such as those in Smart Cities. Heterogeneity in edge computing resources and diverse application requirements demand adaptive optimization techniques, such as service placement, to conform to changing conditions. A service placement model must optimize the selection of edge nodes for deploying and executing services, thereby improving application QoS and maximizing resource utilization. Numerous optimization techniques for adaptive service placement problem have been proposed in the recent past. However, in most cases, the results have been evaluated in limited scenarios. This paper presents a comprehensive comparative study evaluating representative optimization algorithms applied to the problem of dynamic service placement across various application scenarios. The study covers nature-inspired approaches, including both meta-heuristics and reinforcement learning. Our experimental findings offer valuable insights into the strengths and weaknesses of the selected nature-inspired algorithms for service placement optimization, evaluated for applications with different QoS requirements. In our analysis, the Genetic Algorithm shows superior performance in achieving lower average distance and the average number of servers selected. Particle Swarm Optimization excels in minimizing average waiting time and placement decision time. The Artificial Bee Colony maintains low average latency, whereas the RL Proximal Policy Optimization demonstrates superior performance in terms of balancing the utilization of network resources.","2169-3536","","10.1109/ACCESS.2024.3520701","Taighde Éireann - Research Ireland(grant numbers:13/RC/2077_P2); Research Ireland Centre for Future Networks; VMware by Broadcom; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10810420","Edge computing;dynamic service placement;multi-objective optimization;meta-heuristics;nature-inspired algorithms;service offloading;computational offloading;service scheduling","Servers;Cloud computing;Edge computing;Privacy;Quality of service;Microservice architectures;Heuristic algorithms;Metaheuristics;Low latency communication;Computational modeling","","","","76","CCBY","20 Dec 2024","","","IEEE","IEEE Journals"
"Mobility-Aware Dependent Task Offloading in Edge Computing: A Digital Twin-Assisted Reinforcement Learning Approach","X. Chen; J. Cao; Y. Sahni; M. Zhang; Z. Liang; L. Yang","Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong, SAR, China; Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong, SAR, China; Department of Building Environment and Energy Engineering, Hong Kong Polytechnic University, Hung Hom, Hong Kong, SAR, China; Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong, SAR, China; Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong, SAR, China; School of Software Engineering, South China University of Technology, Guangzhou, China",IEEE Transactions on Mobile Computing,"6 Mar 2025","2025","24","4","2979","2994","Collaborative edge computing (CEC) has emerged as a promising paradigm, enabling edge nodes to collaborate and execute tasks from end devices. Task offloading is a fundamental problem in CEC that decides when and where tasks are executed upon the arrival of tasks. However, the mobility of users often results in unstable connections, leading to network failures and resource underutilization. Existing works have not adequately addressed joint mobility-aware dependent task offloading and network flow scheduling, resulting in network congestion and suboptimal performance. To address this, we formulate an online joint mobility-aware dependent task offloading and bandwidth allocation problem, to improve the quality of service by reducing task completion time and energy consumption. We introduce a Mobility-aware Digital Twin-assisted Deep Reinforcement Learning (MDT-DRL) algorithm. Our digital twin model equips the reinforcement learning process by providing future states of mobile users, enabling efficient offloading plans for adapting to the mobile CEC system. Experimental results on real-world and synthetic datasets show that MDT-DRL surpasses state-of-the-art baselines on average task completion time and energy consumption.","1558-0660","","10.1109/TMC.2024.3506221","Research Institute for Artificial Intelligence of Things; Hong Kong Polytechnic University; Hong Kong (HK) Research Grant Council; Theme-based Research Scheme(grant numbers:T43-513/23-N); National Natural Science Foundation of China(grant numbers:CRS_PolyU501/23); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10767295","Collaborative edge computing;deep reinforcement learning;digital twin;mobility;task offloading","Digital twins;Internet of Things;Edge computing;Vehicle dynamics;Dynamic scheduling;Processor scheduling;Real-time systems;Energy consumption;Mobile computing;Job shop scheduling","","","","40","IEEE","25 Nov 2024","","","IEEE","IEEE Journals"
"EMO-TS: An Enhanced Multi-Objective Optimization Algorithm for Energy-Efficient Task Scheduling in Cloud Data Centers","S. Nambi; P. Thanapal","School of Computer Science Engineering and Information Systems, Vellore Institute of Technology, Vellore, India; School of Computer Science Engineering and Information Systems, Vellore Institute of Technology, Vellore, India",IEEE Access,"14 Jan 2025","2025","13","","8187","8200","The rapid expansion of cloud data centers, driven by the increasing demand for diverse user services, has escalated energy consumption and greenhouse gas emissions, posed severe environmental risks, and increased operational costs. Addressing these challenges requires innovative solutions for optimizing resource allocation without compromising service quality. This paper presents the Enhanced Multi-Objective Optimization Algorithm for Task Scheduling (EMO-TS). This novel approach integrates Deep Reinforcement Learning (DRL) and Enhanced Electric Fish Optimization (EEFO) to create an adaptive, dynamic, and energy-efficient scheduling framework. The primary objective of EMO-TS is to significantly reduce the energy consumption of cloud data centers while maintaining high levels of resource utilization, time efficiency, and service quality. Through the hybrid methodology of DRL and EEFO, EMO-TS dynamically adjusts task scheduling based on real-time workloads and operational conditions, effectively minimizing power consumption without sacrificing system performance. Additionally, EMO-TS introduces improvements in makespan and task execution, ensuring timely completion and optimal resource use. A comprehensive set of experiments and simulations demonstrates the practical implications of EMO-TS’s results. EMO-TS outperforms traditional scheduling approaches, reducing energy consumption by up to 25% and decreasing makespan by 15%. These results underscore the algorithm’s potential to address cloud service providers’ economic and environmental concerns, offering a practical solution for green cloud computing initiatives. Furthermore, the integration of renewable energy sources within the EMO-TS framework shows potential for further reducing the carbon footprint of cloud operations, aligning with global sustainability goals.","2169-3536","","10.1109/ACCESS.2025.3527031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10833625","Cloud data centers;deep reinforcement learning;electric fish optimization;energy efficiency;makespan;task scheduling","Cloud computing;Data centers;Processor scheduling;Resource management;Dynamic scheduling;Energy consumption;Heuristic algorithms;Energy efficiency;Real-time systems;Scalability","","","","46","CCBY","8 Jan 2025","","","IEEE","IEEE Journals"
"Adaptive Digital Twin Migration in Vehicular Edge Computing and Networks","F. Mou; J. Lou; Z. Tang; Y. Wu; W. Jia; Y. Zhang; W. Zhao","Hong Kong Baptist University, Kowloon Tong, Hong Kong; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Institute of Artificial Intelligence and Future Networks, Beijing Normal University, Zhuhai, China; State Key Lab of IoT for Smart City, University of Macau, SAR Macau, China; Institute of Artificial Intelligence and Future Networks, Beijing Normal University, Zhuhai, China; Department of Informatics, University of Oslo, Oslo, Norway; Shenzhen University of Advanced Technology, Shenzhen, China",IEEE Transactions on Vehicular Technology,"5 Mar 2025","2025","74","3","4839","4854","The surge in mobile vehicles and data traffic in Vehicular Edge Computing and Networks (VECONs) requires innovative approaches for low latency, stable connectivity, and efficient resource usage in fast-moving vehicles. Existing studies have identified that utilizing digital twins (DTs) can effectively improve service quality in VECONs. However, it still faces substantial challenges posed by large-scale complex DT communications in sustaining real-time collaborative endeavors. In particular, within the dynamic VECONs, the decision regarding DT migration plays a pivotal role in sustaining the quality of services. In this paper, we propose an adaptive DT migration (ADM) algorithm to minimize the overall migration costs when DTs deliver services. Specifically, 1) We formulate ADM as a combinatorial optimization problem in VECONs, comprehensively considering communication latency and migration latency under complex DT communications, vehicular mobilities, and dynamic states of edges; 2) An ADM algorithm based on off-policy actor-critic reinforcement learning is proposed to make migration decisions. Moreover, the ADM agent employs warm-up policies to address exploration challenges in sparse state spaces; 3) Simulations based on real-world, large-scale urban vehicular mobility datasets demonstrate that our method outperforms existing algorithms by approximately 39% on average, and it can achieve results close to the optimal.","1939-9359","","10.1109/TVT.2024.3492349","National Natural Science Foundation of China(grant numbers:62272050,62302048); Science and Technology Development Fund of Macau SAR(grant numbers:0158/2022/A); Guangdong Key Lab of AI and Multi-modal Data Processing, United International College (UIC), Zhuhai(grant numbers:2023-2024); Department of Education of Guangdong Province; Institute of Artificial Intelligence and Future Networks; Engineering Center of AI and Future Education, Guangdong Provincial Department of Science and Technology, China; Zhuhai Science-Tech Innovation Bureau(grant numbers:2320004002772); Interdisciplinary Intelligence Super Computer Center of Beijing Normal University at Zhuhai; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10746337","Digital twin;migration;vehicular edge computing;deep reinforcement learning","Vehicle dynamics;Real-time systems;Heuristic algorithms;Costs;Feature extraction;Edge computing;Approximation algorithms;Adaptive systems;Servers;Training","","","","52","IEEE","7 Nov 2024","","","IEEE","IEEE Journals"
"A C-ITS Architecture for MEC and Cloud Native Back-End Services","J. Arin; G. Velez; P. Bustamante","Fundación Vicomtech, Basque Research and Technology Alliance (BRTA), Donostia-San Sebastian, Spain; Fundación Vicomtech, Basque Research and Technology Alliance (BRTA), Donostia-San Sebastian, Spain; Tecnun, Universidad de Navarra, Donostia-San Sebastian, Spain",IEEE Access,"9 May 2024","2024","12","","64531","64550","Advances in connectivity and computing infrastructure facilitate the introduction of innovative Cooperative Intelligent Transport Systems (C-ITS) services. However, meeting the requirements of these highly demanding services calls for novel computing architectures that handle extensive device connections, minimize latency, and support multiple resource-intensive services concurrently. To overcome these challenges, this work presents an architecture that comprises three layers: 1) on-board unit (OBU) mainly as a data producer; 2) intermediate edge layer where low-latency backend services can be deployed; and 3) cloud layer for non-real-time backend services. The OBU software stack implements the ETSI C-ITS standard and supports multicast over the cellular network. The edge layer includes an in-memory database, and the cloud layer a persistent database. Each layer has its own Application Programming Interface (API) for data consumption. We conducted several experiments to demonstrate the feasibility of our proposed system that ensures scalability and interconnection between vehicles, edge and cloud servers. We also assess the delay caused by each of the elements of the architecture, and we discuss the potential solutions for the identified issues.","2169-3536","","10.1109/ACCESS.2024.3397467","European Union’s Horizon 2020 Research and Innovation Program through the Project Monetizing car & mobility data for new Entrants, Technologies and Actors (5GMETA)(grant numbers:957360); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10521473","V2X;C-V2X;5G;MEC;cloud;C-ITS","Standards;Computer architecture;Long Term Evolution;Ad hoc networks;Cloud computing;Databases;Low latency communication;Multiaccess communication;Edge computing","","","","53","CCBYNCND","6 May 2024","","","IEEE","IEEE Journals"
"Fusing Algorithms for Intersection of Computer Science and Art: Innovations in Generative Art and Interactive Digital Installations","J. Xie; M. Yu; G. Liu","School of Arts, Anyang Normal University, Anyang, Henan, China; School of Chinese Language and Literature, Anyang Normal University, Anyang, Henan, China; School of Computer and Information Engineering, Fuyang Normal University, Anhui, Fuyang, China",IEEE Access,"25 Nov 2024","2024","12","","173255","173267","This article investigates the integration of Variational Autoencoders (VAEs) and Particle Swarm Optimization (PSO) in the realm of generative art and interactive digital installations. The study focuses on how these advanced algorithms can enhance artistic expression and interactivity, providing novel approaches for generating and optimizing art. Key innovations include the application of VAEs to create diverse and complex art forms, coupled with PSO to fine-tune these generative processes. The research demonstrates that VAEs significantly improve the aesthetic quality and variety of generated artworks, achieving an average aesthetic score of 8.3 out of 10. Integrating PSO further optimizes these results, enhancing the quality of outputs with a final score of 9.0. The study also reveals that this combination improves user engagement and satisfaction, with interactive installations utilizing VAE + PSO achieving a satisfaction score of 9.0, compared to 7.0 for traditional methods. The findings highlight the transformative impact of these algorithms on art generation, showing that while computational resources and time are higher, the artistic and interactive benefits are substantial. This research underscores the potential of combining deep learning and optimization techniques to push the boundaries of digital creativity and offers new perspectives for artists and designers. The article concludes that the synergy of VAEs and PSO represents a significant advancement in generative art and interactive installations, opening new avenues for future exploration and development in the field.","2169-3536","","10.1109/ACCESS.2024.3488398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10738802","Variational autoencoders (VAEs);particle swarm optimization (PSO);generative art;interactive digital installations;artistic expression;deep learning algorithms;algorithmic creativity","Art;Heuristic algorithms;Sensors;Creativity;Accuracy;Visualization;Software algorithms;Real-time systems;Tuning;Technological innovation;Particle swarm optimization;Deep learning","","","","25","CCBYNCND","30 Oct 2024","","","IEEE","IEEE Journals"
"Architecture Transformation: Integrating Smart Systems for Intelligent Agent-Based Service Management in Smart Organizations","A. Setijadi Prihatmanto; A. Sukoco; A. Budiyono; D. Tresnawati; N. Lestari; H. Teja Sukmana; U. Atiah; S. Wahyuni; W. Ratnawati; K. Abimanyu Munastha","Research Center for Information Communication Technology, Institut Teknologi Bandung, Bandung, Indonesia; Computer Science Faculty, Universitas Bandar Lampung, Bandar Lampung, Indonesia; Indonesia Center for Technology Empowerment, CAP Solutions, Bandung, Indonesia; Institut Teknologi Garut, Garut, Indonesia; Faculty of Engineering, Universitas Sangga Buana, Bandung, Indonesia; Faculty of Science and Technology, UIN Jakarta, Jakarta, Indonesia; Neuro Engineering Study Group, Labuang Baji General Hospital, Makassar, South Sulawesi, Indonesia; Neuro Engineering Study Group, Makassar City General Hospital, Makassar, South Sulawesi, Indonesia; Neuro Engineering Study Group, Tenriawaru General Hospital, Bone, South of Sulawesi, Indonesia; Faculty of Engineering, Universitas Sangga Buana, Bandung, Indonesia",IEEE Access,"17 Oct 2024","2024","12","","146968","146995","In the pursuit of organizational goals, addressing the fundamental needs of employees, such as ensuring the availability of drinking water, plays a pivotal role in fostering a productive, healthy, and value-aligned work environment. This imperative necessitates the implementation of intelligent service management solutions that are both efficient and intelligent. In this research, we propose a visionary architectural transformation that seamlessly integrates intelligent agent-based smart systems within the domain of Smart Organizations. Our novel architectural approach draws inspiration from Activity Theory, strategically orchestrating interactions between employees, company objectives, and tools. Additionally, we employ a meticulously crafted 4-layer organizational system structure to delineate roles, establish rules, foster communities, and optimize task allocation. This forward- looking architecture is further fortified through the integration of cutting-edge technologies, encompassing the Internet of Things (IoT) with precision dispensing scales for real-time predictive capabilities, human-computer interaction (HCI) mobile applications and web services to enhance user-system engagement, and pervasive artificial intelligence (AI) implementations across every layer of the system. The architectural framework is consolidated within a robust Big Data platform, enabling the collection and comprehensive analysis of data on a grand scale. This multifaceted approach seeks to usher in profound and all-encompassing changes in the management of drinking water supply services, encompassing organizational paradigms, technological advancements, and methodological enhancements to enhance efficiency and effectiveness. The focus of this transformation is primarily directed toward companies involved in supply activities, with particular emphasis on drinking water provisioning. Through the development of predictive system prototypes and meticulous performance analysis, our study conclusively demonstrates that the Smart System-based prediction architecture significantly enhances operational performance and elevates the quality of drinking water supply services.","2169-3536","","10.1109/ACCESS.2024.3456845","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10670395","Architectural transformation;smart system;service management;smart organization","Artificial intelligence;Neural engineering;Big Data;Regulation;Real-time systems;Intelligent systems;Organizational aspects;Service-oriented architecture;Internet of Things;Resource management;Human computer interaction;Water resources;Intelligent sensors;Agent-based modeling","","","","94","CCBYNCND","10 Sep 2024","","","IEEE","IEEE Journals"
"Digital Twins for Cyber-Physical Healthcare Systems: Architecture, Requirements, Systematic Analysis, and Future Prospects","M. S. Roopa; K. R. Venugopal","Department of Computer Science and Engineering, Nitte Meenakshi Institute of Technology, Bengaluru, India; Department of Computer Science and Engineering, UVCE, Bangalore University, Bengaluru, India",IEEE Access,"17 Mar 2025","2025","13","","44963","44996","Over the past few years, the demand for personalized and accurate healthcare solutions has significantly increased, aiming to enhance medical effectiveness. Digital Twins (DTs), a transformative technology in the evolution of Industry 4.0, have expanded their applications from traditional fields like manufacturing and automotive industries to revolutionizing healthcare. DTs offer a more advanced and precise method of healthcare delivery, enabling better patient-specific care by creating real-time digital replicas of patients’ health conditions. Despite these advancements, healthcare professionals still heavily rely on their expertise to make critical decisions, highlighting the importance of continuous refinement of knowledge and skills to navigate complex healthcare challenges. Personalized treatments and comprehensive healthcare support remain essential for optimizing patient outcomes, though they are still in the early phases of development. This paper introduces a reference architecture specifically designed for Digital Twins in cyber-physical healthcare systems, outlining the key requirements and challenges in developing digital frameworks for such systems. It explores the fundamental principles of Digital Twins in healthcare, focusing on their applications in product design, production processes, and system performance. It also provides a detailed review of current research on healthcare data management, personalized healthcare, and operational efficiency, along with the associated requirements, challenges, and application scenarios. Furthermore, the paper discusses major obstacles and outlines potential future research directions, offering a roadmap to advance healthcare data management, personalized medicine, and operational control within Digital Twin frameworks.","2169-3536","","10.1109/ACCESS.2025.3547991","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10910158","Cyber physical healthcare systems;healthcare;digital twins;DTs-CPHS;healthcare data management;operational efficiency;personalized healthcare","Medical services;Digital twins;Reviews;Medical diagnostic imaging;Real-time systems;Surveys;Computer architecture;System performance;Production;Precision medicine","","","","142","CCBY","5 Mar 2025","","","IEEE","IEEE Journals"
"A Tail Latency SLO Guaranteed Task Scheduling Scheme for User-Facing Services","Z. Wang; H. Li; L. Sun; S. Rosenkrantz; H. Che; H. Jiang","Department of Computer Science and Engineering, The University of Texas at Arlington, Arlington, TX, USA; Department of Computer Science and Engineering, The University of Texas at Arlington, Arlington, TX, USA; Department of Computer Science and Engineering, The University of Texas at Arlington, Arlington, TX, USA; Department of Computer Science and Engineering, The University of Texas at Arlington, Arlington, TX, USA; Department of Computer Science and Engineering, The University of Texas at Arlington, Arlington, TX, USA; Department of Computer Science and Engineering, The University of Texas at Arlington, Arlington, TX, USA",IEEE Transactions on Parallel and Distributed Systems,"5 Mar 2025","2025","36","4","759","774","A primary design objective for user-facing services for cloud and edge computing is to maximize query throughput, while meeting query tail latency Service Level Objectives (SLOs) for individual queries. Unfortunately, the existing solutions fall short of achieving this design objective, which we argue, is largely attributed to the fact that they fail to take the query fanout explicitly into account. In this paper, we propose TailGuard based on a Tail-latency-SLO-and-Fanout-aware Earliest-Deadline-First Queuing policy (TF-EDFQ) for task queuing at individual task servers the query tasks are fanned out to. With the task pre-dequeuing time deadline for each task being derived based on both query tail latency SLO and query fanout, TailGuard takes an important first step towards achieving the design objective. A query admission control scheme is also developed to provide tail latency SLO guarantee in the presence of resource shortages. TailGuard is evaluated against First-In-First-Out (FIFO) task queuing, task PRIority Queuing (PRIQ) and Tail-latency-SLO-aware EDFQ (T-EDFQ) policies by both simulation and testing in the Amazon EC2 cloud. It is driven by three types of applications in the Tailbench benchmark suite, featuring web search, in-memory key-value store, and transactional database applications. The results demonstrate that TailGuard can significantly improve resource utilization (e.g., up to 80% compared to FIFO), while also meeting the targeted tail latency SLOs, as compared with the other three policies. TailGuard is also implemented and tested in a highly heterogeneous Sensing-$a$as-a-Service (SaS) testbed for a data sensing service, demonstrating performance gains of up to 33% . These results are consistent with both the simulation and Amazon EC2 results.","1558-2183","","10.1109/TPDS.2025.3542638","National Science Foundation(grant numbers:CCF SHF-2008835,CCF-SHF2226117); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10891045","Task scheduling;resource management;tail latency SLO;user-facing service","Tail;Servers;Resource management;Sensors;Cloud computing;Web search;Time factors;Throughput;Crowdsensing;Admission control","","","","55","IEEE","14 Feb 2025","","","IEEE","IEEE Journals"
"DAG-Based Swarm Learning Approach in Healthcare: A Survey","D. Gana; F. Jamil","Department of Computer Science, University of Huddersfield, Huddersfield, U.K.; Department of Computer Science, University of Huddersfield, Huddersfield, U.K.",IEEE Access,"24 Jan 2025","2025","13","","13796","13815","Healthcare systems are advancing at a rapid pace as a result of new technologies to address several issues in the sector such as shortage of skilled health workers and to deal with new diseases like COVID-19. Incorporating technologies like blockchain, federated learning, swarm learning, and Directed Acyclic Graphs is transforming healthcare. This article thoroughly examines recent progress and uses at the intersection of these technologies within the healthcare field. Blockchain’s innovative consensus mechanisms and secure data flow systems offer encouraging solutions to crucial issues in healthcare data management and security. Also, federated learning has been deployed in various ways to tackle healthcare challenges enabling collaborative data analysis while upholding patient confidentiality. Swarm learning algorithms have been notably effective in healthcare, enriching medical diagnostics, disease prognosis, and precision medicine. Solutions based on Directed Acyclic Graphs present scalable and effective alternative to traditional blockchain frameworks, providing improved consensus speed and decreased bottlenecks in transaction processing. These advancements signify a shift in direction towards fully decentralised and secure healthcare systems. This paper highlights the transformative impact of these technologies on medical diagnostics, disease prediction, and precision medicine.","2169-3536","","10.1109/ACCESS.2025.3531216","University of Huddersfield, Huddersfield, U.K; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10844296","Blockchain;directed acyclic graph;federated learning;IoT;privacy;scalability;smart healthcare;swarm learning","Medical services;Blockchains;Diseases;Federated learning;Internet of Things;Security;Accuracy;Surveys;Prediction algorithms;Machine learning algorithms","","","","176","CCBY","17 Jan 2025","","","IEEE","IEEE Journals"
"Modelling IoT Behaviour in Supply Chain Business Processes with BPMN: A Systematic Literature Review","I. Abouzid; Y. K. Bekali; R. Saidi","SI2M Laboratory, National Institute of Statistics and Applied Economics, Rabat, Morocco; LRIT, Faculty of Sciences, Mohammed V University in Rabat, Morocco; SI2M Laboratory, National Institute of Statistics and Applied Economics, Rabat, Morocco",Journal of ICT Standardization,"22 Sep 2023","2022","10","3","439","467","The Internet of Things (IoT) enables to connect physical world to digital processes, allowing real-world data to be fed into business processes. This revolution helps in the making of more informed business decisions as well as the automation and/or improvement of business processes tasks. The successful integration of IoT into business operations is required to realize these benefits. Supporting the modelling of IoT-enhanced business proccesess is the first step toward this goal. Despite the fact that numerous papers studied this topic, it is unclear what the current state of the art is in terms of modelling solutions and gaps. We conduct a systematic literature review in this work to determine how current solutions model IoT into business operations, and whether the standard Business Process Model and Notation (BPMN) has emerged as the de facto standard for business process modelling [20], [26]). The Object Management Group (OMG) developed BPMN, which is now an ISO standard BPMN is already enough for a full modelling of IoT integration, or the extensions are needed. We found and analysed the several existing alternative solutions after reviewing all the literature on this issue. Furthermore, we discuss some key aspects of the planned additions that should be addressed in the near future, such as the absence of standardization.","2246-0853","","10.13052/jicts2245-800X.1035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255414","Systematic literature review;business process modelling;BPMN;IoT;BPMN extensions;process execution;supply chain","Surveys;Analytical models;Systematics;Runtime;Bibliographies;Supply chains;Internet of Things","","","","40","","22 Sep 2023","","","River Publishers","River Publishers Journals"
"Cooperative Dew Computing for Computational Offloading in Healthcare Monitoring","T. Salam; W. Ur Rehman; I. Ud Din; A. Almogren; M. Mohammed Al Ezzi Sufyan; Kainat; M. Yasar Khan; A. Altameem","Department of Computer Science, Shaheed Benazir Bhutto Women University (SBBWU), Peshawar, Pakistan; Department of Computer Science, University of Peshawar, Peshawar, Pakistan; Department of Information Technology, The University of Haripur, Haripur, Pakistan; Department of Computer Science, Chair of Cyber Security, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Department of Computer Science, University of Peshawar, Peshawar, Pakistan; Department of Computer Science, Shaheed Benazir Bhutto Women University (SBBWU), Peshawar, Pakistan; SFI-Funded E-Governance Unit, Insight Centre for Data Analytics, University of Galway, Galway, Ireland; Department of Natural and Engineering Sciences, College of Applied Studies and Community Services, King Saud University, Riyadh, Saudi Arabia",IEEE Access,"21 Nov 2024","2024","12","","170041","170056","In modern healthcare monitoring, wearable sensors play a crucial role in collecting patient data, especially for individuals with disabilities. However, analyzing this data presents significant challenges, such as high energy consumption, latency, and task allocation inefficiencies. Healthcare monitoring systems face computational demands due to the processing of data from wearable sensors, which are constrained by limited resources and often affected by connectivity issues for data offloading. To address these challenges, this paper proposes a Cooperative Dew-Computing for Healthcare (CCH) framework, which integrates wearable sensors and dew devices within a cloud-fog-dew infrastructure. The CCH framework ensures efficient task distribution and computational offloading by selecting appropriate anchor devices using probabilistic and fuzzy logic techniques, taking into account energy efficiency and computational capabilities. Cooperative communication among anchor devices enables efficient task management, allowing tasks to be offloaded or distributed among multiple anchor devices. The proposed framework enhances healthcare monitoring systems by improving computational efficiency, optimizing resource utilization, and mitigating connectivity issues. This is particularly beneficial for disabled patients who rely on continuous monitoring and efficient data processing. Our evaluations demonstrate that the CCH framework outperforms existing techniques in terms of energy efficiency, latency, and task delay reduction, underscoring its effectiveness in providing timely and efficient data processing for healthcare monitoring systems.","2169-3536","","10.1109/ACCESS.2024.3498911","King Salman Center for Disability Research through Research Group(grant numbers:KSRG-2023-449); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10753614","Dew computing;cooperative communication;healthcare monitoring;disability;computational offloading;Internet of Medical Things","Energy consumption;Medical services;Data processing;Energy efficiency;Computational efficiency;Delays;Resource management;Dew computing;Monitoring;Wearable sensors","","","","39","CCBYNCND","15 Nov 2024","","","IEEE","IEEE Journals"
"Energy-Aware Microservice-Based SaaS Deployment in a Cloud Data Center Using Hybrid Particle Swarm Optimization","A. Alzahrani; M. Tang","School of Computer Science, Faculty of Science, Queensland University of Technology, Brisbane, QLD, Australia; School of Computer Science, Faculty of Science, Queensland University of Technology, Brisbane, QLD, Australia",IEEE Access,"4 Oct 2024","2024","12","","140884","140899","The deployment of software as a service (SaaS) using a microservice architecture offers several benefits, including scalability, flexibility, and ease of maintenance. One of the most important advantages of the new microservice-based SaaS deployment is that the increase in energy consumption incurred by the deployment of a new microservice-based SaaS can be considered. With the aim of reducing the increase in energy consumption, this paper proposes a new method, namely Hybrid Particle Swarm Optimization (HPSO), to solve the microservice-based SaaS deployment problem. The HPSO incorporates adaptive inertia weight, cognitive, and social parameters to balance the trade-off between exploration and exploitation during the optimization process. Furthermore, the HPSO incorporates a local optimizer to improve the best global solution within the swarm, with a specific emphasis on improving energy efficiency. To evaluate the performance of the HPSO, we have implemented it and compared it with a GA method by experiment. The experimental results have shown that the HPSO can further reduce the increase in energy consumption by 3.68% compared to GA.","2169-3536","","10.1109/ACCESS.2024.3462894","Saudi Cultural Mission and Prince Sattam Bin Abdulaziz University, Saudi Arabia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10683711","Cloud computing;data center;deployment;energy consumption;hybrid particle swarm optimization;microservice;optimization;software as a service","Microservice architectures;Data centers;Software as a service;Genetic algorithms;Energy consumption;Cloud computing;Servers;Particle swarm optimization","","","","32","CCBYNCND","18 Sep 2024","","","IEEE","IEEE Journals"
"Service-Aware Computation Offloading for Parallel Tasks in VEC Networks","J. Yang; K. Yang; X. Dai; Z. Xiao; H. Jiang; F. Zeng; B. Li","College of Computer Science and Electronic Engineering and the Chongqing Research Institute, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering and the Chongqing Research Institute, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering and the Chongqing Research Institute, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering and the Chongqing Research Institute, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering and the Chongqing Research Institute, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering and the Chongqing Research Institute, Hunan University, Changsha, China; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China",IEEE Internet of Things Journal,"24 Jan 2025","2025","12","3","2979","2993","Vehicular edge computing (VEC) emerges as a promising paradigm for processing computing-intensive parallel vehicular tasks, where vehicular tasks can be offloaded to the edge nodes [e.g., roadside units (RSUs)] to seek less computing delay. Considering the impact of computation services on offloading efficiency, there are several works that jointly study the decision making of task offloading and service caching. However, the existing works fail to consider the time-varying service requests and ignore the time-slots correlation of the computation services. To bridge the gap, this work designs a service-aware parallel task offloading approach, which is the first work to jointly explore time-varying computation services and task offloading based on real-world vehicular trajectory data in VEC networks. Specifically, we first propose a computation service prediction algorithm using the real-world vehicular trajectory data. Guided by this, RSUs flexibly precache computation services. Then, we propose a learning-based parallel task offloading algorithm, which allows vehicles to make offloading decisions based on the history of the edge selections. Furthermore, we conduct simulations to validate the proposed algorithm. The results demonstrate that the proposed algorithm reduces task delay by 45%, 58%, and 55% compared to the algorithms without service-aware computation offloading under various CPU cycles, task numbers, and time slots.","2327-4662","","10.1109/JIOT.2024.3476476","National Key Research and Development Program of China(grant numbers:2023YFB3001805); Key Program of the National Natural Science Foundation of China(grant numbers:62232007,62272152); Special Key Projects for Technological Innovation and Application Development of Chongqing(grant numbers:CSTB2022TIAD-DEX0016); Key Research and Development Program of Hunan Province(grant numbers:2024AQ2032); Cultivation Project of Yuelu Mountain Industrial Innovation Center(grant numbers:2023YCII0123); Natural Science Foundation Project of Chongqing(grant numbers:cstc2021jcyj-msxm4115,CSTB2024NSCQ-MSX0920); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10710330","Computation offloading;online learning;service caching;service prediction;vehicular edge computing (VEC)","Delays;Prediction algorithms;Heuristic algorithms;Computational modeling;Servers;Computational efficiency;Resource management;Edge computing;Trajectory;Energy consumption","","","","60","IEEE","9 Oct 2024","","","IEEE","IEEE Journals"
"Mitigating COVID-19 Spread in Closed Populations Using Networked Robots and Internet of Things","K. C. Okafor; K. Anoh; T. I. Chinebu; B. Adebisi; G. A. Chukwudebe","Manchester Metropolitan University, Manchester, U.K.; Electrical and Electronic Engineering Department, University of Chichester, Bognor Regis, U.K.; Department of Applied Science, Federal University of Allied Health Sciences, Enugu, Nigeria; Manchester Metropolitan University, Manchester, U.K.; Electrical and Electronic Engineering Department, Federal University of Technology, Owerri, Nigeria",IEEE Internet of Things Journal,"9 Dec 2024","2024","11","24","39424","39434","Infectious diseases like coronavirus disease 2019 (COVID-19) have remained a primary public and global health concern. Internet of Things (IoT) of networked robots and physiological intervention can be combined to identify and control the spread of the different variants of COVID-19 disease. With this approach, governments and healthcare institutions can plan for such diseases in the future. This article presents a compact computational model (CCM) to identify and control different COVID-19 variants using IoT-networked robots. The CCM comprises seven physiological variables (PVs) and robotic identification (RI) of infected individuals as alternative intervention strategies. This study uses Market Place Service Robots that correctly identify PV and RI for positively infected individuals. The conditions of the existence and the solution of the deterministic model are derived from a compact flow architecture that we develop. We show that the model has COVID-19-free equilibrium and endemic equilibrium. While PV with appropriate isolation and hospital treatment reduces the COVID-19 disease impact by 19% more than RI alone, this study also shows that combining two PV with RI minimizes the impact better than PV or RI alone, by 36% and 43%, respectively. When the PV control parameters are increased, up to 5, in the presence of IoT and RI, up to 99.99% improvement is seen. With all seven PV control parameters in the presence of IoT and RI, the proposed CCM guarantees an infection-free population.","2327-4662","","10.1109/JIOT.2024.3431874","IEEE HAC/SIGHT—Focus on COVID-19 Response and Pressing Community Need(grant numbers:21-Cov2-185); Tetfund Nigeria Program on Resilient AI and Smart Systems(grant numbers:TETF/ES/UNIV/IMO STATE/TSAS/2021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10606015","Computational complexity;computational intelligence;coronavirus disease 2019 (COVID-19) infection;global vaccination;Internet of Things (IoT);market place service robot (MPSR);smart health infrastructure","COVID-19;Statistics;Diseases;Computational modeling;Internet of Things;Computational complexity;Computational intelligence;Vaccines;Smart healthcare;Medical robotics","","","","32","IEEE","22 Jul 2024","","","IEEE","IEEE Journals"
"A Framework for Dynamically Meeting Performance Objectives on a Service Mesh","F. S. Samani; R. Stadler","Department of Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; Department of Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden",IEEE Transactions on Network and Service Management,"19 Dec 2024","2024","21","6","5992","6007","We present a framework for achieving end-to-end management objectives for multiple services that concurrently execute on a service mesh. We apply reinforcement learning (RL) techniques to train an agent that periodically performs control actions to reallocate resources. We develop and evaluate the framework using a laboratory testbed where we run information and computing services on a service mesh, supported by the Istio and Kubernetes platforms. We investigate different management objectives that include end-to-end delay bounds on service requests, throughput objectives, cost-related objectives, and service differentiation. Our framework supports the design of a control agent for a given management objective. The management objective is defined first and then mapped onto available control actions. Several types of control actions can be executed simultaneously, which allows for efficient resource utilization. Second, the framework separates the learning of the system model and the operating region from the learning of the control policy. By first learning the system model and the operating region from testbed traces, we can instantiate a simulator and train the agent for different management objectives. Third, the use of a simulator shortens the training time by orders of magnitude compared with training the agent on the testbed. We evaluate the learned policies on the testbed and show the effectiveness of our approach in several scenarios. In one scenario, we design a controller that achieves the management objectives with 50% less system resources than Kubernetes HPA autoscaling.","1932-4537","","10.1109/TNSM.2024.3434328","Swedish Governmental Agency for Innovation Systems, VINNOVA, through project ANIARA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10612769","Performance management;adaptive resource allocation;microservice;reinforcement learning;operating region","Microservice architectures;Measurement;Training;Reinforcement learning;Delays;Resource management;Throughput","","","","82","CCBY","26 Jul 2024","","","IEEE","IEEE Journals"
"Hierarchical Decentralized Edge Interoperability","T. Azad; M. A. H. Newton; J. Trevathan; A. Sattar","School of Information and Communication Technology, Griffith University, Nathan, QLD, Australia; Institute for Integrated and Intelligent Systems, Griffith University, Nathan, QLD, Australia; School of Information and Communication Technology and the Institute for Integrated and Intelligent Systems, Griffith University, Nathan, QLD, Australia; School of Information and Communication Technology and the Institute for Integrated and Intelligent Systems, Griffith University, Nathan, QLD, Australia",IEEE Internet of Things Journal,"9 Apr 2024","2024","11","8","13948","13960","The Internet of Things (IoT) has many important applications in multiple domains that include home automation, smart cities, healthcare, agriculture, and environment. IoT comprises a wide range of sensors and actuators that communicate with each other over cloud, fog, and edge level networks. Moreover, these devices use various communication protocols and are made by different manufactures. To deal with these diversities, IoT essentially needs interoperable communication interfaces among devices. Unfortunately, existing interoperability solutions are centralized and use fog or cloud level computing resources, making IoT communications latency-prone and poorly scalable. These issues could be handled effectively, if edge level devices could be made interoperable within the edge level and without needing fog or cloud level access. This article proposes a decentralized interoperability solution that stays fully within the edge level. The solution relies on controller devices that work on the interface boundaries of the edge devices. Unlike existing solutions, the proposed solution adopts a hierarchical interoperability model to handle interoperability at network, syntactical, semantic, and organizational levels. Our solution is nonproprietary, generic over vendors and platforms, and easily extendable to new devices. We compare our proposed solution with existing interoperability solutions for edge devices and show its mobility, efficiency, and flexibility.","2327-4662","","10.1109/JIOT.2023.3340298","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10347477","Edge devices;Internet of Things (IoT);network interoperability;organizational interoperability;semantic interoperability;syntactic interoperability","Interoperability;Internet of Things;Cloud computing;Sensors;Protocols;Semantics;Actuators","","","","47","IEEE","7 Dec 2023","","","IEEE","IEEE Journals"
"Cross-Domain Solutions (CDS): A Comprehensive Survey","V. Sundaravarathan; H. Alqalaf; A. Siddiqui; K. Kim; S. Lee; M. Reisslein; A. S. Thyagaturu; N. Ross; J. Howard; S. Tayal","School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA; School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA; College of Computer and Information Sciences, Prince Sultan University, Riyadh, Saudi Arabia; School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA; Hyundai Transys, Mesa, AZ, USA; School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA; Network and Edge Group, Intel Corporation, Chandler, AZ, USA; Network and Edge Group, Intel Corporation, Chandler, AZ, USA; Extreme Scale Computing Group, Intel Corporation, Hillsboro, OR, USA; Extreme Scale Computing Group, Intel Corporation, Hillsboro, OR, USA",IEEE Access,"11 Nov 2024","2024","12","","163551","163620","A domain is commonly defined as a set of system resources, e.g., computers, to which certain users have prescribed access rights as governed by some security policies. The access (viewing) and transfer of data between distinct domains facilitates a wide range of information technology applications. A Cross-Domain Solution (CDS) can provide the security mechanisms that are required to properly restrict the access and exchange of sensitive information between different domains. This article provides a comprehensive up-to-date survey of CDS. This survey is organized according to a CDS taxonomy with access, transfer, and Multi-Level Security (MLS) as the main CDS categories. An access solution helps a client to view the data present in a server (which is in a different domain, other than the client’s domain); thus an access solution effectively provides isolation between domains by allowing data access (without data transfer capabilities) between these domains. A transfer solution secures the transfer of data between domains to avoid leaks of data to unauthorized entities. Thereby, a transfer solution effectively connects domains in either a unidirectional or bidirectional manner. An MLS solution handles data with multiple levels of security (sensitivity of the data), such as top-secret, secret, or unclassified data. Aside from the fundamental conceptual approaches from the existing CDS research, this survey gives a comprehensive overview of the existing commercial CDS products. We identify the limitations of the existing CDS concepts and products and outline directions for future research and development to address these limitations and advance the overall CDS research area.","2169-3536","","10.1109/ACCESS.2024.3483659","Defense Advanced Research Projects Agency (DARPA) to Intel Corp; Department of Defense or the US Government; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10721459","Access;data diode;cross-domain solution (CDS);multi level security (MLS);transfer","Security;Hardware;Wireless communication;Surveys;Privacy;Military computing;Buildings;Wireless sensor networks;Testing","","","","365","CCBY","18 Oct 2024","","","IEEE","IEEE Journals"
"Run-Time Value Chain Analysis and Cost Accounting via Microservices in Agile Manufacturing","S. Javed; C. Paniagua; I. Javed; J. van Deventer; J. Delsing","Department of Computer Science, Electrical and Space Engineering, Lulea University of Technology, Lulea, Sweden; Department of Computer Science, Electrical and Space Engineering, Lulea University of Technology, Lulea, Sweden; Independent Researcher, Dubai, UAE; Department of Computer Science, Electrical and Space Engineering, Lulea University of Technology, Lulea, Sweden; Department of Computer Science, Electrical and Space Engineering, Lulea University of Technology, Lulea, Sweden",IEEE Open Journal of the Industrial Electronics Society,"7 Feb 2025","2025","6","","181","201","The rapid evolution of manufacturing processes driven by Industry 4.0 demands systems capable of quickly adapting to dynamic market conditions and evolving customer needs. Agile manufacturing emphasizes flexibility, adaptability, and real-time responsiveness, posing challenges in run-time value chain analysis (VCA), including cost flows and production times. This article presents a novel two-stage VCA approach using an activity-based costing mechanism via microservices to address these challenges. The VCA system enables real-time cost accounting and decision-making, supporting both pre and postproduction VCA, contrasting with traditional methods that rely on historical data. The first stage involves top–down cost calculations from resources to microservices. In contrast, the second focuses on constructing efficient manufacturing activities based on product requirements, allowing for granular analysis of costs and production times across microservices, activities, broader business processes, and finally, cost objects (e.g., customized products, batches of products, or customer invoices). The approach is validated through a proof-of-concept implementation of the VCA system integrated with the Eclipse Arrowhead framework and simulating Fischertechnik indexed line milling, drilling, and conveying operations. The results demonstrate the effectiveness of the proposed method in providing detailed insights into costs and production times, enhancing the efficiency and competitiveness of agile manufacturers.","2644-1284","","10.1109/OJIES.2025.3532664","European Commission and Arrowhead flexible Production Value Network; Electronic Components and Systems for European Leadership(grant numbers:101111977); AI-REDGIO5.0(grant numbers:101092069); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10849611","Activity-based costing (ABC);agile manufacturing;eclipse arrowhead framework;Industry 4.0 (I4.0);real-time cost accounting and decision-making","Microservice architectures;Agile manufacturing;Costs;Production;Real-time systems;Manufacturing;Cost accounting;Industrial electronics;Decision making;Resource management","","","","46","CCBY","22 Jan 2025","","","IEEE","IEEE Journals"
"AI-Native Multi-Access Future Networks—The REASON Architecture","K. Katsaros; I. Mavromatis; K. Antonakoglou; S. Ghosh; D. Kaleshi; T. Mahmoodi; H. Asgari; A. Karousos; I. Tavakkolnia; H. Safi; H. Hass; C. Vrontos; A. Emami; J. Marcelo Parra-Ullauri; S. Moazzeni; D. Simeonidou","Digital Catapult, London, U.K.; Digital Catapult, London, U.K.; Digital Catapult, London, U.K.; Digital Catapult, London, U.K.; Digital Catapult, London, U.K.; Department of Engineering, King’s College London, Strand Campus, London, U.K.; Thales Research and Technology, Reading, U.K.; Real Wireless, Pulborough, U.K.; Department of Engineering, University of Cambridge, Cambridge, U.K.; Department of Engineering, University of Cambridge, Cambridge, U.K.; Department of Engineering, University of Cambridge, Cambridge, U.K.; Faculty of Engineering, University of Bristol, Bristol, U.K.; Faculty of Engineering, University of Bristol, Bristol, U.K.; Faculty of Engineering, University of Bristol, Bristol, U.K.; Faculty of Engineering, University of Bristol, Bristol, U.K.; Faculty of Engineering, University of Bristol, Bristol, U.K.",IEEE Access,"9 Dec 2024","2024","12","","178586","178622","The development of the sixth generation of communication networks (6G) has been gaining momentum over the past years, with a target of being introduced by 2030. Several initiatives worldwide are developing innovative solutions and setting the direction for the key features of these networks. Some common emerging themes are the tight integration of AI, the convergence of multiple access technologies and sustainable operation, aiming to meet stringent performance and societal requirements. To that end, we are introducing REASON - Realising Enabling Architectures and Solutions for Open Networks. The REASON project aims to address technical challenges in future network deployments, such as E2E service orchestration, sustainability, security and trust management, and policy management, utilising AI-native principles, considering multiple access technologies and cloud-native solutions. This paper presents REASON’s architecture and the identified requirements for future networks. The architecture is meticulously designed for modularity, interoperability, scalability, simplified troubleshooting, flexibility, and enhanced security, taking into consideration current and future standardisation efforts, and the ease of implementation and training. It is structured into four horizontal layers: Physical Infrastructure, Network Service, Knowledge, and End-User Application, complemented by two vertical layers: Management and Orchestration, and E2E Security. This layered approach ensures a robust, adaptable framework to support the diverse and evolving requirements of 6G networks, fostering innovation and facilitating seamless integration of advanced technologies.","2169-3536","","10.1109/ACCESS.2024.3507186","Project REASON, a U.K. Government funded project under the Future Open Networks Research Challenge (FONRC) sponsored by the Department of Science Innovation and Technology (DSIT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10769448","Future networks;6G;AI/ML;open networks;reference architecture;native AI","6G mobile communication;Ethics;Artificial intelligence;5G mobile communication;Real-time systems;Energy efficiency;Sustainable development;Security;Resilience;Interoperability","","","","113","CCBYNCND","27 Nov 2024","","","IEEE","IEEE Journals"
"PEaF-Production Environment Analyzer Framework: Assisting Continuous Deployment of 5G Workloads Using AI/ML","K. Subramaniam; S. Kumar; A. Mishra; A. Bhandari; J. Manja Ppallan; G. Chandrasekaran","Samsung Electronics Research and Development, Bengaluru, India; Samsung Electronics Research and Development, Bengaluru, India; Samsung Electronics Research and Development, Bengaluru, India; Samsung Electronics Research and Development, Bengaluru, India; Samsung Electronics Research and Development, Bengaluru, India; Samsung Electronics Research and Development, Bengaluru, India",IEEE Access,"17 Oct 2024","2024","12","","147012","147022","A Production Environment Analyzer Framework (PEaF) is proposed to address the limitations of the Continuous Deployment (CD) process for 5G workflow lifecycle management. By integrating an AI/ML-based PEaF into the CD pipeline, we aim to ensure reliable deployments. PEaF uses AI/ML techniques to analyze the production environment and predict the health status of the hardware components. It collects raw data, applies K-Means clustering to group similar data points, and assigns scores to each cluster. These scores serve as features for training Support Vector Machine (SVM) and Random Forest (RF) classifiers to classify hardware health status. Experimental results show that PEaF achieves high classification accuracies of 97.26% and 96.44% for SVM and RF, respectively, with clustering. By analyzing the production environment and excluding deteriorating hardware from the CD, service failures are reduced by at least 27.04%. Moreover, PEaF decreases the polling frequency of hardware status by 48.7%, enhancing operational efficiency. Overall, PEaF contributes to advancing Continuous Integration/Continuous Deployment (CI/CD) practices in the 5G ecosystem, ensuring the reliability and stability of the production environment before deploying/upgrading services.","2169-3536","","10.1109/ACCESS.2024.3472498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10703063","5G;AI/ML;CI/CD;classification;feature selection;K-means clustering;random forest;regression;support vector machine","Hardware;5G mobile communication;Pipelines;Clustering methods;Production;Communications technology;DevOps;Random forests;Telecommunication network reliability;Support vector machines;Classification algorithms","","","","16","CCBYNCND","3 Oct 2024","","","IEEE","IEEE Journals"
"Architecture Design and Application of IIoT Platform in Automobile Manufacturing Based on Microservices and Deep Learning Techniques","Y. He; Y. Zhang; C. Wu; M. Yang; W. Xu; H. Wan; Z. Chen","School of Mechanical and Automotive Engineering, South China University of Technology, Guangzhou, China; Guangzhou MINO Equipment Company Ltd., Guangzhou, China; Guangzhou MINO Equipment Company Ltd., Guangzhou, China; Guangzhou MINO Equipment Company Ltd., Guangzhou, China; School of Mechanical and Automotive Engineering, South China University of Technology, Guangzhou, China; School of Mechanical and Automotive Engineering, South China University of Technology, Guangzhou, China; State Key Laboratory of Precision Electronic Manufacturing Technology and Equipment, Guangdong University of Technology, Guangzhou, China",IEEE Access,"15 Nov 2024","2024","12","","166834","166842","An Internet of Things (IoT) platform is a software architecture that enables the connection, management, and analysis of IoT devices, sensors, and data. It provides a centralized system for IoT devices to interact with each other and with the cloud, facilitating the collection, processing, and analysis of data from these devices. However, in the automotive manufacturing industry, traditional Internet of Things (IoT) platforms are facing challenges such as bottleneck issues due to business volume growth and system challenges. To address these challenges, we propose a design methodology for an IoT platform based on microservices. The platform’s modules are divided into front end, database, security, and operation maintenance architecture, all effectively designed. Through practical applications, the platform enables interconnections between different information systems, production status monitoring, efficiency management, performance evaluation, energy consumption analysis, quality detection, and equipment asset evaluation. Finally, a data-driven deep learning algorithm, named Long Short-Term Memory Neural Network (LSTM) is developed for the state recognition of the industrial robot based on the Intelligent data services platform, which validate the effectiveness of the constructed IoT platforms. This platform offers advantages in extendibility, reusability, and provides methods for upgrading, expanding functions, and maintaining industrial IoT platforms in the discrete manufacturing industry.","2169-3536","","10.1109/ACCESS.2024.3487832","National Natural Science Foundation of China(grant numbers:52205101); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021A1515110708,2023A1515240021); Young Talent Support Project of Guangzhou Association for Science and Technology(grant numbers:QT-2024-28); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10737327","Automobile manufacturing;IoT platform;microservices;deep learning;long short-term memory recurrent neural network (LSTM);state recognition","Microservice architectures;Industrial Internet of Things;Computer architecture;Maintenance;Manufacturing;Business;Security;Databases;Monitoring;Automotive engineering;Long short term memory","","","","39","CCBYNCND","29 Oct 2024","","","IEEE","IEEE Journals"
"WorkloadDiff: Conditional Denoising Diffusion Probabilistic Models for Cloud Workload Prediction","W. Zheng; Z. Chen; K. Zheng; W. Zheng; Y. Chen; X. Fan","School of Computer Science, South China Normal University, Guangzhou, China; School of Computer Science, South China Normal University, Guangzhou, China; School of Computer Science, South China Normal University, Guangzhou, China; School of Computer Science, South China Normal University, Guangzhou, China; School of Computer Science, South China Normal University, Guangzhou, China; College of Big Data and Internet, Shenzhen Technology University, Shenzhen, China",IEEE Transactions on Cloud Computing,"5 Dec 2024","2024","12","4","1291","1304","Accurate workload forecasting plays a crucial role in optimizing resource allocation, enhancing performance, and reducing energy consumption in cloud data centers. Deep learning-based methods have emerged as the dominant approach in this field, exhibiting exceptional performance. However, most existing methods lack the ability to quantify confidence, limiting their practical decision-making utility. To address this limitation, we propose a novel denoising diffusion probabilistic model (DDPM)-based method, termed WorkloadDiff, for multivariate probabilistic workload prediction. WorkloadDiff leverages both original and noisy signals from input conditions using a two-path neural network. Additionally, we introduce a multi-scale feature extraction method and an adaptive fusion approach to capture diverse temporal patterns within the workload. To enhance consistency between conditions and predicted values, we incorporate a resampling strategy into the inference of WorkloadDiff. Extensive experiments conducted on four public datasets demonstrate the superior performance of WorkloadDiff over all baseline models, establishing it as a robust tool for resource management in cloud data centers.","2168-7161","","10.1109/TCC.2024.3461649","National Key R&D Program of China(grant numbers:2019YFB1804003); National Natural Science Foundation of China(grant numbers:42071399); Natural Science Foundation of Top Talent of SZTU(grant numbers:GDRC202318); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10681248","Cloud workload prediction;diffusion models;resource management;resampling","Predictive models;Cloud computing;Diffusion models;Time series analysis;Data models;Hidden Markov models;Forecasting","","","","51","IEEE","16 Sep 2024","","","IEEE","IEEE Journals"
"Fractal-Based N-Environment Multiagent IoT System Reliability","I. Eroshkin; L. Vojtech; M. Neruda","Department of Telecommunication Engineering, Faculty of Electrical Engineering, Czech Technical University in Prague, Prague, Czech Republic; Department of Telecommunication Engineering, Faculty of Electrical Engineering, Czech Technical University in Prague, Prague, Czech Republic; Department of Telecommunication Engineering, Faculty of Electrical Engineering, Czech Technical University in Prague, Prague, Czech Republic",IEEE Internet of Things Journal,"5 Aug 2024","2024","11","16","27463","27479","The tremendous growth of Internet of Things (IoT) systems and the popularity of distributed systems, which were brought back into fashion by containerization, create new challenges for system maintenance. Each IoT-specific service is characterized by a specific domain, or environment, making the maintenance of distributed IoT systems with dozens of agents a challenging task. This article proposes a fractal-based mathematical model, called the fractal multiagent IoT system (FMAIS), for scalable multiagent systems (e.g., cloud-native services, wide-scale IoT services). A reliability estimation model called the multienvironment enhanced real-time customer-oriented reliability estimation (ME-ERT-CORE) model is proposed as a method for estimating the reliability of a multiagent IoT system, where each agent can be specified by its own domain-specific environment (e.g., healthcare, logistics, smart factories, etc.). An estimation of the time complexity of the FMAIS is performed, and a model called ME-ERT-CORE for estimating its reliability is formulated. The results show that the FMAIS can be used for real-time simulations of system behavior and ME-ERT-CORE can be used for the real-time estimation of its reliability, monitoring purposes, and, for instance, Quality-of-Service (QoS) descriptions based on service level agreement (SLA) requirements. The conducted measurements show that the reliability of the FMAIS with four layers and 96 applications, where each application deploys 96 instances, can be computed in 8.022 ms. With the proposed optimization method, the reliability for the same FMAIS can be computed in 0.219 ms, which is 36 times faster than the ME-ERT-CORE reliability evaluation based on the definition. An FMAIS with the aforementioned parameters can be simulated in 84.300 ms.","2327-4662","","10.1109/JIOT.2024.3398406","České Vysoké Učení Technické v Praze(grant numbers:SGS24/139/OHK3/3T/13); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10525061","Fractal;Internet of Things (IoT);mathematical model;multiagent systems (MASs);multienvironment (ME);real-time systems;reliability estimation;unified reliability","Reliability;Internet of Things;Fractals;Mathematical models;Estimation;Real-time systems;Reliability theory","","","","50","IEEE","8 May 2024","","","IEEE","IEEE Journals"
"Hybrid Medium Access Control Strategy for Internet-of-Things-Enabled Intravehicular Health Monitoring System","M. Karim; M. A. Rahman; M. Atiquzzaman","Faculty of Computing, University Malaysia Pahang, Kuantan, Malaysia; School of Engineering, Computing and Mathematical Sciences, University of Wolverhampton, Wolverhampton, U.K.; School of Computer Science, University of Oklahoma, Norman, OK, USA",IEEE Internet of Things Journal,"5 Feb 2025","2025","12","4","3846","3857","The increasing importance of intravehicular health monitoring systems (IVHMSs) necessitates robust communication protocols within vehicles, especially with the integration of the Internet of Things (IoT). This integration presents challenges in enhancing vehicle health monitoring efficiency due to the constrained space for numerous sensing devices, leading to scalability and performance issues. Existing medium access control (MAC) schemes often result in congestion, decreased throughput, and increased delays. This study proposes a scalable hybrid MAC strategy to improve throughput and reduce congestion in IVHMS. The approach features a two-phase communication model—nonemergency and emergency phases—for scalability. Our hybrid MAC strategy combines a history-based approach for emergency communication and a priority-based approach for nonemergency Communication. A Markov chain model evaluates the expected throughput and delay of the proposed MAC strategy, with numerical analysis validating the approach. Results show the hybrid MAC achieves a 66.7% throughput improvement over the history-based strategy and a 16.7% improvement over the priority-based approach while effectively reducing data collisions and delays. Furthermore, the hybrid MAC demonstrates an 8.3% increase in throughput compared to a previously proposed distributed hybrid MAC. Implementing this hybrid MAC in industrial-scale vehicular health monitoring can enhance vehicle safety, benefiting manufacturers and passengers alike.","2327-4662","","10.1109/JIOT.2024.3483228","University of Wolverhampton, UK, University of Oklahoma, USA; UK-Saudi Challenge Fund 2024 by British Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10735082","History-based medium access control (MAC);hybrid MAC;Internet of Things (IoT);Markov chain;priority-based MAC;vehicular health monitoring","Media Access Control;Throughput;Delays;Monitoring;Internet of Things;Scalability;Analytical models;Protocols;Sensors;IEEE 802.15 Standard","","","","41","IEEE","24 Oct 2024","","","IEEE","IEEE Journals"
"Application of Category Theory to Network Service Fault Detection","P. Martinez-Julia; V. P. Kafle; H. Asaeda","Network Architecture Laboratory, Network Research Institute, National Institute of Information and Communications Technology, Tokyo, Japan; Network Architecture Laboratory, Network Research Institute, National Institute of Information and Communications Technology, Tokyo, Japan; Network Architecture Laboratory, Network Research Institute, National Institute of Information and Communications Technology, Tokyo, Japan",IEEE Open Journal of the Communications Society,"30 Jul 2024","2024","5","","4417","4443","Network automation has become crucial in supporting services in 6G networks. This mainly derives from the complexity of the composition of numerous distributed virtual network functions (VNFs) in creating highly flexible virtual network services. Therefore, a network service automation system is a key technology enabler for 6G. However, the added complexity renders network service automation systems particularly sensitive to faults, some of which cause network outages that harm the smooth operation of basic societal services. Current state-of-the-art (SotA) solutions for fault detection can barely detect hidden faults. Herein, we propose a mechanism for automated network service analysis (ANSA), which constructs and analyzes a digital twin of a network service. The digital twin represents the available information about the network service based on category theory. It uses the properties of category theory to perform an analysis through which the faults of the network service are identified. We evaluate a prototype of a network service automation system that incorporates ANSA to demonstrate 1) the benefits of using digital twins for analyzing network services, 2) the benefits of using category theory for constructing digital twins of the network services, and 3) the resulting improvements in fault detection. Overall, ANSA can detect an average of 94% of the faults present in a network service. In comparison, previous SotA solutions can detect only 30%–50% of all faults.","2644-125X","","10.1109/OJCOMS.2024.3425831","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10592034","Network automation;fault detection;category theory","Fault detection;Automation;Proposals;Digital twins;Monitoring;Data models;6G mobile communication","","","","47","CCBY","10 Jul 2024","","","IEEE","IEEE Journals"
"Network Digital Twin Toward Networking, Telecommunications, and Traffic Engineering: A Survey","R. Poorzare; D. N. Kanellopoulos; V. K. Sharma; P. Dalapati; O. P. Waldhorst","Institute of Data-Centric Software Systems (IDSS), Karlsruhe University of Applied Sciences, Karlsruhe, Germany; Department of Mathematics, University of Patras, Patras, Greece; Department of Computer Science and Engineering, The LNM Institute of Information Technology, Jaipur, India; Department of Computer Science and Engineering, The LNM Institute of Information Technology, Jaipur, India; Institute of Data-Centric Software Systems (IDSS), Karlsruhe University of Applied Sciences, Karlsruhe, Germany",IEEE Access,"28 Jan 2025","2025","13","","16489","16538","Network Digital Twin (NDT) is an evolving technology that provides a framework through which a network administrator can have a virtual representation of a computer network. As a result, analysis, monitoring, testing, running new protocols, and more can be performed using the NDT before the final deployment of the developed approach. In this way, the consequences of direct deployment and the negative impact on network operations can be avoided. Telecommunications, along with traffic engineering as one of its critical components, play a prominent role across various networking domains, including Internet service providers, data centers, cellular networks, intelligent transportation systems, and smart cities. In this context, NDT has the potential to serve as a key enabler for optimizing these domains by providing a digital framework, which can facilitate the evaluation and enhancement of different scenarios. Accordingly, this paper presents a comprehensive survey on how NDT can facilitate advancements in network traffic engineering across a wide range of networking domains. First, we start with an in-depth analysis of the evolution of the network digital twin technology and provide a comparison with simulation tools. Next, we examine the role of NDT in various networking and telecommunication domains. We also explore the applicability of NDT technology from a traffic engineering perspective across different network types. Subsequently, we highlight key open research questions and potential future directions that warrant further investigation. Finally, we conclude by outlining the promising future trajectory of NDT within the aforementioned domains.","2169-3536","","10.1109/ACCESS.2025.3531947","Ministry of Science, Research, and the Arts Baden-Württemberg (MWK)(grant numbers:bwNET2.0); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10847826","Digital twin;network digital twin;telecommunications;traffic engineering;traffic optimization","Telecommunications;Data models;Digital twins;Telecommunication traffic;Surveys;Routing;Industrial Internet of Things;Big Data;Testing;Standardization","","","","303","CCBY","20 Jan 2025","","","IEEE","IEEE Journals"
"Enhanced Classification System for Real-Time Embedded Vision Applications","R. Khelifi; B. Nini; M. Berkane","Research Laboratory on Computer Science’s Complex Systems (ReLa(CS)2), University of Oum El Bouaghi, Oum El Bouaghi, Algeria; Research Laboratory on Computer Science’s Complex Systems (ReLa(CS)2), University of Oum El Bouaghi, Oum El Bouaghi, Algeria; Artificial Intelligence and Autonomous Things Laboratory, University of Oum El Bouaghi, Oum El Bouaghi, Algeria",IEEE Access,"8 Nov 2024","2024","12","","162311","162326","Embedded computer vision systems are increasingly being adopted across various domains, playing a pivotal role in enabling advanced technologies such as autonomous vehicles and industrial automation. Their cost-effectiveness, compact size, and portability make them particularly well-suited for diverse implementations and operations. In real-time scenarios, these systems must process visual data with minimal latency, which is crucial for immediate decision-making. However, these solutions continue to face significant challenges related to computational efficiency, memory usage, and accuracy. This research addresses these challenges by enhancing classification methodologies, specifically in Gray Level Co-occurrence Matrix (GLCM) feature extraction and Support Vector Machine (SVM) classifiers. To maintain a high level of accuracy while preserving performance, a smaller feature set is selected following a comprehensive complexity analysis and is further refined through Correlation-based Feature Selection (CFS). The proposed method achieves an overall classification accuracy of 84.76% with a feature set reduced by 79.2%, resulting in a 72.45% decrease in processing time, a 50% reduction in storage requirements, and up to a 77.8% decrease in memory demand during prediction. These improvements demonstrate the effectiveness of the proposed approach in improving the adaptability and capabilities of embedded vision systems (EVS), optimizing their performance under the constraints of real-time limited-resource environments.","2169-3536","","10.1109/ACCESS.2024.3489476","Research Laboratory on Computer Science’s Complex Systems (ReLa(CS)2), University of Oum El Bouaghi, Oum El Bouaghi, Algeria; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10740277","Embedded computer vision;limited resource systems;machine learning;pattern classification;real-time image processing","Accuracy;Support vector machines;Real-time systems;Feature extraction;Memory management;Computer vision;Surveillance;Bandwidth;Wildlife;Machine learning;Image processing","","","","53","CCBYNCND","31 Oct 2024","","","IEEE","IEEE Journals"
"Design and Validation of Cyber-Physical Systems Through Co-Simulation: The Voronoi Tessellation Use Case","C. Bernardeschi; A. Domenici; A. Fagiolini; M. Palmieri","Department of Information Engineering, University of Pisa, Pisa, Italy; Department of Information Engineering, University of Pisa, Pisa, Italy; Department of Engineering, MIRPALab.it, University of Palermo, Palermo, Italy; Department of Information Engineering, University of Pisa, Pisa, Italy",IEEE Access,"3 Jan 2024","2024","12","","1064","1075","This paper reports on the use of co-simulation techniques to build prototypes of co-operative autonomous robotic cyber-physical systems. Designing such systems involves a mission-specific planner algorithm, a control algorithm to drive an agent performing its task; and the plant model to simulate the agent dynamics. An application aimed at positioning a swarm of unmanned aerial vehicles (drones) in a bounded area, exploiting a Voronoi tessellation algorithm developed in this work, is taken as a case study. The paper shows how co-simulation allows testing the complex system at the design phase using models created with different languages and tools. The paper then reports on how the adopted co-simulation platform enables control parameters calibration, by exploiting design space exploration technology. The INTO-CPS co-simulation platform, compliant with the Functional Mock-up Interface standard to exchange dynamic simulation models using various languages, was used in this work. The different software modules were written in Modelica, C, and Python. In particular, the latter was used to implement an original variant of the Voronoi algorithm to tesselate a convex polygonal region, by means of dummy points added at appropriate positions outside the bounding polygon. A key contribution of this case study is that it demonstrates how an accurate simulation of a cooperative drone swarm requires modeling the physical plant together with the high-level coordination algorithm. The coupling of co-simulation and design space exploration has been demonstrated to support control parameter calibration to optimize energy consumption and convergence time to the target positions of the drone swarm. From a practical point of view, this makes it possible to test the ability of the swarm to self-deploy in space in order to achieve optimal detection coverage and allow unmanned aerial vehicles in a swarm to coordinate with each other.","2169-3536","","10.1109/ACCESS.2023.3347494","Italian Ministry of Education and Research (MIUR) in the framework of the FoReLab project (Departments of Excellence); Italian Ministry of Education and Research (MIUR) in the framework of the CrossLab project (Departments of Excellence); European Union through the Next Generation EU(grant numbers:ECS00000017); ‘Ecosistema dell’Innovazione’ Tuscany Health Ecosystem (THE), Piano Nazionale di Ripresa e Resilienza (PNRR), Spoke 3: Advanced technologies, methods and materials for human health and well-being; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10374084","Cyber-physical systems;co-simulation;unmanned aerial vehicles;space coverage;Voronoi tessellation;control parameter calibration","Drones;Mathematical models;Heuristic algorithms;Quadrotors;Vehicle dynamics;Autonomous aerial vehicles;Cyber-physical systems","","","","43","CCBY","25 Dec 2023","","","IEEE","IEEE Journals"
"VISE: QoS-Aware Virtual Sensor Orchestration in Green Multi-Sensor-Cloud","A. Mondal; S. Misra; G. Das","Advanced Technology Development Center, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; G. S. Sanyal School of Telecommunications, Indian Institute of Technology Kharagpur, Kharagpur, India",IEEE Transactions on Green Communications and Networking,"19 Nov 2024","2024","8","4","1440","1449","This work presents the problem of optimal resource orchestration for green Sensor-as-a-Service (Se-aaS) in the presence of multiple sensor-cloud service providers (SCSPs) and vehicle owners (VOs). It leads to an oligopolistic market, where the shareholders may adopt unfair means to maximize revenue. Therefore, we consider the trust factors among the SCSPs and the VOs and ensure high QoS to minimize service delay, maximize service availability, and maximize energy efficiency. We propose a virtual sensor orchestration scheme, VISE, to ensure high QoS in green multi-sensor-cloud systems. VISE is a multi-level expected utility game-theoretic scheme. In VISE, the requested SCSP initially aims to find an optimal set of vehicles to send the sensed data from its own sensor-equipped vehicles to the roadside units (RSUs). We consider that the intermediate nodes may belong to other SCSPs. Further, to address the inadequacy of resources, the requested SCSP in VISE relies on selecting an optimal SCSP from the other SCSPs to serve vehicular Se-aaS. We evaluate the performance of VISE both theoretically and experimentally and compare it with benchmark schemes. Simulation results depict 33.33 – 53.34% and 21.57 – 29.13% increase in end-user satisfaction and network lifetime, respectively, while ensuring a 37.25 – 42.85% increase in revenue sharing among the VOs.","2473-2400","","10.1109/TGCN.2024.3391628","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10505912","Green multi-sensor-cloud;vehicular networks;trust factor;oligopoly;resource allocation;expected utility theory","Virtualization;Quality of service;Cloud computing;Soft sensors;Wireless sensor networks;Resource management;Internet of Things;Vehicular ad hoc networks;Trusted computing","","","","23","IEEE","19 Apr 2024","","","IEEE","IEEE Journals"
"Heuristic Scheduling Algorithm for Workflow Applications in Cloud-Fog Computing Based on Realistic Client Port Communication","W. Chongdarakul; N. Aunsri","School of Information Technology, Mae Fah Luang University, Chiang Rai, Thailand; School of Information Technology, Mae Fah Luang University, Chiang Rai, Thailand",IEEE Access,"30 Sep 2024","2024","12","","134453","134485","Cloud-fog computing systems require task scheduling to efficiently manage resources and ensure timely task execution in a distributed environment. Fog computing extends computation, storage, and networking to the edge of the network, thereby improving the user experience, and reducing latency, and data transfer. Task scheduling and client-processing resource management in cloud-fog computing affect cloud system performance and client services. Currently, task scheduling methods mainly focus on assigning client tasks to the most suitable cloud resource or virtual machine (VM) to obtain the minimum scheduled time. However, these techniques do not consider a realistic communication model that, allows many parallel tasks to be sent and received simultaneously without delay. This paper introduces a heuristic workflow scheduling algorithm called Collision-Avoid based on Cloud task Scheduling (CACS) to assign client workflow represented by a directed acyclic graph (DAG) and execute the most appropriate cloud-fog nodes under the single client I/O port restriction to achieve minimum workflow completion time, least delay, and higher resource performance. FogWorkflowSim was used as the cloud-fog computing system simulation. The experimental results showed that the proposed method achieved much better scheduling results than the other compared algorithms.","2169-3536","","10.1109/ACCESS.2024.3462518","Office of the Permanent Secretary, Ministry of Higher Education, Science, Research and Innovation (OPS MHESI), Thailand Science Research and Innovation (TSRI), and Mae Fah Luang University(grant numbers:RGNS 63-193); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10681397","Cloud computing;cloud-fog computing system;cloudsim;fog computing;FogWorkflowSim;task scheduling;workflow scheduling;heuristic task scheduling","Cloud computing;Heuristic algorithms;Costs;Dynamic scheduling;Optimal scheduling;Scheduling algorithms;Resource management;Edge computing","","","","49","CCBYNCND","17 Sep 2024","","","IEEE","IEEE Journals"
"Serverless Microservice Architecture for Cloud-Edge Intelligence in Sensor Networks","D. Loconte; S. Ieva; F. Gramegna; I. Bilenchi; C. Fasciano; A. Pinto; G. Loseto; F. Scioscia; M. Ruta; E. Di Sciascio","Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Department of Engineering, LUM “Giuseppe Degennaro” University, Casamassima, Italy; Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy",IEEE Sensors Journal,"5 Mar 2025","2025","25","5","7875","7885","Machine learning (ML) is increasingly exploited in a wide range of application areas to analyze data streams from large-scale sensor networks, train predictive models, and perform inference. The cloud-edge intelligence (CEI) computing paradigm integrates cloud infrastructures for resource-intensive ML tasks with devices at the border of a local network for distributed data preprocessing, small-scale model training, and prediction tasks. This can achieve a tunable trade-off of ML accuracy with improved data privacy, response latency, and bandwidth usage. Prevalent CEI architectures are based on microservices encapsulated in containers, but serverless computing is emerging as an alternative model. It is based on stateless event-driven functions to facilitate the development and provisioning of application components, increase the infrastructure elasticity, and reduce management effort. This article proposes a novel CEI framework for sensor-based applications, exploiting serverless computing for data management and ML tasks. Small-scale model training occurs at the edge with local data for quick prediction response, while large-scale models are trained in the cloud with the full sensor network data, and then, they are fed back to edge nodes for a progressive accuracy improvement. A fully functional prototype has been built by leveraging open-source software tools, selected devices for field sensing and edge computing (EC), and a commercial cloud platform. Experiments validate the feasibility and sustainability of the proposal, compared with an existing container-oriented microservice architecture.","1558-1748","","10.1109/JSEN.2024.3502254","Spoke 9 of the Italian National Center for High-Performance Computing, Big Data and Quantum Computing; Digital Enterprise Grant; Lutech S.p.A. and European Regional Development Fund for Apulia Region 2014/2020 Operating Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10767204","Cloud-edge intelligence (CEI);machine learning (ML);microservices;sensor networks;serverless computing","Cloud computing;Sensors;Training;Computer architecture;Microservice architectures;Computational modeling;Data models;Accuracy;Internet of Things;Intelligent sensors","","","","41","CCBY","25 Nov 2024","","","IEEE","IEEE Journals"
"Adaptive VNF Placement Considering Overall Latency and 5G Wireless Channel Reliability in Industry 4.0: A Reinforcement Learning Based Approach","N. Saqib; N. F. Abdullah; A. Abu-Samah; H. A. H. Alobaidy; R. Nordin","Department of Electrical, Electronic and Systems Engineering, Wireless Research@UKM, Universiti Kebangsaan Malaysia, Bangi, Malaysia; Department of Electrical, Electronic and Systems Engineering, Wireless Research@UKM, Universiti Kebangsaan Malaysia, Bangi, Malaysia; Department of Electrical, Electronic and Systems Engineering, Wireless Research@UKM, Universiti Kebangsaan Malaysia, Bangi, Malaysia; Department of Electrical, Electronic and Systems Engineering, Wireless Research@UKM, Universiti Kebangsaan Malaysia, Bangi, Malaysia; Department of Engineering, School of Engineering and Technology, Sunway University, Bandar Sunway, Selangor, Malaysia",IEEE Access,"2 Jul 2024","2024","12","","88883","88896","Industry 4.0 incorporates the integration of cloud computing, Industrial Internet of Things (IIoT), and modern communication technologies within the industrial automation systems. Various devices with different network requirements of high reliability and low latency, rely on connectivity. The 5G and Beyond (B5G) software-defined architecture facilitates Network Function Virtualization (NFV), which is an essential solution for fulfilling these stringent demands. NFV allows for the implementation and control of Virtual Network Functions (VNFs) in dynamic network environments. VNF placement optimization has been extensively studied in the 5G perspective outside the industry environment with a focus on minimizing delay and cost, increasing VNF reliability, and increasing resource efficiency. However, the complex dynamics of the wireless channel in industrial environments have a considerable impact on the essential delay factors that are important for optimizing the deployment of VNFs. This study focuses on modeling a Wireless Sensor Network (WSN) based Industry 4.0 factory automation scenario at mmWave band, formulating an optimization problem to minimize overall delay while considering packet loss rate in the 5G industrial wireless channel. The optimization problem is formulated as a Markov Decision Process (MDP) and two Reinforcement Learning (RL) based algorithms AVP-Q and AVP-DQN are proposed for optimizing the VNF placement. The proposed algorithms are extensively evaluated against the Value Iteration algorithm which assumes a completely known MDP model and two other algorithms from the literature. The simulated results show that AVP-DQN outperforms existing algorithms for this scenario by 39% and 22.6% and the achieved performance is only close to that of the Value Iteration algorithm.","2169-3536","","10.1109/ACCESS.2024.3419065","Universiti Kebangsaan Malaysia(grant numbers:DPK-2022-009,GUP-2021-023); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10571938","5G;industry 4.0;deep reinforcement learning;VNF placement;URLLC;mmWave","Wireless sensor networks;5G mobile communication;Wireless communication;Cloud computing;Production facilities;Reliability;Fourth Industrial Revolution;Deep reinforcement learning;Millimeter wave communication","","","","61","CCBYNCND","26 Jun 2024","","","IEEE","IEEE Journals"
"Drone-Based AI System for Wildfire Monitoring and Risk Prediction","C. A. S. Lelis; J. J. Roncal; L. Silveira; R. D. G. De Aquino; C. A. C. Marcondes; J. Marques; D. S. Loubach; F. A. N. Verri; V. V. Curtis; D. G. De Souza","Computer Science Division, Instituto Tecnológico de Aeronáutica (ITA), São José dos Campos, Brazil; Computer Science Division, Instituto Tecnológico de Aeronáutica (ITA), São José dos Campos, Brazil; Computer Science Division, Instituto Tecnológico de Aeronáutica (ITA), São José dos Campos, Brazil; Computer Science Division, Instituto Tecnológico de Aeronáutica (ITA), São José dos Campos, Brazil; Computer Science Division, Instituto Tecnológico de Aeronáutica (ITA), São José dos Campos, Brazil; Computer Science Division, Instituto Tecnológico de Aeronáutica (ITA), São José dos Campos, Brazil; Computer Science Division, Instituto Tecnológico de Aeronáutica (ITA), São José dos Campos, Brazil; Computer Science Division, Instituto Tecnológico de Aeronáutica (ITA), São José dos Campos, Brazil; Computer Science Division, Instituto Tecnológico de Aeronáutica (ITA), São José dos Campos, Brazil; Computer Science Division, Instituto Tecnológico de Aeronáutica (ITA), São José dos Campos, Brazil",IEEE Access,"4 Oct 2024","2024","12","","139865","139882","Wildfires pose a significant threat to ecosystems, human lives, and infrastructure worldwide. Traditional wildfire detection and risk assessment methods often suffer from limitations such as delayed detection and low confidence in certain regions. In this paper, we propose a novel computational system based on Machine Learning for wildfire risk assessment using data collected by drones. The system can integrate various sensors to capture spatiotemporal data on environmental factors such as temperature, humidity, and vegetation. By leveraging high-resolution data collected through autonomous drone missions, our system enhances wildfire risk estimation and enables proactive mission planning. Although the system is mainly designed to address wildfire monitoring using drone-collected data, it can be easily adapted to other environmental monitoring applications and other sources of data. We demonstrate the effectiveness of our approach through a comprehensive evaluation and validation process in both simulated and real-world environments. Our work contributes to advancing wildfire monitoring capabilities, improving early detection, and mitigating the impact of wildfires on communities and the environment.","2169-3536","","10.1109/ACCESS.2024.3462436","Financiadora de Estudos e Projetos (FINEP)(grant numbers:01.22.0615.00); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10681400","Aerial drones;artificial intelligence;environmental monitoring;machine learning;risk assessment;spatiotemporal data;wildfire detection;wildfire risk estimation","Wildfires;Predictive models;Sensors;Normalized difference vegetation index;Measurement;Drones;Artificial intelligence;Environmental monitoring;Machine learning;Risk management;Spatiotemporal phenomena","","","","37","CCBYNCND","17 Sep 2024","","","IEEE","IEEE Journals"
"Middleware for Distributed Fulcrum Coding on the Fly: Leveraging Efficient Communications for 5G-IoT Heterogeneous Mobile Devices","Y. R. Julio; I. G. Garcia; J. M. Díaz; R. Agüero","Department of Computer Science and Engineering, Universidad del Norte, Barranquilla, Colombia; Department of Mathematics, Universidad del Norte, Barranquilla, Colombia; Department of Computer Science and Engineering, Universidad del Norte, Barranquilla, Colombia; Department of Communications Engineering, Universidad de Cantabria, Santander, Spain",IEEE Access,"21 Dec 2023","2023","11","","142538","142551","We formalize and evaluate an adaptation of the Middleware Fulcrum coding scheme, developed under a Fog computing architecture, for 5G-IoT mobile devices. This adaptation includes a D2D extension with distributed coding, specifically designed to address the challenges posed by a dynamic topology involving heterogeneous devices in processing capabilities. The proposed method is established at the application layer level to allow for the mapping of packet segments to a finite field scheme (Galois field), which allows for exploiting the algebraic properties of the system to exercise data retrieval with encoded information. In addition, it will enable dynamic adjustment of the coding parameters according to the processing capacity of the communication devices, wireless channel conditions, network topology, and type of stream in the system. In this sense, high-performance coding is distributed to nodes that can offer higher coding complexity in the design, effectively improving the wireless network’s performance in many aspects, such as delay, energy consumption, and goodput. By characterizing the system as an algebraic system, it is possible to exploit the advantages of FEC through decoding to reduce the signaling in the system.","2169-3536","","10.1109/ACCESS.2023.3342916","Universidad del Norte within the research “Design of Next-Generation Distributed Codes and a Balance in the Performance of Multicast Networks”; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10360114","Middleware;fulcrum code;fog computing;Galois field;5G-IoT mobile devices","Encoding;Codes;Network coding;5G mobile communication;Middleware;Wireless communication;Performance evaluation","","","","54","CCBYNCND","14 Dec 2023","","","IEEE","IEEE Journals"
"The Role of Blockchain in Transforming Industries Beyond Finance","C. Vanmathi; A. Farouk; S. M. Alhammad; R. Mangayarkarasi; S. Bhattacharya; M. S. B. Kasyapa","School of Computer Science Engineering and Information Systems, Vellore Institute of Technology, Vellore, Tamil Nadu, India; Department of Computer Science, Faculty of Computers and Artificial Intelligence, Hurghada University, Hurghada, Egypt; Department of Computer Sciences, College of Computer and Information Sciences, Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia; School of Computer Science Engineering and Information Systems, Vellore Institute of Technology, Vellore, Tamil Nadu, India; School of Computer Science Engineering and Information Systems, Vellore Institute of Technology, Vellore, Tamil Nadu, India; School of Computer Science Engineering and Information Systems, Vellore Institute of Technology, Vellore, Tamil Nadu, India",IEEE Access,"18 Oct 2024","2024","12","","148845","148867","Blockchain technology is rapidly transforming how businesses and individuals interact. Its inherent security features, including immutability, tamper-proofing, and verifiable data provenance, are driving its adoption across numerous sectors. The financial sector heavily influences blockchain for its tamper-proof nature, leading to the rise of cryptocurrencies, the technology offers much more. Beyond security, it streamlines processes, reduces costs, and improves customer experience. Smart contracts are one of the predominant key components that automate predetermined operations, further enhancing blockchain’s capabilities. Understanding the importance of blockchain in non-financial applications is crucial for fully exploiting its potential. This analysis explores the fundamentals of blockchain technology and its applications beyond finance. To meet the growing demand across various sectors, blockchain must integrate with other trending technologies such as the Internet of Things (IoT), Artificial Intelligence (AI), and the edge-cloud paradigm. The analysis examines the role of these technologies in facilitating blockchain-enabled applications. Furthermore, the paper researches the specific use cases such as insurance, energy, healthcare, digital voting, supply chain management and government. Concentrating on these sectors elucidates how organisations harness blockchain to tackle intricate challenges, thereby fostering the advancement of sophisticated digital processes and contributing to societal progression.","2169-3536","","10.1109/ACCESS.2024.3468611","Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia, through the Researchers Supporting Project(grant numbers:PNURSP2024R442); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10695085","Blockchain;non-financial applications;energy;insurance;healthcare;decentralization","Blockchains;Medical services;Smart contracts;Supply chains;Supply chain management;Open source software;Finance","","","","180","CCBYNCND","26 Sep 2024","","","IEEE","IEEE Journals"
"A Survey on Task Allocation and Scheduling in Robotic Network Systems","S. Alirezazadeh; L. A. Alexandre","C4-Cloud Computing Competence Center, Universidade da Beira Interior, Covilhã, Portugal; NOVA LINCS, Universidade da Beira Interior, Covilhã, Portugal",IEEE Internet of Things Journal,"10 Jan 2025","2025","12","2","1484","1508","Robotic networks are increasingly relied upon to perform complex tasks that require efficient scheduling and task allocation to optimize processing power, resource management, and energy use. The primary goal in these systems is to enhance performance by minimizing completion time, energy consumption, and delays, while maximizing resource utilization and task throughput. Numerous studies have examined different aspects of task allocation and scheduling, from static approaches to dynamic models that adapt to real-time conditions. This article presents a comprehensive survey of the methods and strategies used in robotic network systems, considering not only traditional approaches but also the role of emerging technologies, such as cloud, fog, and edge computing. We categorize the literature from three perspectives: 1) architectures and applications; 2) methods; and 3) parameters. Furthermore, we analyze the limitations of each approach and propose directions for future research, with a particular focus on scalability, real-world applicability, and the integration of these technologies in dynamic environments.","2327-4662","","10.1109/JIOT.2024.3491944","Operation Centro-01-0145-FEDER-000019— C4—Centro de Competências em Cloud Computing, co-financed by the European Regional Development Fund (ERDF); Programa Operacional Regional do Centro (Centro 2020), in the scope of the Sistema de Apoio à Investigação Cientifíca e Tecnológica—Programas Integrados de IC&DT;; NOVA LINCS; FCT.IP(grant numbers:UIDB/04516/2020,UIDP/04516/2020); Computer Science and Communication Research Centre (CIIC), Escola Superior de Tecnologia e Gestão(grant numbers:UIDB/04524/2020); FCT-Fundação para a Ciência e a Tecnologia(grant numbers:CEECINST/00060/2021/CP2902/CT0009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10742644","Cloud;edge;fog;load balancing;robotic network;scheduling;task allocation","Robots;Resource management;Cloud computing;Processor scheduling;Dynamic scheduling;Edge computing;Job shop scheduling;Optimization;Internet of Things;Real-time systems","","","","109","IEEE","5 Nov 2024","","","IEEE","IEEE Journals"
"GAS: DVFS-Driven Energy Efficiency Approach for Latency-Guaranteed Edge Computing Microservices","Z. Bellal; L. Lahlou; N. Kara; I. El Khayat","Software and Information Technology Engineering Department, École de technologie supérieure, Montréal, QC, Canada; Software and Information Technology Engineering Department, École de technologie supérieure, Montréal, QC, Canada; Software and Information Technology Engineering Department, École de technologie supérieure, Montréal, QC, Canada; Ericsson, Mississauga, ON, Canada",IEEE Transactions on Green Communications and Networking,"13 Feb 2025","2025","9","1","108","124","Edge computing-based microservices (ECM) are pivotal infrastructure components for latency-critical applications such as Virtual Reality/Augmented Reality (VR/AR) and the Internet of Things (IoT). ECM involves strategically deploying microservices at the network’s edge to fulfill the low latency needs of modern applications. However, achieving efficient resource and energy consumption while meeting the latency requirement in the ECM environment remains challenging. Dynamic Voltage and Frequency Scaling (DVFS) is a common technique to address this issue. It adjusts the CPU frequency and voltage to balance energy cost and performance. However, selecting the optimal CPU frequency depends on the nature of the microservice workload (e.g., CPU-bound, memory-bound, or mixed). Moreover, various microservices with different latency requirement can be deployed on the same edge node. This makes the DVFS application extremely challenging, particularly for a chip-wide DVFS implementation for which CPU cores operate at the same frequency and voltage. To this end, we propose GAS, enerGy Aware microServices edge computing framework, which enables CPU frequency scaling to meet diverse microservice latency requirement with the minimum energy cost. Our evaluation indicates that our CPU scaling policy decreases energy consumption by 5% to 23% compared to Linux governors while maintaining latency requirement and significantly contributing to sustainable edge computing.","2473-2400","","10.1109/TGCN.2024.3420957","Natural Sciences and Engineering Research Council and Ericsson Canada(grant numbers:561771); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10578041","Edge computing;microservice;DVFS;energy-efficient;container autoscaling","Microservice architectures;Edge computing;Energy consumption;Energy efficiency;Task analysis;Frequency diversity;Time-frequency analysis","","","","42","IEEE","1 Jul 2024","","","IEEE","IEEE Journals"
"A Long-Distance Smart Driving Service Based on Floating Car Data and Open Data","D. Escuín; L. Polo; D. Ciprés; C. Millán; J. Carcas","Instituto Tecnológico de Aragón, Zaragoza, Spain; Instituto Tecnológico de Aragón, Zaragoza, Spain; Instituto Tecnológico de Aragón, Zaragoza, Spain; Instituto Tecnológico de Aragón, Zaragoza, Spain; Grupo Sese, Zaragoza, Spain",IEEE Access,"8 Aug 2022","2022","10","","80833","80846","The purpose of this manuscript is to present the Smart Driving Service (SDS), a customized mobile application, and a complex microservices framework intended not only for professional drivers but also for novel people who need help during the driving time in their long-distance journeys. The European regulation on driving times, breaks and rest periods for drivers engaged in the carriage of freight is implemented in the system. Additionally, it is necessary to have a feedback report to detect the behavior of drivers and what to do differently to improve driving. This issue is addressed by implementing a Route Performance Index (RPI) to measure the driver compliance. The proposed service has been running in a production stage for 6 months with a reduction in consumption of 2 liters/100 km. Considering that the company runs more than 100M km per year, the savings in fuel are relevant apart from the environmental impact reduction.","2169-3536","","10.1109/ACCESS.2022.3195569","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9846968","Driver assistance;route navigation;floating car data;telemetry;truck sensors;open data","Autonomous automobiles;Road traffic;Behavioral sciences;Sensors;Navigation;Intelligent sensors;Telemetry;Open data","","","","42","CCBY","1 Aug 2022","","","IEEE","IEEE Journals"
"An AI-Assisted Framework for Lifecycle Management of Beyond 5G Services","A. -I. Manolopoulos; V. -M. Alevizaki; M. Anastasopoulos; A. Tzanakaki","Department of Physics, National and Kapodistrian University of Athens, Athens, Greece; Department of Physics, National and Kapodistrian University of Athens, Athens, Greece; Department of Physics, National and Kapodistrian University of Athens, Athens, Greece; Department of Physics, National and Kapodistrian University of Athens, Athens, Greece",IEEE Access,"5 Dec 2024","2024","12","","179449","179463","Future mobile communication networks aim to offer services and applications in the most flexible, adaptable and cost-effective manner. B5G networks aim at a fully softwarized network architecture, where hardware and software programming is used for the design, implementation, deployment, management, monitoring and maintenance of network equipment/components/services. Artificial Intelligence (AI) and Machine Learning (ML) techniques are steadily being integrated into 5G systems, offering intelligent automation, proactive network management, and resource allocation optimization. In this environment, the role of Management and Orchestration (MANO) is vital to ensure efficient infrastructure utilization and fulfillment of heterogeneous service requirements. Despite the development of various tools and platforms to facilitate MANO in 5G systems, in most cases there is still the need of human intervention and manual input for configuring the 5G elements according to service requirements. In this paper, a MANO framework has been developed, that specifically targets the orchestration operations of 5G networks. The proposed framework focuses on the lifecycle management of the 5G components, in order to achieve an operational environment with minimal human intervention or manual configuration (Zero Touch Networks -ZTN). Within this ecosystem, an Analytics & AI/ML Platform has comprehensive monitoring capabilities and influences decisions across various layers or aspects of the infrastructure. This includes optimizing the allocation and orchestration of both networking and edge/cloud computing virtual resources within the infrastructure.","2169-3536","","10.1109/ACCESS.2024.3507359","European Union Projects 5G Trusted and Secure Network Services(grant numbers:101127973); Seamless Integration of Efficient 6G Wireless Technologies for Communication and Sensing(grant numbers:101139282); ECO-eNET(grant numbers:101139133); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10769360","5G;B5G;6G;MANO;slicing;NFV;LSTM;LCM;ZSM","5G mobile communication;Cloud computing;Monitoring;Computer architecture;Quality of service;Resource management;Optimization;Maintenance;Virtualization;Ultra reliable low latency communication","","","","51","CCBY","27 Nov 2024","","","IEEE","IEEE Journals"
"Design of an Optimal Scheduling Control System for Smart Manufacturing Processes in Tobacco Industry","X. Liu; J. Li; H. Wang; W. Jia; J. Yang; Z. Guo","Qingzhou Cigarette Factory, China Tobacco Shandong Industrial Co.Ltd., Qingzhou, China; Qingzhou Cigarette Factory, China Tobacco Shandong Industrial Co.Ltd., Qingzhou, China; Qingzhou Cigarette Factory, China Tobacco Shandong Industrial Co.Ltd., Qingzhou, China; Qingzhou Cigarette Factory, China Tobacco Shandong Industrial Co.Ltd., Qingzhou, China; School of Artificial Intelligence, Chongqing Technology and Business University, Chongqing, China; School of Artificial Intelligence, Chongqing Technology and Business University, Chongqing, China",IEEE Access,"6 Apr 2023","2023","11","","33027","33036","The whole process of tobacco production is composed of many components, in which their operation and administration are currently independent. It is required to deploy smart manufacturing workflow for the whole production process, in order to realize centralized effective global scheduling. This requires an advanced administration control platform that has strong abilities of multisource data integration and automatic decision support. To bridge such research gap, this paper designs an optimal scheduling control system for smart manufacturing processes of tobacco industry. First of all, this work discusses major characteristics of future-generation production control patterns in intelligent tobacco factories (ITF). Then, a five-layer architecture for optimal scheduling control of ITF is proposed, which contains Internet-of-Things layer, centralized control layer, model layer, platform layer and operation layer. In addition, a production scheduling optimization strategy is also developed for the proposed system to serve as the software algorithm that drives the running of whole smart manufacturing processes. Finally, this paper presents a comparative analysis of the proposed system’s transformation in a cigarette factory. Naturally, the effectiveness of the proposed production optimization scheduling strategy is verified through simulation.","2169-3536","","10.1109/ACCESS.2023.3261883","China Tobacco Shandong Industrial Co., Ltd.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10081360","Smart manufacturing;optimal scheduling control;Internet of Things;production decision","Production;Manufacturing;Industries;Production facilities;Centralized control;Data models;Process control","","","","30","CCBY","27 Mar 2023","","","IEEE","IEEE Journals"
"Long Live the Image: On Enabling Resilient Production Database Containers for Microservice Applications","Z. Li; N. Saldías-Vallejos; D. Seco; M. A. Rodríguez; R. Ranjan","School of Electronics, Electrical Engineering and Computer Science, Queen’s University Belfast, Belfast, U.K.; Department of Computer Science, University of Concepción, Concepción, Chile; Department of Computer Science and Information Technologies, Universidade de A Coruña, A Coruña, Spain; Department of Computer Science, University of Concepción, Concepción, Chile; School of Computing, Newcastle University, Newcastle upon Tyne, U.K.",IEEE Transactions on Software Engineering,"18 Sep 2024","2024","50","9","2363","2378","Microservices architecture advocates decentralized data ownership for building software systems. Particularly, in the Database per Service pattern, each microservice is supposed to maintain its own database and to handle the data related to its functionality. When implementing microservices in practice, however, there seems to be a paradox: The de facto technology (i.e., containerization) for microservice implementation is claimed to be unsuitable for the microservice component (i.e., database) in production environments, mainly due to the data persistence issues (e.g., dangling volumes) and security concerns. As a result, the existing discussions generally suggest replacing database containers with cloud database services, while leaving the on-premises microservice implementation out of consideration. After identifying three statelessness-dominant application scenarios, we proposed container-native data persistence as a conditional solution to enable resilient database containers in production. In essence, this data persistence solution distinguishes stateless data access (i.e., reading) from stateful data processing (i.e., creating, updating, and deleting), and thus it aims at the development of stateless microservices for suitable applications. In addition to developing our proposal, this research is particularly focused on its validation, via prototyping the solution and evaluating its performance, and via applying this solution to two real-world microservice applications. From the industrial perspective, the validation results have proved the feasibility, usability, and efficiency of fully containerized microservices for production in applicable situations. From the academic perspective, this research has shed light on the operation-side micro-optimization of individual microservices, which fundamentally expands the scope of “software micro-optimization” and reveals new research opportunities.","1939-3520","","10.1109/TSE.2024.3436623","ANID Millennium Science Initiative Program(grant numbers:Code ICN17_002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10620003","Container;data persistence;read-only database;stateless microservice;microservices architecture","Databases;Containers;Microservice architectures;Production;Usability;Runtime;Prototypes","","","","67","IEEE","1 Aug 2024","","","IEEE","IEEE Journals"
"Retrospecting Available CPU Resources: SMT-Aware Scheduling to Prevent SLA Violations in Data Centers","H. Liao; T. -y. Liu; J. Guo; B. Huang; D. Yang; J. Ding","School of Data Science and Engineering, East China Normal University, Shanghai, China; School of Data Science and Engineering, East China Normal University, Shanghai, China; School of Data Science and Engineering, East China Normal University, Shanghai, China; School of Data Science and Engineering, East China Normal University, Shanghai, China; TRE of Alibaba Group, Zhejiang, China; SATG of Intel, Shanghai, China",IEEE Transactions on Parallel and Distributed Systems,"28 Nov 2024","2025","36","1","67","83","The article focuses on an understudied yet fundamental problem: existing methods typically average the utilization of multiple hardware threads to evaluate the available CPU resources. However, the approach could underestimate the actual usage of the underlying physical core for Simultaneous Multi-Threading (SMT) processors, leading to an overestimation of remaining resources. The overestimation propagates from microarchitecture to operating systems and cloud schedulers, which may misguide scheduling decisions, exacerbate CPU overcommitment, and increase Service Level Agreement (SLA) violations. To address the potential overestimation problem, we propose an SMT-aware and purely data-driven approach named Remaining CPU (RCPU) that reserves more CPU resources to restrict CPU overcommitment and prevent SLA violations. RCPU requires only a few modifications to the existing cloud infrastructures and can be scaled up to large data centers. Extensive evaluations in the data center proved that RCPU contributes to a reduction of SLA violations by 18% on average for 98% of all latency-sensitive applications. Under a benchmarking experiment, we prove that RCPU increases the accuracy by 69% in terms of Mean Absolute Error (MAE) compared to the state-of-the-art.","1558-2183","","10.1109/TPDS.2024.3494879","National Natural Science Foundation of China(grant numbers:62272167); Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10748366","Cloud computing;SMT interference;data center;QoS;microarchitecture;latency-sensitive applications","Hardware;Data centers;Cloud computing;Servers;Processor scheduling;Program processors;Monitoring;Message systems;Benchmark testing;Accuracy","","","","102","IEEE","8 Nov 2024","","","IEEE","IEEE Journals"
"DigiWind-An Open-Source Digital Twin Framework for Wind Energy Systems","M. Wiens; G. Steindl; C. Tubeuf; F. Birkelbach; J. Burfeind; T. Meyer","Fraunhofer IWES, Bremerhaven, Germany; Institute of Computer Engineering, TU Wien, Vienna, Austria; Institute of Energy Systems and Thermodynamics, TU Wien, Vienna, Austria; Institute of Energy Systems and Thermodynamics, TU Wien, Vienna, Austria; Fraunhofer IWES, Bremerhaven, Germany; Fraunhofer IWES, Bremerhaven, Germany",IEEE Access,"20 Jun 2024","2024","12","","84046","84063","This study introduces DigiWind, an extensible digital twin platform specifically designed for the wind energy domain. The research aims to identify the fundamental requirements and architectural design for such a platform. Functional requirements are identified through a requirements engineering process using the use-case methodology. Existing digital twin and co-simulation platforms are reviewed, and the proposed DigiWind architecture is presented in detail. Three use cases are presented to highlight the integration of workflows and simulation models in the digital twin process. The DigiWind platform features a layered architecture with core implementations such as the template service, model assembly service, co-simulation service, and measurement data service. These components enable the automation of simulations, incorporation of historic measurement data, and data feedback and exchange. The platform supports various use cases including retrospective evaluation, performance monitoring, and scenario simulations. Additionally, a knowledge base and a versioning system ensure automation, documentation, and reproducibility of simulation results. The platform’s openness promotes collaboration among wind energy stakeholders and supports standardized models for co-simulation using the Functional Mock-up Interface. Overall, DigiWind offers a solution for developing, managing, and integrating digital twins in the wind energy sector, enhancing wind farm performance and operational efficiency.","2169-3536","","10.1109/ACCESS.2024.3414335","vgbe energy e.V.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556529","Automation;digital twin;knowledge representation;ontologies;wind energy","Digital twins;Wind energy;Python;Production;Biological system modeling;Wind turbines;Data models;Automation;Knowledge representation","","","","40","CCBYNCND","13 Jun 2024","","","IEEE","IEEE Journals"
"Process Manufacturing Intelligence Empowered by Industrial Metaverse: A Survey","W. Luo; K. Huang; X. Liang; H. Ren; N. Zhou; C. Zhang; C. Yang; W. Gui","Department of Strategic and Advanced Interdisciplinary Research, Pengcheng Laboratory, Shenzhen, China; Department of Strategic and Advanced Interdisciplinary Research, Pengcheng Laboratory, Shenzhen, China; Department of Strategic and Advanced Interdisciplinary Research, Pengcheng Laboratory, Shenzhen, China; Department of Strategic and Advanced Interdisciplinary Research, Pengcheng Laboratory, Shenzhen, China; Department of Strategic and Advanced Interdisciplinary Research, Pengcheng Laboratory, Shenzhen, China; Department of Strategic and Advanced Interdisciplinary Research, Pengcheng Laboratory, Shenzhen, China; Department of Strategic and Advanced Interdisciplinary Research, Pengcheng Laboratory, Shenzhen, China; Department of Strategic and Advanced Interdisciplinary Research, Pengcheng Laboratory, Shenzhen, China",IEEE Transactions on Cybernetics,"30 Oct 2024","2024","54","11","6679","6692","The intelligent goal of process manufacturing is to achieve high efficiency and greening of the entire production. Whereas the information system it used is functionally independent, resulting to knowledge gaps between each level. Decision-making still requires lots of knowledge workers making manually. The industrial metaverse is a necessary means to bridge the knowledge gaps by sharing and collaborative decision-making. Considering the safety and stability requirements of the process manufacturing, this article conducts a thorough survey on the process manufacturing intelligence empowered by industrial metaverse. First, it analyzes the current status and challenges of process manufacturing intelligence, and then summarizes the latest developments about key enabling technologies of industrial metaverse, such as interconnection technologies, artificial intelligence, cloud-edge computing, digital twin (DT), immersive interaction, and blockchain technology. On this basis, taking into account the characteristics of process manufacturing, a construction approach and architecture for the process industrial metaverse is proposed: a virtual-real fused industrial metaverse construction method that combines DTs with physical avatar, which can effectively ensure the safety of metaverse’s application in industrial scenarios. Finally, we conducted preliminary exploration and research, to prove the feasibility of proposed method.","2168-2275","","10.1109/TCYB.2024.3420958","Major Key Project of PCL(grant numbers:PCL2023A09,PCL2023A08); National Natural Science Foundation of China(grant numbers:62103208,62203241); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10605094","Industrial metaverse;intelligent manufacturing;process manufacturing;virtual-real fusion","Metaverse;Manufacturing;Decision making;Process control;Optimization;Intelligent manufacturing systems;Process monitoring;Virtual reality;Industrial engineering;Production engineering","","","","95","IEEE","19 Jul 2024","","","IEEE","IEEE Journals"
"Understanding Serverless Inference in Mobile-Edge Networks: A Benchmark Approach","J. Chen; Y. Lin; S. Peng; S. Wu; K. Kent; H. Dai; K. Ye; Y. Wang","Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; University of New Brunswick, Canada; University of Southampton, U.K; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China",IEEE Transactions on Cloud Computing,"7 Mar 2025","2025","13","1","198","212","Although the emerging serverless paradigm has the potential to become a dominant way of deploying cloud-service tasks across millions of mobile and IoT devices, the overhead characteristics of executing these tasks on such a volume of mobile devices remain largely unclear. To address this issue, this paper conducts a deep analysis based on the OpenFaaS platform—a popular open-source serverless platform for mobile edge environments—to investigate the overhead of performing deep learning inference tasks on mobile devices. To thoroughly evaluate the inference overhead, we develop a performance benchmark, named ESBench, whereby a set of comprehensive experiments are conducted with respect to a bunch of simulated mobile devices associated with an edge cluster. Our investigation reveals that the performance of deep learning inference tasks is significantly influenced by the model size and resource contention in mobile devices, leading to up to $3\times$3× degradation in performance. Moreover, we observe that the network environment can negatively impact the performance of mobile inference, increasing the CPU overhead under poor network conditions. Based on our findings, we further propose some recommendations for designing efficient serverless platforms and resource management strategies as well as for deploying serverless computing in the mobile edge environment.","2168-7161","","10.1109/TCC.2024.3521657","3rd Xinjiang Scientific Expedition Program(grant numbers:2021xjkk1300); SIAT-SunTang Big Data&AI Joint Innovation Laboratory(grant numbers:E3Z092); Shenzhen Science and Technology Plan Project(grant numbers:SGDX20220530111001003); Shenzhen Science and Technology Program(grant numbers:CJGJZD20230724093659004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10814988","Serverless;edge computing;deep learning inference;model serving;benchmark","Performance evaluation;Computational modeling;Mobile handsets;Serverless computing;Deep learning;Memory management;Degradation;Benchmark testing;Loading;Runtime environment","","","","59","IEEE","24 Dec 2024","","","IEEE","IEEE Journals"
"Accelerating Digital Twin Development With Generative AI: A Framework for 3D Modeling and Data Integration","S. Gebreab; A. Musamih; K. Salah; R. Jayaraman; D. Boscovic","Department of Computer and Information Engineering, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Management Science and Engineering, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Computer and Information Engineering, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Industrial Engineering, New Mexico State University, Las Cruces, NM, USA; Center for AI and Data Analytics, Blockchain Research Laboratory, Arizona State University, Tempe, AZ, USA",IEEE Access,"17 Dec 2024","2024","12","","185918","185936","Digital twins (DTs) have been introduced as valuable tools for digitally representing physical objects or assets. However, developing comprehensive and accurate DTs remains challenging due to the complexity of adding diverse data sources, creating realistic models, and enabling real-time synchronization. In this paper, we propose a DT framework that uses Generative Artificial Intelligence (GenAI) techniques integrated into the DT development pipeline to address these challenges and accelerate the creation of these virtual representations. We demonstrate how 3D generative models utilizing pre-trained 2D diffusion models, and Large Language Models (LLMs) can automate and accelerate key stages of the DT development process, which include 3D modeling, data acquisition and integration, as well as simulation and monitoring. By providing a use-case scenario of a smart medical cooler box, we demonstrate the effectiveness of the proposed framework, highlighting the potential of GenAI to reduce manual effort and streamline the integration of DT components. In particular, we illustrate how it can accelerate the creation of 3D models for DTs from 2D images by using 2D-to-3D generative models. Additionally, we show the use of LLM-based agents in automating the integration of data sources with a DT and connecting physical devices with their virtual counterparts. Challenges related to computational scalability, data privacy, and model hallucinations are highlighted, which need to be addressed for the widespread adoption of GenAI in DT development.","2169-3536","","10.1109/ACCESS.2024.3514175","Khalifa University of Science and Technology(grant numbers:RIG-2023-049); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10786996","Generative AI;large language models;3D generative models;diffusion models;digital twin","Three-dimensional displays;Solid modeling;Data models;Rendering (computer graphics);Monitoring;Computational modeling;Adaptation models;Accuracy;Surface treatment;Soft sensors","","","","55","CCBY","9 Dec 2024","","","IEEE","IEEE Journals"
"5G Networks Security Mitigation Model: An ANN-ISM Hybrid Approach","R. A. Khan; H. U. Khan; H. S. Alwageed; H. A. Hashimi; I. Keshta","Department of Computer Science and IT, Software Engineering Research Group, University of Malakand, Malakand, Paksitan; Department of Accounting and Information Systems, College of Business and Economics, Qatar University, Doha, Qatar; College of Computer and Information Sciences, Jouf University, Sakaka, Saudi Arabia; Software Engineering Department, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Computer Science and Information Systems Department, College of Applied Sciences, Almaarefa University, Riyadh, Saudi Arabia",IEEE Open Journal of the Communications Society,"31 Jan 2025","2025","6","","881","925","The advent of Fifth-Generation (5G) networks has introduced significant security challenges due to increased complexity and diverse use cases. Conventional threat models may fall short of addressing these emerging threats effectively. This paper presents a new security mitigation model using artificial neural network (ANN) with interpretive structure modeling (ISM) to improve the 5G network security system. The main goal of this study is to develop a 5G network security mitigation model (5GN-SMM) that leverages the predictive capabilities of ANN and the analysis of ISM to identify and mitigate security threats by providing practices in 5G networks. This model aims to improve the accuracy and effectiveness of security measures by integrating advanced computational practices with systematic modeling. Initially, a systematic evaluation of existing 5G network security threats was conducted to identify gaps and incorporate best practices into the proposed model. In the second phase, an empirical survey was conducted to identify and validate the systematic literature review (SLR) findings. In the third phase, we employed a hybrid approach integrating ANN for real-time threat detection and risk assessment and utilizing ISM to analyze the relationships between security threats and vulnerabilities, creating a structured framework for understanding their interdependencies. A case study was conducted in the last stage to test and evaluate 5GN-SMM. The given article illustrates that the proposed hybrid model of ANN-ISM shows a better understanding and management of the security threats than the conventional techniques. The component of the ANN then comes up with the potential of the security breach with improved accuracy, and the ISM framework helps in understanding the relationship and the priorities of the threats. We identified 15 security threats and 144 practices in 5G networks through SLR and empirical surveys. The identified security threats were then analyzed and categorized into 15 process areas and five levels of 5GN-SMM. The proposed model includes state-of-the-art machine learning with traditional information security paradigms to offer an integrated solution to the emerging complex security issues related to 5G. This approach enhances the capacity to detect threats and contributes to good policy enforcement and other risk-related activities to enhance safer 5G networks.","2644-125X","","10.1109/OJCOMS.2025.3529717","Qatar National Library; Qatar University(grant numbers:IRCC-2021-010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10841375","5G networks;security threats and practices;systematic literature review;survey and case study;artificial neural networks (ANN);interpretive structure modeling (ISM);threat prediction and assessment","5G mobile communication;Security;Network security;Computational modeling;Artificial neural networks;Prevention and mitigation;Accuracy;Telecommunication traffic;Systematic literature review;Privacy","","","","87","CCBY","14 Jan 2025","","","IEEE","IEEE Journals"
"Transparent Computing: Development and Current Status","Y. ZHANG; S. DUAN; D. ZHANG; J. REN","Department of Computer Science and Technology, Tsinghua University, Beijing, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China",Chinese Journal of Electronics,"28 Jan 2025","2020","29","5","793","811","Current terminal devices, such as Personal computers (PCs), diskless workstation, mobile terminals and embedded Internet of things (IoT) devices, are facing several challenges, including limited computing capabilities and storage resources, complex maintenance and management of the software and data, as well as secure service provisioning. Facing these challenges, Cloud computing (CC) and Edge computing (EC) encourage the terminal devices to shift compute-intensive works to the cloud servers or edge nodes. Different from CC and EC, Transparent computing (TC) is a novel user-centric computing paradigm in which the hardware and software are separated in different places. Specificaly, all the software, including Operating systems (OSes), applications programs and management tools, are deposited on the servers and transmitted on demand to terminal devices in a transparent way, while the computation is performed on terminals. In this article, we give a broad survey on transparent computing about the development and current status and compare the differences with CC and EC. To realize the provisioning of the cross-platform service, TC shields hardware differences of heterogeneous devices and builds standardized hardware and software interfaces. From the perspective of security, TC entails high-level security mechanisms and the Meta OS can detect malware and OS-level attacks. Specifically, we first review the development courses of network computing paradigms and the motivation of TC. Then, we present a comparison of several emerging computing paradigms and TC from three perspectives, i.e., virtualization, location for computing, and location for storage. We also show the basic architecture of TC, development process, current status, and key enabling technologies of TC for PCs and lightweight terminals. Lastly, several new challenges and future research directions are indicated to inspire continuous investigations. We believe that this survey can help readers to obtain complete information about TC.","2075-5597","","10.1049/cje.2020.07.001","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10856186","Transparent computing;Network computing;Survey","Surveys;Cloud computing;Reviews;Computer architecture;Hardware;Workstations;Internet of Things;Servers;Security;Virtualization","","","","97","","28 Jan 2025","","","CIE","CIE Journals"
"Advancing Manufacturing Through Artificial Intelligence: Current Landscape, Perspectives, Best Practices, Challenges, and Future Direction","R. Rakholia; A. L. Suárez-Cetrulo; M. Singh; R. Simón Carbajo","Ireland’s Centre for Artificial Intelligence (CeADAR), School of Computer Science, University College Dublin (UCD), Dublin 4, Ireland; Ireland’s Centre for Artificial Intelligence (CeADAR), School of Computer Science, University College Dublin (UCD), Dublin 4, Ireland; Ireland’s Centre for Artificial Intelligence (CeADAR), School of Computer Science, University College Dublin (UCD), Dublin 4, Ireland; Ireland’s Centre for Artificial Intelligence (CeADAR), School of Computer Science, University College Dublin (UCD), Dublin 4, Ireland",IEEE Access,"24 Sep 2024","2024","12","","131621","131637","The industrial sector is currently undergoing a transformative era of intelligent automation driven by Artificial Intelligence (AI) capabilities. This synergy greatly enhances efficiency and seamlessly enables data-driven decision-making processes. These advantages enable more efficient resource allocation and enhance production planning precision. This paper aims to provide state-of-the-art and ongoing developments in the AI landscape within the manufacturing industry. In addition, the review explores the key areas where AI is being applied in manufacturing, such as predictive maintenance, quality control, process optimization, supply chain management, robotics and automation, and intelligent decision support systems. The review also encompasses an exploration of the challenges encountered by the manufacturing sector, alongside an investigation into the potential of AI to mitigate these challenges. Furthermore, this work thoroughly reviews recent AI advancements, including explainable AI, human-robot collaboration, edge computing, and the Internet of Things (IoT) integration. The review concludes by providing recommendations, highlighting best practices, and providing insights into potential collaborative opportunities.","2169-3536","","10.1109/ACCESS.2024.3458830","PeRCEPTION Project, which focuses on the predictive development of functional foods using data analytics and machine learning; Enterprise Ireland under the Disruptive Technologies Innovation Fund (DTIF)(grant numbers:DT2020214); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10677409","Smart manufacturing;Industry 4.0;artificial intelligence;automation;machine learning;quality control;Internet of Things;robotics","Artificial intelligence;Manufacturing;Industries;Machine learning algorithms;Reviews;Automation;Production;Smart manufacturing;Fourth Industrial Revolution;Quality control;Internet of Things;Best practices","","","","110","CCBY","11 Sep 2024","","","IEEE","IEEE Journals"
"Multi-Objectives Firefly Algorithm for Task Offloading in the Edge-Fog-Cloud Computing","F. A. Saif; R. Latip; Z. Mohd Hanapi; S. Kamarudin; A. V. Senthil Kumar; A. Salem Bajaher","Department of Information Technology, Gulf Colleges, Hafar Al Batin, Saudi Arabia; Department of Communication Technology and Network, Faculty of Computer Science and Information Technology, Universiti Putra Malaysia (UPM), Serdang, Malaysia; Department of Communication Technology and Network, Faculty of Computer Science and Information Technology, Universiti Putra Malaysia (UPM), Serdang, Malaysia; Department of Communication Technology and Network, Faculty of Computer Science and Information Technology, Universiti Putra Malaysia (UPM), Serdang, Malaysia; Department of Communication Technology and Network, Faculty of Computer Science and Information Technology, Universiti Putra Malaysia (UPM), Serdang, Malaysia; Department of MCA, Hindusthan College of Arts & Science, Coimbatore, India",IEEE Access,"6 Nov 2024","2024","12","","159561","159578","This study proposes a two-stage strategy to ensure the successful execution of critical tasks. In the first stage, an Enhanced Task Offloading (ETO) algorithm is introduced to determine the appropriate computing layer—edge, fog, or cloud—for offloading incomplete tasks. The algorithm makes this decision by assessing the availability of idle computing resources relative to the task’s computational requirements. Additionally, it verifies the status of the server (on/off) before offloading; if the server is unavailable, the algorithm proceeds to check the next layer. In the second stage, the strategy employs a Multi-objective Firefly (MFA) algorithm to assign the optimal computational device within the selected layer. Experimental simulations compare the proposed strategy with a benchmark task offloading algorithm. The results demonstrate the superiority of the proposed strategy, achieving reductions in energy consumption and delay and maximizing resource utilization compared to the baseline algorithms.","2169-3536","","10.1109/ACCESS.2024.3488032","Universiti Putra Malaysia; Ministry of Education Malaysia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10738798","Task offloading;optimization;firefly algorithm;metaheuristic;edge-fog-cloud computing;Internet of Things;energy consumption;transmission delay;Pareto","Internet of Things;Delays;Cloud computing;Energy consumption;Scheduling;Edge computing;Optimization;Heuristic algorithms;Servers;Quality of service","","","","47","CCBYNCND","30 Oct 2024","","","IEEE","IEEE Journals"
"A Lean Simulation Framework for Stress Testing IoT Cloud Systems","J. Li; B. Moeini; S. Nejati; M. Sabetzadeh; M. McCallen","University of Ottawa, Ottawa, Canada; University of Ottawa, Ottawa, Canada; University of Ottawa, Ottawa, Canada; University of Ottawa, Ottawa, Canada; Cheetah Networks, Ottawa, Canada",IEEE Transactions on Software Engineering,"17 Jul 2024","2024","50","7","1827","1851","The Internet of Things (IoT) connects a plethora of smart devices globally across various applications like smart cities, autonomous vehicles, and health monitoring. Simulation plays a key role in the testing of IoT systems, noting that field testing of a complete IoT product may be infeasible or prohibitively expensive. This paper addresses a specific yet important need in simulation-based testing for IoT: Stress testing of cloud systems that are increasingly employed in IoT applications. Existing stress testing solutions for IoT demand significant computational resources, making them ill-suited and costly. We propose a lean simulation framework designed for IoT cloud stress testing. The framework enables efficient simulation of a large array of IoT and edge devices that communicate with the cloud. To facilitate simulation construction for practitioners, we develop a domain-specific language (DSL), named IoTECS, for generating simulators from model-based specifications. We provide the syntax and semantics of IoTECS and implement IoTECS using Xtext and Xtend. We assess simulators generated from IoTECS specifications for stress testing two real-world systems: a cloud-based IoT monitoring system developed by our industry partner and an IoT-connected vehicle system. Our empirical results indicate that simulators created using IoTECS: (1) achieve best performance when configured with Docker containerization; (2) effectively assess the service capacity of our case-study systems, and (3) outperform industrial stress-testing baseline tools, JMeter and Locust, by a factor of 3.5 in terms of the number of IoT and edge devices they can simulate using identical hardware resources. To gain initial insights about the usefulness of IoTECS in practice, we interviewed two engineers from our industry partner who have firsthand experience with IoTECS. Feedback from these interviews suggests that IoTECS is effective in stress testing IoT cloud systems, saving significant time and effort.","1939-3520","","10.1109/TSE.2024.3402157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10535730","Simulation-based testing;stress testing;IoT cloud;model-driven engineering;Xtext","Internet of Things;Testing;Cloud computing;Stress;Containers;Industries;Performance evaluation","","","","76","IEEE","21 May 2024","","","IEEE","IEEE Journals"
"Security, Privacy, and Trust for Open Radio Access Networks in 6G","P. Porambage; M. Christopoulou; B. Han; M. Asif Habibi; H. Bogucka; P. Kryszkiewicz","VTT Technical Research Centre of Finland, Espoo, Finland; National Centre of Scientific Research “Demokritos,”, Athens, Greece; University of Kaiserslautern (RPTU), Kaiserslautern, Germany; University of Kaiserslautern (RPTU), Kaiserslautern, Germany; Poznan University of Technology, Poznań, Poland; Poznan University of Technology, Poznań, Poland",IEEE Open Journal of the Communications Society,"8 Jan 2025","2025","6","","332","361","The programmability of radio access networks was already introduced in the fifth generation of mobile networks by leading the pathway towards the Open RAN. To cater the stringent requirements emerging with the novel services and use cases, Open RAN is becoming integral to 6G as well. The most highlighted characteristics of Open RAN such as disaggregated architecture, cloudification of network functions, open interfaces, and intelligent network management are also associated with many challenges related to the security of the end-to-end telco networks. Most existing research works and specifications focus on security in Open RAN in 5G. In this paper, we discuss the role of Open RAN in 6G focusing on three aspects such as security, privacy, and trust, while highlighting their emerging trends and relevance to 6G. Our contributions include presenting a holistic view of Open RAN security within the 6G ecosystem, with insights into use case and stakeholder perspectives and standardization efforts. This includes a concise update on Open RAN security in the 5G era and their applicability to 6G exploration. Moreover, we identify the key research endeavors focused on privacy and trust in Open RAN, with an emphasis on their development toward 6G. Finally, we discuss the possibilities of incorporating energy awareness, quantum-safe solutions, and other emerging technological enablers with Open RAN security.","2644-125X","","10.1109/OJCOMS.2024.3519725","XcARet Project funded by the Research Council of Finland(grant numbers:355114); German Federal Ministry for Education and Research through the Open6GHub Project(grant numbers:16KISK004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10807044","Open radio access network;security;privacy;trust;6G","Open RAN;6G mobile communication;Security;5G mobile communication;Privacy;Computer architecture;Surveys;Ecosystems;Reviews;3GPP","","","","172","CCBY","18 Dec 2024","","","IEEE","IEEE Journals"
"An efficient tasks offloading procedure for an integrated edge-computing architecture","B. Picano; R. Fantacci","University of Florence, 50139 Firenze, Italy; University of Florence, 50139 Firenze, Italy",Journal of Communications and Networks,"7 May 2024","2024","26","2","215","224","The advent of sixth-generation networks has given rise to numerous challenges, requiring the synergistic exploitation of both ground and air edge computing facilities. This paper considers an integrated ground-air edge computing scenario where the computation offloading of a set of delay sensitive tasks has to be performed in a context where ground and air computational facilities are already involved in monitoring and control proceduers in a remote area under an unpredictable overload of computation requests, e.g., related to the management of an emergency situation. In this reference, a matching game is proposed to assign tasks to the most suitable computation nodes, in order to minimize the outage probability of the newly arrived tasks, i.e., the probability with which tasks experience a completion time greater than the corresponding deadline. To this regard, we have considered that new allocated task suffer for a waiting time due to the time needed to complete the service of all the tasks already in the ground or air computation node. As a consequence, to statistically characterize such waiting time, under proper assumptions, we have resorted to the G/G/1 queuing system model and the Lindley's integral equation approach to define a suitable metric to formulate a tasks allocation procedure based on the matching theory. Furthermore, matching stability has been theoretically proved for the proposed approach. Finally, numerical results have been provided in order to highlight the better behavior of the proposed task allocation scheme in comparison with different state-of-the-art alternatives.","1976-5541","","10.23919/JCN.2024.000004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10522525","Queueing system;task offloading;unmanned aerial vehicle","Task analysis;Delays;Resource management;Edge computing;Drones;Computational modeling;Queueing analysis","","","","","","7 May 2024","","","KICS","KICS Journals"
"Guest Editorial Introduction to the Special Section on Vehicular Networks in the Era of 6G: End-Edge-Cloud Orchestrated Intelligence","Y. Zhang; Y. Zhang; J. Ren; J. Misic; A. M. Tulino","Chinese Academy of Engineering, Tsinghua University, Beijing, China; Central South University, Changsha, China; Central South University, Changsha, China; Ryerson University, Toronto, ON, Canada; New York University, New York, NY, USA",IEEE Transactions on Vehicular Technology,"8 Jul 2021","2021","70","6","5192","5196","The articles in this special section focus on vehicular networks in the era of 6G mobile communication. With the growth of the vehicle population, vehicular networks play a key role in building safe, efficient, and intelligent transport systems and has been attracting a lot of attention from both academic and industrial communities around the world. The rise of autonomous driving technology and the prosperity of mobile applications, e.g., real-time video analytic, image-aided navigation, natural language processing, and etc, have brought tremendous pressure on current vehicular networks, e.g., high bandwidth, ultra-low latency, high reliability, high security, powerful computation capability, and massive connections. It is necessary to continue to develop vehicular networks by combining the latest research intends in other fields to meet quickly rising communication and computation demands. The upcoming 6G technology, which provides Holographic and Artificial Intelligence (AI) enabled communications, together with the increasing implementation of artificial intelligence in mobile devices, will lead to a new research trend to end-edge-cloud orchestrated computing with intelligence. It means that, not only the intelligent communication protocols, but also the intelligent computing resource management and machine learning algorithms among the mobile vehicles, the edge and the cloud, should be redesigned to support the development of vehicular networks.","1939-9359","","10.1109/TVT.2021.3084829","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9477551","","Special issues and sections;6G mobile communication;Security;Processor scheduling;Resource management;Reinforcement learning;Mobile ad hoc networks;Data privacy","","4","","0","IEEE","8 Jul 2021","","","IEEE","IEEE Journals"
"Guest Editorial Special Issue on Empowering the Future Generation Systems: Opportunities by the Convergence of Cloud, Edge, AI, and IoT","F. Firouzi; M. Daneshmand; J. Song; K. Mankodiya","Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA; Department of Business Intelligence and Analytics, Stevens Institute of Technology, Hoboken, NJ, USA; Department of Computer and Information Security, Sejong University, Seoul, South Korea; Department of Electrical, Computer, and Biomedical Engineering, University of Rhode Island, Kingston, RI, USA",IEEE Internet of Things Journal,"20 Feb 2023","2023","10","5","3681","3685","The future generation of the Internet of Things (IoT) systems is characterized by the fusion of technologies—from edge–fog–cloud computing to artificial intelligence (AI) and blockchain—closing the gap between the physical and digital worlds [A1]. Although these technologies have been developed separately over time, the synergy among them has taken a giant leap. We are witnessing a fast-paced convergence of these technologies resulting in a fundamental paradigm shift unlocking vast benefits and opportunities across vertical markets. However, there are still several barriers, such as a lack of consensus toward any reference models or best practices, hindering the full fusion of these technologies [A1]. To tackle these challenges and facilitate this promising transformation, this special issue was organized to provide a holistic multidisciplinary reference for solutions, architectures, protocols, services, and applications addressing all aspects of the future generation of IoT systems via the fusion of edge, cloud, AI, and blockchain, while considering the corresponding challenges. Thanks to the enormous support from the Editor-in-Chief, Prof. Honggang Wang, and the dedicated work of many reviewers, after a rigorous review process, 27 excellent articles out of 125 submissions were accepted for inclusion in this special issue of the IEEE Internet of Things Journal. We introduce these papers and highlight their key contributions below.","2327-4662","","10.1109/JIOT.2022.3232084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049217","","Special issues and sections;Internet of Things;Artificial intelligence;Computer architecture;Cloud computing;Blockchains;Best practices","","3","","0","IEEE","20 Feb 2023","","","IEEE","IEEE Journals"
"IEEE Access Special Section Editorial: Cloud-Fog-Edge Computing in Cyber–Physical–Social Systems (CPSS)","M. A. Rahman; Z. Tari; D. Zhu; F. Piccialli; X. Wang","Faculty of Computing, University Malaysia Pahang, Gambang, Malaysia; Centre for Cyber Security Research and Innovation, Royal Melbourne Institute of Technology University, Melbourne, VIC, Australia; Department of Computer Science, The University of Texas at San Antonio, San Antonio, TX, USA; Department of Computer Science, University of Naples Federico II, Napoli, Italy; Department of Computer Science, St. Francis Xavier University, Antigonish, NS, Canada",IEEE Access,"22 Dec 2020","2020","8","","222859","222864","Cyber–Physical–Social Systems (CPSS) integrate the cyber, physical, and social spaces together. One of the ultimate goals of cyber–physical–social systems is to make our lives more convenient and intelligent by providing prospective and personalized services for users. To achieve this goal, a wide range of data in CPSS are employed as the starting point for research, since the data contain the users’ historical behavior trajectory and the users’ demand preference. Generated and collected from social and physical spaces and integrated into the cyberspace, CPSS data are complex and heterogeneous, recording all aspects of users’ lives in the forms of image, audio, video, and text. Generally, the collected or generated data in CPSS satisfy the 4Vs (volume, variety, velocity, and veracity) of big data. Thus, knowing how to deal with CPSS big data efficiently is the key to providing services for users. From another perspective, CPSS big data are specified as the global historical data and the local real-time data. Cloud computing, as a powerful paradigm for implementing the data-intensive applications, has an irreplaceable role in processing global historical data. On the other hand, with the increasing computing capacity and communication capabilities of mobile terminal devices, fog-edge computing, as an important and effective supplement of cloud computing, has been widely used to process local real-time data. Therefore, the question of how to systematically and efficiently process CPSS big data (including both the global historical data and the local real-time data) in CPSS has become the key for providing services. The goal of this Special Section is therefore to provide insights and views into the area of Cloud-Fog-Edge Computing in CPSS, as well as to provide directions for research in the field.","2169-3536","","10.1109/ACCESS.2020.3042584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9302565","","","","3","","0","CCBY","22 Dec 2020","","","IEEE","IEEE Journals"
"IEEE Access Special Section Editorial: Edge Computing and Networking for Ubiquitous AI","V. C. M. Leung; X. Wang; A. Jamalipour; X. Chen; S. Bouzefrane","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Intelligent Computing, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; CEDRIC Lab, Conservatoire National des Arts et Métiers, Paris, France",IEEE Access,"29 Jun 2021","2021","9","","90933","90936","With its rapid development recently, edge computing with processing, storage, and networking capabilities has become an important solution to break through the bottleneck of emerging technology development by virtue of its advantages in reducing data transmission, decreasing service latency, and easing cloud computing pressure. Among several application scenarios such as network optimization, intelligent manufacturing, and real-time video analytics, edge computing can work with artificial intelligence (AI) synergistically. Therefore, many researchers are investigating edge computing with AI from two perspectives. One is that the emergence of AI solves the optimization problem of edge computing. For example, when network devices need to process some complex and fuzzy information, the powerful learning and reasoning ability of AI can help to extract valuable information from the massive data and realize intelligent management. Another is how edge computing supports AI in a networking environment. For example, AI training and inference can be efficiently enabled by a multitude of computing resources from edge computing. Therefore, edge computing and AI are mutually beneficial in networking.","2169-3536","","10.1109/ACCESS.2021.3090143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9467255","","","","3","","0","CCBY","29 Jun 2021","","","IEEE","IEEE Journals"
"Physiological Electrosignal Asynchronous Acquisition Technology: Insight and Perspectives","B. Hu; L. Zhu; Q. Dong; K. Qian; H. Cai; F. Tian","Ministry of Education Key Laboratory of Brain Health Intelligent Evaluation and Intervention, School of Medical Technology, Beijing Institute of Technology, Beijing, China; School of Medical Technology, Beijing Institute of Technology, Beijing, China; School of Medical Technology, Beijing Institute of Technology, Beijing, China; School of Medical Technology, Beijing Institute of Technology, Beijing, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China; School of Medical Technology, Beijing Institute of Technology, Beijing, China",IEEE Transactions on Computational Social Systems,"25 Jan 2024","2024","11","1","5","24","With great pride and enthusiasm, we present the inaugural edition of IEEE Transactions on Computational Social Systems (TCSS) for 2024. Reflecting on the year gone by, 2023 stands as a hallmark of academic excellence and prolific output, wherein our journal has successfully disseminated a substantial volume of scholarly work—301 articles encompassing approximately 3600 pages, distributed across six distinct issues.","2329-924X","","10.1109/TCSS.2024.3350958","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10414277","","","","2","","31","IEEE","25 Jan 2024","","","IEEE","IEEE Journals"
"IEEE Access Special Section Editorial: Data Mining for Internet of Things","C. -W. Tsai; M. -Y. Chen; F. Piccialli; T. Qiu; J. J. Jung; P. C. K. Hung; S. Zeadally","Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan; Department of Engineering Science, National Cheng Kung University, Tainan, Taiwan; Department of Mathematics and Applications “R. Caccioppoli”, University of Naples Federico II (UNINA), Naples, Italy; School of Computer Science and Technology, Tianjin University, Tianjin, China; Department of Computer Engineering, Chung-Ang University, Seoul, South Korea; Faculty of Business and Information Technology, University of Ontario Institute of Technology, Oshawa, ON, Canada; College of Communication and Information, University of Kentucky, Lexington, KY, USA",IEEE Access,"29 Jun 2021","2021","9","","90418","90427","It is an irrefutable fact that the Internet of Things (IoT) will eventually change our daily lives because its applications and relevant technologies have been or will be penetrating our daily lives. Also, the IoT is aimed to connect all the things (e.g., devices and systems) together via the Internet, thus making it easy to collect the data of users or environments and to find out useful information from the gathered data by using data mining technologies. As a consequence, how intelligent systems are developed for the IoT has become a critical research topic today. This means that artificial intelligence (AI) technologies (e.g., supervised learning, unsupervised learning, and semi-supervised learning) were used in the development of intelligent systems for analyzing the data captured from IoT devices or making decisions for IoT systems. It can be easily seen that AI can make an IoT system more intelligent and thus more accurate. For example, various sensors can be used for a smart home system to pinpoint the location and analyze the behavior of a human; however, with AI technologies, a more accurate prediction can be provided on the two pieces of information of a human. One of the most important uses for AI technologies is to make IoT systems more intelligent in order to provide a more convenient environment for users; thus, how to use existing AI technologies or develop new AI technologies to construct a better IoT system has attracted the attention of researchers from different disciplines in recent years. That is why, besides using existing supervised, unsupervised, semi-supervised learning algorithms, data mining algorithms, and machine learning algorithms, several recent studies have also attempted to develop new intelligent methods for the devices or systems for the IoT. All these approaches for making an IoT system more intelligent can also be found in the articles of this Special Section.","2169-3536","","10.1109/ACCESS.2021.3090137","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9467257","","","","2","","0","CCBY","29 Jun 2021","","","IEEE","IEEE Journals"
"Guest Editorial Special Issue on Emerging Trends and Challenges in Fog Computing for IoT","C. X. Mavromoustakis; M. Mukherjee; G. Mastorakis; H. Song; M. Gorlatova; M. Aazam","University of Nicosia, Nicosia, Cyprus; College of Artificial Intelligence, Nanjing University of Information Science and Technology, Nanjing, China; Department of Management Science and Technology, Hellenic Mediterranean University, Agios Nikolaos, Greece; Department of Electrical Engineering and Computer Science, Security and Optimization for Networked Globe Laboratory, Embry–Riddle Aeronautical University, Daytona Beach, USA; Department of Electrical and Computer Engineering, Duke University, Durham, USA; Department of Computer Science, Carnegie Mellon University, Doha, Qatar",IEEE Internet of Things Journal,"13 May 2020","2020","7","5","4155","4159","With the emergence of the Internet of Things (IoT), billions of heterogeneous physical objects are connected through a network for collecting and sharing information, which can improve various aspects of daily lives, including smart living and transportation, and smart ambient environment, including smart city, smart home, smart agriculture, smart water, waste management, etc. The main objective of the IoT devices is to provide seamless services to the users without their intervention. The all-connected paradigm (i.e., connecting people, things, processes, and data in the network) is based on near Internet ubiquity and includes three types of communication: 1) machine-to-machine; 2) person-to-machine; and 3) person-to-person, and consists as the base for reliable services provision to the end devices at the edge. Fog paradigm implements effectively and efficiently the all-data requests to be shared in a reliable manner as fog implementations complement the cloud computing paradigm by extending computing and caching capabilities to the edges of the network, and it facilitates smart localization decisions and rapid responses. The wide range of IoT services calls for a disruptive, highly efficient, scalable, and flexible communication network able to cope with the increasing demands and the number of connected devices, as well as the diverse and stringent application requirements.","2327-4662","","10.1109/JIOT.2020.2982520","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9091965","","Special issues and sections;Edge computing;Smart devices;Smart homes;Smart cities;Machine-to-machine communication","","2","","0","IEEE","13 May 2020","","","IEEE","IEEE Journals"
"Sixth Edition of the IEEE JSAC Series on Network Softwarization and Enablers","T. Taleb Aalto","Department of Communications and Networking, School of Electrical Engineering, Aalto University, Espoo, Finland",IEEE Journal on Selected Areas in Communications,"21 May 2020","2020","38","6","985","987","The emergence of 5G, the fifth generation of cellular technology, will have a huge impact on what seemed yesterday a figment of our imagination. Outperforming 4G download and upload by up to 100 times, and offering a more reliable connectivity will pave the way to make use-cases such as fully autonomous vehicles, safer and more reliable, all the while reducing the Capital Expenditure (CAPEX) and Operational Expenditure (OPEX).","1558-0008","","10.1109/JSAC.2020.2986666","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9098105","","Special issues and sections;5G mobile communication;Cellular networks;Heuristic algotithms;Routing;Privacy;Network function virtualization;Cloud computing;Software defined networking","","","","0","IEEE","21 May 2020","","","IEEE","IEEE Journals"
"Guest Editorial: Thematic Section on Applications of Emerging Computing Technologies in Smart Manufacturing and Industry 4.0","D. J. Pagliari; F. Schirrmeister; N. Bagherzadeh; E. Macii","Politecnico di Torino, Torino, Italy; Cadence Design Systems Inc., San Jose, CA, USA; University of California, Irvine, Irvine, CA, USA; Politecnico di Torino, Torino, Italy",IEEE Transactions on Emerging Topics in Computing,"3 Mar 2022","2022","10","1","6","8","The papers in this special section focus on applications of emerging computing technologies in smart manufacturing and Industry 4.0. Both of these paradigms are transforming factories into highly complex IT systems, generating massive amounts of data. The modeling and optimization of smart industrial processes, to support decision making, has consequently become a new application domain, with its own unique and peculiar issues, for some of the most prominent emerging technologies across the entire computing stack, from hardware and systems design to applicationlevel software. The massive data collection which forms the foundation of digital industries is enabled by increasingly complex and heterogeneous cyber physical systems (CPSs). Owing to that, the design of software for CPSs is receiving renewed interest from research and industry in relation to challenges such as managing variability and dependencies with an unprecedented degree of customization.","2168-6750","","10.1109/TETC.2022.3146784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9726694","","Special issues and sections;Smart manufacturing;Fourth Industrial Revolution;Artificial intelligence;Digital twin;Computational modeling;Production facilities;Manufacturing automation","","","","0","IEEE","3 Mar 2022","","","IEEE","IEEE Journals"
"Guest Editorial Special Issue on Smart IoT System: Opportunities by Linking Cloud, Edge, and AI","W. Yang; L. T. Yang; A. T. Chronopoulos","Department of Computer Science and Technology, Hunan University, Changsha, China; Department of Computer Science, St. Francis Xavier University, Antigonish, Canada; Department of Computer Science, University of Texas at San Antonio, San Antonio, TX, USA",IEEE Internet of Things Journal,"5 Aug 2021","2021","8","16","12478","12480","Recently, the Internet of Things (IoT) technologies have made their entrances into many fields, such as smart city, healthcare, intelligent transportation, forest protection, and environmental monitoring.","2327-4662","","10.1109/JIOT.2021.3092440","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9507561","","Special issues and sections;Artificial intelligence;Internet of Things;Smart devices","","","","0","IEEE","5 Aug 2021","","","IEEE","IEEE Journals"
"2019 Index IEEE Transactions on Parallel and Distributed Systems Vol. 30","",,IEEE Transactions on Parallel and Distributed Systems,"20 Dec 2019","2020","31","2","1","33","Presents the 2019 subject/author index for this publication.","1558-2183","","10.1109/TPDS.2019.2954077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8938150","","","","","","","IEEE","20 Dec 2019","","","IEEE","IEEE Journals"
"Table of contents","",,IEEE Internet of Things Journal,"9 Aug 2018","2018","5","4","i","iv","Presents the table of contents for this issue of the publication.","2327-4662","","10.1109/JIOT.2018.2858958","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8430662","","","","","","","IEEE","9 Aug 2018","","","IEEE","IEEE Journals"
"Scanning the Issue","A. Eskandarian","Dean, College of Engineering and William H. Goodwin Jr. Endowed Chair, Virginia Commonwealth University, Richmond, VA, USA",IEEE Transactions on Intelligent Transportation Systems,"30 Aug 2023","2023","24","9","8899","8918","Edge Intelligence in Intelligent Transportation Systems: A Survey T. Gong, L. Zhu, F. R. Yu, and T. Tang Edge intelligence (EI) is becoming one of the research hotspots among researchers, which is believed to help empower intelligent transportation systems (ITS). ITS generates a large amount of data at the network edge by millions of devices and sensors. Data-driven artificial intelligence (AI) is at the core of ITS development. By pushing the AI frontier to the network edge, EI enables ITS AI applications to have lower latency, higher security, less pressure on the backbone network, and better use of edge big data. This article surveys edge intelligence in intelligent transportation systems. The authors first introduce the challenges ITS faces and explain the motivation for using EI in ITS. They then explore the framework of using EI in ITS, including the EI-based ITS architecture, the data gathering and communication methods, the data processing and service delivery, and the performance indexes. The enabling technologies, such as AI models, the Internet of Things, and edge computing technologies used in EI-based ITS, are reviewed intensively. They discuss edge intelligence applications and research fields in ITS in depth. Typical application scenarios, such as autonomous driving, vehicular edge computing, intelligent vehicular transportation system, unmanned aerial vehicle (UAV) in ITS environment, and rail transportation control and management, are explored. The general platforms of EI, the EI training and inference in ITS, as well as the benchmark datasets, are introduced. Finally, they discuss some of the challenges and future directions of using EI in ITS.","1558-0016","","10.1109/TITS.2023.3299370","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10235285","","","","","","0","IEEE","30 Aug 2023","","","IEEE","IEEE Journals"
"2018 IndexIEEE Internet of Things JournalVol. 5","",,IEEE Internet of Things Journal,"21 Jan 2019","2018","5","6","5348","5413","Presents the 2018 subject/author index for this publication.","2327-4662","","10.1109/JIOT.2019.2894196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8620612","","","","","","","IEEE","21 Jan 2019","","","IEEE","IEEE Journals"
