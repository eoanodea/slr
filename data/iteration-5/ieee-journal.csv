"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"On Multi-Access Edge Computing: A Survey of the Emerging 5G Network Edge Cloud Architecture and Orchestration","T. Taleb; K. Samdanis; B. Mada; H. Flinck; S. Dutta; D. Sabella","Aalto University, Espoo, Finland; Huawei European Research Center, Munich, Germany; Aalto University, Espoo, Finland; Nokia Bell Labs, Espoo, Finland; Aalto University, Espoo, Finland; Intel, Munich, Germany",IEEE Communications Surveys & Tutorials,"21 Aug 2017","2017","19","3","1657","1681","Multi-access edge computing (MEC) is an emerging ecosystem, which aims at converging telecommunication and IT services, providing a cloud computing platform at the edge of the radio access network. MEC offers storage and computational resources at the edge, reducing latency for mobile end users and utilizing more efficiently the mobile backhaul and core networks. This paper introduces a survey on MEC and focuses on the fundamental key enabling technologies. It elaborates MEC orchestration considering both individual services and a network of MEC platforms supporting mobility, bringing light into the different orchestration deployment options. In addition, this paper analyzes the MEC reference architecture and main deployment scenarios, which offer multitenancy support for application developers, content providers, and third parties. Finally, this paper overviews the current standardization activities and elaborates further on open research challenges.","1553-877X","","10.1109/COMST.2017.2705720","TAKE 5 Project; Finnish Funding Agency for Technology and Innovation; Finnish Ministry of Employment and the Economy; European Union’s Horizon 2020 Research and Innovation Programme through the EU/JP 5G!Pagoda Project(grant numbers:723172); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7931566","MEC;Multi-Access Edge Computing;Mobile Edge Computing;5G Mobile Communications;Edge Computing;Application Virtualization;Network Function Virtualization;Software Defined Networking;Network Softwarization","Cloud computing;Edge computing;Mobile computing;Computer architecture;5G mobile communication","","1360","","209","IEEE","18 May 2017","","","IEEE","IEEE Journals"
"Convergence of Edge Computing and Deep Learning: A Comprehensive Survey","X. Wang; Y. Han; V. C. M. Leung; D. Niyato; X. Yan; X. Chen","Tianjin Key Laboratory of Advanced Networking, College of Intelligence and Computing, Tianjin University, Tianjin, China; Tianjin Key Laboratory of Advanced Networking, College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; 2012 Lab, Huawei Technologies, Shenzhen, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China",IEEE Communications Surveys & Tutorials,"27 May 2020","2020","22","2","869","904","Ubiquitous sensors and smart devices from factories and communities are generating massive amounts of data, and ever-increasing computing power is driving the core of computation and services from the cloud to the edge of the network. As an important enabler broadly changing people's lives, from face recognition to ambitious smart factories and cities, developments of artificial intelligence (especially deep learning, DL) based applications and services are thriving. However, due to efficiency and latency issues, the current cloud computing service architecture hinders the vision of “providing artificial intelligence for every person and every organization at everywhere”. Thus, unleashing DL services using resources at the network edge near the data sources has emerged as a desirable solution. Therefore, edge intelligence, aiming to facilitate the deployment of DL services by edge computing, has received significant attention. In addition, DL, as the representative technique of artificial intelligence, can be integrated into edge computing frameworks to build intelligent edge for dynamic, adaptive edge maintenance and management. With regard to mutually beneficial edge intelligence and intelligent edge, this paper introduces and discusses: 1) the application scenarios of both; 2) the practical implementation methods and enabling technologies, namely DL training and inference in the customized edge computing framework; 3) challenges and future trends of more pervasive and fine-grained intelligence. We believe that by consolidating information scattered across the communication, networking, and DL areas, this survey can help readers to understand the connections between enabling technologies while promoting further discussions on the fusion of edge intelligence and intelligent edge, i.e., Edge DL.","1553-877X","","10.1109/COMST.2020.2970550","National Basic Research Program of China (973 Program)(grant numbers:2019YFB2101901,2018YFC0809803); National Science Foundation of China(grant numbers:61702364,61972432,U1711265); Program for Guangdong Introducing Innovative and Enterpreneurial Teams(grant numbers:2017ZT07X355); Chinese National Engineering Laboratory for Big Data System Computing Technology; Canadian Natural Sciences and Engineering Research Council; Singapore NRF National Satellite of Excellence, Design Science and Technology for Secure Critical Infrastructure National Satellite of Excellence(grant numbers:DeST-SCI2019-0007); A*STAR-NTU-SUTD Joint Research Grant Call on Artificial Intelligence for the Future of Manufacturing(grant numbers:RGANS1906); WASP/NTU(grant numbers:M4082187 (4080)); Singapore MOE Tier 1(grant numbers:2017-T1-002-007 RG122/17); MOE Tier 2(grant numbers:MOE2014-T2-2-015 ARC4/15); Singapore(grant numbers:NRF2015-NRF-ISF001-2277); Singapore Energy Market Authority Energy Resilience(grant numbers:NRF2017EWT-EP003-041); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8976180","Edge computing;deep learning;wireless communication;computation offloading;artificial intelligence","Edge computing;Cloud computing;Training;Computational modeling;Reliability;Wireless communication;Internet of Things","","909","","134","IEEE","30 Jan 2020","","","IEEE","IEEE Journals"
"6G Wireless Systems: Vision, Requirements, Challenges, Insights, and Opportunities","H. Tataria; M. Shafi; A. F. Molisch; M. Dohler; H. Sjöland; F. Tufvesson","Department of Electrical and Information Technology, Lund University, Lund, Sweden; Spark New Zealand, Wellington, New Zealand; Ming Hsieh Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA, USA; Center for Telecoms Research, King’s College London, London, U.K.; Department of Electrical and Information Technology, Lund University, Lund, Sweden; Department of Electrical and Information Technology, Lund University, Lund, Sweden",Proceedings of the IEEE,"18 Jun 2021","2021","109","7","1166","1199","Mobile communications have been undergoing a generational change every ten years or so. However, the time difference between the so-called “G’s” is also decreasing. While fifth-generation (5G) systems are becoming a commercial reality, there is already significant interest in systems beyond 5G, which we refer to as the sixth generation (6G) of wireless systems. In contrast to the already published papers on the topic, we take a top-down approach to 6G. More precisely, we present a holistic discussion of 6G systems beginning with lifestyle and societal changes driving the need for next-generation networks. This is followed by a discussion into the technical requirements needed to enable 6G applications, based on which we dissect key challenges and possibilities for practically realizable system solutions across all layers of the Open Systems Interconnection stack (i.e., from applications to the physical layer). Since many of the 6G applications will need access to an order-of-magnitude more spectrum, utilization of frequencies between 100 GHz and 1 THz becomes of paramount importance. As such, the 6G ecosystem will feature a diverse range of frequency bands, ranging from below 6 GHz up to 1 THz. We comprehensively characterize the limitations that must be overcome to realize working systems in these bands and provide a unique perspective on the physical and higher layer challenges relating to the design of next-generation core networks, new modulation and coding methods, novel multiple-access techniques, antenna arrays, wave propagation, radio frequency transceiver design, and real-time signal processing. We rigorously discuss the fundamental changes required in the core networks of the future, such as the redesign or significant reduction of the transport architecture that serves as a major source of latency for time-sensitive applications. This is in sharp contrast to the present hierarchical network architectures that are not suitable to realize many of the anticipated 6G services. While evaluating the strengths and weaknesses of key candidate 6G technologies, we differentiate what may be practically achievable over the next decade, relative to what is possible in theory. Keeping this in mind, we present concrete research challenges for each of the discussed system aspects, providing inspiration for what follows.","1558-2256","","10.1109/JPROC.2021.3061701","Ericsson AB, Sweden; ELLIIT: The Linköping-Lund Excellence Center on IT and Mobile Communication; National Science Foundation (NSF); National Institute of Standards and Technology (NIST); Samsung Research America; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9390169","Beamforming;next-generation core network;physical layer (PHY);radio frequency (RF) transceivers;signal processing;sixth-generation (6G);terahertz (THz);ultramassive multiple-input multiple-output (MIMO);waveforms","Array signal processing;Transceivers;Signal processing;6G mobile communication;Massive MIMO;Radio frequency;Physical layer;Next generation networking","","840","","194","CCBY","30 Mar 2021","","","IEEE","IEEE Journals"
"Network Slicing and Softwarization: A Survey on Principles, Enabling Technologies, and Solutions","I. Afolabi; T. Taleb; K. Samdanis; A. Ksentini; H. Flinck","Department of Communications and Networking, Aalto University, Espoo, Finland; Department of Computer and Information Security, Sejong University, Seoul, South Korea; Huawei European Research Center, Munich, Germany; Department of Communications System, Eurecom, Nice, France; Nokia Bell Labs, Espoo, Finland",IEEE Communications Surveys & Tutorials,"21 Aug 2018","2018","20","3","2429","2453","Network slicing has been identified as the backbone of the rapidly evolving 5G technology. However, as its consolidation and standardization progress, there are no literatures that comprehensively discuss its key principles, enablers, and research challenges. This paper elaborates network slicing from an end-to-end perspective detailing its historical heritage, principal concepts, enabling technologies and solutions as well as the current standardization efforts. In particular, it overviews the diverse use cases and network requirements of network slicing, the pre-slicing era, considering RAN sharing as well as the end-to-end orchestration and management, encompassing the radio access, transport network and the core network. This paper also provides details of specific slicing solutions for each part of the 5G system. Finally, this paper identifies a number of open research challenges and provides recommendations toward potential solutions.","1553-877X","","10.1109/COMST.2018.2815638","Academy of Finland(grant numbers:311654); European Union’s Horizon 2020 Research and Innovation Program through the 5G!Pagoda Project(grant numbers:723172); Global5G.org Project(grant numbers:761816); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8320765","Network slice;5G;network softwarization;orchestration;network management;NFV;SDN;cloud;mobile network;MANO;open source","5G mobile communication;Network slicing;Business;Cloud computing;Industries","","808","","169","IEEE","21 Mar 2018","","","IEEE","IEEE Journals"
"On the Road to 6G: Visions, Requirements, Key Technologies, and Testbeds","C. -X. Wang; X. You; X. Gao; X. Zhu; Z. Li; C. Zhang; H. Wang; Y. Huang; Y. Chen; H. Haas; J. S. Thompson; E. G. Larsson; M. D. Renzo; W. Tong; P. Zhu; X. Shen; H. V. Poor; L. Hanzo","National Mobile Communications Research Laboratory, School of Information Science and Engineering, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, School of Information Science and Engineering, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, School of Information Science and Engineering, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, School of Information Science and Engineering, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, School of Information Science and Engineering, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, School of Information Science and Engineering, Southeast University, Nanjing, China; Pervasive Communication Research Center, Purple Mountain Laboratories, Nanjing, China; National Mobile Communications Research Laboratory, School of Information Science and Engineering, Southeast University, Nanjing, China; Department of Engineering, University of Durham, Durham, U.K.; Department of Electronic and Electrical Engineering, LiFi Research and Development Center, University of Strathclyde, Glasgow, U.K.; Institute for Digital Communications, School of Engineering, University of Edinburgh, Edinburgh, U.K.; Department of Electrical Engineering (ISY), Linköping University, Linköping, Sweden; Université Paris-Saclay, CNRS, CentraleSupélec, Laboratoire des Signaux et Systèmes, Gif-sur-Yvette, France; Wireless Advanced System and Competency Centre, HUAWEI Technologies Canada Company Ltd., Ottawa, ON, Canada; Wireless Technology Lab, HUAWEI Technologies Canada Company Ltd., Ottawa, Canada; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada; Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ, USA; School of Electronics and Computer Science, University of Southampton, Southampton, U.K.",IEEE Communications Surveys & Tutorials,"22 May 2023","2023","25","2","905","974","Fifth generation (5G) mobile communication systems have entered the stage of commercial deployment, providing users with new services, improved user experiences as well as a host of novel opportunities to various industries. However, 5G still faces many challenges. To address these challenges, international industrial, academic, and standards organizations have commenced research on sixth generation (6G) wireless communication systems. A series of white papers and survey papers have been published, which aim to define 6G in terms of requirements, application scenarios, key technologies, etc. Although ITU-R has been working on the 6G vision and it is expected to reach a consensus on what 6G will be by mid-2023, the related global discussions are still wide open and the existing literature has identified numerous open issues. This paper first provides a comprehensive portrayal of the 6G vision, technical requirements, and application scenarios, covering the current common understanding of 6G. Then, a critical appraisal of the 6G network architecture and key technologies is presented. Furthermore, existing testbeds and advanced 6G verification platforms are detailed for the first time. In addition, future research directions and open challenges are identified to stimulate the on-going global debate. Finally, lessons learned to date concerning 6G networks are discussed.","1553-877X","","10.1109/COMST.2023.3249835","National Key Research and Development Program of China(grant numbers:2018YFB1801101); National Natural Science Foundation of China(grant numbers:61960206006,62122020); Key Technologies Research and Development Program of Jiangsu (Prospective and Key Technologies for Industry)(grant numbers:BE2022067,BE2022067-1,BE2022067-5); EU H2020 RISE TESTBED2 Project(grant numbers:872172); EU H2020 ARIADNE Project(grant numbers:871464); EU H2020 RISE-6G Project(grant numbers:101017011); U.S. National Science Foundation(grant numbers:CCF-1908308,CNS-2128448); Engineering and Physical Sciences Research Council Project(grant numbers:EP/W016605/1,EP/X01228X/1); European Research Council’s Advanced Fellow Grant QuantCom(grant numbers:789028); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10054381","6G vision;6G key performance indicators (KPIs);6G application scenarios;6G network architecture;6G key technologies;6G testbeds;6G challenges","6G mobile communication;5G mobile communication;Wireless communication;Industries;Tutorials;Millimeter wave communication;Research and development","","659","","662","IEEE","27 Feb 2023","","","IEEE","IEEE Journals"
"Communications in the 6G Era","H. Viswanathan; P. E. Mogensen","Nokia Bell Labs, Murray Hill, USA; Nokia Bell Labs, Aalborg, Denmark",IEEE Access,"30 Mar 2020","2020","8","","57063","57074","The focus of wireless research is increasingly shifting toward 6G as 5G deployments get underway. At this juncture, it is essential to establish a vision of future communications to provide guidance for that research. In this paper, we attempt to paint a broad picture of communication needs and technologies in the timeframe of 6G. The future of connectivity is in the creation of digital twin worlds that are a true representation of the physical and biological worlds at every spatial and time instant, unifying our experience across these physical, biological and digital worlds. New themes are likely to emerge that will shape 6G system requirements and technologies, such as: (i) new man-machine interfaces created by a collection of multiple local devices acting in unison; (ii) ubiquitous universal computing distributed among multiple local devices and the cloud; (iii) multi-sensory data fusion to create multi-verse maps and new mixed-reality experiences; and (iv) precision sensing and actuation to control the physical world. With rapid advances in artificial intelligence, it has the potential to become the foundation for the 6G air interface and network, making data, compute and energy the new resources to be exploited for achieving superior performance. In addition, in this paper we discuss the other major technology transformations that are likely to define 6G: (i) cognitive spectrum sharing methods and new spectrum bands; (ii) the integration of localization and sensing capabilities into the system definition, (iii) the achievement of extreme performance requirements on latency and reliability; (iv) new network architecture paradigms involving sub-networks and RAN-Core convergence; and (v) new security and privacy schemes.","2169-3536","","10.1109/ACCESS.2020.2981745","Nokia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040431","6G;AI/ML driven air interface;network localization and sensing;cognitive spectrum sharing;sub-terahertz;RAN-Core convergence;subnetworks;security;privacy;network as a platform","6G mobile communication;5G mobile communication;Robot sensing systems;Biology;Digital twin;User interfaces","","528","","41","CCBY","18 Mar 2020","","","IEEE","IEEE Journals"
"Integrated Blockchain and Edge Computing Systems: A Survey, Some Research Issues and Challenges","R. Yang; F. R. Yu; P. Si; Z. Yang; Y. Zhang","Faculty of Information Technology, Beijing University of Technology, Beijing, China; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada; Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China",IEEE Communications Surveys & Tutorials,"31 May 2019","2019","21","2","1508","1532","Blockchain, as the underlying technology of crypto-currencies, has attracted significant attention. It has been adopted in numerous applications, such as smart grid and Internet-of-Things. However, there is a significant scalability barrier for blockchain, which limits its ability to support services with frequent transactions. On the other side, edge computing is introduced to extend the cloud resources and services to be distributed at the edge of the network, but currently faces challenges in its decentralized management and security. The integration of blockchain and edge computing into one system can enable reliable access and control of the network, storage, and computation distributed at the edges, hence providing a large scale of network servers, data storage, and validity computation near the end in a secure manner. Despite the prospect of integrated blockchain and edge computing systems, its scalability enhancement, self organization, functions integration, resource management, and new security issues remain to be addressed before widespread deployment. In this survey, we investigate some of the work that has been done to enable the integrated blockchain and edge computing system and discuss the research challenges. We identify several vital aspects of the integration of blockchain and edge computing: motivations, frameworks, enabling functionalities, and challenges. Finally, some broader perspectives are explored.","1553-877X","","10.1109/COMST.2019.2894727","Beijing University of Technology; Beijing Post-Doctoral Fund; Support Project of High-Level Teachers in Beijing Municipal Universities in the Period of 13th Five-Year Plan(grant numbers:067175315000); National Natural Science Foundation of China(grant numbers:61671029,61571021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8624417","Blockchain;edge computing;network;storage;computation","Blockchain;Edge computing;Bitcoin;Peer-to-peer computing;Scalability;Cloud computing","","513","","132","IEEE","23 Jan 2019","","","IEEE","IEEE Journals"
"Edge Computing in Industrial Internet of Things: Architecture, Advances and Challenges","T. Qiu; J. Chi; X. Zhou; Z. Ning; M. Atiquzzaman; D. O. Wu","School of Computer Science and Technology, College of Intelligence and Computing, Tianjin University, Tianjin, China; School of Computer Science and Technology, College of Intelligence and Computing, Tianjin University, Tianjin, China; School of Computer Science and Technology, College of Intelligence and Computing, Tianjin University, Tianjin, China; School of Software, Dalian University of Technology, Dalian, China; School of Computer Science, University of Oklahoma, Norman, OK, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA",IEEE Communications Surveys & Tutorials,"20 Nov 2020","2020","22","4","2462","2488","The Industrial Internet of Things (IIoT) is a crucial research field spawned by the Internet of Things (IoT). IIoT links all types of industrial equipment through the network; establishes data acquisition, exchange, and analysis systems; and optimizes processes and services, so as to reduce cost and enhance productivity. The introduction of edge computing in IIoT can significantly reduce the decision-making latency, save bandwidth resources, and to some extent, protect privacy. This paper outlines the research progress concerning edge computing in IIoT. First, the concepts of IIoT and edge computing are discussed, and subsequently, the research progress of edge computing is discussed and summarized in detail. Next, the future architecture from the perspective of edge computing in IIoT is proposed, and its technical progress in routing, task scheduling, data storage and analytics, security, and standardization is analyzed. Furthermore, we discuss the opportunities and challenges of edge computing in IIoT in terms of 5G-based edge communication, load balancing and data offloading, edge intelligence, as well as data sharing security. Finally, we introduce some typical application scenarios of edge computing in IIoT, such as prognostics and health management (PHM), smart grids, manufacturing coordination, intelligent connected vehicles (ICV), and smart logistics.","1553-877X","","10.1109/COMST.2020.3009103","National Key Research and Development Program of China(grant numbers:2019YFB1703601); National Natural Science Foundation of China(grant numbers:61672131,61971084,61702365); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139976","Industrial Internet of Things (IIoT);edge computing;reference architecture;advances and challenges;application scenarios","Edge computing;Cloud computing;Internet of Things;Computer architecture;Security;Delays","","481","","148","IEEE","14 Jul 2020","","","IEEE","IEEE Journals"
"Survey on 6G Frontiers: Trends, Applications, Requirements, Technologies and Future Research","C. D. Alwis; A. Kalla; Q. -V. Pham; P. Kumar; K. Dev; W. -J. Hwang; M. Liyanage","Department of Electrical and Electronic Engineering, University of Sri Jayewardenepura, Nugegoda, Sri Lanka; Centre for Wireless Communications, University of Oulu, Oulu, Finland; Korean Southeast Center for the 4th Industrial Revolution Leader Education, Pusan National University, Busan, Republic of Korea; Department of Computer Science, Swansea University, Swansea, U.K.; Nimbus Research Centre, Munster Technological University, Bishopstown, Cork, Ireland; Department of Biomedical Convergence Engineering, Pusan National University, Yangsan, South Korea; Centre for Wireless Communications, University of Oulu, Oulu, Finland",IEEE Open Journal of the Communications Society,"16 Apr 2021","2021","2","","836","886","Emerging applications such as Internet of Everything, Holographic Telepresence, collaborative robots, and space and deep-sea tourism are already highlighting the limitations of existing fifth-generation (5G) mobile networks. These limitations are in terms of data-rate, latency, reliability, availability, processing, connection density and global coverage, spanning over ground, underwater and space. The sixth-generation (6G) of mobile networks are expected to burgeon in the coming decade to address these limitations. The development of 6G vision, applications, technologies and standards has already become a popular research theme in academia and the industry. In this paper, we provide a comprehensive survey of the current developments towards 6G. We highlight the societal and technological trends that initiate the drive towards 6G. Emerging applications to realize the demands raised by 6G driving trends are discussed subsequently. We also elaborate the requirements that are necessary to realize the 6G applications. Then we present the key enabling technologies in detail. We also outline current research projects and activities including standardization efforts towards the development of 6G. Finally, we summarize lessons learned from state-of-the-art research and discuss technical challenges that would shed a new light on future research directions towards 6G.","2644-125X","","10.1109/OJCOMS.2021.3071496","National Research Foundation of Korea (NRF); Korean Government (MSIT)(grant numbers:NRF-2019R1C1C1006143,NRF-2019R1I1A3A01060518); European Union in RESPONSE 5G(grant numbers:789658); Academy of Finland in 6Genesis(grant numbers:318927); Secure Connect Projects; Institute of Information and Communications Technology Planning and Evaluation (IITP); Korea Government (MSIT)(grant numbers:2020-0-01450); Artificial Intelligence Convergence Research Center, Pusan National University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9397776","Beyond 5G;6G;mobile communication;emerging technologies;survey","6G mobile communication;5G mobile communication;Mobile communication;Internet of Things;Market research;Reliability;Broadband communication","","436","","378","CCBY","7 Apr 2021","","","IEEE","IEEE Journals"
"Smart Cities: A Survey on Data Management, Security, and Enabling Technologies","A. Gharaibeh; M. A. Salahuddin; S. J. Hussini; A. Khreishah; I. Khalil; M. Guizani; A. Al-Fuqaha","German Jordanian University, Amman, Jordan; University of Waterloo, Waterloo, ON, Canada; Western Michigan University, Kalamazoo, MI, USA; New Jersey Institute of Technology, Newark, NJ, USA; Qatar Computing Research Institute, Doha, Qatar; University of Idaho, Moscow, ID, USA; Western Michigan University, Kalamazoo, MI, USA",IEEE Communications Surveys & Tutorials,"20 Nov 2017","2017","19","4","2456","2501","Integrating the various embedded devices and systems in our environment enables an Internet of Things (IoT) for a smart city. The IoT will generate tremendous amount of data that can be leveraged for safety, efficiency, and infotainment applications and services for city residents. The management of this voluminous data through its lifecycle is fundamental to the realization of smart cities. Therefore, in contrast to existing surveys on smart cities we provide a data-centric perspective, describing the fundamental data management techniques employed to ensure consistency, interoperability, granularity, and reusability of the data generated by the underlying IoT for smart cities. Essentially, the data lifecycle in a smart city is dependent on tightly coupled data management with cross-cutting layers of data security and privacy, and supporting infrastructure. Therefore, we further identify techniques employed for data security and privacy, and discuss the networking and computing technologies that enable smart cities. We highlight the achievements in realizing various aspects of smart cities, present the lessons learned, and identify limitations and research challenges.","1553-877X","","10.1109/COMST.2017.2736886","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8003273","Smart cities;Internet of Things (IoT);data management;data security;network functions virtualization (NFV);software-defined networking (SDN);cloud computing","Smart cities;Data security;Data privacy;Tutorials;Internet of Things;Virtualization;Cloud computing;Software defined networking","","425","","423","IEEE","7 Aug 2017","","","IEEE","IEEE Journals"
"Digital Twin in the IoT Context: A Survey on Technical Features, Scenarios, and Architectural Models","R. Minerva; G. M. Lee; N. Crespi","CNRS Lab UMR5157, Telecom SudParis, Institut Polytechnique de Paris, Evry, France; Computer Science Department, Liverpool John Moores University, Liverpool, U.K.; CNRS Lab UMR5157, Telecom SudParis, Institut Polytechnique de Paris, Evry, France",Proceedings of the IEEE,"28 Sep 2020","2020","108","10","1785","1824","Digital twin (DT) is an emerging concept that is gaining attention in various industries. It refers to the ability to clone a physical object (PO) into a software counterpart. The softwarized object, termed logical object, reflects all the important properties and characteristics of the original object within a specific application context. To fully determine the expected properties of the DT, this article surveys the state-of-the-art starting from the original definition within the manufacturing industry. It takes into account related proposals emerging in other fields, namely augmented and virtual reality (e.g., avatars), multiagent systems, and virtualization. This survey thereby allows for the identification of an extensive set of DT features that point to the “softwarization” of POs. To properly consolidate a shared DT definition, a set of foundational properties is identified and proposed as a common ground outlining the essential characteristics (must-haves) of a DT. Once the DT definition has been consolidated, its technical and business value is discussed in terms of applicability and opportunities. Four application scenarios illustrate how the DT concept can be used and how some industries are applying it. The scenarios also lead to a generic DT architectural model. This analysis is then complemented by the identification of software architecture models and guidelines in order to present a general functional framework for the DT. This article, eventually, analyses a set of possible evolution paths for the DT considering its possible usage as a major enabler for the softwarization process.","1558-2256","","10.1109/JPROC.2020.2998530","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9120192","Artificial intelligence (AI);business models;cyber physical systems (CPSs);digital twin (DT);Internet of Things (IoT);machine learning (ML);multiagent systems;network function virtualization;sensors;servitization;smart city;software architecture;softwarization;virtual and augmented reality","Internet of Things;Cloning;Digital twin;Software engineering;Solid modeling;Software architecture;Manufacturing processing","","412","","293","IEEE","18 Jun 2020","","","IEEE","IEEE Journals"
"Fog Computing: Survey of Trends, Architectures, Requirements, and Research Directions","R. K. Naha; S. Garg; D. Georgakopoulos; P. P. Jayaraman; L. Gao; Y. Xiang; R. Ranjan","School of Technology, Environments and Design, University of Tasmania, Hobart, TAS, Australia; School of Technology, Environments and Design, University of Tasmania, Hobart, TAS, Australia; Faculty of Science, Engineering and Technology, Swinburne University of Technology, Melbourne, VIC, Australia; Faculty of Science, Engineering and Technology, Swinburne University of Technology, Melbourne, VIC, Australia; School of Information Technology, Deakin University, Burwood, VIC, Australia; School of Information Technology, Deakin University, Burwood, VIC, Australia; School of Computing, Newcastle University, Newcastle upon Tyne, U.K.",IEEE Access,"20 Sep 2018","2018","6","","47980","48009","Emerging technologies such as the Internet of Things (IoT) require latency-aware computation for real-time application processing. In IoT environments, connected things generate a huge amount of data, which are generally referred to as big data. Data generated from IoT devices are generally processed in a cloud infrastructure because of the on-demand services and scalability features of the cloud computing paradigm. However, processing IoT application requests on the cloud exclusively is not an efficient solution for some IoT applications, especially time-sensitive ones. To address this issue, Fog computing, which resides in between cloud and IoT devices, was proposed. In general, in the Fog computing environment, IoT devices are connected to Fog devices. These Fog devices are located in close proximity to users and are responsible for intermediate computation and storage. One of the key challenges in running IoT applications in a Fog computing environment are resource allocation and task scheduling. Fog computing research is still in its infancy, and taxonomy-based investigation into the requirements of Fog infrastructure, platform, and applications mapped to current research is still required. This survey will help the industry and research community synthesize and identify the requirements for Fog computing. This paper starts with an overview of Fog computing in which the definition of Fog computing, research trends, and the technical differences between Fog and cloud are reviewed. Then, we investigate numerous proposed Fog computing architectures and describe the components of these architectures in detail. From this, the role of each component will be defined, which will help in the deployment of Fog computing. Next, a taxonomy of Fog computing is proposed by considering the requirements of the Fog computing paradigm. We also discuss existing research works and gaps in resource allocation and scheduling, fault tolerance, simulation tools, and Fog-based microservices. Finally, by addressing the limitations of current research works, we present some open issues, which will determine the future research direction for the Fog computing paradigm.","2169-3536","","10.1109/ACCESS.2018.2866491","University of Tasmania; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8444370","Fog computing;Internet of Things (IoT);fog devices;fault tolerance;IoT application;microservices","Edge computing;Cloud computing;Computer architecture;Market research;Internet of Things;Resource management;Taxonomy","","392","","142","OAPA","22 Aug 2018","","","IEEE","IEEE Journals"
"Toward Edge Intelligence: Multiaccess Edge Computing for 5G and Internet of Things","Y. Liu; M. Peng; G. Shou; Y. Chen; S. Chen","Yaqiong Liu and Guochu Shou are all with the School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering and the State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Yaqiong Liu and Guochu Shou are all with the School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Cyberspace Security, Beijing University of Posts and Telecommunications, Beijing, China; International School, Beijing University of Posts and Telecommunications, Beijing, China",IEEE Internet of Things Journal,"12 Aug 2020","2020","7","8","6722","6747","To satisfy the increasing demand of mobile data traffic and meet the stringent requirements of the emerging Internet-of-Things (IoT) applications such as smart city, healthcare, and augmented/virtual reality (AR/VR), the fifth-generation (5G) enabling technologies are proposed and utilized in networks. As an emerging key technology of 5G and a key enabler of IoT, multiaccess edge computing (MEC), which integrates telecommunication and IT services, offers cloud computing capabilities at the edge of the radio access network (RAN). By providing computational and storage resources at the edge, MEC can reduce latency for end users. Hence, this article investigates MEC for 5G and IoT comprehensively. It analyzes the main features of MEC in the context of 5G and IoT and presents several fundamental key technologies which enable MEC to be applied in 5G and IoT, such as cloud computing, software-defined networking/network function virtualization, information-centric networks, virtual machine (VM) and containers, smart devices, network slicing, and computation offloading. In addition, this article provides an overview of the role of MEC in 5G and IoT, bringing light into the different MEC-enabled 5G and IoT applications as well as the promising future directions of integrating MEC with 5G and IoT. Moreover, this article further elaborates research challenges and open issues of MEC for 5G and IoT. Last but not least, we propose a use case that utilizes MEC to achieve edge intelligence in IoT scenarios.","2327-4662","","10.1109/JIOT.2020.3004500","National Natural Science Foundation of China(grant numbers:61901052); Higher Education Discipline Innovation Project(grant numbers:B17007); Director Funds of Beijing Key Laboratory of Network System Architecture and Convergence(grant numbers:2017BKL-NSAC-ZJ-02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9123504","Edge computing;edge intelligence;fifth generation (5G);Internet of Things (IoT);low latency;mobile-edge computing;multiaccess edge computing (MEC)","Internet of Things;5G mobile communication;Cloud computing;Servers;Edge computing;Bandwidth;Real-time systems","","369","","192","IEEE","23 Jun 2020","","","IEEE","IEEE Journals"
"Understanding O-RAN: Architecture, Interfaces, Algorithms, Security, and Research Challenges","M. Polese; L. Bonati; S. D’Oro; S. Basagni; T. Melodia","Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA",IEEE Communications Surveys & Tutorials,"22 May 2023","2023","25","2","1376","1411","The Open Radio Access Network (RAN) and its embodiment through the O-RAN Alliance specifications are poised to revolutionize the telecom ecosystem. O-RAN promotes virtualized RANs where disaggregated components are connected via open interfaces and optimized by intelligent controllers. The result is a new paradigm for the RAN design, deployment, and operations: O-RAN networks can be built with multi-vendor, interoperable components, and can be programmatically optimized through a centralized abstraction layer and data-driven closed-loop control. Therefore, understanding O-RAN, its architecture, its interfaces, and workflows is key for researchers and practitioners in the wireless community. In this article, we present the first detailed tutorial on O-RAN. We also discuss the main research challenges and review early research results. We provide a deep dive of the O-RAN specifications, describing its architecture, design principles, and the O-RAN interfaces. We then describe how the O-RAN RAN Intelligent Controllers (RICs) can be used to effectively control and manage 3GPP-defined RANs. Based on this, we discuss innovations and challenges of O-RAN networks, including the Artificial Intelligence (AI) and Machine Learning (ML) workflows that the architecture and interfaces enable, security, and standardization issues. Finally, we review experimental research platforms that can be used to design and test O-RAN networks, along with recent research results, and we outline future directions for O-RAN development.","1553-877X","","10.1109/COMST.2023.3239220","U.S. National Science Foundation(grant numbers:CNS-1923789,CNS-2112471); U.S. Office of Naval Research(grant numbers:N00014-20-1-2132); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10024837","Open RAN;O-RAN;cellular;5G;6G","Computer architecture;Security;Precoding;3GPP;Radio frequency;Radio access networks;Optimization","","338","","215","CCBY","23 Jan 2023","","","IEEE","IEEE Journals"
"Internet of Things for the Future of Smart Agriculture: A Comprehensive Survey of Emerging Technologies","O. Friha; M. A. Ferrag; L. Shu; L. Maglaras; X. Wang","Networks and Systems Laboratory, University of Badji Mokhtar-Annaba, Annaba, Algeria; Department of Computer Science, Guelma University, Gulema, Algeria; School of Engineering, University of Lincoln, Lincoln, UK; School of Computer Science and Informatics, De Montfort University, Leicester, UK; Department of Electrical Engineering, Nanjing Agricultural University, Nanjing, China",IEEE/CAA Journal of Automatica Sinica,"10 Mar 2021","2021","8","4","718","752","This paper presents a comprehensive review of emerging technologies for the internet of things (IoT)-based smart agriculture. We begin by summarizing the existing surveys and describing emergent technologies for the agricultural IoT, such as unmanned aerial vehicles, wireless technologies, open-source IoT platforms, software defined networking (SDN), network function virtualization (NFV) technologies, cloud/fog computing, and middleware platforms. We also provide a classification of IoT applications for smart agriculture into seven categories: including smart monitoring, smart water management, agrochemicals applications, disease management, smart harvesting, supply chain management, and smart agricultural practices. Moreover, we provide a taxonomy and a side-by-side comparison of the state-of-the-art methods toward supply chain management based on the blockchain technology for agricultural IoTs. Furthermore, we present real projects that use most of the aforementioned technologies, which demonstrate their great performance in the field of smart agriculture. Finally, we highlight open research challenges and discuss possible future research directions for agricultural IoTs.","2329-9274","","10.1109/JAS.2021.1003925","Research Start-Up Fund for Talent Researcher of Nanjing Agricultural University(grant numbers:77H0603); National Natural Science Foundation of China(grant numbers:62072248); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9374808","Agricultural internet of things (IoT);internet of things (IoT);smart agriculture;smart farming;sustainable agriculture","Wireless communication;Supply chain management;Agriculture;Unmanned aerial vehicles;Internet of Things;Software defined networking;Open source software","","324","","258","","10 Mar 2021","","","IEEE","IEEE Journals"
"Computation Offloading Toward Edge Computing","L. Lin; X. Liao; H. Jin; P. Li","College of Mathematics and Informatics, Fujian Normal University, Fuzhou, China; Service Computing Technology and System Laboratory, Cluster and Grid Computing Laboratory, National Engineering Research Center for Big Data Technology and System, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Service Computing Technology and System Laboratory, Cluster and Grid Computing Laboratory, National Engineering Research Center for Big Data Technology and System, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science and Engineering, The University of Aizu, Aizuwakamatsu, Japan",Proceedings of the IEEE,"6 Aug 2019","2019","107","8","1584","1607","We are living in a world where massive end devices perform computing everywhere and everyday. However, these devices are constrained by the battery and computational resources. With the increasing number of intelligent applications (e.g., augmented reality and face recognition) that require much more computational power, they shift to perform computation offloading to the cloud, known as mobile cloud computing (MCC). Unfortunately, the cloud is usually far away from end devices, leading to a high latency as well as the bad quality of experience (QoE) for latency-sensitive applications. In this context, the emergence of edge computing is no coincidence. Edge computing extends the cloud to the edge of the network, close to end users, bringing ultra-low latency and high bandwidth. Consequently, there is a trend of computation offloading toward edge computing. In this paper, we provide a comprehensive perspective on this trend. First, we give an insight into the architecture refactoring in edge computing. Based on that insight, this paper reviews the state-of-the-art research on computation offloading in terms of application partitioning, task allocation, resource management, and distributed execution, with highlighting features for edge computing. Then, we illustrate some disruptive application scenarios that we envision as critical drivers for the flourish of edge computing, such as real-time video analytics, smart “things” (e.g., smart city and smart home), vehicle applications, and cloud gaming. Finally, we discuss the opportunities and future research directions.","1558-2256","","10.1109/JPROC.2019.2922285","National Basic Research Program of China (973 Program)(grant numbers:2018YFB1003500); National Natural Science Foundation of China(grant numbers:61832006,61825202,61502103); Japan Society for the Promotion of Science(grant numbers:19K20258); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8758310","Computation offloading;edge computing;Internet of Things (IoT);mobile cloud computing (MCC);mobile edge computing (MEC)","Backscatter;Wireless communication;Throughput;Radio frequency;Edge computing;Resource management;Energy harvesting;Internet of Things;Cloud computing","","320","","236","IEEE","9 Jul 2019","","","IEEE","IEEE Journals"
"A Survey on Emerging SDN and NFV Security Mechanisms for IoT Systems","I. Farris; T. Taleb; Y. Khettab; J. Song","Department of Communications and Networking, Aalto University, Espoo, Finland; Computer and Information Security Department, Sejong University, Seoul, South Korea; Department of Communications and Networking, Aalto University, Espoo, Finland; Computer and Information Security Department, Sejong University, Seoul, South Korea",IEEE Communications Surveys & Tutorials,"22 Feb 2019","2019","21","1","812","837","The explosive rise of Internet of Things (IoT) systems have notably increased the potential attack surfaces for cybercriminals. Accounting for the features and constraints of IoT devices, traditional security countermeasures can be inefficient in dynamic IoT environments. In this vein, the advantages introduced by software defined networking (SDN) and network function virtualization (NFV) have the potential to reshape the landscape of cybersecurity for IoT systems. To this aim, we provide a comprehensive analysis of security features introduced by NFV and SDN, describing the manifold strategies able to monitor, protect, and react to IoT security threats. We also present lessons learned in the adoption of SDN/NFV-based protection approaches in IoT environments, comparing them with conventional security countermeasures. Finally, we deeply discuss the open challenges related to emerging SDN- and NFV-based security mechanisms, aiming to provide promising directives to conduct future research in this fervent area.","1553-877X","","10.1109/COMST.2018.2862350","ANASTACIA Project through the European Unions Horizon 2020 Research and Innovation Programme(grant numbers:731558); Swiss State Secretariat for Education, Research, and Innovation; National Research Foundation of Korea; Ministry of Education(grant numbers:NRF-2017R1D1A1B03036285); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8424018","Internet of Things;security;SDN;NFV;cloud;edge computing","Security;Wireless sensor networks;Protocols;Internet of Things;Cloud computing;Robot sensing systems","","305","","204","IEEE","1 Aug 2018","","","IEEE","IEEE Journals"
"Industrial Artificial Intelligence in Industry 4.0 - Systematic Review, Challenges and Outlook","R. S. Peres; X. Jia; J. Lee; K. Sun; A. W. Colombo; J. Barata","Centre of Technology and Systems, UNINOVA Instituto Desenvolvimento de Novas Tecnologias, Caparica, Portugal; Department of Mechanical and Materials Engineering, University of Cincinnati, Cincinnati, OH, USA; Department of Mechanical and Materials Engineering, University of Cincinnati, Cincinnati, OH, USA; Foxconn Industrial Internet, Milwaukee, WI, USA; Department of Electrotechnical and Industrial Informatics, University of Applied Sciences Emden/Leer, Emden, Germany; Centre of Technology and Systems, UNINOVA Instituto Desenvolvimento de Novas Tecnologias, Caparica, Portugal",IEEE Access,"16 Dec 2020","2020","8","","220121","220139","The advent of the Industry 4.0 initiative has made it so that manufacturing environments are becoming more and more dynamic, connected but also inherently more complex, with additional inter-dependencies, uncertainties and large volumes of data being generated. Recent advances in Industrial Artificial Intelligence have showcased the potential of this technology to assist manufacturers in tackling the challenges associated with this digital transformation of Cyber-Physical Systems, through its data-driven predictive analytics and capacity to assist decision-making in highly complex, non-linear and often multistage environments. However, the industrial adoption of such solutions is still relatively low beyond the experimental pilot stage, as real environments provide unique and difficult challenges for which organizations are still unprepared. The aim of this paper is thus two-fold. First, a systematic review of current Industrial Artificial Intelligence literature is presented, focusing on its application in real manufacturing environments to identify the main enabling technologies and core design principles. Then, a set of key challenges and opportunities to be addressed by future research efforts are formulated along with a conceptual framework to bridge the gap between research in this field and the manufacturing industry, with the goal of promoting industrial adoption through a successful transition towards a digitized and data-driven company-wide culture. This paper is among the first to provide a clear definition and holistic view of Industrial Artificial Intelligence in the Industry 4.0 landscape, identifying and analysing its fundamental building blocks and ongoing trends. Its findings are expected to assist and empower researchers and manufacturers alike to better understand the requirements and steps necessary for a successful transition into Industry 4.0 supported by AI, as well as the challenges that may arise during this process.","2169-3536","","10.1109/ACCESS.2020.3042874","FCT/MCTES (UNINOVA-CTS)(grant numbers:UIDB/00066/2020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9285283","Artificial intelligence;Industry 4.0;digital transformation;guidelines;systematic review;framework;manufacturing","Artificial intelligence;Industries;Robots;Systematics;Manufacturing;Decision making;Service robots","","290","","102","CCBY","7 Dec 2020","","","IEEE","IEEE Journals"
"Elasticity in Cloud Computing: State of the Art and Research Challenges","Y. Al-Dhuraibi; F. Paraiso; N. Djarallah; P. Merle","Scalair Company, Hem, France; Inria, Lille - Nord Europe, Villeneuve d'Ascq, France; Scalair Company, Hem, France; Inria, Lille - Nord Europe, Villeneuve d'Ascq, France",IEEE Transactions on Services Computing,"6 Apr 2018","2018","11","2","430","447","Elasticity is a fundamental property in cloud computing that has recently witnessed major developments. This article reviews both classical and recent elasticity solutions and provides an overview of containerization, a new technological trend in lightweight virtualization. It also discusses major issues and research challenges related to elasticity in cloud computing. We comprehensively review and analyze the proposals developed in this field. We provide a taxonomy of elasticity mechanisms according to the identified works and key properties. Compared to other works in literature, this article presents a broader and detailed analysis of elasticity approaches and is considered as the first survey addressing the elasticity of containers.","1939-1374","","10.1109/TSC.2017.2711009","OCCIware research and development project; French Programme d'Investissements d'Avenir; Scalair company; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7937885","Elasticity;cloud computing;auto-scaling;resource provision;scalability;containers","Elasticity;Cloud computing;Virtualization;Containers;Scalability;Operating systems;Taxonomy","","267","","197","IEEE","1 Jun 2017","","","IEEE","IEEE Journals"
"Internet of Things (IoT): A Review of Its Enabling Technologies in Healthcare Applications, Standards Protocols, Security, and Market Opportunities","M. N. Bhuiyan; M. M. Rahman; M. M. Billah; D. Saha","Institute of Information Technology, Noakhali Science and Technology University, Noakhali, Bangladesh; Department of Information and Communication Technology, Islamic University, Kushtia, Bangladesh; Department of Computer Science, American International University-Bangladesh, Dhaka, Bangladesh; Institute of Information Technology, Noakhali Science and Technology University, Noakhali, Bangladesh",IEEE Internet of Things Journal,"22 Jun 2021","2021","8","13","10474","10498","The Internet of Things (IoT) is a methodology or a system that encompasses real-world things to interact and communicate with each other with the assistance of networking technologies. This article describes surveys on advances in IoT-based healthcare methods and reviews the state-of-the-art technologies in detail. Moreover, this review classifies an existing IoT-based healthcare network and represents a summary of all perspective networks. IoT healthcare protocols are analyzed in this context and provide a broad discussion on it. It also initiates a comprehensive survey on IoT healthcare applications and services. Extensive insights into IoT healthcare security, its requirements, challenges, and privacy issues are visualized in IoT surrounding healthcare. In this review, we analyze security and privacy features consisting of data protection, network architecture, Quality of Services (QoS), app development, and continuous monitoring of healthcare that are facing difficulties in many IoT-based healthcare architectures. To mitigate the security problems, an IoT-based security architectural model has been proposed in this review. Furthermore, this review discloses the market opportunity that will enhance the IoT healthcare market development. To conduct the survey, we searched through established journal and conference databases using specific keywords to find scholarly works. We applied a filtering mechanism to collect only papers that were relevant to our research works. The selected papers were then examined carefully to understand their contributions/research focus. Eventually, the paper reviews were analyzed to identify any existing research gaps and untouched areas of research and to discover possible features for sustainable IoT healthcare development.","2327-4662","","10.1109/JIOT.2021.3062630","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9365708","Architectures;healthcare;Internet of Things (IoT);networks;security","Medical services;Security;Internet of Things;Monitoring;Sensors;Protocols;Analytical models","","250","","165","IEEE","1 Mar 2021","","","IEEE","IEEE Journals"
"A Full Dive Into Realizing the Edge-Enabled Metaverse: Visions, Enabling Technologies, and Challenges","M. Xu; W. C. Ng; W. Y. B. Lim; J. Kang; Z. Xiong; D. Niyato; Q. Yang; X. Shen; C. Miao","School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; Alibaba Group and Alibaba-NTU Joint Research Institute, Nanyang Technological University, Jurong West, Singapore; Alibaba Group and Alibaba-NTU Joint Research Institute, Nanyang Technological University, Jurong West, Singapore; School of Automation, Guangdong University of Technology, Guangzhou, China; Pillar of Information Systems Technology and Design, Singapore University of Technology and Design, Tampines, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada; School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore",IEEE Communications Surveys & Tutorials,"23 Feb 2023","2023","25","1","656","700","Dubbed “the successor to the mobile Internet,” the concept of the Metaverse has grown in popularity. While there exist lite versions of the Metaverse today, they are still far from realizing the full vision of an immersive, embodied, and interoperable Metaverse. Without addressing the issues of implementation from the communication and networking, as well as computation perspectives, the Metaverse is difficult to succeed the Internet, especially in terms of its accessibility to billions of users today. In this survey, we focus on the edge-enabled Metaverse to realize its ultimate vision. We first provide readers with a succinct tutorial of the Metaverse, an introduction to the architecture, as well as current developments. To enable ubiquitous, seamless, and embodied access to the Metaverse, we discuss the communication and networking challenges and survey cutting-edge solutions and concepts that leverage next-generation communication systems for users to immerse as and interact with embodied avatars in the Metaverse. Moreover, given the high computation costs required, e.g., to render 3D virtual worlds and run data-hungry artificial intelligence-driven avatars, we discuss the computation challenges and cloud-edge-end computation framework-driven solutions to realize the Metaverse on resource-constrained edge devices. Next, we explore how blockchain technologies can aid in the interoperable development of the Metaverse, not just in terms of empowering the economic circulation of virtual user-generated content but also to manage physical edge resources in a decentralized, transparent, and immutable manner. Finally, we discuss the future research directions towards realizing the true vision of the edge-enabled Metaverse.","1553-877X","","10.1109/COMST.2022.3221119","National Research Foundation (NRF) and Infocomm Media Development Authority through the Future Communications Research and Development Programme (FCP), through the AI Singapore Programme (AISG) and DSO National Laboratories(grant numbers:AISG2-RP-2020-019); Energy Research Test-Bed and Industry Partnership Funding Initiative; Energy Grid (EG) 2.0 Programme through the DesCartes and the Campus for Research Excellence and Technological Enterprise (CREATE) Programme; Alibaba Group through Alibaba Innovative Research (AIR) Program and Alibaba-NTU Singapore Joint Research Institute (JRI); NSFC(grant numbers:62102099,U22A2054); SUTD(grant numbers:SRG-ISTD-2021-165); SUTD-ZJU IDEA Grant [SUTDZJU (VP)](grant numbers:202102); SUTD-ZJU IDEA Seed Grant [SUTD-ZJU (SD)](grant numbers:202101); Ministry of Education, Singapore, through its SUTD Kickstarter Initiative(grant numbers:SKI 20210204); Wallenberg-NTU Presidential Postdoctoral Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9944868","Metaverse;edge networks;communication and networking;computation;blockchain;Internet technology","Metaverse;Internet;Artificial intelligence;Blockchains;Tutorials;Computer architecture;Avatars","","240","","322","IEEE","10 Nov 2022","","","IEEE","IEEE Journals"
"A Prospective Look: Key Enabling Technologies, Applications and Open Research Topics in 6G Networks","L. Bariah; L. Mohjazi; S. Muhaidat; P. C. Sofotasios; G. K. Kurt; H. Yanikomeroglu; O. A. Dobre","Department of Electrical and Computer Engineering, KU Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, United Arab Emirates; School of Engineering, University of Glasgow, Glasgow, U.K.; Department of Electrical and Computer Engineering, KU Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical and Computer Engineering, KU Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, United Arab Emirates; Electronics and Communication Engineering Department, Istanbul Technical University, Istanbul, Turkey; Department of Systems and Computer Engineering, Carleton University, Ottawa, Canada; Department of Electrical and Computer Engineering, Memorial University, St. John’s, Canada",IEEE Access,"2 Oct 2020","2020","8","","174792","174820","The fifth generation (5G) mobile networks are envisaged to enable a plethora of breakthrough advancements in wireless technologies, providing support of a diverse set of services over a single platform. While the deployment of 5G systems is scaling up globally, it is time to look ahead for beyond 5G systems. This is mainly driven by the emerging societal trends, calling for fully automated systems and intelligent services supported by extended reality and haptics communications. To accommodate the stringent requirements of their prospective applications, which are data-driven and defined by extremely low-latency, ultra-reliable, fast and seamless wireless connectivity, research initiatives are currently focusing on a progressive roadmap towards the sixth generation (6G) networks, which are expected to bring transformative changes to this premise. In this article, we shed light on some of the major enabling technologies for 6G, which are expected to revolutionize the fundamental architectures of cellular networks and provide multiple homogeneous artificial intelligence-empowered services, including distributed communications, control, computing, sensing, and energy, from its core to its end nodes. In particular, the present paper aims to answer several 6G framework related questions: What are the driving forces for the development of 6G? How will the enabling technologies of 6G differ from those in 5G? What kind of applications and interactions will they support which would not be supported by 5G? We address these questions by presenting a comprehensive study of the 6G vision and outlining seven of its disruptive technologies, i.e., mmWave communications, terahertz communications, optical wireless communications, programmable metasurfaces, drone-based communications, backscatter communications and tactile internet, as well as their potential applications. Then, by leveraging the state-of-the-art literature surveyed for each technology, we discuss the associated requirements, key challenges, and open research problems. These discussions are thereafter used to open up the horizon for future research directions.","2169-3536","","10.1109/ACCESS.2020.3019590","Khalifa University of Science, Technology and Research(grant numbers:KU/FSU-8474000122,KU/RC1-C2PS-2/847400012237); Natural Sciences and Engineering Research Council of Canada (NSERC), through its Discovery Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9178307","6G;backscatter communications;drone-based communications;terahertz communications;metasurfaces;mm-wave;optical wireless communications;tactile internet","5G mobile communication;Wireless communication;Wireless sensor networks;Market research;Artificial intelligence;Reliability;Haptic interfaces","","238","","350","CCBY","26 Aug 2020","","","IEEE","IEEE Journals"
"Security and Privacy for 6G: A Survey on Prospective Technologies and Challenges","V. -L. Nguyen; P. -C. Lin; B. -C. Cheng; R. -H. Hwang; Y. -D. Lin","Department of Computer Science and Information Engineering, National Chung Cheng University, Chiayi, Minhsiung, Taiwan; Department of Computer Science and Information Engineering, National Chung Cheng University, Chiayi, Minhsiung, Taiwan; Department of Communications Engineering, National Chung Cheng University, Chiayi, Minhsiung, Taiwan; Department of Computer Science and Information Engineering, Advanced Institute of Manufacturing with High-Tech Innovations, National Chung Cheng University, Chiayi, Minhsiung, Taiwan; Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan",IEEE Communications Surveys & Tutorials,"19 Nov 2021","2021","23","4","2384","2428","Sixth-generation (6G) mobile networks will have to cope with diverse threats on a space-air-ground integrated network environment, novel technologies, and an accessible user information explosion. However, for now, security and privacy issues for 6G remain largely in concept. This survey provides a systematic overview of security and privacy issues based on prospective technologies for 6G in the physical, connection, and service layers, as well as through lessons learned from the failures of existing security architectures and state-of-the-art defenses. Two key lessons learned are as follows. First, other than inheriting vulnerabilities from the previous generations, 6G has new threat vectors from new radio technologies, such as the exposed location of radio stripes in ultra-massive MIMO systems at Terahertz bands and attacks against pervasive intelligence. Second, physical layer protection, deep network slicing, quantum-safe communications, artificial intelligence (AI) security, platform-agnostic security, real-time adaptive security, and novel data protection mechanisms such as distributed ledgers and differential privacy are the top promising techniques to mitigate the attack magnitude and personal data breaches substantially.","1553-877X","","10.1109/COMST.2021.3108618","Ministry of Science and Technology (MOST) of Taiwan(grant numbers:110-2811-E-194-501-MY2,108-2221-E-194-022-MY3,108-2221-E-194-019-MY3); Advanced Institute of Manufacturing with High-Tech Innovations (AIM-HI) through the Featured Areas Research Center Program within the framework of the Higher Education Sprout Project by the Ministry of Education (MOE) in Taiwan; Ministry of Education and Training (MOET) of Vietnam and Thai Nguyen University(grant numbers:B2021-TNA-02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9524814","6G;security and privacy;AI security;physical layer security;connection security;service security","Security;6G mobile communication;Privacy;5G mobile communication;Computer architecture;Systematics;Standards","","234","","251","IEEE","30 Aug 2021","","","IEEE","IEEE Journals"
"Survey on Multi-Access Edge Computing Security and Privacy","P. Ranaweera; A. D. Jurcut; M. Liyanage","School of Computer Science, University College Dublin, Dublin 4, Ireland; School of Computer Science, University College Dublin, Dublin 4, Ireland; School of Computer Science, University College Dublin, Dublin 4, Ireland",IEEE Communications Surveys & Tutorials,"21 May 2021","2021","23","2","1078","1124","The European Telecommunications Standards Institute (ETSI) has introduced the paradigm of Multi-Access Edge Computing (MEC) to enable efficient and fast data processing in mobile networks. Among other technological requirements, security and privacy are significant factors in the realization of MEC deployments. In this paper, we analyse the security and privacy of the MEC system. We introduce a thorough investigation of the identification and the analysis of threat vectors in the ETSI standardized MEC architecture. Furthermore, we analyse the vulnerabilities leading to the identified threat vectors and propose potential security solutions to overcome these vulnerabilities. The privacy issues of MEC are also highlighted, and clear objectives for preserving privacy are defined. Finally, we present future directives to enhance the security and privacy of MEC services.","1553-877X","","10.1109/COMST.2021.3062546","European Union under RESPONSE 5G(grant numbers:789658); Academy of Finland under 6Genesis Flagship Projects(grant numbers:318927); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9364272","Multi-access edge computing (MEC);security;privacy;Internet of Things (IoT);5G;cloud computing;future networks","Security;Privacy;Cloud computing;5G mobile communication;Servers;Real-time systems;Outsourcing","","211","","208","IEEE","26 Feb 2021","","","IEEE","IEEE Journals"
"A Survey on Edge Computing Systems and Tools","F. Liu; G. Tang; Y. Li; Z. Cai; X. Zhang; T. Zhou","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; Key Laboratory of Science and Technology on Information System Engineering, National University of Defense Technology, Changsha, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; College of Computer, National University of Defense Technology, Changsha, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; College of Computer, National University of Defense Technology, Changsha, China",Proceedings of the IEEE,"6 Aug 2019","2019","107","8","1537","1562","Driven by the visions of Internet of Things and 5G communications, the edge computing systems integrate computing, storage, and network resources at the edge of the network to provide computing infrastructure, enabling developers to quickly develop and deploy edge applications. At present, the edge computing systems have received widespread attention in both industry and academia. To explore new research opportunities and assist users in selecting suitable edge computing systems for specific applications, this survey paper provides a comprehensive overview of the existing edge computing systems and introduces representative projects. A comparison of open-source tools is presented according to their applicability. Finally, we highlight energy efficiency and deep learning optimization of edge computing systems. Open issues for analyzing and designing an edge computing system are also studied in this paper.","1558-2256","","10.1109/JPROC.2019.2920341","National Basic Research Program of China (973 Program)(grant numbers:2016YFB1000302); National Natural Science Foundation of China(grant numbers:61433019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8746691","Deep learning optimization;edge computing systems;energy efficiency;open-source tools;survey","Cloud computing;Edge computing;Computer architecture;Energy efficiency;Internet of Things;Computational modeling;Deep learning;Open source software;Optimization","","210","","84","IEEE","26 Jun 2019","","","IEEE","IEEE Journals"
"Fog/Edge Computing-Based IoT (FECIoT): Architecture, Applications, and Research Issues","B. Omoniwa; R. Hussain; M. A. Javed; S. H. Bouk; S. A. Malik","Computer Science Programme, National Mathematical Centre, Abuja, Nigeria; Department of Electrical Engineering, COMSATS University, Islamabad, Pakistan; Department of Electrical Engineering, COMSATS University, Islamabad, Pakistan; Information and Communication Engineering, Daegu Gyeongbuk Institute of Science and Technology (DGIST), Daegu, South Korea; Department of Electrical Engineering, COMSATS University, Islamabad, Pakistan",IEEE Internet of Things Journal,"18 Jun 2019","2019","6","3","4118","4149","The Internet-of-Things (IoT) is the future of the Internet, where everything will be connected. Studies have revealed that fog/edge computing-based services will play a major role in extending the cloud by carrying out intermediary services at the edge of the network. Fog/edge computing-based IoT's (FECIoT) distributed architecture enhances service provisioning along the Cloud-to-Things continuum, thereby making it suitable for mission-critical applications. Furthermore, the proximity of fog/edge devices to where the data is produced makes it stand-out in terms of resource allocation, service delivery, and privacy. From the business perspective, FECIoT will lead to a boom and spring up of small-to-medium-sized enterprises, thereby encouraging inclusion for all. To this end, we present a comprehensive survey on state-of-the-art IoT literature over the period 2008-2018 and propose the FECIoT framework which covers the enabling technologies, services, and open research issues. A tutorial approach is employed, progressing from basic to more advanced concepts within the IoT domain. Lastly, we show how FECIoT can be deployed in real-life cyber-physical systems, such as the intelligent transportation system, smart grid, smart health-care, smart homes, and smart environment.","2327-4662","","10.1109/JIOT.2018.2875544","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8489908","Cyber-physical systems (CPSs);enabling technologies;fog/edge computing (FEC);Internet-of-Things (IoT);service-oriented architecture (SoA)","Cloud computing;Protocols;Computational modeling;Computer architecture;Security;Privacy;Internet of Things","","193","","181","IEEE","11 Oct 2018","","","IEEE","IEEE Journals"
"A Survey of Recent Advances in Edge-Computing-Powered Artificial Intelligence of Things","Z. Chang; S. Liu; X. Xiong; Z. Cai; G. Tu","School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; School of National Cybersecurity, Wuhan University, Wuhan, China",IEEE Internet of Things Journal,"6 Sep 2021","2021","8","18","13849","13875","The Internet of Things (IoT) has created a ubiquitously connected world powered by a multitude of wired and wireless sensors generating a variety of heterogeneous data over time in a myriad of fields and applications. To extract complete information from these data, advanced artificial intelligence (AI) technology, especially deep learning (DL), has proved successful in facilitating data analytics, future prediction and decision making. The collective integration of AI and the IoT has greatly promoted the rapid development of AI-of-Things (AIoT) systems that analyze and respond to external stimuli more intelligently without involvement by humans. However, it is challenging or infeasible to process massive amounts of data in the cloud due to the destructive impact of the volume, velocity, and veracity of data and fatal transmission latency on networking infrastructures. These critical challenges can be adequately addressed by introducing edge computing. This article conducts an extensive survey of an end-edge-cloud orchestrated architecture for flexible AIoT systems. Specifically, it begins with articulating fundamental concepts including the IoT, AI and edge computing. Guided by these concepts, it explores the general AIoT architecture, presents a practical AIoT example to illustrate how AI can be applied in real-world applications and summarizes promising AIoT applications. Then, the emerging technologies for AI models regarding inference and training at the edge of the network are reviewed. Finally, the open challenges and future directions in this promising area are outlined.","2327-4662","","10.1109/JIOT.2021.3088875","Major Projects of Technical Innovation of Hubei Province(grant numbers:2018AAA046); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9453402","Artificial intelligence (AI);deep learning (DL);edge computing;Internet of Things (IoT);machine learning (ML)","Artificial intelligence;Internet of Things;Cloud computing;Edge computing;Computer architecture;Computational modeling;Training","","177","","131","IEEE","14 Jun 2021","","","IEEE","IEEE Journals"
"6G Ecosystem: Current Status and Future Perspective","J. R. Bhat; S. A. Alqahtani","St. Joseph Engineering College, Mangalore, India; Research Chair of Pervasive and Mobile Computing, King Saud University, Riyadh, Saudi Arabia",IEEE Access,"19 Mar 2021","2021","9","","43134","43167","Next-generation of the cellular network will attempt to overcome the limitations of the current Fifth Generation (5G) networks and equip itself to address the challenges which become obvious in the future. Currently, academia and industry have focused their attention on the Sixth Generation (6G) network, which is anticipated to be the next big game-changer in the telecom industry. The outbreak of COVID'19 has made the whole world to opt for virtual meetings, live video interactions ranging from healthcare, business to education. However, we miss an immersive experience due to the lack of supporting technology. Experts have anticipated that starting from the post-pandemic age, the performance requirements of technology for virtual and real-time communication, the rise of several verticals such as industrial automation, robotics, and autonomous driving will increase tremendously, and will skyrocket during the next decade. In this manuscript, we study the latest perspectives and future megatrends that are most likely to drive 6G. Initially, we describe the instances that lead us to the vision of 6G. Later, we narrate some of the use cases and the KPIs essential to meet their performance requirement. Further, we highlight the key requirements of 6G based on contemporary research such as UN sustainability goals, business model, edge intelligence, digital divide, and the trends in machine learning for 6G.","2169-3536","","10.1109/ACCESS.2021.3054833","Deanship of Scientific Research, Research Chair of Pervasive and Mobile Computing, King Saud University, Riyadh, Saudi Arabia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9335927","6G;artificial intelligence;cloud computing;sustainability goals;digital divide;healthcare;machine learning;Tera hertz communication;cellular network;6G architecture","6G mobile communication;5G mobile communication;Reliability;Security;Industries;Surgery;Ecosystems","","172","","159","CCBYNCND","26 Jan 2021","","","IEEE","IEEE Journals"
"Advancing Software-Defined Networks: A Survey","J. H. Cox; J. Chung; S. Donovan; J. Ivey; R. J. Clark; G. Riley; H. L. Owen","Soar Technology, Inc., Ann Arbor, MI, USA; Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; College of Computing, Georgia Institute of Technology, Atlanta, GA, USA; Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA",IEEE Access,"7 Dec 2017","2017","5","","25487","25526","Having gained momentum from its promise of centralized control over distributed network architectures at bargain costs, software-defined Networking (SDN) is an ever-increasing topic of research. SDN offers a simplified means to dynamically control multiple simple switches via a single controller program, which contrasts with current network infrastructures where individual network operators manage network devices individually. Already, SDN has realized some extraordinary use cases outside of academia with companies, such as Google, AT&T, Microsoft, and many others. However, SDN still presents many research and operational challenges for government, industry, and campus networks. Because of these challenges, many SDN solutions have developed in an ad hoc manner that are not easily adopted by other organizations. Hence, this paper seeks to identify some of the many challenges where new and current researchers can still contribute to the advancement of SDN and further hasten its broadening adoption by network operators.","2169-3536","","10.1109/ACCESS.2017.2762291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8066287","Software-defined networking (SDN);network virtualization (NV);network functions virtualization (NFV);standards;SDN interfaces and APIs;data plane;middleboxes;SDN security;hybrid networks;software-defined exchange (SDX);software-defined infrastructure (SDI);software-defined wireless networks (SDWN);Internet of Things (IoT);information-centric networking (ICN);cloud;software-defined RAN;5G","Security;Government;Industries;Virtualization;Standards organizations;Centralized control","","170","","240","CCBY","12 Oct 2017","","","IEEE","IEEE Journals"
"A Survey on Edge and Edge-Cloud Computing Assisted Cyber-Physical Systems","K. Cao; S. Hu; Y. Shi; A. W. Colombo; S. Karnouskos; X. Li","College of Information Science and Technology, Jinan University, Guangzhou, China; School of Electronics and Computer Science, University of Southampton, Highfield, Southampton, U.K.; Department of Mechanical Engineering, University of Victoria, Victoria, BC, Canada; Institute for Industrial Informatics, Automation and Robotics, University of Applied Sciences Emden/Leer, Emden, Germany; Research, SAP, Walldorf, Germany; Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA",IEEE Transactions on Industrial Informatics,"30 Jul 2021","2021","17","11","7806","7819","In recent years, the investigations on cyber-physical systems (CPS) have become increasingly popular in both academia and industry. A primary obstruction against the booming deployment of CPS applications lies in how to process and manage large amounts of generated data for decision making. To tackle this predicament, researchers advocate the idea of coupling edge computing, or edge-cloud computing into the design of CPS. However, this coupling process raises a diversity of challenges to the quality-of-services (QoS) of CPS applications. In this article, we present a survey on edge computing or edge-cloud computing assisted CPS designs from the QoS optimization perspective. We first discuss critical challenges in service latency, energy consumption, security, privacy, and reliability during the integration of CPS with edge computing or edge-cloud computing. Afterwards, we give an overview on the state-of-the-art works tackling different challenges for QoS optimization, and present a systematic classification during outlining literature for highlighting their similarities and differences. We finally summarize the experiences learned from surveyed works and envision future research directions on edge computing or edge-cloud computing assisted CPS optimization.","1941-0050","","10.1109/TII.2021.3073066","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2020YFB1005600); Guangdong Key R&D Plan 2020(grant numbers:2020B0101090002); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2019B1515120010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9403939","Cyber-physical systems (CPS);edge computing;edge-cloud computing;energy;latency;privacy;reliability;security","Edge computing;Optimization;Cloud computing;Servers;Computer architecture;Reliability;Energy consumption","","170","","76","IEEE","13 Apr 2021","","","IEEE","IEEE Journals"
"Physical-Layer Security in Space Information Networks: A Survey","B. Li; Z. Fei; C. Zhou; Y. Zhang","School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Institute of High Energy Physics, Chinese Academy of Sciences, Beijing, China; Department of Informatics, University of Oslo, Oslo, Norway",IEEE Internet of Things Journal,"10 Jan 2020","2020","7","1","33","52","Research and processing development on satellite communications has strongly re-emerged in recent years. Following the prosperity of various wireless services provided by satellite communications, the security issue has raised growing concerns since the space information network is susceptible to be eavesdropped by illegal adversaries in such a large-scale wireless network. Recently, the physical-layer security (PLS) has emerged as an alternative security paradigm that explores the randomness of the wireless channel to achieve confidentiality and authentication. The success story of the PLS technique now spans a decade and thrives to provide a layer of defense in satellite communications. With this position, a comprehensive survey of satellite communications is conducted in this article with an emphasis on PLS. We first briefly introduce essential background and the view of the satellite Internet of Things (IoT), as well as discuss related research challenges faced by the emerging integrated network architecture. Then, we revisit the most popular satellite channel model influenced by many factors and list the commonly used secrecy performance metrics. Also, we provide an exhaustive review of state-of-the-art research activity on PLS in satellite communications, which we categorize by different architectures including land mobile satellite communication networks, hybrid satellite-terrestrial relay networks, and satellite-terrestrial integrated networks. In addition, a number of open research problems are identified as possible future research directions.","2327-4662","","10.1109/JIOT.2019.2943900","National Natural Science Foundation of China(grant numbers:61871032,61901447); European Union’s Horizon 2020 Research and Innovation Programme under Marie Skłodowska-Curie(grant numbers:824019); Sichuan Science and Technology Program(grant numbers:2019YFH0033); Startup Foundation for Introducing Talent of Nanjing University of Information Science and Technology; Guilin University of Electronic Technology(grant numbers:CRKL190204); Priority Academic Program Development of Jiangsu Higher Education Institutions; Natural Science Foundation of Shandong Province(grant numbers:ZR2019PF018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8850067","Fifth generation (5G) and beyond;heterogeneous networks;satellite communication;satellite Internet of Things (IoT);security","Satellite broadcasting;Security;Satellites;Internet of Things;Earth;Wireless communication","","161","","135","IEEE","26 Sep 2019","","","IEEE","IEEE Journals"
"Mobility Digital Twin: Concept, Architecture, Case Study, and Future Challenges","Z. Wang; R. Gupta; K. Han; H. Wang; A. Ganlath; N. Ammar; P. Tiwari","InfoTech Labs, Toyota Motor North America Research and Development, Mountain View, CA, USA; InfoTech Labs, Toyota Motor North America Research and Development, Mountain View, CA, USA; InfoTech Labs, Toyota Motor North America Research and Development, Mountain View, CA, USA; InfoTech Labs, Toyota Motor North America Research and Development, Mountain View, CA, USA; InfoTech Labs, Toyota Motor North America Research and Development, Mountain View, CA, USA; InfoTech Labs, Toyota Motor North America Research and Development, Mountain View, CA, USA; InfoTech Labs, Toyota Motor North America Research and Development, Mountain View, CA, USA",IEEE Internet of Things Journal,"7 Sep 2022","2022","9","18","17452","17467","A Digital Twin is a digital replica of a living or nonliving physical entity, and this emerging technology attracted extensive attention from different industries during the past decade. Although a few Digital Twin studies have been conducted in the transportation domain very recently, there is no systematic research with a holistic framework connecting various mobility entities together. In this study, a mobility digital twin (MDT) framework is developed, which is defined as an artificial intelligence (AI)-based data-driven cloud–edge–device framework for mobility services. This MDT consists of three building blocks in the physical space (namely, Human, Vehicle, and Traffic), and their associated Digital Twins in the digital space. An example cloud–edge architecture is built with Amazon Web Services (AWS) to accommodate the proposed MDT framework and to fulfill its digital functionalities of storage, modeling, learning, simulation, and prediction. A case study of the personalized adaptive cruise control (P-ACC) system is conducted, which integrates the key microservices of all three digital building blocks of the MDT framework: 1) the Human Digital Twin with user management and driver type classification; 2) the Vehicle Digital Twin with cloud-based advanced driver-assistance systems (ADAS); and 3) the Traffic Digital Twin with traffic flow monitoring and variable speed limit. Future challenges of the proposed MDT framework are discussed toward the end of the article, including standardization, AI for computing, public or private cloud service, and network heterogeneity.","2327-4662","","10.1109/JIOT.2022.3156028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9724183","Amazon Web services (AWS);cloud computing;connected vehicles;digital twin;edge computing","Digital twin;Cloud computing;Connected vehicles;Internet of Things;Vehicles;Space vehicles;Computational modeling","","139","","98","IEEE","2 Mar 2022","","","IEEE","IEEE Journals"
"Cloud-Edge Orchestration for the Internet of Things: Architecture and AI-Powered Data Processing","Y. Wu","College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, U.K.",IEEE Internet of Things Journal,"5 Aug 2021","2021","8","16","12792","12805","The Internet of Things (IoT) has been deeply penetrated into a wide range of important and critical sectors, including smart city, water, transportation, manufacturing, and smart factory. Massive data are being acquired from a fast growing number of IoT devices. Efficient data processing is a necessity to meet diversified and stringent requirements of many emerging IoT applications. Due to the constrained computation and storage resources, IoT devices have resorted to the powerful cloud computing to process their data. However, centralized and remote cloud computing may introduce unacceptable communication delay since its physical location is far away from IoT devices. Edge cloud has been introduced to overcome this issue by moving the cloud in closer proximity to IoT devices. The orchestration and cooperation between the cloud and the edge provides a crucial computing architecture for IoT applications. Artificial intelligence (AI) is a powerful tool to enable the intelligent orchestration in this architecture. This article first introduces such a kind of computing architecture from the perspective of IoT applications. It then investigates the state-of-the-art proposals on AI-powered cloud-edge orchestration for the IoT. Finally, a list of potential research challenges and open issues is provided and discussed, which can provide useful resources for carrying out future research in this area.","2327-4662","","10.1109/JIOT.2020.3014845","Engineering and Physical Sciences Research Council of United Kingdom(grant numbers:EP/R030863/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9162084","Artificial intelligence (AI);cloud computing;edge computing;Internet of Things (IoT);offloading","Cloud computing;Servers;Computer architecture;Internet of Things;Edge computing;Data processing;Medical services","","132","","124","IEEE","7 Aug 2020","","","IEEE","IEEE Journals"
"Blockchain for Edge of Things: Applications, Opportunities, and Challenges","T. R. Gadekallu; Q. -V. Pham; D. C. Nguyen; P. K. R. Maddikunta; N. Deepa; B. Prabadevi; P. N. Pathirana; J. Zhao; W. -J. Hwang","School of Information Technology and Engineering, Vellore Institute of Technology, Vellore, India; Korean Southeast Center for the 4th Industrial Revolution Leader Education, Pusan National University, Busan, South Korea; School of Engineering, Deakin University, Waurn Ponds, VIC, Australia; School of Information Technology and Engineering, Vellore Institute of Technology, Vellore, India; School of Information Technology and Engineering, Vellore Institute of Technology, Vellore, India; School of Information Technology and Engineering, Vellore Institute of Technology, Vellore, India; School of Engineering, Deakin University, Waurn Ponds, VIC, Australia; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Department of Biomedical Convergence Engineering, Pusan National University, Yangsan, South Korea",IEEE Internet of Things Journal,"6 Jan 2022","2022","9","2","964","988","In recent years, blockchain networks have attracted significant attention in many research areas beyond cryptocurrency, one of them being the Edge of Things (EoT) that is enabled by the combination of edge computing and the Internet of Things (IoT). In this context, blockchain networks enabled with unique features, such as decentralization, immutability, and traceability, have the potential to reshape and transform the conventional EoT systems with higher security levels. Particularly, the convergence of blockchain and EoT leads to a new paradigm, called BEoT that has been regarded as a promising enabler for future services and applications. In this article, we present a state-of-the-art review of recent developments in the BEoT technology and discover its great opportunities in many application domains. We start our survey by providing an updated introduction to blockchain and EoT along with their recent advances. Subsequently, we discuss the use of BEoT in a wide range of industrial applications, from smart transportation, smart city, smart healthcare to smart home, and smart grid. Security challenges in the BEoT paradigm are also discussed and analyzed, with some key services, such as access authentication, data privacy preservation, attack detection, and trust management. Finally, some key research challenges and future directions are also highlighted to instigate further research in this promising area.","2327-4662","","10.1109/JIOT.2021.3119639","National Research Foundation of Korea (NRF); Korean Government (MSIT)(grant numbers:NRF-2019R1C1C1006143,NRF-2019R1I1A3A01060518); Institute of Information and Communications Technology Planning and Evaluation (IITP); Korea Government (MSIT)(grant numbers:2020-0-01450); Artificial Intelligence Convergence Research Center [Pusan National University]); Ministry of Science and ICT (MSIT), South Korea; Grand Information Technology Research Center Support Program; Institute for Information Communications, Technology, Planning and Evaluation (IITP)(grant numbers:IITP-2021-2016-0-00318); BK21 Four, Korean Southeast Center for the 4th Industrial Revolution Leader Education; Pusan National University Research Grant, 2020; Nanyang Technological University Startup Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9569742","Blockchain;edge computing;Edge of Things (EoT);industrial applications;Internet of Things (IoT);security","Blockchains;Internet of Things;Security;Edge computing;Medical services;Smart transportation;Smart grids","","130","","139","IEEE","13 Oct 2021","","","IEEE","IEEE Journals"
"Fog Computing: A Comprehensive Architectural Survey","P. Habibi; M. Farhoudi; S. Kazemian; S. Khorsandi; A. Leon-Garcia","Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Department of Computer Engineering and Information Technology, Amirkabir University of Technology, Tehran, Iran; Department of Computer Engineering and Information Technology, Amirkabir University of Technology, Tehran, Iran; Department of Computer Engineering and Information Technology, Amirkabir University of Technology, Tehran, Iran; Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada",IEEE Access,"22 Apr 2020","2020","8","","69105","69133","Fog computing is an emerging technology to address computing and networking bottlenecks in large scale deployment of IoT applications. It is a promising complementary computing paradigm to cloud computing where computational, networking, storage and acceleration elements are deployed at the edge and network layers in a multi-tier, distributed and possibly cooperative manner. These elements may be virtualized computing functions placed at edge devices or network elements on demand, realizing the “computing everywhere” concept. To put the current research in perspective, this paper provides an inclusive taxonomy for architectural, algorithmic and technologic aspects of fog computing. The computing paradigms and their architectural distinctions, including cloud, edge, mobile edge and fog computing are subsequently reviewed. Practical deployment of fog computing includes a number of different aspects such as system design, application design, software implementation, security, computing resource management and networking. A comprehensive survey of all these aspects from the architectural point of view is covered. Current reference architectures and major application-specific architectures describing their salient features and distinctions in the context of fog computing are explored. Base architectures for application, software, security, computing resource management and networking are presented and are evaluated using a proposed maturity model.","2169-3536","","10.1109/ACCESS.2020.2983253","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9046806","Cloud Computing;edge computing;fog computing;Internet of Things (IoT);advanced internet architecture","Edge computing;Cloud computing;Computer architecture;Resource management;Security;Taxonomy;Software","","126","","191","CCBY","25 Mar 2020","","","IEEE","IEEE Journals"
"Investigating Messaging Protocols for the Internet of Things (IoT)","E. Al-Masri; K. R. Kalyanam; J. Batts; J. Kim; S. Singh; T. Vo; C. Yan","School of Engineering and Technology, University of Washington Tacoma, Tacoma, USA; School of Engineering and Technology, University of Washington Tacoma, Tacoma, USA; School of Engineering and Technology, University of Washington Tacoma, Tacoma, USA; School of Engineering and Technology, University of Washington Tacoma, Tacoma, USA; School of Engineering and Technology, University of Washington Tacoma, Tacoma, USA; School of Engineering and Technology, University of Washington Tacoma, Tacoma, USA; School of Engineering and Technology, University of Washington Tacoma, Tacoma, USA",IEEE Access,"28 May 2020","2020","8","","94880","94911","As the number of Internet of Things (IoT) devices proliferates, the magnitude and velocity of data continues to increase rapidly. IoT systems rely primarily on using messaging protocols for exchanging IoT data and there exists several protocols or frameworks that support distinct types of messaging patterns. Given that IoT devices typically have limited computational resources and processing power, choosing a lightweight, reliable, scalable, interoperable, extensible and secure messaging protocol becomes a very challenging task. As a result, it is not uncommon that IoT systems may employ multiple messaging protocols for supporting device heterogeneity and different message exchange patterns. In addition, basic similarities among existing several messaging protocols or frameworks that exist today for exchanging IoT data within IoT systems suggest the potential of interoperability. Given that IoT systems help facilitate the interconnectivity among distributed, heterogeneous entities, interoperability among existing messaging protocols will play an increasingly important role in simplifying the development and deployment of IoT systems. In this paper, we present a comprehensive review of the existing messaging protocols that can be used in deploying IoT systems. Throughout this paper, we highlight the protocols' distinctive approaches and applicability of using them across various IoT environments. In addition, we highlight challenges, strengths and weaknesses of these messaging protocols in the context of IoT.","2169-3536","","10.1109/ACCESS.2020.2993363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9090208","Internet of Things;IoT;HTTP;MQTT;CoAP;AMQP;XMPP;DDS;data distribution service;constrained application protocol;message queuing telemetry transport;extensible messaging and presence protocol;HyperText transfer protocol;edge computing;fog computing;cloud applications","Protocols;Internet of Things;Object recognition;Open systems;Middleware;Wireless sensor networks;Hardware","","123","","176","CCBY","8 May 2020","","","IEEE","IEEE Journals"
"Research on Artificial Intelligence Enhancing Internet of Things Security: A Survey","H. Wu; H. Han; X. Wang; S. Sun","School of Software and Microelectronics, Peking University, Beijing, China; Department of Food and Resource Economics, University of Copenhagen, Copenhagen, Denmark; School of Software and Microelectronics, Peking University, Beijing, China; School of Software and Microelectronics, Peking University, Beijing, China",IEEE Access,"1 Sep 2020","2020","8","","153826","153848","Through three development routes of authentication, communication, and computing, the Internet of Things (IoT) has become a variety of innovative integrated solutions for specific applications. However, due to the openness, extensiveness and resource constraints of IoT, each layer of the three-tier IoT architecture suffers from a variety of security threats. In this work, we systematically review the particularity and complexity of IoT security protection, and then find that Artificial Intelligence (AI) methods such as Machine Learning (ML) and Deep Learning (DL) can provide new powerful capabilities to meet the security requirements of IoT. We analyze the technical feasibility of AI in solving IoT security problems and summarize a general process of AI solutions for IoT security. For four serious IoT security threats: device authentication, Denial of Service (DoS) and Distributed Denial of Service (DDoS) attacks defense, intrusion detection and malware detection, we summarize representative AI solutions and compare the different algorithms and technologies used by various solutions. It should be noted that although AI provides many new capabilities for the security protection of IoT, it also brings new potential challenges and possible negative effects to IoT in terms of data, algorithm and architecture. In the future, how to solve these challenges can serve as potential research directions.","2169-3536","","10.1109/ACCESS.2020.3018170","National Basic Research Program of China (973 Program)(grant numbers:2018YFB1402900,2018YFB1403000); Natural Science Foundation of Jiangsu Province(grant numbers:BK20151132); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9172062","Artificial intelligence;deep learning;Internet of Things;machine learning;security","Internet of Things;Artificial intelligence;Authentication;Computer architecture;Software;Computer crime","","108","","100","CCBY","20 Aug 2020","","","IEEE","IEEE Journals"
"Edge Computing for Internet of Everything: A Survey","X. Kong; Y. Wu; H. Wang; F. Xia","College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; School of Software, Dalian University of Technology, Dalian, China; School of Software, Dalian University of Technology, Dalian, China; Institute of Innovation, Science and Sustainability, Federation University Australia, Ballarat, VIC, Australia",IEEE Internet of Things Journal,"18 Nov 2022","2022","9","23","23472","23485","In this era of the Internet of Everything (IoE), edge computing has emerged as the critical enabling technology to solve a series of issues caused by an increasing amount of interconnected devices and large-scale data transmission. However, the deficiencies of edge computing paradigm are gradually being magnified in the context of IoE, especially in terms of service migration, security and privacy preservation, and deployment issues of edge node. These issues can not be well addressed by conventional approaches. Thanks to the rapid development of upcoming technologies, such as artificial intelligence (AI), blockchain, and microservices, novel and more effective solutions have emerged and been applied to solve existing challenges. In addition, edge computing can be deeply integrated with technologies in other domains (e.g., AI, blockchain, 6G, and digital twin) through interdisciplinary intersection and practice, releasing the potential for mutual benefit. These promising integrations need to be further explored and researched. In addition, edge computing provides strong support in applications scenarios, such as remote working, new physical retail industries, and digital advertising, which has greatly changed the way we live, work, and study. In this article, we present an up-to-date survey of the edge computing research. In addition to introducing the definition, model, and characteristics of edge computing, we discuss a set of key issues in edge computing and novel solutions supported by emerging technologies in IoE era. Furthermore, we explore the potential and promising trends from the perspective of technology integration. Finally, new application scenarios and the final form of edge computing are discussed.","2327-4662","","10.1109/JIOT.2022.3200431","National Natural Science Foundation of China(grant numbers:62072409); Zhejiang Provincial Natural Science Foundation(grant numbers:LR21F020003); Fundamental Research Funds for the Provincial Universities of Zhejiang(grant numbers:RF-B2020001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9863881","6G;artificial intelligence (AI);blockchain;digital twin (DT);edge computing;microservices","Edge computing;Cloud computing;Internet of Things;Blockchains;Security;Computational modeling;Task analysis","","108","","99","IEEE","22 Aug 2022","","","IEEE","IEEE Journals"
"A Survey on the Integration of Blockchain With IoT to Enhance Performance and Eliminate Challenges","A. A. Sadawi; M. S. Hassan; M. Ndiaye","Department of Engineering Systems Management, American University of Sharjah, Sharjah, United Arab Emirates; Department of Electrical Engineering, American University of Sharjah, Sharjah, United Arab Emirates; Department of Engineering Systems Management, American University of Sharjah, Sharjah, United Arab Emirates",IEEE Access,"13 Apr 2021","2021","9","","54478","54497","Internet of things IoT is playing a remarkable role in the advancement of many fields such as healthcare, smart grids, supply chain management, etc. It also eases people's daily lives and enhances their interaction with each other as well as with their surroundings and the environment in a broader scope. IoT performs this role utilizing devices and sensors of different shapes and sizes ranging from small embedded sensors and wearable devices all the way to automated systems. However, IoT networks are growing in size, complexity, and number of connected devices. As a result, many challenges and problems arise such as security, authenticity, reliability, and scalability. Based on that and taking into account the anticipated evolution of the IoT, it is extremely vital not only to maintain but to increase confidence in and reliance on IoT systems by tackling the aforementioned issues. The emergence of blockchain opened the door to solve some challenges related to IoT networks. Blockchain characteristics such as security, transparency, reliability, and traceability make it the perfect candidate to improve IoT systems, solve their problems, and support their future expansion. This paper demonstrates the major challenges facing IoT systems and blockchain's proposed role in solving them. It also evaluates the position of current researches in the field of merging blockchain with IoT networks and the latest implementation stages. Additionally, it discusses the issues related to the IoT-blockchain integration itself. Finally, this research proposes an architectural design to integrate IoT with blockchain in two layers using dew and cloudlet computing. Our aim is to benefit from blockchain features and services to guarantee a decentralized data storage and processing and address security and anonymity challenges and achieve transparency and efficient authentication service.","2169-3536","","10.1109/ACCESS.2021.3070555","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9393912","Blockchain;IoT;smart contract;trust;IoT challenges;IoT security;decentralized IoT;cloudlet computing;dew computing;cloudlet-dew architecture","Internet of Things;Blockchain;Security;Sensors;Reliability;Performance evaluation;Wireless sensor networks","","101","","91","CCBY","2 Apr 2021","","","IEEE","IEEE Journals"
"Comprehensive Survey on T-SDN: Software-Defined Networking for Transport Networks","R. Alvizu; G. Maier; N. Kukreja; A. Pattavina; R. Morro; A. Capello; C. Cavazzoni","SWAN Networks, Politecnico di Milano, Milan, Italy; SWAN Networks, Politecnico di Milano, Milan, Italy; Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milan, Italy; SWAN Networks, Politecnico di Milano, Milan, Italy; Telecom Italia SpA, Turin, Italy; Telecom Italia SpA, Turin, Italy; Telecom Italia SpA, Turin, Italy",IEEE Communications Surveys & Tutorials,"20 Nov 2017","2017","19","4","2232","2283","Paradoxically, with an ever-increasing traffic demand, today transport-network operators experience a progressive erosion of their margins. The alarms of change are set, and software define networking (SDN) is coming to the rescue with the promise of reducing capital expenditures and operational expenses. Driven by economic needs and network innovation facilities, today transport SDN (T-SDN) is a reality. It gained big momentum in the last years, however, in the networking industry, the transport network will be perhaps the last segment to embrace SDN, mainly due to the heterogeneous nature and complexity of the optical equipment composing it. This survey guides the reader through a fascinating technological adventure that provides an organic analysis of the T-SDN development and evolution considering contributions from: academic research, standardization bodies, industrial development, open source projects, and alliances among them. After creating a comprehensive picture of T-SDN, we provide an analysis of many open issues that are expected to need significant future work, and give our vision in this path toward a fully programmable and dynamic transport network.","1553-877X","","10.1109/COMST.2017.2715220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7947156","Software defined networking;optical transport network;transport SDN;software defined optical networking;network programmability;orchestration;transport API;network controller;network virtualization;network function virtualization;OpenFlow;GMPLS","Optical fiber networks;Hardware;Technological innovation;Operating systems;Optical sensors;Tutorials","","94","","250","IEEE","13 Jun 2017","","","IEEE","IEEE Journals"
"A Comprehensive Survey of the Tactile Internet: State-of-the-Art and Research Directions","N. Promwongsa; A. Ebrahimzadeh; D. Naboulsi; S. Kianpisheh; F. Belqasmi; R. Glitho; N. Crespi; O. Alfandi","CIISE, Concordia University, Montréal, QC, Canada; CIISE, Concordia University, Montréal, QC, Canada; Département de Génie Logiciel et des Technologies de l’information, école de Technologie Supérieure, Montréal, QC, Canada; CIISE, Concordia University, Montréal, QC, Canada; College of Technological Innovation, Zayed University, Abu Dhabi, UAE; CIISE, Concordia University, Montréal, QC, Canada; Institut Mines-Telecom, Telecom SudParis, Evry, France; College of Technological Innovation, Zayed University, Abu Dhabi, UAE",IEEE Communications Surveys & Tutorials,"23 Feb 2021","2021","23","1","472","523","The Internet has made several giant leaps over the years, from a fixed to a mobile Internet, then to the Internet of Things, and now to a Tactile Internet. The Tactile Internet goes far beyond data, audio and video delivery over fixed and mobile networks, and even beyond allowing communication and collaboration among things. It is expected to enable haptic communications and allow skill set delivery over networks. Some examples of potential applications are tele-surgery, vehicle fleets, augmented reality and industrial process automation. Several papers already cover many of the Tactile Internet-related concepts and technologies, such as haptic codecs, applications, and supporting technologies. However, none of them offers a comprehensive survey of the Tactile Internet, including its architectures and algorithms. Furthermore, none of them provides a systematic and critical review of the existing solutions. To address these lacunae, we provide a comprehensive survey of the architectures and algorithms proposed to date for the Tactile Internet. In addition, we critically review them using a well-defined set of requirements and discuss some of the lessons learned as well as the most promising research directions.","1553-877X","","10.1109/COMST.2020.3025995","Canadian Natural Sciences and Engineering Research Council and by Ericsson through a CRD Grant; Canada Research Chair Program and Zayed University (United Arab Emirates) through the Research Incentive Fund Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9203885","5G/6G;artificial intelligence;edge computing;machine learning;tactile Internet","Haptic interfaces;Tutorials;Computer architecture;Surgery;Internet of Things;Reliability","","93","","192","CCBY","22 Sep 2020","","","IEEE","IEEE Journals"
"Machine Learning-Based Scaling Management for Kubernetes Edge Clusters","L. Toka; G. Dobreff; B. Fodor; B. Sonkoly","MTA-BME Network Softwarization Research Group, Budapest University of Technology and Economics, Budapest, Hungary; MTA-BME Network Softwarization Research Group, Budapest University of Technology and Economics, Budapest, Hungary; MTA-BME Network Softwarization Research Group, Budapest University of Technology and Economics, Budapest, Hungary; MTA-BME Network Softwarization Research Group, Budapest University of Technology and Economics, Budapest, Hungary",IEEE Transactions on Network and Service Management,"10 Mar 2021","2021","18","1","958","972","Kubernetes, the container orchestrator for cloud-deployed applications, offers automatic scaling for the application provider in order to meet the ever-changing intensity of processing demand. This auto-scaling feature can be customized with a parameter set, but those management parameters are static while incoming Web request dynamics often change, not to mention the fact that scaling decisions are inherently reactive, instead of being proactive. We set the ultimate goal of making cloud-based applications' management easier and more effective. We propose a Kubernetes scaling engine that makes the auto-scaling decisions apt for handling the actual variability of incoming requests. In this engine various machine learning forecast methods compete with each other via a short-term evaluation loop in order to always give the lead to the method that suits best the actual request dynamics. We also introduce a compact management parameter for the cloud-tenant application provider to easily set their sweet spot in the resource over-provisioning vs. SLA violation trade-off. We motivate our scaling solution with analytical modeling and evaluation of the current Kubernetes behavior. The multi-forecast scaling engine and the proposed management parameter are evaluated both in simulations and with measurements on our collected Web traces to show the improved quality of fitting provisioned resources to service demand. We find that with just a few, but fundamentally different, and competing forecast methods, our auto-scaler engine, implemented in Kubernetes, results in significantly fewer lost requests with just slightly more provisioned resources compared to the default baseline.","1932-4537","","10.1109/TNSM.2021.3052837","National Research, Development and Innovation Fund of Hungary under the FK_20 funding scheme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9328525","Cloud computing;machine learning;auto-scaling;Kubernetes;forecast;resource management","Cloud computing;Containers;Engines;Servers;Resource management;Analytical models;Adaptation models","","92","","44","IEEE","19 Jan 2021","","","IEEE","IEEE Journals"
"Zero Trust Architecture (ZTA): A Comprehensive Survey","N. F. Syed; S. W. Shah; A. Shaghaghi; A. Anwar; Z. Baig; R. Doss","Centre for Cyber Security Research and Innovation (CSRI), Deakin University, Geelong, VIC, Australia; Centre for Cyber Security Research and Innovation (CSRI), Deakin University, Geelong, VIC, Australia; Centre for Cyber Security Research and Innovation (CSRI), Deakin University, Geelong, VIC, Australia; Centre for Cyber Security Research and Innovation (CSRI), Deakin University, Geelong, VIC, Australia; Centre for Cyber Security Research and Innovation (CSRI), Deakin University, Geelong, VIC, Australia; Centre for Cyber Security Research and Innovation (CSRI), Deakin University, Geelong, VIC, Australia",IEEE Access,"7 Jun 2022","2022","10","","57143","57179","We present a detailed survey of the Zero Trust (ZT) security paradigm which has a growing number of advocates in the critical infrastructure risk management space. The article employs a descriptive approach to present the fundamental tenets of ZT and provides a review of numerous potential options available for successful realization of this paradigm. We describe the role of authentication and access control in Zero Trust Architectures (ZTA) and present an in-depth discussion of state-of-the-art techniques for authentication and access control in different scenarios. Furthermore, we comprehensively discuss the conventional approaches to encryption, micro-segmentation, and security automation available for instantiating a ZTA. The article also details various challenges associated with contemporary authentication mechanisms, access control schemes, trust and risk computation techniques, micro-segmentation approaches, and Software-Defined Perimeter, that can impact the implementation of ZT in its true sense. Based upon our analysis, we finally pinpoint the potential future research directions for successful realization of ZT in critical infrastructures.","2169-3536","","10.1109/ACCESS.2022.3174679","Australian Department of Defence; Defence Science and Technology Group’s Operations Research Network; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9773102","Zero trust architecture (ZTA);access control;authentication;micro-segmentation;software-defined parameter (SDP)","Access control;Authentication;Computer architecture;NIST;Encryption;Critical infrastructure;Automation","","91","","190","CCBY","12 May 2022","","","IEEE","IEEE Journals"
"A Roadmap Toward the Resilient Internet of Things for Cyber-Physical Systems","D. Ratasich; F. Khalid; F. Geissler; R. Grosu; M. Shafique; E. Bartocci","Institute of Computer Engineering, TU Wien, Vienna, Austria; Institute of Computer Engineering, TU Wien, Vienna, Austria; CPS Dependability Research Lab, Intel Deutschland GmbH, Neubiberg, Germany; Institute of Computer Engineering, TU Wien, Vienna, Austria; Institute of Computer Engineering, TU Wien, Vienna, Austria; Institute of Computer Engineering, TU Wien, Vienna, Austria",IEEE Access,"5 Feb 2019","2019","7","","13260","13283","The Internet of Things (IoT) is a ubiquitous system connecting many different devices - the things - which can be accessed from the distance. The cyber-physical systems (CPSs) monitor and control the things from the distance. As a result, the concepts of dependability and security get deeply intertwined. The increasing level of dynamicity, heterogeneity, and complexity adds to the system's vulnerability, and challenges its ability to react to faults. This paper summarizes the state of the art of existing work on anomaly detection, fault-tolerance, and self-healing, and adds a number of other methods applicable to achieve resilience in an IoT. We particularly focus on non-intrusive methods ensuring data integrity in the network. Furthermore, this paper presents the main challenges in building a resilient IoT for the CPS, which is crucial in the era of smart CPS with enhanced connectivity (an excellent example of such a system is connected autonomous vehicles). It further summarizes our solutions, work-in-progress and future work to this topic to enable “Trustworthy IoT for CPS”. Finally, this framework is illustrated on a selected use case: a smart sensor infrastructure in the transport domain.","2169-3536","","10.1109/ACCESS.2019.2891969","IoT4CPS; Österreichische Forschungsförderungsgesellschaft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8606923","Anomaly detection;cyber-physical systems (CPS);Internet of Things (IoT);monitoring;resilience;long-term dependability and security;self-adaptation;self-healing","Resilience;Security;Internet of Things;Cyber-physical systems;Robustness;Safety;Monitoring","","89","","203","CCBY","10 Jan 2019","","","IEEE","IEEE Journals"
"Multi-Access Edge Computing Architecture, Data Security and Privacy: A Review","B. Ali; M. A. Gregory; S. Li","School of Engineering, RMIT University, Melbourne, VIC, Australia; School of Engineering, RMIT University, Melbourne, VIC, Australia; School of Engineering, RMIT University, Melbourne, VIC, Australia",IEEE Access,"17 Feb 2021","2021","9","","18706","18721","Multi-Access Edge Computing (MEC) is an extension of cloud computing that aims to provide computation, storage, and networking capabilities at the edge of the network in close proximity to end-users. The MEC architecture supports applications and services that bridge between cloud computing and end-users. The architecture includes devices and systems that are interconnected, layered, and flexibly deployed. As a result of the technological advancements, MEC is facing a myriad of highly sophisticated threats. This paper provides a review of MEC Architecture, use cases, conceptual guidelines for MEC security architecture, security and privacy techniques, and identifies current and future challenges, their implications, and approaches to overcome the challenges. This research examined significant threats, described the MEC architecture, identified the susceptible functional layers, the different categories of threats, and the potential security safeguards. The research recommends that MEC providers should implement multiple layers of security controls to mitigate targeted attacks.","2169-3536","","10.1109/ACCESS.2021.3053233","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9330515","Multi-access edge computing;MEC;security;privacy;SDN;5G","Security;Computer architecture;Data security;Cloud computing;Servers;Data privacy;Edge computing","","85","","105","CCBY","21 Jan 2021","","","IEEE","IEEE Journals"
"Green Mobile Networks for 5G and Beyond","M. Masoudi; M. G. Khafagy; A. Conte; A. El-Amine; B. Françoise; C. Nadjahi; F. E. Salem; W. Labidi; A. Süral; A. Gati; D. Bodéré; E. Arikan; F. Aklamanu; H. Louahlia-Gualous; J. Lallet; K. Pareek; L. Nuaymi; L. Meunier; P. Silva; N. T. Almeida; T. Chahed; T. Sjölund; C. Cavdar","KTH Royal Institute of Technology, Kista, Sweden; KTH Royal Institute of Technology, Kista, Sweden; Nokia Bell Labs, Nozay, France; Departement Système Réseaux, Cybersécurité et Droit du numérique (SRCD), IMT Atlantique, CS 17607 35576, Cesson-Sévigné, France; Orange Gardens, Chátillon, France; LUSAC, UNICAEN, Normandie University, Caen, France; Orange Gardens, Chátillon, France; Institut Polytechnique Paris, Telecom SudParis, UMR CNRS Samovar, Evry, France; Polaran Ltd, Ankara, Turkey; Orange Gardens, Chátillon, France; Orange Gardens, Chátillon, France; Polaran Ltd, Ankara, Turkey; Nokia Bell Labs, Nozay, France; LUSAC, UNICAEN, Normandie University, Caen, France; Nokia Bell Labs, Nozay, France; MIC Nordic AB, Kista, Sweden; Departement Système Réseaux, Cybersécurité et Droit du numérique (SRCD), IMT Atlantique, CS 17607 35576, Cesson-Sévigné, France; Tele2, Sverige AB, Kista, Sweden; INESC TEC and Faculty of Engineering, University of Porto, Porto, Portugal; INESC TEC and Faculty of Engineering, University of Porto, Porto, Portugal; Institut Polytechnique Paris, Telecom SudParis, UMR CNRS Samovar, Evry, France; MIC Nordic AB, Kista, Sweden; KTH Royal Institute of Technology, Kista, Sweden",IEEE Access,"14 Aug 2019","2019","7","","107270","107299","The heated 5G network deployment race has already begun with the rapid progress in standardization efforts, backed by the current market availability of 5G-enabled network equipment, ongoing 5G spectrum auctions, early launching of non-standalone 5G network services in a few countries, among others. In this paper, we study current and future wireless networks from the viewpoint of energy efficiency (EE) and sustainability to meet the planned network and service evolution toward, along, and beyond 5G, as also inspired by the findings of the EU Celtic-Plus SooGREEN Project. We highlight the opportunities seized by the project efforts to enable and enrich this green nature of the network as compared to existing technologies. In specific, we present innovative means proposed in SooGREEN to monitor and evaluate EE in 5G networks and beyond. Further solutions are presented to reduce energy consumption and carbon footprint in the different network segments. The latter spans proposed virtualized/cloud architectures, efficient polar coding for fronthauling, mobile network powering via renewable energy and smart grid integration, passive cooling, smart sleeping modes in indoor systems, among others. Finally, we shed light on the open opportunities yet to be investigated and leveraged in future developments.","2169-3536","","10.1109/ACCESS.2019.2932777","EU Celtic Plus Projects, mainly by SooGREEN: Service-Oriented Optimization of Green Mobile Networks and in part by AI4Green: Artificial Intelligence for Green Mobile Networks; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8786138","CRAN;DAS;energy efficiency;monitoring;storage;green mobile networks;passive cooling;renewable energy;sleep modes;smart grid;virtualization;Wi-Fi","Energy consumption;5G mobile communication;Green products;Optimization;Sustainable development;Cooling;Quality of service","","84","","73","CCBY","5 Aug 2019","","","IEEE","IEEE Journals"
"Cloud-Native Network Slicing Using Software Defined Networking Based Multi-Access Edge Computing: A Survey","S. D. A. Shah; M. A. Gregory; S. Li","School of Engineering, RMIT University, Melbourne, VIC, Australia; School of Engineering, RMIT University, Melbourne, VIC, Australia; School of Engineering, RMIT University, Melbourne, VIC, Australia",IEEE Access,"20 Jan 2021","2021","9","","10903","10924","Fifth-Generation (5G) mobile cellular networks provide a promising platform for new, innovative and diverse IoT applications, such as ultra-reliable and low latency communication, real-time and dynamic data processing, intensive computation, and massive device connectivity. End-to-End (E2E) network slicing candidates present a promising approach to resource allocation and distribution that permit operators to flexibly provide scalable virtualized and dedicated logical networks over common physical infrastructure. Though network slicing promises the provision of services on demand, many of its use cases, such as self-driving cars and Google's Stadia, would require the integration of a Multi-Access Edge Computing (MEC) platform in 5G networks. Edge Computing is envisioned as one of the key drivers for 5G and Sixth-Generation (6G) mobile cellular networks, but its role in network slicing remains to be fully explored. We investigate MEC and network slicing for the provision of 5G service focused use cases. Recently, changes to the cloud-native 5G core are a focus with MEC use cases providing network scalability, elasticity, flexibility, and automation. A cloud-native microservices architecture, along with its potential use cases for 5G network slicing, is envisioned. This paper also elaborates on the recent advances made in enabling E2E network slicing, its enabling technologies, solutions, and current standardization efforts. Finally, this paper identifies open research issues and challenges and provides possible solutions and recommendations.","2169-3536","","10.1109/ACCESS.2021.3050155","Royal Melbourne Institute of Technology (RMIT) University Research Stipend Scholarship (RRSS); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9317860","Network slicing;software defined networking;multi-access edge computing;cloud native;ultra-reliable;low latency communication","5G mobile communication;Network slicing;Cloud computing;Computer architecture;Edge computing;Software;Servers","","83","","102","CCBY","8 Jan 2021","","","IEEE","IEEE Journals"
"Towards 6G-Enabled Internet of Vehicles: Security and Privacy","D. P. Moya Osorio; I. Ahmad; J. D. V. Sánchez; A. Gurtov; J. Scholliers; M. Kutila; P. Porambage","Centre for Wireless Communications, University of Oulu, Oulu, Finland; VTT Technical Research Centre of Finland Ltd., Espoo, Finland; Departamento de Electrónica, Telecomunicaciones y Redes de Información, Escuela Politécnica Nacional, Quito, Ecuador; Department of Computer and Information Science (IDA), Linköping University, Linköping, Sweden; VTT Technical Research Centre of Finland Ltd., Espoo, Finland; VTT Technical Research Centre of Finland Ltd., Espoo, Finland; Centre for Wireless Communications, University of Oulu, Oulu, Finland",IEEE Open Journal of the Communications Society,"3 Feb 2022","2022","3","","82","105","The conceptualisation of the sixth generation of mobile wireless networks (6G) has already started with some potential disruptive technologies resonating as enablers for driving the emergence of a number of innovative applications. Particularly, 6G will be a prominent supporter for the evolution towards a truly Intelligent Transportation System and the realization of the Smart City concept by fulfilling the limitations of 5G, once vehicular networks are becoming highly dynamic and complex with stringent requirements on ultra-low latency, high reliability, and massive connections. More importantly, providing security and privacy to such critical systems should be a top priority as vulnerabilities can be catastrophic, thus there are huge concerns regarding data collected from sensors, people and their habits. In this paper, we provide a timely deliberation of the role that promissory 6G enabling technologies such as artificial intelligence, network softwarisation, network slicing, blockchain, edge computing, intelligent reflecting surfaces, backscatter communications, terahertz links, visible light communications, physical layer authentication, and cell-free massive multiple-input multiple-output (MIMO) will play on providing the expected level of security and privacy for the Internet of Vehicles.","2644-125X","","10.1109/OJCOMS.2022.3143098","Academy of Finland 6G Flagship(grant numbers:318927,334280); 5G-SAFEplus Celtic+ Project; Business Finland; EU Project AirMOUR (Enabling Sustainable Airmobility in Urrban Contexts via Emergency and Medical Services)(grant numbers:101006601); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9681822","6G networks;Internet of Vehicles;privacy;security;vehicle-to-everything communications","Security;6G mobile communication;5G mobile communication;Privacy;Vehicle-to-everything;Long Term Evolution;Data privacy","","82","","153","CCBY","14 Jan 2022","","","IEEE","IEEE Journals"
"Utilizing Blockchain to Overcome Cyber Security Concerns in the Internet of Things: A Review","B. Alotaibi","Department of Information Technology, University of Tabuk, Tabuk, Saudi Arabia",IEEE Sensors Journal,"14 Nov 2019","2019","19","23","10953","10971","The Internet of Things (IoT) is a wide network consisting of Internet-connected objects using installed software, such as home appliances, vehicles, and other entities embedded with sensors, actuators, radio-frequency identification (RFID), and electronics to exchange data. In the last two decades, numerous IoT solutions have been developed by small, medium-sized, and large enterprises to make our lives easier. Furthermore, private and academic researchers have extensively investigated some practical IoT solutions. The rapid expansion of IoT solutions accompanies numerous security concerns because the underlying IoT protocols and communication technologies have not considered security. Recently, blockchain has emerged to become one of the promising technologies that might overcome some of the IoT limitations (security limitations, in particular). Blockchain technology is a database ledger that uses a peer-to-peer (P2P) network and stores transactions and asset registries. Blockchain can be described as a mounting list of records (i.e., blocks) with the following properties: distributed, decentralized, immutable, and shared. This paper surveyed recent security advances to overcome IoT limitations using blockchain. In this article, the blockchain attempts to overcome IoT limitations that are related to cyber security have been classified into four categories: end-to-end traceability; data privacy and anonymity; identity verification and authentication; and confidentiality, data integrity, and availability (CIA). Intended as a guideline for future research, this paper also explores systematic processes.","1558-1748","","10.1109/JSEN.2019.2935035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8795541","Internet of Things;blockchain;cyber security;end-to-end traceability;data privacy;anonymity;identity verification;data integrity;big data;cloud computing","Internet of Things;Blockchain;Protocols;Radiofrequency identification;Computer crime","","79","","141","IEEE","13 Aug 2019","","","IEEE","IEEE Journals"
"Blockchain for Industry 5.0: Vision, Opportunities, Key Enablers, and Future Directions","A. Verma; P. Bhattacharya; N. Madhani; C. Trivedi; B. Bhushan; S. Tanwar; G. Sharma; P. N. Bokoro; R. Sharma","Department of Computer Science and Engineering, Institute of Technology, Nirma University, Gujarat, Ahmedabad, India; Department of Computer Science and Engineering, Institute of Technology, Nirma University, Gujarat, Ahmedabad, India; Department of Computer Science and Engineering, Institute of Technology, Nirma University, Gujarat, Ahmedabad, India; Department of Computer Science and Engineering, Institute of Technology, Nirma University, Gujarat, Ahmedabad, India; School of Engineering and Technology, Sharda University, Greater Noida, India; Department of Computer Science and Engineering, Institute of Technology, Nirma University, Gujarat, Ahmedabad, India; Department of Electrical Engineering Technology, University of Johannesburg, Johannesburg, South Africa; Department of Electrical Engineering Technology, University of Johannesburg, Johannesburg, South Africa; Centre for Inter-Disciplinary Research and Innovation, University of Petroleum and Energy Studies, Dehradun, India",IEEE Access,"13 Jul 2022","2022","10","","69160","69199","Industry 4.0 have witnessed a paradigm shift from cyber-physical systems (CPS) that aims at massive automation, to a more customer-driven approach. The shift has been attributed to the design of hyper-cognitive systems, integration of virtual and extended reality, digital machinery prototyping and twin designs, trusted machine boundaries, collaborative robots, and artificial intelligence (AI)-based supply chains. This new wave, termed Industry 5.0, is expected to leverage massive production with user-centric customization outside the scope of Industry 4.0 ecosystems. Industry 5.0 is expected to assist diverse industrial verticals like healthcare, smart farming, drones, smart grids, and supply chain production ecosystems. However, data is shared among multiple heterogeneous networks, spanning different authoritative domains. Thus, trusted and secured data transfer is crucial to synergize and secure the industrial perimeters. Blockchain (BC) is a preferred choice as a security enabler to Industry 5.0 ecosystems owing to its inherent property of immutability, chronology, and auditability in industrial systems. Limited works are proposed that present the vision and holistic view of BC-assisted Industry 5.0 applications. The article presents a first-of-its-kind survey on BC as a security enabler in Industry 5.0. Based on a descriptive survey methodology and research questions, we presented the key drivers, and potential applications, and propose an architectural vision of BC-based Industry 5.0 in diverse applicative verticals. The survey intends to present solutions that would assist industry practitioners, academicians, and researchers to drive novel BC-assisted solutions in Industry 5.0 verticals.","2169-3536","","10.1109/ACCESS.2022.3186892","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9809962","Blockchain;Industry 5.0;Internet-of-Things;security;privacy","Industries;Production;Fourth Industrial Revolution;Security;Service robots;Artificial intelligence;Blockchains","","77","","168","CCBY","28 Jun 2022","","","IEEE","IEEE Journals"
"Distributed Redundant Placement for Microservice-based Applications at the Edge","H. Zhao; S. Deng; Z. Liu; J. Yin; S. Dustdar","College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Distributed Systems Group, Technische Universität Wien, Vienna, Austria",IEEE Transactions on Services Computing,"17 Jun 2022","2022","15","3","1732","1745","Multi-access edge computing (MEC) is booming as a promising paradigm to push the computation and communication resources from cloud to the network edge to provide services and to perform computations. With container technologies, mobile devices with small memory footprint can run composite microservice-based applications without time-consuming backbone. Service placement at the edge is of importance to put MEC from theory into practice. However, current state-of-the-art research does not sufficiently take the composite property of services into consideration. Besides, although Kubernetes has certain abilities to heal container failures, high availability cannot be ensured due to heterogeneity and variability of edge sites. To deal with these problems, we propose a distributed redundant placement framework SAA-RP and a GA-based Server Selection (GASS) algorithm for microservice-based applications with sequential combinatorial structure. We formulate a stochastic optimization problem with the uncertainty of microservice request considered, and then decide for each microservice, how it should be deployed and with how many instances as well as on which edge sites to place them. Benchmark policies are implemented in two scenarios, where redundancy is allowed and not, respectively. Numerical results based on a real-world dataset verify that GASS significantly outperforms all the benchmark policies.","1939-1374","","10.1109/TSC.2020.3013600","National Key Research and Development Program of China(grant numbers:2017YFB1400601); Key Research and Development Project of Zhejiang Province(grant numbers:2017C01015); National Natural Science Foundation of China(grant numbers:61772461); Natural Science Foundation of Zhejiang Province(grant numbers:LR18F020003,LY17F020014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9154603","Redundancy;service placement;multi-access edge computing;composite service;sample average approximation","Redundancy;Containers;Mobile handsets;Cloud computing;Numerical models;Stochastic processes;Edge computing","","75","","40","IEEE","3 Aug 2020","","","IEEE","IEEE Journals"
"Big Data Driven Edge-Cloud Collaboration Architecture for Cloud Manufacturing: A Software Defined Perspective","C. Yang; S. Lan; L. Wang; W. Shen; G. G. Q. Huang","School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Economics and Management, University of Chinese Academy of Science, Beijing, China; Department of Production Engineering, KTH Royal Institute of Technology, Stockholm, Sweden; State Key Lab of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Industrial and Manufacturing Systems Engineering, The University of Hong Kong, Hong Kong",IEEE Access,"12 Mar 2020","2020","8","","45938","45950","In the practice of cloud manufacturing, there still exist some major challenges, including: 1) cloud based big data analytics and decision-making cannot meet the requirements of many latency-sensitive applications on shop floors; 2) existing manufacturing systems lack enough reconfigurability, openness and evolvability to deal with shop-floor disturbances and market changes; and 3) big data from shop-floors and the Internet has not been effectively utilized to guide the optimization and upgrade of manufacturing systems. This paper proposes an open evolutionary architecture of the intelligent cloud manufacturing system with collaborative edge and cloud processing. Hierarchical gateways connecting and managing shop-floor things at the “edge” side are introduced to support latency-sensitive applications for real-time responses. Big data processed both at the gateways and in the cloud will be used to guide continuous improvement and evolution of edge-cloud systems for better performance. As software tools are becoming dominant as the “brain” of manufacturing control and decision-making, this paper also proposes a new mode - “AI-Mfg-Ops” (AI enabled Manufacturing Operations) with a supporting software defined framework, which can promote fast operation and upgrading of cloud manufacturing systems with smart monitoring-analysis-planning-execution in a closed loop. This research can contribute to the rapid response and efficient operation of cloud manufacturing systems.","2169-3536","","10.1109/ACCESS.2020.2977846","Beijing Institute of Technology Research Fund Program for Young Scholars; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9020166","Cloud manufacturing;big data;edge-cloud collaboration;software-defined architecture;Internet of Things","Cloud computing;Big Data;Computer architecture;Collaboration;Real-time systems;Manufacturing systems","","73","","36","CCBY","2 Mar 2020","","","IEEE","IEEE Journals"
"Emerging Trends, Techniques and Open Issues of Containerization: A Review","J. Watada; A. Roy; R. Kadikar; H. Pham; B. Xu","Research Institute of Quantitative Economics, Zhejiang Gongshang University, Hangzhou, China; Department of CIS, Universiti Technologi Petronas, Seri Iskandar, Malaysia; InfraCloud Technologies Pvt. Ltd., Pune, India; Department of ISE, State University of New Jersey, Piscataway, NJ, USA; Research Institute of Quantitative Economics, Zhejiang Gongshang University, Hangzhou, China",IEEE Access,"28 Oct 2019","2019","7","","152443","152472","Containerization is revolutionizing the way that many industries operate, provisioning major impact to modern computing technologies because it is extra lightweight, highly portable, energy, resource and storage efficient, cost-effective, performance efficient, and extremely quick during boot up. These often facilitate efficient load balancing, low-level system maintenance, server consolidation (for efficient energy and resource utilization) and replication of instances over geographical locations for better fault tolerance to escalate application reliability. However, some recent literature have addressed various challenges (such as complex networking, persistent storage facilities, cross data centers and multicloud supports, security issues, and lack of available, capable container management APIs, etc.) regarding successful container adoption in industries, which might have resulted in a seemingly meager increase in industrial deployments of containerization over the past few years despite bestowing efficient lightweight virtualization. Moreover, a comprehensive overview of containerizations along with their popularity dynamics has still not been found in contemporary literature, which further extends knowledge gap between developers and available technologies. Hence, current study touches upon different technicalities involved in containerization with potential problems and possible solutions along with various important industrial applications to manifest its existing supports and technical hardships. Finally, we have conducted a comprehensive experimental study to compare the performance of VMs, containers and unikernels in terms of CPU utilization, memory footprints, network bandwidth, execution time and technological maturity using standard benchmarks and observed containers to deliver satisfactory performance in almost all aspects, however, are still not free from issues regarding isolation & security, performance stability, lack of available efficient tools for crossplatform support and persistent storage. Unikernels deliver good performance with VM-like isolation but still need to achieve desired technical maturity (in terms of microprocessor stability, process containment, persistent storage, etc.). VMs, on the other hand, are found to provide stable performance throughout, though bigger memory footprints and slower spin up/down remain their biggest weaknesses.","2169-3536","","10.1109/ACCESS.2019.2945930","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8861307","Virtualization;containerization;management and orchestration;isolation and security","Containers;Virtualization;Security;Cloud computing;Virtual machine monitors;Linux;Tools","","72","","138","CCBY","7 Oct 2019","","","IEEE","IEEE Journals"
"Blockchain Meets Metaverse and Digital Asset Management: A Comprehensive Survey","V. T. Truong; L. Le; D. Niyato","Institut National De La Recherche Scientifique (INRS), University of Québec, Montréal, QC, Canada; Institut National De La Recherche Scientifique (INRS), University of Québec, Montréal, QC, Canada; School of Computer Science and Engineering (SCSE), Nanyang Technological University, Jurong West, Singapore",IEEE Access,"29 Mar 2023","2023","11","","26258","26288","Envisioned to be the next-generation Internet, the metaverse has been attracting enormous attention from both the academia and industry. The metaverse can be viewed as a 3D immersive virtual world, where people use Augmented/Virtual Reality (AR/VR) devices to access and interact with others through digital avatars. While early versions of the metaverse exist in several Massively Multiplayer Online (MMO) games, the full-flesh metaverse is expected to be more complex and enabled by various advanced technologies. Blockchain is one of the crucial technologies that could revolutionize the metaverse to become a decentralized and democratic virtual society with its own economic and governance system. Realizing the importance of blockchain for the metaverse, our goal in this paper is to provide a comprehensive survey that clarifies the role of blockchain in the metaverse including in-depth analysis of digital asset management. To this end, we discuss how blockchain can enable the metaverse from different perspectives, ranging from user applications to virtual services and the blockchain-enabled economic system. Furthermore, we describe how blockchain can shape the metaverse from the system perspective, including various solutions for the decentralized governance system and data management. The potential of blockchain for security and privacy aspects of the metaverse infrastructure is also figured out, while a full flow of blockchain-based digital asset management for the metaverse is investigated. Finally, we discuss a wide range of open challenges of the blockchain-empowered metaverse.","2169-3536","","10.1109/ACCESS.2023.3257029","National Research Foundation (NRF), Singapore, and Infocomm Media Development Authority under the Future Communications Research Development Programme (FCP)(grant numbers:FCP-NTU-RG-2022-010); DSO National Laboratories under the AI Singapore Programme(grant numbers:AISG2-RP-2020-019); Energy Research Test-Bed and Industry Partnership Funding Initiative; Energy Grid (EG) 2.0 programme, and under DesCartes and the Campus for Research Excellence and Technological Enterprise (CREATE) programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10068493","Metaverse;blockchain;artificial intelligence;digital asset management;the Internet of Things;digital twin;VR/AR","Metaverse;Blockchains;Asset management;Security;Games;Decentralized applications;Artificial intelligence","","71","","171","CCBYNCND","14 Mar 2023","","","IEEE","IEEE Journals"
"MOERA: Mobility-Agnostic Online Resource Allocation for Edge Computing","L. Wang; L. Jiao; J. Li; J. Gedeon; M. Mühlhäuser","Telecooperation Lab, Technische Universität Darmstadt, Darmstadt, Germany; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA; Telecooperation Lab, Technische Universität Darmstadt, Darmstadt, Germany; Telecooperation Lab, Technische Universität Darmstadt, Darmstadt, Germany",IEEE Transactions on Mobile Computing,"1 Jul 2019","2019","18","8","1843","1856","To better support emerging interactive mobile applications such as those VR-/AR-based, cloud computing is quickly evolving into a new computing paradigm called edge computing. Edge computing has the promise of bringing cloud resources to the network edge to augment the capability of mobile devices in close proximity to the user. One big challenge in edge computing is the efficient allocation and adaptation of edge resources in the presence of high dynamics imposed by user mobility. This paper provides a formal study of this problem. By characterizing a variety of static and dynamic performance measures with a comprehensive cost model, we formulate the online edge resource allocation problem with a mixed nonlinear optimization problem. We propose MOERA, a mobility-agnostic online algorithm based on the “regularization” technique, which can be used to decompose the problem into separate subproblems with regularized objective functions and solve them using convex programming. Through rigorous analysis we are able to prove that MOERA can guarantee a parameterized competitive ratio, without requiring any a priori knowledge on input. We carry out extensive experiments with various real-world data and show that MOERA can achieve an empirical competitive ratio of less than 1.2, reduces the total cost by $4 \times$4× compared to static approaches, and outperforms the online greedy one-shot solution by 70 percent. Moreover, we verify that even being future-agnostic, MOERA can achieve comparable performance to approaches with perfect partial future knowledge. We also discuss practical issues with respect to the implementation of our algorithm in real edge computing systems.","1558-0660","","10.1109/TMC.2018.2867520","Deutsche Forschungsgemeinschaft(grant numbers:392046569); National Natural Science Foundation of China(grant numbers:61761136014); DFG Collaborative Research Center; National Science Foundation(grant numbers:1564348); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449107","Edge computing;resource allocation;online optimization;competitive analysis","Cloud computing;Resource management;Edge computing;Delays;Mobile computing;Mobile applications;Dynamic scheduling","","71","","45","IEEE","28 Aug 2018","","","IEEE","IEEE Journals"
"Adaptive Resource Efficient Microservice Deployment in Cloud-Edge Continuum","K. Fu; W. Zhang; Q. Chen; D. Zeng; M. Guo","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Computer Science, China University of Geosciences, Wuhan, Hubei, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China",IEEE Transactions on Parallel and Distributed Systems,"8 Dec 2021","2022","33","8","1825","1840","User-facing services are now evolving towards the microservice architecture where a service is built by connecting multiple microservice stages. Since the entire service is heavy, the microservice architecture shows the opportunity to only offload some microservice stages to the edge devices that are close to the end users. However, emerging techniques often result in the violation of Quality-of-Service (QoS) of microservice-based services in cloud-edge continuum, as they do not consider the communication overhead or the resource contention between microservices and external co-located tasks. We propose Nautilus, a runtime system that effectively deploys microservice-based user-facing services in cloud-edge continuum. Nautilus ensures the QoS of microservice-based user-facing services while minimizing the required computational resources, which is comprised of a communication-aware microservice mapper, a contention-aware resource manager and an IO-sensitive and load-aware microservice migration scheduler. The mapper divides the microservice graph into multiple partitions based on the communication overhead and maps the partitions to appropriate nodes. On each node, the resource manager determines the optimal resource allocation for its microservices based on reinforcement learning that may capture the complex contention behaviors. Once the microservices are suffered from external IO pressure, the IO-sensitive microservice scheduler migrates the critical one to idle nodes. Furthermore, when the load of microservices changes dynamically, the load-aware microservice scheduler migrates microservices from busy nodes to idle ones to ensure the QoS goal of the entire service. Our experimental results show that Nautilus can guarantee the required QoS target under external shared resources contention while the state-of-the-art suffers from QoS violations. Meanwhile, Nautilus reduces the computational resource usage by 23.9% and the network bandwidth usage by 53.4%, while achieving the required 99%-ile latency.","1558-2183","","10.1109/TPDS.2021.3128037","National Key Research and Development Program of China(grant numbers:2018YFB1004800); National Natural Science Foundation of China(grant numbers:62022057,61832006,61632017,61872240); Open Research Projects of Zhejiang Lab(grant numbers:2021KE0AB02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9615028","Cloud-edge continuum;QoS;microservice resources management","Quality of service;Cloud computing;Task analysis;Resource management;Computer architecture;Runtime;Bandwidth","","69","","57","IEEE","15 Nov 2021","","","IEEE","IEEE Journals"
"Machine-Learning-Assisted Security and Privacy Provisioning for Edge Computing: A Survey","S. Singh; R. Sulthana; T. Shewale; V. Chamola; A. Benslimane; B. Sikdar","Department of Electrical and Electronics Engineering & APPCAIR, Birla Institute of Technology and Science-Pilani (Pilani Campus), Pilani, India; Department of Computer Science, Birla Institute of Technology and Science-Pilani (Dubai Campus), Dubai, UAE; Department of Electrical and Electronics Engineering & APPCAIR, Birla Institute of Technology and Science-Pilani (Pilani Campus), Pilani, India; Department of Electrical and Electronics Engineering & APPCAIR, Birla Institute of Technology and Science-Pilani (Pilani Campus), Pilani, India; Department of Computer Science, University of Avignon, Avignon, France; Department of Electrical and Computer Engineering, National University of Singapore, Singapore",IEEE Internet of Things Journal,"22 Dec 2021","2022","9","1","236","260","Edge computing (EC), is a technological game changer that has the ability to connect millions of sensors and provide services at the device end. The broad vision of EC integrates storage, processing, monitoring, and control of operations in the Edge of the network. Though EC provides end-to-end connectivity, speeds up operation, and reduces latency of data transfer, security is a major concern. The tremendous growth in the number of Edge Devices and the amount of sensitive information generated at the device and the cloud creates a broad surface of attack and therefore, the need to secure the static and mobile data is imperative. This article is a comprehensive survey that describes the security and privacy issues in various layers of the EC architecture that result from the networking of heterogeneous devices. Second, it discusses the wide range of machine learning and deep learning algorithms that are applied in EC use cases. Following this, this article broadly details the different types of attacks that the Edge network confronts, and the intrusion detection systems and the corresponding machine learning algorithms that overcome these security and privacy concerns. The details of machine learning and deep learning techniques for EC security are tabulated. Finally, the open issues in securing Edge networks and future research directions are provided.","2327-4662","","10.1109/JIOT.2021.3098051","ASEAN—India Collaborative Research and Development Scheme (ASEAN-India S&T Development Fund (AISTDF) sponsored)(grant numbers:CRD/2020/000369); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9490350","Attacks;deep learning;edge computing (EC);machine learning;privacy;security","Security;Edge computing;Cloud computing;Machine learning algorithms;Computer architecture;Servers;Protocols","","68","","172","IEEE","19 Jul 2021","","","IEEE","IEEE Journals"
"Decentralized SDN Control Plane for a Distributed Cloud-Edge Infrastructure: A Survey","D. Espinel Sarmiento; A. Lebre; L. Nussbaum; A. Chari","CISS/IIE/NAVI, Orange Labs, Lannion, France; STACK, IMT Atlantique Bretagne-Pays de la Loire (Campus de Nantes), Nantes, France; RESIST, Universite de Lorraine, Nancy, France; CISS/IIE/NAVI, Orange Labs, Lannion, France",IEEE Communications Surveys & Tutorials,"23 Feb 2021","2021","23","1","256","281","Today's emerging needs (Internet of Things applications, Network Function Virtualization services, Mobile Edge computing, etc.) are challenging the classic approach of deploying a few large data centers to provide cloud services. A massively distributed Cloud-Edge architecture could better fit these new trends' requirements and constraints by deploying on-demand infrastructure services in Point-of-Presences within backbone networks. In this context, a key feature is establishing connectivity among several resource managers in charge of operating, each one a subset of the infrastructure. After explaining the networking management challenges related to distributed Cloud-Edge infrastructures, this article surveys and analyzes the characteristics and limitations of existing technologies in the Software Defined Network field that could be used to provide the inter-site connectivity feature. We also introduce Kubernetes, the new de facto container orchestrator platform, and analyze its use in the proposed context. This survey is concluded by providing a discussion about some research directions in the field of SDN applied to distributed Cloud-Edge infrastructures' management.","1553-877X","","10.1109/COMST.2021.3050297","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9319748","IaaS;SDN;virtualization;networking;automation","Cloud computing;Neutrons;Computer architecture;Routing;Wide area networks;Tutorials;Quality of service","","68","","196","IEEE","11 Jan 2021","","","IEEE","IEEE Journals"
"An Ingestion and Analytics Architecture for IoT Applied to Smart City Use Cases","P. Ta-Shma; A. Akbar; G. Gerson-Golan; G. Hadash; F. Carrez; K. Moessner","IBM Research, Haifa, Israel; Institute for Communication Systems, University of Surrey, Guildford, U.K.; IBM Research, Haifa, Israel; IBM Research, Haifa, Israel; Institute for Communication Systems, University of Surrey, Guildford, U.K.; Institute for Communication Systems, University of Surrey, Guildford, U.K.",IEEE Internet of Things Journal,"10 Apr 2018","2018","5","2","765","774","As sensors are adopted in almost all fields of life, the Internet of Things (IoT) is triggering a massive influx of data. We need efficient and scalable methods to process this data to gain valuable insight and take timely action. Existing approaches which support both batch processing (suitable for analysis of large historical data sets) and event processing (suitable for realtime analysis) are complex. We propose the hut architecture, a simple but scalable architecture for ingesting and analyzing IoT data, which uses historical data analysis to provide context for real-time analysis. We implement our architecture using open source components optimized for Big Data applications and extend them, where needed. We demonstrate our solution on two real-world smart city use cases in transportation and energy management.","2327-4662","","10.1109/JIOT.2017.2722378","European Union’s FP7 Project COSMOS(grant numbers:609043); European Union’s Horizon 2020 Project CPaaS.io(grant numbers:723076); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7964673","Big data;complex event processing (CEP);context-aware;energy management;ingestion;Internet of Things (IoT);machine learning;smart cities;spark;transportation","Real-time systems;Computer architecture;Internet of Things;Transportation;Big Data;Batch production systems;Sparks","","67","","36","IEEE","30 Jun 2017","","","IEEE","IEEE Journals"
"Computing Paradigms in Emerging Vehicular Environments: A Review","L. Silva; N. Magaia; B. Sousa; A. Kobusińska; A. Casimiro; C. X. Mavromoustakis; G. Mastorakis; V. H. C. de Albuquerque","Department of Computer Science, LASIGE, Faculty of Sciences, University of Lisbon, Lisbon, Portugal; Department of Computer Science, LASIGE, Faculty of Sciences, University of Lisbon, Lisbon, Portugal; Department of Computer Science, LASIGE, Faculty of Sciences, University of Lisbon, Lisbon, Portugal; Laboratory of Computing Science, Poznań University of Technology, Poznan, Poland; Department of Computer Science, LASIGE, Faculty of Sciences, University of Lisbon, Lisbon, Portugal; Department of Computer Science, University of Nicosia, Nicosia, Cyprus; Department of Management Science and Technology, Hellenic Mediterranean University, Agios Nikolaos, Greece; Science and Technology of Ceará, LAPISCO, Federal Institute of Education, Fortaleza, Brazil",IEEE/CAA Journal of Automatica Sinica,"3 Feb 2021","2021","8","3","491","511","Determining how to structure vehicular network environments can be done in various ways. Here, we highlight vehicle networks' evolution from vehicular ad-hoc networks (VANET) to the internet of vehicles (IoVs), listing their benefits and limitations. We also highlight the reasons in adopting wireless technologies, in particular, IEEE 802.11p and 5G vehicle-to-everything, as well as the use of paradigms able to store and analyze a vast amount of data to produce intelligence and their applications in vehicular environments. We also correlate the use of each of these paradigms with the desire to meet existing intelligent transportation systems' requirements. The presentation of each paradigm is given from a historical and logical standpoint. In particular, vehicular fog computing improves on the deficiences of vehicular cloud computing, so both are not exclusive from the application point of view. We also emphasize some security issues that are linked to the characteristics of these paradigms and vehicular networks, showing that they complement each other and share problems and limitations. As these networks still have many opportunities to grow in both concept and application, we finally discuss concepts and technologies that we believe are beneficial. Throughout this work, we emphasize the crucial role of these concepts for the well-being of humanity.","2329-9274","","10.1109/JAS.2021.1003862","Brazilian National Council for Research and Development (CNPq)(grant numbers:304315/2017-6,430274/2018-1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9346073","Computing paradigm;cloud;edge;fog;internet of vehicle (IoV);vehicular networks","Wireless communication;5G mobile communication;Urban areas;Vehicular ad hoc networks;Security;Wireless fidelity;Vehicle-to-everything","","66","","164","","3 Feb 2021","","","IEEE","IEEE Journals"
"Offloading Using Traditional Optimization and Machine Learning in Federated Cloud–Edge–Fog Systems: A Survey","B. Kar; W. Yahya; Y. -D. Lin; A. Ali","Department of Computer Science and Information Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Electrical Engineering and Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Department of Computer Science, National Yang Ming Chiao Tung University, Hsnchu, Taiwan; Department of Electrical Engineering and Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan",IEEE Communications Surveys & Tutorials,"22 May 2023","2023","25","2","1199","1226","The huge amount of data generated by the Internet of Things (IoT) devices needs the computational power and storage capacity provided by cloud, edge, and fog computing paradigms. Each of these computing paradigms has its own pros and cons. Cloud computing provides enhanced data storage and computing power but causes high communication latency. Edge and fog computing provide similar services with lower latency but limited capacity, capability, and coverage. A single computing paradigm cannot fulfill all the requirements of IoT devices and a federation between them is needed to extend their capacity, capability, and services. This federation is beneficial to both subscribers and providers and also reveals research issues in traffic offloading between clouds, edges, and fogs. Optimization has traditionally been used to solve the problem of traffic offloading. However, in such a complex federated system, traditional optimization cannot keep up with the strict latency requirements of decision-making, ranging from milliseconds to sub-seconds. Machine learning approaches, especially reinforcement learning, are consequently becoming popular because they could quickly solve offloading problems in dynamic environments with some unknown information. This study provides a novel federal classification between cloud, edge, and fog and presents a comprehensive research roadmap on offloading for different federated scenarios. We survey the relevant literature on the various optimization approaches used to solve this offloading problem and compare their salient features. We then provide a comprehensive survey on offloading in federated systems with machine learning approaches and the lessons learned as a result of these surveys. Finally, we outline several directions for future research and challenges that have to be faced in order to achieve such a federation.","1553-877X","","10.1109/COMST.2023.3239579","Ministry of Science and Technology (MOST), Taiwan(grant numbers:109-2221-E-011-104-MY3); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10025811","Offloading;cloud computing;edge computing;fog computing;federation;optimization;machine learning","Cloud computing;Optimization;Edge computing;Internet of Things;Task analysis;Wireless fidelity;Surveys","","63","","152","IEEE","25 Jan 2023","","","IEEE","IEEE Journals"
"Communication-Aware Container Placement and Reassignment in Large-Scale Internet Data Centers","L. Lv; Y. Zhang; Y. Li; K. Xu; D. Wang; W. Wang; M. Li; X. Cao; Q. Liang","Department of Computer Science and Technology, Tsinghua University, Beijing, China; School of Software Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Department of Computer Science, Nankai University, Tianjin, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Baidu, Beijing, China; Baidu, Beijing, China; Baidu, Beijing, China",IEEE Journal on Selected Areas in Communications,"14 Feb 2019","2019","37","3","540","555","Containerization has been used in many applications for isolation purposes due to its lightweight, scalable, and highly portable properties. However, to apply containerization in large-scale Internet data centers faces a big challenge. Services in data centers are always instantiated as a group of containers, which often generate heavy communication workloads and therefore resulting in inefficient communications and downgraded service performance. Although assigning the containers of the same service to the same server can reduce the communication overhead, this may cause heavily imbalanced resource utilization since containers of the same service are usually intensive to the same resource. To reduce communication cost as well as balance the resource utilization in large-scale data centers, we further explore the container distribution issues in a real industrial environment and find that such conflict lies in two phases-container placement and container reassignment. The objective of this paper is to address the container distribution problem in these two phases. For the container placement problem, we propose an efficient communication aware worst fit decreasing algorithm to place a set of new containers into data centers. For the container reassignment problem, we propose a two-stage algorithm called Sweep&Search to optimize a given initial distribution of containers by migrating containers among servers. We implement the proposed algorithms in Baidu's data centers and conduct extensive evaluations. Compared with the state-of-the-art strategies, the evaluation results show that our algorithms perform better up to 70% and increase the overall service throughput up to 90% simultaneously.","1558-0008","","10.1109/JSAC.2019.2895473","National Key Research and Development Program of China(grant numbers:2018YFB0803405); China Postdoctoral Science Foundation(grant numbers:2018M630117); National Natural Science Foundation of China(grant numbers:61802024); Huawei Autonomous and Service 2.0 Project(grant numbers:A2018185); Baidu Songguo Plan; NSF of China(grant numbers:61602266); NSF of Tianjin(grant numbers:16JCYBJC41900); National Key Research and Development Program of China(grant numbers:2018YFB0803405); China National Funds for Distinguished Young Scientists(grant numbers:61825204); Beijing Outstanding Young Scientist Project; PolyU G-YBAG; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8631120","Container communication;multi-resource load balance;large-scale data centers;container placement;container reassignment","Containers;Servers;Resource management;Data centers;Internet;Throughput;Time factors","","63","","88","IEEE","31 Jan 2019","","","IEEE","IEEE Journals"
"IoT Microservice Deployment in Edge-Cloud Hybrid Environment Using Reinforcement Learning","L. Chen; Y. Xu; Z. Lu; J. Wu; K. Gai; P. C. K. Hung; M. Qiu","School of Computer Science and the Engineering Research Center of Cyber Security Auditing and Monitoring, Ministry of Education, Fudan University, Shanghai, China; School of Computer Science and the Engineering Research Center of Cyber Security Auditing and Monitoring, Ministry of Education, Fudan University, Shanghai, China; School of Computer Science and the Shanghai Blockchain Engineering Research Center, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Cyberspace Security, Beijing Institute of Technology, Beijing, China; Faulty of Business and Information Technology, Ontario Tech University, Oshawa, Canada; Department of Computer Science, Texas A&M University--Commerce, Commerce, TX, USA",IEEE Internet of Things Journal,"5 Aug 2021","2021","8","16","12610","12622","The edge-cloud hybrid environment requires complex deployment strategies to enable the smart Internet-of-Things (IoT) system. However, current service deployment strategies use simple, generalized heuristics and ignore the heterogeneous characteristics in the edge-cloud hybrid environment. In this article, we devise a method to find a microservice-based service deployment strategy that can reduce the average waiting time of IoT devices in the hybrid environment. For this purpose, we first propose a microservice-based deployment problem (MSDP) based on the heterogeneous and dynamic characteristics in the edge-cloud hybrid environment, including heterogeneity of edge server capacities, dynamic geographical information of IoT devices, and changing device preference for applications and complex application structures. We then propose a multiple buffer deep deterministic policy gradient (MB_DDPG) to provide more preferable service deployment solutions. Our algorithm leverages reinforcement learning and neural network to learn a deployment strategy without any human instruction. Therefore, the service provider can make full use of limited resources to improve the Quality of Service (QoS). Finally, we implement MB_DDPG based on real-world data sets and some synthetic data, and we also implement another two algorithms, genetic algorithm and random algorithm, as a contrast. The experimental results demonstrate that MB_DDPG is able to learn a preferable strategy which, in terms of average waiting time, outperforms genetic algorithm and the random algorithm by 32% and 44%, respectively.","2327-4662","","10.1109/JIOT.2020.3014970","National Key Research and Development Program of China(grant numbers:2019YFB1405000); National Natural Science Foundation of China(grant numbers:61873309,61972034,61572137,61728202); Shanghai Innovation Action Plan Project(grant numbers:19510710500,18510760200,18510732000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9162056","Edge-cloud hybrid environment;microservice deployment;reinforcement learning;smart Internet-of-Things (IoT) system","Servers;Cloud computing;Heuristic algorithms;Internet of Things;Edge computing;Quality of service;Image edge detection","","60","","45","IEEE","7 Aug 2020","","","IEEE","IEEE Journals"
"Service Coverage for Satellite Edge Computing","Q. Li; S. Wang; X. Ma; Q. Sun; H. Wang; S. Cao; F. Yang","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Key Laboratory of Space Utilization, Technology and Engineering Center for Space Utilization, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Space Utilization, Technology and Engineering Center for Space Utilization, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China",IEEE Internet of Things Journal,"22 Dec 2021","2022","9","1","695","705","Recently, increasing investments in satellite-related technologies make the low earth orbit (LEO) satellite constellation a strong complement to terrestrial networks. To mitigate the limitations of the traditional satellite constellation “bent-pipe” architecture, satellite edge computing (SEC) has been proposed by placing computing resources at the LEO satellite constellation. Most existing works focus on space-air-ground integrated network architecture and SEC computing framework. Beyond these works, we are the first to investigate how to efficiently deploy services on the SEC nodes to realize robustness aware service coverage with constrained resources. Facing the challenges of spatial-temporal system dynamics and service coverage-robustness conflict, we propose a novel online service placement algorithm with a theoretical performance guarantee by leveraging Lyapunov optimization and Gibbs sampling. Extensive simulation results show that our algorithm can improve the service coverage by  $4.3\times $  compared with the baseline.","2327-4662","","10.1109/JIOT.2021.3085129","Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B010164002); NSFC(grant numbers:62032003,61922017); Open Research Fund of Key Laboratory of Space Utilization, Chinese Academy of Sciences(grant numbers:LSUKFJJ-2019-03); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9444334","Satellite edge computing (SEC);service coverage;space-air-ground integrated networks","Satellites;Robustness;Low earth orbit satellites;Edge computing;Orbits;Internet of Things;Computer architecture","","60","","51","IEEE","31 May 2021","","","IEEE","IEEE Journals"
"Internet of Things: Device Capabilities, Architectures, Protocols, and Smart Applications in Healthcare Domain","M. M. Islam; S. Nooruddin; F. Karray; G. Muhammad","Department of Electrical and Computer Engineering, Centre for Pattern Analysis and Machine Intelligence, University of Waterloo, Waterloo, Canada; Department of Electrical and Computer Engineering, Centre for Pattern Analysis and Machine Intelligence, University of Waterloo, Waterloo, Canada; Department of Electrical and Computer Engineering, Centre for Pattern Analysis and Machine Intelligence, University of Waterloo, Waterloo, Canada; Department of Computer Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia",IEEE Internet of Things Journal,"6 Feb 2023","2023","10","4","3611","3641","Nowadays, the Internet has spread to practically every country around the world and is having unprecedented effects on people’s lives. The Internet of Things (IoT) is getting more popular and has a high level of interest in both practitioners and academicians in the age of wireless communication due to its diverse applications. The IoT is a technology that enables everyday things to become savvier, everyday computation toward becoming intellectual, and everyday communication to become a little more insightful. In this article, the most common and popular IoT device capabilities, architectures, and protocols are demonstrated in brief to provide a clear overview of the IoT technology to the researchers in this area. The common IoT device capabilities, including hardware (Raspberry Pi, Arduino, and ESP8266) and software (operating systems (OSs), and built-in tools) platforms are described in detail. The widely used architectures that have recently evolved and used are the three-layer architecture, service-oriented architecture, and middleware-based architecture. The popular protocols for IoT are demonstrated which include constrained application protocol, message queue telemetry transport, extensible messaging and presence protocol, advanced message queuing protocol, data distribution service, low power wireless personal area network, Bluetooth low energy, and ZigBee that are frequently utilized to develop smart IoT applications. Additionally, this research provides an in-depth overview of the potential healthcare applications based on IoT technologies in the context of addressing various healthcare concerns. Finally, this article summarizes state-of-the-art knowledge, highlights open issues and shortcomings, and provides recommendations for further studies which would be quite beneficial to anyone with a desire to work in this field and make breakthroughs to get expertise in this area.","2327-4662","","10.1109/JIOT.2022.3228795","Natural Sciences and Engineering Research Council (NSERC) Discovery Grant, Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9983826","Communication protocol;device capabilities;healthcare applications;Internet of Things (IoT);IoT architecture","Internet of Things;Medical services;Computer architecture;Protocols;Hardware;Software;Security","","59","","250","IEEE","13 Dec 2022","","","IEEE","IEEE Journals"
"Towards Low-Latency Service Delivery in a Continuum of Virtual Resources: State-of-the-Art and Research Directions","J. Santos; T. Wauters; B. Volckaert; F. De Turck","Department of Information Technology, imec, IDLab, Ghent University, Gent, Belgium; Department of Information Technology, imec, IDLab, Ghent University, Gent, Belgium; Department of Information Technology, imec, IDLab, Ghent University, Gent, Belgium; Department of Information Technology, imec, IDLab, Ghent University, Gent, Belgium",IEEE Communications Surveys & Tutorials,"19 Nov 2021","2021","23","4","2557","2589","The advent of softwarized networks has enabled the deployment of chains of virtual network and service components on computational resources from the cloud up to the edge, creating a continuum of virtual resources. The next generation of low latency applications (e.g., Virtual Reality (VR), autonomous cars) adds even more stringent requirements to the infrastructure, calling for considerable advancements towards cloud-native micro-service-based architectures. This article presents a comprehensive survey on ongoing research aiming to effectively support low latency services throughout their execution lifetime in next-generation networks. The current state-of-the-art is critically reviewed to identify the most promising trends that will strongly impact the full applicability and high performance of low latency services. This article proposes a taxonomy as well as specific evaluation criteria to classify research across different domains addressing low latency service delivery. Current architectural paradigms such as Multi-access Edge Computing (MEC) and Fog Computing (FC) alongside novel trends on communication networks are discussed. Among these, the integration of Machine Learning (ML) and Artificial intelligence (AI) is introduced as a key research field in current literature towards autonomous network management. A discussion on open challenges and future research directions on low-latency service delivery leads to the conclusion, offering lessons learned and prospects on emerging use cases such as Extended Reality (XR), in which novel trends will play a major role.","1553-877X","","10.1109/COMST.2021.3095358","Scientific Research-Flanders (FWO-V) through the Project “Intelligent Dense and Longe Range IoT Networks (IDEAL-IoT)”(grant numbers:S004017N); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9476028","Low latency;next-generation networks;cloud-native;autonomous networks;orchestration;5G/6G","Tutorials;Next generation networking;Taxonomy;Market research;5G mobile communication;Computer architecture;Cloud computing","","57","","189","IEEE","7 Jul 2021","","","IEEE","IEEE Journals"
"An IoT-Aware Approach for Elderly-Friendly Cities","R. Mulero; A. Almeida; G. Azkune; P. Abril-Jiménez; M. T. Arredondo Waldmeyer; M. Páramo Castrillo; L. Patrono; P. Rametta; I. Sergi","DeustoTech–Deusto Institute of Technology, University of Deusto, Bilbao, Spain; DeustoTech–Deusto Institute of Technology, University of Deusto, Bilbao, Spain; DeustoTech–Deusto Institute of Technology, University of Deusto, Bilbao, Spain; Universidad Politécnica de Madrid, Madrid, Spain; Universidad Politécnica de Madrid, Madrid, Spain; Universidad Politécnica de Madrid, Madrid, Spain; University of Salento, Lecce, Italy; University of Salento, Lecce, Italy; University of Salento, Lecce, Italy",IEEE Access,"7 Mar 2018","2018","6","","7941","7957","The ever-growing life expectancy of people requires the adoption of proper solutions for addressing the particular needs of elderly people in a sustainable way, both from service provision and economic point of view. Mild cognitive impairments and frailty are typical examples of elderly conditions which, if not timely addressed, can turn out into more complex diseases that are harder and costlier to treat. Information and communication technologies, and in particular Internet of Things technologies, can foster the creation of monitoring and intervention systems, both on an ambient-assisted living and smart city scope, for early detecting behavioral changes in elderly people. This allows to timely detect any potential risky situation and properly intervene, with benefits in terms of treatment's costs. In this context, as part of the H2020-funded City4Age project, this paper presents the data capturing and data management layers of the whole City4Age platform. In particular, this paper deals with an unobtrusive data gathering system implementation to collect data about daily activities of elderly people, and with the implementation of the related linked open data (LOD)-based data management system. The collected data are then used by other layers of the platform to perform risk detection algorithms and generate the proper customized interventions. Through the validation of some use-cases, it is demonstrated how this scalable approach, also characterized by unobtrusive and low-cost sensing technologies, can produce data with a high level of abstraction useful to define a risk profile of each elderly person.","2169-3536","","10.1109/ACCESS.2018.2800161","City4Age Project through the European Union’s Horizon 2020 Research and Innovation Programme(grant numbers:689731); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8277181","Elderly;IoT;LOD;MCI;monitoring systems;risk detection;smart environments","Senior citizens;Middleware;Smart cities;Monitoring;Sensors;Europe;Aging","","56","","36","OAPA","31 Jan 2018","","","IEEE","IEEE Journals"
"Trust Management in Social Internet of Things: Architectures, Recent Advancements, and Future Challenges","W. Z. Khan; Q. -u. -A. Arshad; S. Hakak; M. K. Khan; Saeed-Ur-Rehman","Center of Excellence in Information Assurance, King Saud University, Riyadh, Saudi Arabia; Center of Excellence in Information Assurance, King Saud University, Riyadh, Saudi Arabia; Canadian Institute for Cybersecurity, Faculty of Computer Science, University of New Brunswick, Fredericton, Canada; Center of Excellence in Information Assurance, King Saud University, Riyadh, Saudi Arabia; Department of Computer Science, COMSATS University Islamabad, Islamabad, Pakistan",IEEE Internet of Things Journal,"7 May 2021","2021","8","10","7768","7788","Social Internet of Things (SIoT) is an extension of the Internet of Things (IoT) that converges with social networking concepts to create social networks of interconnected smart objects. This convergence allows the enrichment of the two paradigms, resulting into new ecosystems. While IoT follows two interaction paradigms, human to human (H2H) and thing to thing (T2T), SIoT adds on human-to-thing (H2T) interactions. SIoT enables smart “social objects” that intelligently mimic the social behavior of human in the daily life. These social objects (SOs) are equipped with social functionalities capable of discovering other SOs in the surroundings and establishing social relationships. They crawl through the social network of objects for the sake of searching for services and information of interest. The notion of trust and trustworthiness in social communities formed in SIoT is still new and in an early stage of investigation. In this article, our contributions are threefold. First, we present the fundamentals of SIoT and trust concepts in SIoT, clarifying the similarities and differences between IoT and SIoT. Second, we categorize the trust management solutions proposed so far in the literature for SIoT over the last six years and provide a comprehensive review. We then perform a comparison of the state-of-the-art trust management schemes devised for SIoT by performing comparative analysis in terms of trust management process. Third, we identify and discuss the challenges and requirements in the emerging new wave of SIoT, and also highlight the challenges in developing trust and evaluating trustworthiness among the interacting SOs.","2327-4662","","10.1109/JIOT.2020.3039296","Deputyship for Research and Innovation, “Ministry of Education” in Saudi Arabia(grant numbers:IFKSURP-43); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264256","Challenges;Internet of Things (IoT);Social IoT (SIoT);social objects (SOs);social relationships;trust management","Internet of Things;Trust management;Social networking (online);Analytical models;Scalability;Vehicle dynamics;Search problems","","55","","99","IEEE","19 Nov 2020","","","IEEE","IEEE Journals"
"Cryptomining Detection in Container Clouds Using System Calls and Explainable Machine Learning","R. R. Karn; P. Kudva; H. Huang; S. Suneja; I. M. Elfadel","Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, UAE; IBM Research, Yorktown Heights, NY, USA; IBM Research, Yorktown Heights, NY, USA; IBM Research, Yorktown Heights, NY, USA; Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, UAE",IEEE Transactions on Parallel and Distributed Systems,"26 Oct 2020","2021","32","3","674","691","The use of containers in cloud computing has been steadily increasing. With the emergence of Kubernetes, the management of applications inside containers (or pods) is simplified. Kubernetes allows automated actions like self-healing, scaling, rolling back, and updates for the application management. At the same time, security threats have also evolved with attacks on pods to perform malicious actions. Out of several recent malware types, cryptomining has emerged as one of the most serious threats with its hijacking of server resources for cryptocurrency mining. During application deployment and execution in the pod, a cryptomining process, started by a hidden malware executable can be run in the background, and a method to detect malicious cryptomining software running inside Kubernetes pods is needed. One feasible strategy is to use machine learning (ML) to identify and classify pods based on whether or not they contain a running process of cryptomining. In addition to such detection, the system administrator will need an explanation as to the reason(s) of the ML's classification outcome. The explanation will justify and support disruptive administrative decisions such as pod removal or its restart with a new image. In this article, we describe the design and implementation of an ML-based detection system of anomalous pods in a Kubernetes cluster by monitoring Linux-kernel system calls (syscalls). Several types of cryptominers images are used as containers within an anomalous pod, and several ML models are built to detect such pods in the presence of numerous healthy cloud workloads. Explainability is provided using SHAP, LIME, and a novel auto-encoding-based scheme for LSTM models. Seven evaluation metrics are used to compare and contrast the explainable models of the proposed ML cryptomining detection engine.","1558-2183","","10.1109/TPDS.2020.3029088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9215018","Cryptomining;docker;kubernetes;containers;machine learning;explainability;pod;anomaly","Containers;Cloud computing;Malware;Machine learning;Cryptocurrency;Data mining","","54","","93","CCBY","6 Oct 2020","","","IEEE","IEEE Journals"
"Blockchain-Powered Value Creation in the 5G and Smart Grid Use Cases","K. Valtanen; J. Backman; S. Yrjölä","VTT Technical Research Centre of Finland Ltd., Oulu, Finland; VTT Technical Research Centre of Finland Ltd., Oulu, Finland; Nokia Corporate Strategy and Development, Oulu, Finland",IEEE Access,"7 Mar 2019","2019","7","","25690","25707","Before the implementation of a solution, it is cost-efficient and practical to be able to evaluate and analyze the expected value of use cases. Especially, this is emphasized in blockchain (BC) use cases, which typically have a wide business ecosystem and possibly disruptive business models. This paper presents two BC use case value evaluations and results. The IoT use cases were selected from two different industry segments: telecommunications-oriented 5G network slice brokering and the energy industry-related internal electricity allocation in a housing society. The use case value was assessed by applying a resource configuration framework and 4C - commerce, context, content, and connection - business model typology against BC and smart contracts characteristics and capabilities. The results derived from the data collected from the expert workshops proved the expected value of the use cases, and in general, the feasibility of BC technology for facilitating various value-creating resource configuration processes was shown. Furthermore, the resource configuration framework proved to be a valuable theoretical approach for analyzing and developing also the BC-enabled novel use cases and business models. According to the findings, further development of the framework is proposed with an introduced novel decentralized resource configuration prototype that can replace predominant platform-based business models.","2169-3536","","10.1109/ACCESS.2019.2900514","Blockchains Boosting Finnish Industry (BOND) project through Business Finland, the Finnish innovation funding, trade, investment, and travel promotion organization (formerly Tekes, the Finnish Funding Agency for Innovation); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8648405","Blockchain;business models;decentralization;resource configuration;smart grid;4C typology;5G","Business;Blockchain;Biological system modeling;Industries;5G mobile communication;Conferences;Smart grids","","54","","73","CCBY","21 Feb 2019","","","IEEE","IEEE Journals"
"Survey on Placement Methods in the Edge and Beyond","B. Sonkoly; J. Czentye; M. Szalay; B. Németh; L. Toka","Department of Telecommunications and Media Informatics, Faculty of Electrical Engineering and Informatics, MTA-BME Network Softwarization Research Group, Budapest University of Technology and Economics, Budapest, Hungary; Department of Telecommunications and Media Informatics, Faculty of Electrical Engineering and Informatics, MTA-BME Network Softwarization Research Group, Budapest University of Technology and Economics, Budapest, Hungary; Department of Telecommunications and Media Informatics, Faculty of Electrical Engineering and Informatics, MTA-BME Network Softwarization Research Group, Budapest University of Technology and Economics, Budapest, Hungary; Department of Telecommunications and Media Informatics, Faculty of Electrical Engineering and Informatics, MTA-BME Network Softwarization Research Group, Budapest University of Technology and Economics, Budapest, Hungary; Department of Telecommunications and Media Informatics, Faculty of Electrical Engineering and Informatics, MTA-BME Network Softwarization Research Group, Budapest University of Technology and Economics, Budapest, Hungary",IEEE Communications Surveys & Tutorials,"19 Nov 2021","2021","23","4","2590","2629","Edge computing is a (r)evolutionary extension of traditional cloud computing. It expands central cloud infrastructure with execution environments close to the users in terms of latency in order to enable a new generation of cloud applications. This paradigm shift has opened the door for telecommunications operators, mobile and fixed network vendors: they have joined the cloud ecosystem as essential stakeholders considerably influencing the future success of the technology. A key problem in edge computing is the optimal placement of computational units (virtual machines, containers, tasks or functions) of novel distributed applications. These components are deployed to a geographically distributed virtualized infrastructure and heterogeneous networking technologies are invoked to connect them while respecting quality requirements. The optimal hosting environment should be selected based on multiple criteria by novel scheduler algorithms which can cope with the new challenges of distributed cloud architecture where networking aspects cannot be ignored. The research community has dedicated significant efforts to this topic during recent years and a vast number of theoretical results have been published addressing different variants of the related mathematical problems. However, a comprehensive survey focusing on the technical and analytical aspects of the placement problem in various edge architectures is still missing. This survey provides a comprehensive summary and a structured taxonomy of the vast research on placement of computational entities in emerging edge infrastructures. Following the given taxonomy, the research papers are analyzed and categorized according to several dimensions, such as the capabilities of the underlying platforms, the structure of the supported services, the problem formulation, the applied mathematical methods, the objectives and constraints incorporated in the optimization problems, and the complexity of the proposed methods. We summarize the gained insights and important lessons learned, and finally, we reveal some important research gaps in the current literature.","1553-877X","","10.1109/COMST.2021.3101460","Ministry of Innovation and Technology of Hungary from the National Research, Development and Innovation Fund through 2018-2.1.17-TÉT-KR, FK_20; 2019-2.1.13-TÉT-IN Funding Schemes(grant numbers:2018-2.1.17-TÉT-KR-2018-00012,135074,2019-2.1.13-TÉT-IN-2020-00021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9502167","Edge/fog computing;MEC;cloudlets;resource orchestration;function placement optimization;offloading","Cloud computing;Optimization;Data centers;Taxonomy;Tutorials;Telecommunications;Task analysis","","53","","204","CCBY","30 Jul 2021","","","IEEE","IEEE Journals"
"Zero Touch Management: A Survey of Network Automation Solutions for 5G and 6G Networks","E. Coronado; R. Behravesh; T. Subramanya; A. Fernàndez-Fernàndez; M. S. Siddiqui; X. Costa-Pérez; R. Riggio","Software Networks, i2CAT Foundation, Barcelona, Spain; Wireless and Networked Systems, Fondazione Bruno Kessler, Trento, Italy; Architecture, Security and Automation, Nokia Standards, Munich, Germany; Software Networks, i2CAT Foundation, Barcelona, Spain; Software Networks, i2CAT Foundation, Barcelona, Spain; AI-Driven Systems, i2CAT Foundation, Barcelona, Spain; Information Engineering Department, Polytechnic University of Marche, Ancona, Italy",IEEE Communications Surveys & Tutorials,"21 Nov 2022","2022","24","4","2535","2578","Mobile networks are facing an unprecedented demand for high-speed connectivity originating from novel mobile applications and services and, in general, from the adoption curve of mobile devices. However, coping with the service requirements imposed by current and future applications and services is very difficult since mobile networks are becoming progressively more heterogeneous and more complex. In this context, a promising approach is the adoption of novel network automation solutions and, in particular, of zero-touch management techniques. In this work, we refer to zero-touch management as a fully autonomous network management solution with human oversight. This survey sits at the crossroad between zero-touch management and mobile and wireless network research, effectively bridging a gap in terms of literature review between the two domains. In this paper, we first provide a taxonomy of network management solutions. We then discuss the relevant state-of-the-art on autonomous mobile networks. The concept of zero-touch management and the associated standardization efforts are then introduced. The survey continues with a review of the most important technological enablers for zero-touch management. The network automation solutions from the RAN to the core network, including end-to-end aspects such as security, are then surveyed. Finally, we close this article with the current challenges and research directions.","1553-877X","","10.1109/COMST.2022.3212586","European Union’s Horizon 2020 through the Project AI@EDGE; EU(grant numbers:101015922,5GZORRO); EU(grant numbers:871533); CERCA Programme/Generalitat de Catalunya; EU “NextGenerationEU/PRTR,” MCIN and AEI (Spain)(grant numbers:IJC2020-043058-I); ONOFRE-3 through the Project MCIN/AEI/10.13039/501100011033(grant numbers:PID2020-112675RB-C43); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9913206","Network management;autonomous networks;zero-touch management;mobile networking;wireless networking;5G;6G","5G mobile communication;Automation;Radio access networks;Protocols;Tutorials;Terminology;Cloud computing","","53","","284","CCBY","6 Oct 2022","","","IEEE","IEEE Journals"
"A Survey on In-Network Computing: Programmable Data Plane and Technology Specific Applications","S. Kianpisheh; T. Taleb","Centre for Wireless Communications, University of Oulu, Oulu, Finland; Centre for Wireless Communications, University of Oulu, Oulu, Finland",IEEE Communications Surveys & Tutorials,"23 Feb 2023","2023","25","1","701","761","In comparison with cloud computing, edge computing offers processing at locations closer to end devices and reduces the user experienced latency. The new recent paradigm of in-network computing employs programmable network elements to compute on the path and prior to traffic reaching the edge or cloud servers. It advances common edge/cloud server based computing through proposing line rate processing capabilities at closer locations to the end devices. This paper discusses use cases, enabler technologies and protocols for in-network computing. According to our study, considering programmable data plane as an enabler technology, potential in-network computing applications are in-network analytics, in-network caching, in-network security, and in-network coordination. There are also technology specific applications of in-network computing in the scopes of cloud computing, edge computing, 5G/6G, and NFV. In this survey, the state of the art, in the framework of the proposed categorization, is reviewed. Furthermore, comparisons are provided in terms of a set of proposed criteria which assess the methods from the aspects of methodology, main results, as well as application-specific criteria. Finally, we discuss lessons learned and highlight some potential research directions.","1553-877X","","10.1109/COMST.2022.3213237","European Unions Horizon 2020 Research and Innovation Program through the Charity and aerOS Projects(grant numbers:101016509,101069732); Academy of Finland 6Genesis Project(grant numbers:318927); Academy of Finland IDEA-MILL Project(grant numbers:352428); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9919270","In-network computing;programmable data plane;software defined networking;cloud computing;edge computing;6G;and network function virtualization","Servers;Cloud computing;Switches;Tutorials;Protocols;Field programmable gate arrays;Fabrics","","53","","177","CCBY","14 Oct 2022","","","IEEE","IEEE Journals"
"Security and Trust in the 6G Era","V. Ziegler; P. Schneider; H. Viswanathan; M. Montag; S. Kanugovi; A. Rezaki","Nokia Bell Labs, Munich, Germany; Nokia Bell Labs, Munich, Germany; Nokia Bell Labs, Murray Hill, NJ, USA; Nokia Bell Labs, Munich, Germany; Nokia Standards, Bengaluru, India; Nokia Standards, Munich, Germany",IEEE Access,"26 Oct 2021","2021","9","","142314","142327","A comprehensive set of security technology enablers will be critically required for communication systems for the 6G era of the 2030s. Trustworthiness must be assured across IoT, heterogenous cloud and networks, devices, sub-networks, and applications. The 6G threat vector will be defined by 6G architectural disaggregation, open interfaces and an environment with multiple stakeholders. Broadly decomposed into domains of cyber-resilience, privacy and trust and their respective intersection, we explore relevant security technology enablers including automated software creation and automated closed-loop security operation, privacy preserving technologies, hardware and cloud embedded anchors of trust, quantum-safe security, jamming protection and physical layer security as well as distributed ledger technologies. Artificial intelligence and machine learning (AI/ML) as a key technology enabler will be pervasive and of pivotal relevance across the security technology stack and architecture. A novel vision for a trustworthy Secure Telecom Operation Map is developed as part of the automated closed loop operations paradigm.","2169-3536","","10.1109/ACCESS.2021.3120143","Nokia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9570274","6G;security;cyber-resilience;privacy;trustworthiness;sub-networks;wireless networks","Security;6G mobile communication;5G mobile communication;Privacy;Cloud computing;Authentication;3GPP","","53","","44","CCBY","14 Oct 2021","","","IEEE","IEEE Journals"
"Toward Software-Defined Networking-Based IoT Frameworks: A Systematic Literature Review, Taxonomy, Open Challenges and Prospects","S. Siddiqui; S. Hameed; S. A. Shah; I. Ahmad; A. Aneiba; D. Draheim; S. Dustdar","Department of Computer Science, NUCES, Karachi, Pakistan; Department of Computer Science, NUCES, Karachi, Pakistan; School of Computing and Digital Technology, Birmingham City University, Birmingham, U.K.; VTT Technical Research Centre of Finland, Espoo, Finland; School of Computing and Digital Technology, Birmingham City University, Birmingham, U.K.; Information Systems Group, Tallinn University of Technology, Tallinn, Estonia; Distributed Systems Group, Vienna University of Technology, Vienna, Austria",IEEE Access,"12 Jul 2022","2022","10","","70850","70901","Internet of Things (IoT) is characterized as one of the leading actors for the next evolutionary stage in the computing world. IoT-based applications have already produced a plethora of novel services and are improving the living standard by enabling innovative and smart solutions. However, along with its rapid adoption, IoT technology also creates complex challenges regarding the management of IoT networks due to its resource limitations (computational power, energy, and security). Hence, it is urgently needed to refine the IoT-based application’s architectures to robustly manage the overall IoT infrastructure. Software-defined networking (SDN) has emerged as a paradigm that offers software-based controllers to manage hardware infrastructure and traffic flow on a network effectively. SDN architecture has the potential to provide efficient and reliable IoT network management. This research provides a comprehensive survey investigating the published studies on SDN-based frameworks to address IoT management issues in the dimensions of fault tolerance, energy management, scalability, load balancing, and security service provisioning within the IoT networks. We conducted a Systematic Literature Review (SLR) on the research studies (published from 2010 to 2022) focusing on SDN-based IoT management frameworks. We provide an extensive discussion on various aspects of SDN-based IoT solutions and architectures. We elaborate a taxonomy of the existing SDN-based IoT frameworks and solutions by classifying them into categories such as network function virtualization, middleware, OpenFlow adaptation, and blockchain-based management. We present the research gaps by identifying and analyzing the key architectural requirements and management issues in IoT infrastructures. Finally, we highlight various challenges and a range of promising opportunities for future research to provide a roadmap for addressing the weaknesses and identifying the benefits from the potentials offered by SDN-based IoT solutions.","2169-3536","","10.1109/ACCESS.2022.3188311","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9815064","Internet of Things (IoT);software-defined networking (SDN);SDN-based IoT management frameworks;systematic literature review;network function virtualization;OpenFlow;middleware;blockchain;security management;fault tolerance;load balancing;scalability;energy management","Internet of Things;Blockchains;Middleware;Load management;Computer architecture;Sensors;Security","","53","","230","CCBY","4 Jul 2022","","","IEEE","IEEE Journals"
"Reliable and Resilient AI and IoT-Based Personalised Healthcare Services: A Survey","N. Taimoor; S. Rehman","Institute of Computer Technology, Technische Universität Wien (TU Wien), Wien, Austria; Institute of Computer Technology, Technische Universität Wien (TU Wien), Wien, Austria",IEEE Access,"4 Jan 2022","2022","10","","535","563","Recent technological (e.g., IoT, 5G), and economic (e.g., UN 2030 Sustainable Development Goals) developments have transformed the healthcare sector towards more personalized and IoT-based healthcare services. These services are realized through control and monitoring applications that are typically developed using artificial intelligence (AI)/machine learning (ML) based algorithms, that play a significant role to highlight the efficiency of traditional healthcare systems. Current personalized healthcare services are dedicated in a specific environment to support technological personalization (e.g., personalized gadgets/devices). However, they are unable to consider different inter-related health conditions, leading to inappropriate diagnosis and affect sustainability and the long-term health/life of patients. Towards this problem, the state-of-the-art Healthcare 5.0 technology has evolved that supersede previous healthcare technologies. The goal of healthcare 5.0 is to achieve a fully autonomous healthcare service, that takes into account the interdependent effect of different health conditions of a patient. This paper conducts a comprehensive survey on personalized healthcare services. In particular, we first present an overview of key requirements of comprehensive personalized healthcare services (CPHS) in modern healthcare Internet of Things (HIoT), including the definition of personalization and an example use case scenario as a representative for modern HIoT. Second, we explored a fundamental three-layer architecture for IoT-based healthcare systems using both AI and non-AI-based approaches, considering key requirements for CPHS followed by their strengths and weaknesses in the frame of personalized healthcare services. Third, we highlighted different security threats against each layer of IoT architecture along with the possible AI and non-AI-based solutions. Finally, we propose a methodology to develop reliable, resilient, and personalized healthcare services that address the identified weaknesses of existing approaches.","2169-3536","","10.1109/ACCESS.2021.3137364","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9658494","Healthcare 5.0;IoT;medicine 4.0;reliability;resilience;personalization;sustainability","Medical services;Monitoring;Resilience;Biomedical monitoring;Artificial intelligence;Software reliability","","53","","187","CCBY","22 Dec 2021","","","IEEE","IEEE Journals"
"Dynamic Service Placement in Multi-Access Edge Computing: A Systematic Literature Review","H. Tabatabaee Malazi; S. R. Chaudhry; A. Kazmi; A. Palade; C. Cabrera; G. White; S. Clarke","Department of Computer Science, Maynooth University, Maynooth, Co. Kildare, Ireland; Department of Computer Science, Munster Technological University, Cork, Ireland; School of Computer Science and Statistics, Trinity College Dublin, Dublin 2, Ireland; School of Computer Science and Statistics, Trinity College Dublin, Dublin 2, Ireland; Department of Computer Science and Technology, University of Cambridge, Cambridge, U.K.; School of Computer Science and Statistics, Trinity College Dublin, Dublin 2, Ireland; School of Computer Science and Statistics, Trinity College Dublin, Dublin 2, Ireland",IEEE Access,"30 Mar 2022","2022","10","","32639","32688","The advent of new cloud-based applications such as mixed reality, online gaming, autonomous driving, and healthcare has introduced infrastructure management challenges to the underlying service network. Multi-access edge computing (MEC) extends the cloud computing paradigm and leverages servers near end-users at the network edge to provide a cloud-like environment. The optimum placement of services on edge servers plays a crucial role in the performance of such service-based applications. Dynamic service placement problem addresses the adaptive configuration of application services at edge servers to facilitate end-users and those devices that need to offload computation tasks. While reported approaches in the literature shed light on this problem from a particular perspective, a panoramic study of this problem reveals the research gaps in the big picture. This paper introduces the dynamic service placement problem and outline its relations with other problems such as task scheduling, resource management, and caching at the edge. We also present a systematic literature review of existing dynamic service placement methods for MEC environments from networking, middleware, applications, and evaluation perspectives. In the first step, we review different MEC architectures and their enabling technologies from a networking point of view. We also introduce different cache deployment solutions in network architectures and discuss their design considerations. The second step investigates dynamic service placement methods from a middleware viewpoint. We review different service packaging technologies and discuss their trade-offs. We also survey the methods and identify eight research directions that researchers follow. Our study categorises the research objectives into six main classes, proposing a taxonomy of design objectives for the dynamic service placement problem. We also investigate the reported methods and devise a solutions taxonomy comprising six criteria. In the third step, we concentrate on the application layer and introduce the applications that can take advantage of dynamic service placement. The fourth step investigates evaluation environments used to validate the solutions, including simulators and testbeds. We introduce real-world datasets such as edge server locations, mobility traces, and service requests used to evaluate the methods. We compile a list of open issues and challenges categorised by various viewpoints in the last step.","2169-3536","","10.1109/ACCESS.2022.3160738","Science Foundation Ireland (SFI), Enable Project(grant numbers:16/SP/3804); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9738624","Mobile edge computing;decentralised cloud;MEC server;service caching;service offloading;computational offloading;service deployment;resource management;service orchestration","Cloud computing;Servers;Vehicle dynamics;Resource management;Wireless fidelity;Taxonomy;Task analysis","","51","","284","CCBY","21 Mar 2022","","","IEEE","IEEE Journals"
"A Two-Layer Water Demand Prediction System in Urban Areas Based on Micro-Services and LSTM Neural Networks","A. A. Nasser; M. Z. Rashad; S. E. Hussein","Faculty of Computers and Information Systems, Mansoura University, Mansoura, Egypt; Faculty of Computers and Information Systems, Mansoura University, Mansoura, Egypt; Department of Computer Engineering and Control Systems, Faculty of Engineering, Mansoura University, Mansoura, Egypt",IEEE Access,"18 Aug 2020","2020","8","","147647","147661","In recent years, scarce water resources became one of the main problems that endanger human species existence and the advancement of any nation. In this research, smart water meters were implemented, distributed, and installed in a regional area in Cairo while data were collected at uniform intervals then sent to the cloud instantly. The solution paradigm uses an Internet of Things (IoT) based on micro-services and containers. The design incorporates real-time streaming and infrastructure performance optimization to store data. A second layer to analyze the acquired data was used to model water consumption using Long Short-Term Memory (LSTM). The designed LSTM is validated and tested to be utilized in the forecast of future water demand. Moreover, two alternative machine learning methods, namely Support Vector Regression and Random Forest commonly utilized in time series forecasting applications, were used for a comparative analysis of which LSTM has proven to be superior. The proper integration of the system elements is the key to the proposed system success. Based on the success of the designed system, it can be applicable on a national scale. That can enable the optimal management of consumers’ demand and improve water infrastructure utilization. The proposed paradigm presents a testbed for various scenarios that can be used in water resources management.","2169-3536","","10.1109/ACCESS.2020.3015655","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9163328","Water smart meters;time series;resources management;IoT;LSTM;micro-services","Water resources;Computer architecture;Demand forecasting;Real-time systems;Artificial intelligence;Cloud computing;Support vector machines","","49","","68","CCBY","10 Aug 2020","","","IEEE","IEEE Journals"
"Performance Modeling of Serverless Computing Platforms","N. Mahmoudi; H. Khazaei","Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada; Department of Electrical Engineering and Computer Science, York University, Toronto, ON, Canada",IEEE Transactions on Cloud Computing,"5 Dec 2022","2022","10","4","2834","2847","Analytical performance models have been leveraged extensively to analyze and improve the performance and cost of various cloud computing services. However, in the case of serverless computing, which is projected to be the dominant form of cloud computing in the future, we have not seen analytical performance models to help with the analysis and optimization of such platforms. In this work, we propose an analytical performance model that captures the unique details of serverless computing platforms. The model can be leveraged to improve the quality of service and resource utilization and reduce the operational cost of serverless platforms. Also, the proposed performance model provides a framework that enables serverless platforms to become workload-aware and operate differently for different workloads to provide a better trade-off between the cost and performance depending on the user's preferences. The current serverless offerings require the user to have extensive knowledge of the internals of the platform to perform efficient deployments. Using the proposed analytical model, the provider can simplify the deployment process by calculating the performance metrics for users even before physical deployments. We validate the applicability and accuracy of the proposed model by extensive experimentation on AWS Lambda. We show that the proposed model can calculate essential performance metrics such as average response time, probability of cold start, and the average number of function instances in the steady-state. Also, we show how the performance model can be used to tune the serverless platform for each workload, which will result in better performance or lower cost without scarifying the other. The presented model assumes no non-realistic restrictions, so that it offers a high degree of fidelity while maintaining tractability at large scale.","2168-7161","","10.1109/TCC.2020.3033373","Shared Hierarchical Academic Research Computing Network; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9238484","Serverless computing;performance modeling;optimization;queuing theory;stochastic processes","Computational modeling;Analytical models;Cloud computing;Concurrent computing;Measurement;Servers;Quality of service","","49","","61","IEEE","23 Oct 2020","","","IEEE","IEEE Journals"
"Evaluating Docker for Lightweight Virtualization of Distributed and Time-Sensitive Applications in Industrial Automation","M. Sollfrank; F. Loch; S. Denteneer; B. Vogel-Heuser","Technical University of Munich, Garching, Germany; Technical University of Munich, Garching, Germany; Technical University of Munich, Garching, Germany; Technical University of Munich, Garching, Germany",IEEE Transactions on Industrial Informatics,"23 Feb 2021","2021","17","5","3566","3576","A trend, accompanying the change of automation systems and their architectures, is the virtualization of software components. Virtualization strengthens platform-independent development and the provision of secure and isolated applications. Virtualization introduces well-defined interfaces to strengthen modularity, which facilitates the scalability of applications. However, virtualization includes additional software components and layers and, thus, additional computing costs. This additional effort can conflict with the real-time requirements of automation processes. Current research lacks the investigation of the time behavior of container-based virtualizations concerning their use in real-time systems. An assessment concerning real-time applications is required to prepare it for use in industrial automation. This article examines the effects of virtualization on the time delays of a software component based on Docker containers by providing measurements on a hardware testbed in a realistic use case. The experiments indicate that Docker virtualization can meet soft real-time requirements and can be used in industrial automation.","1941-0050","","10.1109/TII.2020.3022843","VDI/VDE(grant numbers:IUK459/002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9187833","Embedded software;real-time systems;software architecture;software tools;virtual machines (VMs)","Real-time systems;Containers;Automation;Virtualization;Computer architecture;Delays;Software","","49","","32","IEEE","8 Sep 2020","","","IEEE","IEEE Journals"
"Internet of Things for Agricultural Applications: The State of the Art","T. Ojha; S. Misra; N. S. Raghuwanshi","Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Agricultural and Food Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India",IEEE Internet of Things Journal,"6 Jul 2021","2021","8","14","10973","10997","The advent of the Internet of Things (IoT) inspired various new and enhanced sets of applications in multiple domains including agriculture. The recent drive in the adoption of IoT technologies offers a major enhancement for the agricultural sectors in terms of efficiency and scalability. In this article, we investigate the specific issues and challenges associated with IoT, and review various IoT architectures, communication, middleware, and information processing technologies. We, then, discuss few IoT applications for agriculture-presenting various case studies to thoroughly analyze the solutions along with their design and implementation related parameters. Consequently, we provide a comprehensive review of the available simulation tools, data sets, and testbeds which provisions experimentation with IoT in agriculture. We enumerate open issues and challenges present in enabling IoT for agriculture. Finally, this article concludes while giving directions for future research.","2327-4662","","10.1109/JIOT.2021.3051418","Information Technology Research Academy (ITRA), Government of India(grant numbers:ITRA/15(69)/WATER/M2M/01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9321474","Agriculture;automation;Internet of Things (IoT);precision agriculture","Internet of Things;Agriculture;Cloud computing;Sensors;Production;Irrigation;Diseases","","48","","196","IEEE","13 Jan 2021","","","IEEE","IEEE Journals"
"What the Fog? Edge Computing Revisited: Promises, Applications and Future Challenges","J. Gedeon; F. Brandherm; R. Egert; T. Grube; M. Mühlhäuser","Department of Computer Science, Technische Universität Darmstadt, Darmstadt, Germany; Department of Computer Science, Technische Universität Darmstadt, Darmstadt, Germany; Department of Computer Science, Technische Universität Darmstadt, Darmstadt, Germany; Department of Computer Science, Technische Universität Darmstadt, Darmstadt, Germany; Department of Computer Science, Technische Universität Darmstadt, Darmstadt, Germany",IEEE Access,"28 Oct 2019","2019","7","","152847","152878","Edge computing brings computing and storage resources closer to (mobile) end users and data sources, thus bypassing expensive and slow links to distant cloud computing infrastructures. Often leveraged opportunistically, these heterogeneous resources can be used to offload data and computations, enabling upcoming demanding applications such as augmented reality and autonomous driving. Research in this direction has addressed various challenges, from architectural concerns to runtime optimizations. As of today, however, we lack a widespread availability of edge computing-partly because it remains unclear which of the promised benefits of edge computing are relevant for what types of applications. This article provides a comprehensive snapshot of the current edge computing landscape, with a focus on the application perspective. We outline the characteristics of edge computing and its postulated benefits and drawbacks. To understand the functional composition of applications, we first define common application components that are relevant w.r.t. edge computing. We then present a classification of proposed use cases and analyze them according to their expected benefits from edge computing and which components they use. Furthermore, we illustrate existing products and industry solutions that have recently surfaced and outline future research challenges.","2169-3536","","10.1109/ACCESS.2019.2948399","National Natural Science Foundation of China(grant numbers:392046569 (DFG),61761136014 (NSFC)); Collaborative Research Center 1053–MAKI (DFG) and as part of the project D.4 within the RTG 2050 “Privacy and Trust for Mobile Users” (DFG); Technische Universität Darmstadt; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8877785","Edge computing;heterogeneous networks;next generation networking mobile applications;Internet of Things;ubiquitous computing","Edge computing;Cloud computing;Mobile handsets;Task analysis;Taxonomy;Industries;Hardware","","48","","293","CCBY","21 Oct 2019","","","IEEE","IEEE Journals"
"Container Placement and Migration in Edge Computing: Concept and Scheduling Models","O. Oleghe","Systems Engineering Department, University of Lagos, Lagos, Nigeria",IEEE Access,"11 May 2021","2021","9","","68028","68043","Containers are a form of software virtualization, rapidly becoming the de facto way of providing edge computing services. Research on container-based edge computing is plentiful, and this has been buoyed by the increasing demand for single digit, milliseconds latency computations. A container scheduler is part of the architecture that is used to manage and orchestrate multiple container-based applications on heterogenous computing nodes. The scheduler decides how incoming computing requests are allocated to containers, which edge nodes the containers are placed on, and where already deployed containers are migrated to. This paper aims to clarify the concept of container placement and migration in edge servers and the scheduling models that have been developed for this purpose. The study illuminates the frameworks and algorithms upon which the scheduling models are built. To convert the problem to one that can be solved using an algorithm, the container placement problem in mostly abstracted using multi-objective optimization models or graph network models. The scheduling algorithms are predominantly heuristic-based algorithms, which are able to arrive at sub-optimal solutions very quickly. There is paucity of container scheduling models that consider distributed edge computing tasks. Research in decentralized scheduling systems is gaining momentum and the future outlook is in scheduling containers for mobile edge nodes.","2169-3536","","10.1109/ACCESS.2021.3077550","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9422781","Algorithm;container;edge computing;migration;placement;scheduling","Containers;Edge computing;Processor scheduling;Servers;Computational modeling;Cloud computing;Task analysis","","46","","105","CCBY","4 May 2021","","","IEEE","IEEE Journals"
"IoT-to-the-Rescue: A Survey of IoT Solutions for COVID-19-Like Pandemics","N. Pathak; P. K. Deb; A. Mukherjee; S. Misra","Advanced Technology Development Center, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India",IEEE Internet of Things Journal,"23 Aug 2021","2021","8","17","13145","13164","The atmospheric buoyancy and intangible nature of fatal communicable viruses lead to rapid transmissions among individuals, resulting in global pandemics. Strategic lockdowns and mandatory social distancing are immediate solutions in such scenarios. However, this leads to operational disruptions in education, manufacturing, economy, transportation, governance, and community. Although technological assistance is beneficial in overcoming such issues, the current Internet of Things (IoT) infrastructure has limitations. In this article, we provide a comprehensive review of the possible IoT-based solutions that have the capacity of combating the COVID-19-like viruses. We highlight the societal impacts due to pandemics and identify the specific lacunae in current IoT solutions. We also provide comprehensive detail on how to overcome the challenges along with directions toward the possible technological trends for future research. Compared to existing reviews, our work offers a holistic view of the cause, effects, and the possible solutions that are existing, along with already existing solutions that can be customized to serve the special needs during the pandemic.","2327-4662","","10.1109/JIOT.2021.3082838","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9438712","COVID-19;education;governance;healthcare;industry;Internet of Things (IoT);IoT architecture;pandemic;SARS-COV-2;transportation","COVID-19;Pandemics;Coronaviruses;Internet of Things;Medical services;Diseases;Government","","45","","142","IEEE","21 May 2021","","","IEEE","IEEE Journals"
"Architecting Digital Twins","E. Ferko; A. Bucaioni; M. Behnam","Division of Product Realisation, Mälardalen University, Eskilstuna, Sweden; Division of Product Realisation, Mälardalen University, Eskilstuna, Sweden; Division of Product Realisation, Mälardalen University, Eskilstuna, Sweden",IEEE Access,"16 May 2022","2022","10","","50335","50350","In 2002, Grieves defined the concept of the digital twin as a virtual instance of physical assets capable of continuously mirroring them. Ever since then, driven by remarkable industrial attention, digital twins flourished and ripened in several sectors. The notable industrial adoption has been sided by a growing interest from the software engineering community in general and the software architecture community in particular as demonstrated by the growing number of published peer-reviewed publications and proposed software architectural solutions for digital twins. In this paper, we report on the planning, execution, and results of a systematic mapping study on architecting digital twins. The study captures crucial aspects of software architectures for digital twins as types of architectural solutions, quality attributes, and architectural patterns. It supports practitioners in creating digital twins tailored to their specific needs and researchers in identifying trends and open challenges. Starting from an initial set of potentially relevant 1630 peer-reviewed publications, we selected 140 primary studies. We analysed the set of primary studies using thorough data extraction, analysis, and synthesis process. To compensate for single method limitations and reduce possible threats to conclusion validity, we discussed the results of our study with experts in the software architecture community. Based on our results, the field of software architecture for digital twins is lively and an increasing number of architectural solutions are being proposed. Although there is a lack of widely accepted reference architectural solutions for digital twins, most of them are built using a combination of the layered and service-oriented patterns and address maintainability, performance efficiency, and compatibility quality attributes.","2169-3536","","10.1109/ACCESS.2022.3172964","Swedish Knowledge Foundation [Stiftelsen för kunskaps- och kompetensutveckling (KKS)] through the Assured Cloud Platforms for Industrial Cyber-physical Systems (ACICS) and Model-Based DevOps for Cyber-Physical System Product Lines (Modev) projects; Sweden’s Innovation Agency [Verket för innovationssystem (VINNOVA)] through the PANORAMA Project; Excellence in Production Research [Excellence in Production Research (XPRES)] Framework; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770073","Architectural patterns;digital twin;software architectures;quality attributes.","Digital twin;Systematics;Software architecture;Computer architecture;Standards;Market research;Planning","","45","","40","CCBY","5 May 2022","","","IEEE","IEEE Journals"
"Digital Twins: A Maturity Model for Their Classification and Evaluation","J. -F. Uhlenkamp; J. B. Hauge; E. Broda; M. Lütjen; M. Freitag; K. -D. Thoben","BIBA—Bremer Institut für Produktion und Logistik GmbH, University of Bremen, Bremen, Germany; BIBA—Bremer Institut für Produktion und Logistik GmbH, University of Bremen, Bremen, Germany; Faculty of Production Engineering, University of Bremen, Bremen, Germany; BIBA—Bremer Institut für Produktion und Logistik GmbH, University of Bremen, Bremen, Germany; BIBA—Bremer Institut für Produktion und Logistik GmbH, University of Bremen, Bremen, Germany; BIBA—Bremer Institut für Produktion und Logistik GmbH, University of Bremen, Bremen, Germany",IEEE Access,"7 Jul 2022","2022","10","","69605","69635","Digital Twins represent a powerful tool for transforming production and logistics towards Industry 4.0. They mirror physical assets in the digital world, enriching them with additional capabilities and features such as decision-making or lifecycle management. Due to the diverse possibilities associated with the Digital Twin, their design and implementation are also wide-ranging. This paper aims to contribute to the formalization and standardization of the description of Digital Twins. It presents a method for evaluating them through their lifecycle, from design to operation. The paper is based on an overview of their potential functionalities and properties with ranked stages of development. This method allows for an application-specific evaluation of Digital Twins and describes how they can be improved to suit the application better. The maturity model development follows the procedure for developing maturity models for IT management. Relevant capabilities and features were identified with a systematic literature review following the PRISMA guidelines. The results of this review were ranked and categorized and constitute the core of the maturity model, which was validated on five use-cases from different domains in production and logistics. The maturity model assesses Digita Twins in seven categories (context, data, computing capabilities, model, integration, control, human-machine interface) with 31 ranked characteristics. It evaluates existing solutions for potential improvements for a given application or the transfer to a new use-case. The resulting method and a supplementary web service present a generalized model for the evaluation of Digital Twins. Based on a description of a potential application, this is the first step towards a systematic evaluation, improving the structured development of such applications","2169-3536","","10.1109/ACCESS.2022.3186353","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9807313","Digital twin;maturity model;characteristics;dimensions;literature review;use cases;product development;systematization;digital twin application","Digital twins;Analytical models;Bibliographies;Computational modeling;Logistics;Fourth Industrial Revolution;Adaptation models","","45","","98","CCBYNCND","27 Jun 2022","","","IEEE","IEEE Journals"
"A Theoretical Discussion and Survey of Network Automation for IoT: Challenges and Opportunity","S. T. Arzo; C. Naiga; F. Granelli; R. Bassoli; M. Devetsikiotis; F. H. P. Fitzek","Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Deutsche Telekom Chair of Communication Networks, Institute of Communication Technology, Faculty of Electrical and Computer Engineering, and the Centre for Tactile Internet with Human-in-the-Loop, Cluster of Excellence, Technische Universität Dresden, Dresden, Germany; Department of Electrical and Computer Engineering, University of New Mexico, Albuquerque, NM, USA; Deutsche Telekom Chair of Communication Networks, Institute of Communication Technology, Faculty of Electrical and Computer Engineering, and the Centre for Tactile Internet with Human-in-the-Loop, Cluster of Excellence, Technische Universität Dresden, Dresden, Germany",IEEE Internet of Things Journal,"26 Jul 2021","2021","8","15","12021","12045","The introduction of the Internet of Things (IoT) and massive machine-type communications has implied an increase in network size and complexity. In particular, there is already a huge number of IoT devices in the market in various sectors, such as smart agriculture, smart city, smart home, smart transportation, etc. The IoT interconnectivity technologies are also increasing. Therefore, these are increasingly overwhelming the efforts of network administrators as they try to design, reconfigure and manage such networks. Relying on humans to manage such complex and dynamic networks is becoming unsustainable. Network automation promises to reduce the cost of administration and maintenance of network infrastructure, by offering networks the capability to manage themselves. Network automation is the ability of the network to manage itself. Various standardization organizations are taking the initiative in introducing network automation, such as European Telecommunication Standardization Institute (ETSI). ETSI is leading the standardization activities for network automation. It has provided different versions of reference architecture called generic autonomic network architecture (GANA), which describes a four-level abstraction for network-management decision elements (DEs), protocol level, function level, node level, and network level. In this article, we review and survey the existing works before and after the introduction of software-defined networking (SDN) and network-function-virtualization (NFV). We relate the main trending paradigms being followed, such as SDN, NFV, machine learning (ML), microservices, multiagent system (MAS), containerization, and cloudification, as a pivotal enabler of full network automation. We also discuss the autonomic architectures proposed in the literature. Finally, we presented possible future research directions and challenges that need to be tackled to progress in achieving full network automation.","2327-4662","","10.1109/JIOT.2021.3075901","NATO Science for Peace and Security (SPS) Programme in the Framework of the Project “Dynamic Architecture based on UAVs Monitoring for Border Security and Safety”(grant numbers:SPS G5428); German Research Foundation (DFG, Deutsche Forschungsgemeinschaft) as part of Germany’s Excellence Strategy—EXC 2050/1—Project ID 390696704—Cluster of Excellence “Centre for Tactile Internet with Human-in-the-Loop” (CeTI) of Technische Universität Dresden; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9416288","Autonomic networking;machine learning;multiagent system (MAS);network function virtualization;network management system (NMS);network softwarization;software-defined networking (SDN)","Automation;Internet of Things;Computer architecture;Market research;Software;Monitoring;Cloud computing","","44","","156","IEEE","26 Apr 2021","","","IEEE","IEEE Journals"
"AI-Empowered Fog/Edge Resource Management for IoT Applications: A Comprehensive Review, Research Challenges, and Future Perspectives","G. K. Walia; M. Kumar; S. S. Gill","Department of Information Technology, Dr. B.R. Ambedkar National Institute of Technology, Jalandhar, India; Department of Information Technology, Dr. B.R. Ambedkar National Institute of Technology, Jalandhar, India; School of Electronic Engineering and Computer Science, Queen Mary University of London, London, U.K.",IEEE Communications Surveys & Tutorials,"26 Feb 2024","2024","26","1","619","669","The proliferation of ubiquitous Internet of Things (IoT) sensors and smart devices in several domains embracing healthcare, Industry 4.0, transportation and agriculture are giving rise to a prodigious amount of data requiring ever-increasing computations and services from cloud to the edge of the network. Fog/Edge computing is a promising and distributed computing paradigm that has drawn extensive attention from both industry and academia. The infrastructural efficiency of these computing paradigms necessitates adaptive resource management mechanisms for offloading decisions and efficient scheduling. Resource Management (RM) is a non-trivial issue whose complexity is the result of heterogeneous resources, incoming transactional workload, edge node discovery, and Quality of Service (QoS) parameters at the same time, which makes the efficacy of resources even more challenging. Hence, the researchers have adopted Artificial Intelligence (AI)-based techniques to resolve the above-mentioned issues. This paper offers a comprehensive review of resource management issues and challenges in Fog/Edge paradigm by categorizing them into provisioning of computing resources, task offloading, resource scheduling, service placement, and load balancing. In addition, existing AI and non-AI based state-of-the-art solutions have been discussed, along with their QoS metrics, datasets analysed, limitations and challenges. The survey provides mathematical formulation corresponding to each categorized resource management issue. Our work sheds light on promising research directions on cutting-edge technologies such as Serverless computing, 5G, Industrial IoT (IIoT), blockchain, digital twins, quantum computing, and Software-Defined Networking (SDN), which can be integrated with the existing frameworks of fog/edge-of-things paradigms to improve business intelligence and analytics amongst IoT-based applications.","1553-877X","","10.1109/COMST.2023.3338015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10335918","Edge computing;resource management;fog computing;artificial intelligence;machine learning;cloud computing;IoT","Internet of Things;Cloud computing;Sensors;Task analysis;Resource management;Surveys;Intelligent sensors","","43","","205","IEEE","30 Nov 2023","","","IEEE","IEEE Journals"
"Hierarchical Blockchain Design for Distributed Control and Energy Trading Within Microgrids","J. Yang; J. Dai; H. B. Gooi; H. D. Nguyen; P. Wang","School of Electrical and Electronics Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronics Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of EEE, Nanyang Technological University, Singapore; Department of Electrical Engineering and Computer Science, York University, Toronto, ON, Canada",IEEE Transactions on Smart Grid,"21 Jun 2022","2022","13","4","3133","3144","Blockchain technology is recognized as a suitable tool to secure the energy trading because it could perfectly match the distributed structure of peer-to-peer (P2P) energy market. But its usage is stuck on the transaction level. Control systems are significant to the microgrid as they ensure a stable power delivery system and regulate the performance of parameters such as active power and frequency. This paper proves that the blockchain technology is also effective in securing the distributed control systems against the false data injection attack. A six-prosumer microgrid is tested with the implementation of the hierarchical blockchain system. The security of both the control system and energy trading system of the microgrid is ensured. Smart contracts are created to calculate the feedback measurements for the control system and execute the energy transactions. According to the hierarchical structure, the private blockchain with static nodes is implemented for the distributed control to match the sampling rate. A Proof-of-Authority based blockchain is utilized to support the energy trading. In addition, a double auction based simple iteration (DA-SI) pricing scheme is designed to improve the social welfare of the microgrid. Finally, case studies are presented to verify the proposed hierarchical blockchain system as an effective method to safeguard the control system and maximize the benefits of prosumers. Numerical results show the effectiveness and feasibility of the proposed approach.","1949-3061","","10.1109/TSG.2022.3153693","National Research Foundation, Singapore, and EMA-ESG, Singapore(grant numbers:NRF2019NRF-CG002-002); Agency for Science, Technology and Research (A*SATR), Singapore, under its Singapore-Germany Academic-Industry (2 + 2) International Collaboration(grant numbers:A1990b0060); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9718520","Distributed control;peer-to-peer energy trading;cyber-security;hierarchical blockchain;smart grid","Blockchains;Microgrids;Peer-to-peer computing;Decentralized control;Pricing;Smart contracts;Game theory","","42","","44","IEEE","23 Feb 2022","","","IEEE","IEEE Journals"
"Advancements in Industrial Cyber-Physical Systems: An Overview and Perspectives","K. Zhang; Y. Shi; S. Karnouskos; T. Sauter; H. Fang; A. W. Colombo","Department of Mechanical Engineering, University of Victoria, Victoria, BC, Canada; Department of Mechanical Engineering, University of Victoria, Victoria, BC, Canada; SAP, Walldorf, Germany; Institute of Computer Technology, Vienna University of Technology, Vienna, Austria; Department of Mechanical Engineering, University of Kansas, Lawrence, KS, USA; Institute for Industrial Informatics, Automation and Robotics, University of Applied Sciences Emden/Leer, Emden, Germany",IEEE Transactions on Industrial Informatics,"9 Nov 2022","2023","19","1","716","729","Cyber-physical systems (CPSs) have attracted increasing attention in recent years due to their promise for substantial and long-term benefits to society, economy, environment, and citizens. In addition, the rapid advances in computing, communication, and storage technologies have resulted in a revolution in the information communication technology domain and domination in the industry context. The utilization of CPSs in industrial settings has led to industrial cyber-physical systems (ICPSs), which, in conjunction with the information-driven interactions, enables large-scale cooperation in industrial facilities and among all the stakeholders of the value chain. Hence, the research on ICPSs is essential, especially with respect to the engineering of such systems for industrial applications. This article presents an overview of recent developments in ICPSs. We first introduce the architecture of ICPSs. Then, we review the developments of ICPSs in relevant research domains. Finally, this article concludes by presenting some potential future research directions on ICPSs.","1941-0050","","10.1109/TII.2022.3199481","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9860057","Communication and networking;control;cyber security;industrial cyber-physical system (ICPS);information acquisition","Production;Industries;Internet;Informatics;Fourth Industrial Revolution;Process control;Cloud computing","","41","","100","IEEE","17 Aug 2022","","","IEEE","IEEE Journals"
"Automated Fine-Grained CPU Cap Control in Serverless Computing Platform","Y. K. Kim; M. R. HoseinyFarahabady; Y. C. Lee; A. Y. Zomaya","University of Sydney, Camperdown, Australia; University of Sydney, Camperdown, Australia; Macquarie University, Macquarie Park, Australia; University of Sydney, Camperdown, Australia",IEEE Transactions on Parallel and Distributed Systems,"8 May 2020","2020","31","10","2289","2301","Serverless computing has emerged as a new cloud computing execution model that liberates users and application developers from explicitly managing `physical' resources, leaving such a resource management burden to service providers. In this article, we study the problem of resource allocation for multi-tenant serverless computing platforms explicitly taking into account workload fluctuations including sudden surges. In particular, we investigate different root causes of performance degradation in these platforms where tenants (their applications) have different workload characteristics. To this end, we develop a fine-grained CPU cap control solution as a resource manager that dynamically adjusts CPU usage limit (or CPU cap) concerning applications with same/similar performance requirements, i.e., application groups. The adjustment of CPU caps applies primarily to co-located worker processes of serverless computing platforms to minimize resource contention, which is the major source of performance degradation. The actual adjustment decisions are made based on performance metrics (e.g., throttled time and queue length) using a group-aware scheduling algorithm. The extensive experimental results performed in our local cluster confirm that the proposed resource manager can effectively eliminate the burden of explicit reservation of computing capacity, even when fluctuations and sudden surges in the incoming workload exist. We measure the robustness of the proposed resource manager by comparing it with several heuristics which extensively used in practice, including the enhanced version of round robin and the least length queue scheduling policies, under various workload intensities driven by real-world scenarios. Notably, our resource manager outperforms other heuristics by decreasing skewness and average response time up to 44 and 94 percent, respectively, while it does not over-use the CPU resources.","1558-2183","","10.1109/TPDS.2020.2989771","Australian Research Council(grant numbers:DP190103710); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9076860","Serverless computing;virtualized cloud platforms;operating system process management;dynamic CPU scheduling;performance modeling","Interference;Round robin;Time factors;Resource management;Measurement;Computer architecture","","40","","29","IEEE","23 Apr 2020","","","IEEE","IEEE Journals"
"Disrupting Healthcare Silos: Addressing Data Volume, Velocity and Variety With a Cloud-Native Healthcare Data Ingestion Service","R. Ranchal; P. Bastide; X. Wang; A. Gkoulalas-Divanis; M. Mehra; S. Bakthavachalam; H. Lei; A. Mohindra","IBM Watson Health, Cambridge, MA, USA; IBM Watson Health, Cambridge, MA, USA; IBM Watson Health, Cambridge, MA, USA; IBM Watson Health, Cambridge, MA, USA; IBM Watson Health, Cambridge, MA, USA; IBM Watson Health, Cambridge, MA, USA; IBM Watson Health, Cambridge, MA, USA; IBM Watson Health, Cambridge, MA, USA",IEEE Journal of Biomedical and Health Informatics,"4 Nov 2020","2020","24","11","3182","3188","Healthcare enterprises are starting to adopt cloud computing due to its numerous advantages over traditional infrastructures. This has become a necessity because of the increased volume, velocity and variety of healthcare data, and the need to facilitate data correlation and large-scale analysis. Cloud computing infrastructures have the power to offer continuous acquisition of data from multiple heterogeneous sources, efficient data integration, and big data analysis. At the same time, security, availability, and disaster recovery are critical factors aiding towards the adoption of cloud computing. However, the migration of healthcare workloads to cloud is not straightforward due to the vagueness in healthcare data standards, heterogeneity and sensitive nature of healthcare data, and many regulations that govern its usage. This paper highlights the need for providing healthcare data acquisition using cloud infrastructures and presents the challenges, requirements, use-cases, and best practices for building a state-of-the-art healthcare data ingestion service on cloud.","2168-2208","","10.1109/JBHI.2020.3001518","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9113446","Cloud computing;electronic health record;healthcare information system;healthcare data ingestion","Medical services;Cloud computing;Biomedical imaging;Informatics;Standards organizations;Interoperability","Big Data;Cloud Computing;Computer Security;Delivery of Health Care;Eating;Humans","40","","27","IEEE","10 Jun 2020","","","IEEE","IEEE Journals"
"Reinforcement Learning-Empowered Mobile Edge Computing for 6G Edge Intelligence","P. Wei; K. Guo; Y. Li; J. Wang; W. Feng; S. Jin; N. Ge; Y. -C. Liang","Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Shanghai Key Laboratory of Multidimensional Information Processing, School of Communications and Electronics Engineering, East China Normal University, Shanghai, China; School of Information Science and Technology, Nantong University, Nantong, China; School of Information Science and Technology, Nantong University, Nantong, China; Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; National Key Laboratory on Communications, Center for Intelligent Networking and Communications, University of Electronic Science and Technology of China, Chengdu, China",IEEE Access,"24 Jun 2022","2022","10","","65156","65192","Mobile edge computing (MEC) is considered a novel paradigm for computation-intensive and delay-sensitive tasks in fifth generation (5G) networks and beyond. However, its uncertainty, referred to as dynamic and randomness, from the mobile device, wireless channel, and edge network sides, results in high-dimensional, nonconvex, nonlinear, and NP-hard optimization problems. Thanks to the evolved reinforcement learning (RL), upon iteratively interacting with the dynamic and random environment, its trained agent can intelligently obtain the optimal policy in MEC. Furthermore, its evolved versions, such as deep reinforcement learning (DRL), can achieve higher convergence speed efficiency and learning accuracy based on the parametric approximation for the large-scale state-action space. This paper provides a comprehensive research review on RL-enabled MEC and offers insight for development in this area. More importantly, associated with free mobility, dynamic channels, and distributed services, the MEC challenges that can be solved by different kinds of RL algorithms are identified, followed by how they can be solved by RL solutions in diverse mobile applications. Finally, the open challenges are discussed to provide helpful guidance for future research in RL training and learning MEC.","2169-3536","","10.1109/ACCESS.2022.3183647","National Key Research and Development Program of China(grant numbers:2020YFA0711301); National Natural Science Foundation of China(grant numbers:61941104,61901298,61922049,61801248,61771264); Natural Science Foundation of Tianjin(grant numbers:20JCQNJC00300); Science and Technology Development Fund of Tianjin Education Commission for Higher Education(grant numbers:2018KJ211); Shanghai Municipal Science and Technology Major Project(grant numbers:2018SHZDZX04); Tsinghua University–China Mobile Communications Group Company Ltd. Joint Institute; Natural Science Foundation of Jiangsu Province(grant numbers:BK20180943); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9797722","Mobile edge computing (MEC);network uncertainty;reinforcement learning (RL)","Task analysis;Optimization;Uncertainty;Servers;Ions;Resource management;Multi-access edge computing","","39","","185","CCBY","16 Jun 2022","","","IEEE","IEEE Journals"
"Serverless Blockchain-Enabled Architecture for IoT Societal Applications","S. Benedict","Indian Institute of Information Technology Kottayam, Valavoor, India",IEEE Transactions on Computational Social Systems,"10 Nov 2020","2020","7","5","1146","1158","IoT-enabled applications, such as cloud manufacturing, guided water quality or air quality analysis, energy-conscious societal applications, and smart agricultural economics, are designed using a blend of high-end computing technologies, such as cloud, edge, and fog. Smart cities and governmental authorities keep a keen eye out for implementing IoT applications in an automated/decentralized approach with enhanced security measures so that tens of thousands of users, including entrepreneurs, are benefited. Existing IoT architectures are prone to energy inefficiency or resource underutilization problems due to the avoidance of apt technologies, such as serverless computing. This article proposes to set forth a serverless blockchain-enabled IoT architecture for societal applications. It explores the existing IoT architectures and pinpoints the advantages of applying serverless blockchains on IoT architectures. In addition, the proposed IoT architecture is illustrated with a specific use case of IoT societal applications namely air quality monitoring for smart cities (AQMS). This article discloses how air quality sensor data from defective industries were securely transacted to blockchain networks surpassing from the three levels of computing namely edge, fog, and cloud while utilizing serverless and server-oriented functions. In addition, this article exposes a list of the most potent serverless functions that assist AQMS IoT societal applications in detail. The IoT architecture, discussed in this article, will enable innovations and research works for IoT developers and researchers.","2329-924X","","10.1109/TCSS.2020.3008995","AIC-IIITKottayam; BEL Projects; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9149700","Blockchain;cloud computing;decentralized services;IoT;societal applications","Cloud computing;Computer architecture;Air quality;Atmospheric measurements;Pollution measurement;Servers","","39","","39","IEEE","27 Jul 2020","","","IEEE","IEEE Journals"
"A Survey on Observability of Distributed Edge & Container-Based Microservices","M. Usman; S. Ferlin; A. Brunstrom; J. Taheri","Department of Computer Science, Karlstad University, Karlstad, Sweden; Department of Computer Science, Karlstad University, Karlstad, Sweden; Department of Computer Science, Karlstad University, Karlstad, Sweden; Department of Computer Science, Karlstad University, Karlstad, Sweden",IEEE Access,"24 Aug 2022","2022","10","","86904","86919","Edge computing is proposed as a technical enabler for meeting emerging network technologies (such as 5G and Industrial Internet of Things), stringent application requirements and key performance indicators (KPIs). It aims to alleviate the problems associated with centralized cloud computing systems by placing computational resources to the network’s edge, closer to the users. However, the complexity of distributed edge infrastructures grows when hosting containerized workloads as microservices, resulting in hard to detect and troubleshoot outages on critical use cases such as industrial automation processes. Observability aims to support operators in managing and operating complex distributed infrastructures and microservices architectures by instrumenting end-to-end runtime performance. To the best of our knowledge, no survey article has been recently proposed for distributed edge and containerized microservices observability. Thus, this article surveys and classifies state-of-the-art solutions from various communities. Besides surveying state-of-the-art, this article also discusses the observability concept, requirements, and design considerations. Finally, we discuss open research issues as well as future research directions that will inspire additional research in this area.","2169-3536","","10.1109/ACCESS.2022.3193102","Knowledge Foundation of Sweden (KKS) through the Synergy Project AIDA—A Holistic AI-Driven Networking and Processing Framework for Industrial IoT(grant numbers:Rek:20200067); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9837035","Automation;cloud-native;containers;DevOps;edge;Industrial Internet of Things (IIoT);microservices;monitoring;observability;operation;software-defined infrastructure;visibility","Monitoring;Observability;Cloud computing;Microservice architectures;Computer architecture;Industrial Internet of Things;Image edge detection","","38","","106","CCBY","21 Jul 2022","","","IEEE","IEEE Journals"
"Network Functions Virtualization: The Long Road to Commercial Deployments","A. U. Rehman; R. L. Aguiar; J. P. Barraca","Instituto de Telecomunicações, Aveiro, Portugal; Instituto de Telecomunicações, Aveiro, Portugal; Instituto de Telecomunicações, Aveiro, Portugal",IEEE Access,"20 May 2019","2019","7","","60439","60464","Network operators are under pressure to offer efficient network-based services while keeping service deployment costs to a minimum. Network functions virtualization (NFV) can potentially revolutionize network-based services bringing low-deployment costs for network operators. The NFV has been introduced to ultimately extend the non-proprietary and open-standard-based model to network and service deployments, significant improvements to today’s proprietary locked implementations. Notwithstanding the continuous efforts of both academia and industry to support the NFV paradigm, the current NFV solutions offered are still in its infancy. In this survey, we provide a detailed background of NFV to establish a comprehensive understanding of the subject, ranging from the basics to more advanced topics. Moreover, we offer a comprehensive overview of the NFV main concepts, standardization efforts, the benefits of NFV, and discussions of the NFV architecture as defined by the European telecommunications standardization institute (ETSI). Furthermore, we discuss the NFV applicability and current open source projects. We then highlight NFV requirements, design considerations, and developmental architectural impairments and barriers to commercial NFV deployments. Finally, we conclude enumerating future directions for NFV development.","2169-3536","","10.1109/ACCESS.2019.2915195","FCT/MEC through national funds; FEDER - PT2020 partnership agreement(grant numbers:UID/EEA/50008/2019); Fundação para a Ciência e a Tecnologia(grant numbers:PD/BD/113822/2015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8708314","Network functions virtualization;virtual network functions;server virtualization;hypervisors;containers;Unikernels;network management and orchestration;network programmability;network softwarization","Virtualization;Telecommunications;Hardware;Software;Roads;Computer architecture;Overlay networks","","38","","202","OAPA","7 May 2019","","","IEEE","IEEE Journals"
"Multi-Objective Optimization for Virtual Machine Allocation and Replica Placement in Virtualized Hadoop","C. Guerrero; I. Lera; B. Bermejo; C. Juiz","Computer Science Department, Balearic Islands University, Palma, Spain; Computer Science Department, Balearic Islands University, Palma, Spain; Computer Science Department, Balearic Islands University, Palma, Spain; Computer Science Department, Balearic Islands University, Palma, Spain",IEEE Transactions on Parallel and Distributed Systems,"9 Oct 2018","2018","29","11","2568","2581","Resource management is a key factor in the performance and efficient utilization of cloud systems, and many research works have proposed efficient policies to optimize such systems. However, these policies have traditionally managed the resources individually, neglecting the complexity of cloud systems and the interrelation between their elements. To illustrate this situation, we present an approach focused on virtualized Hadoop for a simultaneous and coordinated management of virtual machines and file replicas. Specifically, we propose determining the virtual machine allocation, virtual machine template selection, and file replica placement with the objective of minimizing the power consumption, physical resource waste, and file unavailability. We implemented our solution using the non-dominated sorting genetic algorithm-II, which is a multi-objective optimization algorithm. Our approach obtained important benefits in terms of file unavailability and resource waste, with overall improvements of approximately 400 and 170 percent compared to three other optimization strategies. The benefits for the power consumption were smaller, with an improvement of approximately 1.9 percent.","1558-2183","","10.1109/TPDS.2018.2837743","Spanish Government; Agencia Estatal de Investigación; European Commission; Fondo Europeo de Desarrollo Regional(grant numbers:TIN2017-88547-P); MINECO/AEI/FEDER; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8360490","Virtual machine allocation;file replica placement;hadoop;evolutionary computing and genetic algorithms","Resource management;Power demand;Genetic algorithms;Optimization;Virtual machining;Cloud computing;Sorting","","38","","59","IEEE","17 May 2018","","","IEEE","IEEE Journals"
"Autonomous Shuttle-as-a-Service (ASaaS): Challenges, Opportunities, and Social Implications","A. Bucchiarone; S. Battisti; A. Marconi; R. Maldacea; D. C. Ponce","Fondazione Bruno Kessler, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy; I-Mobility Garage, Verona, Italy; Navya Tech, Lyon, France",IEEE Transactions on Intelligent Transportation Systems,"31 May 2021","2021","22","6","3790","3799","Providing mobility services effectively to residents and visitors is a complex socio-technical system task to city public managers. Smart mobility systems aim to support the efficient exploitation of city transport facilities and sustainable mobility within the urban environment. People need to travel quickly and conveniently between locations at different scales, ranging from a few blocks within a city to a journey across cities. At the same time, goods need to be timely delivered, considering both the users and the businesses' needs. Several cities indicated an interest in using Autonomous Vehicles (AV) for the “last-mile” mobility services in the last few years. With them, it seems to be easier to get people and goods around using fewer vehicles. In this context, Autonomous Shuttles (AS) are beginning to be thought of as a new mobility/delivery service into the city center where narrow streets are not easily served by traditional buses. They allow them to perform critical areas with minimal new infrastructure and reduce noise and pollution. The article analyses the state-of-art on autonomous shuttles by proposing four application scenarios targeting the last-mile delivery of goods, the tourist experiences, and the shared and integrated mobility. Furthermore, we contribute with the proposition of the Autonomous Shuttles-as-a service (ASaaS) concept as the key pillar for the realization of innovative and sustainable proximity mobility. Our research proposed new research challenges for ASaaS, and we discuss social implications and governance challenges that consider user engagement and sustainability. It also recommended extending new research to focus on simulation and machine learning techniques for last-mile mobility planning and explore the journeys tracking certification via artificial intelligence and blockchain-based techniques.","1558-0016","","10.1109/TITS.2020.3025670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9210552","Smart mobility;autonomous shuttles;proximity mobility;last mile delivery;mobility services","Autonomous vehicles;Biological system modeling;Automobiles;Safety;Smart cities","","37","","62","IEEE","1 Oct 2020","","","IEEE","IEEE Journals"
"Federated Learning for the Healthcare Metaverse: Concepts, Applications, Challenges, and Future Directions","A. K. Bashir; N. Victor; S. Bhattacharya; T. Huynh-The; R. Chengoden; G. Yenduri; P. K. R. Maddikunta; Q. -V. Pham; T. R. Gadekallu; M. Liyanage","Department of Computing and Mathematics, Manchester Metropolitan University, Manchester, U.K.; School of Information Technology, Vellore Institute of Technology, Vellore, India; School of Information Technology, Vellore Institute of Technology, Vellore, India; Department of Computer and Communications Engineering, Ho Chi Minh City University of Technology and Education, Ho Chi Minh City, Vietnam; School of Information Technology, Vellore Institute of Technology, Vellore, India; School of Information Technology, Vellore Institute of Technology, Vellore, India; School of Information Technology, Vellore Institute of Technology, Vellore, India; School of Computer Science and Statistics, Trinity College Dublin, The University of Dublin, Dublin 2, Ireland; School of Information Technology, Vellore Institute of Technology, Vellore, India; School of Computer Science, University College Dublin, D04 V1W8, Ireland",IEEE Internet of Things Journal,"11 Dec 2023","2023","10","24","21873","21891","Recent technological advancements have considerably improved healthcare systems to provide various intelligent services, improving life quality. The Metaverse, often described as the next evolution of the Internet, helps the users interact with each other and the environment, thus offering a seamless connection between the virtual and physical worlds. Additionally, the Metaverse, by integrating emerging technologies, such as artificial intelligence (AI), cloud edge computing, Internet of Things (IoT), blockchain, and semantic communications, can potentially transform many vertical domains in general and the healthcare sector (healthcare Metaverse) in particular. The healthcare Metaverse holds huge potential to revolutionize the development of intelligent healthcare systems, thus presenting new opportunities for significant advancements in healthcare delivery, personalized healthcare experiences, medical education, collaborative research, and so on. However, various challenges are associated with the realization of the healthcare Metaverse, such as privacy, interoperability, data management, and security. Federated learning (FL), a new branch of AI, opens up enormous opportunities to deal with the aforementioned challenges in the healthcare Metaverse by exploiting the data and computing resources available at the distributed devices. This motivated us to present a survey on adopting FL for the healthcare Metaverse. Initially, we present the preliminaries of IoT-based healthcare systems, FL in conventional healthcare, and the healthcare Metaverse. Furthermore, the benefits of the FL in the healthcare Metaverse are discussed. Subsequently, we discuss the several applications of FL-enabled healthcare Metaverse, including medical diagnosis, patient monitoring, medical education, infectious disease, and drug discovery. Finally, we highlight the significant challenges and potential solutions toward realizing FL in the healthcare Metaverse.","2327-4662","","10.1109/JIOT.2023.3304790","European Union in SPATIAL(grant numbers:101021808); CONFIDENTIAL-6G(grant numbers:101096435); Science Foundation Ireland under CONNECT Phase 2 Projects(grant numbers:13/RC/2077_P2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10215363","Cobots;digital twins;disease diagnosis;federated learning (FL);healthcare;healthcare metaverse;metaverse","Medical services;Metaverse;Data privacy;Medical diagnostic imaging;Artificial intelligence;Collaboration;Security","","36","","111","IEEE","14 Aug 2023","","","IEEE","IEEE Journals"
"Smart Audio Sensors in the Internet of Things Edge for Anomaly Detection","M. Antonini; M. Vecchio; F. Antonelli; P. Ducange; C. Perera","OpenIoT Research Unit, FBK CREATE-NET, Povo (TN), Italy; OpenIoT Research Unit, FBK CREATE-NET, Povo (TN), Italy; OpenIoT Research Unit, FBK CREATE-NET, Povo (TN), Italy; SMARTEST Research Centre, ECampus University, Novedrate (CO), Italy; School of Computer Science and Informatics, Cardiff University, Cardiff, U.K.",IEEE Access,"28 Nov 2018","2018","6","","67594","67610","Everyday objects are becoming smart enough to directly connect to other nearby and remote objects and systems. These objects increasingly interact with machine learning applications that perform feature extraction and model inference in the cloud. However, this approach poses several challenges due to latency, privacy, and dependency on network connectivity between data producers and consumers. To alleviate these limitations, computation should be moved as much as possible towards the IoT edge, that is on gateways, if not directly on data producers. In this paper, we propose a design framework for smart audio sensors able to record and pre-process raw audio streams, before wirelessly transmitting the computed audio features to a modular IoT gateway. In this paper, an anomaly detection algorithm executed as a micro-service is capable of analyzing the received features, hence detecting audio anomalies in real-time. First, to assess the effectiveness of the proposed solution, we deployed a real smart environment showcase. More in detail, we adopted two different anomaly detection algorithms, namely Elliptic Envelope and Isolation Forest, that were purposely trained and deployed on an affordable IoT gateway to detect anomalous sound events happening in an office environment. Then, we numerically compared both the deployments, in terms of end-to-end latency and gateway CPU load, also deriving some ideal capacity bounds.","2169-3536","","10.1109/ACCESS.2018.2877523","Horizon 2020 Framework Programme(grant numbers:688088); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8502761","Anomaly detection;digital signal processing;edge computing;embedded devices;Internet of Things;IoT gateway;machine learning;novelty detection;open-source platforms;outlier detection","Anomaly detection;Logic gates;Intelligent sensors;Synthetic aperture sonar;Internet of Things;Software","","36","","66","CCBY","23 Oct 2018","","","IEEE","IEEE Journals"
"A Survey on Mobility of Edge Computing Networks in IoT: State-of-the-Art, Architectures, and Challenges","F. Shirin Abkenar; P. Ramezani; S. Iranmanesh; S. Murali; D. Chulerttiyawong; X. Wan; A. Jamalipour; R. Raad","WiNG Lab, School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; WiNG Lab, School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; School of Electrical, Computer and Telecommunication Engineering, University of Wollongong, Wollongong, NSW, Australia; WiNG Lab, School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; WiNG Lab, School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; WiNG Lab, School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; WiNG Lab, School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; School of Electrical, Computer and Telecommunication Engineering, University of Wollongong, Wollongong, NSW, Australia",IEEE Communications Surveys & Tutorials,"21 Nov 2022","2022","24","4","2329","2365","Edge computing leverages computing resources closer to the end-users at the edge of the network, rather than distant cloud servers in the centralized IoT architecture. Edge computing nodes (ECNs), experience less transmission latency and usually save on energy while network overheads are mitigated. The ECNs can be fixed or mobile in their positions. We will focus on mobile ECNs in this survey. This paper presents a comprehensive survey on mobile ECNs and identifies some open research questions. In particular, mobile ECNs are classified into four categories, namely aerial, ground vehicular, spatial, and maritime nodes. For each specific group, any mutual basic terms used in the state-of-the-art are described, different types of nodes employed in the group are reviewed, the general network architecture is introduced, the existing methods and algorithms are studied, and the challenges that the group is scrimmaging against are explored. Moreover, the integrated architectures are surveyed, wherein two different categories of the aforementioned nodes jointly play the role of ECNs in the network. Finally, the research gaps, that are yet to be filled in the area of mobile ECNs, are discussed along with directions for future research and investigation in this promising area.","1553-877X","","10.1109/COMST.2022.3211462","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910565","Mobile edge computing nodes;aerial nodes;ground vehicular nodes;spatial nodes;maritime vessels;architectures;challenges;applications","Cloud computing;Task analysis;Internet of Things;Edge computing;Computer architecture;Servers;Delays","","36","","171","IEEE","4 Oct 2022","","","IEEE","IEEE Journals"
"Combining Federated Learning and Edge Computing Toward Ubiquitous Intelligence in 6G Network: Challenges, Recent Advances, and Future Directions","Q. Duan; J. Huang; S. Hu; R. Deng; Z. Lu; S. Yu","Information Sciences and Technology Department, The Pennsylvania State University, Abington, PA, USA; Electrical Engineering and Computer Science Department, South Dakota State University, Brookings, SD, USA; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, University of Technology Sydney, Sydney, NSW, Australia",IEEE Communications Surveys & Tutorials,"21 Nov 2023","2023","25","4","2892","2950","Full leverage of the huge volume of data generated on a large number of user devices for providing intelligent services in the 6G network calls for Ubiquitous Intelligence (UI). A key to developing UI lies in the involvement of the large number of network devices, which contribute their data to collaborative Machine Learning (ML) and provide their computational resources to support the learning process. Federated Learning (FL) is a new ML method that enables data owners to collaborate in model training without exposing private data, which allows user devices to contribute their data to developing UI. Edge computing deploys cloud-like capabilities at the network edge, which enables network devices to offer their computational resources for supporting FL. Therefore, a combination of FL and edge computing may greatly facilitate the development of ubiquitous intelligence in the 6G network. In this article, we present a comprehensive survey of the recent developments in technologies for combining FL and edge computing with a holistic vision across the fields of FL and edge computing. We conduct our survey from both the perspective of an FL framework deployed in an edge computing environment (FL in Edge) and the perspective of an edge computing system providing a platform for FL (Edge for FL). From the FL in Edge perspective, we first identify the main challenges to FL in edge computing and then survey the representative technical strategies for addressing the challenges. From the Edge for FL perspective, we first analyze the key requirements for edge computing to support FL and then review the recent advances in edge computing technologies that may be exploited to meet the requirements. Then we discuss open problems and identify some possible directions for future research on combining FL and edge computing, with the hope of arousing the research community’s interest in this emerging and exciting interdisciplinary field.","1553-877X","","10.1109/COMST.2023.3316615","Shanghai Science and Technology Project(grant numbers:22510761000); National Natural Science Foundation of China(grant numbers:61873309,92046024,92146002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10258360","Federated learning;edge computing;ubiquitous intelligence;6G network","Edge computing;Surveys;6G mobile communication;Training;Computational modeling;Internet of Things;Federated learning","","36","","312","IEEE","22 Sep 2023","","","IEEE","IEEE Journals"
"Is Fragmentation a Threat to the Success of the Internet of Things?","M. Aly; F. Khomh; Y. -G. Guéhéneuc; H. Washizaki; S. Yacout","SWAT Laboratory, Polytechnique Montréal, Montreal, QC, Canada; SWAT Laboratory, Polytechnique Montréal, Montreal, QC, Canada; Ptidej Team, Concordia University, Montreal, QC, Canada; Reliable Software Engineering Laboratory, Waseda University, Tokyo, Japan; MAGI, Polytechnique Montréal, Montreal, QC, Canada",IEEE Internet of Things Journal,"24 Feb 2019","2019","6","1","472","487","Internet of Things (IoT) aims to bring connectivity to almost every objects, i.e., things, found in the physical space. It extends connectivity to everyday things, however, such increase in the connectivity creates many prominent challenges. Context: Generally, IoT opens the door for new applications for machine-to-machine and human-to-human communications. The current trend of collaborating, distributed teams through the Internet, mobile communications, and autonomous entities, e.g., robots, is the first phase of the IoT to develop and deliver diverse services and applications. However, such collaborations is threatened by the fragmentation that we witness in the industry nowadays as it brings difficulty to integrate the diverse technologies of the various objects found in IoT systems. Diverse technologies induce interoperability issues while designing and developing various services and applications, hence, limiting the possibility of reusing the data, more specifically, the software (including frameworks, firmware, applications programming interfaces, and user interfaces) as well as of facing issues, like security threats and bugs, when developing new services or applications. Different aspects of handling data collection ranging from discovering smart sensors for data collection, integrating and applying reasoning on them must be available to provide interoperability and flexibility to the diverse objects interacting in the system. However, such approaches are bound to be challenged in future IoT scenarios as they bring substantial performance impairments in settings with the very large number of collaborating devices and technologies. Objective: We raise the awareness of the community about the lack of interoperability among technologies developed for IoT and challenges that their integration poses. We also provide guidelines for researchers and practitioners interested in connecting IoT networks and devices to develop services and applications. Method: We apply the methods advocated by the evidence-based software engineering paradigm. This paradigm and its core tool, the systematic literature review (SLR), were introduced to the software-engineering research community early 2004 to help researchers and industry systematically and objectively gather and aggregate evidences about different topics. In this paper, we conduct an SLR of both IoT interoperability issues and the state-of-practice of IoT technologies in the industry, highlighting the integration challenges related to the IoT that have significantly shifted the landscape of Internet-based collaborative services and applications nowadays. Results: Our SLR identifies a number of studies from journals, conferences, and workshops with the highest quality in the field. This SLR reports different trends, including frameworks and technologies, for the IoT for better comprehension of the paradigm and discusses the integration and interoperability challenges across the different layers of this technology while shedding light on the current IoT state-of-practice. It also discusses some future research directions for the community.","2327-4662","","10.1109/JIOT.2018.2863180","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8424819","Internet of Things (IoT);interoperability;models;protocols;standards;technologies and frameworks","Interoperability;Internet of Things;Biological system modeling;Machine-to-machine communications;Wireless sensor networks;Data models","","34","","130","IEEE","3 Aug 2018","","","IEEE","IEEE Journals"
"MLOps: A Taxonomy and a Methodology","M. Testi; M. Ballabio; E. Frontoni; G. Iannello; S. Moccia; P. Soda; G. Vessio","Integrated Research Centre, Università Campus Bio-Medico di Roma, Rome, Italy; DeepLearningItalia, Bergamo, Italy; Department of Political Sciences, Communication and International Relations, VRAI Laboratory, Università degli Studi di Macerata, Macerata, Italy; Department of Engineering, Unit of Computer Systems and Bioinformatics, Università Campus Bio-Medico di Roma, Rome, Italy; The BioRobotics Institute, Scuola Superiore Sant’Anna, Pisa, Italy; Department of Engineering, Unit of Computer Systems and Bioinformatics, Università Campus Bio-Medico di Roma, Rome, Italy; Department of Computer Science, Università degli Studi di Bari Aldo Moro, Bari, Italy",IEEE Access,"20 Jun 2022","2022","10","","63606","63618","Over the past few decades, the substantial growth in enterprise-data availability and the advancements in Artificial Intelligence (AI) have allowed companies to solve real-world problems using Machine Learning (ML). ML Operations (MLOps) represents an effective strategy for bringing ML models from academic resources to useful tools for solving problems in the corporate world. The current literature on MLOps is still mostly disconnected and sporadic. In this work, we review the existing scientific literature and we propose a taxonomy for clustering research papers on MLOps. In addition, we present methodologies and operations aimed at defining an ML pipeline to simplify the release of ML applications in the industry. The pipeline is based on ten steps: business problem understanding, data acquisition, ML methodology, ML training & testing, continuous integration, continuous delivery, continuous training, continuous monitoring, explainability, and sustainability. The scientific and business interest and the impact of MLOps have grown significantly over the past years: the definition of a clear and standardized methodology for conducting MLOps projects is the main contribution of this paper.","2169-3536","","10.1109/ACCESS.2022.3181730","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9792270","MLOps;continuous monitoring;continuous integration;continuous delivery;continuous training;XAI;sustainability","Pipelines;Production;Monitoring;Training;Business;Automation;Surgery","","34","","98","CCBY","9 Jun 2022","","","IEEE","IEEE Journals"
"Bi-Objective Optimization for Energy Aware Internet of Things Service Composition","O. Alsaryrah; I. Mashal; T. -Y. Chung","Yuan Ze University, Chung-Li, Taoyuan, TW; Computer Science Department, Aqaba University of Technology, Aqaba, Jordan; Yuan Ze University, Chung-Li, Taoyuan, TW",IEEE Access,"5 Jun 2018","2018","6","","26809","26819","In recent years, service-oriented-based Internet of Things (IoT) has received massive attention from research and industry. Integrating and composing smart objects functionalities or their services is required to create and promote more complex IoT applications with advanced features. When many smart objects are deployed, selecting the most appropriate set of smart objects to compose a service by considering both energy and quality of service (QoS) is an essential and challenging task. In this paper, we reduced the problem of finding an optimal balance between QoS level and the consumed energy of the IoT service composition to a bi-objective shortest path optimization (BSPO) problem and used an exact algorithm named pulse to solve the problem. The BSPO has two objectives, minimizing the QoS including execution time, network latency, and service price, and minimize the energy consumption of the composite service. Experimental evaluations show that the proposed approach has short execution time in various complex service profiles. Meanwhile, it can obtain good performance in energy consumption and thus network lifetime while maintaining a reasonable QoS level.","2169-3536","","10.1109/ACCESS.2018.2836334","Ministry of Science and Technology, Taiwan(grant numbers:MOST 106-2221-E-155-014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8359282","IoT services;energy efficiency;service composition","Quality of service;Energy consumption;Optimization;Task analysis;Energy efficiency;Internet of Things;Wireless sensor networks","","34","","29","OAPA","15 May 2018","","","IEEE","IEEE Journals"
"Multi-Agent Based Autonomic Network Management Architecture","S. T. Arzo; R. Bassoli; F. Granelli; F. H. P. Fitzek","Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Deutsche Telekom Chair of Communication Networks, Institute of Communication Technology, Faculty of Electrical and Computer Engineering, Technische Universität Dresden, Dresden, Germany; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Deutsche Telekom Chair of Communication Networks, Institute of Communication Technology, Faculty of Electrical and Computer Engineering, Technische Universität Dresden, Dresden, Germany",IEEE Transactions on Network and Service Management,"8 Sep 2021","2021","18","3","3595","3618","The advent of network softwarization is enabling multiple innovative solutions through software-defined networking (SDN) and network function virtualization (NFV). Specifically, network softwarization paves the way for autonomic and intelligent networking, which has gained popularity in the research community. Along with the arrival of 5G and beyond, which interconnects billions of devices, the complexity of network management is significantly increasing both investments and operational costs. Autonomic networking is the creation of self-organizing, self-managing, and self-protecting networks, to afford the network management complexes and heterogeneous networks. To achieve full network automation, various aspects of networking need to be addressed. So, this article proposes a novel architecture for the multi-agent-based network automation of the network management system (MANA-NMS). The architecture rely on network function atomization, which defines atomic decision-making units. Such units could represent virtual network functions. These atomic units are autonomous and adaptive. First, the article presents a theoretical discussion of the challenges arisen by automating the decision-making process. Next, the proposed multi-agent system is presented along with its mathematical modeling. Finally, MANA-NMS architecture is mathematically evaluated from functionality, reliability, latency, and resource consumption performance perspectives.","1932-4537","","10.1109/TNSM.2021.3059752","NATO Science for Peace and Security (SPS) Programme in the framework of the project SPS G5428 “Dynamic Architecture based on UAVs Monitoring for Border Security and Safety.”; German Research Foundation (DFG, Deutsche Forschungsgemeinschaft) as part of Germany’s Excellence Strategy Ű EXC 2050/1 Ű Project ID 390696704 Ű Cluster of Excellence “Centre for Tactile Internet with Human-in-the-Loop” (CeTI) of Technische Universität Dresden; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354865","Autonomic networking;multi-agent system;network management;network function virtualization;software defined networking;C-RAN;5G;future networks","Automation;Computer architecture;Software;Network function virtualization;Quality of service;Monitoring;Mathematical model","","33","","88","IEEE","16 Feb 2021","","","IEEE","IEEE Journals"
"AI and 6G Into the Metaverse: Fundamentals, Challenges and Future Research Trends","M. Zawish; F. A. Dharejo; S. A. Khowaja; S. Raza; S. Davy; K. Dev; P. Bellavista","Walton Institute for Information and Communication Systems Science, South East Technological University, Waterford, Ireland; Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, UAE; Faculty of Engineering and Technology, University of Sindh, Jamshoro, Pakistan; Department of Electronic Engineering, Quaid-e-Awam University of Engineering, Science and Technology, Larkana, Pakistan; Centre for Sustainable Digital Technologies, Technological University Dublin, Dublin, Ireland; Department of Computer Science, Munster Technological University, Cork, Ireland; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy",IEEE Open Journal of the Communications Society,"29 Jan 2024","2024","5","","730","778","Since Facebook was renamed Meta, a lot of attention, debate, and exploration have intensified about what the Metaverse is, how it works, and the possible ways to exploit it. It is anticipated that Metaverse will be a continuum of rapidly emerging technologies, usecases, capabilities, and experiences that will make it up for the next evolution of the Internet. Several researchers have already surveyed the literature on artificial intelligence (AI) and wireless communications in realizing the Metaverse. However, due to the rapid emergence and continuous evolution of technologies, there is a need for a comprehensive and in-depth survey of the role of AI, 6G, and the nexus of both in realizing the immersive experiences of Metaverse. Therefore, in this survey, we first introduce the background and ongoing progress in augmented reality (AR), virtual reality (VR), mixed reality (MR) and spatial computing, followed by the technical aspects of AI and 6G. Then, we survey the role of AI in the Metaverse by reviewing the state-of-the-art in deep learning, computer vision, and Edge AI to extract the requirements of 6G in Metaverse. Next, we investigate the promising services of B5G/6G towards Metaverse, followed by identifying the role of AI in 6G networks and 6G networks for AI in support of Metaverse applications, and the need for sustainability in Metaverse. Finally, we enlist the existing and potential applications, usecases, and projects to highlight the importance of progress in the Metaverse. Moreover, in order to provide potential research directions to researchers, we underline the challenges, research gaps, and lessons learned identified from the literature review of the aforementioned technologies.","2644-125X","","10.1109/OJCOMS.2024.3349465","Science Foundation Ireland(grant numbers:21/FFP-A/9174); Science Foundation Ireland and the Department of Agriculture, Food and Marine on behalf of the Government of Ireland VistaMilk Research Centre(grant numbers:16/RC/3835); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10415393","Metaverse;5G;6G;AI;cloud and edge computing;AR/VR/XR;spatial computing","6G mobile communication;Surveys;Wireless communication;Computer vision;Telepresence;Metaverse;Artificial intelligence","","33","","307","CCBYNCND","29 Jan 2024","","","IEEE","IEEE Journals"
"A Cooperative Coevolution Genetic Programming Hyper-Heuristics Approach for On-Line Resource Allocation in Container-Based Clouds","B. Tan; H. Ma; Y. Mei; M. Zhang","School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand",IEEE Transactions on Cloud Computing,"2 Sep 2022","2022","10","3","1500","1514","Containers are lightweight and provide the potential to reduce more energy consumption of data centers than Virtual Machines (VMs) in container-based clouds. On-line resource allocation is the most common operation in clouds. However, the on-line Resource Allocation in Container-based clouds (RAC) is new and challenging because of its two-level architecture, i.e., the allocations of containers to VMs and the allocation of VMs to physical machines. These two allocations interact with each other, and hence cannot be made separately. Since on-line container allocation requires a real-time response, most current allocation techniques rely on heuristics (e.g., First Fit and Best Fit), which do not consider the comprehensive information such as workload patterns and VM types. As a result, resources are not used efficiently and the energy consumption is not sufficiently optimized. We first propose a novel model of the on-line RAC problem with the consideration of VM overheads, VM types and an affinity constraint. Then, we design a Cooperative Coevolution Genetic Programming (CCGP) hyper-heuristic approach to solve the RAC problem, named CCGP-RAC. CCGP-RAC can learn the workload patterns and VM types from historical workload traces and generate allocation rules. The experiments show significant improvement in energy consumption compared to the state-of-the-art algorithms.","2168-7161","","10.1109/TCC.2020.3026338","New Zealand Marsden Fund(grant numbers:VUW1510,VUW1614); Royal Society of New Zealand; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9205601","Container-based clouds;container allocation;energy efficiency;genetic programming;hyper-heuristic","Resource management;Containers;Cloud computing;Energy consumption;Genetic programming;Mathematical model;Data centers","","33","","62","IEEE","24 Sep 2020","","","IEEE","IEEE Journals"
"Management and Orchestration of Edge Computing for IoT: A Comprehensive Survey","Y. Chiang; Y. Zhang; H. Luo; T. -Y. Chen; G. -H. Chen; H. -T. Chen; Y. -J. Wang; H. -Y. Wei; C. -T. Chou","Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Communication Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Communication Engineering, National Taiwan University, Taipei, Taiwan",IEEE Internet of Things Journal,"7 Aug 2023","2023","10","16","14307","14331","With the development of telecommunication technologies and the proliferation of network applications in the past decades, the traditional cloud network architecture becomes unable to accommodate such demands due to the heavy burden on the backhaul links and long latency. Therefore, edge computing, which brings network functions close to end-users by providing caching, computing and communication resources at network edges, turns into a promising paradigm. Benefit from its nature, edge computing enables emerging scenarios and use cases, such as augmented reality (AR) and Internet of Things (IowT). However, it also creates complexities to efficiently orchestrate heterogeneous services and manage distributed resources in the edge network. In this survey, we make a comprehensive review of the research efforts on service orchestration and resource management for edge computing. We first give an overview of edge computing, including architectures, advantages, enabling technologies and standardization. Next, a comprehensive survey of state-of-the-art techniques in the management and orchestration of edge computing is presented. Subsequently, the state-of-the-art research on the infrastructure of edge computing is discussed in various aspects. Finally, open research challenges and future directions are presented as well.","2327-4662","","10.1109/JIOT.2023.3245611","National Science and Technology Council (NSTC) of Taiwan(grant numbers:111-2221-E-002-097-MY3,111-2622-8-002-030-,109-2221-E-002-148-MY2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10045724","Computation offloading;content caching;edge computing;resource management;virtual network embedding (VNE)","Edge computing;Cloud computing;Computer architecture;Internet of Things;Resource management;Wireless fidelity;Task analysis","","32","","204","IEEE","16 Feb 2023","","","IEEE","IEEE Journals"
"Toward Greener 5G and Beyond Radio Access Networks—A Survey","L. M. P. Larsen; H. L. Christiansen; S. Ruepp; M. S. Berger","Department of Radio Access Networks, TDC Net, Copenhagen, Denmark; Department of Electrical and Photonics Engineering, Technical University of Denmark, Lyngby, Denmark; Department of Electrical and Photonics Engineering, Technical University of Denmark, Lyngby, Denmark; Department of Electrical and Photonics Engineering, Technical University of Denmark, Lyngby, Denmark",IEEE Open Journal of the Communications Society,"28 Mar 2023","2023","4","","768","797","Mobile network traffic is increasing and so is the energy consumption. The Radio Access Network (RAN) part is responsible for the largest share of the mobile network energy consumption, and thus; an important consideration when expanding mobile networks to meet traffic demands. This work analyses how the energy consumption of future mobile networks can be minimised by using the right RAN architecture, share the network with other operators and implementing the most efficient energy minimising technologies in the RAN. It is explored how the different approaches can be realised in real life networks as well as the research state of the art is highlighted. Furthermore, this work provides an overview of future research directions for 6G energy saving potentials. Different energy saving contributions are evaluated by a common methodology for more realistic comparison, based on the potential energy saving of the overall mobile network consumption. Results show that implementing selected technologies and architectures, the mobile network overall energy consumption can be reduced by approximately 30%, corresponding to almost half of the RAN energy consumption. Following this, a set of guidelines towards an energy optimised mobile network is provided, proposing changes to be made initially and in the longer run for brownfield network operators as well as a target network for greenfield network operators.","2644-125X","","10.1109/OJCOMS.2023.3257889","Innovationsfonden(grant numbers:1045-00047B); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10076806","Green RAN;C-RAN;vRAN;O-RAN;MOCN;MORAN;network slicing;5G;6G;AI","Energy consumption;5G mobile communication;Energy efficiency;Radio access networks;Computer architecture;6G mobile communication;Optimization","","32","","196","CCBY","20 Mar 2023","","","IEEE","IEEE Journals"
"Rusty: Runtime Interference-Aware Predictive Monitoring for Modern Multi-Tenant Systems","D. Masouros; S. Xydis; D. Soudris","Department of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; Department of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; Department of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece",IEEE Transactions on Parallel and Distributed Systems,"18 Aug 2020","2021","32","1","184","198","Modern micro-service and container-based cloud-native applications have leveraged multi-tenancy as a first class system design concern. The increasing number of co-located services/workloads into server facilities stresses resource availability and system capability in an unconventional and unpredictable manner. To efficiently manage resources in such dynamic environments, run-time observability and forecasting are required to capture workload sensitivities under differing interference effects, according to applied co-location scenarios. While several research efforts have emerged on interference-aware performance modelling, they are usually applied at a very coarse-grained manner e.g., estimating the overall performance degradation of an application, thus failing to effectively quantify, predict or provide educated insights on the impact of continuous runtime interference on per-resource allocations. In this paper, we present Rusty, a predictive monitoring system that leverages the power of Long Short-Term Memory networks to enable fast and accurate runtime forecasting of key performance metrics and resource stresses of cloud-native applications under interference. We evaluate Rusty under a diverse set of interference scenarios for a plethora of representative cloud workloads, showing that Rusty i) achieves extremely high prediction accuracy, average R2 value of 0.98, ii) enables very deep prediction horizons retaining high accuracy, e.g., R2 of around 0.99 for a horizon of 1 sec ahead and around 0.94 for an horizon of 5 sec ahead, while iii) satisfying, at the same time, the strict latency constraints required to make Rusty practical for continuous predictive monitoring at runtime.","1558-2183","","10.1109/TPDS.2020.3013948","European Union's Horizon 2020 Research and Innovation programme(grant numbers:825061); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9158547","predictive monitoring;system predictability;LSTM networks;interference aware;multi-tenant systems","Monitoring;Interference;Runtime;Resource management;Measurement;Degradation;Servers","","32","","88","IEEE","4 Aug 2020","","","IEEE","IEEE Journals"
"On Construction of Sensors, Edge, and Cloud (iSEC) Framework for Smart System Integration and Applications","E. Kristiani; C. -T. Yang; C. -Y. Huang; P. -C. Ko; H. Fathoni","Department of Industrial Engineering and Enterprise Information, Tunghai University, Taichung, Taiwan; Department of Computer Science, Tunghai University, Taichung, Taiwan; Department of Industrial Engineering and Enterprise Information, Tunghai University, Taichung, Taiwan; Department of Computer Science, Tunghai University, Taichung, Taiwan; Department of Industrial Engineering and Enterprise Information, Tunghai University, Taichung, Taiwan",IEEE Internet of Things Journal,"22 Dec 2020","2021","8","1","309","319","Intelligent systems influence many aspects of daily life. With the emergence of the Internet of Things (IoT), artificial intelligence (AI), and machine learning (ML), opportunities have been created for smart computing infrastructure. However, problems might arise from the lack of interconnectivity, higher reliability, real-time predictive analytics, and low-latency requirements. Therefore, in this article, we propose the sensors, edge, and cloud (iSEC) framework. The project deploys a smart cloud edge-computing architecture to provide ML and deep learning in the cloud edge environment. Two pilot projects of air quality monitoring system and object detection are demonstrated to evaluate the iSEC framework.","2327-4662","","10.1109/JIOT.2020.3004244","Ministry of Science and Technology (MOST), Taiwan(grant numbers:108-2221-E-029-010,108-2745-8-029-007,108-2622-E-029-007-CC3); National Applied Research Laboratories (NARLabs), Taiwan(grant numbers:03108F1106); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9122603","Cloud computing;edge computing;Internet of Things (IoT);long-range low-power wide-area network (LPWAN);message passing interface (MPI)","Cloud computing;Internet of Things;Sensors;Edge computing;Image edge detection;Wireless sensor networks;Servers","","32","","32","IEEE","22 Jun 2020","","","IEEE","IEEE Journals"
"A Flexible Distributed Infrastructure for Real-Time Cosimulations in Smart Grids","L. Bottaccioli; A. Estebsari; E. Pons; E. Bompard; E. Macii; E. Patti; A. Acquaviva","Department of Control and Computer Engineering, Politecnico di Torino, Torino, Italy; Department of Energy, Politecnico di Torino, Torino, Italy; Department of Energy, Politecnico di Torino, Torino, Italy; Department of Energy, Politecnico di Torino, Torino, Italy; Department of Control and Computer Engineering, Politecnico di Torino, Torino, Italy; Department of Control and Computer Engineering, Politecnico di Torino, Torino, Italy; Department of Control and Computer Engineering, Politecnico di Torino, Torino, Italy",IEEE Transactions on Industrial Informatics,"7 Dec 2017","2017","13","6","3265","3274","Due to the increasing penetration of distributed generation, storage, electric vehicles, and new information communication technologies, distribution networks are evolving toward the smart grid paradigm. For this reason, new control strategies, algorithms, and technologies need to be tested and validated before their actual field implementation. In this paper, we present a novel modular distributed infrastructure, based on real-time simulation, for multipurpose smart grid studies. The different components of the infrastructure are described, and the system is applied to a case study based on a real urban district located in northern Italy. The presented infrastructure is shown to be flexible and useful for different and multidisciplinary smart grid studies.","1941-0050","","10.1109/TII.2017.2702206","EU project FLEXMETER; Edifici a Zero Consumo Energetico in Distretti Urbani Intelligenti; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7921440","Cosimulation;distributed infrastructure;photovoltaic (PV);real-time simulation;renewable energy;smart grid","Real-time systems;Smart grids;Load modeling;Internet of things;Performance evaluation;Renewable energy sources","","31","","39","IEEE","8 May 2017","","","IEEE","IEEE Journals"
"A Comprehensive Survey of Load Balancing Strategies Using Hadoop Queue Scheduling and Virtual Machine Migration","N. S. Dey; T. Gunasekhar","Department of Computer Science and Engineering, Koneru Lakshmaiah Educational Foundation, Guntur, India; Department of Information Technology, B. V. Raju Institute of Technology, Hyderabad, India",IEEE Access,"23 Jul 2019","2019","7","","92259","92284","The recent growth in the demand for scalable applications from the consumers of the services has motivated the application development community to build and deploy the applications on cloud in the form of services. The deployed applications have significant dependency on the infrastructure available with the application providers. Bounded by the limitations of available resource pools on-premises, many application development companies have migrated the applications to third party cloud environments called data centers. The data center owners or the cloud service providers are entitled to ensure high performance and high availability of the applications and at the same time the desired scalability for the applications. Also, the cloud service providers are also challenging in terms of cost reduction and energy consumption reductions for better manageability of the data center without degrading the performance of the deployed applications. It is to be noted that the performance of the application does not only depend on the responsiveness of the applications rather also must be measured in terms of service level agreements. The violation of the service level agreements or SLA can easily disprove the purpose of application deployments on cloud-based data centers. Thus, the data center owners apply multiple load balancing strategies for maintaining the desired outcomes from the application owners at the minimized cost of data center maintainability. Hence, the demand of the research is to thoroughly study and identify the scopes for improvements in the parallel research outcomes. As the number of applications ranging from small data-centric applications coming with the demand of frequent updates with higher computational capabilities to the big data-centric application as big data analytics applications coming with efficient algorithms for data and computation load managements, the data center owners are forced to think for efficient algorithms for load managements. The algorithms presented by various research attempts have engrossed on application specific demands for load balancing using virtual machine migrations and the solution as the proposed algorithms have become application problem specific. Henceforth, the further demand of the research is a guideline for selecting the appropriate load balancing algorithm via virtual machine migration for characteristics-based specific applications. Hence, this paper presents a comprehensive survey on existing virtual machine migration and selection processes to understand the specific application-oriented capabilities of these strategies with the advantages and bottlenecks. Also, with the understanding of the existing measures for load balancing, it is also important to furnish the further improvement strategies, which can be made possible with a detailed understanding of the parallel research outcomes. Henceforth, this paper also equips the study with guidelines for improvements and for further study. Nonetheless, the study cannot be completed without the mathematical analysis for better understanding and experimental analysis on different standards of datasets for better conclusive decisions. Hence, this paper also presents the discussion on mathematical models and experimental result analysis for the conclusive decision on the improvement factors and the usability of the migration methods for various purposes. Finally, this paper is a comprehensive survey on the background of the research, recent research outcomes using mathematical modeling and experimental studies on various available datasets, and finally identify the scopes of improvements considering various aspects such as execution time, mean time before a VM migration, mean time before a host shutdown, number of node shutdowns, SLA performance degradation, VM migrations, and energy consumption.","2169-3536","","10.1109/ACCESS.2019.2927076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8762113","Data center;load balancing;task scheduler;FIFO;FAIR;capacity;hybrid;LATE;SAMR;context-aware;threshold;IQR;LR;MAD;LRR;THR;VM consolidation;VM migration;MC;MMT;RS;MU;PlanetLab;metric;VM migration analysis;energy consumption analysis;SLA analysis","Task analysis;Load management;Cloud computing;Data centers;Resource management;Virtual machining;Indexes","","31","","74","CCBY","15 Jul 2019","","","IEEE","IEEE Journals"
"Joint Online Optimization of Data Sampling Rate and Preprocessing Mode for Edge–Cloud Collaboration-Enabled Industrial IoT","Y. Shi; C. Yi; B. Chen; C. Yang; K. Zhu; J. Cai","College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Electrical and Computer Engineering, Concordia University, Montreal, QC, Canada",IEEE Internet of Things Journal,"24 Aug 2022","2022","9","17","16402","16417","Edge–cloud collaboration is critical in the Industrial Internet of Things (IIoT) for serving computation-intensive tasks (e.g., bearing fault monitoring) that require low-response delay, low energy consumption, and high processing accuracy. In this article, an energy-efficient resource management framework for IIoT with closed-loop control on end devices, edge servers, and cloud center is studied. In the considered model, each edge server aggregates the data collected by industrial sensors (i.e., end devices) and forms computation tasks for corresponding data analysis. In order to minimize the system-wide energy consumption, while maintaining a guaranteed service delay and a satisfied data processing accuracy for each IIoT application, a joint optimization of: 1) sensors’ sampling rate adaption; 2) edge servers’ preprocessing mode selection; and 3) edge–cloud communication and computing resource allocation is formulated. Further taking into account the time-varying channel conditions and randomness of data arrivals, we propose a low-complexity online algorithm, which solves the problem in a dynamic manner. Particularly, the Lyapunov optimization method is first utilized to decompose the long-term problem into a series of instant ones [mixed-integer nonlinear programming (MINLP) problems], and then a Markov approximation algorithm is applied to solve such instant problems to near optimum with the consideration of future impacts. Performance analyses and simulation results show that the proposed algorithm is feasible under long-term service satisfaction constraints, and its energy consumption and service delay are approximately 20% and 28% lower than those of the benchmark schemes, respectively.","2327-4662","","10.1109/JIOT.2022.3150386","National Natural Science Foundation of China (NSFC)(grant numbers:62002164,61701230,62176122); Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery Grant, CRD Grant, and Concordia University PERFORM Research Chair Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9709555","Communication and computing resource allocation;edge–cloud collaboration;Industrial Internet of Things (IIoT);online optimization;preprocessing mode selection;sampling rate adaption","Industrial Internet of Things;Servers;Delays;Collaboration;Cloud computing;Resource management;Energy consumption","","31","","49","IEEE","10 Feb 2022","","","IEEE","IEEE Journals"
"Edge Computing Technology Enablers: A Systematic Lecture Study","S. Douch; M. R. Abid; K. Zine-Dine; D. Bouzidi; D. Benhaddou","National School of Computer Science and Systems Analysis (ENSIAS), Mohammed V University in Rabat, Rabat, Morocco; TSYS School of Computer Science, Columbus State University, Columbus, GA, USA; Faculty of Sciences (FSR), Mohammed V University in Rabat, Rabat, Morocco; National School of Computer Science and Systems Analysis (ENSIAS), Mohammed V University in Rabat, Rabat, Morocco; Department of Engineering Technology, University of Houston, Houston, TX, USA",IEEE Access,"13 Jul 2022","2022","10","","69264","69302","With the increasing stringent QoS constraints (e.g., latency, bandwidth, jitter) imposed by novel applications (e.g., e-Health, autonomous vehicles, smart cities, etc.), as well as the rapidly increasing number of connected IoT (Internet of Things) devices, the core network is becoming increasingly congested. To cope with those constraints, Edge Computing (EC) is emerging as an innovative computing paradigm that leverages Cloud computing and brings it closer to the customer. “EC” refers to transferring computing power and intelligence from the central Cloud to the network’s Edge. With that, EC promotes the idea of processing and caching data at the Edge, thus reducing network congestion and latency. This paper presents a detailed, thorough, and well-structured assessment of Edge Computing and its enabling technologies. Initially, we start by defining EC from the ground up, outlining its architectures and evolution from Cloudlets to Multi-Access Edge Computing. Next, we survey recent studies on the main cornerstones of an EC system, including resource management, computation offloading, data management, network management, etc. Besides, we emphasized EC technology enablers, starting with Edge Intelligence, the branch of Artificial Intelligence (AI) that integrates AI models at resource-constrained edge nodes with significant heterogeneity and mobility. Then, moving on to 5G and its empowering technologies, we explored how EC and 5G complement each other. After that, we studied virtualization and containerization as promising hosting runtime for edge applications. Further to that, we delineated a variety of EC use-case scenarios, e.g., smart cities, e-Health, military applications, etc. Finally, we concluded our survey by highlighting the role of EC integration with future concerns regarding green energy and standardization.","2169-3536","","10.1109/ACCESS.2022.3183634","National Academy of Sciences (NAS)/United States AID (USAID) under the PEER Cycle 5 Project(grant numbers:5-398); Towards Smart Microgrid: Renewable Energy Integration Into Smart Buildings; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9797685","Edge computing;cloud computing;fog computing;multi-access edge computing;edge intelligence;5G;containerization","Edge computing;Cloud computing;Quality of service;5G mobile communication;Internet of Things;Terminology","","31","","373","CCBY","16 Jun 2022","","","IEEE","IEEE Journals"
"Quantum Cryptography in 5G Networks: A Comprehensive Overview","M. Mehic; L. Michalek; E. Dervisevic; P. Burdiak; M. Plakalovic; J. Rozhon; N. Mahovac; F. Richter; E. Kaljic; F. Lauterbach; P. Njemcevic; A. Maric; M. Hamza; P. Fazio; M. Voznak","Department of Telecommunications, Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina; VSB—Technical University of Ostrava, Ostrava, Czechia; Department of Telecommunications, Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina; VSB—Technical University of Ostrava, Ostrava, Czechia; Department of Telecommunications, Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina; VSB—Technical University of Ostrava, Ostrava, Czechia; Department of Telecommunications, Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina; Deutsche Telekom Security, T-Mobile CZ, Prague, Czechia; Department of Telecommunications, Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina; VSB—Technical University of Ostrava, Ostrava, Czechia; Department of Telecommunications, Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina; Department of Telecommunications, Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina; Department of Telecommunications, Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina; VSB—Technical University of Ostrava, Ostrava, Czechia; Department of Telecommunications, Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina",IEEE Communications Surveys & Tutorials,"26 Feb 2024","2024","26","1","302","346","Every attempt to access to the Internet through a Web browser, email sent, VPN connection, VoIP call, instant message or other use of telecommunications systems involves cryptographic techniques. The most commonly applied technique is asymmetric cryptography, which is generally executed in the background without the user even being aware. It establishes a cryptographic code based on the computational complexity of mathematical problems. However, this type of cryptography, which is widely used in today’s telecommunications systems, is under threat as electronics and computing rapidly develop. The development of fifth-generation cellular networks (5G) is gaining momentum, and given its wide field of application, security requires special attention. This is especially true faced with the development of quantum computers. One solution to this security challenge is to use more advanced techniques to establish cryptographic keys that are not susceptible to attack. An essential part of quantum cryptography, Quantum Key Distribution (QKD) uses the principles of quantum physics to establish and distribute symmetric cryptographic keys between two geographically distant users. QKD establishes information-theoretically secure cryptographic keys that are resistant to eavesdropping when they are created. In this paper, we survey the security challenges and approaches in 5G networks concerning network protocols, interfaces and management organizations. We begin by examining the fundamentals of QKD and discuss the creation of QKD networks and their applications. We then outline QKD network architecture and its components and standards, following with a summary of QKD and post-quantum key distribution techniques and approaches for its integration into existing security frameworks such as VPNs (IPsec and MACsec). We also discuss the requirements, architecture and methods for implementing the FPGA-based encryptors needed to execute cryptographic algorithms with security keys. We discuss the performance and technologies of post-quantum cryptography, and finally, examine reported 5G demonstrations which have used quantum technologies, highlighting future research directions.","1553-877X","","10.1109/COMST.2023.3309051","Ministry of the Interior of the Czech Republic Project “Network Cybersecurity in the Post-Quantum Era”(grant numbers:VJ01010008); NATO SPS G5894 Project “Quantum Cybersecurity in 5G Networks (QUANTUM5)”; H2020 Project OPENQKD(grant numbers:857156); Ministry of Civil Affairs of Bosnia and Herzegovina(grant numbers:10-33-11-7111/22); Ministry of Science, Higher Education and Youth of Canton Sarajevo, Bosnia and Herzegovina(grant numbers:27-02-35-35137-29/22,27-02-35-35143-6/22); Project DQKDNM 2023; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10231341","5G;quantum cryptography;post-quantum cryptography;security","Security;Cryptography;Surveys;Quantum computing;5G mobile communication;Tutorials;Protocols","","31","","236","IEEE","28 Aug 2023","","","IEEE","IEEE Journals"
"Human in the Loop: Industry 4.0 Technologies and Scenarios for Worker Mediation of Automated Manufacturing","C. J. Turner; R. Ma; J. Chen; J. Oyekan","Surrey Business School, University of Surrey, Surrey, Guildford, U.K.; Department of Automatic Control and Systems Engineering, The University of Sheffield, Sheffield, U.K.; Department of Automatic Control and Systems Engineering, The University of Sheffield, Sheffield, U.K.; Department of Automatic Control and Systems Engineering, The University of Sheffield, Sheffield, U.K.",IEEE Access,"29 Jul 2021","2021","9","","103950","103966","Industry 4.0 derived technologies have the potential to enable a new wave of digital manufacturing solutions for semi and fully automated production. In addition, this paradigm encompasses the use of communication technologies to transmit data to processing stations as well as the utilization of cloud based computational resources for data mining. Despite the rise in automation, future manufacturing systems will initially still require humans in the loop to provide supervisory level mediation for even the most autonomous production scenarios. Through a structured review, this paper details a number of key technologies that are most likely to shape this future and describes a range of scenarios for their use in delivering human mediated automated and autonomous production. This paper argues that in all cases of future manufacturing management it is key that the human has oversight of critical information flows and remains an active participant in the delivery of the next generation of production systems.","2169-3536","","10.1109/ACCESS.2021.3099311","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9493179","Human computer interaction;intelligent systems;visualization;interactive systems;context awareness","Manufacturing;Manufacturing systems;Production;Collaboration;Service robots;Job shop scheduling;Context-aware services","","31","","123","CCBYNCND","26 Jul 2021","","","IEEE","IEEE Journals"
"Design and Development of an IoT Gateway for Smart Building Applications","A. Nugur; M. Pipattanasomporn; M. Kuzlu; S. Rahman","Bradley Department of Electrical and Computer Engineering, Advanced Research Institute, Virginia Tech, Arlington, VA, USA; Advanced Research Institute, Virginia Tech, Arlington, VA, USA; Department of Engineering Technology, Old Dominion University, Norfolk, VA, USA; Bradley Department of Electrical and Computer Engineering, Advanced Research Institute, Virginia Tech, Arlington, VA, USA",IEEE Internet of Things Journal,"9 Oct 2019","2019","6","5","9020","9029","Due to the depletion of energy resources and increased energy demand, there is an increased focus on the energy consumption and management in buildings. Many building energy management (BEM) software platforms are commercially available to monitor and control energy consumption. These platforms are hosted on the physical hardware within the building, due to which the hardware specifications limit their performance. To address this limitation, cloud technology emerged which facilitates software to be deployed at a remote location that has scalable hardware resources. Conventional BEM software can leverage such a cloud platform to offer scalable and maintenance-free installation. Once hosted on a remote cloud platform, BEM software lacks direct connectivity to building sensors/controllers, hence requires a device to support remote accessibility. Most devices are bound to a local area network and therefore, need an additional functional layer on top of its communication stack to perform network address translation (NAT)-Traversal. This functionality is implemented on a scalable software which connects to the devices in the network and acts as a gateway for cloud-based BEM software to access devices in the local area network. Any message sent to this gateway is translated to a respective device protocol. This paper describes the design and implementation of such an Internet of Things (IoT) gateway for a cloud-based BEM system that requires support for BACnet, Modbus, and HTTP RESTful interface devices.","2327-4662","","10.1109/JIOT.2019.2926099","U.S. Department of Energy(grant numbers:DE-EE-0006352); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8752396","Building energy management;cloud computing;fog computing;Internet of Things (IoT);IoT gateway","Cloud computing;Logic gates;Software;Protocols;Performance evaluation;Internet of Things;Buildings","","30","","29","IEEE","1 Jul 2019","","","IEEE","IEEE Journals"
"Offloading Mechanisms Based on Reinforcement Learning and Deep Learning Algorithms in the Fog Computing Environment","D. H. Abdulazeez; S. K. Askar","Department of Computer Science, University of Duhok, Duhok, Iraq; Erbil Technical Engineering College, Erbil Polytechnic University, Erbil, Iraq",IEEE Access,"10 Feb 2023","2023","11","","12555","12586","Fog computing has emerged as a computing paradigm for resource-restricted Internet of things (IoT) devices to support time-sensitive and computationally intensive applications. Offloading can be utilized to transfer resource-intensive tasks from resource-limited end devices to a resource-rich fog or cloud layer to reduce end-to-end latency and enhance the performance of the system. However, this advantage is still challenging to achieve in systems with a high request rate because it leads to long queues of tasks in fog nodes and reveals inefficiencies in terms of delays. In this regard, reinforcement learning (RL) is a well-known method for addressing such decision-making issues. However, in large-scale wireless networks, both action and state spaces are complex and extremely extensive. Consequently, reinforcement learning techniques may not be able to identify an efficient strategy within an acceptable time frame. Hence, deep reinforcement learning (DRL) was developed to integrate RL and deep learning (DL) to address this problem. This paper presents a systematic analysis of using RL or DRL algorithms to address offloading-related issues in fog computing. First, the taxonomy of fog computing offloading mechanisms based on RL and DRL algorithms was divided into three major categories: value-based, policy-based, and hybrid-based algorithms. These categories were then compared based on important features, including offloading problem formulation, utilized techniques, performance metrics, evaluation tools, case studies, their strengths and drawbacks, offloading directions, offloading mode, SDN-based architecture, and offloading decisions. Finally, the future research directions and open issues are discussed thoroughly.","2169-3536","","10.1109/ACCESS.2023.3241881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10035389","Fog computing;Internet of Things (IoT);offloading;reinforcement learning;deep reinforcement learning","Deep learning;Edge computing;Reinforcement learning;Systematics;Resource management;Heuristic algorithms;Internet of Things","","29","","147","CCBY","2 Feb 2023","","","IEEE","IEEE Journals"
"Congestion-Aware and Energy-Aware Virtual Network Embedding","M. Pham; D. B. Hoang; Z. Chaczko","Faculty of Engineering and IT, University of Technology Sydney, Ultimo, Australia; Faculty of Engineering and IT, University of Technology Sydney, Ultimo, Australia; Faculty of Engineering and IT, University of Technology Sydney, Ultimo, Australia",IEEE/ACM Transactions on Networking,"14 Feb 2020","2020","28","1","210","223","Network virtualization is an inherent component of future internet architectures. Network resources are virtualized from the underlying substrate and elastically provisioned and offered to customers on-demand. Optimal allocation of network resources in terms of utilization, quality of service, and energy consumption has been a challenge. Existing solutions consider congestion control in a single-objective virtual network embedding (VNE) problem. This paper defines a multiple-objective VNE problem called the congestion-aware, energy-aware VNE (CEVNE). The aim is to seek a solution that saves cost, saves energy and avoids network congestion simultaneously. CEVNE modelling techniques and solution approaches apply both the weighting method and the constraint method to search for pareto-optimal solutions that produce the best compromised solutions for all three objectives. Solving VNE problem is, however, NP-hard. A heuristic solution is proposed involving a two-stage coordinated CEVNE. The node-mapping algorithm searches for the sub-optimal solutions for three objectives. The link mapping process is an SDN-based heuristic algorithm that deploys a path service and a resource monitoring application on an SDN controller. The solution is realized using SDN, Segment Routing, and open network operating system platform (ONOS) technologies. The energy minimization is implemented with a registry that keeps track of active nodes and sets inactive nodes to sleep mode. The evaluation results showed that the multiple-objective CEVNE approach is feasible and achieves its goals of optimizing the resource allocation, improving the runtime, saving the energy consumption and controlling the network congestion.","1558-2566","","10.1109/TNET.2019.2958367","Australian Government Research Training Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8945162","Virtual network embedding;network virtualization;software-defined networks;segment routing;energy saving;congestion control;path services;multiple-objective optimization","Substrates;Virtualization;Optimization;Bandwidth;Energy consumption;Routing;Resource management","","29","","39","IEEE","30 Dec 2019","","","IEEE","IEEE Journals"
"Self-Learning Multi-Objective Service Coordination Using Deep Reinforcement Learning","S. Schneider; R. Khalili; A. Manzoor; H. Qarawlus; R. Schellenberg; H. Karl; A. Hecker","Department for Computer Science, Computer Networks Research Group, Paderborn University, Paderborn, Germany; Munich Research Center, Huawei Technologies Düsseldorf, Munich, Germany; Department for Computer Science, Computer Networks Research Group, Paderborn University, Paderborn, Germany; Logistics Department, Fraunhofer ISST, Dortmund, Germany; Department for Computer Science, Computer Networks Research Group, Paderborn University, Paderborn, Germany; Department for Computer Science, Computer Networks Research Group, Paderborn University, Paderborn, Germany; Department for Computer Science, Computer Networks Research Group, Paderborn University, Paderborn, Germany",IEEE Transactions on Network and Service Management,"8 Sep 2021","2021","18","3","3829","3842","Modern services consist of interconnected components, e.g., microservices in a service mesh or machine learning functions in a pipeline. These services can scale and run across multiple network nodes on demand. To process incoming traffic, service components have to be instantiated and traffic assigned to these instances, taking capacities, changing demands, and Quality of Service (QoS) requirements into account. This challenge is usually solved with custom approaches designed by experts. While this typically works well for the considered scenario, the models often rely on unrealistic assumptions or on knowledge that is not available in practice (e.g., a priori knowledge). We propose DeepCoord, a novel deep reinforcement learning approach that learns how to best coordinate services and is geared towards realistic assumptions. It interacts with the network and relies on available, possibly delayed monitoring information. Rather than defining a complex model or an algorithm on how to achieve an objective, our model-free approach adapts to various objectives and traffic patterns. An agent is trained offline without expert knowledge and then applied online with minimal overhead. Compared to a state-of-the-art heuristic, DeepCoord significantly improves flow throughput (up to 76%) and overall network utility (more than 2x) on real-world network topologies and traffic traces. It also supports optimizing multiple, possibly competing objectives, learns to respect QoS requirements, generalizes to scenarios with unseen, stochastic traffic, and scales to large real-world networks. For reproducibility and reuse, our code is publicly available.","1932-4537","","10.1109/TNSM.2021.3076503","German Research Foundation within the Collaborative Research Centre “On-The-Fly Computing” (SFB 901), the German Federal Ministry of Education and Research under Software Campus(grant numbers:01IS17046 (RealVNF)); European Commission under 5G-PPP project FUDGE-5G(grant numbers:H2020-ICT-42-2020,957242); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9419067","Network and service management;reinforcement learning;self-learning;self-adaptation;multi-objective","Quality of service;Monitoring;Runtime;Network topology;Edge computing;Dynamic scheduling;Delays","","29","","68","IEEE","29 Apr 2021","","","IEEE","IEEE Journals"
"VNF and CNF Placement in 5G: Recent Advances and Future Trends","W. Attaoui; E. Sabir; H. Elbiaze; M. Guizani","Intelligent Network and Mobile Charging Department, INWI Corporate, Casablanca, Morocco; Department of Computer Science, University of Quebec at Montreal, Montreal, Canada; Department of Computer Science, University of Quebec at Montreal, Montreal, Canada; Machine Learning Department, Mohamed Bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE",IEEE Transactions on Network and Service Management,"12 Dec 2023","2023","20","4","4698","4733","With the growing demand for openness, scalability, and granularity, mobile network function virtualization (NFV) has emerged as a key enabler for the most of mobile network operators. NFV decouples network functions from hardware devices. This decoupling allows network services, called Virtualized Network Functions (VNFs), to be hosted on commodity hardware which simplifies and enhances service deployment and management for providers, improves flexibility, and leads to efficient and scalable resource usage, and lower costs. The proper placement of VNFs in the hosting infrastructures is one of the main technical challenges. This placement significantly influences the network’s performance, reliability, and operating costs. The VNF placement is NP-Hard. Therefore, there is a need for placement methods that can cope with the complexity of the problem and find appropriate solutions in a reasonable duration. The primary purpose of this study is to provide a taxonomy of optimization techniques used to tackle the VNF placement problems. We classify the studied papers based on performance metrics, methods, algorithms, and environment. Virtualization is not limited to simply replacing physical machines with virtual machines or VNFs, but may also include micro-services, containers, and cloud-native systems. In this context, the second part of our article focuses on the placement of Containers Network Functions (CNFs) in edge/fog computing. Many issues have been considered as traffic congestion, resource utilization, energy consumption, performance degradation, etc. For each matter, various solutions are proposed through different surveys and research papers in which each one addresses the placement problem in a specific manner by suggesting single objective or multi-objective methods based on different types of algorithms such as heuristic, meta-heuristic, and machine learning algorithms.","1932-4537","","10.1109/TNSM.2023.3264005","Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE(grant numbers:8481000021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10090468","Virtual network function;container;placement;5G network slicing;cloud native","5G mobile communication;Containers;Cloud computing;Virtualization;Network slicing;Internet of Things;Quality of service","","29","","165","CCBY","31 Mar 2023","","","IEEE","IEEE Journals"
"The Evolution of Federated Learning-Based Intrusion Detection and Mitigation: A Survey","L. Lavaur; M. -O. Pahl; Y. Busnel; F. Autrel","IMT Atlantique and the Chair Cybersecurity of Critical Networked Infrastructures (Cyber CNI), Rennes, France; IMT Atlantique and the Chair Cybersecurity of Critical Networked Infrastructures (Cyber CNI), Rennes, France; Department of Network Systems, Cybersecurity and Digital Law (SRCD), IMT Atlantique, IRISA, Rennes, France; Department of Network Systems, Cybersecurity and Digital Law (SRCD), IMT Atlantique, IRISA, Rennes, France",IEEE Transactions on Network and Service Management,"12 Oct 2022","2022","19","3","2309","2332","In 2016, Google introduced the concept of Federated Learning (FL), enabling collaborative Machine Learning (ML). FL does not share local data but ML models, offering applications in diverse domains. This paper focuses on the application of FL to Intrusion Detection Systems (IDSs). There, common criteria to compare existing solutions are missing. In particular, this survey shows: (i) how FL-based IDSs are used in different domains; (ii) what differences exist between architectures; (iii) the state of the art of FL-based IDS. With a structured literature survey, this work identifies the relevant state of the art in FL–based intrusion detection from its creation in 2016 until 2021. It provides a reference architecture and a taxonomy to serve as guidelines to compare and design FL-based IDSs. Both are validated with the existing works. Finally, it identifies research directions for the application of FL to intrusion detection systems.","1932-4537","","10.1109/TNSM.2022.3177512","chair CyberCNI.fr; FEDER development fund of the Brittany region; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9780571","Federated learning;machine learning;intrusion detection systems;collaborative sharing;network security management;attack mitigation","Collaboration;Taxonomy;Statistical analysis;Organizations;Collaborative work;Standardization;Semantics","","29","","118","IEEE","24 May 2022","","","IEEE","IEEE Journals"
"iBrownout: An Integrated Approach for Managing Energy and Brownout in Container-Based Clouds","M. Xu; A. N. Toosi; R. Buyya","Cloud Computing and Distributed Systems (CLOUDS) Lab, University of Melbourne, Parkville, VIC, Australia; Cloud Computing and Distributed Systems (CLOUDS) Lab, University of Melbourne, Parkville, VIC, Australia; Cloud Computing and Distributed Systems (CLOUDS) Lab, University of Melbourne, Parkville, VIC, Australia",IEEE Transactions on Sustainable Computing,"7 Mar 2019","2019","4","1","53","66","Energy consumption of Cloud data centers has been a major concern of many researchers, and one of the reasons for huge energy consumption of Clouds lies in the inefficient utilization of computing resources. Besides energy consumption, another challenge of data centers is the unexpected loads, which leads to the overloads and performance degradation. Compared with VM consolidation and Dynamic Voltage Frequency Scaling that cannot function well when the whole data center is overloaded, brownout has shown to be a promising technique to handle both overloads and energy consumption through dynamically deactivating application optional components, which are also identified as containers/microservices. In this work, we propose an integrated approach to manage energy consumption and brownout in container-based cloud data centers. We also evaluate our proposed scheduling policies with real traces in a prototype system. The results show that our approach reduces about 40, 20, and 10 percent energy than the approach without power-saving techniques, brownout-overbooking approach and auto-scaling approach, respectively, while ensuring Quality of Service.","2377-3782","","10.1109/TSUSC.2018.2808493","China Scholarship Council; Australia Research Council Future Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8302602","Cloud data centers;energy efficiency;QoS;containers;microservices;brownout","Data centers;Energy consumption;Cloud computing;Quality of service;Servers;Containers;Prototypes","","29","","36","IEEE","26 Feb 2018","","","IEEE","IEEE Journals"
"NFV Platforms: Taxonomy, Design Choices and Future Challenges","T. Zhang; H. Qiu; L. Linguaglossa; W. Cerroni; P. Giaccone","Department of Network and Computer Science, Telecom Paris, Paris, France; Department of Network and Computer Science, Telecom Paris, Paris, France; Department of Network and Computer Science, Telecom Paris, Paris, France; Department of Electrical, Electronic, and Information Engineering “Guglielmo Marconi,”, University of Bologna, Bologna, Italy; Consorzio Nazionale Interuniversitario per le Telecomunicazioni, Parma, Italy",IEEE Transactions on Network and Service Management,"10 Mar 2021","2021","18","1","30","48","Due to the intrinsically inefficient service provisioning in traditional networks, Network Function Virtualization (NFV) keeps gaining attention from both industry and academia. By replacing the purpose-built, expensive, proprietary network equipment with software network functions consolidated on commodity hardware, NFV envisions a shift towards a more agile and open service provisioning paradigm. During the last few years, a large number of NFV platforms have been implemented to facilitate the development, deployment, and management of Virtual Network Functions (VNFs). Nonetheless, just like any complex system, such platforms commonly consist of abounding software and hardware components and usually incorporate disparate design choices based on distinct motivations or use cases. This broad collection of convoluted alternatives makes it extremely arduous for network operators to make proper choices. Although numerous efforts have been devoted to investigating different aspects of NFV, none of them specifically focused on NFV platforms or attempted to explore their design space. In this article, we present a comprehensive survey on the NFV platform design. Our study solely targets existing NFV platform implementations. We begin with a top-down architectural view of the standard reference NFV platform and present our taxonomy of existing NFV platforms based on what features they provide in terms of a typical network function life cycle. Then we thoroughly explore the design space and elaborate on the implementation choices each platform opts for. We also envision future challenges for NFV platform design in the incoming 5G era. We believe that our study gives a detailed guideline for network operators or service providers to choose the most appropriate NFV platform based on their respective requirements. Our work also provides guidelines for implementing new NFV platforms.","1932-4537","","10.1109/TNSM.2020.3045381","European Horizon 2020 Programme through the project 5G-EVE on “European 5G validation platform for extensive trials”(grant numbers:815074); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9302614","Network function virtualization;service function chaining;service management and orchestration;NFV infrastructure;VNF life cycle","Hardware;Software;Middleboxes;Space exploration;Monitoring;Switches;Virtualization","","29","","162","IEEE","22 Dec 2020","","","IEEE","IEEE Journals"
"A Survey on Blockchain-Based Telecommunication Services Marketplaces","R. -V. Tkachuk; D. Ilie; K. Tutschku; R. Robert","Department of Computer Science, Blekinge Institute of Technology, Karlshamn, Sweden; Department of Computer Science, Blekinge Institute of Technology, Karlshamn, Sweden; Department of Computer Science, Blekinge Institute of Technology, Karlshamn, Sweden; Ericsson Research, Stockholm, Sweden",IEEE Transactions on Network and Service Management,"10 Mar 2022","2022","19","1","228","255","Digital marketplaces were created recently to accelerate the delivery of applications and services to customers. Their appealing feature is to activate and dynamize the demand, supply, and development of digital goods, applications, or services. By being an intermediary between producer and consumer, the primary business model for a marketplace is to charge the producer with a commission on the amount paid by the consumer. However, most of the time, the commission is dictated by the marketplace facilitator itself and creates an imbalance in value distribution, where producer and consumer sides suffer monetarily. In order to eliminate the need for a centralized entity between the producer and consumer, a blockchain-based decentralized digital marketplace concept was introduced. It provides marketplace actors with the tools to perform business transactions in a trusted manner and without the need for an intermediary. In this work, we provide a survey on Telecommunication Services Marketplaces (TSMs) which employ blockchain technology as the main trust enabling entity in order to avoid any intermediaries. We provide an overview of scientific and industrial proposals on the blockchain-based online digital marketplaces at large, and TSMs in particular. We consider in this study the notion of telecommunication services as any service enabling the capability for information transfer and, increasingly, information processing provided to a group of users by a telecommunications system. We discuss the main standardization activities around the concepts of TSMs and provide particular use-cases for the TSM business transactions such as SLA settlement. Also, we provide insights into the main foundational services provided by the TSM, as well as a survey of the scientific and industrial proposals for such services. Finally, a prospect for future developments is given.","1932-4537","","10.1109/TNSM.2021.3123680","Swedish Knowledge Foundation through the project Symphony - Supply-and-Demand-based Service Exposure using Robust Distributed Concepts; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9592675","Digital marketplace;telecommunication services marketplace;blockchain technology;communication service provider;distributed ledger technology","Blockchains;Business;Distributed ledger;Communications technology;Proposals;Internet of Things;Telecommunication services","","28","","171","CCBYNCND","28 Oct 2021","","","IEEE","IEEE Journals"
"Aveiro Tech City Living Lab: A Communication, Sensing, and Computing Platform for City Environments","P. Rito; A. Almeida; A. Figueiredo; C. Gomes; P. Teixeira; R. Rosmaninho; R. Lopes; D. Dias; G. Vítor; G. Perna; M. Silva; C. Senna; D. Raposo; M. Luís; S. Sargento; A. Oliveira; N. B. de Carvalho","Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal; Pólo de Aveiro, Instituto de Telecomunicações, Aveiro, Portugal",IEEE Internet of Things Journal,"25 Jul 2023","2023","10","15","13489","13510","This article presents the deployment and experimentation architecture of the Aveiro Tech City Living Lab (ATCLL) in Aveiro, Portugal. This platform comprises a large number of Internet of Things (IoT) devices with communication, sensing, and computing capabilities. The communication infrastructure, built on fiber and millimeter-wave (mmWave) links, integrates a communication network with radio terminals [WiFi, ITS-G5, cellular vehicular-to-everything, 5G and LoRa(WAN)], multiprotocol, spread throughout 44 connected points of access in the city. Additionally, public transportation has also been equipped with communication and sensing units. All these points combine and interconnect a set of sensors, such as mobility (radars, light detection and rangings (LiDARs), and video cameras) and environmental sensors. Combining edge computing and cloud management to deploy the services and manage the platform, and a data platform to gather and process the data, the living lab supports a wide range of services and applications: IoT, intelligent transport systems (ITSs) and assisted driving, environmental monitoring, emergency and safety, and among others. This article describes the architecture, implementation, and deployment to make the overall platform to work and integrate researchers and citizens. Moreover, it showcases some examples of the performance metrics achieved in the city infrastructure, the data that can be collected, visualized, and used to build services and applications to the cities, and, finally, different use cases in the mobility and safety scenarios.","2327-4662","","10.1109/JIOT.2023.3262627","European Regional Development Fund (ERDF), included in the “Urban Innovative Actions” Programme, through Project Aveiro STEAM City(grant numbers:UIA03-084); FCT/MEC through National Funds(grant numbers:PTDC/EEI-COM/5284/2020); FCT/MCTES through National Funds and when applicable co-funded EU funds(grant numbers:UIDB/50008/2020-UIDP/50008/2020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10083171","Connectivity management;smart cities;software-defined networks;test-bed and trials;vehicular networks","Sensors;Smart cities;Internet of Things;Wireless fidelity;Safety;Temperature sensors;Computer architecture","","28","","49","IEEE","28 Mar 2023","","","IEEE","IEEE Journals"
"Software-Defined “Hardware” Infrastructures: A Survey on Enabling Technologies and Open Research Directions","A. Roozbeh; J. Soares; G. Q. Maguire; F. Wuhib; C. Padala; M. Mahloo; D. Turull; V. Yadhav; D. Kostić","Ericsson Research, Ericsson AB, Stockholm, Sweden; Ericsson Research, Ericsson AB, Stockholm, Sweden; Communication Systems Department, KTH Royal Institute of Technology, Stockholm, Sweden; Ericsson Research, Ericsson AB, Stockholm, Sweden; Ericsson Research, Ericsson AB, Stockholm, Sweden; Ericsson Research, Ericsson AB, Stockholm, Sweden; Ericsson Research, Ericsson AB, Stockholm, Sweden; Ericsson Research, Ericsson AB, Stockholm, Sweden; Communication Systems Department, KTH Royal Institute of Technology, Stockholm, Sweden",IEEE Communications Surveys & Tutorials,"21 Aug 2018","2018","20","3","2454","2485","This paper provides an overview of software-defined “hardware” infrastructures (SDHI). SDHI builds upon the concept of hardware (HW) resource disaggregation. HW resource disaggregation breaks today's physical server-oriented model where the use of a physical resource (e.g., processor or memory) is constrained to a physical server's chassis. SDHI extends the definition of of software-defined infrastructures (SDI) and brings greater modularity, flexibility, and extensibility to cloud infrastructures, thus allowing cloud operators to employ resources more efficiently and allowing applications not to be bounded by the physical infrastructure's layout. This paper aims to be an initial introduction to SDHI and its associated technological advancements. This paper starts with an overview of the cloud domain and puts into perspective some of the most prominent efforts in the area. Then, it presents a set of differentiating use-cases that SDHI enables. Next, we state the fundamentals behind SDI and SDHI, and elaborate why SDHI is of great interest today. Moreover, it provides an overview of the functional architecture of a cloud built on SDHI, exploring how the impact of this transformation goes far beyond the cloud infrastructure level in its impact on platforms, execution environments, and applications. Finally, an in-depth assessment is made of the technologies behind SDHI, the impact of these technologies, and the associated challenges and potential future directions of SDHI.","1553-877X","","10.1109/COMST.2018.2834731","Knut och Alice Wallenbergs Stiftelse; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8356736","CR-software-defined infrastructure;resource disaggregation;cloud infrastructure;rack-scale;hyperscale computing;disaggregated DC","Servers;Cloud computing;Virtualization;Hardware;Virtual machine monitors;Computer architecture","","27","","202","IEEE","9 May 2018","","","IEEE","IEEE Journals"
"Energy-Aware Service Function Chain Embedding in Edge–Cloud Environments for IoT Applications","N. H. Thanh; N. Trung Kien; N. V. Hoa; T. T. Huong; F. Wamser; T. Hossfeld","School of Electronics and Telecommunications, Hanoi University of Science and Technology, Hanoi, Vietnam; School of Electronics and Telecommunications, Hanoi University of Science and Technology, Hanoi, Vietnam; School of Electronics and Telecommunications, Hanoi University of Science and Technology, Hanoi, Vietnam; School of Electronics and Telecommunications, Hanoi University of Science and Technology, Hanoi, Vietnam; Chair of Communication Networks, University of Würzburg, Würzburg, Germany; Chair of Communication Networks, University of Würzburg, Würzburg, Germany",IEEE Internet of Things Journal,"23 Aug 2021","2021","8","17","13465","13486","The implementation of Internet-of-Things (IoT) applications faces several challenges in practice, such as compliance with Quality-of-Service requirements, resource constraints, and energy consumption. In this context, the joint edge–cloud paradigm for IoT applications can resolve some of the issues arising in pure cloud computing scenarios, such as those related to latency, energy, or privacy. Therefore, an edge–cloud environment could be promising for resource and energy-efficient IoT applications that implement virtual network functions (VNFs) bound together into service function chains (SFCs). However, a resource and energy-efficient SFC placement requires smart SFC embedding mechanisms in the edge–cloud environment, as several challenges arise, such as IoT service chain modeling and evaluation, the tradeoff between resource allocation, energy efficiency and performance, and the resource dynamics. In this article, we address issues in modeling resource and energy utilization for IoT applications in edge–cloud environments. A smart traffic monitoring IP camera system is deployed as a use case for a realistic modeling of a service chain. The system is implemented in our testbed, which is designed and developed specifically to model and investigate the resource and energy utilization of SFC embedding strategies. A resource and energy-aware SFC strategy in the edge–cloud environment for IoT applications is then proposed. Our algorithm is able to cope with dynamic load and resource situations emerging from dynamic SFC requests. The strategy is evaluated systematically in terms of the acceptance ratio of SFC requests, resource efficiency and utilization, power consumption, and VNF migrations depending on the offered system load. Results show that our strategy outperforms some existing approaches in terms of resource and energy efficiency, thus it overcomes the relevant challenges from practice and meets the demands of IoT applications.","2327-4662","","10.1109/JIOT.2021.3064986","Hanoi University of Science and Technology (HUST)(grant numbers:T2020-SAHEP-009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9373577","Edge–cloud computing;Internet-of-Things (IoT) applications;network function virtualization (NFV);resource and energy-aware service chain embedding (RE-SCE);smart city","Internet of Things;Cloud computing;Data models;Computational modeling;Computer architecture;Edge computing;Heuristic algorithms","","27","","91","IEEE","9 Mar 2021","","","IEEE","IEEE Journals"
"Empowering the 6G Cellular Architecture With Open RAN","M. Polese; M. Dohler; F. Dressler; M. Erol-Kantarci; R. Jana; R. Knopp; T. Melodia","Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Advanced Technology Group, Ericsson Inc., Santa Clara, CA, USA; School of Electrical Engineering and Computer Science, TU Berlin, Berlin, Germany; School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Canada; Google, New York City, NY, USA; EURECOM, Sophia Antipolis, Biot, France; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA",IEEE Journal on Selected Areas in Communications,"16 Jan 2024","2024","42","2","245","262","Innovation and standardization in 5G have brought advancements to every facet of the cellular architecture. This ranges from the introduction of new frequency bands and signaling technologies for the radio access network (RAN), to a core network underpinned by micro-services and network function virtualization (NFV). However, like any emerging technology, the pace of real-world deployments does not instantly match the pace of innovation. To address this discrepancy, one of the key aspects under continuous development is the RAN with the aim of making it more open, adaptive, functional, and easy to manage. In this paper, we highlight the transformative potential of embracing novel cellular architectures by transitioning from conventional systems to the progressive principles of Open RAN. This promises to make 6G networks more agile, cost-effective, energy-efficient, and resilient. It opens up a plethora of novel use cases, ranging from ubiquitous support for autonomous devices to cost-effective expansions in regions previously underserved. The principles of Open RAN encompass: (i) a disaggregated architecture with modular and standardized interfaces; (ii) cloudification, programmability and orchestration; and (iii) AI-enabled data-centric closed-loop control and automation. We first discuss the transformative role Open RAN principles have played in the 5G era. Then, we adopt a system-level approach and describe how these Open RAN principles will support 6G RAN and architecture innovation. We qualitatively discuss potential performance gains that Open RAN principles yield for specific 6G use cases. For each principle, we outline the steps that research, development and standardization communities ought to take to make Open RAN principles central to next-generation cellular network designs.","1558-0008","","10.1109/JSAC.2023.3334610","U.S. National Science Foundation(grant numbers:CNS-2117814); Federal Ministry of Education and Research (BMBF), Germany, within the 6G Platform(grant numbers:16KISK050); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10329947","Open RAN;6G;architecture;end-to-end","5G mobile communication;Cellular networks;6G mobile communication;Wireless communication;Technological innovation;3GPP;Quality of service","","27","","153","IEEE","28 Nov 2023","","","IEEE","IEEE Journals"
"Toward an Open, Intelligent, and End-to-End Architectural Framework for Network Slicing in 6G Communication Systems","M. A. Habibi; B. Han; A. Fellan; W. Jiang; A. G. Sánchez; I. L. Pavon; A. Boubendir; H. D. Schotten","Department of Electrical and Computer Engineering, Division of Wireless Communications and Radio Navigation, Rheinland-Pfälzische Technische Universität, Kaiserslautern, Germany; Department of Electrical and Computer Engineering, Division of Wireless Communications and Radio Navigation, Rheinland-Pfälzische Technische Universität, Kaiserslautern, Germany; Department of Electrical and Computer Engineering, Division of Wireless Communications and Radio Navigation, Rheinland-Pfälzische Technische Universität, Kaiserslautern, Germany; Intelligent Networking Research Group, German Research Center for Artificial Intelligence, Kaiserslautern, Germany; Research and Innovation Department, ATOS Spain, Madrid, Spain; Research and Innovation Department, ATOS Spain, Madrid, Spain; Network Architecture and Automation Department, Orange Labs Networks, Issy-les-Moulineaux, France; Department of Electrical and Computer Engineering, Division of Wireless Communications and Radio Navigation, Rheinland-Pfälzische Technische Universität, Kaiserslautern, Germany",IEEE Open Journal of the Communications Society,"4 Aug 2023","2023","4","","1615","1658","The definition of the fundamental concepts and the design of the architectural framework for network slicing in fifth-generation communication systems have been successfully concluded; the standardization activities are almost over; and the commercial deployment has already commenced worldwide. To compete for digital supremacy and to be seen as front-runners in the international technological race, researchers from various regions and countries have begun exploring the technical requirements, envisioning potential applications, identifying innovative enablers, developing testbeds for the preliminary validation of several terrestrial and non-terrestrial technologies, and conceptualizing the architectural design for the next generation of mobile communication systems – the sixth-generation (6G) – aiming to connect the human, physical, and digital worlds with a high level of intelligence and openness for the 2030s. In support of such an ambitious vision, this article extends the end-to-end network slicing concepts, methods, solutions, and functioning architectures towards 6G. To this intent, the study first presents several decisive motivating trends behind such an extension of network slicing in order to make forthcoming mobile networks fully slicing-aware. Following that, the paper attempts to highlight the intelligentization of a number of key enabling technologies that will bring a renaissance to network slicing in the next decade. It then proposes a unified architectural framework and its principal building blocks in several layers, paving the way for the implementation of an open and intelligent network and network slicing in 6G. The proposed architectural solution harmonizes the most recent specifications of the relevant de jure and de facto standards development organizations in their applicable layers with the aim of architecting a pre-standard-compliant and preliminary framework for slicing the 6G network. Finally, the article is intended to spur interest and lay the groundwork for further investigations and subsequent research and development by highlighting a number of open research challenges and directions in this flourishing field.","2644-125X","","10.1109/OJCOMS.2023.3294445","Horizon 2020 Research and Innovation Program of the European Union through Hexa-X(grant numbers:101015956); Hexa-X-II(grant numbers:101095759); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10178010","6G;automation;cloudification;end-to-end architecture;intelligentization;intelligent networks;intelligent network slice;management and orchestration;mobile communication systems;network architecture;network exposure;network slicing;network slice;openness;open network;open network slice;softwarization;standardization;standards;virtualization","6G mobile communication;Network slicing;Computer architecture;Automation;Optimization;Industries;Next generation networking","","27","","145","CCBY","11 Jul 2023","","","IEEE","IEEE Journals"
"IECL: An Intelligent Energy Consumption Model for Cloud Manufacturing","Z. Zhou; M. Shojafar; M. Alazab; F. Li","School of Computer Engineering and Applied Mathematics, Changsha University, Changsha, China; 5G/6GIC, Institute for Communication Systems (ICS), University of Surrey, Guildford, U.K.; Charles Darwin University, Casuarina, NT, Australia; School of Computer Engineering and Applied Mathematics, Changsha University, Changsha, China",IEEE Transactions on Industrial Informatics,"30 Sep 2022","2022","18","12","8967","8976","The high computational capability provided by a data center makes it possible to solve complex manufacturing issues and carry out large-scale collaborative cloud manufacturing. Accurately, real-time estimation of the power required by a data center can help resource providers predict the total power consumption and improve resource utilization. To enhance the accuracy of server power models, we propose a real-time energy consumption prediction method called IECL that combines the support vector machine, random forest, and grid search algorithms. The random forest algorithm is used to screen the input parameters of the model, while the grid search method is used to optimize the hyperparameters. The error confidence interval is also leveraged to describe the uncertainty in the energy consumption by the server. Our experimental results suggest that the average absolute error for different workloads is less than 1.4% with benchmark models.","1941-0050","","10.1109/TII.2022.3165085","Scientific Research Project of Education Department of Hunan Province(grant numbers:21B0779); Hunan Province Key Laboratory of Industrial Internet Technology and Security(grant numbers:2019TP1011); Marie Curie Global Fellowship; European Commission(grant numbers:MSCA-IF-GF-839255); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9749900","Cloud manufacturing;data center;energy consumption prediction;power model;support vector machine (SVM)","Energy consumption;Servers;Data centers;Data models;Manufacturing;Predictive models;Feature extraction","","27","","29","IEEE","5 Apr 2022","","","IEEE","IEEE Journals"
"Comparing Apples and Oranges in IoT Context: A Deep Dive Into Methods for Comparing IoT Platforms","A. Mijuskovic; I. Ullah; R. Bemthuis; N. Meratnia; P. Havinga","EEMCS Faculty, Pervasive Systems Group, University of Twente, NB Enschede, The Netherlands; EEMCS Faculty, Pervasive Systems Group, University of Twente, NB Enschede, The Netherlands; EEMCS Faculty, Pervasive Systems Group, University of Twente, NB Enschede, The Netherlands; EEMCS Faculty, Pervasive Systems Group, University of Twente, NB Enschede, The Netherlands; EEMCS Faculty, Pervasive Systems Group, University of Twente, NB Enschede, The Netherlands",IEEE Internet of Things Journal,"22 Jan 2021","2021","8","3","1797","1816","Many researchers try to make a comparison between various Internet-of-Things (IoT) platforms based on specific requirements. However, none of the reviewed studies proposed a thorough analysis of the variety of comparative methods. Since there is a lack of comparison frameworks for IoT platforms, individuals or companies have difficulties when selecting a suitable IoT platform matching their associated business requirements. In order to support this selection process, a set of functional and nonfunctional requirements is identified. A framework containing methods in selecting an IoT platform is presented. The methodology is based on statistical and visualization techniques to recommend a suitable IoT platform. Five IoT platforms: 1) Azure; 2) AWS; 3) SaS; 4) ThingWorx; and 5) Kaa IoT are studied to evaluate the performance of the framework. Different comparison methods are proposed and a multicriteria decision analysis method was applied by using an analytical hierarchical process (AHP). One of the methods clusters the functional requirements and compares the IoT platforms based on their ability in supporting a specific requirement or not. The  $K$ -means clustering was applied to determine the clusters of functional requirements. The comparison was made based on the hierarchical level of requirements per main requirement. The other methods use the following statistical tests: error bar test, one-way Anova test, and Tukey’s honest significant difference test. Based on the selected requirements, an approach is suggested for which IoT platform can be used.","2327-4662","","10.1109/JIOT.2020.3016921","OP Oost; Project CountDown; Netherlands Organization for Scientific Research (NWO), Project DataRel(grant numbers:628.009.015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9169714","Functional requirements;Internet-of-Things (IoT) platforms;statistical comparative techniques;visualization","Internet of Things;Security;Taxonomy;Scalability;Resource management;Protocols","","27","","66","IEEE","17 Aug 2020","","","IEEE","IEEE Journals"
"Microservices Monitoring with Event Logs and Black Box Execution Tracing","M. Cinque; R. D. Corte; A. Pecchia","Dipartimento di Ingegneria Elettrica e delle Tecnologie dell'Informazione (DIETI), Università degli Studi di Napoli Federico II, via Claudio 21, Napoli, Italy; Dipartimento di Ingegneria Elettrica e delle Tecnologie dell'Informazione (DIETI), Università degli Studi di Napoli Federico II, via Claudio 21, Napoli, Italy; Dipartimento di Ingegneria Elettrica e delle Tecnologie dell'Informazione (DIETI), Università degli Studi di Napoli Federico II, via Claudio 21, Napoli, Italy",IEEE Transactions on Services Computing,"3 Feb 2022","2022","15","1","294","307","Monitoring is a core practice in any software system. Trends in microservices systems exacerbate the role of monitoring and pose novel challenges to data sources being used for monitoring, such as event logs. Current deployments create a distinct log per microservice; moreover, composing microservices by different vendors exacerbates format and semantic heterogeneity of logs. Understanding and traversing the logs from different microservices demands for substantial cognitive work by human experts. This paper proposes a novel approach to accompany microservices logs with black box tracing to help practitioners in making informed decisions for troubleshooting. Our approach is based on the passive tracing of request-response messages of the REpresentational State Transfer (REST) communication model. Differently from many existing tools for microservices, our tracing is application transparent and non-intrusive. We present an implementation called MetroFunnel and conduct an assessment in the context of two case studies: a Clearwater IP Multimedia Subsystem (IMS) setup consisting of Docker microservices and a Kubernetes orchestrator deployment hosting tens of microservices. MetroFunnel allows making useful attributions in traversing the logs; more important, it reduces the size of collected monitoring data at negligible performance overhead with respect to traditional logs.","1939-1374","","10.1109/TSC.2019.2940009","Compagnia di San Paolo(grant numbers:Programme STAR); Ministero dell Istruzione dell Universita e della Ricerca; Universita degli Studi di Napoli Federico II; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8826375","monitoring;microservices;REST;docker;clearwater;kubernetes;log analysis","Monitoring;Instruments;Microservice architecture;Measurement;Semantics;Runtime;Software systems;Market research;IP networks","","26","","52","IEEE","6 Sep 2019","","","IEEE","IEEE Journals"
"Emerging Technologies for Next Generation Remote Health Care and Assisted Living","I. Ahmad; Z. Asghar; T. Kumar; G. Li; A. Manzoor; K. Mikhaylov; S. A. Shah; M. Höyhtyä; J. Reponen; J. Huusko; E. Harjula","VTT Technical Research Centre of Finland, Espoo, Finland; Navigil, Espoo, Finland; Centre for Wireless Communications, University of Oulu, Oulu, Finland; Institute of Cyber Security, Shanghai Jiao Tong University, Shanghai, China; Centre for Wireless Communications, University of Oulu, Oulu, Finland; Centre for Wireless Communications, University of Oulu, Oulu, Finland; School of Computing and Digital Technology, Birmingham City University, Birmingham, U.K.; VTT Technical Research Centre of Finland, Espoo, Finland; Centre for Wireless Communications, University of Oulu, Oulu, Finland; VTT Technical Research Centre of Finland, Espoo, Finland; Centre for Wireless Communications, University of Oulu, Oulu, Finland",IEEE Access,"1 Jun 2022","2022","10","","56094","56132","Remote health care is currently one of the most promising solutions to ensure a high level of treatment outcome, cost-efficiency and sustainability of the healthcare systems worldwide. Even though research on remote health care can be traced back to the early days of the Internet, the recent COVID-19 has necessitated further improvement in existing health care systems with invigorated research on remote health care technologies. In this article we delve into the state-of-the-art research in latest technologies and technological paradigms that play a vital role in enabling the next generation remote health care and assisted living. First the need of using the latest technological developments in the domain of remote health care is briefly discussed. Then the most important technologies and technological paradigms that are crucial in enabling remote health care and assisted living are emphasised. Henceforth, a detailed survey of existing technologies, potential challenges in those technologies, and possible solutions is conducted. Finally, missing research gaps and important future research directions in each enabling technology are brought forth to motivate further research in remote health care.","2169-3536","","10.1109/ACCESS.2022.3177278","Academy of Finland through the DigiHealth Project(grant numbers:326291); 6G Flagship Project(grant numbers:318927,346208); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9780134","Telemedicine;5G/6G;medical IoT;remote care;big data;AI/ML;medical informatics","Medical services;Aging;Older adults;Statistics;Sociology;Diseases;Next generation networking","","26","","317","CCBY","23 May 2022","","","IEEE","IEEE Journals"
"Kubernetes-Oriented Microservice Placement With Dynamic Resource Allocation","Z. Ding; S. Wang; C. Jiang","Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, China; Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, China; Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, China",IEEE Transactions on Cloud Computing,"6 Jun 2023","2023","11","2","1777","1793","Microservices and Kubernetes are widely used in the development and operations of cloud-native applications. By providing automated placement and scaling, Kubernetes has become the main tool for managing microservices. However, existing work and Kubernetes fail to consider the dynamic competition and availability of microservices as well as the problem of shared dependency libraries among multiple microservice instances. To this end, this article proposes an integer nonlinear microservice placement model for Kubernetes with the goal of cost minimization. Specifically, we calculate the number of instances based on microservice availability and construct a model in which the total resource demand of multiple microservice instances exceeds the appropriate proportion of node resources when dynamic resource competition exists and the size of the shared dependency library is less than the node storage capacity. Finally, this article solves the microservice placement model using an improved genetic algorithm. The experimental results demonstrate that higher throughput is obtained with the same costs and that the same throughput is obtained with lower costs.","2168-7161","","10.1109/TCC.2022.3161900","National Key Research and Development Program of China(grant numbers:2019YFB1704102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9741392","Microservice placement;dynamic resource competition;microservice availability;shared dependency libraries;heuristic solving","Microservice architectures;Containers;Cloud computing;Costs;Libraries;Dynamic scheduling;Throughput","","26","","50","IEEE","24 Mar 2022","","","IEEE","IEEE Journals"
"A Data-Centric Internet of Things Framework Based on Azure Cloud","Y. Liu; K. Akram Hassan; M. Karlsson; Z. Pang; S. Gong","Department of Science and Technology, Linköping University, Campus Norrköping, Norrköping, Sweden; Department of Science and Technology, Linköping University, Campus Norrköping, Norrköping, Sweden; Department of Science and Technology, Linköping University, Campus Norrköping, Norrköping, Sweden; Corporate Research, ABB AB, Västerås, Sweden; Department of Science and Technology, Linköping University, Campus Norrköping, Norrköping, Sweden",IEEE Access,"2 May 2019","2019","7","","53839","53858","Internet of Things (IoT) has been found pervasive use cases and become a driving force to constitute a digital society. The ultimate goal of IoT is data and the intelligence generated from data. With the progress in public cloud computing technologies, more and more data can be stored, processed and analyzed in cloud to release the power of IoT. However, due to the heterogeneity of hardware and communication protocols in the IoT world, the interoperability and compatibility among different link layer protocols, sub-systems, and back-end services have become a significant challenge to IoT practices. This challenge cannot be addressed by public cloud suppliers since their efforts are mainly put into software and platform services but can hardly be extended to end devices. In this paper, we propose a data-centric IoT framework that incorporates three promising protocols with fundamental security schemes, i.e., WiFi, Thread, and LoRaWAN, to cater to massive IoT and broadband IoT use cases in local, personal, and wide area networks. By taking advantages of the Azure cloud infrastructure, the framework features a unified device management model and data model to conquer the interoperability challenge. We also provide implementation and a case study to validate the framework for practical applications.","2169-3536","","10.1109/ACCESS.2019.2913224","Environmental Protection Agency; Norrköping Fund for Research and Development, Sweden; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698762","Internet of Things;framework;cloud;azure;IoT hub;thread;WiFi;lorawan","Internet of Things;Cloud computing;Protocols;Wireless fidelity;Broadband communication;Monitoring;Interoperability","","26","","38","OAPA","25 Apr 2019","","","IEEE","IEEE Journals"
"Control Networks and Smart Grid Teleprotection: Key Aspects, Technologies, Protocols, and Case-Studies","L. F. F. De Almeida; j. R. D. Santos; L. A. M. Pereira; A. C. Sodré; L. L. Mendes; J. J. P. C. Rodrigues; R. A. L. Rabelo; A. M. Alberti","National Institute of Telecommunications (Inatel), Santa Rita do Sapucaí, Brazil; National Institute of Telecommunications (Inatel), Santa Rita do Sapucaí, Brazil; National Institute of Telecommunications (Inatel), Santa Rita do Sapucaí, Brazil; National Institute of Telecommunications (Inatel), Santa Rita do Sapucaí, Brazil; National Institute of Telecommunications (Inatel), Santa Rita do Sapucaí, Brazil; Post-Graduation Program in Electrical Engineering, Federal University of Piauí (UFPI), Teresina, Brazil; Computing Department, Federal University of Piauí (UFPI), Teresina, Brazil; National Institute of Telecommunications (Inatel), Santa Rita do Sapucaí, Brazil",IEEE Access,"29 Sep 2020","2020","8","","174049","174079","The appeal for more reliable, efficient and resilient power grids has become indispensable with the recent technological development and ever-increasing demand for electrical energy. These goals might be achieved by introducing sensors and actuators in the power grid, with the purpose of enabling a smart control of the power network. All power devices must be connected using a reliable communication network, which should be able to operate even when the power grid fails. Currently, several communication technologies have been applied for supporting this new application scenario. This article aims for reviewing both academic and market use cases for technologies applied to the mission-critical, control networks and Smart Grid applications, such as teleprotection, self-healing, communication with control centers and field devices, among others. More specifically, the key aspects, potential technologies, main protocols and use cases of operating data networks in energy transmission, power distribution and smart grid environments, including circuit and packet switching technologies, are discussed into details. Furthermore, resilience and the main telecommunications technologies used in power grids, as well, the correlation among them, are exploited in practical point of view. The article provides a broad discussion on the best telecommunication options to build the emerging intelligent energy distribution systems, covering control networks, teleprotection, and smart grid applications.","2169-3536","","10.1109/ACCESS.2020.3025235","Modelo de Referência para a Rede Operativa de Dados da Companhia Energética de Minas Gerais (CEMIG) funded by the Fundação de Amparo à Pesquisa do Estado de Minas Gerais/Companhia Energética de Minas Gerais/Agência Nacional de Energia Elétrica (FAPEMIG/CEMIG/ANEEL)(grant numbers:D0640); Rede Nacional de Ensino e Pesquisa (RNP), with resources from Ministério da Ciência, Tecnologia e Inovações (MCTIC), through the Radiocommunication Reference Center (Centro de Referência em Radiocomunicações—CRR) Project of the National Institute of Telecommunications (Instituto Nacional de Telecomunicações—Inatel), Brazil(grant numbers:01250.075413/2018-04); Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES), Brazil(grant numbers:Finance Code 001); Fundação para a Ciência e a Tecnologia/Ministério da Ciência, Tecnologia e Ensino Superior (FCT/MCTES) through national funds and when applicable co-funded EU(grant numbers:UIDB/EEA/50008/2020); Brazilian National Council for Scientific and Technological Development (CNPq)(grant numbers:309335/2017-5); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200485","Power grids;telecommunications;power system protection;smart grids;packet switching;circuit and systems;optical fiber networks;SCADA systems;wireless networks;distributed control","Smart grids;Telecommunications;Mission critical systems;Wireless communication;Protocols","","25","","285","CCBY","18 Sep 2020","","","IEEE","IEEE Journals"
"A Multi-Clustering Approach to Scale Distributed Tenant Networks for Mobile Edge Computing","R. Bruschi; F. Davoli; P. Lago; J. F. Pajo","National Laboratory of Smart, Sustainable and Secure Internet Technologies and Infrastructures (S3ITI), Italian National Consortium for Telecommunications (CNIT), Genoa, Italy; Department of Electrical, Electronic and Telecommunications Engineering, and Naval Architecture (DITEN), University of Genoa, Genoa, Italy; National Laboratory of Smart, Sustainable and Secure Internet Technologies and Infrastructures (S3ITI), Italian National Consortium for Telecommunications (CNIT), Genoa, Italy; Department of Electrical, Electronic and Telecommunications Engineering, and Naval Architecture (DITEN), University of Genoa, Genoa, Italy",IEEE Journal on Selected Areas in Communications,"14 Feb 2019","2019","37","3","499","514","Fifth generation (5G) mobile networks will lead to a deep integration between networks and applications. Through novel paradigms like network functions virtualization and edge computing, the new classes of heterogeneous application services will be enabled to run close to the mobile end-user devices with zero-perceived latency and fully-cognitive dynamic reconfiguration capabilities. Such “vertical” applications exhibit diverse performance/scalability requirements, and will rely on highly distributed, extremely virtualized, multi-tenant, and software-defined infrastructures. In such a context, handling the required operations in a scalable and dynamic fashion will be of paramount importance. A specific aspect, addressed by software-defined networking (SDN), regards the provision of suitable communication channels, once resource allocation mechanisms have performed the most efficient deployment of virtual network function instances, and VNF chaining needs to be implemented to enable network services. In this respect, this paper introduces the multi-cluster overlay (MCO) network paradigm: a tunnel-less SDN scheme for scalable realization of virtual tenant networks across the 5G distributed infrastructure, able to support (bulk) migrations of software instances among geo-distributed computing resources in a seamless and effective fashion. The numerical simulation and experimental results show that the MCO achieves up to over one order of magnitude smaller number of forwarding rules than the other state-of-the-art SDN mechanisms, while also assuring high performance during reconfiguration operations.","1558-0008","","10.1109/JSAC.2019.2894236","INPUT and MATILDA projects; European Commission(grant numbers:644672,761898); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8624501","Edge computing;5G;SDN;mobility","5G mobile communication;Logic gates;Edge computing;Network function virtualization;Resource management;Cloud computing;Quality of service","","25","","55","IEEE","23 Jan 2019","","","IEEE","IEEE Journals"
"In.IoT—A New Middleware for Internet of Things","M. A. A. da Cruz; J. J. P. C. Rodrigues; P. Lorenz; V. V. Korotaev; V. H. C. de Albuquerque","Instituto de Telecomunicações, Santa Rita do Sapucaí, Brazil; PPGEE, Federal University of Piauí, Teresina, Brazil; IUT, University of Haute Alsace, Colmar, France; International Institute of Photonics and Optoinformatics, ITMO University, St. Petersburg, Russia; ARMTEC Tecnologia em Robótica, Fortaleza, Brazil",IEEE Internet of Things Journal,"7 May 2021","2021","8","10","7902","7911","The evolution of Internet of Things (IoT) led to the construction of many IoT middleware, a software that plays a key role since it supports the communication among devices, users, and applications. Although various solutions and studies were proposed, they rarely address crucial privacy and security considerations, especially regarding the message queuing telemetry transport (MQTT) protocol. Moreover, in the majority of the solutions, integrating new devices is a time-consuming task performed manually that cannot be accomplished in a scenario with thousands, maybe millions of devices. In this sense, this article proposes a new IoT middleware, called In.IoT, a scalable, secure, and innovative middleware solution that addresses the middleware concerns identified in this article. In.IoT architectural recommendations and requirements are detailed and can be replicated by new and available solutions. It supports MQTT, CoAP, and HTTP as application-layer protocols. Its performance is evaluated in comparison with the most promising solutions available in the literature and the results obtained by the proposed solution are extremely promising. In.IoT is evaluated, demonstrated, validated, and it is ready and available for use.","2327-4662","","10.1109/JIOT.2020.3041699","Fundo de Apoio ao Desenvolvimento das Comunicações, presidential decree no 264/10, November 26, 2010, Republic of Angola; FCT/MCTES through National Funds and when Applicable Co-Funded EU Funds(grant numbers:UIDB/50008/2020); RNP, with resources from MCTIC(grant numbers:01250.075413/2018-04); Centro de Referência em Radiocomunicações Project of the Instituto Nacional de Telecomunicações (Inatel), Brazil; Government of Russian Federation(grant numbers:08-08); Brazilian National Council for Research and Development (CNPq)(grant numbers:304315/2017-6,430274/2018-1,431726/2018-3,309335/2017-5); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9274362","Constrained application protocol (CoAP);hypertext transfer protocol (HTTP);InIoT;Internet of Things (IoT);message queuing telemetry transport (MQTT);middleware;middleware architecture;platform;software","Middleware;Internet of Things;Protocols;Computer architecture;Security;Service-oriented architecture;Servers","","25","","27","IEEE","1 Dec 2020","","","IEEE","IEEE Journals"
"Broadcast Core-Network: Converging Broadcasting With the Connected World","J. Montalban; R. Cabrera; E. Iradier; P. Angueira; Y. Wu; L. Zhang; W. Li; Z. Hong","Department of Communications Engineering, University of the Basque County, Bilbao, Spain; Department of Communications Engineering, University of the Basque County, Bilbao, Spain; Department of Communications Engineering, University of the Basque County, Bilbao, Spain; Department of Communications Engineering, University of the Basque County, Bilbao, Spain; Communications Research Centre Canada, Ottawa, ON, Canada; Communications Research Centre Canada, Ottawa, ON, Canada; Communications Research Centre Canada, Ottawa, ON, Canada; Communications Research Centre Canada, Ottawa, ON, Canada",IEEE Transactions on Broadcasting,"2 Sep 2021","2021","67","3","558","569","The new ways to consume and deliver media content in a wireless environment have led to a significant increase in the demand for spectrum resources. In addition, the arrival of next-generation wireless communication technologies has encouraged the creating of new verticals targeting novel applications, such as industry IoT, e-health, distance learning, smart city, smart-metering. All these new use cases are centred on an IP-based infrastructure. A promising alternative to satisfy the demand for resources is to incorporate a new delivery mechanism that can alleviate the congestion of classical cell-based point-to-point broadband wireless infrastructures. The best-positioned candidate is the next-generation Digital Terrestrial Television system, which offers a very spectrum efficient one-to-many PHY, and for the first time, native IP support. In ATSC 3.0, the whole system was designed with an IP-centric view, whereas DVB has created standards to deliver services to generic IP devices (DVB-I). Nevertheless, there is still a last missing piece to bring the broadcast infrastructure to the IP-connected world: a Broadcast Core Network (BCN). In this paper, the basics of a BCN are presented with new use cases that are attractive to Broadcast Network Operator (BNO). A possible roadmap towards implementation is described, and a service-based architecture for a BCN is presented and discussed.","1557-9611","","10.1109/TBC.2021.3105026","Eusko Jaurlaritza(grant numbers:IT1234-19); PREDOC Grant Program(grant numbers:PRE_2019_2_0037); Spanish Government through Project PHANTOM(grant numbers:RTI2018-099162-B-I00 (MCIU/AEI/FEDER, UE)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9521563","ATSC 3.0;DVB;core network;digital terrestrial television;NFV;SDN","Media;Digital video broadcasting;Broadcasting;Production;IP networks;Wireless communication;Standards","","24","","47","IEEE","24 Aug 2021","","","IEEE","IEEE Journals"
"Online Deployment Algorithms for Microservice Systems With Complex Dependencies","X. He; Z. Tu; M. Wagner; X. Xu; Z. Wang","Faculty of Computing, Harbin Institute of Technology, Harbin, Heilongjiang, China; Faculty of Computing, Harbin Institute of Technology, Harbin, Heilongjiang, China; School of Computer Science, The University of Adelaide, Adelaide, SA, Australia; Faculty of Computing, Harbin Institute of Technology, Harbin, Heilongjiang, China; Faculty of Computing, Harbin Institute of Technology, Harbin, Heilongjiang, China",IEEE Transactions on Cloud Computing,"6 Jun 2023","2023","11","2","1746","1763","Cloud and edge computing have been widely adopted in many application scenarios. With the increasing demand of fast iteration and complexity of business logic, it is challenging to achieve rapid development and continuous delivery in such highly distributed cloud and edge computing environment. At present, the microservice-based architecture has been the dominant deployment style, and a microservice system has to evolve agilely to offer stable Quality of Service (QoS) in the situation where user requirement changes frequently. A lot of research have been conducted to optimally re-deploy microservices to adapt to changing requirements. Nevertheless, complex dependencies between microservices and the existence of multiple instances of one single microservice in a microservice system together have not been fully considered in existing work. This article defines SPPMS, the Service Placement Problem in Microservice Systems that feature complex dependencies and multiple instances, as a Fractional Polynomial Problem (FPP). Considering the high computation complexity of FPP, it is then transformed into a Quadratic Sum-of-Ratios Fractional Problem (QSRFP) which is further solved by the our proposed greedy-based algorithms. Experiments demonstrate that our models and algorithms outperform existing approaches in both qualities of the generated solutions and computation speed.","2168-7161","","10.1109/TCC.2022.3161684","National Key Research and Development Program of China(grant numbers:2018YFB1402500); National Natural Science Foundation of China(grant numbers:61832014,61772155,61832004); Australian Research Council(grant numbers:DP200102364,DP210102670); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9740415","Cloud computing;microservice systems;multiple instance coexistence;service dependencies;service placement","Microservice architectures;Quality of service;Servers;Cloud computing;Time factors;Production facilities;Task analysis","","24","","55","IEEE","23 Mar 2022","","","IEEE","IEEE Journals"
"Exploring the Potential of Blockchain Technology in an IoT-Enabled Environment: A Review","Deepak; P. Gulia; N. S. Gill; M. Yahya; P. Gupta; P. K. Shukla; P. K. Shukla","Department of Computer Science and Applications, Maharshi Dayanand University, Rohtak, India; Department of Computer Science and Applications, Maharshi Dayanand University, Rohtak, India; Department of Computer Science and Applications, Maharshi Dayanand University, Rohtak, India; Department of Computer Science, Oakland University, Rochester, MI, USA; School of Computer Science, University College Dublin, Dublin 4, Ireland; Department of Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, Guntur, Andhra Pradesh, India; Department of Computer Science and Engineering, University Institute of Technology RGPV, Bhopal, Madhya Pradesh, India",IEEE Access,"1 Mar 2024","2024","12","","31197","31227","Internet of Things (IoT) plays an essential contribution in connecting devices and enabling seamless data exchange, leading to increased efficiency and convenience. However, security concerns in IoT systems are significant, as compromised devices can lead to data breaches and privacy violations. Blockchain technology can enhance IoT security by providing decentralized consensus, immutability, and transparent transaction records, ensuring secure and trustworthy communication and data integrity. This review article gives a succinct but thorough understanding of blockchain technology, covering architecture of blockchain, working principles, types, applications, platforms, and its role in the IoT environment. The study highlights potential benefits of blockchain like enhanced security and privacy, and explores its integration with IoT. Additionally, the study discusses various real-world applications, examines blockchain platforms, and addresses the limitations and challenges associated with blockchain technology. This review serves as a valuable resource for researchers and practitioners seeking a deeper understanding of blockchain’s potential and its implications in the IoT landscape.","2169-3536","","10.1109/ACCESS.2024.3366656","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10445250","Blockchain;data exchange;Internet of Things (IoT);privacy;security","Blockchains;Security;Internet of Things;Smart contracts;Proof of Work;Reviews;Computer science;Data transfer;Information exchange;Privacy","","24","","175","CCBYNCND","26 Feb 2024","","","IEEE","IEEE Journals"
"Toward Ubiquitous Semantic Metaverse: Challenges, Approaches, and Opportunities","K. Li; B. P. L. Lau; X. Yuan; W. Ni; M. Guizani; C. Yuen","Real-Time and Embedded Computing Systems Research Centre, Porto, Portugal; Department of Engineering Product Development, Singapore University of Technology and Design, Tampines, Singapore; Digital Productivity and Services Flagship, Commonwealth Scientific and Industrial Research Organization, Sydney, NSW, Australia; Digital Productivity and Services Flagship, Commonwealth Scientific and Industrial Research Organization, Sydney, NSW, Australia; Machine Learning Department, Mohamed Bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE; School of Electrical and Electronics Engineering, Nanyang Technological University, Jurong West, Singapore",IEEE Internet of Things Journal,"11 Dec 2023","2023","10","24","21855","21872","In recent years, ubiquitous semantic Metaverse has been studied to revolutionize immersive cyber-virtual experiences for augmented reality (AR) and virtual reality (VR) users, which leverages advanced semantic understanding and representation to enable seamless, context-aware interactions within mixed-reality environments. This survey focuses on the intelligence and spatiotemporal characteristics of four fundamental system components in ubiquitous semantic Metaverse, i.e., artificial intelligence (AI), spatiotemporal data representation (STDR), Semantic Internet of Things (SIoT), and semantic-enhanced digital twin (SDT). We thoroughly survey the representative techniques of the four fundamental system components that enable intelligent, personalized, and context-aware interactions with typical use cases of the ubiquitous semantic Metaverse, such as remote education, work and collaboration, entertainment and socialization, healthcare, and e-commerce marketing. Furthermore, we outline the opportunities for constructing the future ubiquitous semantic Metaverse, including scalability and interoperability, privacy and security, performance measurement and standardization, as well as ethical considerations and responsible AI. Addressing those challenges is important for creating a robust, secure, and ethically sound system environment that offers engaging immersive experiences for the users and AR/VR applications.","2327-4662","","10.1109/JIOT.2023.3302159","National Funds through the FCT/MCTES (Portuguese Foundation for Science and Technology) through the CISTER Research Unit(grant numbers:UIDP/UIDB/04234/2020); Project ADANET(grant numbers:PTDC/EEICOM/3362/2021); Project IBEX(grant numbers:PTDC/CCI-COM/4280/2021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10208153","Artificial intelligence (AI);augmented reality (AR);data representation;digital twin;Internet of Things;semantic communications;survey;ubiquitous semantic metaverse;virtual reality (VR)","Semantics;Metaverse;Social Internet of Things;Urban areas;Surveys;Ethics;Security;Artificial intelligence;Augmented reality;Digital twins;Virtual reality","","24","","153","IEEE","4 Aug 2023","","","IEEE","IEEE Journals"
"Edge Learning for 6G-Enabled Internet of Things: A Comprehensive Survey of Vulnerabilities, Datasets, and Defenses","M. A. Ferrag; O. Friha; B. Kantarci; N. Tihanyi; L. Cordeiro; M. Debbah; D. Hamouda; M. Al-Hawawreh; K. -K. R. Choo","AI and Digital Science Research Center, Technology Innovation Institute, Abu Dhabi, UAE; Networks and Systems Laboratory, Badji Mokhtar-Annaba University, Annaba, Algeria; School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Canada; AI and Digital Science Research Center, Technology Innovation Institute, Abu Dhabi, UAE; Department of Computer Science, The University of Manchester, Manchester, U.K; Khalifa University 6G Research Center, Khalifa University of Science and Technology, Abu Dhabi, UAE; Department of Computer Science, Labstic Laboratory, Guelma University, Guelma, Algeria; School of Information Technology, Deakin University, Burwood, VIC, Australia; Department of Information Systems and Cyber Security, University of Texas at San Antonio, San Antonio, TX, USA",IEEE Communications Surveys & Tutorials,"21 Nov 2023","2023","25","4","2654","2713","The deployment of the fifth-generation (5G) wireless networks in Internet of Everything (IoE) applications and future networks (e.g., sixth-generation (6G) networks) has raised a number of operational challenges and limitations, for example in terms of security and privacy. Edge learning is an emerging approach to training models across distributed clients while ensuring data privacy. Such an approach when integrated in future network infrastructures (e.g., 6G) can potentially solve challenging problems such as resource management and behavior prediction. However, edge learning (including distributed deep learning) are known to be susceptible to tampering and manipulation. This survey article provides a holistic review of the extant literature focusing on edge learning-related vulnerabilities and defenses for 6G-enabled Internet of Things (IoT) systems. Existing machine learning approaches for 6G–IoT security and machine learning-associated threats are broadly categorized based on learning modes, namely: centralized, federated, and distributed. Then, we provide an overview of enabling emerging technologies for 6G–IoT intelligence. We also provide a holistic survey of existing research on attacks against machine learning and classify threat models into eight categories, namely: backdoor attacks, adversarial examples, combined attacks, poisoning attacks, Sybil attacks, byzantine attacks, inference attacks, and dropping attacks. In addition, we provide a comprehensive and detailed taxonomy and a comparative summary of the state-of-the-art defense methods against edge learning-related vulnerabilities. Finally, as new attacks and defense technologies are realized, new research and future overall prospects for 6G-enabled IoT are discussed.","1553-877X","","10.1109/COMST.2023.3317242","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255264","Edge learning;6G;IoT;federated learning;AI vulnerabilities;security","6G mobile communication;Security;Internet of Things;Surveys;Peer-to-peer computing;Image edge detection;Federated learning","","23","","300","IEEE","19 Sep 2023","","","IEEE","IEEE Journals"
"Resource Allocation With Workload-Time Windows for Cloud-Based Software Services: A Deep Reinforcement Learning Approach","X. Chen; L. Yang; Z. Chen; G. Min; X. Zheng; C. Rong","College of Computer and Data Science, Fuzhou University, Fuzhou, China; College of Computer and Data Science, Fuzhou University, Fuzhou, China; College of Computer and Data Science, Fuzhou University, Fuzhou, China; Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, U.K.; College of Computer and Data Science, Fuzhou University, Fuzhou, China; Department of Electronic Engineering and Computer Science, University of Stavanger, Stavanger, Norway",IEEE Transactions on Cloud Computing,"6 Jun 2023","2023","11","2","1871","1885","As the workloads and service requests in cloud computing environments change constantly, cloud-based software services need to adaptively allocate resources for ensuring the Quality-of-Service (QoS) while reducing resource costs. However, it is very challenging to achieve adaptive resource allocation for cloud-based software services with complex and variable system states. Most of the existing methods only consider the current condition of workloads, and thus cannot well adapt to real-world cloud environments subject to fluctuating workloads. To address this challenge, we propose a novel Deep Reinforcement learning based resource Allocation method with workload-time Windows (DRAW) for cloud-based software services that considers both the current and future workloads in the resource allocation process. Specifically, an original Deep Q-Network (DQN) based prediction model of management operations is trained based on workload-time windows, which can be used to predict appropriate management operations under different system states. Next, a new feedback-control mechanism is designed to construct the objective resource allocation plan under the current system state through iterative execution of management operations. Extensive simulation results demonstrate that the prediction accuracy of management operations generated by the proposed DRAW method can reach 90.69%. Moreover, the DRAW can achieve the optimal/near-optimal performance and outperform other classic methods by 3$\sim$∼13% under different scenarios.","2168-7161","","10.1109/TCC.2022.3169157","National Natural Science Foundation of China(grant numbers:62072108); Natural Science Foundation of Fujian Province for Distinguished Young Scholars(grant numbers:2020J06014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9761727","Cloud-based software services;resource allocation;workload-time windows;deep reinforcement learning;feedback control","Resource management;Cloud computing;Software;Quality of service;Costs;Control theory;Predictive models","","23","","40","IEEE","21 Apr 2022","","","IEEE","IEEE Journals"
"An Integrated Scalable Framework for Cloud and IoT Based Green Healthcare System","M. M. Islam; Z. A. Bhuiyan","Department of Computer Science and Engineering, United International University, Badda, Dhaka, Bangladesh; Department of Computer Science and Engineering, United International University, Badda, Dhaka, Bangladesh",IEEE Access,"9 Mar 2023","2023","11","","22266","22282","Recent developments in IoT-enabled cloud computing and interactive applications have made researchers rethink how healthcare services are currently provided. The IoT-cloud-based systems facilitate remote monitoring and support for patients. However, in the existing area, much emphasis has not been given to making the healthcare systems green. So, in this paper, we present an integrated framework for green healthcare and use cutting-edge technology to make an interactive user interface. We have also ensured the system’s scalability and performance ratio. This system interface has been designed and developed for patients and doctors, where patients can send their healthcare data using wearable sensors, and doctors can receive those data in real-time. For data identification and analysis, we have adopted Hierarchical Clustering Algorithms. Finally, we have come up with a solution for how to make the interactive healthcare experience better for everyone.","2169-3536","","10.1109/ACCESS.2023.3250849","Institute of Advanced Research (IAR), United International University (UIU), through the Research Grant Scheme(grant numbers:UIU/IAR/01/2021/SE/27); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10056935","Internet of Things;smart healthcare framework;cloud computing;interactive digital healthcare;green healthcare","Medical services;Sensors;Internet of Things;Cloud computing;Monitoring;Sensor systems;Green products;Smart healthcare;Green products","","23","","44","CCBYNCND","1 Mar 2023","","","IEEE","IEEE Journals"
"Robust Learning-Enabled Intelligence for the Internet of Things: A Survey From the Perspectives of Noisy Data and Adversarial Examples","Y. Wu","College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, U.K.",IEEE Internet of Things Journal,"4 Jun 2021","2021","8","12","9568","9579","The Internet of Things (IoT) has been widely adopted in a range of verticals, e.g., automation, health, energy, and manufacturing. Many of the applications in these sectors, such as self-driving cars and remote surgery, are critical and high stakes applications, calling for advanced machine learning (ML) models for data analytics. Essentially, the training and testing data that are collected by massive IoT devices may contain noise (e.g., abnormal data, incorrect labels, and incomplete information) and adversarial examples. This requires high robustness of ML models to make reliable decisions for IoT applications. The research of robust ML has received tremendous attention from both academia and industry in recent years. This article will investigate the state of the art and representative works of robust ML models that can enable high resilience and reliability of IoT intelligence. Two aspects of robustness will be focused on, i.e., when the training data of ML models contain noises and adversarial examples, which may typically happen in many real-world IoT scenarios. In addition, the reliability of both neural networks and reinforcement learning framework will be investigated. Both of these two ML paradigms have been widely used in handling data in IoT scenarios. The potential research challenges and open issues will be discussed to provide future research directions.","2327-4662","","10.1109/JIOT.2020.3018691","Engineering and Physical Sciences Research Council of United Kingdom(grant numbers:EP/R030863/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9174633","Efficiency;Internet of Things (IoT);machine learning (ML);reliability;robustness","Internet of Things;Data models;Noise measurement;Robustness;Artificial intelligence;Training","","22","","105","IEEE","24 Aug 2020","","","IEEE","IEEE Journals"
"RLOps: Development Life-Cycle of Reinforcement Learning Aided Open RAN","P. Li; J. Thomas; X. Wang; A. Khalil; A. Ahmad; R. Inacio; S. Kapoor; A. Parekh; A. Doufexi; A. Shojaeifard; R. J. Piechocki","Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K; Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K; Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K; Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K; Vilicom U.K. Ltd., Reading, U.K; Vilicom U.K. Ltd., Reading, U.K; Applied Research, Suffolk, U.K; Applied Research, Suffolk, U.K; Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K; InterDigital Communications Inc., Wilmington, DE, USA; Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K",IEEE Access,"7 Nov 2022","2022","10","","113808","113826","Radio access network (RAN) technologies continue to evolve, with Open RAN gaining the most recent momentum. In the O-RAN specifications, the RAN intelligent controllers (RICs) are software-defined orchestration and automation functions for the intelligent management of RAN. This article introduces principles for machine learning (ML), in particular, reinforcement learning (RL) applications in the O-RAN stack. Furthermore, we review the state-of-the-art research in wireless networks and cast it onto the RAN framework and the hierarchy of the O-RAN architecture. We provide a taxonomy for the challenges faced by ML/RL models throughout the development life-cycle: from the system specification to production deployment (data acquisition, model design, testing and management, etc.). To address the challenges, we integrate a set of existing MLOps principles with unique characteristics when RL agents are considered. This paper discusses a systematic model development, testing and validation life-cycle, termed: RLOps. We discuss fundamental parts of RLOps, which include: model specification, development, production environment serving, operations monitoring and safety/security. Based on these principles, we propose the best practices for RLOps to achieve an automated and reproducible model development process. At last, a holistic data analytics platform rooted in the O-RAN deployment is designed and implemented, aiming to embrace and fulfil the aforementioned principles and best practices of RLOps.","2169-3536","","10.1109/ACCESS.2022.3217511","Innovate UK/CELTIC-NEXT European collaborative project on AI-enabled Massive MIMO (AIMM); Next-Generation Converged Digital Infrastructure (NG-CDI) Project; BT and Engineering and Physical Sciences Research Council (EPSRC)(grant numbers:EP/R004935/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9931127","O-RAN;machine learning;reinforcement learning;MLOps;RLOps;digital twins;data engineering","Radio access networks;Computer architecture;Reinforcement learning;Task analysis;Adaptation models;3GPP;Biological system modeling","","22","","96","CCBY","26 Oct 2022","","","IEEE","IEEE Journals"
"The Many Faces of Edge Intelligence","E. Peltonen; I. Ahmad; A. Aral; M. Capobianco; A. Y. Ding; F. Gil-Castiñeira; E. Gilman; E. Harjula; M. Jurmu; T. Karvonen; M. Kelanti; T. Leppänen; L. Lovén; T. Mikkonen; N. Mohan; P. Nurmi; S. Pirttikangas; P. Sroka; S. Tarkoma; T. Yang","Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, Finland; VTT Technical Research Centre of Finland Ltd., Espoo, Finland; Faculty of Computer Science, University of Vienna, Wien, Austria; Business Innovation Manager, Pordenone, Italy; Department of Engineering Systems and Services, TU Delft, Delft, The Netherlands; Enxeñaría telemática, University of Vigo, Vigo, Spain; Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, Finland; Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, Finland; VTT Technical Research Centre of Finland Ltd., Espoo, Finland; Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, Finland; Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, Finland; Information Technology, Oulu University of Applied Sciences, Oulu, Finland; Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, Finland; Department of Computer Science, University of Helsinki, Helsinki, Finland; Connected Mobility, Technical University of Munich, München, Germany; Department of Computer Science, University of Helsinki, Helsinki, Finland; Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, Finland; Institute of Radiocommunications, Poznan University of Technology, Poznań, Poland; Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, Finland; Pengcheng Laboratory, Shenzhen, China",IEEE Access,"11 Oct 2022","2022","10","","104769","104782","Edge Intelligence (EI) is an emerging computing and communication paradigm that enables Artificial Intelligence (AI) functionality at the network edge. In this article, we highlight EI as an emerging and important field of research, discuss the state of research, analyze research gaps and highlight important research challenges with the objective of serving as a catalyst for research and innovation in this emerging area. We take a multidisciplinary view to reflect on the current research in AI, edge computing, and communication technologies, and we analyze how EI reflects on existing research in these fields. We also introduce representative examples of application areas that benefit from, or even demand the use of EI.","2169-3536","","10.1109/ACCESS.2022.3210584","an international expert group, led by the 6G Flagship at the University of Oulu, Finland(grant numbers:318927,326291,323630); European Union’s Horizon 2020 research and innovation programme(grant numbers:101021808); Marie Skłodowska-Curie(grant numbers:956090); National Science Centre in Poland(grant numbers:2018/29/B/ST7/01241); Austrian Science Fund(grant numbers:Y 904-N31,I 5201-N); CHIST-ERA(grant numbers:CHIST-ERA-19-CES-005); City of Vienna (5G Use Case Challenge InTraSafEd 5G); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9905603","Edge intelligence;edge computing;5G;6G","Artificial intelligence;Edge computing;Cloud computing;Low latency communication;Real-time systems;Technological innovation;Image edge detection","","22","","74","CCBY","28 Sep 2022","","","IEEE","IEEE Journals"
"Exploring the Effectiveness of Service Decomposition in Fog Computing Architecture for the Internet of Things","B. Alturki; S. Reiff-Marganiec; C. Perera; S. De","Department of Informatics, University of Leicester, Leicester, United Kingdom; Department of Informatics, University of Leicester, Leicester, United Kingdom; School of Computer Science and Informatics, Cardiff University, Cardiff, United Kingdom; Institute for Communication Systems, University of Surrey, Guildford, United Kingdom",IEEE Transactions on Sustainable Computing,"9 Jun 2022","2022","7","2","299","312","The Internet of Things (IoT) aims to connect everyday physical objects to the internet. These objects will produce a significant amount of data. The traditional cloud computing architecture aims to process data in the cloud. As a result, a significant amount of data needs to be communicated to the cloud. This creates a number of challenges, such as high communication latency between the devices and the cloud, increased energy consumption of devices during frequent data upload to the cloud, high bandwidth consumption, while making the network busy by sending the data continuously, and less privacy because of less control on the transmitted data to the server. Fog computing has been proposed to counter these weaknesses. Fog computing aims to process data at the edge and substantially eliminate the necessity of sending data to the cloud. However, combining the Service Oriented Architecture (SOA) with the fog computing architecture is still an open challenge. In this paper, we propose to decompose services to create linked-microservices (LMS). Linked-microservices are services that run on multiple nodes but closely linked to their linked-partners. Linked-microservices allow distributing the computation across different computing nodes in the IoT architecture. Using four different types of architectures namely cloud, fog, hybrid, and fog+cloud, we explore and demonstrate the effectiveness of service decomposition by applying four experiments to three different type of datasets. Evaluation of the four architectures shows that decomposing services into nodes reduce the data consumption over the network by 10 - 70 percent. Overall, these results indicate that the importance of decomposing services in the context of fog computing for enhancing the quality of service.","2377-3782","","10.1109/TSUSC.2019.2907405","Saudi Arabian Cultural Mission; King Abdul Aziz University; TagItSmart!; European Horizon 2020 programme(grant numbers:688061); EPSRC PETRAS 2(grant numbers:EP/S035362/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8676374","Internet of Things (IoT);cloud computing;fog computing;edge computing;data analytics;distributed data analytics;constraint awareness","Edge computing;Computer architecture;Cloud computing;Internet of Things;Machine learning algorithms;Data processing;Distributed databases","","21","","56","IEEE","29 Mar 2019","","","IEEE","IEEE Journals"
"A Service Oriented Architecture for the Digitalization and Automation of Distribution Grids","M. Pau; M. Mirz; J. Dinkelbach; P. Mckeever; F. Ponci; A. Monti","Institute for Automation of Complex Power Systems, RWTH Aachen University, Aachen, Germany; Institute for Automation of Complex Power Systems, RWTH Aachen University, Aachen, Germany; Institute for Automation of Complex Power Systems, RWTH Aachen University, Aachen, Germany; Institute for Automation of Complex Power Systems, RWTH Aachen University, Aachen, Germany; Institute for Automation of Complex Power Systems, RWTH Aachen University, Aachen, Germany; Institute for Automation of Complex Power Systems, RWTH Aachen University, Aachen, Germany",IEEE Access,"11 Apr 2022","2022","10","","37050","37063","Modern distribution grids are complex systems that need advanced management for their secure and reliable operation. The Information and Communication Technology domain today offers unprecedented opportunities for the smart design of tools in support of grid operators. This paper presents a new philosophy for the digitalization and automation of distribution grids, based on a modular architecture of microservices implemented via container technology. This architecture enables a service-oriented deployment of the intelligence needed in the Distribution Management Systems, moving beyond the traditional view of monolithic software installations in the control rooms. The proposed architecture unlocks a broad set of possibilities, including cloud-based implementations, extension of legacy systems and fast integration of machine learning-based analytic tools. Moreover, it potentially opens a completely new market of turnkey services for distribution grid management, thus avoiding large upfront investments for grid operators. This paper presents the main concepts and benefits of the proposed philosophy, together with an example of field implementation based on open source components carried out in the context of the European project SOGNO.","2169-3536","","10.1109/ACCESS.2022.3164393","Service Oriented Grid for the Network of the Future (SOGNO) and Platform for Operation of Distribution Networks (PLATONE), which are European Projects; European Union’s Horizon 2020 Research and Innovation Program(grant numbers:774613,864300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9748116","Cloud computing;distribution grid automation;distribution management system;IoT platform;microservice architecture;smart grid","Computer architecture;Cloud computing;Automation;Software;Power systems;Smart grids;Microservice architectures","","21","","73","CCBY","4 Apr 2022","","","IEEE","IEEE Journals"
"SCADA Systems With Focus on Continuous Manufacturing and Steel Industry: A Survey on Architectures, Standards, Challenges and Industry 5.0","M. Sverko; T. G. Grbac; M. Mikuc","Faculty of Electrical Engineering and Computing, University of Zagreb, 10000, Croatia; Department of Engineering, Juraj Dobrila University of Pula, Pula, Croatia; Faculty of Electrical Engineering and Computing, University of Zagreb, 10000, Croatia",IEEE Access,"19 Oct 2022","2022","10","","109395","109430","Recent technological advances encompassed by the smart factory concept have fundamentally changed industrial control systems in the way they are structured and how they operate. Majority of these changes affect Supervisory Control And Data Acquisition (SCADA) systems, shifting them to a higher level of interoperability, heterogeneous networks, big data and toward internet technologies and services in general. However, this transformation does not affect all SCADA systems equally. The immediate industrial environment and controlled processes have a significant impact as well. This paper presents a holistic approach to SCADA systems implemented in continuous flow production control within the steel industry production environment. We outline the multi-layer architecture of the SCADA control framework and the aspects of interoperability and interconnectivity within the architecture reference models, together with the research challenges and opportunities arising from the recent rapid increasement of the industrial control systems complexity and digital transformation under the Industry 4.0 paradigm, resulting in disrupting levels of the traditional automation pyramid based on Purdue model toward a higher level of integration and interoperability enabling cross-level data exchange empowered by the Industrial Internet of Things. Furthermore, the paper addresses the problem of proprietary SCADA systems and elaborates the causal correlation between SCADA quality requirements and adoption of new technologies in relation to the specific industrial environment of the steel manufacturing process.","2169-3536","","10.1109/ACCESS.2022.3211288","Croatian Science Foundation(grant numbers:HRZZ-IP-2019-04-4216); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9907002","Supervisory control and data acquisition;SCADA;supervisory control;data acquisition;industrial process control;cyber-physical;continuous flow production;manufacturing;steel industry;industry 4.0;industry 5.0;smart factory","SCADA systems;Production processes;Data acquisition;Supervisory control;Manufacturing processes;Smart manufacturing;Steel industry;Process control;Cyber-physical systems;Fourth Industrial Revolution;Fifth Industrial Revolution","","21","","215","CCBY","3 Oct 2022","","","IEEE","IEEE Journals"
"Challenges in Building an End-to-End System for Acquisition, Management, and Integration of Diverse Data From Sensor Networks in Watersheds: Lessons From a Mountainous Community Observatory in East River, Colorado","C. Varadharajan; D. A. Agarwal; W. Brown; M. Burrus; R. W. H. Carroll; D. S. Christianson; B. Dafflon; D. Dwivedi; B. J. Enquist; B. Faybishenko; A. Henderson; M. Henderson; V. C. Hendrix; S. S. Hubbard; Z. Kakalia; A. Newman; B. Potter; H. Steltzer; R. Versteeg; K. H. Williams; C. Wilmer; Y. Wu","Lawrence Berkeley National Laboratory, Earth and Environmental Sciences Area, Berkeley, USA; Lawrence Berkeley National Laboratory, Computing Sciences Area, Berkeley, USA; Bugs Unlimited, LLC Crested Butte, USA; Lawrence Berkeley National Laboratory, Earth and Environmental Sciences Area, Berkeley, USA; Desert Research Institute, Reno, USA; Lawrence Berkeley National Laboratory, Computing Sciences Area, Berkeley, USA; Lawrence Berkeley National Laboratory, Earth and Environmental Sciences Area, Berkeley, USA; Lawrence Berkeley National Laboratory, Earth and Environmental Sciences Area, Berkeley, USA; Department of Ecology and Evolutionary Biology, The University of Arizona, Tucson, USA; Lawrence Berkeley National Laboratory, Earth and Environmental Sciences Area, Berkeley, USA; Rocky Mountain Biological Laboratory, Crested Butte, USA; Lawrence Berkeley National Laboratory, Computing Sciences Area, Berkeley, USA; Lawrence Berkeley National Laboratory, Computing Sciences Area, Berkeley, USA; Lawrence Berkeley National Laboratory, Earth and Environmental Sciences Area, Berkeley, USA; Lawrence Berkeley National Laboratory, Earth and Environmental Sciences Area, Berkeley, USA; Rocky Mountain Biological Laboratory, Crested Butte, USA; Navarro Research and Engineering, Grand Junction, USA; Fort Lewis College, Durango, USA; Subsurface Insights, LLC, Hanover, USA; Lawrence Berkeley National Laboratory, Earth and Environmental Sciences Area, Berkeley, USA; Rocky Mountain Biological Laboratory, Crested Butte, USA; Lawrence Berkeley National Laboratory, Earth and Environmental Sciences Area, Berkeley, USA",IEEE Access,"30 Dec 2019","2019","7","","182796","182813","The U.S. Department of Energy's Watershed Function Scientific Focus Area (SFA), centered in the East River, Colorado, generates diverse datasets including hydrological, geological, geochemical, geophysical, ecological, microbiological and remote sensing data. The project has deployed extensive field infrastructure involving hundreds of sensors that measure highly diverse phenomena (e.g. stream and groundwater hydrology, water quality, soil moisture, weather) across the watershed. Data from the sensor network are telemetered and automatically ingested into a queryable database. The data are subsequently quality checked, integrated with the United States Geological Survey's stream monitoring network using a custom data integration broker, and published to a portal with interactive visualizations. The resulting data products are used in a variety of scientific modeling and analytical efforts. This paper describes the SFA's end-to-end infrastructure and services that support the generation of integrated datasets from a watershed sensor network. The development and maintenance of this infrastructure, presents a suite of challenges from practical field logistics to complex data processing, which are addressed through various solutions. In particular, the SFA adopts a holistic view for data collection, assessment and integration, which dramatically improves the products generated, and enables a co-design approach wherein data collection is informed by model results and vice-versa.","2169-3536","","10.1109/ACCESS.2019.2957793","U.S. Department of Energy(grant numbers:DE-AC02-05CH11231); WatershedFunction Scientific Focus Area funded by the U.S. Department of Energy, Office of Science, Office of Biological, and Environmental Research(grant numbers:DE-AC02-05CH11231); National Energy Research Scientific Computing Center (NERSC), U.S. Department of Energy Office of Science User Facility operated(grant numbers:DE-AC02-05CH11231); Environmental Systems Science Data Infrastructure for a Virtual Ecosystem (ESS-DIVE) data repository operated(grant numbers:DE-AC02-05CH11231); Subsurface Insights for development of some of the core abilities used here(grant numbers:DE-SC0009732,DE-SC0018447); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8924700","Sensor systems and applications;sensors;geoscience;water resources;watershed;data management;data integration;data processing;co-design","Rivers;Data models;Monitoring;Temperature measurement;Data collection;Snow;Biology","","21","","58","CCBY","5 Dec 2019","","","IEEE","IEEE Journals"
"Micro-Safe: Microservices- and Deep Learning-Based Safety-as-a-Service Architecture for 6G-Enabled Intelligent Transportation System","C. Roy; R. Saha; S. Misra; K. Dev","Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Institute of Intelligent Systems, University of Johannesburg, Auckland Park, South Africa",IEEE Transactions on Intelligent Transportation Systems,"11 Jul 2022","2022","23","7","9765","9774","In this paper, we propose a microservices and deep learning-based scheme, termed as Micro-Safe, for provisioning Safety-as-a-Service (Safe-aaS) in a 6G environment. A Safe-aaS infrastructure provides customized safety-related decisions dynamically to the registered end-users. As the decisions are time-sensitive in nature, the generation of these decisions should incur minimum latency and high accuracy. Further, scalability and extension of the coverage of the entire Safe-aaS platform are also necessary. Considering road transportation as the application scenario, we propose Safe-aaS, which is a microservices- and deep learning-based platform for provisioning ultra-low latency safety services to the end-users in a 6G scenario. We design the proposed solution in two stages. In the first stage, we develop the microservices-enabled application layer to improve the scalability and adaptability of the traditional Safe-aaS platform. Moreover, we apply the state space model to represent the decision parameters requested and the decision delivered to the end-users. During the second stage, we use deep learning models to improve the accuracy in the decisions delivered to the end-users. Additionally, we apply an assortment of activation functions to analyze and compare the accuracy of the decisions generated in the proposed scheme. Extensive simulation of our proposed scheme, Micro-Safe, demonstrates that latency is improved by 26.1 – 31.2%, energy consumption is reduced by 22.1 – 29.9%, throughput is increased by 26.1 – 31.7%, compared to the existing schemes.","1558-0016","","10.1109/TITS.2021.3110725","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9548710","6G;microservices;deep learning;state space model;safety-as-a-service (safe-aas);road transportation;decision parameters","6G mobile communication;Safety;Deep learning;Scalability;Delays;Cloud computing;Throughput","","20","","28","IEEE","27 Sep 2021","","","IEEE","IEEE Journals"
"Federated Learning-Empowered Mobile Network Management for 5G and Beyond Networks: From Access to Core","J. Lee; F. Solat; T. Y. Kim; H. V. Poor","Department of Computing, Gachon University, Seongnam, Republic of Korea; Department of Computing, Gachon University, Seongnam, Republic of Korea; Network Intelligence Research Section, Electronics and Telecommunications Research Institute, Daejeon, Republic of Korea; Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ, USA",IEEE Communications Surveys & Tutorials,"22 Aug 2024","2024","26","3","2176","2212","The fifth generation (5G) and beyond wireless networks are envisioned to provide an integrated communication and computing platform that will enable multipurpose and intelligent networks driven by a growing demand for both traditional end users and industry verticals. This evolution will be realized by innovations in both core and access capabilities, mainly from virtualization technologies and ultra-dense networks, e.g., software-defined networking (SDN), network slicing, network function virtualization (NFV), multi-access edge computing (MEC), terahertz (THz) communications, etc. However, those technologies require increased complexity of resource management and large configurations of network slices. In this new milieu, with the help of artificial intelligence (AI), network operators will strive to enable AI-empowered network management by automating radio and computing resource management and orchestration processes in a data-driven manner. In this regard, most of the previous AI-empowered network management approaches adopt a traditional centralized training paradigm where diverse training data generated at network functions over distributed base stations associated with MEC servers are transferred to a central training server. On the other hand, to exploit distributed and parallel processing capabilities of distributed network entities in a fast and secure manner, federated learning (FL) has emerged as a distributed AI approach that can enable many AI-empowered network management approaches by allowing for AI training at distributed network entities without the need for data transmission to a centralized server. This article comprehensively surveys the field of FL-empowered mobile network management for 5G and beyond networks from access to the core. Specifically, we begin with an introduction to the state-of-the-art of FL by exploring and analyzing recent advances in FL in general. Then, we provide an extensive survey of AI-empowered network management, including background on 5G network functions, mobile traffic prediction, and core/access network management regarding standardization and research activities. We then present an extensive survey of FL-empowered network management by highlighting how FL is adopted in AI-empowered network management. Important lessons learned from this review of AI and FL-empowered network management are also provided. Finally, we complement this survey by discussing open issues and possible directions for future research in this important emerging area.","1553-877X","","10.1109/COMST.2024.3352910","National Research Foundation of Korea (NRF); Korea Government (MSIT)(grant numbers:2021R1F1A1048098); ICT Research and Development Program of MSICT/IITP (Development of Intelligent 6G Mobile Core Network Technologies)(grant numbers:2022-0-00862); U.S. National Science Foundation(grant numbers:CNS-2128448,ECCS-2335876); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10400810","Federated learning;5G;6G;network management;artificial intelligence;machine learning","Surveys;6G mobile communication;Servers;Artificial intelligence;Internet of Things;5G mobile communication;Training","","20","","222","IEEE","16 Jan 2024","","","IEEE","IEEE Journals"
"Service Management for IoT: Requirements, Taxonomy, Recent Advances and Open Research Challenges","A. I. A. Ahmed; A. Gani; S. H. A. Hamid; A. Abdelmaboud; H. J. Syed; R. A. A. Habeeb Mohamed; I. Ali","Center for Mobile Cloud Computing Research, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia; Center for Mobile Cloud Computing Research, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia; Center for Mobile Cloud Computing Research, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia; Department of Information Systems, King Khalid University, Muhayil Asir, Saudi Arabia; Department of Computer Science, National University of Computer and Emerging Sciences, Karachi, Pakistan; Center for Mobile Cloud Computing Research, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia; Center for Mobile Cloud Computing Research, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia",IEEE Access,"1 Nov 2019","2019","7","","155472","155488","The exponential growth in the number of smart devices connected to the Internet of Things (IoT), and associated with various IoT-based smart applications and services, raises interoperability challenges which could affect the sustainability of IoT services. IoT software applications are built using different software platforms and embedded in diverse types of terminals and sensing devices. Aiming to offer smart services over a range of network technologies that use different communication protocols. The concept of Web service with service-oriented solutions was introduced to cope with the heterogeneity of hardware and software, and to tackle issues of interoperability, flexibility and scalability. The main step of this solution was the integration of Web of Things technologies into smart device networks, with the utilization of IoT gateways. Service management is a crucial factor in sustaining service-oriented solutions in dynamic and highly scalable IoT systems, and is concerned with several issues associated with service provisioning, orchestration, composition and adaption. This work was motivated by the need for robust and flexible service management systems that can meet the requirements for the rapid scalability and heterogeneity associated with the exponential growth of IoT systems. In the literature there is no survey of service management issues and associated research efforts in the field of IoT. In this article, we identify the key requirements for managing IoT services as well as common service management platforms for IoT. We provide a thematic taxonomy based on the important factors, and investigate recent advances in service management for IoT systems. Finally, the major challenges that remain open are presented as a guide for future research directions.","2169-3536","","10.1109/ACCESS.2019.2948027","King Khalid University(grant numbers:R.G.P-319-2019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8874971","Internet of Things;web service;web of things;SOA;microservice;service composition;service orchestration","Internet of Things;Protocols;Interoperability;Web services;Security;Metadata","","20","","80","CCBY","17 Oct 2019","","","IEEE","IEEE Journals"
"Organic 6G Networks: Vision, Requirements, and Research Approaches","M. -I. Corici; F. Eichhorn; R. Bless; M. Gundall; D. Lindenschmitt; B. Bloessl; M. Petrova; L. Wimmer; R. Kreuch; T. Magedanz; H. D. Schotten","Fraunhofer Institute for Open Communication Systems (FOKUS), Berlin, Germany; Fraunhofer Institute for Open Communication Systems (FOKUS), Berlin, Germany; Institute of Telematics (TM), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; German Research Center for Artificial Intelligence GmbH (DFKI), Kaiserslautern, Germany; Institute for Wireless Communication and Navigation (WICON), RPTU Kaiserslautern-Landau, Kaiserslautern, Germany; Secure Mobile Networking Lab (SEEMOO) Technische Universität Darmstadt (TUDa), Darmstadt, Germany; Chair and Institute of Networked Systems (iNETS), RWTH Aachen University, Aachen, Germany; Leibniz Institute for High Performance Microelectronics (IHP), Frankfurt (Oder), Germany; Architekturen der Vermittlungsknoten (AV), Technische Universität Berlin (TUB), Berlin, Germany; Fraunhofer Institute for Open Communication Systems (FOKUS), Berlin, Germany; German Research Center for Artificial Intelligence GmbH (DFKI), Kaiserslautern, Germany",IEEE Access,"19 Jul 2023","2023","11","","70698","70715","Building upon the significant number of already published 6G position papers, we are concentrating on the immediate next steps toward turning the research vision of software-centric networks into reality. This is accomplished, by summarizing and assessing the various requirements documents and providing a significant number of specific research directions and approaches in order to fulfill them. This article complements the existing body of work, by focusing on future core networks and their infrastructures, yet maintaining a system-level perspective and progressing in the direction of scoping key technology elements and providing high-potential research approaches for them. Additionally, we rigorously discuss the impact that different technological advancements have on the other parts of the system, to provide a coherent, end-to-end network understanding. This is in strong contrast to current approaches, where from the challenges, each research direction becomes independent and, thus, its advances are potentially cancelled out by the next technology in the chain. By maintaining this system perspective, the adoption of the different technologies becomes easier, as they are developed in unison. To address the requirements in a coherent, holistic, and unified way, we extend our high-level architecture concept named “Organic 6G Networks” towards a comprehensive end-to-end system. A holistic software-centric system, adapting the latest software development advancements from the IT industry. The Organic 6G network provides support for building a streamlined software network architecture and offers the next step on the path towards the development and specification of future mobile networks.","2169-3536","","10.1109/ACCESS.2023.3293055","German Federal Ministry for Education and Research (BMBF) through the Project Open6GHub(grant numbers:16KISK003K,16KISK004,16KISK006,16KISK010,16KISK013,16KISK014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10175535","Mobile networks;6G;core networks;organic core networks;organic networks","6G mobile communication;5G mobile communication;Computer architecture;Software engineering;Network architecture;Wireless communication;Complexity theory;Organic electronics","","20","","45","CCBY","7 Jul 2023","","","IEEE","IEEE Journals"
"Mobility-Aware IoT Application Placement in the Cloud – Edge Continuum","D. Kimovski; N. Mehran; C. E. Kerth; R. Prodan","Institute of Information Technology, University of Klagenfurt, Klagenfurt am Wörthersee, Austria; Institute of Information Technology, University of Klagenfurt, Klagenfurt am Wörthersee, Austria; Institute of Information Technology, University of Klagenfurt, Klagenfurt am Wörthersee, Austria; Institute of Information Technology, University of Klagenfurt, Klagenfurt am Wörthersee, Austria",IEEE Transactions on Services Computing,"14 Dec 2022","2022","15","6","3358","3371","The Edge computing extension of the Cloud services towards the network boundaries raises important placement challenges for IoT applications running in a heterogeneous environment with limited computing capacities. Unfortunately, existing works only partially address this challenge by optimizing a single or aggregate objective (e.g., response time), and not considering the edge devices’ mobility and resource constraints. To address this gap, we propose a novel mobility-aware multi-objective IoT application placement (mMAPO) method in the Cloud – Edge Continuum that optimizes completion time, energy consumption, and economic cost as conflicting objectives. mMAPO utilizes a Markov model for predictive analysis of the Edge device mobility and constrains the optimization to devices that do not frequently move through the network. We evaluate the quality of the mMAPO placements using simulation and real-world experimentation on two IoT applications. Compared to related work, mMAPO reduces the economic cost by 28 percent and decreases the completion time by 80 percent while maintaining a stable energy consumption.","1939-1374","","10.1109/TSC.2021.3094322","European Commission(grant numbers:101016835); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9473013","Cloud;edge continuum;mobility;application placement;multi-objective optimization;energy consumption;cost","Internet of Things;Cloud computing;Energy consumption;Optimization;Predictive models;Markov processes;Economics","","19","","46","CCBY","2 Jul 2021","","","IEEE","IEEE Journals"
"ChainFaaS: An Open Blockchain-Based Serverless Platform","S. Ghaemi; H. Khazaei; P. Musilek","Department of Electrical and Computer Engineering, University of Alberta, Edmonton, Alberta, Canada; Department of Electrical Engineering and Computer Science, York University, Toronto, Canada; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, Alberta, Canada",IEEE Access,"24 Jul 2020","2020","8","","131760","131778","Due to the rapid increase in the total amount of data generated in the world, the need for more computational resources is also increasing dramatically. This trend results in huge data centers and massive server farms being built around the world, which have a negative impact on global carbon emissions. On the other hand, there are many underutilized personal computers around the world that can be used towards distributed computing. To better understand the capacity of personal computers, we have conducted a survey that aims to find their unused computational power. The results indicate that the typical CPU utilization of a personal computer is only 24.5% and, on average, a personal computer is only used 4.5 hours per day. This shows a significant computational potential that can be used towards distributed computing. In this paper, we introduce ChainFaaS with the motivation to use the computational capacity of personal computers as well as to improve developers' experience of internet-based computing services by reducing their costs, enabling transparency, and providing reliability. ChainFaaS is an open, public, blockchain-based serverless platform that takes advantage of personal computers' computational capacity to run serverless tasks. If a substantial number of personal computers were connected to this platform, some tasks could be offloaded from data centers. As a result, the need for building new data centers would be reduced with a positive impact on the environment. We have proposed the design of ChainFaaS, and then implemented and evaluated a prototype of this platform to show its feasibility.","2169-3536","","10.1109/ACCESS.2020.3010119","Future Energy Systems Research Initiative under the Canada First Research Excellence Fund (CFREF) at the University of Alberta, Canada; Compute Canada; SAVI Testbed Cloud; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9143110","Blockchain;distributed ledger technology;serverless computing;green computing;distributed computing","Microcomputers;Cloud computing;Task analysis;Data centers;Servers;Carbon dioxide","","19","","58","CCBY","17 Jul 2020","","","IEEE","IEEE Journals"
"Performance Evaluation of a Satellite Communication-Based MEC Architecture for IoT Applications","M. Luglio; M. Marchese; F. Patrone; C. Roseti; F. Zampognaro","Electronics Department, University of Rome “Tor Vergata,”, Rome, Italy; Electrical, Electronics and Telecommunication Engineering and Naval Architecture Department (DITEN), University of Genoa, Genoa, Italy; Electrical, Electronics and Telecommunication Engineering and Naval Architecture Department (DITEN), University of Genoa, Genoa, Italy; Electronics Department, University of Rome “Tor Vergata,”, Rome, Italy; Electronics Department, University of Rome “Tor Vergata,”, Rome, Italy",IEEE Transactions on Aerospace and Electronic Systems,"10 Oct 2022","2022","58","5","3775","3785","New scenarios and use cases are raising following the birth of the fifth generation of mobile communications. The Internet of Things (IoT) is one of the main use cases which are growing, leading to a massive amount of data that need to be exchanged throughout the Internet. Satellite communication networks are essential in remote and isolated environments and can support fully connected environments by offloading the terrestrial infrastructure concerning delay–tolerant traffic flows. However, satellite network resources are limited and expensive, so they need to be carefully used in order to avoid waste and satisfy the required user performance. The multi-access edge computing (MEC) concept can be exploited in this context to allow data preprocessing at the edge, i.e., close to the users, so reducing the amount of data that has to traverse the backhaul satellite link and, in some cases, reducing data delivery times. This article analyses the performance of a satellite architecture in the IoT framework highlighting the advantages brought by MEC, also including data aggregation and compression techniques.","1557-9603","","10.1109/TAES.2022.3199330","ESA; VIBeS project(grant numbers:4000122991/18/U.K./ND); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858607","Internet of Things (IoT);Message Queuing Telemetry Transport (MQTT);multi-access edge computing (MEC);satellite communications (SatCom)","Satellites;Internet of Things;Protocols;Satellite broadcasting;Computer architecture;Performance evaluation;Bandwidth","","19","","42","CCBY","17 Aug 2022","","","IEEE","IEEE Journals"
"Performance Evaluation of Containerization in Edge-Cloud Computing Stacks for Industrial Applications: A Client Perspective","Y. Liu; D. Lan; Z. Pang; M. Karlsson; S. Gong","Department of Science and Technology, Linköping University, Norrköping, Sweden; Department of Informatics, University of Oslo, Oslo, Norway; ABB AB, Corporate Research, Forskargränd 7, Västerås, Västmanland, Sweden; Department of Science and Technology, Linköping University, Norrköping, Sweden; Department of Science and Technology, Linköping University, Norrköping, Sweden",IEEE Open Journal of the Industrial Electronics Society,"19 Feb 2021","2021","2","","153","168","Today, the edge-cloud computing paradigm starts to gain increasing popularity, aiming to enable short latency, fast decision-making and intelligence at the network edge, especially for industrial applications. The container-based virtualization technology has been put on the roadmap by the industry to implement edge-cloud computing infrastructures. Has the performance of the container-based edge-cloud computing stacks reached industry requirement? In this paper, from the industrial client perspective, we provide a performance evaluation methodology and apply it to the state-of-the-art containerization-based edge-cloud computing infrastructures. The influences of the message sending interval, payload, network bandwidth and concurrent devices on full stack latency are measured, and the processing capability of executing machine learning tasks are benchmarked. The results show that containerization on the edge does not introduce noticeable performance degradation in terms of communication, computing and intelligence capabilities, making it a promising technology for the edge-cloud computing paradigm. However, there is a large room for performance improvement between current implementation of the edge-cloud infrastructure and the demanding requirements anticipated by time-critical industrial applications. We also emphasize and showcase that partitioning of an industrial application into microservices throughout the whole stack can be considered during solution design. The proposed evaluation methodology can be a reference to users of edge-cloud computing as well as developers to get a client perspective overview of system performance.","2644-1284","","10.1109/OJIES.2021.3055901","Swedish Innovation Agency, Vinnova; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9343777","Performance evaluation;edge-cloud computing;containerization;partitioning","Cloud computing;Performance evaluation;Computational modeling;Containers;Protocols;Industrial electronics;Computer architecture","","19","","57","CCBY","1 Feb 2021","","","IEEE","IEEE Journals"
"Diktyo: Network-Aware Scheduling in Container-Based Clouds","J. Santos; C. Wang; T. Wauters; F. De Turck","Department of Information Technology, IDLab, Ghent University-imec, Gent, Belgium; AI Cloud Platform Department, IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; Department of Information Technology, IDLab, Ghent University-imec, Gent, Belgium; Department of Information Technology, IDLab, Ghent University-imec, Gent, Belgium",IEEE Transactions on Network and Service Management,"12 Dec 2023","2023","20","4","4461","4477","Containers have revolutionized application deployment and life-cycle management in current cloud platforms. Applications have evolved from single monoliths to complex graphs of loosely-coupled microservices. However, the efficient allocation of microservice-based applications is challenging due to their complex inter-dependencies. Further, recent applications are becoming even more delay-sensitive, demanding lower latency between dependent microservices. Scheduling policies in popular container orchestration platforms mainly aim to increase the resource efficiency of the infrastructure, insufficient for latency-sensitive applications. Application domains such as the Internet of Things and multi-tier Web services would benefit from network-aware policies that consider network latency and bandwidth in the scheduling process. Previous works have studied network-aware scheduling via theoretical formulations or heuristic-based methods evaluated via simulations or small testbeds, making their full applicability in popular platforms difficult. This paper proposes a novel network-aware framework for the popular Kubernetes (K8s) platform named Diktyo that determines the placement of dependent microservices in long-running applications focused on reducing the application’s end-to-end latency and guaranteeing bandwidth reservations. Simulations show that Diktyo can significantly reduce the network latency for various applications across different infrastructure topologies compared to default K8s scheduling plugins. Also, experiments in a K8s cluster with microservice benchmark applications show that Diktyo can increase database throughput by 22% and reduce application response time by 45%.","1932-4537","","10.1109/TNSM.2023.3271415","Research Foundation Flanders (FWO)(grant numbers:1299323N); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10111024","Microservices;container scheduling;Kubernetes;network-aware","Microservice architectures;Containers;Network topology;Topology;Bandwidth;Resource management;Cloud computing","","18","","99","IEEE","28 Apr 2023","","","IEEE","IEEE Journals"
"Statistical Analysis of Remote Health Monitoring Based IoT Security Models & Deployments From a Pragmatic Perspective","K. Ashok; S. Gopikrishnan","School of Computer Science and Engineering, VIT-AP University, Amaravati, Andhra Pradesh, India; School of Computer Science and Engineering, VIT-AP University, Amaravati, Andhra Pradesh, India",IEEE Access,"11 Jan 2023","2023","11","","2621","2651","Remote health monitoring-based Internet of Things (IoT) network security is a multi-domain task, that involves identification of network attack, evaluation of mitigation strategies, design of performance aware data security models, integration of privacy models, and modeling of device-level security methods. Internal designs for each of these models is highly complex, and varies in terms of quantitative & qualitative performance measures. This is due to their variation in terms of design nuances, functional advantages, context-based limitations, and possible deployment-specific future scopes. Due to this variation, it is highly ambiguous to select these models for performance-specific IoT deployments. Moreover, these models also vary in terms of security level, Quality of Service (QoS) parameters, scalability performance, computational complexity, deployment costs, and other performance metrics. Thus, to identify optimum models, researchers & network designers are required to test & validate multiple security models for their deployments. Due to which, the cost & time to market for IoT devices is increased, thereby affecting viability of IoT products. To overcome these selection issues, an empirical survey of different IoT security models including block-chains, encryption techniques, hashing models, privacy preservation techniques, machine learning based security methods, etc. are discussed in this text. This text also discusses various attack mitigation models that provide node-level security, network-level security, physical security, & route-level security. This discussion will assist in initially evaluating different operating characteristics of these models, which will allow readers to identify most suited models for their application-specific use cases. This article also assesses the models’ performance in terms of computational latency, energy consumption, security levels, deployment complexity, and scalability measures. These metrics are compared between different security models, which will further assist readers to identify optimum models for their performance-specific use cases. To further assist in model selection, this text proposes evaluation of a novel IoT Security Performance Rank (ISRP), that combines various performance metrics to form a singular rank which can be used to describe overall performance of these models. Readers will be able to consider optimal security approaches for new and current IoT installations based on this ranking.","2169-3536","","10.1109/ACCESS.2023.3234632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10007827","IoT;security;blockchain;QoS;medical signal detection;energy;attacks;data;route;physical;privacy","Security;Medical services;Internet of Things;Computational modeling;Blockchains;Data models;Monitoring;Quality of service;Signal detection;Remote monitoring","","18","","137","CCBYNCND","5 Jan 2023","","","IEEE","IEEE Journals"
"A Comprehensive Survey on Auction Mechanism Design for Cloud/Edge Resource Management and Pricing","N. Sharghivand; F. Derakhshan; N. Siasi","Department of Computer Engineering, University of Tabriz, Tabriz, Iran; Department of Computer Engineering, University of Tabriz, Tabriz, Iran; Department of Physics, Computer Science and Engineering, Christopher Newport University, Newport News, VA, USA",IEEE Access,"17 Sep 2021","2021","9","","126502","126529","Driven by the vision of cloud and edge computing as the 5th utility, a major challenge is the efficient resource management and pricing taking into account incentives and preferences of cloud/edge providers, users, and the system. In recent years, auction mechanism design has gained wide attention as a tool for addressing this challenge. Here existing surveys lack detailed studies in this area, hence this work presents a comprehensive survey on the state-of-the-art auction-based mechanisms in the field of cloud/edge computing. First, the problem statements and background are presented for cloud/edge computing and auction theory. This is followed by a collaboration model on the benefits of auction mechanism designs to cloud/edge computing. Next, various auction mechanisms in cloud computing are reviewed based upon the following aspects: (1) direction of bids-forward, reverse, and two-sided auctions, (2) the heterogeneity of resources-combinatorial auctions, (3) the number and types of adopted attributes in winner determination-multi-attribute auctions, (4) the setting of auction-offline, online, or sequential auctions, and (5) the interoperability among cloud providers-inter-clouds. For each aspect, existing studies are summarized along with their saliencies and drawbacks. Moreover, existing auction mechanisms in the context of edge computing are also presented. Our comprehensive survey shows that although significant progress has been achieved in this field, there still exist key challenges that need to be investigated, as discussed here to provide future research directions.","2169-3536","","10.1109/ACCESS.2021.3110914","Iran National Science Foundation (INSF); University of Tabriz(grant numbers:97013453); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9530537","Auction mechanism design;cloud computing;edge computing;resource management;resource pricing","Cloud computing;Resource management;Pricing;Computational modeling;Edge computing;Servers;Software as a service","","18","","162","CCBY","7 Sep 2021","","","IEEE","IEEE Journals"
"Enabling All In-Edge Deep Learning: A Literature Review","P. Joshi; M. Hasanuzzaman; C. Thapa; H. Afli; T. Scully","Computer Science Department, Munster Technological University, Cork, Ireland; Computer Science Department, Munster Technological University, Cork, Ireland; CSIRO Data61, Marsfield, NSW, Australia; Computer Science Department, Munster Technological University, Cork, Ireland; Computer Science Department, Munster Technological University, Cork, Ireland",IEEE Access,"12 Jan 2023","2023","11","","3431","3460","In recent years, deep learning (DL) models have demonstrated remarkable achievements on non-trivial tasks such as speech recognition, image processing, and natural language understanding. One of the significant contributors to the success of DL is the proliferation of end devices that act as a catalyst to provide data for data-hungry DL models. However, computing DL training and inference still remains the biggest challenge. Moreover, most of the time central cloud servers are used for such computation, thus opening up other significant challenges, such as high latency, increased communication costs, and privacy concerns. To mitigate these drawbacks, considerable efforts have been made to push the processing of DL models to edge servers (a mesh of computing devices near end devices). Recently, the confluence point of DL and edge has given rise to edge intelligence (EI), defined by the International Electrotechnical Commission (IEC) as the concept where the data is acquired, stored, and processed utilizing edge computing with DL and advanced networking capabilities. Broadly, EI has six levels of categories based on the three locations where the training and inference of DL take place, e.g., cloud server, edge server, and end devices. This survey paper focuses primarily on the fifth level of EI, called all in-edge level, where DL training and inference (deployment) are performed solely by edge servers. All in-edge is suitable when the end devices have low computing resources, e.g., Internet-of-Things, and other requirements such as latency and communication cost are important such as in mission-critical applications (e.g., health care). Besides, 5G/6G networks are envisioned to use all in-edge. Firstly, this paper presents all in-edge computing architectures, including centralized, decentralized, and distributed. Secondly, this paper presents enabling technologies, such as model parallelism, data parallelism, and split learning, which facilitates DL training and deployment at edge servers. Thirdly, model adaptation techniques based on model compression and conditional computation are described because the standard cloud-based DL deployment cannot be directly applied to all in-edge due to its limited computational resources. Fourthly, this paper discusses eleven key performance metrics to evaluate the performance of DL at all in-edge efficiently. Finally, several open research challenges in the area of all in-edge are presented.","2169-3536","","10.1109/ACCESS.2023.3234761","ADVANCE CRT Ph.D. Cohort(grant numbers:18/CRT/6222); ADAPT SFI Research Centre at Munster Technological University; Science Foundation Ireland through the SFI Research Centres Programme; European Regional Development Fund (ERDF)(grant numbers:13/RC/2106\_P2); Horizon 2020 projects STOP Obesity Platform(grant numbers:823978); ITFLOWS(grant numbers:882986); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10007810","Artificial intelligence;all in-edge;deep learning;distributed systems;decentralized systems;edge intelligence","Cloud computing;Servers;Computational modeling;Training data;Edge computing;Adaptation models;Deep learning;Artificial intelligence;Edge computing;Distributed processing","","18","","287","CCBY","5 Jan 2023","","","IEEE","IEEE Journals"
"Explainable AI for 6G Use Cases: Technical Aspects and Research Challenges","S. Wang; M. A. Qureshi; L. Miralles-Pechuán; T. Huynh-The; T. R. Gadekallu; M. Liyanage","School of Computer Science, University College Dublin, Dublin 4, Ireland; ADAPT Centre, Explainable Analytics Group, Faculty of Business, Technological University Dublin, Dublin 2, Ireland; School of Computing, Technological University Dublin, Dublin 7, Ireland; Department of Computer and Communications Engineering, Ho Chi Minh City University of Technology and Education, Ho Chi Minh City, Vietnam; Division of Research and Development, Lovely Professional University, Phagwara, India; School of Computer Science, University College Dublin, Dublin 4, Ireland",IEEE Open Journal of the Communications Society,"1 May 2024","2024","5","","2490","2540","Around 2020, 5G began its commercialization journey, and discussions about the next-generation networks (such as 6G) emerged. Researchers predict that 6G networks will have higher bandwidth, coverage, reliability, energy efficiency, and lower latency, and will be an integrated “human-centric” network system powered by artificial intelligence (AI). This 6G network will lead to many real-time automated decisions, ranging from network resource allocation to collision avoidance for self-driving cars. However, there is a risk of losing control over decision-making due to the high-speed, data-intensive AI decision-making that may go beyond designers’ and users’ comprehension. To mitigate this risk, explainable AI (XAI) methods can be used to enhance the transparency of the black-box AI decision-making process. This paper surveys the application of XAI towards the upcoming 6G age, including 6G technologies (such as intelligent radio and zero-touch network management) and 6G use cases (such as industry 5.0). Additionally, the paper summarizes the lessons learned from recent attempts and outlines important research challenges in applying XAI for 6G use cases soon.","2644-125X","","10.1109/OJCOMS.2024.3386872","European Commission in SPATIAL(grant numbers:101021808); Academy of Finland in 6Genesis(grant numbers:318927); Science Foundation Ireland through CONNECT Phase 2 Project(grant numbers:13/RC/2077_P2); ADAPT Centre Phase 2 Project(grant numbers:13/RC/2106_P2); Industry Fellowship(grant numbers:21/IRDIF/9839); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10499970","B5G;6G;AI;XAI;explainability","6G mobile communication;Artificial intelligence;5G mobile communication;Explainable AI;Resource management;Security;Closed box","","18","","303","CCBYNCND","16 Apr 2024","","","IEEE","IEEE Journals"
"Split Federated Learning for 6G Enabled-Networks: Requirements, Challenges, and Future Directions","H. Hafi; B. Brik; P. A. Frangoudis; A. Ksentini; M. Bagaa","Faculty of New Information and Communication Technologies, Abdelhamid Mehri University, Constantine, Algeria; Computer Science Department, College of Computing and Informatics, Sharjah University, Sharjah, United Arab Emirates; Distributed Systems Group, TU Wien, Vienna, Austria; Communication Systems Department, EURECOM, Sophia-Antipolis, France; Department of Electrical and Computer Engineering, Université du Québec à Trois-Rivières, Trois-Rivières, Canada",IEEE Access,"22 Jan 2024","2024","12","","9890","9930","Sixth-generation (6G) networks anticipate intelligently supporting a wide range of smart services and innovative applications. Such a context urges a heavy usage of Machine Learning (ML) techniques, particularly Deep Learning (DL), to foster innovation and ease the deployment of intelligent network functions/operations, which are able to fulfill the various requirements of the envisioned 6G services. The revolution of 6G networks is driven by massive data availability, moving from centralized and big data towards small and distributed data. This trend has motivated the adoption of distributed and collaborative ML/DL techniques. Specifically, collaborative ML/DL consists of deploying a set of distributed agents that collaboratively train learning models without sharing their data, thus improving data privacy and reducing the time/communication overhead. This work provides a comprehensive study on how collaborative learning can be effectively deployed over 6G wireless networks. In particular, our study focuses on Split Federated Learning (SFL), a technique that recently emerged promising better performance compared with existing collaborative learning approaches. We first provide an overview of three emerging collaborative learning paradigms, including federated learning, split learning, and split federated learning, as well as of 6G networks along with their main vision and timeline of key developments. We then highlight the need for split federated learning towards the upcoming 6G networks in every aspect, including 6G technologies (e.g., intelligent physical layer, intelligent edge computing, zero-touch network management, intelligent resource management) and 6G use cases (e.g., smart grid 2.0, Industry 5.0, connected and autonomous systems). Furthermore, we review existing datasets along with frameworks that can help in implementing SFL for 6G networks. We finally identify key technical challenges, open issues, and future research directions related to SFL-enabled 6G networks.","2169-3536","","10.1109/ACCESS.2024.3351600","European Union’s Horizon 2020 Research and Innovation Program under the Agile and Cognitive Cloud edge Continuum management (AC3) Project(grant numbers:101093129); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385040","6G networks;wireless communication;federated deep learning;split deep learning;split federated learning","6G mobile communication;Federated learning;Artificial intelligence;Surveys;Training;Wireless networks;Market research;Smart devices","","17","","240","CCBYNCND","9 Jan 2024","","","IEEE","IEEE Journals"
"QoE-Driven IoT Architecture: A Comprehensive Review on System and Resource Management","B. Saovapakhiran; W. Naruephiphat; C. Charnsripinyo; S. Baydere; S. Özdemir","National Electronics and Computer Technology Center, NSTDA, Pathumthani, Thailand; National Electronics and Computer Technology Center, NSTDA, Pathumthani, Thailand; National Electronics and Computer Technology Center, NSTDA, Pathumthani, Thailand; Department of Computer Engineering, Yeditepe University, Istanbul, Turkey; Department of Computer Engineering, Hacettepe University, Ankara, Turkey",IEEE Access,"17 Aug 2022","2022","10","","84579","84621","Internet of Things (IoT) services have grown substantially in recent years. Consequently, IoT service providers (SPs) are emerging in the market and competing to offer their services. Many IoT applications utilize these services in an integrated manner with different Quality-of-Service (QoS) requirements. Thus, the provisioning of end-to-end QoS is getting more indispensable for IoT platforms. However, provisioning the system by using only QoS metrics without considering user experiences is not sufficient. Recently, Quality of Experience (QoE) model has become a promising approach to quantify actual user experiences of services. A holistic design approach that considers constraints of various QoS/QoE metrics together is needed to satisfy requirements of these applications and services. Besides, IoT services may operate in environments with limited resources. Therefore, effective management of services and system resources is essential for QoS/QoE support. This paper provides a comprehensive survey for the state-of-the-art studies on IoT services with QoS/QoE perspective. Our contributions are threefold: 1) QoE-driven architecture is demonstrated by classifying vital components according to QoE-related functions in prior studies; 2) QoE metrics and QoE optimization objectives are classified by corresponding system and resource control problems in the architecture; and 3) QoE-aware resource management e.g., QoE-aware offloading, placement and data caching policies with recent Machine Learning approaches are extensively reviewed.","2169-3536","","10.1109/ACCESS.2022.3197585","National Science and Technology Development Agency (NSTDA), Ministry of Higher Education, Science, Research and Innovation, Thailand(grant numbers:P1952667); The Scientific and Technological Research Council of Turkey (TUBITAK), Turkey(grant numbers:119N049); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9852447","Internet of Things;quality of service;quality of experience;IoT services;IoT applications;QoS for IoT services;QoS metrics;QoE metrics;IoT architecture","Quality of experience;Quality of service;Internet of Things;Optimization;Computer architecture;Resource management","","17","","166","CCBY","8 Aug 2022","","","IEEE","IEEE Journals"
"The Security and Privacy of Mobile-Edge Computing: An Artificial Intelligence Perspective","C. Wang; Z. Yuan; P. Zhou; Z. Xu; R. Li; D. O. Wu","Hubei Engineering Research Center on Big Data Security, Key Laboratory of Distributed System Security of Hubei Province, School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; Hubei Engineering Research Center on Big Data Security, Key Laboratory of Distributed System Security of Hubei Province, School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; Hubei Engineering Research Center on Big Data Security, Key Laboratory of Distributed System Security of Hubei Province, School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; School of Software and the Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, Dalian University of Technology, Dalian, China; Intelligent and Distributed Computing Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong",IEEE Internet of Things Journal,"11 Dec 2023","2023","10","24","22008","22032","Mobile-edge computing (MEC) is a new computing paradigm that enables cloud computing and information technology (IT) services to be delivered at the network’s edge. By shifting the load of cloud computing to individual local servers, MEC helps meet the requirements of ultralow latency, localized data processing, and extends the potential of the Internet of Things (IoT) for end-users. However, the crosscutting nature of MEC and the multidisciplinary components necessary for its deployment have presented additional security and privacy concerns. Fortunately, artificial intelligence (AI) algorithms can cope with excessively unpredictable and complex data, which offers a distinct advantage in dealing with sophisticated and developing adversaries in the security industry. Hence, in this article, we comprehensively provide a survey of security and privacy in MEC from the perspective of AI. On the one hand, we use European Telecommunications Standards Institute (ETSI) MEC reference architecture as our-based framework while merging the software-defined network (SDN) and network function virtualization (NFV) to better illustrate a serviceable platform of MEC. On the other hand, we focus on new security and privacy issues, as well as potential solutions from the viewpoints of AI. Finally, we comprehensively discuss the opportunities and challenges associated with applying AI to MEC security and privacy as possible future research directions.","2327-4662","","10.1109/JIOT.2023.3304318","National Natural Science Foundation of China(grant numbers:61972448,62172068,61802048); Hong Kong Research Grants Council, General Research Fund(grant numbers:11203523); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10214503","Artificial intelligence (AI);fifth generation (5G);Internet of Things (IoT);machine learning (ML);mobile-edge computing (MEC);security and privacy;software-defined network (SDN) security;virtual machine security","Security;Privacy;Artificial intelligence;Internet of Things;Cloud computing;Data privacy;Computer architecture","","17","","165","IEEE","11 Aug 2023","","","IEEE","IEEE Journals"
"Performance Modeling of Metric-Based Serverless Computing Platforms","N. Mahmoudi; H. Khazaei","Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada; Department of Electrical Engineering and Computer Science, York University, Toronto, ON, Canada",IEEE Transactions on Cloud Computing,"6 Jun 2023","2023","11","2","1899","1910","Analytical performance models are very effective in ensuring the quality of service and cost of service deployment remain desirable under different conditions and workloads. While various analytical performance models have been proposed for previous paradigms in cloud computing, serverless computing lacks such models that can provide developers with performance guarantees. Besides, most serverless computing platforms still require developers’ input to specify the configuration for their deployment that could affect both the performance and cost of their deployment, without providing them with any direct and immediate feedback. In previous studies, we built such performance models for steady-state and transient analysis of scale-per-request serverless computing platforms (e.g., AWS Lambda, Azure Functions, Google Cloud Functions) that could give developers immediate feedback about the quality of service and cost of their deployments. In this work, we aim to develop analytical performance models for latest trend in serverless computing platforms that use concurrency value and the rate of requests per second for autoscaling decisions. Examples of such serverless computing platforms are Knative and Google Cloud Run (a managed Knative service by Google). The proposed performance model can help developers and providers predict the performance and cost of deployments with different configurations which could help them tune the configuration toward the best outcome. We validate the applicability and accuracy of the proposed performance model by extensive real-world experimentation on Knative and show that our performance model is able to accurately predict the steady-state characteristics of a given workload with minimal amount of data collection.","2168-7161","","10.1109/TCC.2022.3169619","Shared Hierarchical Academic Research Computing Network; Compute Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9763051","Google cloud run;knative;metric-based autoscaling;optimization;performance modelling;serverless computing;stochastic processes","Measurement;Computational modeling;Serverless computing;Concurrent computing;Monitoring;Containers;Analytical models","","17","","50","IEEE","26 Apr 2022","","","IEEE","IEEE Journals"
"Task Scheduling Mechanisms for Fog Computing: A Systematic Survey","M. Hosseinzadeh; E. Azhir; J. Lansky; S. Mildeova; O. H. Ahmed; M. H. Malik; F. Khan","Institute of Research and Development, Duy Tan University, Da Nang, Vietnam; Research and Development Center, Mobile Telecommunication Company of Iran, Tehran, Iran; Department of Computer Science and Mathematics, Faculty of Economic Studies, University of Finance and Administration, Prague, Czech Republic; Department of Computer Science and Mathematics, Faculty of Economic Studies, University of Finance and Administration, Prague, Czech Republic; Department of Information Technology, University of Human Development, Sulaymaniyah, Iraq; School of Computing and Creative Technologies, College of Arts, Technology and Environment (CATE), University of the West of England, Bristol, U.K.; Department of Computer Engineering, Gachon University, Seongnam, South Korea",IEEE Access,"5 Jun 2023","2023","11","","50994","51017","In the Internet of Things (IoT) ecosystem, some processing is done near data production sites at higher speeds without the need for high bandwidth by combining Fog Computing (FC) and cloud computing. Fog computing offers advantages for real-time systems that require high speed internet connectivity. Due to the limited resources of fog nodes, one of the most important challenges of FC is to meet dynamic needs in real-time. Therefore, one of the issues in the fog environment is the optimal assignment of tasks to fog nodes. An efficient scheduling algorithm should reduce various qualitative parameters such as cost and energy consumption, taking into account the heterogeneity of fog nodes and the commitment to perform tasks within their deadlines. This study provides a detailed taxonomy to gain a better understanding of the research issues and distinguishes important challenges in existing work. Therefore, a systematic overview of existing task scheduling techniques for cloud-fog environment, as well as their benefits and drawbacks, is presented in this article. Four main categories are introduced to study these techniques, including machine learning-based, heuristic-based, metaheuristic-based, and deterministic mechanisms. A number of papers are studied in each category. This survey also compares different task scheduling techniques in terms of execution time, resource utilization, delay, network bandwidth, energy consumption, execution deadline, response time, cost, uncertainty, and complexity. The outcomes revealed that 38% of the scheduling algorithms use metaheuristic-based mechanisms, 30% use heuristic-based, 23% use machine learning algorithms, and the other 9% use deterministic methods. The energy consumption is the most significant parameter addressed in most articles with a share of 19%. Finally, a number of important areas for improving the task scheduling methods in the FC in the future are presented.","2169-3536","","10.1109/ACCESS.2023.3277826","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10129024","Fog computing;cloud computing;task scheduling;methods;quality of service","Task analysis;Processor scheduling;Surveys;Edge computing;Dynamic scheduling;Systematics;Costs;Quality of service","","16","","79","CCBYNCND","18 May 2023","","","IEEE","IEEE Journals"
"SDN-Based Multi-Tier Computing and Communication Architecture for Pervasive Healthcare","A. C. Baktir; C. Tunca; A. Ozgovde; G. Salur; C. Ersoy","NETLAB, Computer Networks Research Laboratory, Bogazici University, Istanbul, Turkey; NETLAB, Computer Networks Research Laboratory, Bogazici University, Istanbul, Turkey; Department of Computer Engineering, Galatasaray University, Istanbul, Turkey; Movement Disorders & Dementia Clinic, Istanbul, Turkey; NETLAB, Computer Networks Research Laboratory, Bogazici University, Istanbul, Turkey",IEEE Access,"21 Oct 2018","2018","6","","56765","56781","A large spectrum of healthcare applications, ranging from continuous blood sugar level monitoring to sleep apnea detection are nowadays facilitated by modern mobile gadgets. Wearable and ambient sensors generate enormous amounts of physiological data that demand high computation power for real-time processing and large storage area for recording the personal data. In order to conserve the energy on the battery-limited mainstream mobile devices of the end-users, the execution of the healthcare applications may be offloaded to a remote server. While cloud computing provides unlimited pool of resources for latency-tolerant services such as training a machine learning model, the personalization of healthcare services and the delay sensitivity of continuous health assessment necessitate a computation infrastructure in the vicinity of the end-users. As a remedy to address various demands of a wide range of pervasive healthcare applications, we propose a multi-tier computing and communication architecture composed of end-user devices, edge servers, and legacy cloud data-centers. The dynamic management of this architecture, policies to be applied within the network and the orchestration of the healthcare services are carried out by the concept of programmable networks, in the form of software-defined networking (SDN). As a concrete demonstration of our ideas, a fall risk assessment service is implemented and an experimental study is conducted to evaluate its accuracy and the performance of the multi-tier architecture. The results indicate that the proposed architecture is feasible to enable real-time healthcare services and has significant performance advantages over traditional cloud-based approaches.","2169-3536","","10.1109/ACCESS.2018.2873907","Bogazici University Research Fund(grant numbers:12663); State Planning Organization of Turkey through the TAM Project(grant numbers:2007K120610); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8482108","Edge computing;fall risk assessment;pervasive healthcare;software-defined networking;wearable computing","Medical services;Computer architecture;Edge computing;Biomedical monitoring;Servers;Cloud computing;Monitoring","","16","","44","OAPA","5 Oct 2018","","","IEEE","IEEE Journals"
"Introducing Cloud-Assisted Micro-Service-Based Software Development Framework for Healthcare Systems","J. Zaki; S. M. R. Islam; N. S. Alghamdi; M. Abdullah-Al-Wadud; K. -S. Kwak","Department of Computer and Systems, Faculty of Engineering, Mansoura University, Mansoura, Egypt; Department of Computer Science and Engineering, Sejong University, Seoul, South Korea; Department of Computer Sciences, College of Computer and Information Sciences, Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia; Department of Software Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Department of Information and Communication Engineering, Inha University, Incheon, South Korea",IEEE Access,"31 Mar 2022","2022","10","","33332","33348","In healthcare services, application development is considered the most complex and time-consuming phase. As it is difficult to plan and time-intense, it requires high maintenance. Healthcare applications need strict compliance and the scope of application is immense along with associates, classes in services, and classified system. Application designing in healthcare with the help of traditional approaches such as monolithic and service-oriented architecture (SOA) generate problems in different areas like service availability, remote access to services, service provisioning, scalability, healthcare systems integration with each other. That is why there is a need for less sophisticated and user-friendly healthcare systems, which are easy to plan and develop, inexpensive requirement maintenance, and agile testing. To overcome the aforesaid issues in the domain of healthcare application development, this paper develops a framework of micro services for the development of healthcare services using cloud computing infrastructure. Micro-service-based techniques provide lightly coupled and fine-grained methodology. With the use of micro services technique presented in this work, the efficiency, scalability, and performance are improved. In this research, an approach for development and deployment properly in the cloud for healthcare applications is developed. Thus, it contributes to the system design approach and system analysis. Quantitative and qualitative results are reported showing the advantages of micro services approach used.","2169-3536","","10.1109/ACCESS.2022.3161455","National Research Foundation of Korea-Grant funded by the Korean Government (Ministry of Science)(grant numbers:ICT-NRF-2020R1A2B5B02002478); Sejong university through its faculty research program(grant numbers:20212023); Princess Nourah bint Abdulrahman University Researchers Supporting Project(grant numbers:PNURSP2022R40); Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9739683","Service oriented architecture;micro services;cloud computing;healthcare;application design;monolith approach;application software","Medical services;Microservice architectures;Monitoring;Cloud computing;Costs;Electrocardiography;Computer architecture","","16","","41","CCBY","22 Mar 2022","","","IEEE","IEEE Journals"
"Service Based Virtual RAN Architecture for Next Generation Cellular Systems","E. Zeydan; J. Mangues-Bafalluy; J. Baranda; M. Requena; Y. Turk","Centre Tecnològic de Telecomunicacions de Catalunya, Castelldefels, Barcelona, Spain; Centre Tecnològic de Telecomunicacions de Catalunya, Castelldefels, Barcelona, Spain; Centre Tecnològic de Telecomunicacions de Catalunya, Castelldefels, Barcelona, Spain; Centre Tecnològic de Telecomunicacions de Catalunya, Castelldefels, Barcelona, Spain; Aselsan Corporation, Istanbul, Turkey",IEEE Access,"27 Jan 2022","2022","10","","9455","9470","Service based architecture (SBA) is a paradigm shift from Service-Oriented Architecture (SOA) to microservices, combining their principles. Network virtualization enables the application of SBA in cellular systems. To better guide the software design of this virtualized cellular system with SBA, this paper presents a software perspective and a positional approach to using fundamental development principles for adapting SBA in virtualized Radio Access Networks (vRANs). First, we present the motivation for using an SBA in cellular radio systems. Then, we explore the critical requirements, key principles, and components for the software to provide radio services in SBA. We also explore the potential of applying SBA-based Radio Access Network (RAN) by comparing the functional split requirements of 5G RAN with existing open-source software and accelerated hardware implementations of service bus, and discuss the limitations of SBA. Finally, we present some discussions, future directions, and a roadmap of applying such a high-level design perspective of SBA to next-generation RAN infrastructure.","2169-3536","","10.1109/ACCESS.2022.3144534","European Union (EU) H2020 5GROWTH Project(grant numbers:856709); Generalitat de Catalunya(grant numbers:2017 SGR 1195); National Program on Equipment and Scientific and Technical Infrastructure under the European Regional Development Fund (FEDER)(grant numbers:EQC2018-005257-P); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9684900","Service-based architecture;network interfaces;radio access networks;software","Computer architecture;Software;Radio access networks;5G mobile communication;Cloud computing;Protocols;Hardware","","16","","51","CCBY","18 Jan 2022","","","IEEE","IEEE Journals"
"Development of a Novel IoT-Enabled Power- Monitoring Architecture With Real-Time Data Visualization for Use in Domestic and Industrial Scenarios","A. R. Jadhav; S. Kiran M. P. R.; R. Pachamuthu","Department of Electrical Engineering, IIT Hyderabad, Hyderabad, India; Department of Electrical Engineering, IIT Hyderabad, Hyderabad, India; Department of Electrical Engineering, IIT Hyderabad, Hyderabad, India",IEEE Transactions on Instrumentation and Measurement,"16 Nov 2020","2021","70","","1","14","With the increased interest in smart cities and smart infrastructures, the need for energy conservation is increasing. Especially with the current electrical energy production mainly relying on nonrenewable resources, conservation of electrical energy is one of the challenging aspects across the globe. However, one can only perform energy conservation optimally by identifying consumption patterns at a granular level, which requires accurate and ubiquitous monitoring infrastructure. Because the electrical energy wastage can occur at any granularity (from a small house-hold appliance to grid-level wastage), the development of a low-cost, easy-to-install, and accurate power-monitoring infrastructure is need of the hour. Hence, in this article, we propose the developed designs for IoT-enabled power monitoring. First is the noninvasive power monitor with voltage connection. The second design introduces a novel split architecture with centralized voltage measurement, which removes the need for local voltage measurements. We have proposed the third and final version of the IoT-enabled power monitor to fulfill the need for three-phase power monitoring. Unlike first and second designs, this design can be used with noninvasive and invasive current sensors. The proposed architecture also supports essential features, such as secure data transfer. Developed devices transmit real-time data to the cloud server, which makes the data ubiquitously available anywhere and anytime. For analyzing the performance of the proposed architecture, the developed devices are deployed in real industrial scenarios. As an example use case, the electrical anomaly detection framework using the data collected is also explained, and the corresponding results are discussed.","1557-9662","","10.1109/TIM.2020.3028437","Visvesvaraya Ph.D. Scheme, MeitY, Government of India(grant numbers:MEITY-PHD-1115); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9211481","Anomaly detection;fault detection;IoT;operation optimization;power monitoring;smart-buildings","Monitoring;Sensors;Voltage measurement;Real-time systems;Data visualization;Current measurement;Systems architecture","","15","","29","IEEE","2 Oct 2020","","","IEEE","IEEE Journals"
"A Scaling Mechanism for an Evolved Packet Core Based on Network Functions Virtualization","C. H. T. Arteaga; F. B. Anacona; K. T. T. Ortega; O. M. C. Rendon","Departamento de Telemática, Grupo de Ingenier??a Telem??tica, Universidad del Cauca, Popayán, Colombia; Departamento de Telemática, Grupo de Ingenier??a Telem??tica, Universidad del Cauca, Popayán, Colombia; Departamento de Telemática, Grupo de Ingenier??a Telem??tica, Universidad del Cauca, Popayán, Colombia; Departamento de Telemática, Grupo de Ingenier??a Telem??tica, Universidad del Cauca, Popayán, Colombia",IEEE Transactions on Network and Service Management,"10 Jun 2020","2020","17","2","779","792","The workload variations affect the performance of mobile networks. The scaling task is pivotal for addressing these variations. In the literature, research works have incorporated horizontal or vertical scaling in the virtualized network functions of the Evolved Packet Core (EPC) to improve its performance. However, up to now, these works exploit only horizontal or vertical scaling for achieving their aim. In this paper, we propose a scaling mechanism that utilizes horizontal and vertical scaling and considers workload variations for improving performance in EPC. This mechanism is threshold-based, straightforward, and implementable in real LTE-EPC scenarios. We also develop a mechanism prototype and deploy it in a real public cloud. In this cloud, we conduct a prototype evaluation, regarding registrations per second, latency, CPU, and RAM, and considering a varying workload. The evaluation results reveal that our mechanism increases the registrations per second about 308% and decreases the corresponding latency approximately 70% regarding an EPC without scaling while keeping the CPU usage lower than 90% and the used capacity of registrations per second between 65% and 90%. These results corroborate the importance of used both horizontal and vertical scaling to improve EPC performance, handle workload variations, and save resources.","1932-4537","","10.1109/TNSM.2019.2961988","University of Cauca and the Administrative Department of Science, Technology and Innovation (COLCIENCIAS)???Colombia through the Ph.D. Scholarship(grant numbers:527-2015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8941016","Scaling;evolved packet core;4G;network functions virtualization","Scalability;Cloud computing;Synchronization;Prototypes;Long Term Evolution;Network function virtualization;Authentication","","15","","36","IEEE","24 Dec 2019","","","IEEE","IEEE Journals"
"Lattice: A Vision for Machine Learning, Data Engineering, and Policy Considerations for Digital Agriculture at Scale","S. Chaterji; N. DeLay; J. Evans; N. Mosier; B. Engel; D. Buckmaster; M. R. Ladisch; R. Chandra","Department of Agricultural and Biological Engineering, Purdue University, West Lafayette, IN, USA; Department of Agricultural and Biological Engineering, Purdue University, West Lafayette, IN, USA; Department of Agricultural and Biological Engineering, Purdue University, West Lafayette, IN, USA; Department of Agricultural and Biological Engineering, Purdue University, West Lafayette, IN, USA; Department of Agricultural and Biological Engineering, Purdue University, West Lafayette, IN, USA; Department of Agricultural and Biological Engineering, Purdue University, West Lafayette, IN, USA; Department of Agricultural and Biological Engineering, Purdue University, West Lafayette, IN, USA; Microsoft Research, Microsoft Azure, Redmond, WA, USA",IEEE Open Journal of the Computer Society,"2 Jul 2021","2021","2","","227","240","Digital agriculture, with the incorporation of Internet-of-Things (IoT)-based technologies, presents the ability to control a system at multiple levels (individual, local, regional, and global) and generates tools that allow for improved decision making and higher productivity. Recent advances in IoT hardware, e.g., networks of heterogeneous embedded devices, and software, e.g., lightweight computer vision algorithms and cloud optimization solutions, make it possible to efficiently process data from diverse sources in a connected (smart) farm. By interconnecting these IoT devices, often across large geographical distances, it is possible to collect data at different time scales, including in near real-time (i.e., with delays of only a few tens of seconds). This data can then be used for actionable insights, e.g., precise applications of soil supplements and reduced environmental footprint. Through LATTICE, we present an integrated vision for IoT solutions, data processing, and actionable analytics for digital agriculture. We couple this with discussion of economics and policy considerations that will underlie adoption of such IoT and ML technologies. Our paper starts off with the types of datasets in typical field operations, followed by the lifecycle for the data and storage, cloud and edge analytics, and fast information-retrieval solutions. We discuss what algorithms are proving to be most impactful in this space, e.g., approximate data analytics and on-device/in-network processing. We conclude by discussing analytics for alternative agriculture for generation of biofuels and policy challenges in the implementation of digital agriculture in the wild.","2644-1268","","10.1109/OJCS.2021.3085846","Purdue University; Lilly Endowment; Microsoft Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9444818","Data integration;data analysis;internet of things;Sensor systems;cloud computing","Data integration;Soil;Digital agriculture;Fertilizers;Distributed databases;Cloud computing;Internet of Things;Data analysis;Intelligent sensors","","15","","72","CCBY","1 Jun 2021","","","IEEE","IEEE Journals"
"Applications of Blockchain in Business Processes: A Comprehensive Review","W. Viriyasitavat; L. D. Xu; D. Niyato; Z. Bi; D. Hoonsopon","Chulalongkorn Business School, Chulalongkorn University, Bangkok, Thailand; Department of Information Technology and Decision Sciences, Old Dominion University, Norfolk, VA, USA; School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; Department of Civil and Mechanical Engineering, Purdue University Fort Wayne, Fort Wayne, IN, USA; Chulalongkorn Business School, Chulalongkorn University, Bangkok, Thailand",IEEE Access,"16 Nov 2022","2022","10","","118900","118925","Blockchain (BC), as an emerging technology, is revolutionizing Business Process Management (BPM) in multiple ways. The main adoption is to serve as a trusted infrastructure to guarantee the trust of collaborations among multiple partners in trustless environments. Especially, BC enables trust of information by using Distributed Ledger Technology (DLT). With the power of smart contracts, BC enforces the obligations of counterparties that transact in a business process (BP) by programming the contracts as transactions. This paper aims to study the state-of-the-art of BC technologies by (1) exploring its applications in BPM with the focus on how BC provides the trust of BPs in their lifecycles; (2) identifying the relations of BPM as the need and BC as the solution with the assessment towards BPM characteristics; (3) discussing the up-to-date progresses of critical BC in BPM; (4) identifying the challenges and research directions for future advancement in the domain. The main conclusions of our comprehensive review are (1) the study of adopting BC in BPM has attracted a great deal of attention that has been evidenced by a rapidly growing number of relevant articles. (2) The paradigms of BPM over Internet of Things (IoT) have been shifted from persistent to transient, from static to dynamic, and from centralized to decentralized, and new enabling technologies are highly demanded to fulfill some emerging functional requirements (FRs) at the stages of design, configuration, diagnosis, and evaluation of BPs in their lifecycles. (3) BC has been intensively studied and proven as a promising solution to assure the trustiness for both of business processes and their executions in decentralized BPM. (4) Most of the reported BC applications are at their primary stages, future research efforts are needed to meet the technical challenges involved in interoperation, determination of trusted entities, confirmation of time-sensitive execution, and support of irreversibility.","2169-3536","","10.1109/ACCESS.2022.3217794","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9931722","Blockchain;business process management;smart contracts;trust assurance","Business process management;Smart contracts;Task analysis;Distributed ledger;Uncertainty;Internet of Things;Blockchains;Trust management","","15","","176","CCBY","28 Oct 2022","","","IEEE","IEEE Journals"
"Multimodal Event Processing: A Neural-Symbolic Paradigm for the Internet of Multimedia Things","E. Curry; D. Salwala; P. Dhingra; F. A. Pontes; P. Yadav","Insight SFI Research Centre for Data Analytics, Data Science Institute, National University of Ireland Galway, Galway, Ireland; Insight SFI Research Centre for Data Analytics, Data Science Institute, National University of Ireland Galway, Galway, Ireland; Insight SFI Research Centre for Data Analytics, Data Science Institute, National University of Ireland Galway, Galway, Ireland; Insight SFI Research Centre for Data Analytics, Data Science Institute, National University of Ireland Galway, Galway, Ireland; Insight SFI Research Centre for Data Analytics, Data Science Institute, National University of Ireland Galway, Galway, Ireland",IEEE Internet of Things Journal,"25 Jul 2022","2022","9","15","13705","13724","With the Internet of Multimedia Things (IoMT) becoming a reality, new approaches are needed to process real-time multimodal event streams. Existing approaches to event processing have limited consideration for the challenges of multimodal events, including the need for complex content extraction, and increased computational and memory costs. This article explores event processing as a basis for processing real-time IoMT data. This article introduces the multimodal event processing (MEP) paradigm, which provides a formal basis for native approaches to neural multimodal content analysis (i.e., computer vision, linguistics, and audio) with symbolic event processing rules to support real-time queries over multimodal data streams using the multimodal event processing language to express single, primitive multimodal, and complex multimodal event patterns. The content of multimodal streams is represented using multimodal event knowledge graphs to capture the semantic, spatial, and temporal content of the multimodal streams. The approach is implemented and evaluated within a MEP engine using single and multimodal queries achieving near real-time performance with a throughput of ~30 frames processed per second (fps) and subsecond latency of 0.075–0.30 s for video streams of 30 fps input rate. Support for high input stream rates (45 fps) is achieved through content-aware load-shedding techniques with a ~127X latency improvement resulting in only a minor decrease in accuracy.","2327-4662","","10.1109/JIOT.2022.3143171","Science Foundation Ireland; European Regional Development Fund(grant numbers:SFI/12/RC/2289_P2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9681901","Data management and analytics;event processing;Internet of Multimedia Things (IoMT);service middleware and platform","Streaming media;Internet of Things;Real-time systems;Data models;Engines;Data mining;Costs","","15","","61","CCBY","14 Jan 2022","","","IEEE","IEEE Journals"
"Tactile Based Intelligence Touch Technology in IoT Configured WCN in B5G/6G-A Survey","M. Gupta; R. K. Jha; S. Jain","School of Electronics and Communication Engineering (SoECE), Shri Mata Vaishno Devi University, Katra, Jammu, Jammu and Kashmir, India; Department of Electronics and Communication Engineering, Indian Institute of Information Technology, Design and Manufacturing, Jabalpur (IIITDM Jabalpur), Jabalpur, India; Computer Science Department, Central University of Jammu, Jammu, Jammu and Kashmir, India",IEEE Access,"30 Mar 2023","2023","11","","30639","30689","Touch-enabled sensation and actuation are expected to be the most promising, straightforward, and important uses of the B5G/6G communication networks. In light of the next generation (6G) systems’ prerequisite for low latency, the infrastructure should be reconfigurable, intelligent, and interoperable in the real-time existing wireless network. It has a drastic impact on society due to its high precision, accuracy, reliability, and efficiency, combined with the ability to connect a user from remote areas. Hence, the touch-enabled interaction is primarily concerned with the real-time transmission of tactile-based haptic information over the internet, in addition to the usual audio, visual, and data traffic, thus enabling a paradigm shift towards a real-time control and steering communication system. The existing system latency and overhead often have delays and limitations on the application’s usability. In light of the aforementioned concerns, the study proposes an intelligent touch-enabled system for B5G/6G and an IoT-based wireless communication network, incorporating AR/VR technologies. The tactile internet and network-slicing serve as the backbone of touch technology and incorporates intelligence from techniques such as artificial intelligence and machine/deep learning. The survey also introduces a layered and interfacing architecture with its E2E solution for the intelligent touch-based wireless communication system. It is anticipated for the upcoming 6G system to provide numerous opportunities for various sectors to utilize AR/VR technology in robotics and healthcare facilities to help in addressing several problems faced by society. Conclusively the article presents a few use cases concerning the deployment of touch infrastructure in automation, robotics, and intelligent healthcare systems, assisting in the diagnosis and treatment of the prevailing Covid-19 cases. The paper concludes with some considerable future research aspects of the proposed system with a few ongoing projects concerning the development and incorporation of the 6G wireless communication system.","2169-3536","","10.1109/ACCESS.2022.3148473","5G and Internet of Things (IoT) Laboratory, School of Electronics and Communication Engineering (SoECE), Technology Business Incubation Center (TBIC), Technical Education Quality Improvement Programme of Government of India-III (TEQIP-III) at Shri Mata Vaishno Devi University (SMVDU), Katra, Jammu; 6G and IoT Innovation Laboratory, Indian Institute of Information Technology, Design and Manufacturing (IIITDM), Jabalpur; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9701357","6G;AI;AR;intelligence;IoT;ML;network slicing;tactile internet;VR","Wireless communication;Real-time systems;Virtualization;Next generation networking;NOMA;Learning (artificial intelligence);6G mobile communication","","15","","201","CCBY","1 Feb 2022","","","IEEE","IEEE Journals"
"A Detailed Relevance Analysis of Enabling Technologies for 6G Architectures","D. G. S. Pivoto; T. T. Rezende; M. S. P. Facina; R. Moreira; F. de Oliveira Silva; K. V. Cardoso; S. L. Correa; A. V. D. Araujo; R. S. E. Silva; H. S. Neto; G. R. de Lima Tejerina; A. M. Alberti","Instituto Nacional de Telecomunicações, Santa Rita do Sapucaí, Brazil; Instituto Nacional de Telecomunicações, Santa Rita do Sapucaí, Brazil; Fundação Centro de Pesquisa e Desenvolvimento em Telecomunicações, Campinas, Brazil; Faculty of Computing, Federal University of Uberlândia, Uberlândia, Brazil; Faculty of Computing, Federal University of Uberlândia, Uberlândia, Brazil; Instituto de Informática (INF), Universidade Federal de Goiás, Goiânia, Brazil; Instituto de Informática (INF), Universidade Federal de Goiás, Goiânia, Brazil; Instituto de Informática (INF), Universidade Federal de Goiás, Goiânia, Brazil; Federal Institute of Goiás, Inhumas, Brazil; Federal Institute Catarinense, Concórdia, Brazil; Centre for Wireless Communications, Oulu, Finland; Instituto Nacional de Telecomunicações, Santa Rita do Sapucaí, Brazil",IEEE Access,"29 Aug 2023","2023","11","","89644","89684","As society evolves as a whole, new demands arise with increasingly demanding prerequisites, consequently requiring more significant effort to be met. Such demands cover emerging applications, such as remote surgeries in Smart Health use cases, whose latency and reliability network requirements cannot be met by current communication systems; or simply improving current applications with more challenging requirements to be achieved, such as increasing the transmission rate in a mobile network, offering Quality of Service (QoS), and consequently, better user experience. Therefore, enabling technologies must be chosen to design an appropriate 6G architecture to address such demands. However, the explosion of emerging applications focused on different scopes and requirements to be met makes choosing these enabling technologies extremely complex and unpredictable. Thus, this article aims to create a methodology for analyzing the relevance of enabling technologies and use it to design an optimal architecture capable of meeting the 6G demands. For this purpose, two methods named as Average (AVG) and Analytic Hierarchy Process (AHP) have been selected, whose objective is to determine the relevance of an enabler for the 6G architecture, taking into account different degrees of influencing variables for this analysis, such as adherence to a certain architectural model; popularity in the research area; degree of innovation; synergy with other enablers; and support for requirements. Each of these methods presents a particular result. In the case of the AVG method, the criteria and variables are evaluated independently, and the arithmetic mean is employed to combine the evaluations into a single measure of suitability. In contrast, the AHP method considers the relative importance of criteria and variables in order to classify an optimal set of enabling technologies capable of fulfilling the key roles to be performed by a 6G architecture, and consequently meeting the main 6G demands. Our evaluation provides a unique perspective on 6G enablers, identifying issues and fostering research for future mobile architectures. The results obtained also provide researchers with the necessary information to stay updated on emerging enabling technologies and their suitability for designing new optimized 6G architectures.","2169-3536","","10.1109/ACCESS.2023.3301811","Rede Nacional de Ensino e Pesquisa (RNP), with resources from Ministério da Ciência, Tecnologia e Inovações e Comunicações (MCTIC); Brazil 6G Project of the Radiocommunication Reference Center (Centro de Referência em Radiocomunicações—CRR) of the National Institute of Telecommunications (Instituto Nacional de Telecomunicações, Inatel), Brazil(grant numbers:01245.010604/2020-14); Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES)—Finance(grant numbers:001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10207841","6G architectures;6G enabling technologies;6G requirements;6G use cases;AHP;relevance analysis","6G mobile communication;Network architecture;User experience","","15","","239","CCBYNCND","3 Aug 2023","","","IEEE","IEEE Journals"
"Edge Intelligence for Internet of Vehicles: A Survey","G. Yan; K. Liu; C. Liu; J. Zhang","College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; CICT Connected and Intelligent Technologies Company Ltd., Chongqing, China",IEEE Transactions on Consumer Electronics,"29 Aug 2024","2024","70","2","4858","4877","The Internet of Vehicles (IoV) has become a fundamental platform for advancing Intelligent Transportation Systems (ITSs) and Intelligent Connected Vehicles (ICVs). However, the increasing volume of data generated by vehicle sensors and the computational demands of Artificial Intelligence (AI) algorithms present significant challenges for the platform. Edge Intelligence (EI), which brings intelligent computing and data processing closer to vehicles, has emerged as a potential solution. In this survey, we provide a comprehensive overview of Edge Intelligence for the Internet of Vehicles. We begin by discussing the motivations behind employing EI in the IoV for typical AI computations. To fully exploit the potential of EI in heterogeneous IoV environments, we present a layered vehicular EI architecture and discuss its benefits and challenges. Furthermore, we provide a taxonomy of EI approaches for vehicular networks, focusing on cooperative inference, distributed training, and collaborative sensing, in terms of their schemas and advanced frameworks. Finally, we explore emerging trends and research directions in this field, including vehicle-road-cloud integration, generative AI-driven IoV, and vehicular cyber-physical fusion. By offering insights into state-of-the-art techniques and trends, this survey aims to enable researchers to develop innovative solutions for transforming the intelligent IoV ecosystem.","1558-4127","","10.1109/TCE.2024.3378509","National Natural Science Foundation of China(grant numbers:62172064); Graduate Scientific Research and Innovation Foundation of Chongqing, China(grant numbers:CYB23044); Fundamental Research Funds for the Central Universities(grant numbers:2023CDJQCZX-001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10474509","Internet of Vehicles (IoV);edge intelligence (EI);inference;training;sensing","Sensors;Artificial intelligence;Training;Collaboration;Internet of Vehicles;Edge computing","","15","","113","IEEE","18 Mar 2024","","","IEEE","IEEE Journals"
"5G Multi-Access Edge Computing: A Survey on Security, Dependability, and Performance","G. Nencioni; R. G. Garroppo; R. F. Olimid","Department of Electrical Engineering and Computer Science, University of Stavanger, Stavanger, Norway; Department of Information Engineering, University of Pisa, Pisa, Italy; Department of Computer Science, University of Bucharest, Bucharest, Romania",IEEE Access,"30 Jun 2023","2023","11","","63496","63533","The Fifth Generation (5G) of mobile networks offers new and advanced services with stricter requirements. Multi-access Edge Computing (MEC) is a key technology that enables these new services by deploying multiple devices with computing and storage capabilities at the edge of the network, close to end-users. MEC enhances network efficiency by reducing latency, enabling real-time awareness of the local environment, allowing cloud offloading, and reducing traffic congestion. New mission-critical applications require high security and dependability, which are rarely addressed alongside performance. This survey paper fills this gap by presenting 5G MEC’s three aspects: security, dependability, and performance. The paper provides an overview of MEC, introduces taxonomy, state-of-the-art, and challenges related to each aspect. Finally, the paper presents the challenges of jointly addressing these three aspects.","2169-3536","","10.1109/ACCESS.2023.3288334","Norwegian Research Council through the Management and Orchestration for Data and Network Integration (5G-MODaNeI) Project(grant numbers:308909); Italian Ministry of Education and Research (MIUR) in the framework of the Future-Oriented Research Laboratory (FoReLab) Project (Departments of Excellence); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10158694","5G;MEC;security;dependability;performance","5G mobile communication;Security;Computer architecture;Cloud computing;Virtualization;Multi-access edge computing;Ultra reliable low latency communication","","14","","245","CCBYNCND","21 Jun 2023","","","IEEE","IEEE Journals"
"QoS-Aware Co-Scheduling for Distributed Long-Running Applications on Shared Clusters","J. Zhu; R. Yang; X. Sun; T. Wo; C. Hu; H. Peng; J. Xiao; A. Y. Zomaya; J. Xu","Beihang University, Beijing, China; School of Computing, University of Leeds, Leeds, U.K.; School of Computing, University of Leeds, Leeds, U.K.; Beihang University, Beijing, China; Beihang University, Beijing, China; Beihang University, Beijing, China; Alibaba Group, Hangzhou, Zhejiang, China; University of Sydney, Camperdown, NSW, Australia; School of Computing, University of Leeds, Leeds, U.K.",IEEE Transactions on Parallel and Distributed Systems,"29 Sep 2022","2022","33","12","4818","4834","To achieve a high degree of resource utilization, production clusters need to co-schedule diverse workloads – including both batch analytic jobs with short-lived tasks and long-running applications (LRAs) that execute for a long time frame from hours to months – onto the shared resources. Microservice architecture advances the manifestation of distributed LRAs (DLRAs), comprising multiple interconnected microservices that are executed in long-lived distributed containers and serve massive user requests. Detecting and mitigating QoS violation become even more intractable due to the network uncertainties and latency propagation across dependent microservices. However, current resource managers are only responsible for resource allocation among applications/jobs but agnostic to runtime QoS such as latency at application level. The state-of-the-art QoS-aware scheduling approaches are dedicated for monolithic applications, without considering the temporal-spatio performance variability across distributed microservices. In this paper, we present Toposch, a new scheduling and execution framework to prioritize the QoS of DLRAs whilst balancing the performance of batch jobs and maintaining high cluster utilization through harvesting idle resources. Toposch tracks footprints of every single request across microservices and uses critical path analysis, based on the end-to-end latency graph, to identify microservices that have high risk of QoS violation. Based on microservice and node level risk assessment, we intervene the batch scheduling by adaptively reducing the visible resources to batch tasks and thus delaying their execution to give way to DLRAs. We propose a prediction-based vertical resource auto-scaling mechanism, with the aid of resource-performance modeling and fine-grained resource inference and access control, for prompt recovery of QoS violation. A cost-effective task preemption is leveraged to ensure a low-cost task preemption and resource reclamation during the auto-scaling. Toposch is integrated with Apache YARN and experiments show that Toposch outperforms other baselines in terms of performance guarantee of DLRAs, at an acceptable cost of batch job slowdown. The tail latency of DLRAs is merely 1.12x of the case of executing alone on average in Toposch with a 26% JCT increase of Spark analytic jobs.","1558-2183","","10.1109/TPDS.2022.3202493","MIIT of China(grant numbers:2105-370171-07-02-860873); S&T Program of Hebei(grant numbers:20310101D); Fundamental Research Funds for the Central Universities(grant numbers:20226941); UK EPSRC(grant numbers:EP/T01461X/1); Alan Turing Pilot Project; Alan Turing PDEA Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9869329","Resource scheduling;cluster management;QoS;tail latency;datacenters","Quality of service;Microservice architectures;Task analysis;Runtime;Containers;Resource management;Databases","","14","","65","CCBY","29 Aug 2022","","","IEEE","IEEE Journals"
"SC-TRUST: A Dynamic Model for Trustworthy Service Composition in the Internet of Things","A. A. Adewuyi; H. Cheng; Q. Shi; J. Cao; X. Wang; B. Zhou","Department of Computer Science, Liverpool John Moores University, Liverpool, U.K.; Department of Computer Science, University of Hertfordshire, Hatfield, U.K.; Department of Computer Science, Liverpool John Moores University, Liverpool, U.K.; Department of Computing, Hong Kong Polytechnic University, Hong Kong; State Key Laboratory of Synthetical Automation for Process Industries, College of Computer Science and Engineering, Northeastern University, Shenyang, China; Department of Computer Science, Liverpool John Moores University, Liverpool, U.K.",IEEE Internet of Things Journal,"21 Feb 2022","2022","9","5","3298","3312","A future Internet of Things (IoT) will feature a service-oriented architecture consisting of lightweight computing platforms offering individual, loosely coupled microservices. Often, an end-user will request a bespoke service that will require a composition of two or more microservices offered by different service providers. However, the underlying complexities of soft compositions and the increased security risks are inherent in such a massively decentralized and distributed architecture. The use of trust management to secure the IoT is well studied in the literature. However, there are limitations to its use in service compositions in the IoT. Specifically, transparent (agnostic) trust composition and decomposition remain key problems for this area. A novel model for trustworthy service compositions in the IoT, SC-TRUST, is therefore proposed to deal with these challenges. In this study, the trust properties of service compositions and the effect of service workflows on transparent trust composition and decomposition are investigated. Based on the findings, relevant trust evaluation functions are derived to guide the compositions. SC-TRUST was implemented in a suitable application and its performance, in terms of the utility derived and the trust accuracy, convergence, and resiliency, was evaluated. The results show that SC-TRUST improves the quality of service compositions and adequately mitigates trust-related attacks, thus increasing both efficiency and security.","2327-4662","","10.1109/JIOT.2021.3097980","National Natural Science Foundation of China(grant numbers:61872073); LiaoNing Revitalization Talents Program(grant numbers:XLYC1902010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9493744","Collaborative computing;distributed applications;Internet of Things (IoT);security and privacy protection;service composition;trust management","Internet of Things;Reliability;Collaboration;Trust management;Middleware;Computational modeling;Service-oriented architecture","","14","","52","IEEE","26 Jul 2021","","","IEEE","IEEE Journals"
"Using the Cloud to Improve Sensor Availability and Reliability in Remote Monitoring","B. Y. Ooi; W. L. Beh; W. -K. Lee; S. Shirmohammadi","Faculty of Information and Communication Technology, Universiti Tunku Abdul Rahman, Kampar, Perak, Malaysia; Faculty of Science, Universiti Tunku Abdul Rahman, Kampar, Perak, Malaysia; Faculty of Information and Communication Technology, Universiti Tunku Abdul Rahman, Kampar, Perak, Malaysia; Distributed and Collaborative Virtual Environments Research Laboratory, University of Ottawa, Ottawa, ON, Canada",IEEE Transactions on Instrumentation and Measurement,"17 Apr 2019","2019","68","5","1522","1532","Although there have been significant advancements in low-power remote sensors in recent years, the challenge of sensor availability and data reliability in remote monitoring applications still persists. The fault and failure of sensors will affect the reliability of the monitored data and subsequently the adverse effect will inevitably propagate itself to the data analytics stage. There are many existing solutions focusing on improving sensor nodes to enhance data reliability and couple it with various energy harvesting techniques to prolong the availability of sensor nodes. This paper presents a complementary solution to these existing solutions by analyzing the correlation between data from different sensor nodes using cloud computing resources. The discovered relationship between the sensor nodes can then be used to improve data reliability and availability of sensor nodes. Performance evaluations using real data sets show that there are indeed relationships between the collected data, and through these discovered relationships the fault detection and fault masking methods outperform conventional approaches such as autoregressive-integrated moving average. In addition, this paper also proposes an approach to extend operation of sensor nodes duration through the discovered relationships, with experiments showing promising results.","1557-9662","","10.1109/TIM.2018.2882218","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8574916","Data reliability;remote monitoring;system availability;wireless sensor nodes","Reliability;Cloud computing;Wireless sensor networks;Temperature sensors;Remote monitoring;Batteries","","14","","38","IEEE","13 Dec 2018","","","IEEE","IEEE Journals"
"A Review on Attack Graph Analysis for IoT Vulnerability Assessment: Challenges, Open Issues, and Future Directions","O. S. M. B. H. Almazrouei; P. Magalingam; M. K. Hasan; M. Shanmugam","Advanced Informatics Department, Razak Faculty of Technology and Informatics, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; Advanced Informatics Department, Razak Faculty of Technology and Informatics, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; Center for Cyber Security, Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, Bangi, Malaysia; College of Computing and Informatics, Universiti Tenaga Nasional, Kajang, Malaysia",IEEE Access,"11 May 2023","2023","11","","44350","44376","Vulnerability assessment in industrial IoT networks is critical due to the evolving nature of the domain and the increasing complexity of security threats. This study aims to address the existing gaps in the literature by conducting a comprehensive survey on the use of attack graphs for vulnerability assessment in IoT networks. Attack graphs serve as a valuable cybersecurity tool for modeling and analyzing potential attack scenarios on systems, networks, or applications. The survey covers the research conducted between 2016 and 2021(34 peer-reviewed journal articles and 28 conference papers), identifying and categorizing the main methodologies and technologies employed in generating and analyzing attack graphs. In this review, core modeling techniques for IoT vulnerability assessment are highlighted, such as Markov Decision Processes (MDP), Feature Pyramid Networks (FPN), K-means clustering, and logistic regression models, along with other techniques involving genetic algorithms like fast-forward (FF), contingent fast-forwards (CFF), advanced reinforcement-learning algorithms, and HARMs models. The evaluation of the performance of these attack graph models using IoT networks or devices as case studies is also emphasized. This survey provides valuable insights into the state-of-the-art attack graph techniques for IoT network vulnerability assessment, identifying various applications, performances, research opportunities, and challenges. As a reference source, it serves to inform academicians and practitioners interested in leveraging attack graphs for IoT network vulnerability assessment and guides future research directions in this area.","2169-3536","","10.1109/ACCESS.2023.3272053","Ministry of Higher Education, Government of Malaysia, and the Research Management Centre, Universiti Teknologi Malaysia, through the Fundamental Research Grant Scheme (FRGS) Grant (vote number R.K130000.7856.5F016)(grant numbers:FRGS/1/2018/ICT04/UTM/02/8); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10113644","Attack graph;the Internet of Things;network vulnerabilities;vulnerability assessment","Internet of Things;Computer security;Systematics;Databases;Metadata;Wireless sensor networks;Terminology","","14","","74","CCBYNCND","1 May 2023","","","IEEE","IEEE Journals"
"Development of Mobile IoT Solutions: Approaches, Architectures, and Methodologies","N. Magaia; P. Gomes; L. Silva; B. Sousa; C. X. Mavromoustakis; G. Mastorakis","Department of Computer Science, Faculty of Sciences, LASIGE, University of Lisbon, Lisbon, Portugal; Faculty of Sciences, University of Lisbon, Lisbon, Portugal; Department of Computer Science, Faculty of Sciences, LASIGE, University of Lisbon, Lisbon, Portugal; Faculty of Sciences, University of Lisbon, Lisbon, Portugal; Department of Computer Science, University of Nicosia, Nicosia, Cyprus; Department of Management Science and Technology, Hellenic Mediterranean University, Agios Nikolaos, Greece",IEEE Internet of Things Journal,"4 Nov 2021","2021","8","22","16452","16472","Modern Living, as we know it, has been impacted meaningfully by the Internet of Things (IoT). IoT consists of a network of things that collect data from machines (e.g., mobile devices) and people. Mobile application development is a flourishing tendency, given the increasing popularity of smartphones. Nowadays, users are accessing their desired services on the smartphone by means of dedicated applications as the latter offers a more customized and prompt service. In addition, companies are also looking to persuade users by offering interactive and effective mobile applications. Mobile application developers are using IoT to develop better applications. However, there is no generalized consensus on the selection of best architecture or even the most suitable communications protocols to be used on an IoT application development. Therefore, this article aims at presenting approaches, architectures, and methodologies relevant to the development of mobile IoT solutions.","2327-4662","","10.1109/JIOT.2020.3046441","FCT through the LASIGE Research Unit(grant numbers:UIDB/00408/2020,UIDP/00408/2020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9302583","Architecture;Internet of Things (IoT);methodology;mobile development","Internet of Things;Computer architecture;Protocols;Cloud computing;Software;TCPIP;Security","","14","","198","IEEE","22 Dec 2020","","","IEEE","IEEE Journals"
"Optimization of Sports Training Systems Based on Wireless Sensor Networks Algorithms","J. Yang; W. Lv","Jiaozuo Normal College, Institute of Physical Education, Jiaozuo, China; Jiaozuo Normal College, Institute of Physical Education, Jiaozuo, China",IEEE Sensors Journal,"12 Nov 2021","2021","21","22","25075","25082","This paper aims to design and optimize a sports training system through wireless sensor network technology, which mainly includes a hardware system and software system. In the hardware system, the design of sensor nodes and the base station is realized, which can realize the real-time collection of movement parameters by the movement collectors. In the software system, the design of base station control, node control, and motion database software is realized, which can effectively collect, store, and analyze the motion parameters. Finally, the wireless sensor network-based sports training system is tested, and the test results show that the system designed in this paper can meet the needs of sports training use. The inertial measurement unit in the wearable device is used to collect the data generated by the human body during exercise, and low-power Bluetooth is used as the data transmission protocol between the wearable device and the smartphone. The server side provides computing resources in the form of cloud computing to achieve client-server interaction. In combination with the idea of service containerization, a deployment scheme for cloud services was proposed using container choreography, and compared to the pre-training period, the subjects’ trunk sway in the front and back directions decreased by 60.5% and 54.0%, and the trunk sway in the left and right directions decreased by 67.1% and 50.3% during and after training, respectively, and the ratio of time occupied by the equilibrium state increased by 63.5% and 47.4%, subjects’ balance was effectively improved.","1558-1748","","10.1109/JSEN.2020.3046290","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9301323","Wireless sensors algorithms;networks;athletic training;system optimization","Sensors;Wireless sensor networks;Interference;Sensor systems;Training;Cloud computing;Wireless communication","","14","","20","IEEE","21 Dec 2020","","","IEEE","IEEE Journals"
"Edge on Wheels With OMNIBUS Networking for 6G Technology","M. Ergen; F. Inan; O. Ergen; I. Shayea; M. F. Tuysuz; A. Azizan; N. K. Ure; M. Nekovee","Electronics and Communication Engineering Department, Faculty of Electrical and Electronics Engineering, Istanbul Technical University (ITU), Istanbul, Turkey; Ambeent Inc., Yuba, CA, USA; Electronics and Communication Engineering Department, Faculty of Electrical and Electronics Engineering, Istanbul Technical University (ITU), Istanbul, Turkey; Electronics and Communication Engineering Department, Faculty of Electrical and Electronics Engineering, Istanbul Technical University (ITU), Istanbul, Turkey; Computer and Information Sciences Department, Northumbria University, Newcastle Upon Tyne, U.K.; Advanced Informatics Department, Razak Faculty of Technology and Informatics, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; Istanbul Technical University Artificial Intelligence and Data Science Application and Research Center, ITU Ayazaga Campus, Istanbul, Turkey; Centre for Advanced Communications, Mobile Technology and IoT, School of Engineering and Informatics, University of Sussex, Brighton, U.K.",IEEE Access,"8 Dec 2020","2020","8","","215928","215942","In recent years, both the scientific community and the industry have focused on moving computational resources with remote data centres from the centralized cloud to decentralised computing, making them closer to the source or the so called “edge” of the network. This is due to the fact that the cloud system alone cannot sufficiently support the huge demands of future networks with the massive growth of new, time-critical applications such as self-driving vehicles, Augmented Reality/Virtual Reality techniques, advanced robotics and critical remote control of smart Internet-of-Things applications. While decentralised edge computing will form the backbone of future heterogeneous networks, it still remains at its infancy stage. Currently, there is no comprehensive platform. In this article, we propose a novel decentralised edge architecture, a solution called OMNIBUS, which enables a continuous distribution of computational capacity for end-devices in different localities by exploiting moving vehicles as storage and computation resources. Scalability and adaptability are the main features that differentiate the proposed solution from existing edge computing models. The proposed solution has the potential to scale infinitely, which will lead to a significant increase in network speed. The OMNIBUS solution rests on developing two predictive models: (i) to learn timing and direction of vehicular movements to ascertain computational capacity for a given locale, and (ii) to introduce a theoretical framework for sequential to parallel conversion in learning, optimisation and caching under contingent circumstances due to vehicles in motion.","2169-3536","","10.1109/ACCESS.2020.3038233","Ambeent Inc.; Universiti Teknologi Malaysia, Research University Grant Scheme Tier 2(grant numbers:PY/2019/00325); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9260179","Edge computing;5G;6G;V2X;ubiquitous AI;distributed AI;multi-access edge computing (MEC)","Scalability;Wheels;Timing;Time factors;Robots;Remote control;Edge computing","","14","","96","CCBY","16 Nov 2020","","","IEEE","IEEE Journals"
"AoI-Aware User Service Satisfaction Enhancement in Digital Twin-Empowered Edge Computing","J. Li; S. Guo; W. Liang; J. Wang; Q. Chen; Z. Xu; W. Xu","Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Department of Computer Science, City University of Hong Kong, Hong Kong, China; Department of Computer Science, City University of Hong Kong, Hong Kong, China; School of Computers, Guangdong University of Technology, Guangzhou, China; School of Software, Dalian University of Technology, Dalian, China; College of Computer Science, Sichuan University, Chengdu, China",IEEE/ACM Transactions on Networking,"18 Apr 2024","2024","32","2","1677","1690","The emerging digital twin technique enhances the network management efficiency and provides comprehensive insights on network performance, through mapping physical objects to their digital twins. The user satisfaction on digital twin-enabled service relies on the freshness of digital twin data, which is measured by the Age of Information (AoI). Due to long service delays, the use of the remote cloud for delay-sensitive service provisioning faces serious challenges. Mobile Edge Computing (MEC), as an ideal paradigm for delay-sensitive services, is able to realize real-time data communication between physical objects and their digital twins at the network edge. However, the mobility of physical objects and dynamics of user query arrivals make seamless service provisioning in MEC become challenging. In this paper, we investigate dynamic digital twin placements for improving user service satisfaction in MEC environments, by introducing a novel metric to measure user service satisfaction based on the AoI concept and formulating two user service satisfaction enhancement problems: the static and dynamic utility maximization problems under static and dynamic digital twin placement schemes. To this end, we first formulate an Integer Linear Programming (ILP) solution to the static utility maximization problem when the problem size is small; otherwise, we propose a performance-guaranteed approximation algorithm. We then propose an online algorithm with a provable competitive ratio for the dynamic utility maximization problem, by considering dynamic user query services. Finally, we evaluate the performance of the proposed algorithms via simulations. Simulation results demonstrate that the proposed algorithms outperform the comparison baseline algorithms, improving the algorithm performance by at least 10.7%, compared to the baseline algorithms.","1558-2566","","10.1109/TNET.2023.3324704","Key-Area Research and Development Program of Guangdong Province(grant numbers:2021B0101400003); Hong Kong Research Grants Council (RGC) Research Impact Fund(grant numbers:R5060-19,R5034-18); Areas of Excellence Scheme(grant numbers:AoE/E-601/22-R); General Research Fund(grant numbers:152203/20E,152244/21E,152169/22E,152228/23E); Shenzhen Science and Technology Innovation Commission(grant numbers:JCYJ20200109142008673); City University of Hong Kong(grant numbers:9380137,7005845); Hong Kong Research Grant Council(grant numbers:NSFC/RGC N_CityU 140/20); NSFC(grant numbers:62372118); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2022A1515011032); National Natural Science Foundation of China(grant numbers:61802048); Xinghai Scholar Program in the Dalian University of Technology, China; NSFC(grant numbers:62272328); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10289645","Digital twin;mobile edge computing;age of information;approximation and online algorithms;digital twin placement","Digital twins;Heuristic algorithms;Approximation algorithms;Delays;Prediction algorithms;Cloud computing;Real-time systems","","13","","43","IEEE","20 Oct 2023","","","IEEE","IEEE Journals"
"Energy-Efficient Anomaly Detection With Primary and Secondary Attributes in Edge-Cloud Collaboration Networks","X. Li; Z. Zhou; Z. Shi; X. Xue; Y. Duan","School of Information Engineering, China University of Geosciences (Beijing), Beijing, China; School of Information Engineering, China University of Geosciences (Beijing), Beijing, China; Department of Shale Gas, Research Institute of Petroleum Exploration and Development, PetroChina, Beijing, China; School of Computer Software, College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Information Science and Technology, Hainan University, Haikou, China",IEEE Internet of Things Journal,"26 Jul 2021","2021","8","15","12176","12188","An energy-efficient anomaly detection is fundamental to maintain a healthy status of domain applications in edge-cloud collaboration networks. Generally, various kinds of multimodal sensory data capture heterogeneous attributes, where a certain attribute, called the primary one, may be more significant in detecting certain anomaly. This observation drives us to propose a novel energy-efficient anomaly detection mechanism, where attributes sensed by multimodal smart things (msts) are categorized as primary and secondary ones according to their relevance with the characteristic of this anomaly. This technique includes two steps: 1) an initial anomaly detection in single edge networks. Edge nodes associated with the primary attribute adopt a lightweight object detection model to initially detect the potential occurrence of this anomaly. Certain edge networks are determined where an anomaly is suspected and 2) an anomaly refinement with multimodal and multiattribute smart things in marginal edge networks. The cloud identifies and issues a specific query request to gather anomaly-aware sensory data from smart things with secondary attributes, for refining the detection accuracy of this anomaly, where an adaptive weighted fusion model is developed to analyze sensory data coupling of msts. The experimental results show that this technique performs better than the state of the art on the reduction of energy consumption and query time.","2327-4662","","10.1109/JIOT.2021.3062420","National Key Research and Development Program of China (Key Technologies and Demonstrations of the Internet of Things and Smart Cities)(grant numbers:2019YFB2101803); National Natural Science Foundation of China(grant numbers:61772479,42050103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9369387","Edge-cloud collaboration networks;energy-efficient anomaly detection;multimodal smart things (msts);primary and secondary attributes","Image edge detection;Anomaly detection;Cloud computing;Collaboration;Sensors;Internet of Things;Oils","","13","","34","IEEE","4 Mar 2021","","","IEEE","IEEE Journals"
"MAACO: A Dynamic Service Placement Model for Smart Cities","C. Cabrera; S. Svorobej; A. Palade; A. Kazmi; S. Clarke","Department of Computer Science and Technology, University of Cambridge, Cambridge, U.K; School of Computer Science and Statistics, Trinity College, Dublin, Ireland; School of Computer Science and Statistics, Trinity College, Dublin, Ireland; School of Computer Science and Statistics, Trinity College, Dublin, Ireland; School of Computer Science and Statistics, Trinity College, Dublin, Ireland",IEEE Transactions on Services Computing,"6 Feb 2023","2023","16","1","424","437","Smart cities generate huge volumes of data to be processed by applications with different criticality and requirements. For example, a healthcare application needs lower latency when requested from an ambulance travelling to a hospital during an emergency compared to applications in less-critical domains. Cities can use Multi-access Edge Computing to reduce latency by placing applications’ services closer to users. A service placement process selects the set of servers to run the services for deployment. Smart cities challenge this selection as a large number of servers and services generate a large number of potential solutions with different QoS properties. Additionally, placement approaches must consider applications’ criticality and users’ mobility to offer an appropriate overall latency. Current approaches have considered servers’ utilisation and users’ location to place services. However, they do not consider applications’ criticality and mobile users’ paths. This paper presents MAACO, a Mobility-Aware, priority-driven, ACO-based service placement model that prioritises applications according to their criticality and minimises critical applications’ latency, while considering predicted paths for mobile users. Evaluation results show that MAACO achieves lower latency and waiting time compared against baselines at the cost of reduced load balance between the network servers.","1939-1374","","10.1109/TSC.2022.3143029","Science Foundation Ireland(grant numbers:16/SP/3804); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9681330","Smart cities;multi-access edge computing;dynamic service placement","Servers;Random access memory;Costs;Approximation algorithms;Trajectory;Smart cities;Heuristic algorithms","","13","","41","IEEE","13 Jan 2022","","","IEEE","IEEE Journals"
"Smart Meters for Smart Energy: A Review of Business Intelligence Applications","M. H. Raza; Y. M. Rind; I. Javed; M. Zubair; M. Q. Mehmood; Y. Massoud","Department of Electrical Engineering, MicroNano Laboratory, Information Technology University (ITU), Lahore, Pakistan; Department of Electrical Engineering, MicroNano Laboratory, Information Technology University (ITU), Lahore, Pakistan; Department of Electrical Engineering, MicroNano Laboratory, Information Technology University (ITU), Lahore, Pakistan; Innovative Technologies Laboratories (ITL), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Department of Electrical Engineering, MicroNano Laboratory, Information Technology University (ITU), Lahore, Pakistan; Innovative Technologies Laboratories (ITL), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia",IEEE Access,"2 Nov 2023","2023","11","","120001","120022","Smart Energy (SE) has emerged as a critical technology in tackling global challenges like climate change while addressing the rising energy demands driven by today’s data-intensive industrial revolution. SE integrates information and communication technologies into energy systems, optimizing them to meet these challenges effectively. At the core of SE operations are smart meters, playing a fundamental role in ensuring efficient functionality. These devices collect data, which is then leveraged to derive Business Intelligence (BI) for operations across the entire spectrum, from the sensing infrastructure to the cloud, primarily utilizing the Internet of Things (IoT) technology framework. With the increasing complexity of operations and the growing demand for optimization and enhanced functionality, the SE technology stack is evolving to integrate across all layers and domains. This integration has led to the stratification of computational load across IoT layers, intensifying the dependence on smart meter data for BI. Consequently, smart meters themselves have evolved to become more functional and complex. This paper’s novelty lies in its comprehensive exploration of the integration of BI with smart meter data. It delves into various aspects, including the different layers of intelligent operations within SE systems, the current state of the art, and diverse implementations of smart meters and their applications across operational locations, ranging from consumers to fog computing. The paper concludes by identifying research gaps and future directions, offering insights into the evolving requirements for the next generation of SE systems and the necessary adaptations in smart metering infrastructure to support these roles. This work contributes to a better understanding of the evolving landscape of data and computation in the context of SE, facilitating more efficient and effective energy management solutions.","2169-3536","","10.1109/ACCESS.2023.3326724","Innovative Technologies Laboratories (ITL), King Abdullah University of Science and Technology (KAUST), Saudi Arabia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290882","Smart grids;smart meters;AMI;cloud;business intelligence;artificial intelligence;smart energy","Smart meters;Sensors;Voltage measurement;Power measurement;Meters;Internet of Things;Business intelligence;Smart grids;Artificial intelligence;Energy management;Climate change","","13","","217","CCBYNCND","23 Oct 2023","","","IEEE","IEEE Journals"
"Intelligent VNF Placement to Mitigate DDoS Attacks on Industrial IoT","G. W. De Oliveira; M. Nogueira; A. L. d. Santos; D. M. Batista","Computer Science Department, University of São Paulo, São Paulo, Brazil; Computer Science Department, Federal University of Minas Gerais, Belo Horizonte, Brazil; Computer Science Department, Federal University of Minas Gerais, Belo Horizonte, Brazil; Computer Science Department, University of São Paulo, São Paulo, Brazil",IEEE Transactions on Network and Service Management,"29 Jun 2023","2023","20","2","1319","1331","The Internet of Things (IoT) has undergone rapid popularization, reaching a wide range of application domains, such as manufactures. Hence, more and more heterogeneous IoT devices have been deployed in a variety of industrial environments, progressively becoming common objects to the supply chain. The physical infrastructure of manufacturing systems has become complex and requires efficient and dynamic solutions for managing network performance and security. Network Function Virtualization (NFV) has attracted attention when the intention is to respond to security threats on Industrial IoT (IIoT). Few works use NFV to detect and mitigate security threats on IIoT networks, but even less consider performance indicators of the network context when placing the Virtual Network Functions (VNFs). Thus, this work introduces a Machine Learning (ML) approach to place security VNFs based on NFV performance to mitigate Distributed Denial of Service (DDoS) attacks on IIoT. Experiments considering a new composed data set and diverse ML techniques show ML classification as an alternative for IIoT scenarios, achieving, according to the best-performing technique, 99.40% of accuracy in relation to the ideal placement. To facilitate the reproduction of the work, all the code and data produced are publicly available.","1932-4537","","10.1109/TNSM.2023.3274364","Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq); INCT of the Future Internet for Smart Cities(grant numbers:465446/2014-0); Coordenação de Aperfeiçoamento de Pessoal de Nível Superior – Brasil (CAPES)(grant numbers:001); FAPESP(grant numbers:14/50937-1,15/24485-9); FAPESP(grant numbers:18/23098-0); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10121740","Industrial IoT;NFV;VNF placement;performance management;DDoS attacks;machine learning","Security;Performance evaluation;Industrial Internet of Things;Measurement;Denial-of-service attack;Time factors;Computer crime","","13","","39","IEEE","9 May 2023","","","IEEE","IEEE Journals"
"Toward the Observability of Cloud-Native Applications: The Overview of the State-of-the-Art","J. Kosińska; B. Baliś; M. Konieczny; M. Malawski; S. Zieliński","Institute of Computer Science, AGH University of Krakow, Kraków, Poland; Institute of Computer Science, AGH University of Krakow, Kraków, Poland; Institute of Computer Science, AGH University of Krakow, Kraków, Poland; Institute of Computer Science, AGH University of Krakow, Kraków, Poland; Institute of Computer Science, AGH University of Krakow, Kraków, Poland",IEEE Access,"24 Jul 2023","2023","11","","73036","73052","The Cloud-native model, established to enhance the Twelve-Factor patterns, is an approach to developing and deploying applications according to DevOps concepts, Continuous Integration/Continuous Delivery, containers, and microservices. The notion of observability can help us cope with the complexity of such applications. We present a Systematic Mapping Study (SMS) in the observability of Cloud-native applications. We have chosen 56 studies published between 2018 and 2022. The selected studies were thoroughly analyzed, compared, and classified according to the chosen comparative criteria. The presented SMS assesses engineering approaches, maturity, and efficiency of observability by deliberating around four research questions: 1) What provides the motivations for equipping Cloud-native applications with observability capabilities? 2) Which research areas are addressed in the related literature? 3) How are observability approaches implemented? 4) What are the future trends in the Cloud-native applications observability research?","2169-3536","","10.1109/ACCESS.2023.3281860","European Union’s Horizon 2020 Research and Innovation Program through Sano(grant numbers:857533); Sano Project carried out within the International Research Agendas Program of the Foundation for Polish Science, co-financed by the European Union under the European Regional Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10141603","Cloud-native;microservice architecture;observability;monitoring;logging;tracing;systematic mapping study","Observability;Cloud computing;Monitoring;Telemetry;Systematics;Search engines;Microservice architectures","","13","","91","CCBYNCND","1 Jun 2023","","","IEEE","IEEE Journals"
"A Survey on Trusted Distributed Artificial Intelligence","M. A. AĞCA; S. Faye; D. Khadraoui","Luxembourg Institute of Science and Technology (LIST), Esch-Sur-Alzette, Luxembourg; Luxembourg Institute of Science and Technology (LIST), Esch-Sur-Alzette, Luxembourg; Luxembourg Institute of Science and Technology (LIST), Esch-Sur-Alzette, Luxembourg",IEEE Access,"30 May 2022","2022","10","","55308","55337","Emerging Artificial Intelligence (AI) systems are revolutionizing computing and data processing approaches with their strong impact on society. Data is processed with automated labelling pipelines rather than providing it as input to the system. The innovative nature increases the overall performance of monitoring/detection/reaction mechanisms for efficient system resource management. However, due to hardware-driven design limitations, networking and trust mechanisms are not flexible and adaptive enough to be able to interact and control the resources dynamically. Novel adaptive software-driven design approaches can enable us to build growing intelligent mechanisms with software-defined networking (SDN) features by virtualizing network functionalities with maximized features. These challenges and critical feature sets have been identified and introduced into this survey with their scientific background for AI systems and growing intelligent mechanisms. Furthermore, obstacles and research challenges between 1950–2021 are explored and discussed with a focus on recent years. The challenges are categorized according to three defined architectural perspectives (central, decentral/autonomous, distributed/hybrid) for emerging trusted distributed AI mechanisms. Therefore, resiliency and robustness can be assured in a dynamic context with an end-to-end Trusted Execution Environment (TEE) for growing intelligent mechanisms and systems. Furthermore, as presented in the paper, the trust measurement, quantification, and justification methodologies on top of Trusted Distributed AI (TDAI) can be applied in emerging distributed systems and their underlying diverse application domains, which will be explored and experimented in our future related works.","2169-3536","","10.1109/ACCESS.2022.3176385","Luxembourg Institute of Science and Technology (LIST) Ph.D. Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9777972","Trusted AI;distributed systems;software defined networking (SDN);trusted execution environment (TEE)","Artificial intelligence;Peer-to-peer computing;Security;Monitoring;Scalability;Throughput;Software defined networking","","13","","123","CCBY","18 May 2022","","","IEEE","IEEE Journals"
"Navigating Industry 5.0: A Survey of Key Enabling Technologies, Trends, Challenges, and Opportunities","R. Tallat; A. Hawbani; X. Wang; A. Al-Dubai; L. Zhao; Z. Liu; G. Min; A. Y. Zomaya; S. Hamood Alsamhi","School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; Computing School, Edinburgh Napier University, Edinburgh, U.K.; Computing School, Edinburgh Napier University, Edinburgh, U.K.; Department of Computer and Network Engineering, The University of Electro-Communications, Tokyo, Japan; Department of Computer Science, College of Engineering, Mathematics, and Physical Sciences, University of Exeter, Exeter, U.K.; Computing School of Computer Science, The University of Sydney, Sydney, NSW, Australia; Insight Centre for Data Analytics, University of Galway, Galway, Ireland",IEEE Communications Surveys & Tutorials,"22 May 2024","2024","26","2","1080","1126","This century has been a major avenue for revolutionary changes in technology and industry. Industries have transitioned towards intelligent automation, relying less on human intervention, resulting in the fourth industrial revolution, Industry 4.0. That is why IoT has been the researcher’s arena for quite some time. With Industry 4.0 still in motion, the world is on the verge of the  $5^{\textit {th}}$  industrial revolution, a relatively new concept with many unclear opinions regarding its potential benefits, challenges, opportunities, trends, and impact on society. There is a dire need for a broader and more critical perspective. This research paints a bigger picture of “What is happening?” and “What to expect?” during the transition phase of Industry 5.0. In this comprehensive review, we have addressed the state-of-the-art practices in Industry 4.0 and the transitional phase of Industry 5.0. We have highlighted the most promising key enabling technologies, trends, research topics, rising challenges, and unfolding opportunities that can help prepare society for this paradigm shift. The paper then surveys the work toward the outstanding key enablers, challenges, trends, and opportunities in the IoT evolution for Industry 5.0. To spur further avenues for researchers and industrialists, the paper offers conclusive insights at the end. In addition, the article has a precise set of research questions answered in consequent sections and subsections for the reader’s clarity.","1553-877X","","10.1109/COMST.2023.3329472","Innovation Team and Talents Cultivation Program of the National Administration of Traditional Chinese Medicine(grant numbers:ZYYCXTD-D-202208); National Natural Science Foundation of China(grant numbers:62372310); Liaoning Province Applied Basic Research Program(grant numbers:2023JH2/101300194); Science Foundation Ireland(grant numbers:SFI/12/RC/2289_P2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10305081","Industry 5.0;industry 4.0;digital twin;federated learning;Industrial Internet of Things (IIoT);industrial wireless sensor networks;Internet of Robotic Things (IoRT);blockchain;6G;intelligent sensing","Industries;Fourth Industrial Revolution;Service robots;Artificial intelligence;Surveys;Robot sensing systems;Industrial Internet of Things","","13","","358","IEEE","2 Nov 2023","","","IEEE","IEEE Journals"
"Industrial Internet of Things Intelligence Empowering Smart Manufacturing: A Literature Review","Y. Hu; Q. Jia; Y. Yao; Y. Lee; M. Lee; C. Wang; X. Zhou; R. Xie; F. R. Yu","Future Network Research Center, Purple Mountain Laboratories, Nanjing, China; Future Network Research Center, Purple Mountain Laboratories, Nanjing, China; Department of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Mechanical and Electronic Engineering, Wuhan University of Technology, Wuhan, China; Department of Computer Science, Northwestern Polytechnical University, Xi’an, China; Department of Computer Science, Peking University, Beijing, China; Future Network Research Center, Purple Mountain Laboratories, Nanjing, China; State Key Laboratory of networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada",IEEE Internet of Things Journal,"22 May 2024","2024","11","11","19143","19167","The fiercely competitive business environment and increasingly personalized customization needs are driving the digital transformation and upgrading of the manufacturing industry. Industrial Internet of Things (IIoT) intelligence, which can provide innovative and efficient solutions for various aspects of the manufacturing value chain, illuminates the path of transformation for the manufacturing industry. It is time to provide a systematic vision of IIoT intelligence. However, existing surveys often focus on specific areas of IIoT intelligence, leading researchers and readers to have biases in their understanding of IIoT intelligence, that is, believing that research in one direction is the most important for the development of IIoT intelligence, while ignoring contributions from other directions. Therefore, this article provides a comprehensive overview of IIoT intelligence. We first conduct an in-depth analysis of the inevitability of manufacturing transformation and study the successful experiences from the practices of Chinese enterprises. Then, we give our definition of IIoT intelligence and demonstrate the value of IIoT intelligence for industries in functions, operations, deployments, and application. Afterward, we propose a hierarchical development architecture for IIoT intelligence, which consists of five layers. The practical values of technical upgrades at each layer are illustrated by a close look on lighthouse factories. Following that, we identify seven kinds of technologies that accelerate the transformation of manufacturing, and clarify their contributions. The ethical implications and environmental impacts of adopting IIoT intelligence in manufacturing are analyzed as well. Finally, we explore the open challenges and development trends from four aspects to inspire future researches.","2327-4662","","10.1109/JIOT.2024.3367692","National Natural Science Foundation of China(grant numbers:92367104,92267301); Purple Mountain Talents-Jiangning Baijia Lake Plan Program(grant numbers:74072203-3); Natural Science Foundation of Hubei Province(grant numbers:2023AFB128); Teaching Research Project of Wuhan University of Technology(grant numbers:W2022093); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10440434","Artificial intelligence (AI);Industrial Internet of Things (IIoT);IIoT intelligence;smart manufacturing","Industrial Internet of Things;Manufacturing;Computer architecture;Industries;Smart manufacturing;Market research;Costs","","13","","219","IEEE","20 Feb 2024","","","IEEE","IEEE Journals"
"AI-Enhanced Cloud-Edge-Terminal Collaborative Network: Survey, Applications, and Future Directions","H. Gu; L. Zhao; Z. Han; G. Zheng; S. Song","State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA; School of Engineering, University of Warwick, Coventry, U.K.; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China",IEEE Communications Surveys & Tutorials,"22 May 2024","2024","26","2","1322","1385","The cloud-edge-terminal collaborative network (CETCN) is considered as a novel paradigm for emerging applications owing to its huge potential in providing low-latency and ultra-reliable computing services. However, achieving such benefits is very challenging due to the heterogeneous computing power of terminal devices and the complex environment faced by the CETCN. In particular, the high-dimensional and dynamic environment states cause difficulties for the CETCN to make efficient decisions in terms of task offloading, collaborative caching and mobility management. To this end, artificial intelligence (AI), especially deep reinforcement learning (DRL) has been proven effective in solving sequential decision-making problems in various domains, and offers a promising solution for the above-mentioned issues due to several reasons. Firstly, accurate modelling of the CETCN, which is difficult to obtain for real-world applications, is not required for the DRL-based method. Secondly, DRL can effectively respond to high-dimensional and dynamic tasks through iterative interactions with the environment. Thirdly, due to the complexity of tasks and the differences in resource supply among different vendors, collaboration is required between different vendors to complete tasks. The multi-agent DRL (MADRL) methods are very effective in solving collaborative tasks, where the collaborative tasks can be jointly completed by cloud, edge and terminal devices which provided by different vendors. This survey provides a comprehensive overview regarding the applications of DRL and MADRL in the context of CETCN. The first part of this survey provides a depth overview of the key concepts of the CETCN and the mathematical underpinnings of both DRL and MADRL. Then, we highlight the applications of RL algorithms in solving various challenges within CETCN, such as task offloading, resource allocation, caching and mobility management. In addition, we extend discussion to explore how DRL and MADRL are making inroads into emerging CETCN scenarios like intelligent transportation system (ITS), the industrial Internet of Things (IIoT), smart health and digital agriculture. Furthermore, security considerations related to the application of DRL within CETCN are addressed, along with an overview of existing standards that pertain to edge intelligence. Finally, we list several lessons learned in this evolving field and outline future research opportunities and challenges that are critical for the development of the CETCN. We hope this survey will attract more researchers to investigate scalable and decentralized AI algorithms for the design of CETCN.","1553-877X","","10.1109/COMST.2023.3338153","National Key Research and Development Program of China(grant numbers:2020YFB1807700); Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101120003); Key Research and Development Program of Shaanxi(grant numbers:2022KWZ-09); Postdoctoral Research Program of Shaanxi Province; NSF(grant numbers:CNS-2107216,CNS-2128368,CMMI-2222810,ECCS-2302469); U.S. Department of Transportation; Toyota Motor Corporation; Amazon; National Natural Science Foundation of China(grant numbers:NSFC62071352); Royal Society International Exchanges(grant numbers:IEC/NSFC/181395); NSFC/RGC Joint Research Scheme sponsored by the Research Grants Council of the Hong Kong Special Administrative Region, China; National Natural Science Foundation of China(grant numbers:N_HKUST656/22); Higher Education Discipline Innovation Project(grant numbers:B08038); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10336879","Cloud-edge-terminal collaborative network (CETCN);single-agent reinforcement learning (SARL);multiagent reinforcement learning (MARL);network architecture;task offloading;resource allocation;collaborative caching;mobility management;applications;security;standards","Task analysis;Resource management;Surveys;Delays;Cloud computing;Servers;Federated learning","","12","","449","IEEE","1 Dec 2023","","","IEEE","IEEE Journals"
"6G Cloud-Native System: Vision, Challenges, Architecture Framework and Enabling Technologies","Q. Li; Z. Ding; X. Tong; G. Wu; S. Stojanovski; T. Luetzenkirchen; A. Kolekar; S. Bangolae; S. Palat","Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA",IEEE Access,"19 Sep 2022","2022","10","","96602","96625","Every generation of wireless technologies needs to bring a set of new system capabilities to enable future applications and services, the sixth generation mobile system (6G) is no exception. This paper provides an overview of the technology transformation from the communication-centric the forth generation mobile system (4G) and the fifth generation mobile system (5G) to the compute-centric 6G with the cloud-native system framework as the foundation of the next generation technologies. We explain what 6G plans to achieve, the fundamental reasons for this technology transformation, the architecture framework and enabling technologies to achieve 6G cloud-native technology objectives. This paper intends to provide a technical deep dive on the 6G cloud-native system to trigger more discussions, innovations and bring the technology transformation from concept to reality.","2169-3536","","10.1109/ACCESS.2022.3205341","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9882121","6G;cloud-native;cloud computing;communication-computing convergence;system architecture;ubiquitous computing","6G mobile communication;Cloud computing;5G mobile communication;Artificial intelligence;Ubiquitous computing;Systems architecture;Security","","12","","62","CCBY","8 Sep 2022","","","IEEE","IEEE Journals"
"A Federated Platform Enabling a Systematic Collaboration Among Devices, Data and Functions for Smart Mobility","L. You; M. Danaf; F. Zhao; J. Guan; C. L. Azevedo; B. Atasoy; M. Ben-Akiva","School of Intelligent Systems Engineering, Sun Yat-sen University, Shenzhen, China; Intelligent Transportation Systems Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Future Urban Mobility IRG, Singapore-MIT Alliance for Research and Technology Centre, 4 Engineering Drive 3, Singapore; School of Architecture, Harbin Institute of Technology, Shenzhen, China; Department of Management Engineering, Technical University of Denmark, Kongens Lyngby, Denmark; Intelligent Transportation Systems Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Intelligent Transportation Systems Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA",IEEE Transactions on Intelligent Transportation Systems,"29 Mar 2023","2023","24","4","4060","4074","Through the vast adoption and application of emerging technologies, the intelligence and autonomy of smart mobility can be substantially elevated to address more diversified demands and supplies. Along with this trend, a systematic collaboration among three essential elements of smart mobility services, namely devices, data and functions, is being studied to comprehensively break down the intrinsic barriers that existed in current solutions, to support the integration of connectable devices, the fusion of heterogeneous data, the composability of reusable functions, and the flexibility in their cooperations. To enable such a collaboration, this paper proposes a federated platform, called Future Mobility Sensing Advisor (FMSA), which can 1) manage the three elements through standardized interfaces separately and uniformly; 2) create a fully connected knowledge graph to orchestrate the three elements efficiently and effectively; 3) support the client-server interaction in centralized and federated modes to handle service requests and edge resources with various availability and accessibilities jointly and adaptively; and 4) accommodate various mobility services to foster harmonious and sustainable mobility tenderly and invisibly. Moreover, the efficiency and effectiveness of the platform are also tested through a performance evaluation, and a pilot supported at the Great Boston Area, respectively. As a result, it shows that FMSA can 1) achieve high performance by using the two interaction modes selectively, and 2) renovate smart mobility towards sustainability through personalized services that can measure user preferences and system objectives mutually.","1558-0016","","10.1109/TITS.2023.3236991","National Natural Science Foundation of China(grant numbers:62002398); Collaborative Innovation Center for Transportation of Guangzhou(grant numbers:202206010056); National Research Foundation Singapore through the Future Urban Mobility IRG Research Program of Singapore-MIT Alliance for Research and Technology; Tripod (Sustainable Travel Incentives with Prediction, Optimization and Personalization) Project under the U.S. Department of Energy Advanced Research Projects Agency-6 Energy (ARPA-E) Traveler Response Architecture using Novel Signaling for Network Efficiency in Transportation (TRANSNET) program(grant numbers:DE-AR0000611); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10024386","Smart mobility;systematic collaboration;federated platform;service orchestration;federated computing","Collaboration;Systematics;Sensors;Public transportation;Distributed databases;Data integration;Costs","","12","","34","IEEE","23 Jan 2023","","","IEEE","IEEE Journals"
"Hierarchical Modeling of Complex Internet of Things Systems Using Conceptual Modeling Approaches","K. M. Abbasi; T. A. Khan; I. U. Haq","Department of Software Engineering, Bahria University, Islamabad, Pakistan; Department of Software Engineering, Bahria University, Islamabad, Pakistan; Department of CIS, Pakistan Institute of Engineering and Applied Sciences (PIEAS), Islamabad, Pakistan",IEEE Access,"7 Aug 2019","2019","7","","102772","102791","Popularity of the Internet of things (IoT) is currently on the rise. Academia as well as industry is equally fascinated by this disruptive technology. The IoT research and implementation have succeeded securing their place in numerous domains including, but not limited to, business, transportation and logistics, health care, smart cities, and agriculture. Conceptual modeling is pivotal to the design and implementation of a system. Various modeling approaches have been used for a variety of the IoT applications and case studies. In this paper, we examine various innovative applications of the existing conceptual modeling approaches for the IoT. We analyze the use of these approaches against different concerns in isolation as well as in mutual combination for the IoT. We also define a set of terminologies that we frequently encounter in this manuscript and those likely to be used while adopting proposed modeling approaches for the IoT scenarios. This study also attempts to reconcile different modeling approaches toward a unified modeling system. For this purpose, we map various conceptual modeling approaches against different layers of functional model of the IoT reference architecture. There is also a scenario in the end of the paper which elaborates the use of different modeling approaches in hierarchy for a complex IoT system.","2169-3536","","10.1109/ACCESS.2019.2930933","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8771106","Agent-based modeling;ambient-oriented modeling;aspect-oriented modeling;conceptual modeling approaches;contract-based modeling;fuzzy-logic modeling;the Internet of Things;network-based modeling;object-based modeling;ontology;service-oriented modeling","Internet of Things;Computational modeling;Analytical models;Security;Software;Adaptation models;Computer architecture","","12","","131","CCBY","24 Jul 2019","","","IEEE","IEEE Journals"
"A Survey on the Bottleneck Between Applications Exploding and User Requirements in IoT","S. Cui; F. Farha; H. Ning; Z. Zhou; F. Shi; M. Daneshmand","School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; School of Information Engineering, China University of Geosciences (Beijing), Beijing, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; School of Business, Stevens Institute of Technology, Hoboken, NJ, USA",IEEE Internet of Things Journal,"22 Dec 2021","2022","9","1","261","273","The rapid growth of the Internet of Things (IoT) and the increasing number of connected devices have propelled the proliferation of offered applications, causing “applications exploding.” In the context of IoT, filtering and selecting the most relevant applications in a given situation is a challenging task. Thus, developing techniques that can alleviate applications exploding and meet users’ requirements is highly demanded for IoT development. This survey focuses on applications exploding in the IoT and reviews some of the existing techniques, such as intelligent sensing, content distribution network, microservices, and 5G, which help mitigate the effects of applications exploding. Furthermore, the survey discusses how to describe user requirements and offer application services to better match the two. In addition, this survey presents the smart home as an instance of typical IoT applications and explores how adaptive users’ requirements for food ordering can be better met when various food provider applications are available for choice. Finally, partially resolved and unresolved bottlenecks brought by applications exploding are put forward to be further researched by the technical and scientific community.","2327-4662","","10.1109/JIOT.2021.3097634","National Natural Science Foundation of China(grant numbers:61872038); Fundamental Research Funds for the Central Universities(grant numbers:FRF-BD-18-016A); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9486861","Applications exploding;matching applications bottleneck;smart home;user requirements","Sensors;Internet of Things;Security;Protocols;Cloud computing;Smart homes;Scalability","","12","","76","IEEE","15 Jul 2021","","","IEEE","IEEE Journals"
"A Generic Future Mobility Sensing System for Travel Data Collection, Management, Fusion, and Visualization","L. You; F. Zhao; L. Cheah; K. Jeong; P. C. Zegras; M. Ben-Akiva","Future Urban Mobility IRG, Singapore-MIT Alliance for Research and Technology Centre, Singapore; Future Urban Mobility IRG, Singapore-MIT Alliance for Research and Technology Centre, Singapore; Engineering Systems and Design, Singapore University of Technology and Design, Singapore; Intelligent Transportation Systems Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Urban Studies and Planning, Massachusetts Institute of Technology, Cambridge, MA, USA; Civil and Environmental Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA",IEEE Transactions on Intelligent Transportation Systems,"2 Oct 2020","2020","21","10","4149","4160","In studies of human mobility, there is a need for a holistic system for collection of sensing data, management of data flows, fusion of multiple data sources, and visualization of integrated data to better understand travel behavior. We have designed and implemented a generic Future Mobility Sensing (FMS) system to serve these purposes. FMS harnesses various sensing technologies, heterogeneous multi-source data and analytical functionalities with three dedicated platforms, namely 1) the FMS Data Collection Platform, which intertwines sensing objects, machine learning algorithms and user verifications to collect high resolution, multi-day travel data; 2) the FMS Data Management Platform, which provides standardized APIs to access data stored in an interconnected data model; and 3) the FMS Data Fusion and Visualization Platform, which consolidates multi-source data to be interpreted and presented. Together, the three platforms form a mobility sensing flow to facilitate data-driven analysis and decision-making. With FMS, heterogeneous multi-source data are suitably integrated for analysis, and multi-dimensional knowledge is extracted and presented in intuitive and interactive analytical dashboards. The system is intended to be generic to support different requirements of mobility studies, such as travel surveys, as its function modules are reusable and can be customized to support a unified data collection, management, fusion and visualization process. This paper introduces the overall architecture of the FMS system and summarizes various applications that it can support. Specifically, we present a study of commercial vehicle parking in Singapore to demonstrate the capability of the system.","1558-0016","","10.1109/TITS.2019.2938828","National Research Foundation Singapore through the Singapore-MIT Alliance for Research and Technology’s Future Urban Mobility IRG Research Program; Singapore Ministry of National Development and the National Research Foundation; Land and Liveability National Innovation Challenge(grant numbers:L2NICTDF1-2016-1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8833515","Mobility sensing flow;multi-source data collection;big data;travel survey","Frequency modulation;Sensors;Data visualization;Data collection;Data mining;Data integration;Urban areas","","12","","32","IEEE","11 Sep 2019","","","IEEE","IEEE Journals"
"eBPF: A New Approach to Cloud-Native Observability, Networking and Security for Current (5G) and Future Mobile Networks (6G and Beyond)","D. Soldani; P. Nahi; H. Bour; S. Jafarizadeh; M. F. Soliman; L. Di Giovanna; F. Monaco; G. Ognibene; F. Risso","Rakuten Mobile Inc, Tokyo, Setagaya-ku, Japan; Rakuten Mobile Inc, Tokyo, Setagaya-ku, Japan; Rakuten Mobile Inc, Tokyo, Setagaya-ku, Japan; Rakuten Mobile Inc, Tokyo, Setagaya-ku, Japan; Rakuten Mobile Inc, Tokyo, Setagaya-ku, Japan; DAUIN Department, Politecnico di Torino, Turin, Italy; DAUIN Department, Politecnico di Torino, Turin, Italy; DAUIN Department, Politecnico di Torino, Turin, Italy; DAUIN Department, Politecnico di Torino, Turin, Italy",IEEE Access,"14 Jun 2023","2023","11","","57174","57202","Modern mobile communication networks and new service applications are deployed on cloud-native platforms. Kubernetes (K8s) is the de facto distributed operating system for container orchestration, and the extended version of the Berkeley Packet Filter (eBPF)– in the Linux (and MS Windows) kernel– is fundamentally changing the approach to cloud-native networking, security, and observability. In this paper, we introduce what eBPF is, its potential for Telco cloud, and review some of the most promising pricing and billing models applied to this revolutionary operating system (OS) technology. These models include schemes based on a data source usage model or the number of eBPF agents deployed on the network, linked to specific eBPF modules. These modules encompass network observability, runtime security, and power dissipation monitoring. Next, we present our eBPF platform, named Sauron in this work, and demonstrate how eBPF allows us to write custom code and dynamically load eBPF programs into the kernel. These programs enable us to estimate the energy consumption of cloud-native functions, derive performance counters and gauges for transport networks, 5G applications, and non-access stratum protocols. Additionally, we can detect and respond to unauthorized access to cloud-native resources in real-time using eBPF. Our experimental results demonstrate the technical feasibility of eBPF in achieving highly performant monitoring, observability, and security tooling for current mobile networks (5G, 5G Advanced) as well as future networks (6G and beyond).","2169-3536","","10.1109/ACCESS.2023.3281480","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10138542","eBPF;extended Berkeley packet filter;cloud-native observability;cloud-native security;cloud-native networking;cloud-native monitoring;5G;5G Advanced;6G;Kubernetes;K8s","5G mobile communication;Cloud computing;Security;Observability;Linux;6G mobile communication;Band-pass filters","","12","","42","CCBYNCND","30 May 2023","","","IEEE","IEEE Journals"
"Implementation of Network-Coded Cooperation for Energy Efficient Content Distribution in 5G Mobile Small Cells","R. Torre; I. Leyva-Mayorga; S. Pandi; H. Salah; G. T. Nguyen; F. H. P. Fitzek","Deutsche Telekom Chair of Communication Networks, Technische Universität Dresden, Dresden, Germany; Department of Electronic Systems, Aalborg University, Aalborg, Denmark; Deutsche Telekom Chair of Communication Networks, Technische Universität Dresden, Dresden, Germany; Deutsche Telekom Chair of Communication Networks, Technische Universität Dresden, Dresden, Germany; Deutsche Telekom Chair of Communication Networks, Technische Universität Dresden, Dresden, Germany; Deutsche Telekom Chair of Communication Networks, Technische Universität Dresden, Dresden, Germany",IEEE Access,"19 Oct 2020","2020","8","","185964","185980","The continuous increase of mobile data traffic calls for the design of energy-efficient content distribution mechanisms, to be incorporated in the fifth generation of mobile networks, 5G. One of the biggest concerns of the companies and the research community is to reduce the energy consumption in both the user equipments (UEs) and the network equipment. In this article, we present a novel content distribution framework called Network-Coded Cooperative (NCC) Networks, which benefits from the interplay between mobile clouds (MC) and Random Linear Network Coding (RLNC) to reduce the overall energy consumption in the devices that take part in the communication. This novel framework leads to reduced energy consumption by offloading the cellular interface to a link with greater energy efficiency, for instance, WiFi, within the mobile small cell. We evaluate the performance of our framework analytically and in practical implementation (i.e., testbed) in terms of throughput, energy savings, packet decoding ratio, latency, and synchronicity. In comparison to the conventional content distribution system, for the case of four users, the analytical model and the testbed implementation show energy savings of more than 12% and 8%, respectively. Furthermore, network usage is reduced, losses are neutralized, and the content is synchronously distributed to all users.","2169-3536","","10.1109/ACCESS.2020.3029601","European Union’s H2020 Research and Innovation Program(grant numbers:H2020-MCSA-ITN-2016-SECRET 722424); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9217438","Network coding;random linear network coding (RLNC);cooperative communication;content distribution;cellular networks;mobile small cells;energy efficiency;traffic offload","Energy consumption;Analytical models;Cellular networks;Encoding;5G mobile communication;Throughput;Protocols","","12","","54","CCBY","8 Oct 2020","","","IEEE","IEEE Journals"
"An Investigation of Cyber-Attacks and Security Mechanisms for Connected and Autonomous Vehicles","S. Gupta; C. Maple; R. Passerone","Department of Information Engineering and Computer Science (DISI), University of Trento, Trento, Italy; Secure Cyber Systems Research Group (SCSRG), Warwick Manufacturing Group (WMG), The University of Warwick, Coventry, U.K.; Department of Information Engineering and Computer Science (DISI), University of Trento, Trento, Italy",IEEE Access,"29 Aug 2023","2023","11","","90641","90669","Connected and autonomous vehicles (CAVs) can fulfill the emerging demand for smart transportation on a global scale. Such innovations for transportation can bring manyfold benefits, from fully autonomous driving services to proactive vehicle monitoring and traffic management. However, given the complexity involved in the deployment of CAVs, zero-tolerance safety, and security measures must be incorporated to avert vehicle immobilization, road accidents, disclosure of sensitive data, or any potential threats. In this article, we conceive a reference architecture for a CAVs ecosystem to derive a common attack taxonomy for the investigation of existing and emerging cyber threats. Subsequently, we discuss security mechanisms for the CAVs ecosystem that can be useful for the safe and secure transportation of passengers from one destination to another based on comprehensive studies of academic literature and industry white papers. Our work can provide valuable insights to security engineers and system architects for investigating security problems using a top-to-bottom approach and can aid in envisioning robust security solutions to ensure seamless CAVs operations.","2169-3536","","10.1109/ACCESS.2023.3307473","Fondazione Bruno Kessler (FBK) AI@TN Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10226207","Connected and autonomous vehicles;edge computing;fog computing;cloud computing;cyber attacks;security mechanisms","Security;Ecosystems;Edge computing;Autonomous vehicles;Sensors;Connected vehicles;Vehicle-to-everything;Cloud computing;Cyberattack","","12","","207","CCBYNCND","22 Aug 2023","","","IEEE","IEEE Journals"
"Energy Efficient Placement of Workloads in Composable Data Center Networks","O. O. Ajibola; T. E. H. El-Gorashi; J. M. H. Elmirghani","School of Electronic and Electrical Engineering, University of Leeds, Leeds, U.K.; School of Electronic and Electrical Engineering, University of Leeds, Leeds, U.K.; School of Electronic and Electrical Engineering, University of Leeds, Leeds, U.K.",Journal of Lightwave Technology,"7 May 2021","2021","39","10","3037","3063","This paper studies the energy efficiency of composable data center (DC) infrastructures over network topologies. Using a mixed integer linear programming (MILP) model, we compare the performance of disaggregation at rack-scale and pod-scale over selected electrical, optical and hybrid network topologies relative to a traditional DC. Relative to a pod-scale DC, the results show that physical disaggregation at rack-scale is sufficient for optimal efficiency when the optical network topology is adopted, and resource components are allocated in a suitable manner. The optical network topology also enables optimal energy efficiency in composable DCs. The paper also studies logical disaggregation of traditional DC servers over an optical network topology. Relative to physical disaggregation at rack-scale, logical disaggregation of server resources within each rack enables marginal fall in the total DC power consumption (TDPC) due to improved resource demands placement. Hence, an adaptable composable infrastructure that can support both in memory (access) latency sensitive and insensitive workloads is enabled. We also conduct a study of the adoption of micro-service architecture in both traditional and composable DCs. Our results show that increasing the modularity of workloads improves the energy efficiency in traditional DCs, but disproportionate utilization of DC resources persists. A combination of disaggregation and micro-services achieved up to 23% reduction in the TDPC of the traditional DC by enabling optimal resources utilization and energy efficiencies. Finally, we propose a heuristic for energy efficient placement of workloads in composable DCs which replicates the trends produced by the MILP model formulated in this paper.","1558-2213","","10.1109/JLT.2021.3063325","Engineering and Physical Sciences Research Council; INTelligent Energy aware NETworks(grant numbers:EP/H040536/1); SwiTching And tRansmission(grant numbers:EP/K016873/1); Terabit Bidirectional Multi-user Optical Wireless System(grant numbers:EP/S016570/1); Petroleum Technology Trust Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9369047","Composable infrastructures;energy efficient data centers;micro-services;MILP;optical networks;rack-scale data center;software defined infrastructures","Servers;Software;Network topology;Energy efficiency;Hardware;Topology;Optical fiber networks","","12","","49","IEEE","3 Mar 2021","","","IEEE","IEEE Journals"
"Opportunistic UAV Deployment for Intelligent On-Demand IoV Service Management","H. Sami; R. Saado; A. E. Saoudi; A. Mourad; H. Otrok; J. Bentahar","Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada; Department of CSM, Cyber Security Systems and Applied AI Research Center, Lebanese American University, Beirut, Lebanon; Department of CSM, Cyber Security Systems and Applied AI Research Center, Lebanese American University, Beirut, Lebanon; Department of CSM, Cyber Security Systems and Applied AI Research Center, Lebanese American University, Beirut, Lebanon; Department of EECS, Center of Cyber-Physical Systems, Khalifa University, Abu Dhabi, UAE; Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada",IEEE Transactions on Network and Service Management,"9 Oct 2023","2023","20","3","3428","3442","Due to the current improvement in self-driving cars and the extensive focus and research on the topic of the Internet of Vehicles (IoV), the near future may behold a great revolution in the automotive industry as cars become fully autonomous. This change entails a considerable amount of data to be transferred from Internet of Things (IoT) devices, such as radars, sensors, and actuators. Consequently, overwhelming the existing infrastructure, namely cloud, and Road Side Units (RSU), reduces the quality of service (QoS) experienced by vehicular users. Accordingly, this paper contributes in proposing a new architecture for using Unmanned Ariel Vehicles (UAVs) and On-Boarding Units (OBUs) working in collaboration to achieve a significantly improved QoS. The proposed framework offers an end-to-end solution for master election, cluster management and recovery, vehicle selection, service placement, and accurate localization of vehicles. A QoS improvement is possible through an efficient cluster formation and placement solution that assigns lightweight services, as containers, to OBUs and UAVs while meeting various objectives. The efficiency of the proposed scheme originates from the use of the evolutionary Memetic Algorithm that 1) respects the mobility and energy constraints of UAVs and OBUs, 2) meets the user demands, and 3) uses machine learning for the accurate localization of vehicles. Our experiments using the Mininet-WiFi and SUMO simulators show at least 30% improvement in terms of QoS compared to a state-of-the-art solution.","1932-4537","","10.1109/TNSM.2023.3242205","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10036008","UAV;OBU;IoV;container placement;memetic algorithm;localization;machine learning","Quality of service;Microservice architectures;Containers;Computer architecture;Stability analysis;Optimization;Location awareness","","11","","50","IEEE","3 Feb 2023","","","IEEE","IEEE Journals"
"Design of Industrial Edge Applications Based on IEC 61499 Microservices and Containers","W. Dai; Y. Zhang; L. Kong; J. H. Christensen; D. Huang","Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Huawei Technology Ltd., Shenzhen, China; Holobloc Inc., Cleveland, OH, USA; Department of Automation, Shanghai Jiao Tong University Shanghai, Shanghai, China",IEEE Transactions on Industrial Informatics,"21 Jun 2023","2023","19","7","7925","7935","Industrial automation is entering a new era of the Industrial Internet with enhanced computing, communication, and storage capabilities provided by cloud computing and field devices. The paradigm of automation systems is shifting from the ISA-95 pyramid to the two-layers architecture: industrial cloud and edge computing. Industrial software is also evolving under the new architecture in ways for which dedicated software applications are no longer suited. Service-based industrial cloud and edge applications provide maximum flexibility, interoperability, and efficiency by combining the IEC 61499 standard, microservice architecture, and container technology. This article provides orchestration methods and deployment procedures for the OT-IT hybrid industrial edge applications. The feasibility of the proposed approach is demonstrated by an industrial case study with accompanying performance analysis.","1941-0050","","10.1109/TII.2022.3214199","National R&D Key Program of China(grant numbers:2021YFB1714000); National Natural Science Foundation of China(grant numbers:61973216,62122048); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9918040","Industrial edge computing;IEC 61499;microservices;linux container;virtualization","Microservice architectures;IEC Standards;Cloud computing;Computer architecture;Containers;Automation;Virtualization","","11","","30","IEEE","13 Oct 2022","","","IEEE","IEEE Journals"
"Edge Video Analytics: A Survey on Applications, Systems and Enabling Techniques","R. Xu; S. Razavi; R. Zheng","Department of Computing and Software, McMaster University, Hamilton, ON, Canada; Department of Civil Engineering, McMaster University, Hamilton, ON, Canada; Department of Computing and Software, McMaster University, Hamilton, ON, Canada",IEEE Communications Surveys & Tutorials,"21 Nov 2023","2023","25","4","2951","2982","Video, as a key driver in the global explosion of digital information, can create tremendous benefits for human society. Governments and enterprises are deploying innumerable cameras for a variety of applications, e.g., law enforcement, emergency management, traffic control, and security surveillance, all facilitated by video analytics (VA). This trend is spurred by the rapid advancement of deep learning (DL), which enables more precise models for object classification, detection, and tracking. Meanwhile, with the proliferation of Internet-connected devices, massive amounts of data are generated daily, overwhelming the cloud. Edge computing, an emerging paradigm that moves workloads and services from the network core to the network edge, has been widely recognized as a promising solution. The resulting new intersection, edge video analytics (EVA), begins to attract widespread attention. Nevertheless, only a few loosely-related surveys exist on this topic. The basic concepts of EVA (e.g., definition, architectures) were not fully elucidated due to the rapid development of this domain. To fill these gaps, we provide a comprehensive survey of the recent efforts on EVA. In this paper, we first review the fundamentals of edge computing, followed by an overview of VA. EVA systems and their enabling techniques are discussed next. In addition, we introduce prevalent frameworks and datasets to aid future researchers in the development of EVA systems. Finally, we discuss existing challenges and foresee future research directions. We believe this survey will help readers comprehend the relationship between VA and edge computing, and spark new ideas on EVA.","1553-877X","","10.1109/COMST.2023.3323091","China Scholarship Council(grant numbers:202108320092); NSERC Discovery Grants; Canada Research Chair Program; McMaster Faculty of Engineering’s Multidisciplinary Ph.D. Research Support; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10274950","Video analytics;edge computing;computer vision;deep learning","Surveys;Edge computing;Visual analytics;Tutorials;Cloud computing;Real-time systems;Cameras","","11","","290","IEEE","10 Oct 2023","","","IEEE","IEEE Journals"
"Optimal Selection Techniques for Cloud Service Providers","G. Tricomi; G. Merlino; A. Panarello; A. Puliafito","Dipartimento di Ingegneria, Università degli Studi di Messina, Messina, Italy; Dipartimento di Ingegneria, Università degli Studi di Messina, Messina, Italy; SmartMe.IO S.r.l., Messina, Italy; Dipartimento di Ingegneria, Università degli Studi di Messina, Messina, Italy",IEEE Access,"17 Nov 2020","2020","8","","203591","203618","Nowadays Cloud computing permeates almost every domain in Information and Communications Technology (ICT) and, increasingly, most of the action is shifting from large, dominant players toward independent, heterogeneous, private/hybrid deployments, in line with an ever wider range of business models and stakeholders. The rapid growth in the numbers and diversity of small and medium Cloud providers is bringing new challenges in the as-a-Services space. Indeed, significant hurdles for smaller Cloud service providers in being competitive with the incumbent market leaders induce some innovative players to “federate” deployments in order to pool a larger, virtually limitless, set of resources across the federation, and stand to gain in terms of economies of scale and resource usage efficiency. Several are the challenges that need to be addressed in building and managing a federated environment, that may go under the “Security”, “Interoperability”, “Versatility”, “Automatic Selection” and “Scalability” labels. The aim of this paper is to present a survey about the approaches and challenges belonging to the “Automatic Selection” category. This work provides a literature review of different approaches adopted in the “Automatic and Optimal Cloud Service Provider Selection”, also covering “Federated and Multi-Cloud” environments.","2169-3536","","10.1109/ACCESS.2020.3035816","Italian Project TOOLSMART financed by Open Community(grant numbers:PA 2020); Pon Governance(grant numbers:2014–2020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9248017","Algorithms;cloud federation;matchmaking;multi-cloud;optimal selection;survey","Cloud computing;Computational modeling;Buildings;Economies of scale;Information and communication technology;Stakeholders;Business","","11","","104","CCBY","4 Nov 2020","","","IEEE","IEEE Journals"
"Information-Centric Mobile Networks: A Survey, Discussion, and Future Research Directions","S. Fayyaz; M. Atif Ur Rehman; M. Salah Ud Din; M. I. Biswas; A. K. Bashir; B. -S. Kim","Department of Electronics and Computer Engineering, Hongik University, Sejong, South Korea; Department of Computing and Mathematics, Manchester Metropolitan University, Manchester, U.K; Department of Electronics and Computer Engineering, Hongik University, Sejong, South Korea; Department of Computing and Mathematics, Manchester Metropolitan University, Manchester, U.K; Department of Computing and Mathematics, Manchester Metropolitan University, Manchester, U.K; Department of Software and Communications Engineering, Hongik University, Sejong, South Korea",IEEE Access,"1 May 2023","2023","11","","40328","40372","Information-centric networking (ICN) and its fruition, the named data networking (NDN) is a paradigm shift from host-centric address-based communication architecture to the content-centric name-based one. ICN intends to resolve various major issues faced by today’s internet architecture such as privacy, security, consistent routing, and mobility, to name a few. With the massive increase of mobile data traffic in today’s era, mobility is one of the major concerns in networking. On the one hand, ICN realization i.e., the NDN follows a pull-based communication model and natively supports the consumer (end-user) mobility in wired networks by maintaining the forwarding states on intermediate nodes. Nevertheless, the mobile consumer nodes confront issues in wireless networking environments such as excessive energy consumption as a result of request flooding, content retrieval delays due to intermittent connectivity, and bandwidth consumption due to the broadcasting nature of the wireless medium, among others. The producer (content-generator) mobility, on the other hand, was not initially supported in the original architectural design of NDN for both wired and wireless networks. Therefore, to efficiently address the degradation issues incurred by mobile consumer/producer nodes, a plethora of mobility management schemes have been proposed over the recent few years. In this paper, we provided a detailed survey on the existing research efforts—in the context of producer, consumer, and hybrid mobility, that have been proposed in the literature. Moreover, we outlined various research directions considering the role of mobility in futuristic technologies such as artificial intelligence-enabled smart networks, software-defined networking, edge computing, vehicular-fog computing, autonomous driving, semantic communication, and resource-constrained Internet of Things.","2169-3536","","10.1109/ACCESS.2023.3268775","National Research Foundation of Korea (NRF); Korean Government (MSIT)(grant numbers:2022R1A2C1003549); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10105953","Named data networking;mobility management;producer mobility;wireless networking;artificial intelligence;edge computing;vehicular fog computing;autonomous driving","IP networks;Wireless communication;Delays;Wireless sensor networks;Servers;Broadcasting;Sockets","","11","","138","CCBYNCND","20 Apr 2023","","","IEEE","IEEE Journals"
"Toward Design of Internet of Things and Machine Learning-Enabled Frameworks for Analysis and Prediction of Water Quality","M. A. Rahu; A. F. Chandio; K. Aurangzeb; S. Karim; M. Alhussein; M. S. Anwar","Department of Electronic Engineering, Quaid-e-Awam University of Engineering, Science and Technology, Nawabshah, Pakistan; Department of Electronic Engineering, Quaid-e-Awam University of Engineering, Science and Technology, Nawabshah, Pakistan; Department of Computer Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Department of Telecommunication Engineering, Quaid-e-Awam University of Engineering, Science and Technology, Nawabshah, Pakistan; Department of Computer Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Department of AI and Software, Gachon University, Seongnam-si, South Korea",IEEE Access,"22 Sep 2023","2023","11","","101055","101086","The degradation of water quality has become a critical concern worldwide, necessitating innovative approaches for monitoring and predicting water quality. This paper proposes an integrated framework that combines the Internet of Things (IoT) and machine learning paradigms for comprehensive water quality analysis and prediction. The IoT-enabled framework comprises four modules: sensing, coordinator, data processing, and decision. The IoT framework is equipped with temperature, pH, turbidity, and Total Dissolved Solids (TDS) sensors to collect the data from Rohri Canal, SBA, Pakistan. The acquired data is preprocessed and then analyzed using machine learning models to predict the Water Quality Index (WQI) and Water Quality Class (WQC). With this aim, we designed a machine learning-enabled framework for water quality analysis and prediction. Preprocessing steps such as data cleaning, normalization using the Z-score technique, correlation, and splitting are performed before applying machine learning models. Regression models: LSTM (Long Short-Term Memory), SVR (Support Vector Regression), MLP (Multilayer Perceptron) and NARNet (Nonlinear Autoregressive Network) are employed to predict the WQI, and classification models: SVM (Support Vector Machine), XGBoost (eXtreme Gradient Boosting), Decision Trees, and Random Forest are employed to predict the WQC. Before that, the Dataset used for evaluating machine learning models is split into two subsets: Dataset 1 and Dataset 2. Dataset 1 comprises 600 values for each parameter, while Dataset 2 includes the complete set of 6000 values for each parameter. This division enables comparison and evaluation of the models’ performance. The results indicate that the MLP regression model has strong predictive performance with the lowest Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE) values, along with the highest R-squared (0.93), indicating accurate and precise predictions. In contrast, the SVR model demonstrates weaker performance, evidenced by higher errors and a lower R-squared (0.73). Among classification algorithms, the Random Forest achieves the highest metrics: accuracy (0.91), precision (0.93), recall (0.92), and F1-score (0.91). It is also conceived that the machine learning models perform better when applied to datasets with smaller numbers of values compared to datasets with larger numbers of values. Moreover, comparisons with existing studies reveal this study’s improved regression performance, with consistently lower errors and higher R-squared values. For classification, the Random Forest model outperforms others, with exceptional accuracy, precision, recall, and F1-score metrics.","2169-3536","","10.1109/ACCESS.2023.3315649","King Saud University, Riyadh, Saudi Arabia, through the Researchers Supporting Project(grant numbers:RSPD2023R947); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10251529","Data collection;environmental monitoring;Internet of Things (IoT);machine learning;water quality analysis;water quality class (WQC);water quality index (WQI)","Water quality;Machine learning;Data models;Monitoring;Water resources;Sensors;Predictive models","","11","","99","CCBYNCND","14 Sep 2023","","","IEEE","IEEE Journals"
"Extended Berkeley Packet Filter: An Application Perspective","H. Sharaf; I. Ahmad; T. Dimitriou","Department of Computer Engineering, College of Engineering and Petroleum, Kuwait University, Kuwait, Kuwait; Department of Computer Engineering, College of Engineering and Petroleum, Kuwait University, Kuwait, Kuwait; Department of Computer Engineering, College of Engineering and Petroleum, Kuwait University, Kuwait, Kuwait",IEEE Access,"8 Dec 2022","2022","10","","126370","126393","The extended Berkeley Packet Filter (eBPF) is a lightweight and fast 64-bit RISC-like virtual machine (VM) inside the Linux kernel. eBPF has emerged as the most promising and de facto standard of executing untrusted, user-defined specialized code at run-time inside the kernel with strong performance, portability, flexibility, and safety guarantees. Due to these key benefits and availability of a rich ecosystem of compilers and tools within the Linux kernel, eBPF has received widespread adoption by both industry and academia for a wide range of application domains. The most important include enhancing performance of monitoring tools and providing a variety of new security mechanisms, data collection tools and data screening applications. In this review, we investigate the landscape of existing eBPF use-cases and trends with aim to provide a clear roadmap for researchers and developers. We first introduce the necessary background knowledge for eBPF before delving into its applications. Although, the potential use-cases of eBPF are vast, we restrict our focus on four key application domains related to networking, security, storage, and sandboxing. Then for each application domain, we analyze and summarize solution techniques along with their working principles in an effort to provide an insightful discussion that will enable researchers and practitioners to easily adopt eBPF into their designs. Finally, we delineate several exciting research avenues to fully exploit the revolutionary eBPF technology.","2169-3536","","10.1109/ACCESS.2022.3226269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9968265","BPF;eBPF;XDP;Linux kernel;security;network;sandboxing;storage;containers","Storage management;Band-pass filters;Filtering;Security;Cloud computing;Monitoring;Linux;Containers","","11","","109","CCBYNCND","1 Dec 2022","","","IEEE","IEEE Journals"
"I-ViSE: Interactive Video Surveillance as an Edge Service Using Unsupervised Feature Queries","S. Y. Nikouei; Y. Chen; A. J. Aved; E. Blasch","Department of Electrical and Computer Engineering, Binghamton University—SUNY, Binghamton, NY, USA; Department of Electrical and Computer Engineering, Binghamton University—SUNY, Binghamton, NY, USA; Department of Applied Analytics Branch, U.S. Air Force Research Laboratory, Rome, NY, USA; Department of Applied Analytics Branch, U.S. Air Force Research Laboratory, Rome, NY, USA",IEEE Internet of Things Journal,"25 Oct 2021","2021","8","21","16181","16190","Situation awareness (SAW) is essential for many mission-critical applications. However, SAW is challenging when trying to immediately identify objects of interest or focus on suspicious activities from thousands of video frames. This article develops a queryable system to instantly select interesting content. While face recognition technology is mature, in many scenarios, such as public safety monitoring, the features of objects of interest may be much more complicated than face features. In addition, human operators may not be always able to provide a descriptive, simple, and accurate query. Actually, it is more often that there are only rough, general descriptions of certain suspicious objects or accidents. This article proposes interactive video surveillance as an edge service (I-ViSE) based on unsupervised feature queries. Adopting unsupervised methods that do not reveal any private information, the I-ViSE scheme utilizes general features of a human body and color of clothes. An I-ViSE prototype is built following the edge-fog computing paradigm and the experimental results verified the I-ViSE scheme meets the design goal of scene recognition and target analysis in less than 2 s.","2327-4662","","10.1109/JIOT.2020.3016825","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9167223","Decentralization;online query;privacy preserving;smart surveillance;video feature extraction","Streaming media;Cameras;Computer architecture;Security;Surveillance;Image edge detection;Privacy","","11","","36","IEEE","14 Aug 2020","","","IEEE","IEEE Journals"
"The Road to Trustworthy 6G: A Survey on Trust Anchor Technologies","B. Veith; D. Krummacker; H. D. Schotten","Intelligent Networks Research Group, German Research Center for Artificial Intelligence (DFKI GmbH), Kaiserslautern, Germany; Intelligent Networks Research Group, German Research Center for Artificial Intelligence (DFKI GmbH), Kaiserslautern, Germany; Intelligent Networks Research Group, German Research Center for Artificial Intelligence (DFKI GmbH), Kaiserslautern, Germany",IEEE Open Journal of the Communications Society,"27 Feb 2023","2023","4","","581","595","While 5G networks are driving a growing number of use cases in the fields of iot and industrial applications, the vision of the next generation of mobile communications systems already includes concepts massively transforming the way people will interact with the digital world through the network, as humans are shifting into the center of diverse network driven applications. Envisaged use cases and possibilities to provide services and resources in a distributed manner render an architectural solution for trust establishment a critical component of 6G networks. This survey provides an overview of terms and visions related to the topic of trust in general and in mobile communications systems. Requirements for an end-to-end trust building framework are derived, in order to give a starting point for the design process of a trust anchor service as a component of 6G networks.","2644-125X","","10.1109/OJCOMS.2023.3244274","German Federal Ministry for Education and Research (BMBF) through the Project »Open6GHub«(grant numbers:16KISK003K); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10042484","6G;AI;DLT;trust anchors;trust as a service","6G mobile communication;Security;Peer-to-peer computing;Buildings;Quality of service;Distributed ledger;Biological system modeling","","11","","84","CCBYNCND","13 Feb 2023","","","IEEE","IEEE Journals"
"Classifying Model-View-Controller Software Applications Using Self-Organizing Maps","D. Guamán; S. Delgado; J. Pérez","Departamento de Sistemas Informáticos (DSI), ETS de Ingeniería de Sistemas Informáticos (ETSISI), Universidad Politécnica de Madrid, Madrid, Spain; Departamento de Sistemas Informáticos (DSI), ETS de Ingeniería de Sistemas Informáticos (ETSISI), Universidad Politécnica de Madrid, Madrid, Spain; Departamento de Sistemas Informáticos (DSI), ETS de Ingeniería de Sistemas Informáticos (ETSISI), Universidad Politécnica de Madrid, Madrid, Spain",IEEE Access,"25 Mar 2021","2021","9","","45201","45229","The new era of information and the needs of our society require continuous change in software and technology. Changes are produced very quickly and software systems require evolving at the same velocity, which implies that the decision-making process of software architectures should be (semi-)automated to satisfy changing needs and to avoid wrong decisions. This issue is critical since suboptimal architecture design decisions may lead to high cost and poor software quality. Therefore, systematic and (semi-)automated mechanisms that help software architects during the decision-making process are required. Architectural patterns are one of the most important features of software applications, but the same pattern can be implemented in different ways, leaving to results of different quality. When an application requires to evolve, knowledge extracted from similar applications is useful for driving decisions, since quality pattern implementations can be reproduced in similar applications to improve specific quality attributes. Therefore, clustering methods are especially suitable for classifying similar pattern implementations. In this paper, we apply a novel unsupervised clustering technique, based on the well-known artificial neural network model Self-Organizing Maps, to classify Model-View-Controller (MVC) pattern from a quality point of view. Software quality is analyzed by 24 metrics organized into the categories of Count/Size, Maintainability, Duplications, Complexity, and Design Quality. The main goal of this work is twofold: to identify the quality features that establish the similarity of MVC applications without software architect bias, and to classify MVC applications by means of Self-Organizing Maps based on quality metrics. To that end, this work performs an exploratory study by conducting two analyses with a dataset of 87 Java MVC applications characterized by the 24 metrics and two attributes that describe the technology dimension of the application. The stated findings provide a knowledge base that can help in the decision-making process for the architecture of Java MVC applications.","2169-3536","","10.1109/ACCESS.2021.3066348","Computer Science Department, Universidad Técnica Particular de Loja; Spanish Ministry of Economy and Competitiveness (MINECO) through the Project CROWDSAVING(grant numbers:TIN2016-79726-C2-1-R); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380344","Model-view-controller;software architectures;artificial neural networks;self-organizing maps (SOM);unsupervised clustering techniques;machine learning;software quality","Software;Software architecture;Computer architecture;Decision making;Clustering methods;Clustering algorithms;Self-organizing feature maps","","11","","110","CCBY","17 Mar 2021","","","IEEE","IEEE Journals"
"Monitoring-Aware Optimal Deployment for Applications Based on Microservices","E. Fadda; P. Plebani; M. Vitali","DAUIN Department, Corso Duca degli Abruzzi 24, Turin, Italy; DEIB Department, Politecnico di Milano, Milan, Italy; DEIB Department, Politecnico di Milano, Milan, Italy",IEEE Transactions on Services Computing,"8 Dec 2021","2021","14","6","1849","1863","Modern cloud applications are required to be distributed, scalable, and reliable. The microservice architectural style enables developers to achieve this goal with reduced effort. Nonetheless, microservices deployment is not trivial due to the heterogeneity of the microservices in terms of both functional and non-functional requirements. This is also true when considering the monitoring requirements that are specific to each microservice and must be satisfied in order to enable the verification of the application objectives satisfaction. However, not all providers offer the same set of metrics with the same quality. The goal of this paper is to provide an approach for supporting the deployment of microservices in multi-cloud environments focusing on the Quality of Monitoring. Adopting a multi-objective mixed integer linear optimisation problem, our approach supports the application owner in finding the optimal deployment for satisfying all the constraints and maximising the quality of monitored data, while minimising the costs. To this end, a knowledge base is introduced to mediate between the perspectives of the cloud provider and the application owner, while a Bayesian Network is adopted to enhance the provider’s monitoring capabilities by estimating metrics requested by the application owners that the cloud provider is not able to monitor.","1939-1374","","10.1109/TSC.2019.2910069","European Union's Horizon 2020(grant numbers:RIA 731945); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8685146","Monitoring system;microservice;multi-cloud;cloud computing;deployment optimisation","Monitoring;Measurement;Cloud computing;Optimization;Knowledge based systems;Scalability;Trajectory","","11","","35","IEEE","11 Apr 2019","","","IEEE","IEEE Journals"
"When Clones Flock Near the Fog","S. Abdelwahab; S. Zhang; A. Greenacre; K. Ovesen; K. Bergman; B. Hamdaoui","School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR, USA",IEEE Internet of Things Journal,"8 Jun 2018","2018","5","3","1914","1923","FogMQ is a message brokering and device cloning service in fog and edge computing. Excessive tail end-to-end latency occurs with conventional message brokers when a massive number of geographically distributed devices communicate through a message broker. Latency of broker-less messaging is highly dependent on computational resources of devices. Deviceto-device messaging does not necessarily ensure low messaging latency and cannot scale well for a large number of resourcelimited and geographically distributed devices. For each device, FogMQ provides a high capacity device cloning service that subscribes to device messages. The clones facilitate near-theedge data analytics in resourceful cloud compute nodes. Clones in FogMQ apply Flock; an algorithm mimicking flocking-like behavior and allows the clones to autonomously migrate between heterogeneous cloud platforms. Flock controls and minimizes the weighted tail end-to-end latency. We have implemented FogMQ and evaluated it in a geographically distributed testbed. In our functional evaluation, we show that FogMQ is stable and achieves a bounded tail end-to-end latency that is up to 34% less than existing brokering methods.","2327-4662","","10.1109/JIOT.2018.2817392","NPRP grant from the Qatar National Research Fund (a member of Qatar Foundation)(grant numbers:NPRP 5-319-2-121); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8320280","Game theory;messages brokering;resource management;ubiquitous computing","Cloning;Cloud computing;Servers;Social network services;Internet of Things;Device-to-device communication;Delays","","11","","54","IEEE","20 Mar 2018","","","IEEE","IEEE Journals"
"Evaluation Framework for Electric Vehicle Security Risk Assessment","S. Shirvani; Y. Baseri; A. Ghorbani","Canadian Institute for Cybersecurity (CIC), University of New Brunswick (UNB), Fredericton, Canada; Canadian Institute for Cybersecurity (CIC), University of New Brunswick (UNB), Fredericton, Canada; Canadian Institute for Cybersecurity (CIC), University of New Brunswick (UNB), Fredericton, Canada",IEEE Transactions on Intelligent Transportation Systems,"17 Jan 2024","2024","25","1","33","56","Electric Vehicles (EVs) seem promising for future transportation to solve environmental concerns and energy management problems. According to Reuters, global car makers plan to invest over half a billion in more efficient and intelligent EVs and batteries. However, there are several challenges in EV mass production, including cybersecurity. Due to the cyber-physical nature of EVs and charging stations, their security and trustworthiness are ongoing challenges. In this study, we identify gaps in the security profiling of EVs and categorize them into five components: 1) charging station security, 2) information privacy, 3) software security, 4) connected vehicle security, and 5) autonomous driving security. Our study provides a comprehensive analysis of identified vulnerabilities, threats, challenges and attacks for different EV security aspects, along with their possible surface/subsurface and countermeasures. We develop a comprehensive security risk assessment framework by first using EV security profiles and mapping identified vulnerabilities to a well-known threat model, STRIDE. Then, we classify the risk levels associated with each vulnerability by setting ground criteria for the impact and likelihood of the threats. Finally, we validate our risk assessment framework by applying the same criteria to eight real-world EV attack scenarios. As a result, researchers can adapt the proposed risk assessment framework to discover threats and assess their risks in EVs and charging station ecosystems.","1558-0016","","10.1109/TITS.2023.3307660","NSERC through the Discovery Grant and Canada Research Chair; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247152","Security profiling;risk assessment;electric vehicle;autonomous vehicle;cybersecurity attacks","Security;Risk management;Privacy;Electric vehicles;Autonomous vehicles;Vehicle-to-everything;Transportation;Autonomous driving","","11","","299","IEEE","11 Sep 2023","","","IEEE","IEEE Journals"
"A Taxonomy of Quality Metrics for Cloud Services","X. Guerron; S. Abrahão; E. Insfran; M. Fernández-Diego; F. González-Ladrón-De-Guevara","Instituto Universitario Mixto de Tecnología Informática, Universitat Politècnica de València (UPV), Valencia, Spain; Instituto Universitario Mixto de Tecnología Informática, Universitat Politècnica de València (UPV), Valencia, Spain; Instituto Universitario Mixto de Tecnología Informática, Universitat Politècnica de València (UPV), Valencia, Spain; Departamento de Organización de Empresas, Universitat Politècnica de València (UPV), Valencia, Spain; Departamento de Organización de Empresas, Universitat Politècnica de València (UPV), Valencia, Spain",IEEE Access,"24 Jul 2020","2020","8","","131461","131498","A large number of metrics with which to assess the quality of cloud services have been proposed over the last years. However, this knowledge is still dispersed, and stakeholders have little or no guidance when choosing metrics that will be suitable to evaluate their cloud services. The objective of this paper is, therefore, to systematically identify, taxonomically classify, and compare existing quality of service (QoS) metrics in the cloud computing domain. We conducted a systematic literature review of 84 studies selected from a set of 4333 studies that were published from 2006 to November 2018. We specifically identified 470 metric operationalizations that were then classified using a taxonomy, which is also introduced in this paper. The data extracted from the metrics were subsequently analyzed using thematic analysis. The findings indicated that most metrics evaluate quality attributes related to performance efficiency (64%) and that there is a need for metrics that evaluate other characteristics, such as security and compatibility. The majority of the metrics are used during the Operation phase of the cloud services and are applied to the running service. Our results also revealed that metrics for cloud services are still in the early stages of maturity - only 10% of the metrics had been empirically validated. The proposed taxonomy can be used by practitioners as a guideline when specifying service level objectives or deciding which metric is best suited to the evaluation of their cloud services, and by researchers as a comprehensive quality framework in which to evaluate their approaches.","2169-3536","","10.1109/ACCESS.2020.3009079","Spanish Ministry of Science, Innovation and Universities through the Adapt@Cloud Project(grant numbers:TIN2017-84550-R); Universidad Central del Ecuador (UCE); Banco Central del Ecuador; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139920","Software quality;metrics;cloud services;systematic literature review","Measurement;Cloud computing;Taxonomy;Quality of service;Systematics;NIST;Elasticity","","11","","78","CCBY","14 Jul 2020","","","IEEE","IEEE Journals"
"Continuous Delivery of Customized SaaS Edge Applications in Highly Distributed IoT Systems","R. López-Viana; J. Díaz; V. H. Díaz; J. -F. Martínez","Universidad Politécnica de Madrid, Madrid, Spain; Universidad Politécnica de Madrid, Madrid, Spain; Universidad Politécnica de Madrid, Madrid, Spain; Universidad Politécnica de Madrid, Madrid, Spain",IEEE Internet of Things Journal,"9 Oct 2020","2020","7","10","10189","10199","Edge computing is a reality for the current IoT systems that need fast processing and quick response time to make real-time decisions and IoT systems without permanent connectivity to the cloud (e.g., car manufacturing, precision agriculture, or cattle raising). Additionally, these industries are facing the need for rapid and continuous innovation by accelerating the delivery of over-the-air (OTA) software updates in edge devices. DevOps promotes collaboration between development and operation teams and automation at all steps of software construction to achieve continuous delivery (CD) of business value. Although DevOps has demonstrated numerous successful cases in the Web domain, in the IoT domain and, more specifically, at the edge, there are few reported cases. This work presents a success case of CD of customized software as a service software as a service (SaaS) updates at the IoT Edge. This may enable new business models at the IoT Edge. This article presents an architectural model of a highly distributed (cloud and edge) IoT system and a CD process flow for customized SaaS applications in edge nodes. Both the architectural model and the CD process flow are instantiated in a case study for precision agriculture.","2327-4662","","10.1109/JIOT.2020.3009633","AFARCLOUD European Project (Aggregate Farming in the Cloud); ECSEL JU(grant numbers:783221-AFarCloud-H2020-ECSEL-2017-2); Spanish Ministry of Science, Innovation and Universities(grant numbers:PCI2018-092965); crowdsaving(grant numbers:TIN2016-79726-C2-1-R); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9142209","Continuous delivery (CD);DevOps;edge computing;IoT systems;software as a service (SaaS)","Cloud computing;Software as a service;Edge computing;Internet of Things;Business;Agriculture","","10","","25","IEEE","16 Jul 2020","","","IEEE","IEEE Journals"
"Encryption as a Service for IoT: Opportunities, Challenges, and Solutions","A. Javadpour; F. Ja’fari; T. Taleb; Y. Zhao; B. Yang; C. Benzaïd","Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, Finland; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Faculty of Information Technology and Electrical Engineering, Oulu University, Oulu, Finland; Science and Technology on Communication Security Laboratory, Chengdu, China; ICTFICIAL Oy, Espoo, Finland; Faculty of Information Technology and Electrical Engineering, Oulu University, Oulu, Finland",IEEE Internet of Things Journal,"21 Feb 2024","2024","11","5","7525","7558","The widespread adoption of Internet of Things (IoT) technology has introduced new cybersecurity challenges. Encryption services are being offloaded to cloud and fog platforms to mitigate these risks. Encryption as a Service (EaaS) emerges as a remedy, offering cryptographic solutions tailored to the resource constraints of IoT devices. This study thoroughly examines existing EaaS platforms, categorizing them based on encryption algorithms and service offerings. Additionally, we outline various EaaS architecture types depending on the placement of key components. Practical implementations of these platforms are explored through different testbeds. A key focus lies in dissecting the challenges that EaaS faces, particularly in the context of IoT, while suggesting potential remedies. This work stands out as an all-encompassing exploration, bridging the gap left by previous surveys.","2327-4662","","10.1109/JIOT.2023.3341875","European Union’s Horizon Europe Research and Innovation Program(grant numbers:HORIZON-JU-SNS-2022); RIGOUROUS Project(grant numbers:101095933); Research Council of Finland (former Academy of Finland) 6G Flagship Program(grant numbers:346208); National Science Foundation of China(grant numbers:U20B2049,U20B2048,U20B2046); National Natural Science Foundation of China(grant numbers:62372076); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10363120","Cloud computing;Encryption as a Service (EaaS);fog computing;Internet of Things (IoT)","Encryption;Internet of Things;Security;Cloud computing;Surveys;Heuristic algorithms;Performance evaluation","","10","","66","IEEE","15 Dec 2023","","","IEEE","IEEE Journals"
"Resource Provisioning Using Meta-Heuristic Methods for IoT Microservices With Mobility Management","S. M. Rajagopal; M. Supriya; R. Buyya","Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Bengaluru, India; Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Bengaluru, India; CLOUDS Laboratory, School of Computing and Information Systems, The University of Melbourne, Melbourne, VIC, Australia",IEEE Access,"22 Jun 2023","2023","11","","60915","60938","The fog and edge computing paradigm provide a distributed architecture of nodes with processing capability for smart healthcare systems driven by Internet of Thing (IoT) applications. It also provides a method to reduce big data transmissions that cause latency and enhance the system’s efficiency. Resource provisioning and scheduling in edge and fog systems is a significant problem due to heterogeneity and dispersion of edge/fog/cloud resources. The goal of scheduling is to map tasks to appropriate resources, which belong to NP-hard problems, and it takes much time to find an optimal solution. Meta-heuristic methods achieve near-optimal solutions within a reasonable time. Current edge/fog resource allocation research does not sufficiently address resource allocation problems in mobility-aware microservice-based IoT applications. This paper proposes a meta-heuristic-based micro-service resource provisioning model with mobility management for smart healthcare systems. The proposed approach has been tested on an experimental set-up with a simulation of a critical real-time smart healthcare application with and without considering the mobility of the devices. It applies meta-heuristic methods such as modified genetic and flower pollination algorithms for resource management. The proposed method outperforms the existing solutions in energy consumption, network usage, cost, execution time, and latency by 17%, 20%, 22%, 17%, and 63%, respectively.","2169-3536","","10.1109/ACCESS.2023.3281348","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10138589","Edge computing;fog computing;Internet of Things;meta-heuristic;microservices;mobility;smart healthcare;time critical applications","Edge computing;Internet of Things;Smart healthcare;Metaheuristics;Microservice architectures","","10","","60","CCBY","30 May 2023","","","IEEE","IEEE Journals"
"Towards Edge Computing as a Service: Dynamic Formation of the Micro Data-Centers","M. Simić; I. Prokić; J. Dedeić; G. Sladić; B. Milosavljević","Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia",IEEE Access,"23 Aug 2021","2021","9","","114468","114484","Edge computing brings cloud services closer to the edge of the network, where data originates, and dramatically reduces the network latency of the cloud. It is a bridge linking clouds and users making the foundation for novel interconnected applications. However, edge computing still faces many challenges like remote configuration, well-defined native applications model, and limited node capacity. It lacks geo-organization and a clear separation of concerns. As such edge computing is hard to be offered as a service for future real-time user-centric applications. This paper presents the dynamic organization of geo-distributed edge nodes into micro data-centers to cover any arbitrary area and expand capacity, availability, and reliability. A cloud organization is used as an influence with adaptations for a different environment, and a model for edge applications utilizing these adaptations is presented. It is argued that the presented model can be integrated into existing solutions or used as a base for the development of future systems. Furthermore, a clear separation of concerns is given for the proposed model. With the separation of concerns setup, edge-native applications model, and a unified node organization, we are moving towards the idea of edge computing as a service, like any other utility in cloud computing.","2169-3536","","10.1109/ACCESS.2021.3104475","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9512038","Cloud computing;distributed systems;edge computing;formal specifications;infrastructure as software;platform","Cloud computing;Computational modeling;Adaptation models;Peer-to-peer computing;Edge computing;Organizations;Servers","","9","","65","CCBY","12 Aug 2021","","","IEEE","IEEE Journals"
"Smart City Middleware: A Survey and a Conceptual Framework","C. Goumopoulos","Department of Information and Communications Systems Engineering, University of the Aegean, Samos, Greece",IEEE Access,"9 Jan 2024","2024","12","","4015","4047","Smart city middleware serves as a foundational tool in the evolution of urban digitalization, acting as an intermediary software layer that simplifies the development, deployment, and management of applications tailored for smart urban environments. However, the development of effective middleware for smart cities is challenging. The present research embarks on a comprehensive exploration of the smart city middleware landscape, unraveling the intricacies of its development and the challenges faced therein. Rooted in the assessment of 20 distinct middleware solutions, our study highlights the pivotal technologies, features and functionalities that are imperative for a middleware to effectively support a city’s digital transformation. The functional and non-functional requirements form the nucleus of our evaluation. We also explore the architectural styles pivotal to middleware development and the programming paradigms shaping smart city application development. Our study highlights challenges in using middleware for smart city applications, such as interoperability, scalability, security amidst big data, context management, reliability, quality of service, energy efficiency, and compliance with technological standards and regulations. Based on the detailed analysis, we propose a conceptual framework for smart city middleware, shaped by the challenges and requirements identified in existing literature and middleware solutions. This framework is designed to reflect the diverse demands and complexities of urban digital transformation, and guide smart city middleware development accordingly. As a result, this research stands as a reference study for software developers, urban planners, and researchers, outlining the current state and future directions in the domain of smart city middleware.","2169-3536","","10.1109/ACCESS.2023.3349376","University of the Aegean Research Unit through the “Internet of Things-Intelligent Environments in Next-Generation Networks” Project(grant numbers:70477); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10379798","Smart cities;middleware;conceptual framework;surveys;functional and non-functional requirements;enabling technologies;architectural styles;programming paradigms;challenges","Smart cities;Middleware;Software;Surveys;Interoperability;Security;Computer architecture","","9","","123","CCBY","3 Jan 2024","","","IEEE","IEEE Journals"
"ML4IoT: A Framework to Orchestrate Machine Learning Workflows on Internet of Things Data","J. M. Alves; L. M. Honório; M. A. M. Capretz","Department of Electrical and Computer Engineering, Western University, London, ON, Canada; Department of Electrical Energy, Federal University of Juiz de Fora, Juiz de Fora, Brazil; Department of Electrical and Computer Engineering, Western University, London, ON, Canada",IEEE Access,"28 Oct 2019","2019","7","","152953","152967","Internet of Things (IoT) applications generate vast amounts of real-time data. Temporal analysis of these data series to discover behavioural patterns may lead to qualified knowledge affecting a broad range of industries. Hence, the use of machine learning (ML) algorithms over IoT data has the potential to improve safety, economy, and performance in critical processes. However, creating ML workflows at scale is a challenging task that depends upon both production and specialized skills. Such tasks require investigation, understanding, selection, and implementation of specific ML workflows, which often lead to bottlenecks, production issues, and code management complexity and even then may not have a final desirable outcome. This paper proposes the Machine Learning Framework for IoT data (ML4IoT), which is designed to orchestrate ML workflows, particularly on large volumes of data series. The ML4IoT framework enables the implementation of several types of ML models, each one with a different workflow. These models can be easily configured and used through a simple pipeline. ML4IoT has been designed to use container-based components to enable training and deployment of various ML models in parallel. The results obtained suggest that the proposed framework can manage real-world IoT heterogeneous data by providing elasticity, robustness, and performance.","2169-3536","","10.1109/ACCESS.2019.2948160","Natural Sciences and Engineering Research Council of Canada(grant numbers:STPGP 506840-17); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8876834","Big data;container-based virtualization;IoT;machine learning;machine learning workflow;microservices","Machine learning;Internet of Things;Data models;Training;Big Data;Task analysis;Tools","","9","","41","CCBY","21 Oct 2019","","","IEEE","IEEE Journals"
"Toward Transportation Digital Twin Systems for Traffic Safety and Mobility: A Review","M. S. Irfan; S. Dasgupta; M. Rahman","Department of Civil, Construction, and Environmental Engineering, University of Alabama, Tuscaloosa, AL, USA; Department of Civil, Construction, and Environmental Engineering, University of Alabama, Tuscaloosa, AL, USA; Department of Civil, Construction, and Environmental Engineering, University of Alabama, Tuscaloosa, AL, USA",IEEE Internet of Things Journal,"8 Jul 2024","2024","11","14","24581","24603","Digital twin (DT) systems aim to create virtual replicas of physical objects that are updated in real-time with their physical counterparts and evolve alongside the physical assets throughout their lifecycle. DT technology promises to significantly enhance the capabilities of intelligent transportation systems (ITSs) through improving safety and mobility. The objectives of this review article are to: 1) conduct an in-depth review of DT technology applications in the transportation domain, i.e., safety and mobility applications; 2) develop a reference architecture for a transportation DT (TDT) system based on a hierarchical concept to guide in developing TDT systems at any scale; 3) identify research challenges for each component, i.e., the physical space, the communication gateway and the digital space of the TDT system, to realize safe and operationally efficient ITS; and 4) provide future research perspectives to develop and deploy networkwide transportation DT systems considering the scale and dynamic nature of future connected and automated transportation systems. Thus, this article focuses on providing a comprehensive understanding of the requirements, reference architecture, challenges, and future research opportunities for a TDT system, starting from individual transportation assets and building up to the entire networkwide TDT.","2327-4662","","10.1109/JIOT.2024.3395186","Alabama Transportation Institute (ATI) and Federal Motor Carrier Safety Administration (FMCSA)(grant numbers:25-1121-0006-004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10530969","Automation;connected transportation system;digital twin (DT);mobility;safety","Transportation;Digital twins;Safety;Reviews;Internet of Things;Predictive models;Surveys","","9","","147","IEEE","15 May 2024","","","IEEE","IEEE Journals"
"CLB: Coarse-Grained Precision Traffic-Aware Weighted Cost Multipath Load Balancing on PISA","D. D. Robin; J. I. Khan","Computer Science Department, Kent State University, Kent, OH, USA; Computer Science Department, Kent State University, Kent, OH, USA",IEEE Transactions on Network and Service Management,"9 Jun 2022","2022","19","2","784","803","This paper presents CLB, a programmable switch-based general-purpose in-network load balancer that can adapt to traffic changes at a very high speed. It uses Weighted-Cost Multipath (WCMP) mechanism for traffic-aware load balancing over many paths at a coarse-grained precision. CLB can be configured to match the load balancing requirements of a wide range of applications at line rate. We have analytically shown that CLB can achieve a bounded response time to traffic changes in the data plane. We implement CLB using the P4 programming language. Our experimental evaluation shows CLB can successfully distribute the incoming load over multiple paths for a given path-weight distribution and leads to performance improvement compared to other load balancing schemes.","1932-4537","","10.1109/TNSM.2022.3142106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9676704","Load-balancer;Programmable Switch;P4;SDN;BMV2;Traffic-aware;In-network","Load management;Switches;Servers;Throughput;Hardware;Costs;Random access memory","","9","","61","IEEE","11 Jan 2022","","","IEEE","IEEE Journals"
"A Comprehensive Review of Convolutional Neural Networks for Defect Detection in Industrial Applications","R. Khanam; M. Hussain; R. Hill; P. Allen","Department of Computer Science, School of Computing and Engineering, University of Huddersfield, Huddersfield, U.K.; Department of Computer Science, School of Computing and Engineering, University of Huddersfield, Huddersfield, U.K.; Department of Computer Science, School of Computing and Engineering, University of Huddersfield, Huddersfield, U.K.; Department of Computer Science, School of Computing and Engineering, University of Huddersfield, Huddersfield, U.K.",IEEE Access,"15 Jul 2024","2024","12","","94250","94295","Quality inspection and defect detection remain critical challenges across diverse industrial applications. Driven by advancements in Deep Learning, Convolutional Neural Networks (CNNs) have revolutionized Computer Vision, enabling breakthroughs in image analysis tasks like classification and object detection. CNNs’ feature learning and classification capabilities have made industrial defect detection through Machine Vision one of their most impactful applications. This article aims to showcase practical applications of CNN models for surface defect detection across various industrial scenarios, from pallet racks to display screens. The review explores object detection methodologies and suitable hardware platforms for deploying CNN-based architectures. The growing Industry 4.0 adoption necessitates enhancing quality inspection processes. The main results demonstrate CNNs’ efficacy in automating defect detection, achieving high accuracy and real-time performance across different surfaces. However, challenges like limited datasets, computational complexity, and domain-specific nuances require further research. Overall, this review acknowledges CNNs’ potential as a transformative technology for industrial vision applications, with practical implications ranging from quality control enhancement to cost reductions and process optimization.","2169-3536","","10.1109/ACCESS.2024.3425166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10589380","Computer vision;convolutional neural network;deep learning;industrial defect detection;object detection;quality inspection: manufacturing","Defect detection;Hardware;Convolutional neural networks;Computer architecture;Reviews;Artificial intelligence;Inspection;Computer vision;Deep learning;Quality assessment;Manufacturing processes","","9","","387","CCBYNCND","8 Jul 2024","","","IEEE","IEEE Journals"
"End-Edge-Cloud Collaborative Computing for Deep Learning: A Comprehensive Survey","Y. Wang; C. Yang; S. Lan; L. Zhu; Y. Zhang","School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; School of Economics and Management, University of the Chinese Academy of Sciences, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; Department of Informatics, University of Oslo, Oslo, Norway",IEEE Communications Surveys & Tutorials,"21 Nov 2024","2024","26","4","2647","2683","The booming development of deep learning applications and services heavily relies on large deep learning models and massive data in the cloud. However, cloud-based deep learning encounters challenges in meeting the application requirements of responsiveness, adaptability, and reliability. Edge-based and end-based deep learning enables rapid, near real-time analysis and response, but edge nodes and end devices usually have limited resources to support large models. This necessitates the integration of end, edge, and cloud computing technologies to combine their different advantages. Despite the existence of numerous studies on edge-cloud collaboration, a comprehensive survey for end-edge-cloud computing-enabled deep learning is needed to review the current status and point out future directions. Therefore, this paper: 1) analyzes the collaborative elements within the end-edge-cloud computing system for deep learning, and proposes collaborative training, inference, and updating methods and mechanisms for deep learning models under the end-edge-cloud collaboration framework. 2) provides a systematic investigation of the key enabling technologies for end-edge-cloud collaborative deep learning, including model compression, model partition, and knowledge transfer. 3) highlights six open issues to stimulate continuous research efforts in the field of end-edge-cloud deep learning.","1553-877X","","10.1109/COMST.2024.3393230","National Key Research and Development Program of China(grant numbers:2021YFB1715700); National Natural Science Foundation of China(grant numbers:62103046,72201266,72192843,72192844); Fundamental Research Funds for the Central Universities(grant numbers:E1E40805X2,2023CX01020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10508191","Deep learning;deep neural networks;edge computing;cloud computing;end-edge-cloud collaboration;end-edge-cloud computing","Computational modeling;Artificial intelligence;Collaboration;Surveys;Training;Cloud computing;Deep learning","","9","","252","IEEE","24 Apr 2024","","","IEEE","IEEE Journals"
"Open RAN xApps Design and Evaluation: Lessons Learnt and Identified Challenges","M. Hoffmann; S. Janji; A. Samorzewski; Ł. Kułacz; C. Adamczyk; M. Dryjański; P. Kryszkiewicz; A. Kliks; H. Bogucka","Rimedo Labs, Poznań, Poland; Rimedo Labs, Poznań, Poland; Rimedo Labs, Poznań, Poland; Rimedo Labs, Poznań, Poland; Institute of Radio-communications, Poznan University of Technology, Poznań, Poland; Rimedo Labs, Poznań, Poland; Rimedo Labs, Poznań, Poland; Rimedo Labs, Poznań, Poland; Rimedo Labs, Poznań, Poland",IEEE Journal on Selected Areas in Communications,"16 Jan 2024","2024","42","2","473","486","The concept of open radio access networks (RAN) creates numerous opportunities for developing new technology and economy branches. At the same time, a flexible and modular approach in the disaggregated RAN entails the need for careful design of the overall RAN architecture and the implementation and deployment process of new applications. It is assumed that dedicated and specialized software companies may deliver the latter. A joint effort must be guaranteed among different sectors (industry, academia, and standardization bodies) to make the whole process efficient, safe, and reliable. Here, one of the critical driving forces origins from the open-source community that often stimulates the development of a specific technology. In this paper, we address the challenges that have to be faced by third-party application developers in the context of Open RAN. Based on many implemented applications (called xApps or rApps), we compare various available solutions. We pose the most critical issues that must be tackled in the near future to stimulate the progress in open RAN development further. In particular, we compare available open platforms for xApp development and testing. We present the details of implementing four selected applications describing the problems encountered. The paper is split into two logical parts - first, we identify the key ambiguities related to the development of new xApps, which address more complicated use cases like beam management. In the second part, we present the challenges associated with detailed software implementation in existing open platforms. In the first case, we show that dedicated beam mobility management xApp can reduce beam switches and keep beam failures low. However, it requires access to detailed localization information. Similarly, the signaling storm detection xApp provides expected performance under the assumption that there is access to detailed information on, e.g., time advance resolution parameter. We conclude here that several aspects still need to be well-defined to allow smooth software implementation; these include the rules for data reporting in time, parameters available in service models, and localization features. Concerning the second logical part, related to low-level implementation, we compare the numerical results of the traffic steering and quality-of-service-based resource allocation xApps and draw conclusions related to implementation and testing. In particular, we point out problems associated with the simulator, the software, and conflicts inside. Finally, we identify the key challenges which should be treated as incentives for joint academia-industry cooperation in the field of Open RAN. Thus, the paper presents the lesson learned during the first years of xApp development.","1558-0008","","10.1109/JSAC.2023.3336190","National Centre for Research and Development in Poland within the 5GStar Project on “Advanced Methods and Techniques for Identification and Counteracting Cyberat-tacks on 5G Access Network and Applications(grant numbers:CYBERSECIDENT/487845/IV/NCBR/2021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10329915","Open RAN;5G;6G;xApp;ML","Computer architecture;Optimization;Radio access networks;Resource management;Security;5G mobile communication;6G mobile communication","","9","","43","CCBYNCND","28 Nov 2023","","","IEEE","IEEE Journals"
"Survey of Automated Fare Collection Solutions in Public Transportation","M. Bieler; A. Skretting; P. Büdinger; T. -M. Grønli","Mobile Technology Laboratory, School of Economics, Innovation and Technology, Kristiania University College, Oslo, Norway; Mobile Technology Laboratory, School of Economics, Innovation and Technology, Kristiania University College, Oslo, Norway; Mobile Technology Laboratory, School of Economics, Innovation and Technology, Kristiania University College, Oslo, Norway; Mobile Technology Laboratory, School of Economics, Innovation and Technology, Kristiania University College, Oslo, Norway",IEEE Transactions on Intelligent Transportation Systems,"14 Sep 2022","2022","23","9","14248","14266","Public transportation is expensive to operate and maintain and is often unsatisfactory. The attractiveness of public transportation can be enhanced by making it more seamless, which, in turn, would reduce financial constraints and inefficiencies. The adoption of mobile devices for ticketing solutions is promising. However, current solutions are often inflexible and require manual interactions that produce evanescent data. Therefore, using leading-edge technologies and infrastructure, it is desirable to develop a solution to fully automate fare collection. In this paper, we provide a comprehensive literature review to understand the state of public transportation and to facilitate the development and implementation of automated fare collection solutions. First, we discuss existing mobile technologies and their common ticketing implementations. Second, we provide a predictive behavior model with sensor analytics to better understand customer needs. Finally, we highlight how machine learning can harness transactional ticketing data to create valuable business intelligence. Overall, developing and implementing automated fare collection solutions in urban transportation is expected to have a significant positive impact on customer experiences, the emergence of new business models and the reduction of pollutant emissions.","1558-0016","","10.1109/TITS.2022.3161606","Pilot-T Program of the Norwegian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9756882","Behavior analysis;context awareness;intelligent sensors;intelligent transportation systems;machine learning;mobile applications","Public transportation;Machine learning;Mobile applications;Predictive models;Analytical models;Data models;Technological innovation","","9","","205","IEEE","13 Apr 2022","","","IEEE","IEEE Journals"
"Machine Learning for Service Migration: A Survey","N. Toumi; M. Bagaa; A. Ksentini","Communication Systems Department, EURECOM, Sophia-Antipolis, France; Department of Electrical and Computer Engineering, Université du Québec à Trois-Rivières, Trois-Rivières, Canada; Communication Systems Department, EURECOM, Sophia-Antipolis, France",IEEE Communications Surveys & Tutorials,"22 Aug 2023","2023","25","3","1991","2020","Future communication networks are envisioned to satisfy increasingly granular and dynamic requirements to accommodate the application and user demands. Indeed, novel immersive and mission-critical services necessitate increased computing and network resources, reduced communication latency, and guaranteed reliability. Thus, efficient and adaptive resource management schemes are required to provide and maintain sufficient levels of Quality of Experience (QoE) during the service life-cycle. Service migration is considered a key enabler of dynamic service orchestration. Indeed, moving services on demand is an efficient mechanism for user mobility support, load balancing in case of fluctuations in service demands, and hardware failure mitigation. However, service migration requires planning, as multiple parameters must be optimized to reduce service disruption to a minimum. Recent breakthroughs in computational capabilities allowed the emergence of Machine Learning as a tool for decision making that is expected to enable seamless automation of network resource management by predicting events and learning optimal decision policies. This paper surveys contributions applying Machine Learning (ML) methods to optimize service migration, providing a detailed literature review on recent advances in the field and establishing a classification of current research efforts with an analysis of their strengths and limitations. Finally, the paper provides insights on the main directions for future research.","1553-877X","","10.1109/COMST.2023.3273121","European Union’s H2020 5G!Drones Project(grant numbers:857031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10120920","Machine learning;service migration;management and orchestration;deep learning;reinforcement learning;prediction;placement","Surveys;Cloud computing;Planning;Optimization;Containers;Virtualization;Quality of experience","","9","","252","IEEE","8 May 2023","","","IEEE","IEEE Journals"
"Cloud-Native Computing: A Survey From the Perspective of Services","S. Deng; H. Zhao; B. Huang; C. Zhang; F. Chen; Y. Deng; J. Yin; S. Dustdar; A. Y. Zomaya","First Affiliated Hospital, Zhejiang University School of Medicine, Hangzhou, China; First Affiliated Hospital, Zhejiang University School of Medicine, Hangzhou, China; College of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Distributed Systems Group, Technische Universität Wien, Vienna, Austria; School of Computer Science, The University of Sydney, Sydney, NSW, Australia",Proceedings of the IEEE,"4 Mar 2024","2024","112","1","12","46","The development of cloud computing delivery models inspires the emergence of cloud-native computing. Cloud-native computing, as the most influential development principle for web applications, has already attracted increasingly more attention in both industry and academia. Despite the momentum in the cloud-native industrial community, a clear research roadmap on this topic is still missing. As a contribution to this knowledge, this article surveys key issues during the life cycle of cloud-native applications, from the perspective of services. Specifically, we elaborate on the research domains by decoupling the life cycle of cloud-native applications into four states: building, orchestration, operation, and maintenance. We also discuss the fundamental necessities and summarize the key performance metrics that play critical roles during the development and management of cloud-native applications. We highlight the key implications and limitations of existing works in each state. The challenges, future directions, and research opportunities are also discussed.","1558-2256","","10.1109/JPROC.2024.3353855","National Science Foundation of China(grant numbers:62125206,U20A20173,62202133); National Key Research and Development Program of China(grant numbers:2022YFB4500100); Key Research Project of Zhejiang Province(grant numbers:2022C01145); Zhejiang Provincial National Science Foundation of China(grant numbers:LY23F020015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10433234","Cloud-native applications;microservice;research roadmap;service life-cycle management;survey","Cloud computing;Microservice architectures;Surveys;Software engineering;Computer architecture;Service level agreements;Research initiatives;Life cycle assessment;Modeling","","9","","299","IEEE","12 Feb 2024","","","IEEE","IEEE Journals"
"Opportunistic Federation of CubeSat Constellations: A Game-Changing Paradigm Enabling Enhanced IoT Services in the Sky","G. Araniti; A. Iera; A. Molinaro; S. Pizzi; F. Rinaldi","Department DIIES, University Mediterranea, Reggio Calabria, Italy; National Inter-University Consortium for Telecommunications (CNIT), Napoli, Italy; Department DIIES, University Mediterranea, Reggio Calabria, Italy; Department DIIES, University Mediterranea, Reggio Calabria, Italy; Department DIIES, University Mediterranea, Reggio Calabria, Italy",IEEE Internet of Things Journal,"8 Aug 2022","2022","9","16","14876","14890","Internet of Space Things (IoST) is a challenging paradigm, which is currently attracting great interest from the scientific and industrial communities. IoST is based on the integration of the space segment into the global Internet-of-Things (IoT) infrastructure. In the relevant literature, reference is generally made to multiple constellations of nanosatellite platforms, used to enable IoT services on a global scale, including also disadvantaged and poorly infrastructured areas. In this article, we focus on multitenant IoT scenarios, wherein multiple CubeSats constellations are enabled to offer services by exploiting a dynamic federation model. The objective is to efficiently provide services in an IoST scenario by leveraging an effective cooperation strategy originally designed for terrestrial IoT networks, the Mobile-IoT-Federation-as-a-Service (MIFaaS) paradigm. We extend this vision to IoT satellite networks in order to allow a constellation of satellites to effectively execute tasks through a tight cooperative behavior with other CubeSats constellations. The reported performance evaluation studies show that better performance, in terms of percentage of tasks successfully completed, can be achieved through the implementation of the proposed cooperation paradigm.","2327-4662","","10.1109/JIOT.2021.3115160","RUDN University Strategic Academic Leadership Program and PM3 Project (Italian Ministry of University and Research PON Grant)(grant numbers:ARS01_01181); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9547348","CubeSats;federation;Internet of Space Things (IoST);Internet of Things (IoT);multitenancy;virtualization","Internet of Things;CubeSat;Satellite broadcasting;5G mobile communication;Space vehicles;Low earth orbit satellites;Orbits","","8","","61","IEEE","24 Sep 2021","","","IEEE","IEEE Journals"
"Toward Network-Slicing-Enabled Edge Computing: A Cloud-Native Approach for Slice Mobility","S. D. A. Shah; M. A. Gregory; S. Li","School of Engineering and Information Technology, The University of New South Wales Canberra, Australian Defence Force Academy, Canberra, ACT, Australia; School of Engineering, RMIT University, Melbourne, VIC, Australia; School of Engineering, RMIT University, Melbourne, VIC, Australia",IEEE Internet of Things Journal,"8 Jan 2024","2024","11","2","2684","2700","Network slicing is a key enabler for 5G and beyond networks that permits operators to provide scalable, flexible, and dedicated networks over a common physical infrastructure. To cope with the rising demand for ultrareliable and low-latency communication (URLLC) in beyond 5G networks, the provision of dedicated secure networks closer to the users is essential. Multiaccess edge computing (MEC) is a promising technology that provides data and computational resources closer to mobile users. However, MEC servers are resource-constrained, and offering dedicated service-specific network slices at the edge in a highly dynamic and mobile environment is challenging. Network slicing and MEC are being evolved by two different standardization bodies that limit their integration and raise mobility challenges that deserve more attention. We propose a cloud-native microservices architecture for network slice mobility management in MEC that permits each MEC slice to be distributed as stateless and independently deployable microservices. The proposal separates the MEC slice operational data and the user context, as each network function in a MEC slice stores the context in a separate shared database. The proposed architecture leverages new SDN extended federation modules in compliance with the ETSI requirements for inter-MEC system coordination. The federation modules support a more flexible and scalable creation of network slices at MEC servers, efficient resource utilization, and mobility of network slices across MEC servers. The simulation results show that our proposed architecture outperforms the existing SDN-based approaches for network slicing in MEC by achieving high slice acceptance rates and reduced slice migration delay.","2327-4662","","10.1109/JIOT.2023.3292520","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10173679","5G;cloud-native;microservices;multiaccess edge computing (MEC);network slice mobility (NSM);ultrareliable and low-latency communication (URLLC);vehicle-toeverything (V2X)","Network slicing;Servers;Microservice architectures;Cloud computing;Vehicle dynamics;Internet of Things;Vehicle-to-everything","","8","","50","IEEE","5 Jul 2023","","","IEEE","IEEE Journals"
"A Survey on Security, Privacy, Trust, and Architectural Challenges in IoT Systems","M. Adam; M. Hammoudeh; R. Alrawashdeh; B. Alsulaimy","Computer Engineering Department, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia; Information and Computer Science Department, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia; Information and Computer Science Department, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia; Information and Computer Science Department, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia",IEEE Access,"25 Apr 2024","2024","12","","57128","57149","The Internet of Things (IoT) emerged as a pervasive technology, facilitating the seamless interaction of devices, individuals, and services, enabling data exchange and task execution across various domains. While the impact of IoT is undeniably transformative, its extensive proliferation raised significant concerns surrounding security, privacy, and trust, which stand as critical barriers to the widespread adoption and advancement of IoT technology. This review article explores IoT security, privacy, and trust research using a 3-layer IoT architecture. After introducing the fundamental tenets of IoT security, privacy, and trust, it proceeds to examine the prevalent security requirements within IoT architectures and their associated challenges. Then, the survey investigates the recent trends in research dedicated to addressing security, privacy, and trust issues within IoT systems. Furthermore, this article reviews the latest advancements and methodologies designed to secure IoT systems against security breaches and protect the privacy of sensitive data. Finally, the survey outlines unresolved challenges within the IoT security landscape and potential solutions. By offering this consolidated insight, this article offers a bridge between foundational and advanced IoT security topics, providing researchers with an in-depth understanding of current IoT security, privacy, and trust challenges, as well as cutting-edge solutions tailored to address the security and trust-related obstacles faced by IoT applications. In addition, it provides the IoT community with the knowledge necessary to navigate the complex terrain of security, privacy, and trust in IoT systems.","2169-3536","","10.1109/ACCESS.2024.3382709","Deanship of Scientific Research (DSR) at the King Fahd University of Petroleum and Minerals; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10483048","Countermeasures;IDS;IoT architecture;IoT attacks;mitigation;privacy;security;security requirements;trust","Internet of Things;Security;Privacy;Surveys;Computer architecture;Reviews;Blockchains;Trust management","","8","","124","CCBY","28 Mar 2024","","","IEEE","IEEE Journals"
"X-MAN: A Non-Intrusive Power Manager for Energy-Adaptive Cloud-Native Network Functions","Z. Xiang; M. Höweler; D. You; M. Reisslein; F. H. P. Fitzek","Deutsche Telecom Chair of Communication Networks, Technische Universität Dresden, Dresden, Germany; Deutsche Telecom Chair of Communication Networks, Technische Universität Dresden, Dresden, Germany; Deutsche Telecom Chair of Communication Networks, Technische Universität Dresden, Dresden, Germany; School of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ, USA; Deutsche Telecom Chair of Communication Networks, Technische Universität Dresden, Dresden, Germany",IEEE Transactions on Network and Service Management,"9 Jun 2022","2022","19","2","1017","1035","Emerging microservices demand flexible low-latency processing of network functions in virtualized environments, e.g., as containerized network functions (CNFs). While ensuring highly responsive low-latency CNF processing, the computing environments should conserve energy to reduce costs. In this systems integration study, we develop and evaluate the novel XDP-Monitoring Energy-Adaptive Network Functions (X-MAN) framework for managing the CPU operational states (P-states) so as to reduce the power consumption while prioritizing low-latency service. Architecturally, X-MAN consists of lightweight traffic monitors that are attached to the virtual network interfaces in the kernel space for per-CNF traffic monitoring and a power manager in user space with a global view of the CNFs on a CPU core. Algorithmically, X-MAN monitors the CPU core utilization via hybrid simple and weighted moving average prediction fed by the traffic monitors and a power management based on step-based CPU core frequency (P-state) adjustments. We evaluate X-MAN through extensive measurements in a real physical testbed operating at up to 10 Gbps. We find that X-MAN incurs significantly shorter and more consistent monitoring latencies for the CPU utilization than a state-of-the-art CPU hardware counter approach. Also, X-MAN achieves more responsive CPU core frequency adjustments and more pronounced reductions of the CPU power consumption than a state-of-the-art code instrumentation approach. We make the X-MAN source code publicly available.","1932-4537","","10.1109/TNSM.2021.3126822","German Research Foundation (DFG, Deutsche Forschungsgemeinschaft) as part of Germany’s Excellence Strategy – EXC 2050/1 – Cluster of Excellence “Centre for Tactile Internet with Human-in-the-Loop” (CeTI) of Technische Universität Dresden(grant numbers:390696704); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9610120","Containerized network function (CNF);CPU P-state;express data path (XDP);load monitoring;microservice;network testbed;power management","Monitoring;Power system management;Codes;Central Processing Unit;Hardware;Instruments;Kernel","","8","","112","IEEE","10 Nov 2021","","","IEEE","IEEE Journals"
"Secure IoT Search Engine: Survey, Challenges Issues, Case Study, and Future Research Direction","W. G. Hatcher; C. Qian; F. Liang; W. Liao; E. P. Blasch; W. Yu","Department of Computer and Information Science, Towson University, Towson, MD, USA; Department of Computer and Information Science, Towson University, Towson, MD, USA; Department of Computer Science, Sam Huston State University, Huntsville, TX, USA; Department of Computer and Information Science, Towson University, Towson, MD, USA; Air Force Research Laboratory, Arlington, DC, USA; Department of Computer and Information Science, Towson University, Towson, MD, USA",IEEE Internet of Things Journal,"7 Sep 2022","2022","9","18","16807","16823","The Internet of Things (IoT) encompasses a near-incalculable collection of dispersed and embedded computing devices acting as sensors and actuators, generating data at an incredible scale. However, a lack of coherency and cross-compatibility in IoT deployments has lead to increasing redundancy and waste of resources. To combat this, various concepts have been proposed for an open IoT search engine (IoT-SE) that serves human and machine users. Invariably, the IoT-SE envisions distributed query retrieval to handle massive volumes of devices and data. Incorporating the massively heterogeneous protocols and properties of devices deployed, the search of such a system for timely and pertinent data is massively challenging, to provide useful knowledge and service for IoT systems. Moreover, enabling and maintaining security and privacy in an IoT-SE is likewise a prodigious task, as end users, IoT devices, and the search system itself, have different protocols and requirements. To this end, a study of security issues in IoT search is conducted to outline the challenges ahead, and a case study to resolve practical security vulnerabilities in an IoT-SE system is carried out. The pertinent issues of security in an IoT-SE system are reviewed. Particularly: 1) a taxonomy is detailed for IoT-SE security issues; 2) the vulnerabilities of machine learning (ML) models in the IoT-SE are considered; and 3) defensive mechanisms are presented for securing IoT Search. A case study is carried out to implement basic security features in the IoT search, addressing the risk of false queries through the design of ML-based solutions. Finally, a roadmap for future research is provided, including the security and privacy for IoT systems connected to the IoT-SE, distributed edge computing in IoT-SE, privacy-preserving data markets in IoT-SE, and distributed ML in IoT-SE.","2327-4662","","10.1109/JIOT.2022.3153171","Air Force Office of Scientific Research(grant numbers:FA9550-20-1-0418); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9718521","Internet of Things (IoT);knowledge and service;search engine;security","Security;Internet of Things;Logic gates;Search engines;Smart homes;Taxonomy;Real-time systems","","8","","94","IEEE","23 Feb 2022","","","IEEE","IEEE Journals"
"An End-to-End Implementation of a Service-Oriented Architecture for Data-Driven Smart Buildings","L. Chamari; E. Petrova; P. Pauwels","Department of the Built Environment, Eindhoven University of Technology, Eindhoven, The Netherlands; Department of the Built Environment, Eindhoven University of Technology, Eindhoven, The Netherlands; Department of the Built Environment, Eindhoven University of Technology, Eindhoven, The Netherlands",IEEE Access,"30 Oct 2023","2023","11","","117261","117281","Buildings connect with multiple information systems like Building Management Systems (BMS), Energy Management Systems (EMS), IoT devices, Building Information Models (BIM), the electricity grid, weather services, etc. Data-driven smart building software demands seamless integration of the above systems and their data. The lack of a system architecture with well-defined Application Programming Interfaces (APIs) poses a significant challenge for developing reusable, modular and scalable applications. This article presents a service-oriented system architecture designed with data-driven smart buildings in mind. The architecture relies on the Zachman framework and consists of seven service categories: 1) existing business applications, 2) new microservice-based applications, 3) databases, 4) integration software, 5) infrastructure services, 6) shared services, and 7) user interfaces. It closely resembles the MACH architectural principles: Microservices, API-first, Cloud-based components, and Headless principles. This architecture is implemented as a proof-of-concept, including three smart building applications. These include a Digital Twin application integrating sensor data with a BIM model, a web application merging real-time sensor data with semantic building graphs, and a data exploration tool using sensor data, the Brick ontology, and Grafana dashboards. Future implementations include real-time control applications such as Model Predictive Control (MPC). The proposed architecture and its implementations provide a blueprint for a reusable, modular, and scalable architecture in the smart building domain.","2169-3536","","10.1109/ACCESS.2023.3325767","Dutch Ministry of Economic Affairs and Climate Policy and the Ministry of the Interior and Kingdom Relations; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10287934","Building Information Modeling;data-driven buildings;data integration;IoT;linked building data;microservices;reference architecture;REST API;real-time data acquisition;solution architecture","Computer architecture;Architecture;Buildings;Smart buildings;Business;Data models;Systems architecture;Data integration;Internet of Things;Real-time systems","","8","","102","CCBY","19 Oct 2023","","","IEEE","IEEE Journals"
"Design Methodology for 6G End-to-End System: Hexa-X-II Perspective","S. Kerboeuf; P. Porambage; A. Jain; P. Rugeland; G. Wikström; M. Ericson; D. Thai Bui; A. Outtagarts; H. Karvonen; P. Alemany; R. Muñoz; R. Vilalta; P. Botsinis; A. Ramos; J. Castaneda Cisneros; M. Karaca; C. Karousatou; S. Barmpounakis; P. Demestichas; A. Zafeiropoulos; I. Tzanettis; S. Papavassiliou; P. G. Giardina; G. Landi; B. Han; A. Nimr; M. A. Uusitalo","Nokia Bell Labs, Nokia Networks France, Massy, France; VTT Technical Research Centre of Finland, Espoo, Finland; Nokia Bell Labs, Espoo, Finland; Ericsson Research, Stockholm, Sweden; Ericsson Research, Stockholm, Sweden; Ericsson Research, Stockholm, Sweden; Nokia Bell Labs, Nokia Networks France, Massy, France; Nokia Bell Labs, Nokia Networks France, Massy, France; Siemens Industry Software Oy, Siemens EDA, Oulu, Finland; Centre Tecnològic de Telecomunicacions de Catalunya (CTTC/CERCA), Barcelona, Spain; Centre Tecnològic de Telecomunicacions de Catalunya (CTTC/CERCA), Barcelona, Spain; Centre Tecnològic de Telecomunicacions de Catalunya (CTTC/CERCA), Barcelona, Spain; Apple Technology Engineering, Munich, Germany; ATOS-EVIDEN, Madrid, Spain; Ericsson Research, Stockholm, Sweden; Ericsson Research, Istanbul, Turkey; WINGS ICT Solutions, Athens, Greece; WINGS ICT Solutions, Athens, Greece; WINGS ICT Solutions, Athens, Greece; Institute of Communication and Computer Systems, Athens, Greece; Institute of Communication and Computer Systems, Athens, Greece; Institute of Communication and Computer Systems, Athens, Greece; Research & Developmen, Nextworks, Pisa, Italy; Research & Developmen, Nextworks, Pisa, Italy; Division of Wireless Communications and Radio Positioning (WICON), University of Kaiserslautern (RPTU), Kaiserslautern, Germany; Vodafone Chair for Mobile Communications Systems, Technische Universität Dresden, Dresden, Germany; Nokia Bell Labs, Espoo, Finland",IEEE Open Journal of the Communications Society,"11 Jun 2024","2024","5","","3368","3394","As the fifth generation (5G) mobile communication systems are commercially deployed, they bring new services, enhance user experiences, and offer various opportunities to different industries. Despite its advancements, 5G encounters several challenges. To tackle these issues, global industrial, academic, and standards organizations are actively researching on sixth generation (6G) wireless communication systems. 6G networks are envisioned as a transformative shift in the interactions between the physical, digital, and human realms, paving the way for a pervasive human-centered cyber-physical world. 6G is positioned to be a platform that offers communication and beyond communication services considering both performance and value centric technological development approaches. In this paper, we present the way forward towards the design of 6G endto-end (E2E) system as a consolidated view of leading industry stakeholders and academia in the European level 6G flagship project Hexa-X-II. We discuss the key challenges with 5G and how 6G is expected to tackle those by introducing new technological innovations and supporting novel use case requirements. We provide a comprehensive methodology for the design of a 6G E2E system including ten principles, a blueprint, and a structured design process. The architecture design principles prioritize environmental sustainability, digital inclusiveness, and trustworthiness, considering their impact on the 6G E2E system. The blueprint is described corresponding to the infrastructure, network centric application, and application layers, as well as the pervasive functionalities and the relevant technological innovations. Following the design principles and the system blueprint, the design process is demonstrated as two-way approaches (i.e., 1) key performance and value indicators based design process. 2) top-down versus bottom-up alignment process) through the lens of a collaborative robot use case. Through this use case, a special attention is given to the technological enablers that cover management and orchestration functionalities and their 6G enhancement to go beyond the limitations characterizing the previous network generation. In addition, virtual modelling aspects related to digital twining and simulations for 6G E2E system design are also discussed.","2644-125X","","10.1109/OJCOMS.2024.3398504","Hexa-X-II Project; Smart Networks and Services Joint Undertaking (SNS JU) through the European Union’s Horizon Europe Research and Innovation Programme(grant numbers:101095759); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10525242","Keywords6G;end-to-end;platform;system;blueprint;network architecture;design process;use case;key value indicators;key performance indicators;collaborative robots","6G mobile communication;5G mobile communication;Sustainable development;Industries;Ecosystems;Collaborative robots;Europe","","8","","103","CCBYNCND","8 May 2024","","","IEEE","IEEE Journals"
"Bayesian Inference-Assisted Machine Learning for Near Real-Time Jamming Detection and Classification in 5G New Radio (NR)","S. Jere; Y. Wang; I. Aryendu; S. Dayekh; L. Liu","Bradley Department of ECE, Wireless@Virgnia Tech, Virginia Tech, Blacksburg, VA, USA; School of Systems and Enterprises, Stevens Institute of Technology, Hoboken, NJ, USA; School of Systems and Enterprises, Stevens Institute of Technology, Hoboken, NJ, USA; 5G, Advanced Connectivity and Edge Cyber, Deloitte & Touche LLP, Dallas, TX, USA; Bradley Department of ECE, Wireless@Virgnia Tech, Virginia Tech, Blacksburg, VA, USA",IEEE Transactions on Wireless Communications,"11 Jul 2024","2024","23","7","7043","7059","The increased flexibility and density of spectrum access in 5G New Radio (NR) has made jamming detection and classification a critical research area. To detect coexisting jamming and subtle interference, we introduce a Bayesian Inference-assisted machine learning (ML) methodology. Our methodology uses cross-layer Key Performance Indicator data collected on a Non-Standalone (NSA) 5G NR testbed to leverage supervised learning models, and is further assessed, calibrated, and revealed using Bayesian Network Model (BNM)-based inference. The models can operate on both instantaneous and sequential time-series data samples, achieving an Area under Curve above 0.954 for instantaneous models and above 0.988 for sequential models including the echo state network (ESN) from the Reservoir Computing (RC) family, across various jamming scenarios. The 180 ms instantaneous detection time allows for continuous tracking of the dynamic jamming condition due to UE mobility. Our approach serves as a validation method and a resilience enhancement tool for ML-based jamming detection while also enabling root cause identification for observed performance degradation. The introduced BNM-based inference proof-of-concept is successful in addressing 72.2% of the erroneous predictions of the RC-based sequential detection model caused by insufficient training data samples, thereby demonstrating its near real-time applicability in 5G NR and Beyond-5G networks.","1558-2248","","10.1109/TWC.2023.3337058","Cyber 5G Strategic Growth Offering of Deloitte & Touche LLP, Commonwealth Cyber Initiative (CCI) Southwest Node, and US National Science Foundation(grant numbers:CCF-1937487); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10352089","Jamming;interference;network intrusion;5G NR;O-RAN;near real-time;machine learning;reservoir computing;Bayesian network model;causal analysis and inference","Jamming;5G mobile communication;Interference;Real-time systems;Bayes methods;Computational modeling;Wireless communication","","8","","53","IEEE","8 Dec 2023","","","IEEE","IEEE Journals"
"I2BN: Intelligent Intent Based Networks","P. Szilágyi","Nokia Bell Labs, Budapest, Hungary",Journal of ICT Standardization,"22 Sep 2023","2021","9","2","159","200","Intent based network management reduces the complexity of network programming from a growing set of deeply technical APIs to context-free high-level objectives that the network should autonomously achieve and keep. The practical implementation of an intent based network requires substantial automation technology embedded in the network. Automation should cover the entire lifecycle of intents, from their ingestion to fulfillment and assurance. This article investigates the feasibility of automatically assembling interworking implementation units into intent specific automation pipelines, where units are reusable self-learning closed loop micro-services with self-declared capabilities. Each closed loop may gain knowledge and respond to dynamically changing network conditions, thereby enabling network autonomy in reaching the declared intent objectives. The human-network intent interface for expressing intents is proposed to be based on the aggregation of the deployed network and service automation capabilities, rather than a formalism decoupled from the actual network implementation. This principle removes the ambiguity and compatibility gap between human intent definition and machine intent fulfillment, while retaining the flexibility and extendibility of the intents offered by any specific system via onboarding additional micro-services with novel capabilities. The concepts discussed by the article fit into the architecture and closed loop work items already defined by ETSI ZSM and provides considerations towards new areas such as intent driven autonomous networks and enablers for automation.","2246-0853","","10.13052/jicts2245-800X.926","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255479","Intent;I2BN;network and service automation;network intelligence;ZSM","Autonomous networks;Pipelines;Memory management;Standardization;Machine learning;Programming;Data models","","8","","24","","22 Sep 2023","","","River Publishers","River Publishers Journals"
"Knowledge-Driven Industrial Intelligent System: Concept, Reference Model, and Application Direction","P. Z. H. Sun; Y. Bao; X. Ming; T. Zhou","Department of Industrial Engineering, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Industrial Engineering, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Industrial Engineering, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Industrial Engineering, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China",IEEE Transactions on Computational Social Systems,"2 Aug 2023","2023","10","4","1465","1478","The application of automation technology and artificial intelligence technology has promoted the improvement of the business capabilities of enterprises in industrial scenarios. Compared with the improvement or innovation of the business process, in recent years, part of academic research and practical applications has also shifted their attention from a single point of business intelligence perspective to a comprehensive intelligent upgrade of the industrial system. To the best of our knowledge, however, there is little research on the concept and model of the industrial intelligent system (IIS). To make up for the lack, this article presents the concept and reference model of IIS by analyzing the intelligentization requirement of the industrial systems. Different from academic research on general intelligent system capabilities, the reference model given emphasizes factors that need to be considered when implementing IIS in the industry. By analyzing the reference model, knowledge as the core driving force of IIS is recognized. Then, the four main forms of knowledge in IIS, as well as the role and key technologies of knowledge in different stages of IIS, are discussed in detail. In addition, several important potential applications of IIS are pointed out in this article.","2329-924X","","10.1109/TCSS.2022.3188295","National Natural Science Foundation of China(grant numbers:71632008); National Natural Science Foundation of China through the Main Research Project on Machine Behavior and Human–Machine Collaborated Decision Making Methodology(grant numbers:72192820); Third Research Project on Human Behavior in Human–Machine Collaboration(grant numbers:72192822); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9829421","Industrial intelligent system (IIS);industry intelligentization;knowledge flow;reference model","Business;Information systems;Knowledge based systems;Knowledge acquisition;Intelligent systems;Industries;Decision making","","8","","152","IEEE","13 Jul 2022","","","IEEE","IEEE Journals"
"Edge-Based Runtime Verification for the Internet of Things","C. Tsigkanos; M. M. Bersani; P. A. Frangoudis; S. Dustdar","TU Wien, Vienna, Austria; Politecnico di Milano, Milano, Italy; TU Wien, Vienna, Austria; TU Wien, Vienna, Austria",IEEE Transactions on Services Computing,"6 Oct 2022","2022","15","5","2713","2727","Complex distributed systems such as the ones induced by Internet of Things (IoT) deployments, are expected to operate in compliance to their requirements. This can be checked by inspecting events flowing throughout the system, typically originating from end-devices and reflecting arbitrary actions, changes in state or sensing. Such events typically reflect the behavior of the overall IoT system – they may indicate executions which satisfy or violate its requirements. This article presents a service-based software architecture and technical framework supporting runtime verification for widely deployed, volatile IoT systems. At the lowest level, systems we consider are comprised of resource-constrained devices connected over wide area networks generating events. In our approach, monitors are deployed on edge components, receiving events originating from end-devices or other edge nodes. Temporal logic properties expressing desired requirements are then evaluated on each edge monitor in a runtime fashion. The system exhibits decentralization since evaluation occurs locally on edge nodes, and verdicts possibly affecting satisfaction of properties on other edge nodes are propagated accordingly. This reduces dependence on cloud infrastructures for IoT data collection and centralized processing. We illustrate how specification and runtime verification can be achieved in practice on a characteristic case study of smart parking. Finally, we demonstrate the feasibility of our design over a testbed instantiation, whereupon we evaluate performance and capacity limits of different hardware classes under monitoring workloads of varying intensity using state-of-the-art LPWAN technology.","1939-1374","","10.1109/TSC.2021.3074956","Austrian Science Foundation(grant numbers:M 2778-N); EDENSPACE; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9411895","","Monitoring;Sensors;Runtime;Computer architecture;Temperature sensors;Internet of Things;Software architecture","","7","","48","CCBY","23 Apr 2021","","","IEEE","IEEE Journals"
"Multisensory Metaverse-6G: A New Paradigm of Commerce and Education","M. H. ALSAMH; A. Hawbani; S. Kumar; S. Hamood Alsamhi","Department of Accounting, Faculty of Administrative Science, IBB University, Ibb, Yemen; School of Computer Science, Shenyang Aerospace University, Shenyang, China; Department of CSE, IIIT Naya Raipur, Atal Nagar-Nava Raipur, Chhattisgarh, India; Insight Centre for Data Analytics, University of Galway, Galway, Ireland",IEEE Access,"31 May 2024","2024","12","","75657","75677","Multisensory Metaverse applications ignite a revolution across industries, captivating audiences with immersive experiences. With the impending release of 6G technology, the stage is set for an extraordinary advancement toward an even higher level of engagement and immersion. This paper examines how combining multimodal Metaverse apps with 6G technology might change the game, with an emphasis on business and education. We examine how multisensory Metaverse-6G signifies and reshapes a paradigm change in digital connection and immersion in light of the impending advent of 6G networks, distinguished by ultra-high-speed connectivity, low latency, and immense interconnectedness. We investigate how immersive Metaverse experiences improve virtual purchasing, targeted marketing campaigns, and customer engagement from a business perspective. Within the field of education, we investigate how Metaverse technology might transform conventional pedagogical techniques through interactive learning, skill development, and distant collaboration. We aim to build a more connected, immersive, and inclusive digital world by imagining a day when multimodal Metaverse-6G experiences drive innovation across commerce and education. We foresee a future where Metaverse-6G convergence transforms communication and cooperation, and we also highlight future research areas and potential developments. By embracing innovation and tackling challenges, we can fully utilize this convergence to build a more inclusive, immersive, and networked digital future.","2169-3536","","10.1109/ACCESS.2024.3392838","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10506898","Multisensory;metaverse;extended reality;6G;immersive communication;holographic communication;metaverse-6G immersive experiences;connectivity;interaction;commerce;education","Metaverse;6G mobile communication;Surveys;Computer architecture;Collaboration;Technological innovation;Biological system modeling;Multisensory integration;Immersive experience;Education;Commerce and trade","","7","","107","CCBYNCND","23 Apr 2024","","","IEEE","IEEE Journals"
"Validation of Current O-RAN Technologies and Insights on the Future Evolution","Y. Huang; Q. Sun; N. Li; Z. Chen; J. Huang; H. Ding; C. -L. Ⅰ","China Mobile Research Institute, Beijing, China; Wireless and Terminal Technology Department, China Mobile Research Institute, Beijing, China; Wireless and Terminal Technology Department, China Mobile Research Institute, Beijing, China; Wireless and Terminal Technology Department, China Mobile Research Institute, Beijing, China; Wireless and Terminal Technology Department, China Mobile Research Institute, Beijing, China; China Mobile Research Institute, Beijing, China; China Mobile Research Institute, Beijing, China",IEEE Journal on Selected Areas in Communications,"16 Jan 2024","2024","42","2","487","505","Entering the 5G era, the mobile network operators (MNO) are facing greater challenges in providing services cost effectively than any other previous generations. The potential solutions to this are lying on the emerging trend of deep convergence of information technology (IT), communication technology (CT) and data technology (DT). In particular, the O-RAN technology, the representation of such ICDT convergence and proposed by the O-RAN ALLIANCE in 2018, is transforming Radio Access Networks towards a new paradigm featuring openness, cloudification and intelligence. O-RAN has gained huge attention from both industry and academia since its inception. In this paper, we presented the recent endeavors from China Mobile, including our deployment scenarios, various test results from open fronthaul, cloud platform to the intelligent controller. Our rich and comprehensive tests have demonstrated the viability and superiority of current O-RAN technologies. Furthermore, we also provide our deep thinking on the O-RAN future evolution in order to better serve the emerging applications such as Metaverse, cloud extended-reality (XR), extensive enterprise private 5G verticals and so on.","1558-0008","","10.1109/JSAC.2023.3336180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10330580","O-RAN;cloudification;RAN intelligent controller;prototype;field trial","Computer architecture;Hardware;5G mobile communication;Synchronization;Cloud computing;Testing;Microprocessors;Open RAN","","7","","47","IEEE","28 Nov 2023","","","IEEE","IEEE Journals"
"Visible Light Positioning as a Next-Generation Indoor Positioning Technology: A Tutorial","S. Bastiaens; M. Alijani; W. Joseph; D. Plets","Department of Information Technology, Research Group imec-WAVES, Ghent University, Ghent, Belgium; Department of Information Technology, Research Group imec-WAVES, Ghent University, Ghent, Belgium; Department of Information Technology, Research Group imec-WAVES, Ghent University, Ghent, Belgium; Department of Information Technology, Research Group imec-WAVES, Ghent University, Ghent, Belgium",IEEE Communications Surveys & Tutorials,"21 Nov 2024","2024","26","4","2867","2913","Indoor positioning technology has become undisputedly pervasive in modern society. It can be used for numerous applications in various environments, ranging from agriculture to smart industry. Among the different indoor positioning technologies, the development of the indoor visible light positioning (VLP) system has gained momentum recently thanks to its low cost and centimetre-order accuracy. Moreover, VLP is a promising candidate technology for integration with Light Fidelity (Li-Fi), which relies on the optical spectrum to build future wireless networks and plays a significant role in sixth-generation (6G) wireless networks. This tutorial investigates VLP as a novel positioning technology in depth. The inherent characteristics and fundamentals of this technology are extensively described with the components of a VLP system and possible localization approaches being discussed. Furthermore, a mathematical and technical discussion of a simple minimum retrofit of the single photodiode (PD) received signal strength (RSS) VLP systems is provided. Finally, the paper concludes with open challenges and a future roadmap for VLP.","1553-877X","","10.1109/COMST.2024.3372153","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10457033","Visible light positioning (VLP);indoor positioning systems (IPSs);tutorial","Tutorials;Global navigation satellite system;Surveys;IP networks;Lighting;Costs;Receivers","","7","","469","CCBY","1 Mar 2024","","","IEEE","IEEE Journals"
"PerfSim: A Performance Simulator for Cloud Native Microservice Chains","M. G. Khan; J. Taheri; A. Al-Dulaimy; A. Kassler","Department of Mathematics and Computer Science, Karlstad University, Karlstad, Sweden; Department of Mathematics and Computer Science, Karlstad University, Karlstad, Sweden; School of Innovation, Design and Engineering, Mälardalen University, Västerȧs, Sweden; Department of Mathematics and Computer Science, Karlstad University, Karlstad, Sweden",IEEE Transactions on Cloud Computing,"6 Jun 2023","2023","11","2","1395","1413","Cloud native computing paradigm allows microservice-based applications to take advantage of cloud infrastructure in a scalable, reusable, and interoperable way. However, in a cloud native system, the vast number of configuration parameters and highly granular resource allocation policies can significantly impact the performance and deployment cost. For understanding and analyzing these implications in an easy, quick, and cost-effective way, we present PerfSim, a discrete-event simulator for approximating and predicting the performance of cloud native service chains in user-defined scenarios. To this end, we proposed a systematic approach for modeling the performance of microservices endpoint functions by collecting and analyzing their performance and network traces. With a combination of the extracted models and user-defined scenarios, PerfSim can then simulate the performance behavior of all services over a given period and provide an approximation for system KPIs, such as requests’ average response time. Using the processing power of a single laptop, we evaluated both simulation accuracy and speed of PerfSim in 104 prevalent scenarios and compared the simulation results with the identical deployment in a real Kubernetes cluster. We achieved $\scriptstyle \mathtt {\sim }$∼81-99% simulation accuracy in approximating the average response time of incoming requests and $\scriptstyle \mathtt {\sim }$∼16-1200 times speed-up factor for the simulation.","2168-7161","","10.1109/TCC.2021.3135757","Knowledge Foundation of Sweden(grant numbers:20200067); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9652084","Performance simulator;performance modeling;cloud native computing;service chains;simulation platform","Cloud computing;Computational modeling;Microservice architectures;Resource management;Emulation;Containers;Testing","","7","","55","CCBY","15 Dec 2021","","","IEEE","IEEE Journals"
"FIWARE-Based Telemedicine Apps Modeling for Patients’ Data Management","X. Aizaga-Villon; K. Alarcon-Ballesteros; J. Cordova-Garcia; V. S. Padilla; W. Velasquez","Escuela Superior Politécnica del Litoral, Faculty of Electrical and Computer Engineering, ESPOL Polytechnic University, Guayaquil, Ecuador; Escuela Superior Politécnica del Litoral, Faculty of Electrical and Computer Engineering, ESPOL Polytechnic University, Guayaquil, Ecuador; Escuela Superior Politécnica del Litoral, Faculty of Electrical and Computer Engineering, ESPOL Polytechnic University, Guayaquil, Ecuador; Escuela Superior Politécnica del Litoral, Faculty of Electrical and Computer Engineering, ESPOL Polytechnic University, Guayaquil, Ecuador; Escuela Superior Politécnica del Litoral, Faculty of Electrical and Computer Engineering, ESPOL Polytechnic University, Guayaquil, Ecuador",IEEE Engineering Management Review,"26 Jul 2022","2022","50","2","173","188","Open-source technologies enable communication channels between web platforms and innovative architectures to provide reliable data distribution, in which healthcare applications can particularly benefit from them. This work presents a communication channel design to improve the user experience about telemedicine apps, especially when patients are in remote locations while assuring their information using an innovative approach. The general purpose is to avoid users having to physically go to medical facilities by the correct data management related to their appointments and medical history. By preventing the attendance to healthcare facilities, patients do not expose themselves unnecessarily to viruses and bacteria. Therefore, this research includes a data communications model based on the FIWARE platform and cloud technologies for reliable user medical information distribution. The prototype is developed based on open-source technologies and registered the evaluation of different performance metrics that included cases scenarios in which administrators of healthcare centers configured options according to the availability of assets and informatics resources. The results show the effectiveness of the communication model under realistic conditions for encouraging the acceptance of telemedicine alternatives, especially when patients and medical staff present limitations regarding mobility.","1937-4178","","10.1109/EMR.2022.3169991","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762509","Cloud system;distributed database;FIWARE;remote patient monitoring;telemedicine","Medical services;Telemedicine;Medical diagnostic imaging;Cloud computing;Computer architecture;Hospitals;Proposals","","7","","88","IEEE","25 Apr 2022","","","IEEE","IEEE Journals"
"A Survey of Edge Caching: Key Issues and Challenges","H. Li; M. Sun; F. Xia; X. Xu; M. Bilal","School of Computer Science, Nanjing University of Information Science and Technology, Nanjing, China; Shandong Provincial University Laboratory for Protected Horticulture, Weifang University of Science and Technology, Weifang, China; Reading Academy, Nanjing University of Information Science and Technology, Nanjing, China; School of Software, Nanjing University of Information Science and Technology, Nanjing, China; Department of Computer Engineering, Hankuk University of Foreign Studies, Yongin-si, Republic of Korea",Tsinghua Science and Technology,"4 Dec 2023","2024","29","3","818","842","With the rapid development of mobile communication technology and intelligent applications, the quantity of mobile devices and data traffic in networks have been growing exponentially, which poses a great burden to networks and brings huge challenge to servicing user demand. Edge caching, which utilizes the storage and computation resources of the edge to bring resources closer to end users, is a promising way to relieve network burden and enhance user experience. In this paper, we aim to survey the edge caching techniques from a comprehensive and systematic perspective. We first present an overview of edge caching, summarizing the three key issues regarding edge caching, i.e., where, what, and how to cache, and then introducing several significant caching metrics. We then carry out a detailed and in-depth elaboration on these three issues, which correspond to caching locations, caching objects, and caching strategies, respectively. In particular, we innovate on the issue “what to cache”, interpreting it as the classification of the “caching objects”, which can be further classified into content cache, data cache, and service cache. Finally, we discuss several open issues and challenges of edge caching to inspire future investigations in this research area.","1007-0214","","10.26599/TST.2023.9010051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10339809","edge caching;edge computing;caching location;caching object;caching strategy;5G network architecture;Internet of Things (IoT)","Surveys;Measurement;Systematics;5G mobile communication;User experience;Mobile handsets","","7","","134","","4 Dec 2023","","","TUP","TUP Journals"
"Toward a Live BBU Container Migration in Wireless Networks","E. Schiller; J. Ajayi; S. Weber; T. Braun; B. Stiller","Department of Informatics IfI, Communication Systems Group CSG, Universit&#x00E4;t Z&#x00FC;rich, Z&#x00FC;rich, Switzerland; Department of Computer Science INF, Communication and Distributed Systems Group CDS, Universit&#x00E4;t Bern, Bern, Switzerland; Faculty of Law IUS, Universit&#x00E4;t Z&#x00FC;rich, Z&#x00FC;rich, Switzerland; Department of Computer Science INF, Communication and Distributed Systems Group CDS, Universit&#x00E4;t Bern, Bern, Switzerland; Department of Informatics IfI, Communication Systems Group CSG, Universit&#x00E4;t Z&#x00FC;rich, Z&#x00FC;rich, Switzerland",IEEE Open Journal of the Communications Society,"28 Feb 2022","2022","3","","301","321","Cloud Radio Access Networks (Cloud-RANs) have recently emerged as a promising architecture to meet the increasing demands and expectations of future wireless networks. Such an architecture can enable dynamic and flexible network operations to address significant challenges, such as higher mobile traffic volumes and increasing network operation costs. However, the implementation of compute-intensive signal processing Network Functions (NFs) on the General Purpose Processors (General Purpose Processor) that are typically found in data centers could lead to performance complications, such as in the case of overloaded servers. There is therefore a need for methods that ensure the availability and continuity of critical wireless network functionality in such circumstances. Motivated by the goal of providing highly available and fault-tolerant functionality in Cloud-RAN-based networks, this paper proposes the design, specification, and implementation of live migration of containerized Baseband Units (BBUs) in two wireless network settings, namely Long Range Wide Area Network (LoRaWAN) and Long Term Evolution (LTE) networks. Driven by the requirements and critical challenges of live migration, the approach shows that in the case of LoRaWAN networks, the migration of BBUs is currently possible with relatively low downtimes to support network continuity. The analysis and comparison of the performance of functional splits and cell configurations in both networks were performed in terms of fronthaul throughput requirements. The results obtained from such an analysis can be used by both service providers and network operators in the deployment and optimization of Cloud-RANs services, in order to ensure network reliability and continuity in cloud environments.","2644-125X","","10.1109/OJCOMS.2022.3149965","University of Zürich, Switzerland; University of Bern; European Union H2020 Research and Innovation Program(grant numbers:830927); H2020 Concordia Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9709593","Cloud-RAN;live migration;LoRaWAN;LTE;network function virtualization (NFV)","Long Term Evolution;Baseband;Wireless networks;Computer architecture;Wide area networks;Cloud computing;Time-frequency analysis","","7","","54","CCBY","10 Feb 2022","","","IEEE","IEEE Journals"
"Network Slicing-Based Learning Techniques for IoV in 5G and Beyond Networks","W. Hamdi; C. Ksouri; H. Bulut; M. Mosbah","Department of Computer Engineering, Ege University, Izmir, Turkey; Automated and Connected Mobility, Vedecom Institute, Versailles, France; Department of Computer Engineering, Ege University, Izmir, Turkey; CNRS, Bordeaux INP, LaBRI, UMR 5800, Univ. Bordeaux, Talence, France",IEEE Communications Surveys & Tutorials,"22 Aug 2024","2024","26","3","1989","2047","The effects of transport development on people’s lives are diverse, ranging from economy to tourism, health care, etc. Great progress has been made in this area, which has led to the emergence of the Internet of Vehicles (IoV) concept. The main objective of this concept is to offer a safer and more comfortable travel experience through making available a vast array of applications, by relying on a range of communication technologies including the fifth-generation mobile networks. The proposed applications have personalized Quality of Service (QoS) requirements, which raise new challenging issues for the management and allocation of resources. Currently, this interest has been doubled with the start of the discussion of the sixth-generation mobile networks. In this context, Network Slicing (NS) was presented as one of the key technologies in the 5G architecture to address these challenges. In this article, we try to bring together the effects of NS implications in the Internet of Vehicles field and show the impact on transport development. We begin by reviewing the state of the art of NS in IoV in terms of architecture, types, life cycle, enabling technologies, network parts, and evolution within cellular networks. Then, we discuss the benefits brought by the use of NS in such a dynamic environment, along with the technical challenges. Moreover, we provide a comprehensive review of NS deploying various aspects of Learning Techniques for the Internet of Vehicles. Afterwards, we present Network Slicing utilization in different IoV application scenarios through different domains; terrestrial, aerial, and marine. In addition, we review Vehicle-to-Everything (V2X) datasets as well as existing implementation tools; besides presenting a concise summary of the Network Slicing-related projects that have an impact on IoV. Finally, in order to promote the deployment of Network Slicing in IoV, we provide some directions for future research work. We believe that the survey will be useful for researchers from academia and industry. First, to acquire a holistic vision regarding IoV-based NS realization and identify the challenges hindering it. Second, to understand the progression of IoV powered NS applications in the different fields (terrestrial, aerial, and marine). Finally, to determine the opportunities for using Machine Learning Techniques (MLT), in order to propose their own solutions to foster NS-IoV integration.","1553-877X","","10.1109/COMST.2024.3372083","French National Agency for Research under the Future Investments Program(grant numbers:ANR-20-IDES-0001); Industrial Research Chair “Mobility and Intelligent Transports” (Foundation of Bordeaux University, 2019-2023); European Union’s Horizon 2020 Research and Innovation Programme(grant numbers:951947 (5GMED)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10457570","Internet of Vehicles;network slicing;machine learning techniques","Surveys;Network slicing;5G mobile communication;Resource management;Quality of service;Reviews;Marine vehicles","","7","","407","IEEE","1 Mar 2024","","","IEEE","IEEE Journals"
"On the Feasibility of Using Hierarchical Task Networks and Network Functions Virtualization for Managing Software-Defined Networks","W. Villota; M. Gironza; A. Ordoñez; O. M. Caicedo Rendon","Instituto de Computação, Universidade Estadual de Campinas, Campinas, Brazil; Telematics Engineering Group, Universidad del Cauca, Popayán, Colombia; Intelligent Management Systems Group, Foundation University of Popayán, Popayán, Colombia; Telematics Engineering Group, Universidad del Cauca, Popayán, Colombia",IEEE Access,"30 Jul 2018","2018","6","","38026","38040","Management is an essential process to ensure the proper operation of computer networks. There are a lot of proposals to manage software-defined networks (SDN) from their application plane. However, such proposals share some shortcomings related to low automation of network management tasks, long time needed to handle network situations, and the lack of flexibility and workability. In this paper, we introduce JANO to overcome these shortcomings and investigate the feasibility of using automated planning and network functions' virtualization to manage SDN from a vertical management plane. JANO uses hierarchical task networks' planning to generate automatically plans that automate management tasks and reduce the time required by administrators to face network situations. Also, JANO performs an instantiation of the management orchestrator of network functions virtualization to provide flexibility and workability in the generation and execution of plans targeted to addressing network situations. We evaluate JANO in a proof of concept. The evaluation results corroborated that JANO is a feasible solution to manage SDN, since the planning time slightly and linearly increased with the number of primitive tasks that form a plan, the time-consuming needed by administrators to addressing a network situation was short, and the additional traffic was low.","2169-3536","","10.1109/ACCESS.2018.2852649","Universidad del Cauca; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8408461","Automated planning;hierarchical task network;network functions virtualization;software-defined networks","Task analysis;Planning;Proposals;Monitoring;Network function virtualization;Workability","","6","","54","OAPA","9 Jul 2018","","","IEEE","IEEE Journals"
"Application Areas of Information-Centric Networking: State-of-the-Art and Challenges","L. C. M. Hurali; A. P. Patil","Department of Computer Science and Engineering, Ramaiah Institute of Technology, Bengaluru, Karnataka, India; Department of Computer Science and Engineering, Ramaiah Institute of Technology, Bengaluru, Karnataka, India",IEEE Access,"29 Nov 2022","2022","10","","122431","122446","The Information-Centric Network (ICN) paradigm has gained popularity since its inception. The host-based IP networks were not primarily designed to handle scenarios that it is exposed to on the current Internet. In that direction lot of research has been happening to develop applications such as web applications, multimedia streaming, the Internet of Things, Wireless Sensor Networks and Vehicular networks. In addition, new ICN application areas, such as social networks, Industrial IoTs, etc., are emerging. This review investigates the possible application areas and their deficiencies evenly, broadly and at a certain level of depth with focus on security, scalability, IP interoperability, modularity and other application specific aspects. We discuss the current state-of-the-art in these ICN-based applications and the existing limitations. A comparative analysis of the literary works available is performed to understand the research gaps available, and a detailed discussion of the challenges in each area is provided. We conclude the review with future challenges in the application development with the ICN paradigm to reap its architectural benefits.","2169-3536","","10.1109/ACCESS.2022.3223667","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9956802","Content-centric networks;information-centric networks;named data networking;network security","Security;Routing;Internet;Information-centric networking;IP networks;Authentication;Registers","","6","","97","CCBY","21 Nov 2022","","","IEEE","IEEE Journals"
"Harnessing the Potential of Function-Reuse in Multimedia Cloud Systems","C. Denninnart; M. A. Salehi","Center of Advanced Computer Study, University of Louisiana at Lafayette, Lafayette, LA, USA; Center of Advanced Computer Study, University of Louisiana at Lafayette, Lafayette, LA, USA",IEEE Transactions on Parallel and Distributed Systems,"12 Aug 2021","2022","33","3","617","629","Cloud-based computing systems can get oversubscribed due to the budget constraints of their users or limitations in certain resource types. The oversubscription can, in turn, degrade the users perceived Quality of Service (QoS). The approach we investigate to mitigate both the oversubscription and the incurred cost is based on smart reusing of the computation needed to process the service requests (i.e., tasks). We propose a reusing paradigm for the tasks that are waiting for execution. This paradigm can be particularly impactful in serverless platforms where multiple users can request similar services simultaneously. Our motivation is a multimedia streaming engine that processes the media segments in an on-demand manner. We propose a mechanism to identify various types of “mergeable” tasks and aggregate them to improve the QoS and mitigate the incurred cost. We develop novel approaches to determine when and how to perform task aggregation such that the QoS of other tasks is not affected. Evaluation results show that the proposed mechanism can improve the QoS by significantly reducing the percentage of tasks missing their deadlines and reduce the overall time (and subsequently the incurred cost) of utilizing cloud services by more than 9 percent.","1558-2183","","10.1109/TPDS.2021.3097911","National Science Foundation(grant numbers:CNS-2007209,CNS-2047144); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9490337","Task aggregation;over-subscription;serverless computing;cloud computing;video stream processing","Streaming media;Task analysis;Cloud computing;Containers;Engines;Delays;Admission control","","6","","41","IEEE","19 Jul 2021","","","IEEE","IEEE Journals"
"Forging the Industrial Metaverse for Industry 5.0: Where Extended Reality, IIoT, Opportunistic Edge Computing, and Digital Twins Meet","T. M. Fernández-Caramés; P. Fraga-Lamas","Department of Computer Engineering, Faculty of Computer Science, A Coruña, Spain; Department of Computer Engineering, Faculty of Computer Science, A Coruña, Spain",IEEE Access,"19 Jul 2024","2024","12","","95778","95819","The Industrial Metaverse can benefit from the concepts fostered by Industry 5.0, since it implies making use of dynamic and up-to-date content, as well as fast human-to-machine interactions. To enable such enhancements, this article proposes the concept of Meta-Operator, which is essentially an industrial worker that follows the principles of Industry 5.0 and interacts with Industrial Metaverse applications and with his/her surroundings through advanced Extended Reality (XR) devices. In order to build the foundations of future Meta-Operators, this article provides a thorough description of the main technologies that support such a concept: the main components of the Industrial Metaverse, the latest XR technologies and accessories and the use of Opportunistic Edge Computing (OEC) communications (to detect and interact with the surrounding Internet of Things (IoT) and Industrial IoT (IIoT) devices). Moreover, this paper analyzes how to create the next generation of Industrial Metaverse applications based on the Industry 5.0 concepts, including the most relevant standardization initiatives, the integration of AR/MR devices with IoT/IIoT solutions, the development of advanced communications and software architectures and the creation of shared experiences and opportunistic collaborative protocols. Finally, this article provides an extensive list of potential Industry 5.0 applications for the Industrial Metaverse and analyzes thoroughly the main challenges and research lines. Thus, this article provides a holistic view and useful guidelines for the future developers and researchers that will create the next generation of applications for the Industrial Metaverse.","2169-3536","","10.1109/ACCESS.2024.3422109","PID2020-118857RA-100 (ORBALLO, Opportunistic Edge Computing Based on Mobile and Low-Power IoT Devices); MCIN/AEI/10.13039/501100011033; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10596039","Industrial metaverse;Industry 5.0;Meta-Operator;Augmented Reality;Mixed Reality;Opportunistic Edge Computing;Digital Twins;Metaverse;IIoT","Metaverse;Fifth Industrial Revolution;Fourth Industrial Revolution;Industries;Industrial Internet of Things;Extended reality;Object recognition;Mixed reality;Edge computing","","6","","276","CCBY","11 Jul 2024","","","IEEE","IEEE Journals"
"Network Selection Over 5G-Advanced Heterogeneous Networks Based on Federated Learning and Cooperative Game Theory","C. C. González; E. F. Pupo; E. Iradier; P. Angueira; M. Murroni; J. Montalban","Department of Electrical and Electronic Engineering (DIEE/UdR CNIT), University of Cagliari, Sardinia, Italy; Department of Electrical and Electronic Engineering (DIEE/UdR CNIT), University of Cagliari, Sardinia, Italy; Department of Computer Languages and System, University of the Basque Country (UPV/EHU), Bilbao, Spain; Department of Communications Engineering, University of the Basque Country (UPV/EHU), Bilbao, Spain; Department of Electrical and Electronic Engineering (DIEE/UdR CNIT), University of Cagliari, Sardinia, Italy; Department of Electronic Technology, University of the Basque Country (UPV/EHU), Bilbao, Spain",IEEE Transactions on Vehicular Technology,"15 Aug 2024","2024","73","8","11862","11877","5G-Advanced and Beyond claims a 3D ecosystem with cooperation between terrestrial and non-terrestrial networks to achieve seamless coverage, improve capacity, and enable advanced applications with strict quality of service (QoS) requirements. This complex environment requires a disaggregated Radio Access Network (RAN) deployment with open interfaces, such as the architecture promoted by the O-RAN Alliance. This architecture, supporting the slicing paradigm, is a prominent solution to guarantee dynamism and differentiated traffic management. Furthermore, intelligence is critical for future wireless networks to enable Machine Learning (ML)-based optimization for autonomous RANs, handling ultra-dense heterogeneous environments, and adapting to numerous scenarios. This paper presents an enhanced Dynamic Radio Access Network Selection (eDRANS) algorithm based on Federated Double Deep Q-Network (F-DDQN) and inserted in the novel O-RAN architecture. The proposal selects the most suitable base station (BS) to satisfy multiple service requests, optimizing QoS and slicing resource utilization. Moreover, the solution employs a Cooperative Game Theory (CGT) approach to manage resources in overload situations. This load-balancing process enables the acceptance of new clients without abruptly degrading the active users' perception. eDRANS is adapted to diverse network conditions, multiple service constraints, and several user types with different priorities and mobility behaviors. The proposal is validated through network-level simulations, recreating a heterogeneous environment composed of terrestrial-airborne nodes and using the Max-SINR criterion, a heuristic algorithm, and centralized and distributed ML solutions as benchmarks. Results show that eDRANS correctly learns during multiple trial-and-error interactions with the environment, fulfilling the Service Level Agreement (SLA) and maximizing user satisfaction.","1939-9359","","10.1109/TVT.2024.3373638","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10461128","5G-advanced and beyond;federated deep reinforcement learning;game theory;network slicing;QoS","Heuristic algorithms;Resource management;Quality of service;Vehicle dynamics;Proposals;Handover;Training","","6","","50","IEEE","6 Mar 2024","","","IEEE","IEEE Journals"
"Toward 6G Optical Fronthaul: A Survey on Enabling Technologies and Research Perspectives","A. Fayad; T. Cinkler; J. Rak","Department of Telecommunications and Media Informatics, Budapest University of Technology and Economics, Budapest, Hungary; Department of Telecommunications and Media Informatics, Budapest University of Technology and Economics, Budapest, Hungary; Department of Computer Communications, Gdańsk University of Technology, Gdańsk, Poland",IEEE Communications Surveys & Tutorials,"14 Feb 2025","2025","27","1","629","666","The anticipated launch of the Sixth Generation (6G) of mobile technology by 2030 will mark a significant milestone in the evolution of wireless communication, ushering in a new era with advancements in technology and applications. 6G is expected to deliver ultra-high data rates and almost instantaneous communications, with three-dimensional coverage for everything, everywhere, and at any time. In the 6G Radio Access Networks (RANs) architecture, the Fronthaul connects geographically distributed Remote Units (RUs) to Distributed/Digital Units (DUs) pool. Among all possible solutions for implementing 6G fronthaul, optical technologies will remain crucial in supporting the 6G fronthaul, as they offer high-speed, low-latency, and reliable transmission capabilities to meet the 6G strict requirements. This survey provides an explanation of the 5G and future 6G optical fronthaul concept and presents a comprehensive overview of the current state of the art and future research directions in 6G optical fronthaul, highlighting the key technologies and research perspectives fundamental in designing fronthaul networks for 5G and future 6G. Additionally, it examines the benefits and drawbacks of each optical technology and its potential applications in 6G fronthaul networks. This paper aims to serve as a comprehensive resource for researchers and industry professionals about the current state and future prospects of 6G optical fronthaul technologies, facilitating the development of robust and efficient wireless networks of the future.","1553-877X","","10.1109/COMST.2024.3408090","CHISTERA Sustainable and Adaptive Ultra-High-Capacity Micro Base Stations (SAMBAS) Grant funded by Research Foundation–Flanders (FWO), Agence nationale de la Recherche (ANR), National Research, Development and Innovation Office (NKFIH), and U.K. Research and Innovation (UKRI)(grant numbers:CHIST-ERA-20-SICT-003); Passau International Centre for Advanced Interdisciplinary Studies (PICAIS) of the University of Passau, Germany; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10546919","5G;6G;optical fronthaul;point-to-point (P2P);passive optical networks (PON);free space optics (FSO);radio access network (RAN);optical communication technologies","6G mobile communication;Optical fiber networks;Passive optical networks;Optical network units;Surveys;Optical fibers;5G mobile communication","","6","","279","IEEE","3 Jun 2024","","","IEEE","IEEE Journals"
"Blockchain and Reinforcement Neural Network for Trusted Cloud-Enabled IoT Network","J. K. Samriya; S. Kumar; M. Kumar; M. Xu; H. Wu; S. S. Gill","Department of CSE, Indian Institute of Information Technology Sonepat, Sonepat, India; Department of Computer Engineering and Applications, GLA University, Mathura, India; Department of IT, Dr. B. R. Ambedkar National Institute of Technology, Jalandhar, India; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Center for Applied Mathematics, Tianjin University, Tianjin, China; School of Electronic Engineering and Computer Science, Queen Mary University of London, London, U.K.",IEEE Transactions on Consumer Electronics,"29 Apr 2024","2024","70","1","2311","2322","The rapid integration of Internet of Things (IoT) services and applications across various sectors is primarily driven by their ability to process real-time data and create intelligent environments through artificial intelligence for service consumers. However, the security and privacy of data have emerged as significant threats to consumers within IoT networks. Issues such as node tampering, phishing attacks, malicious code injection, malware threats, and the potential for Denial of Service (DoS) attacks pose serious risks to the safety and confidentiality of information. To solve this problem, we propose an integrated autonomous IoT network within a cloud architecture, employing Blockchain technology to heighten network security. The primary goal of this approach is to establish a Heterogeneous Autonomous Network (HAN), wherein data is processed and transmitted through cloud architecture. This network is integrated with a Reinforced Neural Network (RNN) called ClouD_RNN, specifically designed to classify the data perceived and collected by sensors. Further, the collected data is continuously monitored by an autonomous network and classified for fault detection and malicious activity. In addition, network security is enhanced by the Blockchain Adaptive Windowing Meta Optimization Protocol (BAW_MOP). Extensive experimental results validate that our proposed approach significantly outperforms state-of-the-art approaches in terms of throughput, accuracy, end-to-end delay, data delivery ratio, network security, and energy efficiency.","1558-4127","","10.1109/TCE.2023.3347690","National Natural Science Foundation of China(grant numbers:62071327,62102408); Tianjin Municipal Science and Technology Program(grant numbers:22ZYYYJC00020); Shenzhen Science and Technology Program(grant numbers:RCBS20210609104609044); Chinese Academy of Sciences President’s International Fellowship Initiative(grant numbers:2023VTC0006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10375765","Internet of Things;reinforcement neural network;heterogeneous autonomous network;cloud computing;control system","Internet of Things;Cloud computing;Blockchains;Security;5G mobile communication;Network security;Deep learning;Quality of service","","6","","40","IEEE","28 Dec 2023","","","IEEE","IEEE Journals"
"Testing the Resilience of MEC-Based IoT Applications Against Resource Exhaustion Attacks","R. Pietrantuono; M. Ficco; F. Palmieri","Università degli Studi di Napoli Federico II, Naples, Italy; Department of Computer Science, University of Salerno, Fisciano, SA, Italy; Department of Computer Science, University of Salerno, Fisciano, SA, Italy",IEEE Transactions on Dependable and Secure Computing,"13 Mar 2024","2024","21","2","804","818","Multi-access Edge Computing (MEC) is an emerging computing model that provides the necessary on-demand resources and services to the edge of the network, ensuring powerful computing, storage capacity, mobility, location, and context awareness support to emerging Internet of Things (IoT) applications. Nonetheless, its complex hierarchical model introduces new architectural interdependencies, which can influence the resilience of IoT applications against cyber attacks. Although application resilience has been investigated in the context of cloud computing, existing studies are not directly applicable to such an extended edge-cloud paradigm. The use of different enabling technologies at the edge of the network, such as various wireless access technologies and virtualization, implies several threats and challenges that make the analysis and deployment of resilience mechanisms a technically challenging problem. In this article, we first present an overview of the threat model, describing the threats for the different layers of this paradigm. We then study the impact of resource-exhausting attacks – a particularly relevant class for this paradigm - on three different IoT applications exploiting the services offered by the MEC-based architecture. We adopt a testing-based methodology conceived to characterize the resilience of such applications under attack. A set of most important resilience-related indicators are also identified. The characterization's results are useful to support the analyst in planning proper protection means at individual architectural layers.","1941-0018","","10.1109/TDSC.2023.3263137","NRRP MUR program; EU - NGEU(grant numbers:PE00000014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10086638","Internet of Things;multi-access edge computing;resilience;threat model;resource-exhausting attacks","Internet of Things;Resilience;Security;Cloud computing;Computational modeling;Computer architecture;Virtualization","","6","","53","IEEE","29 Mar 2023","","","IEEE","IEEE Journals"
"Follow the User: A Framework for Dynamically Placing Content Using 5G-Enablers","D. Santos; R. Silva; D. Corujo; R. L. Aguiar; B. Parreira","Instituto de Telecomunicações and Universidade de Aveiro, Campus Universitário de Santiago, Aveiro, Portugal; Instituto de Telecomunicações and Universidade de Aveiro, Campus Universitário de Santiago, Aveiro, Portugal; Instituto de Telecomunicações and Universidade de Aveiro, Campus Universitário de Santiago, Aveiro, Portugal; Instituto de Telecomunicações and Universidade de Aveiro, Campus Universitário de Santiago, Aveiro, Portugal; R&D Department, Altran Portugal, Vila Nova de Gaia, Portugal",IEEE Access,"25 Jan 2021","2021","9","","14688","14709","This article presents a framework for improved and efficient video delivery in scenarios featuring users moving at high speed (e.g., trains), leveraging on dynamic Multi-access Edge Computing (MEC) enabled 5G network capabilities. The framework is location-aware and it allows the content to efficiently follow the users, conserving load usage on network and computational resources, by placing virtualized Content Delivery Network (vCDN) nodes at edge sites. The nodes are controlled by the framework's centralized control unit, which is able to dynamically and preemptively deploy virtulized resources, as the train moves. The framework is capable of segmenting video content and placing the specific portion of content that a user is likely to consume across a set of dynamically deployed vCDN nodes, associated to the coverage section the train is currently passing. A proof of concept was implemented and evaluated, where the benefits of using this framework are assessed. Results showed that the proposed system was able to reduce the load on the core network by 10.9 percent and maximize the cache hit ratio to a value of 99.8 percent.","2169-3536","","10.1109/ACCESS.2021.3051570","Fundação para a Ciência e Tecnologia (FCT) / Ministério para a Educação e Ciência through the National Funds(grant numbers:PTDC/EEI-TEL/30685/2017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324846","vCDN;5G;MEC;location-awareness;video delivery optimization;NFV;SDN","5G mobile communication;Peer-to-peer computing;Servers;Monitoring;Quality of experience;Content distribution networks;Optimization","","6","","64","CCBYNCND","14 Jan 2021","","","IEEE","IEEE Journals"
"Band-Area Resource Management Platform and Accelerated Particle Swarm Optimization Algorithm for Container Deployment in Internet-of-Things Cloud","M. Ouyang; J. Xi; W. Bai; K. Li","School of Software Engineering, South China University of Technology, Guangzhou, China; School of Software Engineering, South China University of Technology, Guangzhou, China; School of Computer Science, Zhaoqing University, Zhaoqing, China; Department of Computer Science, State University of New York, New Paltz, NY, USA",IEEE Access,"24 Aug 2022","2022","10","","86844","86863","The method of building and deploying applications through the combination of container virtualization technology and a microservices framework has been widely used in Internet-of-Things clouds. However, there are gaps and a lack of coordination mechanisms between the Internet-of-Things and cloud computing. This study constructs a resource management platform, which is based on application container virtualization technology and combined with the microservices framework. The platform provide a support environment for the construction and deployment of Internet-of-Things cloud applications. However, there is no unified specification for the microservices templates. Therefore, a new service model called tool service was designed. The invocation relationship between services is studied, and developers can combine services through the invocation relationship between services to form a service function chain. However, container-based service deployment remains an unresolved issue. The deployment method of a container involves the quality of service of end users and the profit of cloud providers. To balance the profits of both parties, it is necessary to minimize the service response time and improve the resource utilization of the cloud data center. To address this problem, an accelerated particle swarm optimization strategy is proposed to realize service deployment. Through the invocation relationship between services, the execution containers are aggregated, so as to reduce the service transmission overhead and improve resource utilization. Compared with the experimental results of existing deployment strategies, the proposed optimization strategy has significantly improved performance parameters such as service transmission overhead, container aggregation, and resource utilization.","2169-3536","","10.1109/ACCESS.2022.3198971","Science and Technology Plan Project of Guangdong Province, China(grant numbers:2014B010112007,2016B010124010); Guangdong Province Educational Science Planning Project(grant numbers:2019KTSCX199); Zhaoqing Science and Technology Special Fund Project(grant numbers:2020G1004); Zhaoqing University Science and Technology Projects(grant numbers:zlgc201933); Teaching Reform Project of University Public Computer Course of Guangdong Province(grant numbers:2021-GGJGJ-012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857907","Accelerated particle swarm optimization;cloud computing;container;Internet-of-things;microservices;multi-objective optimization","Containers;Cloud computing;Internet of Things;Microservice architectures;Resource management;Optimization;Virtualization;Particle swarm optimization","","6","","46","CCBY","16 Aug 2022","","","IEEE","IEEE Journals"
"A Comprehensive Survey on Software as a Service (SaaS) Transformation for the Automotive Systems","D. F. Blanco; F. Le Mouël; T. Lin; M. -P. Escudié","Univ Lyon, INSA Lyon, Inria, CITI, EA3720, Villeurbanne, France; Univ Lyon, INSA Lyon, Inria, CITI, EA3720, Villeurbanne, France; STELLANTIS, Velizy-Villacoublay, France; Univ Lyon, INSA Lyon, Institut Gaston Berger, Villeurbanne, France",IEEE Access,"25 Jul 2023","2023","11","","73688","73753","Over the last few decades, automotive embedded Information and Communication Technology (ICT) systems have been used to enhance vehicle performance and enrich peopleâ’s driving experience, increasing the panel of software features within them. However, even though until now automakers have kept up with the innovation pace in terms of the functionalities that have been offered to passengers, the majority of automakers’ efforts have concentrated on bringing in these new functionalities by adding an unceasingly larger set of ECUs. All of this has been done without evolving any of the embedded software architecture consequently, due to budgetary constraints, legislative limitations, retro-compatibility problems, and a lack of awareness of the trending IT innovation. This unbalanced progress has then led to a substantial increase in in-vehicle architectural complexity, which has become a major concern for automakers nowadays as it makes the vehicle repairing process more complex, decreases software traceability and clashes with the objective of having higher business flexibility, modularity, and dynamicity within the vehicles. In this paper, we are going to go through literature, both academic and industrial, and propose a comprehensive study into automotive system transformation. We begin by giving a detailed analysis of the causes of evolution under five axes - i.e., society, business, industry, application, and technical. Then, we discuss the convergence of cars and software life cycles and propose a three-layered analysis of automotive ICT systems consisting of architecture design, software pipelines, and run-time management. Finally, we are going to propose certain detailed guidelines on the evolution perspectives for automotive systems through deriving from the convergence of advances in IT, as well as current and future automotive environmental constraints.","2169-3536","","10.1109/ACCESS.2023.3294256","Stellantis under the collaborative framework OpenLab VAT@Lyon, involving STELLANTIS and CITI Laboratory (Association Nationale Recherche Technologie (ANRT)(grant numbers:n°2020/1415); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10177956","Automotive ICT system;software defined vehicles;system architecture;vehicle-to-cloud (V2C)/vehicle-to-anything (V2X);software pipelines;E/E architecture","Automotive engineering;Computer architecture;Business;Technological innovation;Pipelines;Convergence;Information and communication technology;Software defined networking;System analysis and design;Vehicle-to-everything;Cloud computing","","6","","710","CCBY","11 Jul 2023","","","IEEE","IEEE Journals"
"Applying a Consumer-Centric Framework for Trust Assessment of Cloud Computing Service Providers","A. Balcão-Filho; N. Ruiz; F. d. F. Rosa; R. Bonacin; M. Jino","The CTI Renato Archer, Campinas, Brazil; The CTI Renato Archer, Campinas, Brazil; The CTI Renato Archer, Campinas, Brazil; The CTI Renato Archer, Campinas, Brazil; UNICAMP, Campinas, Brazil",IEEE Transactions on Services Computing,"6 Feb 2023","2023","16","1","95","107","Cloud computing services consumers do not have enough reliable information about critical characteristics of their providers, such as performance, security, trust and privacy, compliance with laws and regulations, among others. Our proposal addresses these problems presenting a trust assessment framework that integrates three domains: Governance, Transparency, and Security Information. Our approach is consumer-centric and deals with trust aspects from the end-user’s perspective. We use Indicators to communicate the outcomes, which aim to represent the expression of cybersecurity, manageability, and transparency of services under assessment. This paper includes an implementation proposal, prototype, and proof of concept, in which the framework was applied in a real scenario and executed over a long-term (18 months) usage simulation to verify its applicability, sensitivity, and robustness. Our study is intended for use by consumers of cloud computing who seek to know and measure levels of cybersecurity, protection of privacy, transparency of security, and high levels of quality in their services and infrastructure.","1939-1374","","10.1109/TSC.2021.3134125","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645168","Cloud computing;trust assessment;metrics;consumer-centric;security;privacy;framework","Cloud computing;Security;Privacy;Computational modeling;Measurement;Bibliographies;Taxonomy","","5","","53","IEEE","9 Dec 2021","","","IEEE","IEEE Journals"
"From 5G-Advanced to 6G in 2030: New Services, 3GPP Advances, and Enabling Technologies","R. Giuliano","Department of Engineering Science, Guglielmo Marconi University, Rome, Italy",IEEE Access,"8 May 2024","2024","12","","63238","63270","The telecommunications systems are in continuous evolution. After voice, video, mobile internet, and Internet of Things, what services will be supported in the near future? In the paper, three envisioned services are highlighted, which will be provided in the coming years by new telecommunication systems: immersive communications, everything connected, and high-positioning. The author provides a comprehensive description of their characteristics and investigates the developments that will be implemented in 3GPP Releases 17, Release 18, and Release 19, including technologies that could be integrated for supporting the three new services. In order to evaluate the performance of the new technologies and services, it is important to define appropriate Key Performance Indicators (KPIs). The paper reports and proposes new KPIs for network evaluation to support specific new services such as virtual/mixed reality, smart sensors, and gesture recognition, then facilitating the effective design of the next-generation network and its performance assessment optimally. Requirements of the major application fields that will see widespread adoption in the next 3–8 years due to these developments are also investigated. Finally, the paper further outlines the most promising enabling technologies, supporting the three bearer services.","2169-3536","","10.1109/ACCESS.2024.3396361","Guglielmo Marconi University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10517597","5G-advanced;6G;immersive communications;everything connected;high-positioning;3GPP enhancements;Release 18;Release 19","Telecommunications;6G mobile communication;3GPP;5G mobile communication;X reality;Smart phones;Next generation networking","","5","","174","CCBYNCND","2 May 2024","","","IEEE","IEEE Journals"
"Digital Twins for Smart Spaces—Beyond IoT Analytics","N. Hossein Motlagh; M. A. Zaidan; L. Lovén; P. L. Fung; T. Hänninen; R. Morabito; P. Nurmi; S. Tarkoma","Department of Computer Science, University of Helsinki, Helsinki, Finland; Department of Computer Science, University of Helsinki, Helsinki, Finland; Center for Ubiquitous Computing, University of Oulu, Oulu, Finland; Department of Computer Science, University of Helsinki, Helsinki, Finland; Centre for Wireless Communications, University of Oulu, Oulu, Finland; Department of Computer Science, University of Helsinki, Helsinki, Finland; Department of Computer Science, University of Helsinki, Helsinki, Finland; Department of Computer Science, University of Helsinki, Helsinki, Finland",IEEE Internet of Things Journal,"27 Dec 2023","2024","11","1","573","583","Smart spaces, physical spaces that are integrated with sensor-enabled Internet of Things devices, are a powerful paradigm for optimizing the operations of the space and improving its quality for the occupants. Managing the applications and services running in the space is a complex task as the operations of the devices and services are dependent on the physical characteristics of the space, the occupants of the space, and the technologies that are being integrated. Digital twinning, the combination of physical representations with a virtual counterpart, is a potential technology for facilitating the management of smart space devices and services. While digital twins are increasingly adopted in industry, their use in everyday environments remains low due to difficulties in creating and linking the virtual representation with the physical environment. In this article, we propose our vision for the adoption of digital twinning as a pathway to improve the functions of smart spaces. We derive a generic reference architecture that comprises four layers, covering the physical space, the sensing infrastructure, the network interfaces, and the underlying computational infrastructure. Next, we identify and address key requirements for the uptake of digital twins in smart spaces and assess their benefits using the ascendancy model of business analytics. Finally, to demonstrate the practicality of digital twinning, we present a proof-of-concept digital twin for the TellUs smart space at the University of Oulu in Finland and use it to highlight the potential benefits of different ascendancy levels.","2327-4662","","10.1109/JIOT.2023.3287032","Nokia Center for Advanced Research (NCAR); Business Finland Project(grant numbers:8782/31/2022,8754/31/2022); Academy of Finland(grant numbers:335934,345008,339614,355330,346208); Helsinki Institute for Information Technology (HIIT)(grant numbers:75233229); European Union’s Horizon 2020 Research and Innovation Programme the EMME-CARE Project(grant numbers:856612); ECSEL JU FRACTAL Project(grant numbers:877056); Finland Centennial Foundation to Urban Air Quality 2.0 Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10153976","Digital twin;Internet of Things (IoT);long range (LoRa) networks;sensor networks;smart spaces","Sensors;Intelligent sensors;Smart spaces;Digital twins;Wireless communication;Internet of Things;Wireless sensor networks","","5","","26","CCBY","16 Jun 2023","","","IEEE","IEEE Journals"
"Metarobotics for Industry and Society: Vision, Technologies, and Opportunities","E. G. Kaigom","Faculty of Computer Science and Engineering, Frankfurt University of Applied Sciences, Frankfurt, Germany",IEEE Transactions on Industrial Informatics,"4 Apr 2024","2024","20","4","5725","5736","Metarobotics aims to combine next generation wireless communication, multisense immersion, and collective intelligence to provide a pervasive, itinerant, and noninvasive access and interaction with distant robotized applications. Industry and society are expected to benefit from these functionalities. For instance, robot programmers will no longer travel worldwide to plan and test robot motions, even collaboratively. Instead, they will have a personalized access to robots and their environments from anywhere, thus spending more time with family and friends. Students enrolled in robotics courses will be taught under authentic industrial conditions in real-time. This article describes objectives of metarobotics in society, industry, and in-between. It identifies and surveys technologies likely to enable their completion and provides an architecture to put forward the interplay of key components of metarobotics. Potentials for self-determination, self-efficacy, and work-life-flexibility in robotics-related applications in Society 5.0, Industry 4.0, and Industry 5.0 are outlined.","1941-0050","","10.1109/TII.2023.3337380","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10361602","6G;collective intelligence;digital twins (DT);holoportation;Industry 4.0/5.0;metaverse;robotics;Society 5.0","Service robots;Industries;Metaverse;Surveys;Robot sensing systems;Collaboration;6G mobile communication;Fifth Industrial Revolution","","5","","80","IEEE","15 Dec 2023","","","IEEE","IEEE Journals"
"Anomaly Detection in Social-Aware IoT Networks","J. Tang; T. Qin; D. Kong; Z. Zhou; X. Li; Y. Wu; J. Gu","School of Artificial Intelligence, Hebei University of Technology, Tianjin, China; School of Artificial Intelligence, Hebei University of Technology, Tianjin, China; School of Artificial Intelligence, Hebei University of Technology, Tianjin, China; School of Information Engineering, China University of Geosciences, Beijing, China; School of Information Engineering, China University of Geosciences, Beijing, China; School of Information Science and Technology, Jinan University, Guangzhou, China; School of Artificial Intelligence and the Hebei Province Key Laboratory of Big Data Calculation, Hebei University of Technology, Tianjin, China",IEEE Transactions on Network and Service Management,"9 Oct 2023","2023","20","3","3162","3176","Anomaly event coverage is usually related to several attributes, among which the primary attribute dominates at the time of improving detection efficiency. In the case of Internet of Things (IoT) devices with complex social-aware relationships, IoT nodes with primary attributes should cooperate with each other through their social-aware interactions, to detect potential event anomalies and further determine the coverage of such anomalies. Existing research has put a lot of effort into designing IoT detection frameworks to discover anomalous sensor data, rarely caring about the social-aware interactions. This paper targets this important efficiency problem, and develops a novel anomaly detection mechanism in collaborative social-edge-cloud architecture. The focus of it is to first construct a vector space based Aggregation Behavior Comparison Detection Model, and quantify the change of monitoring behavior by defining the clustering threshold of vector space. This can quickly judge whether a local social network is abnormal and speed up the abnormal detection rate. If it is, a Social Behavior Correlation Detection Model is further designed based on the correlation of primary attributes derived from the dominating social-aware interaction behavior captured by (primary) edge nodes. This strategy can help detect specific “abnormal” areas managed by one or more edge devices with higher accuracy. In the process of anomaly detection, we also propose a spatial index tree to store the information of IoT nodes, so as to effectively collect and route the perceived data of IoT nodes for anomaly analysis. Experimental results demonstrate that our anomaly detection method promotes the detection efficiency and accuracy in comparison with the state of art’s techniques.","1932-4537","","10.1109/TNSM.2023.3242320","Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2019B1515120010); National Natural Science Foundation of China(grant numbers:61932011); Guangdong KeyR&D Plan2020(grant numbers:2020B0101090002); National KeyR&D Plan2020(grant numbers:2020YFB1005600); Open Project Program of Tianjin Key Laboratory of Aerospace Intelligent Equipment Technology, Tianjin Institute of Aerospace Mechanical and Electrical Equipment(grant numbers:ZNZB-2021-01); Supplementary Funds for Performance of Hebei Province Key Laboratory of Big Data Calculation(grant numbers:22567603H); National Natural Science Foundation of China(grant numbers:61702232); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10037228","Anomaly detection;Internet of Things;aggregation behavior comparison detection;social behavior correlation detection;spatial index tree","Internet of Things;Image edge detection;Anomaly detection;Behavioral sciences;Cloud computing;Monitoring;Indexes","","5","","43","IEEE","6 Feb 2023","","","IEEE","IEEE Journals"
"Next Generation Edge-Cloud Continuum Architecture for Structural Health Monitoring","L. Gigli; I. Zyrianoff; F. Zonzini; D. Bogomolov; N. Testoni; M. D. Felice; L. De Marchi; G. Augugliaro; C. Mennuti; A. Marzani","Advanced Research Center on Electronic Systems for Information and Communication Technologies “Ercole De Castro, University of Bologna, Bologna, Italy; Advanced Research Center on Electronic Systems for Information and Communication Technologies “Ercole De Castro, University of Bologna, Bologna, Italy; Advanced Research Center on Electronic Systems for Information and Communication Technologies “Ercole De Castro, University of Bologna, Bologna, Italy; Advanced Research Center on Electronic Systems for Information and Communication Technologies “Ercole De Castro, University of Bologna, Bologna, Italy; Advanced Research Center on Electronic Systems for Information and Communication Technologies “Ercole De Castro, University of Bologna, Bologna, Italy; Advanced Research Center on Electronic Systems for Information and Communication Technologies “Ercole De Castro, University of Bologna, Bologna, Italy; Advanced Research Center on Electronic Systems for Information and Communication Technologies “Ercole De Castro, University of Bologna, Bologna, Italy; INAIL Technological Innovations Department, Rome, Italy; INAIL Technological Innovations Department, Rome, Italy; Department of Civil, Chemical, Environmental, and Materials Engineering, University of Bologna, Bologna, Italy",IEEE Transactions on Industrial Informatics,"4 Apr 2024","2024","20","4","5874","5887","Assessing the integrity of industrial and civil appliances has become a priority worldwide. Noteworthy, this goal requires a strong synergy between multiple tools, disciplines, and approaches to be attained via a joint hardware-software co-design of the different Structural Health Monitoring (SHM) system components. This work proposes the $\sf{MAC4PRO}$ architecture, a sensor-to-cloud monitoring platform that seamlessly integrates sensing and software technologies for accurate data measurement, transmission, and analysis. The developed solution stands out for its interoperability and versatility, making it a promising candidate for integration in the next generation of smart structures. Our platform was validated during extensive experimental campaigns targeted at various industrial scenarios. The results show that the $\sf{MAC4PRO}$ architecture can identify subtle changes, such as 1mm size leakage events in pipeline circuits, or less than 1% frequency drifts in civil buildings after seismic excitation, while ensuring more than 90% reduction in the edge-to-cloud data transfer process.","1941-0050","","10.1109/TII.2023.3337391","Istituto Nazionale per l'Assicurazione Contro Gli Infortuni sul Lavoro; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10373409","Edge-cloud continuum;Internet of Things;interoperability;structural health monitoring","Computer architecture;Monitoring;Sensors;Cloud computing;Software;Vibrations;Next generation networking;Edge computing;Internet of Things;Interoperability;Medical services;Biomedical monitoring","","5","","45","CCBYNCND","25 Dec 2023","","","IEEE","IEEE Journals"
"Scheduling and Predictive Maintenance for Smart Toilet","A. Lokman; R. K. Ramasamy; C. -Y. Ting","Faculty of Computing and Informatics, Multimedia University, Cyberjaya, Malaysia; Faculty of Computing and Informatics, Multimedia University, Cyberjaya, Malaysia; Faculty of Computing and Informatics, Multimedia University, Cyberjaya, Malaysia",IEEE Access,"24 Feb 2023","2023","11","","17983","17999","Modern society needs bathrooms. Poor sanitation is caused by worn-out appliances and expensive cleaning. The technique also requires an inexpensive, dependable sensor. This study had three goals. Creating an IoT administration platform is the main goal. Literature evaluations assess the merits and downsides of existing systems. Second, we suggest predictive maintenance to assist predict bathroom equipment breakdowns. Finally, a scheduling algorithm was used to determine how many janitors to hire. We’ll measure the model’s effectiveness and make future recommendations. Infrared, temperature and humidity sensors create an IoT bathroom. Sensors have been studied to understand how to adapt them to the hygienic and private toilet environment. Sensor accuracy and cost-effectiveness could be enhanced with more development and testing. The Auto-Regressive Integrated Moving Average (ARIMA) model accurately predicts time series lags, making it a good candidate for predictive maintenance. Long Short-Term Memory (LSTM) is good in time series predictions, therefore it’s fair to compare the two. We use the ARIMA model to handle Remaining Useful Life (RUL) prediction techniques by altering Moving Average (MA) and Auto-Regressive (AR). A genetic algorithm is used to create a janitorial cleaning schedule. The genetic algorithm was proposed to schedule cleaning workers. This approach improves the genetic algorithm by studying soft and hard scheduling restrictions. The Greedy algorithm is used to compare. Experimental evaluations reveal that the suggested model ARIGA meets both goals.","2169-3536","","10.1109/ACCESS.2023.3241942","TM Research and Development from Telekom Malaysia, Malaysia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10035972","Scheduling;predictive maintenance;IoT","Internet of Things;Scheduling;Monitoring;Temperature sensors;Costs;Temperature measurement;Predictive maintenance","","5","","59","CCBY","3 Feb 2023","","","IEEE","IEEE Journals"
"IoT Capabilities Composition and Decomposition: A Systematic Review","K. Halba; E. Griffor; A. Lbath; A. Dahbura","École doctorale Mathématiques, Sciences et technologies de l’information, Informatique (ED MSTII), Grenoble Alpes University, Saint-Martin-d’Hères, France; Department of Computer Science, National Institute of Standards and Technology, Gaithersburg, MD, USA; École doctorale Mathématiques, Sciences et technologies de l’information, Informatique (ED MSTII), Grenoble Alpes University, Saint-Martin-d’Hères, France; Johns Hopkins University, Baltimore, MD, USA",IEEE Access,"30 Mar 2023","2023","11","","29959","30007","As billions of IoT devices join the Internet, researchers and innovators increasingly explore IoT capabilities achieved via service composition or reuse of existing capabilities via service decomposition. Many systematic literature reviews (SLRs) were produced on this subject; however, two issues remain to be addressed: i) a reference taxonomy of the different aspects of IoT capabilities composition and decomposition is needed, and ii) many formal questions (e.g., standards role, formal representations applications, state-space explosion countermeasures, etc.), technical questions (e.g., composition process types and automation levels synergies, service decomposition categories, the role of AI/ML, etc.), and QoS questions (e.g., privacy, interoperability, and scalability challenges and solutions, etc.) remain unanswered. We introduce this work by discussing notions of IoT capabilities composition and decomposition in a layered IoT architecture while highlighting the strengths and weaknesses of existing SLRs. We identify unanswered questions through gaps in related work and motivate these questions using the PICOC methodology. We explain the search methodology and organize the topic questions using the proposed reference taxonomy. The identified research questions are answered, and trends and gaps that need additional attention from the research community are highlighted. This effort benefits city planners and end-users of IoT systems as it contributes to a better understanding of the role of composition and decomposition of IoT capabilities in building value-added services or reusing existing ones for resource optimization. For researchers, this effort contributes a reference taxonomy for the topic and sheds light on important questions while highlighting corresponding trends and gaps requiring further attention.","2169-3536","","10.1109/ACCESS.2023.3260182","National Institute of Standards and Technology (NIST), Gaithersburg, Maryland, USA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10078256","Capability;composition;decomposition;Internet of Things (IoT);cyber-physical systems (CPS);systematic literature review (SLR)","Internet of Things;Atomic layer deposition;Data models;Cyber-physical systems;Atomic measurements;Temperature measurement","","5","","256","CCBY","22 Mar 2023","","","IEEE","IEEE Journals"
"Toward Reference Architectures: A Cloud-Agnostic Data Analytics Platform Empowering Autonomous Systems","A. C. Marosi; M. Emődi; A. Farkas; R. Lovas; R. Beregi; G. Pedone; B. Németh; P. Gáspár","Laboratory of Parallel and Distributed Systems, Institute for Computer Science and Control, Eötvös Lóránd Research Network, Budapest, Hungary; Laboratory of Parallel and Distributed Systems, Institute for Computer Science and Control, Eötvös Lóránd Research Network, Budapest, Hungary; Laboratory of Parallel and Distributed Systems, Institute for Computer Science and Control, Eötvös Lóránd Research Network, Budapest, Hungary; Laboratory of Parallel and Distributed Systems, Institute for Computer Science and Control, Eötvös Lóránd Research Network, Budapest, Hungary; Centre of Excellence in Production Informatics and Control, Institute for Computer Science and Control, Budapest, Hungary; Centre of Excellence in Production Informatics and Control, Institute for Computer Science and Control, Budapest, Hungary; Systems and Control Laboratory, Institute for Computer Science and Control, Eötvös Lóránd Research Network, Budapest, Hungary; Laboratory of Parallel and Distributed Systems, Institute for Computer Science and Control, Eötvös Lóránd Research Network, Budapest, Hungary",IEEE Access,"13 Jun 2022","2022","10","","60658","60673","This work introduces a scalable, cloud-agnostic and fault-tolerant data analytics platform for state-of-the-art autonomous systems that is built from open-source, reusable building blocks. As the baseline for further new reference architectures, it represents an architecture blueprint for processing, enriching and analyzing various feeds of structured and non-structured input data from advanced Internet-of-Things (IoT) based use cases. The platform builds on industry best practices, leverages on solid open-source components in a reusable fashion, and is based on our experience gathered from numerous IoT and Big Data research projects. The platform is currently used in the framework of the National Laboratory for Autonomous Systems in Hungary (abbreviated as ARNL). The platform is demonstrated through selected use cases from ARNL including the areas of smart/autonomous production systems (collaborative robotic assembly) and autonomous vehicles (mobile robots with smart vehicle control). Finally, we validate the platform through the evaluation of its streaming ingestion capabilities.","2169-3536","","10.1109/ACCESS.2022.3180365","National Research, Development and Innovation (NRDI) Office—NKFIH, Hungary(grant numbers:ED_18-2-2018-0006,SNN 129178); Ministry of Innovation and Technology NRDI Office within the framework of the Autonomous Systems National Laboratory Program; Janos Bolyai Research Scholarship of the Hungarian Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9789158","Reference architecture;blueprint;data analytics;autonomous systems;IoT;IIoT;big data;mobile robots;collaborative robots;smart control","Computer architecture;Data analysis;Cloud computing;Architecture;Open source software;Autonomous systems;Measurement","","5","","63","CCBYNCND","6 Jun 2022","","","IEEE","IEEE Journals"
"Cybersecurity of Satellite Communications Systems: A Comprehensive Survey of the Space, Ground, and Links Segments","S. Salim; N. Moustafa; M. Reisslein","School of Engineering and Information Technology, University of New South Wales, Canberra, ACT, Australia; School of Engineering and Information Technology, University of New South Wales, Canberra, ACT, Australia; School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA",IEEE Communications Surveys & Tutorials,"14 Feb 2025","2025","27","1","372","425","Satellite communications (Satcoms) systems have become an integral part of modern society, providing critical infrastructure for a wide range of applications. However, as the reliance on Satcoms has increased, cyberattacks on Satcoms systems have emerged as a severe concern, with the potential to cause significant disruption, economic losses, and even loss of life. We first give a tutorial-style overview of the architecture of a Satcoms system, which typically consists of a space segment, a ground segment (encompassing the terrestrial ground stations and users), and a links segment. Following the taxonomy provided by this segment structure, we provide—to the best of our knowledge—the first comprehensive survey of the state-of-the-art cyberattacks (cyberthreats) on all three segments of Satcoms systems. For each Satcoms system segment, we organize the cyberattacks according to categories of Satcoms-specific cyberattacks, which we relate to the threat classifications in the general STRIDE cyberthreat model. Also, for all three segments of Satcoms systems, we comprehensively survey the general cybersecurity strategies and the specific cybersecurity mechanisms (techniques) that defend Satcoms systems against cyberattacks. We distill the critical learned lessons associated with Satcoms cybersecurity strategies, such as the need to balance security with cost-effectiveness. Finally, we outline the open challenges and future research directions in Satcoms systems cybersecurity.","1553-877X","","10.1109/COMST.2024.3408277","SmartSat CRC(grant numbers:RG214317); Australian Government’s CRC Program; Australian Research Council/Discovery Early Career Researcher Award(grant numbers:DE23010011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10546924","Cyberattack;cybersecurity;ground segment;links segment;space segment;satellite communications","Computer security;Computer crime;Surveys;Space vehicles;Satellites;Satellite broadcasting;Media","","5","","388","IEEE","3 Jun 2024","","","IEEE","IEEE Journals"
"Research Trends, Detection Methods, Practices, and Challenges in Code Smell: SLR","M. A. A. Hilmi; A. Puspaningrum; Darsih; D. O. Siahaan; H. S. Samosir; A. S. Rahma","Department of Informatics, Politeknik Negeri Indramayu, Indramayu, Indonesia; Department of Informatics, Politeknik Negeri Indramayu, Indramayu, Indonesia; Department of Informatics, Politeknik Negeri Indramayu, Indramayu, Indonesia; Department of Informatics, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Faculty of Vocational Studies, Institut Teknologi Del, Toba, Indonesia; Department of Informatics, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia",IEEE Access,"23 Nov 2023","2023","11","","129536","129551","Context: A code smell indicates a flaw in the design, implementation, or maintenance process that could degrade the software’s quality and potentially cause future disruptions. Since being introduced by Beck and Fowler, the term code smell has attracted several studies from researchers and practitioners. However, over time, studies are needed to discuss whether this issue is still interesting and relevant. Objective: Conduct a thorough systematic literature review to learn the most recent state of the art for studying code smells, including detection methods, practices, and challenges. Also, an overview of trends and future relevance of the topic of code smell, whether it is still developing, or if there has been a shift in the discussion. Method: The search methodology was employed to identify pertinent scholarly articles from reputable databases such as ScienceDirect, IEEE Xplore, ACM Digital Library, SpringerLink, ProQuest, and CiteSeerX. The application of inclusion and exclusion criteria serves to filter the search results. In addition, forward and backward snowballing techniques are employed to enhance the comprehensiveness of the results. Results: The inquiry yielded 354 scholarly articles published over the timeframe spanning from January 2013 to July 2022. After inclusion, exclusion, and snowballing techniques were applied, 69 main studies regarding code smells were identified. Many researchers focus on detecting code smells, primarily via machine learning techniques and, to a lesser extent, deep learning methods. Additional subjects encompass the ramifications of code smells; code smells within specific contexts, the correlation between code smells and software metrics, and facets about security, refactoring, and development habits. Contexts and types of code smells vary in the focus of the study. Some tools used are Jspirit, aDoctor, CAME, and SonarQube. The study also explores the concept of design smells and anti-pattern detection. While a singular dominating technique to code smell detection has yet to be thoroughly investigated, other aspects of code smell detection remain that still need to be examined. Conclusion: The findings underscore scholarly attention’s evolution towards code smells over the years. This study identified significant journals and conferences and influential researchers in this field. The detection methods used include empirical, machine learning, and deep learning. However, challenges include subjective interpretation and limited contextual applicability.","2169-3536","","10.1109/ACCESS.2023.3334258","Politeknik Negeri Indramayu on the 2022–2023 Inter-Higher Education Cooperation Research (PKPT) Scheme in Collaboration with Institut Teknologi Sepuluh Nopember (ITS) Surabaya; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10322720","Code smell;detection;systematic review;software quality;bad smell","Codes;Systematics;Surveys;Software quality;Object recognition;Correlation;Taxonomy;Machine learning;Computer security;Performance evaluation","","5","","95","CCBYNCND","17 Nov 2023","","","IEEE","IEEE Journals"
"A Framework for Cognitive, Decentralized Container Orchestration","R. C. Sofia; J. Salomon; S. Ferlin-Reiter; L. Garcés-Erice; P. Urbanetz; H. Mueller; R. Touma; A. Espinosa; L. M. Contreras; V. Theodorou; N. Psaromanolakis; L. Mamatas; V. Tsaoussidis; X. Fu; T. Yuan; A. del Rio; D. Jiménez; A. Stam; E. Paraskevoulakou; P. Karamolegkos; V. Vieira; J. Martrat; I. M. Prusiel; D. Matzakou; J. Soldatos; D. Remon; M. Jahn","fortiss GmbH, Munich, Germany; RedHat, Ra’anana, Israel; RedHat, Ra’anana, Israel; IBM Research Europe, Zürich, Switzerland; IBM Research Europe, Zürich, Switzerland; Siemens AG, Munich, Germany; i2CAT Foundation, Barcelona, Spain; i2CAT Foundation, Barcelona, Spain; Telefónica, Madrid, Spain; Intracom Telecom, Attica, Greece; Intracom Telecom, Attica, Greece; Athena Research Center, University of Macedonia, Thessaloniki, Greece; Athena Research Center, Democritus University of Thrace, Xanthi, Greece; Institute of Computer Science, University of Göttingen, Göttingen, Germany; Institute of Computer Science, University of Göttingen, Göttingen, Germany; Electrónica Física, Ingeniería Eléctrica y Física Aplicada, Universidad Politécnica de Madrid, Madrid, Spain; Electrónica Física, Ingeniería Eléctrica y Física Aplicada, Universidad Politécnica de Madrid, Madrid, Spain; Almende, Rotterdam, The Netherlands; Data & Cloud Research Group, University of Piraeus Research Centre, Piraeus, Greece; Data & Cloud Research Group, University of Piraeus Research Centre, Piraeus, Greece; INOVA+, Matosinhos, Portugal; ATOS, Madrid, Spain; ATOS, Madrid, Spain; Netcompany-Intrasoft, Luxembourg City, Luxembourg; Netcompany-Intrasoft, Luxembourg City, Luxembourg; Eclipse Foundation, Darmstadt, Germany; Eclipse Foundation, Darmstadt, Germany",IEEE Access,"11 Jun 2024","2024","12","","79978","80008","This perspective paper introduces a novel framework for container orchestration called CODECO. The CODECO orchestration framework relies on a data-network-computing approach to define the best infrastructure that can support the operation of next-generation Internet applications across a mobile, heterogeneous Edge-Cloud continuum. The selection of such an infrastructure is aligned with target performance profiles defined by the user, such as resilience or greenness. CODECO proposes to rely on decentralized Artificial Intelligence approaches to provide the most suitable infrastructure to an application deployment, considering infrastructural challenges, such as intermittent connectivity and node failure. This paper explains the current CODECO framework and gives insight into operational use-cases where CODECO is being deployed, as relevant examples of application for such a framework. Recent developments in the creation of the open-source CODECO framework are described and explained, allowing the use of the framework by the research community. The paper then provides a thorough analysis of CODECO’s features in comparison with existing orchestration frameworks, explaining the benefits introduced with this dynamic orchestration approach.","2169-3536","","10.1109/ACCESS.2024.3406861","European Commission in the Context of the Horizon Europe Cognitive Decentralised Edge-Cloud Orchestration (CODECO)(grant numbers:101092696); Swiss State Secretariat for Education, Research and Innovation (SERI)(grant numbers:23.00028); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10540390","Edge-cloud;orchestration;Kubernetes;AI/ML;heterogeneous networks;data observability","Containers;Europe;Computer architecture;Servers;Internet of Things;Real-time systems;Cloud computing;Edge computing;Heterogeneous networks;Observability;Data models;Internet;Computer applications;Open source software;Automation","","5","","73","CCBYNCND","29 May 2024","","","IEEE","IEEE Journals"
"Airborne Computing: A Toolkit for UAV-Assisted Federated Computing for Sustainable Smart Cities","K. Hayawi; Z. Anwar; A. W. Malik; Z. Trabelsi","College of Interdisciplinary Studies, Zayed University, Abu Dhabi, UAE; Department of Computer Science, North Dakota State University, Fargo, ND, USA; Department of Computing, National University of Sciences and Technology, Islamabad, Pakistan; College of Information Technology, United Arab Emirates University, Abu Dhabi, UAE",IEEE Internet of Things Journal,"19 Oct 2023","2023","10","21","18941","18950","Smart vehicles are equipped with onboard computing units designed to run in-vehicle applications. However, due to limited computing power, the onboard units are unable to execute compute-intensive tasks and those that require near real-time processing. Therefore tasks are offloaded to nearby fog/edge devices that have more powerful processors. However, the fog devices are static, placed at fixed locations such as intersections, and have a limited communication range. Therefore, they can only facilitate vehicles in their immediate vicinity and only limited areas of the city can be covered to provide services on demand. In this article, we propose an unmanned aerial vehicle (UAV)-based computing framework design termed Skywalker to provide computing in regions where there are no static fog units thereby extending coverage. Skywalker’s contributions are threefold: 1) it allows for load-aware UAV placement and provisions a swarm of UAVs to fly to areas experiencing a gap in service where the size of the swarm is proportional to the demand; 2) it implements multiple scheduling algorithms that the UAVs swarm employs to divide up the task processing responsibility for individual UAVs within the swarm; and 3) a zone-based delivery mechanism is being proposed to facilitate the return of completed tasks, either through direct delivery or relay-based methods. The choice between these options depends on the distance covered by the requesting vehicle from the UAV swarm. The efficiency of the framework is compared with existing techniques and it is found that it can greatly extend coverage during peak traffic hours while providing low communication delay and consuming minimum energy.","2327-4662","","10.1109/JIOT.2023.3292308","Zayed University through the Cluster Research(grant numbers:R20140); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10172226","Fog computing;smart cities;task offloading;unmanned aerial vehicle (UAV) swarm","Task analysis;Smart cities;Edge computing;Autonomous aerial vehicles;Servers;Relays;Real-time systems","","5","","21","IEEE","4 Jul 2023","","","IEEE","IEEE Journals"
"Early Evaluation of Mobile Applications’ Resource Consumption and Operating Costs","J. Berrocal; J. García-Alonso; P. Fernandez; A. Pérez-vereda; J. Hernandez; C. Canal; J. M. Murillo; A. Ruiz-Cortes","Department of Computer and Telematic Systems Engineering, University of Extremadura, Badajoz, Spain; Department of Computer and Telematic Systems Engineering, University of Extremadura, Badajoz, Spain; Department of Computer Languages and Systems, University of Sevilla, Sevilla, Spain; Department of Computer Science, University of Malaga, Málaga, Spain; Department of Computer and Telematic Systems Engineering, University of Extremadura, Badajoz, Spain; Department of Computer Science, University of Malaga, Málaga, Spain; Department of Computer and Telematic Systems Engineering, University of Extremadura, Badajoz, Spain; Department of Computer Languages and Systems, University of Sevilla, Sevilla, Spain",IEEE Access,"17 Aug 2020","2020","8","","146648","146665","The explosive growth of the mobile application market in recent years has led to a large concomitant mobile software industry whose components are, in many cases, startups and small-size software providers. The success of these applications and the firms behind them depends on a subtle balance between different dimensions mainly affected by their architectural design, such as user satisfaction, resource consumption, operating costs, and timing. The present communication describes a framework with a specific set of practices for identifying the boundaries of different architectural designs —in this article we apply it to estimate both the smartphone’s resource consumption and the operating costs in the cloud— and thus help in the architectural decision-making process. This will enable mobile software developers to predict at early stages which architectural design best suits their business model in accordance with the number of users and the expected use of the application and even provide an advance alert of when architectural choices will need to be reviewed, obviating the need for costly architectural re-design in further phases.","2169-3536","","10.1109/ACCESS.2020.3015082","MCI/AEI/FEDER,UE, through Projects(grant numbers:RTI2018-094591-B-I00,PGC2018-094905-B-I00); APOLO(grant numbers:U.S.–1264651); HORATIO(grant numbers:RTI2018-101204–B–C21); RCIS Research Network(grant numbers:RED2018-102654-T); 4IE+ Project; Interreg V-A España-Portugal (POCTEP) 2014–2020 Program(grant numbers:0499-4IE-PLUS-4-E); FEDER/Junta de Andalucia(grant numbers:UMA18-FEDERJA-180); Department of Economy, Science, and Digital Agenda of the Government of Extremadura(grant numbers:GR18112,IB18030); European Regional Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9162092","J9 mobile applications;D211 software architectures;D218 software engineering process;M2 services lifecycle","Software;Mobile applications;Investment;Industries;Mobile handsets;Computer architecture","","5","","67","CCBY","7 Aug 2020","","","IEEE","IEEE Journals"
"Cloud-Based Implementation of an Automatic Coverage Estimation Methodology for Self-Organising Network","D. Fernandes; D. Clemente; G. Soares; P. Sebastião; F. Cercas; R. Dinis; L. S. Ferreira","Instituto Universitário de Lisboa (ISCTE-IUL)/IT—Instituto de Telecomunicações, Av. das Forças Armadas, Lisbon, Portugal; Instituto Universitário de Lisboa (ISCTE-IUL)/IT—Instituto de Telecomunicações, Av. das Forças Armadas, Lisbon, Portugal; Multivision—Consultoria, Rua Soeiro Pereira Gomes, Lote N°1, 3°C, Lisbon, Portugal; Instituto Universitário de Lisboa (ISCTE-IUL)/IT—Instituto de Telecomunicações, Av. das Forças Armadas, Lisbon, Portugal; Instituto Universitário de Lisboa (ISCTE-IUL)/IT—Instituto de Telecomunicações, Av. das Forças Armadas, Lisbon, Portugal; FCT—Universidade Nova de Lisboa, Monte da Caparica, Caparica, Portugal; Multivision—Consultoria, Rua Soeiro Pereira Gomes, Lote N°1, 3°C, Lisbon, Portugal",IEEE Access,"16 Apr 2020","2020","8","","66456","66474","One of the main concerns of telecommunications operators is related to network coverage. A weak coverage can lead to a performance decrease, not only in the user experience, when using the operators' services, such as multimedia streaming, but also in the overall Quality of Service. This paper presents a novel cloud-based framework of a semi-empirical propagation model that estimates the coverage in a precise way. The novelty of this model is that it is automatically calibrated by using drive test measurements, terrain morphology, buildings in the area, configurations of the network itself and key performance indicators, automatically extracted from the operator's network. Requirements and use cases are presented as motivations for this methodology. The results achieve an accuracy of about 5 dB, allowing operators to obtain accurate neighbour lists, optimise network planning and automate certain actions on the network by enabling the Self-Organising Network concept. The cloud implementation enables a fast and easy integration with other network management and monitoring tools, such as the Metric platform, optimising operators' resource usage recurring to elastic resources on-demand when needed. This implementation was integrated into the Metric platform, which is currently available to be used by several operators.","2169-3536","","10.1109/ACCESS.2020.2986437","OptiNET-5G project and co-funded by the Centro2020, Portugal2020; European Union(grant numbers:023304); Instituto de Telecomunicações, FCT/MCTES through national funds and co-funded by the EU funds(grant numbers:UIDB/EEA/50008/2020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9060956","Cloud implementation;coverage estimation;drive tests;measurements;propagation model","Estimation;Telecommunications;Planning;Tools;Measurement;Optimization;Cloud computing","","5","","51","CCBY","8 Apr 2020","","","IEEE","IEEE Journals"
"Privacy-Preserving and Secure Industrial Big Data Analytics: A Survey and the Research Framework","L. Liu; J. Li; J. Lv; J. Wang; S. Zhao; Q. Lu","Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Institute of Computing Technology Chinese Academy of Sciences, South China University of Technology, Guangzhou, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China",IEEE Internet of Things Journal,"22 May 2024","2024","11","11","18976","18999","The development of the Industrial Internet will generate a large amount of valuable data, known as industrial big data (IBD). By mining and utilizing IBD, enterprises can improve production efficiency, reduce costs and risks, optimize management processes, and innovate services and business models. However, IBD comes from various institutions in all walks of life and has features such as multisource, heterogeneity, and multimodality. And data sharing and trading (DS&T) occur in the Industrial Internet environment without mutual trust. These characteristics pose new challenges to analytics methods and privacy and security protection technologies. Therefore, this article aims to provide references for privacy-preserving and secure industrial big data analytics (IBDA) from three perspectives: 1) research framework; 2) platform architecture; and 3) key technologies. First, we review the current state of research on theories and technologies related to IBDA. Then, we reveal three challenges to secure and efficient IBDA. We take the analytics and utilization of IBD as systematic engineering, propose the research framework for privacy-preserving and secure IBDA, and point out the specific content to be studied. Further, we design the architecture of the IBDA platform with the idea of layering, including a function model, security architecture, and system architecture. Finally, detailed research proposals and potential technologies for IBD analytics and utilization are presented from three aspects: 1) data fusion and analytics; 2) data privacy and security protection; and 3) blockchain.","2327-4662","","10.1109/JIOT.2024.3353727","National Science Foundation of China(grant numbers:51977155); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10399957","Blockchain;data analytics;data sharing and trading (DS&T);federated learning (FL);industrial big data (IBD);privacy and security","Big Data;Security;Data privacy;Internet;Blockchains;Computer architecture;Privacy","","4","","124","IEEE","15 Jan 2024","","","IEEE","IEEE Journals"
"Energy Consumption of Machine Learning Enhanced Open RAN: A Comprehensive Review","X. Liang; Q. Wang; A. Al-Tahmeesschi; S. B. Chetty; D. Grace; H. Ahmadi","School of Physics, Engineering and Technology, University of York, York, U.K; School of Physics, Engineering and Technology, University of York, York, U.K; School of Physics, Engineering and Technology, University of York, York, U.K; School of Physics, Engineering and Technology, University of York, York, U.K; School of Physics, Engineering and Technology, University of York, York, U.K; School of Physics, Engineering and Technology, University of York, York, U.K",IEEE Access,"14 Jun 2024","2024","12","","81889","81910","The Open Radio Access Network (RAN) emerges as a revolutionary architecture promising unprecedented levels of openness, flexibility, and intelligence within radio access networks. Central to this innovation is the integration of Machine Learning (ML) and Artificial Intelligence (AI) within the RAN Intelligent Controller (RIC), aimed at optimizing network operations and enhancing control mechanisms. This paper undertakes a thorough examination of Open RAN, particularly focusing on its energy consumption aspects, which are pivotal for ensuring the sustainability of future wireless networks. In this paper, we review and compare Open RAN architecture with previous network architectures. In particular we focus on O-RAN Alliance specifications. Additionally, we explore the deployment of ML across various facets of Open RAN and highlights how to estimate the energy consumption of ML models. Through constructing explicit energy consumption models for key O-RAN components, we provide a granular analysis of their energy profiles. Finally we compare the energy dynamics of O-RAN against traditional RAN architectures, delineating the impact of virtualization and disaggregation on energy efficiency.","2169-3536","","10.1109/ACCESS.2024.3412758","Engineering and Physical Sciences Research Council United Kingdom, Impact Acceleration Accounts(grant numbers:EP/X525856/1); Department of Science, Innovation and Technology, U.K., through Yorkshire Open-RAN(grant numbers:TS/X013758/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10552840","Open radio access network (Open RAN);energy efficiency;machine learning;disaggregation","Energy consumption;Computer architecture;Energy efficiency;Resource management;Virtualization;Reviews;5G mobile communication;Radio access networks;Open systems","","4","","94","CCBY","11 Jun 2024","","","IEEE","IEEE Journals"
"Computing Power Networking Meets Blockchain: A Reputation-Enhanced Trading Framework for Decentralized IoT Cloud Services","L. Lin; J. Wu; Z. Zhou; J. Zhao; P. Li; J. Xiong","College of Computer and Cyber Security, Fujian Normal University, Fuzhou, China; College of Computer and Cyber Security, Fujian Normal University, Fuzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Zhejiang-HUST Joint Research Center for Graph Processing, Zhejiang Lab, Hangzhou, China; School of Computer Science and Engineering, University of Aizu, Aizuwakamatsu, Japan; College of Computer and Cyber Security, Fujian Normal University, Fuzhou, China",IEEE Internet of Things Journal,"9 May 2024","2024","11","10","17082","17096","Computing power networking (CPN) represents a transformative paradigm in distributed computing, harnessing the collective capabilities of edge servers dispersed across diverse geographical locations. CPN’s core strengths lie in its ability to accelerate data processing, diminish latency, and scale efficiently, rendering it particularly apt for real-time applications and the Internet of Things. When coupled with blockchain technology, CPN extends its potential by facilitating secure and transparent allocation and trading of computing resources, bolstering data integrity and reliability. However, current research at the intersection of CPN and blockchain primarily focuses on framework development and technology integration, often overlooking the challenge of delivering dependable computing services, especially in the presence of potentially unreliable nodes. To tackle this issue, we introduce a reputation-enhanced resource trading framework, designed to ensure equitable and trustworthy computing power transactions. We establish a decentralized reputation model, capable of accurately assessing node behavior over extended periods. Additionally, we present three optimization mechanisms for reputation updates, accounting for transaction history, quality of service, and transaction amount. Furthermore, our work introduces a reputation-enhanced consensus mechanism within the trading system, strategically employing incentives to motivate participants to deliver high-quality services, thereby increasing their rewards. Simultaneously, it effectively mitigates wealth inequality among resource providers of varying sizes. To validate our approach, we develop a prototype system and conduct performance evaluations, which affirm the superiority of our system in enhancing reputation and delivering robust economic features.","2327-4662","","10.1109/JIOT.2024.3358379","National Natural Science Foundation of China(grant numbers:62272102,62172454,61502103); Natural Science Foundation of Fujian Province(grant numbers:2021J01167,2023J02014); Japan Society for the Promotion of Science (JSPS) KAKENHI(grant numbers:21H03424); Japan Science and Technology Agency (JST) PRESTO(grant numbers:23828673); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10414371","Blockchain;computing power networking (CPN);consensus mechanism;decentralized cloud computing (DCC);reputation","Internet of Things;Servers;Consensus protocol;Cloud computing;Peer-to-peer computing;Computational modeling;Resource management","","4","","47","IEEE","25 Jan 2024","","","IEEE","IEEE Journals"
"Specification and Unattended Deployment of Home Networks at the Edge of the Network","I. Bernabé-Sánchez; D. Díaz-Sánchez; M. Muñoz-Organero","Telematic Engineering Department, Carlos III University of Madrid, Leganés, Spain; Telematic Engineering Department, Carlos III University of Madrid, Leganés, Spain; Telematic Engineering Department, Carlos III University of Madrid, Leganés, Spain",IEEE Transactions on Consumer Electronics,"25 Nov 2020","2020","66","4","279","288","Consumer devices continue to expand their capabilities by connecting to digital services and other devices to form information-sharing ecosystems. This is complex and requires meeting connection requirements and minimal processing capabilities to ensure communication. The emergence of new services, and the evolution of current technologies, constantly redefine the rules of the game by opening up new possibilities and increasing competition among service providers. Paradigms such as edge computing, softwarization of physical devices, self-configuration mechanisms, definition of software as a code and interoperability between devices, define design principles to be taken into account in future service infrastructures. This work analyzes these principles and presents a programmable architecture in which services and virtual devices are instantiated in any computing infrastructure, as cloud or edge computing, upon request according to the needs specified by service providers or users. Considering that the target computing infrastructures are heterogeneous, the solution defines network elements and provides network templates to ensure it can be deployed on different infrastructures irrespectively of the vendor. A prototype has been developed and tested on a virtualized cloud-based home network relying on open source solutions.","1558-4127","","10.1109/TCE.2020.3018543","Project MAGOS(grant numbers:TEC2017-84197-C4-1-R); Comunidad de Madrid (Spain) through the Project CYNAMON(grant numbers:P2018/TCS-4566); European Structural Funds (ESF and FEDER); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9173527","Connected consumer devices;fog computing;orchestrator","Cloud computing;Software;Ecosystems;Computer architecture;Edge computing","","4","","33","CCBY","21 Aug 2020","","","IEEE","IEEE Journals"
"Empowering Cloud Computing With Network Acceleration: A Survey","L. Rosa; L. Foschini; A. Corradi","Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy",IEEE Communications Surveys & Tutorials,"21 Nov 2024","2024","26","4","2729","2768","Modern interactive and data-intensive applications must operate under demanding time constraints, prompting a shift toward the adoption of specialized software and hardware network acceleration technologies. This specialization, however, poses significant scalability, flexibility, security, and economic sustainability challenges for application developers. Cloud computing holds the potential to overcome these obstacles by offering the cost-effective option to access specialized acceleration technologies through standard cloud interfaces. Nevertheless, that integration is still challenging for cloud providers. In the cloud, physical resources are hidden behind a virtualization layer, whereas acceleration technologies make applications directly interact with the hardware. To bridge this gap, recent literature explores the possibility of empowering cloud platforms with accelerated networking as a commodity, thus offering the innovative option of Network Acceleration as a Service. This survey reviews these recent research efforts by adopting popular technologies like XDP, DPDK, and RDMA as a reference. To organize the surveyed research in a comprehensive framework, we identify four key aspects that pose critical problems to the integration of acceleration options in cloud computing - access interfaces, virtualization techniques, serviceability, and security - and systematically discuss the associated challenges. Then, we present the issues to be further addressed and outline the most promising research directions for the full integration of network acceleration within next-generation cloud computing platforms.","1553-877X","","10.1109/COMST.2024.3377531","European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU, partnership on “Telecommunications of the Future”(grant numbers:PE00000001 (Program “RESTART”)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10472517","Cloud computing;network acceleration;next-generation networks;RDMA;XDP;DPDK","Cloud computing;Surveys;Virtualization;Tutorials;Data centers;Software;Reviews","","4","","168","CCBYNCND","14 Mar 2024","","","IEEE","IEEE Journals"
"A Novel 5G-NR Resources Partitioning Framework Through Real-Time User-Provider Traffic Demand Analysis","M. Maule; J. S. Vardakas; C. Verikoukis","Iquadrat Informatica S.L., Barcelona, Spain; Iquadrat Informatica S.L., Barcelona, Spain; the University of Patras, Patras, Greece",IEEE Systems Journal,"9 Dec 2022","2022","16","4","5317","5328","Network slicing (NS) is a key enabler of the 5G and beyond network architectures, allowing multiple dedicated logical networks with selected functionality to be executed on top of a common infrastructure. At the radio access network, novel flexibility paradigms and dynamic response to wireless channel variations are necessary for any NS solution. In this framework, this article proposes a novel real-time NS management framework for the 5G New-Radio, where the slice resource management is achieved through a joint dynamic evaluation of served users’ quality of service and tenants’ service level agreement. The design of the proposed framework within the standardized 5G architecture is discussed, together with the integration of the dynamic resource assignation procedure within the slice life cycle workflow. A novel mathematical model is proposed for the dynamic resource assignation within the slice life cycle workflow. The solution’s accuracy is tested by means of computer simulations, and found to be satisfactory according to the proposed evaluation metrics. Finally, the practicality of the optimal radio slicing configuration extracted from the proposed model is applied to our experimental platform, and the performance of a real case scenario is comprehensively evaluated.","1937-9234","","10.1109/JSYST.2021.3115896","EU H2020 5GSTEP-FWD(grant numbers:722429); EU H2020 5G-ERA(grant numbers:101016681); EU H2020 5G-ROUTES(grant numbers:951867); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9585085","5G;5G New-Radio (5G-NR);cloud radio access network (C-RAN);network slicing (NS);resources optimization;service-based architecture;testbed","5G mobile communication;Computer architecture;Real-time systems;Dynamic scheduling;Protocols;Cloud computing;3GPP","","4","","36","IEEE","25 Oct 2021","","","IEEE","IEEE Journals"
"Convergence of AI and MEC for Autonomous IoT Service Provisioning and Assurance in B5G","K. Abbas; Y. Cho; A. Nauman; P. W. Khan; T. A. Khan; K. Kondepu","Department of Computer Science, Hanyang University, Seoul, South Korea; Department of Computer Science, Hanyang University, Seoul, South Korea; Department of Information and Communication Engineering, Yeungnam University, Gyeongsan, South Korea; Department of Industrial and Management Systems Engineering, West Virginia University, Morgantown, USA; Institute for Communication Systems, University of Surrey, Guildford, U.K.; Department of Computer Science and Engineering, Indian Institute of Technology Dharwad, Dharwad, India",IEEE Open Journal of the Communications Society,"22 Nov 2023","2023","4","","2913","2929","With the exponential growth of Internet of Things (IoT) devices, IoT has become a transformative technology with applications spanning various domains. It encompasses a wide range of public and industrial vertical services that come with diverse and stringent Quality of Service (QoS) requirements. Traditional networks often struggle to meet the demands of these diverse IoT services. As a result, the introduction of 5G and Beyond 5G (B5G) networks holds promise in accommodating these diverse IoT services through network slicing technology. Network slicing involves partitioning a single physical network infrastructure into multiple logically isolated networks and ensures dedicated resources to each service as per QoS requirements. Additionally, Multi-Access Edge Computing (MEC) in B5G networks presents an innovative solution to facilitate low-latency communication for IoT services. However, the automatic provisioning and management of end-to-end (e2e) network slicing for IoT services across multi-domain infrastructures pose significant challenges, including manual error-prone resource configuration, network slice template preparation, and human intervention. This paper proposes an automated Artificial Intelligence (AI) and MEC-enabled solution for provisioning and managing network slice resources across multiple domains specifically tailored for IoT services. Our solution provides an abstraction layer that generates slice templates for each domain and automates the deployment of resources based on the specified QoS requirements. It automates the slice resource configuration process, reduces human intervention, and manages the complete lifecycle of IoT slices. We have conducted several tests with our system, creating multiple IoT slices, and have observed stable performance in slice design, resource provisioning, slice isolation, and management.","2644-125X","","10.1109/OJCOMS.2023.3329420","National Research Foundation of Korea (NRF); Korea Government (MSIT)(grant numbers:NRF-2022R1A4A1032361); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10304258","IoT;beyond 5G networks;MEC;SDN;network slicing;AI for 5G;service automation and management","Internet of Things;5G mobile communication;Quality of service;Network slicing;Artificial intelligence;Resource management;Computer architecture","","4","","61","CCBYNCND","1 Nov 2023","","","IEEE","IEEE Journals"
"Tactful Networking: Humans in the Communication Loop","R. L. Costa; A. C. Viana; A. Ziviani; L. N. Sampaio","Ècole Polytechnique/IPP and Inria, Palaiseau, France; Inria, Palaiseau, France; National Laboratory for Scientific Computing (LNCC), Petropolis, Brazil; Federal University of Bahia, Salvador, Brazil",IEEE Transactions on Emerging Topics in Computational Intelligence,"20 Jan 2021","2021","5","1","92","107","This survey discusses the human-perspective into networking through the Tactful Networking paradigm, whose goal is to add perceptive senses to the network by assigning it with human-like capabilities of observation, interpretation, and reaction to daily-life features and associated entities. To achieve this, knowledge extracted from inherent human behavior in terms of routines, personality, interactions, and others is leveraged, empowering the learning and prediction of user needs to improve QoE and system performance while respecting privacy and fostering new applications and services. Tactful Networking groups solutions from literature and innovative interdisciplinary human aspects studied in other areas. The paradigm is motivated by mobile devices' pervasiveness and increasing presence as a sensor in our daily social activities. With the human element in the foreground, it is essential: (i) to center big data analytics around individuals; (ii) to create suitable incentive mechanisms for user participation; (iii) to design and evaluate both human-aware and system-aware networking solutions; and (iv) to apply prior and innovative techniques to deal with human-behavior sensing and learning. This survey reviews the human aspect in networking solutions through over a decade, followed by discussing the tactful networking impact through literature in behavior analysis and representative examples. This paper also discusses a framework comprising data management, analytics, and privacy for enhancing human raw-data to assist Tactful Networking solutions. Finally, challenges and opportunities for future research are presented.","2471-285X","","10.1109/TETCI.2020.3039520","FAPERJ, FAPESB, CNPq, CAPES, INCT-CiD, and EMBRACE associate team Inria project(grant numbers:AAPG2019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9285188","Human behavioral perception;human-aware;next-generation networks","Social networking (online);Routing;Quality of service;Privacy;Sensors;Data privacy;Quality of experience","","4","","181","IEEE","7 Dec 2020","","","IEEE","IEEE Journals"
"Efficient Large-Scale Multiple Migration Planning and Scheduling in SDN-Enabled Edge Computing","T. He; A. N. Toosi; R. Buyya","CLOUDS Lab, School of Computing and Information Systems, University of Melbourne, Parkville, VIC, Australia; Department of Software Systems and Cybersecurity, Faculty of Information Technology, Monash University, Clayton, VIC, Australia; CLOUDS Lab, School of Computing and Information Systems, University of Melbourne, Parkville, VIC, Australia",IEEE Transactions on Mobile Computing,"7 May 2024","2024","23","6","6667","6680","Services provided by mobile edge clouds offer low-latency responses for large-scale and real-time applications. Dynamic service management algorithms generate live service migration requests to support user mobility and ensure service latency in mobile edge clouds. To handle these migration requests, multiple migration planning and scheduling algorithms are necessary to calculate the migration order and optimize the performance and overhead of multiple migrations. However, current planning and scheduling algorithms in cloud data centers are not suitable for dynamic and large-scale scenarios in edge computing, as the network topology expands and the number of migration requests increases. Edge computing requires near real-time scheduling to handle user mobility-induced live migrations. To address this issue, this paper presents an efficient multiple migration planning and scheduling framework for edge computing. The framework includes a lifecycle management framework and innovative iterative Maximal Independent Set-based scheduling algorithms based on the resource dependency graph of multiple migrations. Our solution is shown to efficiently schedule live migrations at scale using real-world taxi traces and telecom base station coordinates. It can achieve significant processing speedups over existing migration planning algorithms in clouds, up to 3,000 times, while ensuring multiple and individual migration performance for time-critical services.","1558-0660","","10.1109/TMC.2023.3326610","Australian Research Council(grant numbers:DP160102414,DP230100081); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290920","Mobile-edge computing;quality of service;service migration;multiple migration scheduling","Planning;Edge computing;Heuristic algorithms;Scheduling;Quality of service;Data centers;Cloud computing","","4","","47","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Selected Code-Quality Characteristics and Metrics for Internet of Things Systems","M. Klima; M. Bures; K. Frajtak; V. Rechtberger; M. Trnka; X. Bellekens; T. Cerny; B. S. Ahmed","Department of Computer Science, Faculty of Electrical Engineering, Czech Technical University in Prague, Prague, Czech Republic; Department of Computer Science, Faculty of Electrical Engineering, Czech Technical University in Prague, Prague, Czech Republic; Department of Computer Science, Faculty of Electrical Engineering, Czech Technical University in Prague, Prague, Czech Republic; Department of Computer Science, Faculty of Electrical Engineering, Czech Technical University in Prague, Prague, Czech Republic; Department of Computer Science, Faculty of Electrical Engineering, Czech Technical University in Prague, Prague, Czech Republic; Lupovis.io, Glasgow, U.K.; Department of Computer Science, Baylor University, Waco, TX, USA; Department of Computer Science, Faculty of Electrical Engineering, Czech Technical University in Prague, Prague, Czech Republic",IEEE Access,"4 May 2022","2022","10","","46144","46161","Software code is present on multiple levels within current Internet of Things (IoT) systems. The quality of this code impacts system reliability, safety, maintainability, and other quality aspects. In this paper, we provide a comprehensive overview of code quality-related metrics, specifically revised for the context of IoT systems. These metrics are divided into main code quality categories: Size, redundancy, complexity, coupling, unit test coverage and effectiveness, cohesion, code readability, security, and code heterogeneity. The metrics are then linked to selected general quality characteristics from the ISO/IEC 25010:2011 standard by their possible impact on the quality and reliability of an IoT system, the principal layer of the system, the code levels and the main phases of the project to which they are relevant. This analysis is followed by a discussion of code smells and their relation to the presented metrics. The overview presented in the paper is the result of a thorough analysis and discussion of the author’s team with the involvement of external subject-matter experts in which a defined decision algorithm was followed. The primary result of the paper is an overview of the metrics accompanied by applicability notes related to the quality characteristics, the system layer, the level of the code, and the phase of the IoT project.","2169-3536","","10.1109/ACCESS.2022.3170475","Operational Programme (OP) Výzkum, vývoj a vzdělávání (VVV) Funded Project “Research Center for Informatics”(grant numbers:CZ.02.1.01/0.0/0.0/16_019/0000765); Knowledge Foundation of Sweden (KKS) through the Synergi Project AIDA-A Holistic Artificial Intelligence (AI)-Driven Networking and Processing Framework for Industrial Internet of Things (IoT)(grant numbers:Rek: 20200067); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762941","Code quality;code smells;Internet of Things;IoT;maintainability;metrics;quality characteristics;reliability;security","Codes;Measurement;Couplings;Internet of Things;Complexity theory;Security;Software reliability","","4","","92","CCBY","25 Apr 2022","","","IEEE","IEEE Journals"
"Adaptive Cruise Control Based on Real-Time DDS Middleware","B. Almadani; N. Alshammari; A. Al-Roubaiey","Department of Computer Engineering, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia; Department of Computer Engineering, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia; Department of Computer Engineering, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia",IEEE Access,"26 Jul 2023","2023","11","","75407","75423","This paper discusses the Adaptive Cruise Control (ACC) system, which is a solution that allows drivers to minimize the amount of time spent driving. It supports four different driving modes and regulates the acceleration and deceleration of the car in order to maintain a fixed speed or avoid a collision with another vehicle. Real-Time Publish-Subscribe (RTPS) middleware has emerged as one of the most efficient and practical options for the real solutions to the difficulties listed above. This paper proposes to develop a real-time system for integrating the various components of ACC, such as Information Cluster, Radar, Brake Switches, Cruise Switches, ACC Controller, Engine/Throttle Controller, Brake Controller, Brake Actuators, Speed Sensors, and Back Brake Lights. The exchange of data is through a RTPS Data Distribution Service (DDS) middleware. The design of the publish/subscribe model was explained in detail in this paper along with the proper Quality of Service (QoS) policies suggested to govern the behavior of the model.","2169-3536","","10.1109/ACCESS.2023.3296317","King Fahd University of Petroleum and Minerals; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10185018","Adaptive cruise control;DDS middleware;leading vehicle;quality of service policies;ACC controller;engine controller;brake controller","Quality of service;Brakes;Middleware;Control systems;Real-time systems;Cruise control;Vehicles","","4","","37","CCBYNCND","17 Jul 2023","","","IEEE","IEEE Journals"
"A Comprehensive Survey on Resource Management in 6G Network Based on Internet of Things","S. S. Sefati; A. U. Haq; Nidhi; R. Craciunescu; S. Halunga; A. Mihovska; O. Fratu","Telecommunications Department, Faculty of Electronics, Telecommunications and Information Technology, National University of Science and Technology Politehnica Bucharest, Bucharest, Romania; Department of Business Development and Technology, Aarhus University, Aarhus, Denmark; Department of Business Development and Technology, Aarhus University, Aarhus, Denmark; Telecommunications Department, Faculty of Electronics, Telecommunications and Information Technology, National University of Science and Technology Politehnica Bucharest, Bucharest, Romania; Telecommunications Department, Faculty of Electronics, Telecommunications and Information Technology, National University of Science and Technology Politehnica Bucharest, Bucharest, Romania; SmartAvatar B.V. Amsterdam, North Holland, Netherlands; Telecommunications Department, Faculty of Electronics, Telecommunications and Information Technology, National University of Science and Technology Politehnica Bucharest, Bucharest, Romania",IEEE Access,"21 Aug 2024","2024","12","","113741","113784","The transition to 6th Generation (6G) cellular networks offers significant improvements over 5th Generation (5G), enhancing data transfer, reducing latency, and improving network reliability. Advanced Multiple-Input Multiple-Output (MIMO) technology in 6G boosts network efficiency, particularly benefiting Ultra-Reliable Low-Latency Communications (URLLC). This paper reviews literature on resource management in the Internet of Things (IoT) within the 6G context. We categorize the study into four segments: network-aware resource management, dynamic resource allocation, predictive resource distribution based on traffic and architecture, and energy-centric resource allocation considering IoT device mobility and location. We provide a detailed perspective on current research and highlight future research avenues. Key contributions include a comparative study of IoT resource management techniques, an overview of resource management across LTE, 5G, and 6G networks, insights into applications like Intelligent Transportation Systems (ITS), Industrial IoT (IIoT), and Mobile CrowdSensing (MCS), and an emphasis on upcoming challenges. We emphasize the crucial role of efficient resource management in IoT, particularly in the 6G landscape.","2169-3536","","10.1109/ACCESS.2024.3444313","Project “Mobility and Training foR beyond 5G ecosystems (MOTOR5G)”; European Union’s Horizon 2020 Program under the Marie Skłodowska Curie Actions (MSCA) Innovative Training Network (ITN)(grant numbers:861219); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10637461","6G network;Internet of Things;resource management;survey;quality of service (QoS)","6G mobile communication;Resource management;Internet of Things;5G mobile communication;Surveys;Quality of service;Routing protocols","","4","","210","CCBYNCND","15 Aug 2024","","","IEEE","IEEE Journals"
"A Systematic Literature Review of Offensive and Defensive Security Solutions With Software Defined Network","A. Melis; A. A. Sadi; D. Berardi; F. Callegati; M. Prandini","Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy",IEEE Access,"4 Sep 2023","2023","11","","93431","93463","Software Defined Networking (SDN) is one of the most significant innovations in telecommunication systems in the past two decades. From the very beginning, the scientific community understood the importance of investigating the possible usages of SDN as a means to increase network security, but also their potential to be exploited as an attack device. For this reason, there has been a massive production of research works, which, however, do not form a well-defined corpus. The literature is spread over many venues and composed of contributions with very different flavors. Though some review works already exist, in this work we conduct a systematic literature review of the field, gathering 466 relevant publications— the largest curated dataset on the topic to the best of our knowledge. In our work, the dataset undergoes a twofold analysis: (a) quantitative, through publication metadata, which allows us to chart publication outlets, approaches, and tackled issues; (b) qualitative, through 14 research questions that provide an aggregated overview of the literature contributions to the key issues, also to spot gaps left open. From these analyses, we derive a call for action to address the main open challenges.","2169-3536","","10.1109/ACCESS.2023.3276238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10124203","Attack;defense;mitigation;security;SDN;threat","Security;Software defined networking;Systematics;Surveys;Bibliographies;Computer crime;Control systems","","4","","27","CCBYNCND","15 May 2023","","","IEEE","IEEE Journals"
"Efficient Edge Data Management Framework for IIoT via Prediction-Based Data Reduction","L. Yang; Y. Liao; X. Cheng; M. Xia; G. Xie","Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China",IEEE Transactions on Parallel and Distributed Systems,"7 Nov 2023","2023","34","12","3309","3322","Large amounts of time series data are required to support data analysis at the edge in the end-edge-cloud Industrial Internet of Things (IIoT) architecture. Reducing the storage cost is one of the main challenges in edge data management due to the limited storage resource of edge nodes. The state-of-the-art data reduction method has a high time overhead and poor reduction efficiency for unstable data sets. To solve this problem, this study proposes a time-series data management framework that combines data partition and data compression techniques. For the data partition technique, we propose an adaptive selection strategy to integrate the access pattern of the application and the characteristics of the time series data, thereby improving the partition accuracy. For the data compression technique, we propose a compression scheme based on time series data segmentation by using the idea of divide and conquer; we further introduce a change point detection technique to improve the compression efficiency for unstable data sets. Experimental results obtained with three types of real industrial data sets show that our framework is significantly better than the state-of-the-art method in terms of compression ratio and time overhead.","1558-2183","","10.1109/TPDS.2023.3327750","National Natural Science Foundation of China(grant numbers:62372167,61972139,62141212,62133014); Natural Science Foundation of Hunan Province(grant numbers:2021JJ30153,2022JJ10021); Natural Science Foundation of Chongqing(grant numbers:cstc2021jcyj-msxmX0817,cstc2021jcyj-msxmX0461,CSTB2022NSCQ-MSX1393); Chongqing Science and Technology Innovation Project(grant numbers:CQYC20220511500); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10297424","Data reduction;edge data management framework;industrial internet of things (IIoT)","Industrial Internet of Things;Image edge detection;Data compression;Computer architecture;Time series analysis;Real-time systems;Streams","","4","","36","IEEE","26 Oct 2023","","","IEEE","IEEE Journals"
"Enabling and Leveraging AI in the Intelligent Edge: A Review of Current Trends and Future Directions","T. Goethals; B. Volckaert; F. D. Turck","Department of Information Technology, IDLab, Ghent University—imec, Ghent, Belgium; Department of Information Technology, IDLab, Ghent University—imec, Ghent, Belgium; Department of Information Technology, IDLab, Ghent University—imec, Ghent, Belgium",IEEE Open Journal of the Communications Society,"14 Oct 2021","2021","2","","2311","2341","The use of AI on Smart applications and in the organization of the network edge presents a rapidly advancing research field, with a great variety of challenges and opportunities. This article aims to provide a holistic review of studies from 2019 to 2021 related to the Intelligent Edge, a concept comprising both the use of AI to organize edge networks (Edge Intelligence) and Smart applications in the edge. An introduction is given to the technologies required to understand the state of the art of AI in edge networks, and a taxonomy is provided with “Enabling Technology” for Edge Intelligence, “Organization” of the edge using AI, and AI “Applications” in the edge as its main topics. Research trend data from 2015 to 2020 is presented for various subdivisions of these topics, showing both absolute and relative research interest in each subtopic. The “Organization” aspect, being the main focus of this article, has a more fine-grained subdivision, explaining all contributing factors in detail. The trends indicate an exponential increase in research interest in nearly all subtopics, but significant differences between them. For each subdivision of the taxonomy a number of selected studies from 2019 to 2021 are gathered to form a high-level illustration of the state of the art of Edge Intelligence. From these selected studies and the trend data, a number of short-term challenges and high-level visions for Edge Intelligence are formulated, providing a basis for future work.","2644-125X","","10.1109/OJCOMS.2021.3116437","Vlaio by means of the FlexNet Research Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555813","Fog computing;fog networks;edge networks;edge computing;artificial intelligence;review;trends","Artificial intelligence;Software;Market research;Genomics;Taxonomy;Logistics;Training","","4","","163","CCBY","1 Oct 2021","","","IEEE","IEEE Journals"
"Proactive Auto-Scaling Approach of Production Applications Using an Ensemble Model","M. Samir; K. T. Wassif; S. H. Makady","Faculty of Computers and Artificial Intelligence, Cairo University, Giza, Egypt; Faculty of Computers and Artificial Intelligence, Cairo University, Giza, Egypt; Faculty of Computers and Artificial Intelligence, Cairo University, Giza, Egypt",IEEE Access,"17 Mar 2023","2023","11","","25008","25019","The resource usage behaviors of application workloads are currently the primary concern of cloud providers offering hosting services. These services should be able to adapt to workload changes by automatically provisioning and de-provisioning resources so that, at all times, the existing resources in a system match the current service demand. Such behavior can be achieved manually by hiring a DevOps team to manage the application’s resources. Another option would be automating the resource provisioning processing using automated rules. Once such rules are met, the hosting environment will scale the resources accordingly. However, managing a DevOps team or creating flaky rules can lead to over-scaling application resources. This work proposes a new approach: a proactive auto-scaling framework built on an ensemble model. Such a model utilizes several machine learning techniques to scale application resources to match resource demand before the need arises. We evaluated our solution against three real production applications hosted on Cegedim Cloud Hosting Environment, an industrial environment serving several cloud applications from various domains, and against other machine learning models used in similar proactive auto-scaling experiments mentioned in past work. The experimentation results show that predicting application resources like CPU or RAM is feasible. Moreover, even in production environments, our ensemble model performs optimally in the CPU case and is near the optimal model when predicting RAM resources.","2169-3536","","10.1109/ACCESS.2023.3256302","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10066275","Auto-scaling;resource allocation;dynamic resource provisioning;resource management on clouds","Predictive models;Cloud computing;Machine learning;Prediction algorithms;Load modeling;Data models;Time series analysis","","3","","23","CCBYNCND","10 Mar 2023","","","IEEE","IEEE Journals"
"A Survey on the Use of Container Technologies in Autonomous Driving and the Case of BeIntelli","B. Acar; M. G. Augusto; M. Sterling; F. Sivrikaya; S. Albayrak","Chair of Agent Technology, Technische Universität Berlin, Berlin, Germany; Chair of Agent Technology, Technische Universität Berlin, Berlin, Germany; Chair of Agent Technology, Technische Universität Berlin, Berlin, Germany; Chair of Agent Technology, Technische Universität Berlin, Berlin, Germany; Chair of Agent Technology, Technische Universität Berlin, Berlin, Germany",IEEE Open Journal of Intelligent Transportation Systems,"28 Nov 2023","2023","4","","800","814","The application of containerization technology has seen a significant increase in popularity in recent years, both in the business and scientific sectors. In particular, the ability to create portable applications that can be deployed on different machines has become a valuable asset. Autonomous driving has embraced this technology, as it offers a wide range of potential applications, including the operation of autonomous vehicles and the digitization of infrastructure for the development of Cooperative, Connected, and Automated Mobility (CCAM) services. This paper provides a comprehensive analysis of containerization in autonomous driving, emphasizing its application, utility, benefits, and limitations.","2687-7813","","10.1109/OJITS.2023.3331449","German Federal Ministry for Digital and Transport (BMDV) through BeIntelli Project(grant numbers:01MM20004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10314411","Docker;containerization;automotive;CCAM;autonomous driving","Containers;Autonomous vehicles;Virtualization;Virtual machining;Operating systems;Microservice architectures;Industries","","3","","97","CCBYNCND","9 Nov 2023","","","IEEE","IEEE Journals"
"Network traffic characteristics of hyperscale data centers in the era of cloud applications","F. Yan; C. Xie; J. Zhang; Y. Xi; Z. Yao; Y. Liu; X. Lin; J. Huang; Y. Ce; X. Zhang; N. Calabretta","Alibaba Group, Alibaba Cloud, Beijing, China; Alibaba Group, Alibaba Cloud, New York, NY, USA; Beijing University of Posts and Telecommunications, Beijing, China; Alibaba Group, Alibaba Cloud, Beijing, China; Alibaba Group, Alibaba Cloud, New York, NY, USA; Alibaba Group, Alibaba Cloud, Beijing, China; Alibaba Group, Alibaba Cloud, Beijing, China; Alibaba Group, Alibaba Cloud, Beijing, China; Alibaba Group, Alibaba Cloud, Beijing, China; Alibaba Group, Alibaba Cloud, Beijing, China; Eindhoven University of Technology, Eindhoven, Netherlands",Journal of Optical Communications and Networking,"20 Sep 2023","2023","15","10","736","749","We present the network architecture of Alibaba Cloud DCs and investigate their traffic characteristics based on statistical data and captured traces. The statistical coarse-grained data are in the granularity of one minute, while the captured traces are fine-grained data that are in the granularity of one packet. We study the traffic features from the perspective of a macroscopic view, network performance, and microscopic view. The results report that the average utilization ratio of spine switches is stable when the observation time period reaches one day and the intra-ToR traffic ratio is in the range of 2%–10%. By mapping the folded-Clos topology to a tree topology and considering logical switching planes, we obtain the traffic matrix among pods from the average port utilization ratio. As we further investigate the perspective of network performance and the microscopic view, we find that there is no cell loss happening as the normalized queue speed ${Q_s}$ is lower than 0.4. The normalized queue speed ${Q_s}$ is defined as the total bytes of a queue sent in 1 s divided by 100 Gb, which reflects the packet sending speed of the queue. The observed maximum buffer size for one port conforms with the calculated maximum buffer occupation of 2.8 MB. By analyzing the captured traces, we find that the packet length is subject to a trimodal distribution. Under a time granularity of 10 ms, the instant bandwidth of one ToR port could reach 96 Gb/s at an average load of around 0.2 under a maximum link bandwidth of 100 Gb/s.","1943-0639","","10.1364/JOCN.494291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10258089","","Optical switches;Cloud computing;Bandwidth;Servers;Topology;Data centers;Optical fiber networks","","3","","","","20 Sep 2023","","","IEEE","IEEE Journals"
"A Flexible and Reliable Internet-of-Things Solution for Real-Time Production Tracking With High Performance and Secure Communication","B. -Y. Ooi; W. -K. Lee; M. J. W. Shubert; Y. -W. Ooi; C. -Y. Chin; W. -H. Woo","Faculty of Information and Communication Technology, Universiti Tunku Abdul Rahman, Kampar, Malaysia; Department of Computer Engineering, Gachon University, Seongnam-si, South Korea; Department of Electronics and Information, Ostbayerische Technische Hochschule Regensburg, Regensburg, Germany; Starfive Technology International Sdn Bhd, Penang, Malaysia; W Tech Marketing Sdn. Bhd., Ipoh, Malaysia; Arrow Systems Sdn Bhd, Lahat, Malaysia",IEEE Transactions on Industry Applications,"19 May 2023","2023","59","3","3121","3132","The concept of Industrial Revolution 4.0 (IR4.0) has attracted a lot of attention from academia and industry in recent years. Many existing manufacturers are forced to relook into their exiting production processes, exploring modern ways to improve the yields. Unfortunately, this is difficult to achieve when the data is limited. The situation is particularly serious for many SMEs in developing countries, wherein the manufacturing machines are not up-to-date and lack computational and connectivity capabilities. As an initiative to tackle this issue, we present an end-to-end Internet-of-Things (IoT) solution in this paper, aiming at tracking the production performance of old manufacturing machines reliably. This paper goes over the designs and reasoning behind the proposed solution. We also demonstrated that with careful optimization, high-performance secure encryption key encapsulation and decapsulation are achievable, which is critical for secure communication in IoT systems. As of the time of writing, our IoT system had been deployed in a real manufacturing environment and had been running continuously for approximately 365 days without data loss over the wide area network (WAN).","1939-9367","","10.1109/TIA.2023.3255214","Arrow Systems Sdn. Bhd.; Universiti Tunku Abdul Rahman, Malaysia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10065559","Industry 4.0;Internet-of-Things;production tracking;smart manufacturing","Manufacturing;Production;Reliability;Graphics processing units;Internet of Things;Productivity;Real-time systems","","3","","36","IEEE","10 Mar 2023","","","IEEE","IEEE Journals"
"A Secure and Distributed Placement for Quality of Service-Aware IoT Requests in Fog-Cloud of Things: A Novel Joint Algorithmic Approach","S. K. Srichandan; S. K. Majhi; S. Jena; K. Mishra; R. Bhat","Sambalpur University Institute of Information Technology, Sambalpur University, Burla, Odisha, India; Department of Computer Science and Information Technology, Guru Ghasidas Vishwavidyalaya, Bilaspur, Chhattisgarh, India; Sambalpur University Institute of Information Technology, Sambalpur University, Burla, Odisha, India; Gandhi Institute of Technology and Management, Visakhapatnam, Andhra Pradesh, India; Department of Computer Science and Engineering, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, India",IEEE Access,"25 Apr 2024","2024","12","","56730","56748","The fast proliferation of internet-enabled devices generates massive amounts of data every day from every aspect of life. These internet-enabled devices lack the storage, processing power, and capacity necessary to handle and store this massive amount of accurate and volumetric data. Cloud computing has been proposed as a compelling substitute to process these requests. However, the ingress traffic to the Cloud is huge which causes latency overhead due to the gap that exists between end devices and the Cloud datacentre. Additionally, processing these dynamic and heterogeneous requests with disparate requirements is a computationally NP-hard problem. In this regard, Fog computing appears to be an appealing solution to surpass the aforementioned challenges as a complementary to Cloud. Therefore, this research integrated a Fog layer between the end devices and Cloud datacenters enabling a collaborative computation framework. For the classification of requests and determining the target layers for processing, this research suggests an Adaptive Neuro-Fuzzy Inference System (ANFIS). Furthermore, an improved Honey Badger Algorithm (IHBA) is implemented for scheduling those requests at the target layer. To improve the convergence rate of the traditional HBA, a chaos mapping function has been implemented with an Opposition-based Learning (OBL) method. The proposed task consolidation approach has been validated using extensive simulations for QoS parameters on real-world benchmark datasets. With percentage improvements of 13.69%, 28.4%, 33.85%, 7.66%, 26.48%, and 5.63% for makespan, resource utilization, energy consumption, service delay, service cost, and delay violation, respectively, the obtained simulation results show that the suggested algorithm works better than the current state-of-the-art.","2169-3536","","10.1109/ACCESS.2024.3390723","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10504827","Adaptive neuro-fuzzy inference system (ANFIS);cloud computing;improved honey badger algorithm;resource management;tasks scheduling and offloading","Internet of Things;Cloud computing;Quality of service;Costs;Edge computing;Computational modeling;Biological system modeling;Fuzzy neural networks;Inference algorithms;Resource management;Task analysis;Scheduling","","3","","43","CCBYNCND","18 Apr 2024","","","IEEE","IEEE Journals"
"Solving Task Scheduling Problem in Mobile Cloud Computing Using the Hybrid Multi-Objective Harris Hawks Optimization Algorithm","B. Saemi; A. A. R. Hosseinabadi; A. Khodadadi; S. Mirkamali; A. Abraham","Department of Computer, Kavosh Institute of Higher Education, Mahmood Abad, Mazandaran, Iran; Department of Computer Science, University of Regina, Regina, Canada; Department of Computer Engineering, Islamic Azad University, Qazvin Branch, Qazvin, Iran; Department of Computer Engineering and IT, Payame Noor University, Tehran, Iran; School of Computer Science Engineering and Technology, Bennett University, Greater Noida, Uttar Pradesh, India",IEEE Access,"10 Nov 2023","2023","11","","125033","125054","Nowadays, mobile devices can run a wide range of programs, and they all require more and more processing power. Due to their limited resources, mobile devices often make use of cloud computing $'\text{s}$  offloading features to do more complex tasks. The offloading problem in Mobile Cloud Computing (MCC) is the task scheduling problem, which entails deciding where to dump work to maximize its value. The task scheduling problem in MCC is an NP-hard problem because of the difficulty in moving resources and the size of the search space required to find the ideal scheduler, making the use of extensive search techniques impractical. For this reason, metaheuristic search strategies are provided, to yield a best-case or near-best-case scenario in terms of job completion time and energy savings. This work provides a non-dominated multi-objective strategy based on the Harris Hawks Optimization (HHO) technique called Hybrid Multi-objective Harris Hawks Optimization (HMHHO) to handle the described issue in MCC. The objectives of this research were allocating jobs from mobile source nodes to processors in the public cloud, cloud patches, and processors in mobile resources. In comparison to the other four algorithms—the Genetic Algorithm (GA), the Ant Colony Optimization (ACO), the Particle Swarm Optimization (PSO), and the Cuckoo Search Algorithm (CSA) the proposed method completes jobs faster and uses less energy on average.","2169-3536","","10.1109/ACCESS.2023.3329069","Analytical Center for the Government of the Russian Federation, in November 2021(grant numbers:70-2021-00143,IGK 000000D730321P5Q0002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308570","Task scheduling;multi-objective;mobile cloud computing;optimization;metaheuristic algorithm;Harris Hawks optimization","Task analysis;Cloud computing;Optimization;Mobile handsets;Processor scheduling;Dynamic scheduling;Costs;Mobile computing;Metaheuristics","","3","","38","CCBYNCND","3 Nov 2023","","","IEEE","IEEE Journals"
"Assessment of Industrial Internet Platform Application in Manufacturing Enterprises: System Construction and Industrial Practice","J. Li; Y. Zhou; X. Zhang; S. Liu; Q. Li","Department of Automation, Tsinghua University, Beijing, China; China Industrial Control Systems Cyber Emergency Response Team, Beijing, China; China Industrial Control Systems Cyber Emergency Response Team, Beijing, China; China Industrial Control Systems Cyber Emergency Response Team, Beijing, China; Department of Automation, Tsinghua University, Beijing, China",IEEE Access,"28 Jul 2021","2021","9","","103709","103727","To evaluate the effectiveness of the industrial Internet Platform (IIP) applied by manufacturing enterprises, this study proposes a set of assessment systems for guiding and standardizing the application of IIP. Our assessment framework is established based on five dimensions: strategy and organization; basic conditions; platform application; business innovation; and efficiency and benefit. We collect IIP application data from more than 1400 manufacturing enterprises in China. The application performance of the IIP in manufacturing enterprises were comprehensively analyzed from different perspectives such as equipment on the cloud, industrial knowledge accumulation and reuse, and industrial APP innovative application. The research results show that the overall score of IIP application in China is 31.76 according to our assessment criterion. The breadth and depth of platform application need to be improved. Furthermore, this study provides guidance to a wide range of potential market participants, such as manufacturing enterprises, service providers and relevant policy makers, to assess the effectiveness of platform application.","2169-3536","","10.1109/ACCESS.2021.3098940","National Natural Science Foundation of China(grant numbers:61771281); National Social Science Fund of China(grant numbers:15ZDB151); National Key Research and Development Program of China(grant numbers:2018YFF0213700); 2018 Industrial Internet Platform Innovation and Development Project of China; 2018 Integrated Standardization of Intelligent Manufacturing and Application of New Models; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9492081","Industrial internet platform;assessment system;manufacturing enterprise","Manufacturing;Internet;Security;Computer architecture;Cloud computing;Technological innovation;Collaboration","","3","","47","CCBY","20 Jul 2021","","","IEEE","IEEE Journals"
"A Comprehensive Survey on SmartNICs: Architectures, Development Models, Applications, and Research Directions","E. F. Kfoury; S. Choueiri; A. Mazloum; A. AlSabeh; J. Gomez; J. Crichigno","College of Engineering and Computing, University of South Carolina, Columbia, SC, USA; College of Engineering and Computing, University of South Carolina, Columbia, SC, USA; College of Engineering and Computing, University of South Carolina, Columbia, SC, USA; College of Engineering and Computing, University of South Carolina, Columbia, SC, USA; College of Engineering and Computing, University of South Carolina, Columbia, SC, USA; College of Engineering and Computing, University of South Carolina, Columbia, SC, USA",IEEE Access,"9 Aug 2024","2024","12","","107297","107336","The end of Moore’s Law and Dennard Scaling has slowed processor improvements in the past decade. While multi-core processors have improved performance, they are limited by the application’s level of parallelism, as prescribed by Amdahl’s Law. This has led to the emergence of domain-specific processors that specialize in a narrow range of functions. Smart Network Interface Cards (SmartNICs) can be seen as a revolutionary technology that combines heterogeneous domain-specific processors and general-purpose cores to offload infrastructure tasks. Despite the impressive advantages of SmartNICs and their importance in modern networks, the literature has been missing a comprehensive survey. To this end, this paper provides a background encompassing an overview of the evolution of NICs from basic to SmartNICs, describing their architectures, development environments, and advantages over legacy NICs. The paper then presents a comprehensive taxonomy of applications offloaded to SmartNICs, covering network, security, storage, and compute functions. Challenges associated with SmartNIC development and deployment are discussed, along with current initiatives and open research issues.","2169-3536","","10.1109/ACCESS.2024.3437203","National Science Foundation (NSF), Office of Advanced Cyberinfrastructure (OAC)(grant numbers:2118311,2403360,2346726); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10620201","SmartNIC;data processing unit (DPU);infrastructure processing unit (IPU);Moore’s law;application offloading;P4;application specific integrated circuit (ASIC);field programmable gate array (FPGA)","Surveys;Program processors;Computer architecture;Hardware;Software;Engines;Switches;Data processing;Moore's Law;Application specific integrated circuits;Field programmable gate arrays","","3","","222","CCBY","1 Aug 2024","","","IEEE","IEEE Journals"
"Resource Management From Single-Domain 5G to End-to-End 6G Network Slicing: A Survey","S. Ebrahimi; F. Bouali; O. C. L. Haas","Centre for Future Transport and Cities, Coventry University, Coventry, U.K.; Centre for Future Transport and Cities, Coventry University, Coventry, U.K.; Centre for Future Transport and Cities, Coventry University, Coventry, U.K.",IEEE Communications Surveys & Tutorials,"21 Nov 2024","2024","26","4","2836","2866","Network Slicing (NS) is one of the pillars of the fifth/sixth generation (5G/6G) of mobile networks. It provides the means for Mobile Network Operators (MNOs) to leverage physical infrastructure across different technological domains to support different applications. This survey analyzes the progress made on NS resource management across these domains, with a focus on the interdependence between domains and unique issues that arise in cross-domain and End-to-End (E2E) settings. Based on a generic problem formulation, NS resource management functionalities (e.g., resource allocation and orchestration) are examined across domains, revealing their limits when applied separately per domain. The appropriateness of different problem-solving methodologies is critically analyzed, and practical insights are provided, explaining how resource management should be rethought in cross-domain and E2E contexts. Furthermore, the latest advancements are reported through a detailed analysis of the most relevant research projects and experimental testbeds. Finally, the core issues facing NS resource management are dissected, and the most pertinent research directions are identified, providing practical guidelines for new researchers.","1553-877X","","10.1109/COMST.2024.3390613","Centre for Future Transport and Cities (CFTC), Coventry University, Coventry, U.K.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10504277","Network slicing;end-to-end (E2E);resource management;technological domains;radio access networks (RANs);transport networks (TNs);core networks (CNs);5G/6G networks;orchestration;resource allocation (RA)","Resource management;Surveys;5G mobile communication;Reviews;Research and development;Tutorials;Optimization","","3","","276","IEEE","17 Apr 2024","","","IEEE","IEEE Journals"
"Bringing Energy Efficiency Closer to Application Developers: An Extensible Software Analysis Framework","C. Marantos; L. Papadopoulos; C. P. Lamprakos; K. Salapas; D. Soudris","School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece",IEEE Transactions on Sustainable Computing,"8 Jun 2023","2023","8","2","180","193","Green, sustainable and energy-aware computing terms are gaining more and more attention during the last years. The increasing complexity of Internet of Things (IoT) applications makes energy efficiency an important requirement, imposing new challenges to software developers. Software tools capable of providing energy consumption estimations and identifying optimization opportunities are critical during all the phases of application development. This work proposes a novel framework that targets the energy efficiency at application development level. The proposed framework is implemented as a single user-friendly tool-flow, providing a variety of useful features, such as the estimation of the energy consumption without the need of executing the application on the targeted IoT devices and the estimation of potential gains by GPU acceleration on modern heterogeneous IoT architectures. The proposed methodology provides several novel contributions, such as the combination of static analysis and dynamic instrumentation approaches in order to exploit the advantages of both. The framework is evaluated on widely used benchmarks, achieving increased estimation accuracy (more than 90% for similar architectures and more than 72% for the potential use of the GPU). The effectiveness of the framework is further demonstrated using two industrial use-cases achieving an energy reduction from 91% up to 98%.","2377-3782","","10.1109/TSUSC.2022.3222409","EU's Horizon 2020 Research and Innovation Programme(grant numbers:780572); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9950623","Green computing;energy consumption;static analysis;dynamic instrumentation;software design","Software;Energy consumption;Internet of Things;Hardware;Graphics processing units;Estimation;Codes","","3","","60","IEEE","15 Nov 2022","","","IEEE","IEEE Journals"
"A Survey on Spatio-Temporal Big Data Analytics Ecosystem: Resource Management, Processing Platform, and Applications","H. Liang; Z. Zhang; C. Hu; Y. Gong; D. Cheng","School of Computer Science, Wuhan University, Wuhan, Hubei, China; School of Computer Science, Wuhan University, Wuhan, Hubei, China; School of Computer Science, Wuhan University, Wuhan, Hubei, China; School of Computer Science, Wuhan University, Wuhan, Hubei, China; School of Computer Science, Wuhan University, Wuhan, Hubei, China",IEEE Transactions on Big Data,"13 Mar 2024","2024","10","2","174","193","With the rapid evolution of the Internet, Internet of Things (IoT), and geographic information systems (GIS), spatio-temporal Big Data (STBD) is experiencing exponential growth, marking the onset of the STBD era. Recent studies have concentrated on developing algorithms and techniques for the collection, management, storage, processing, analysis, and visualization of STBD. Researchers have made significant advancements by enhancing STBD handling techniques, creating novel systems, and integrating spatio-temporal support into existing systems. However, these studies often neglect resource management and system optimization, crucial factors for enhancing the efficiency of STBD processing and applications. Additionally, the transition of STBD to the innovative Cloud-Edge-End unified computing system needs to be noticed. In this survey, we comprehensively explore the entire ecosystem of STBD analytics systems. We delineate the STBD analytics ecosystem and categorize the technologies used to process GIS data into five modules: STBD, computation resources, processing platform, resource management, and applications. Specifically, we subdivide STBD and its applications into geoscience-oriented and human-social activity-oriented. Within the processing platform module, we further categorize it into the data management layer (DBMS-GIS), data processing layer (BigData-GIS), data analysis layer (AI-GIS), and cloud native layer (Cloud-GIS). The resource management module and each layer in the processing platform are classified into three categories: task-oriented, resource-oriented, and cloud-based. Finally, we propose research agendas for potential future developments.","2332-7790","","10.1109/TBDATA.2023.3342619","National Key Research and Development Program of China(grant numbers:2023YFE0205700); National Natural Science Foundation of China(grant numbers:62302348); Fundamental Research Funds for the Central Universities(grant numbers:2042023kf0132); Hubei Provincial Natural Science Foundation of Chinan(grant numbers:2023AFB831); Hubei Luojia Laboratory(grant numbers:220100016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10356753","Artificial intelligence framework;Big Data system;cloud platform;database management system;geographic information system;resource management;spatio-temporal Big Data","Resource management;Big Data;Cloud computing;Geographic information systems;Artificial intelligence;Ecosystems;Data processing","","3","","130","IEEE","13 Dec 2023","","","IEEE","IEEE Journals"
"Edge-Cloud Architectures for Hybrid Energy Management Systems: A Comprehensive Review","O. Boiko; A. Komin; R. Malekian; P. Davidsson","Department of Information Technologies, Sumy State University, Sumy, Ukraine; Department of Information Technologies, Sumy State University, Sumy, Ukraine; Department of Computer Science and Media Technology and the Internet of Things and People Research Centre, Malmö University, Malmö, Sweden; Department of Computer Science and Media Technology and the Internet of Things and People Research Centre, Malmö University, Malmö, Sweden",IEEE Sensors Journal,"15 May 2024","2024","24","10","15748","15772","This article provides an overview of recent research on edge-cloud architectures in hybrid energy management systems (HEMSs). It delves into the typical structure of an IoT system, consisting of three key layers: the perception layer, the network layer, and the application layer. The edge-cloud architecture adds two more layers: the middleware layer and the business layer. This article also addresses challenges in the proposed architecture, including standardization, scalability, security, privacy, regulatory compliance, and infrastructure maintenance. Privacy concerns can hinder the adoption of HEMS. Therefore, we also provide an overview of these concerns and recent research on edge-cloud solutions for HEMS that addresses them. This article concludes by discussing the future trends of edge-cloud architectures for HEMS. These trends include increased use of artificial intelligence on an edge level to improve the performance and reliability of HEMS and the use of blockchain to improve the security and privacy of edge-cloud computing systems.","1558-1748","","10.1109/JSEN.2024.3382390","J. Gust. Richert stiftelse (Sweco Sverige) for the project Multi-Objective Optimization and Optimal Integration of Hybrid Energy Systems(grant numbers:01003438636); Kamprad Family Foundation (Familjen Kamprads Stiftelse) for the project Models of Distributed Information Processing in Smart Grid Systems; Knowledge Foundation (Stiftelsen för kunskapsoch kompetensutveckling) for the project Intelligent and Trustworthy IoT Systems(grant numbers:20220087-H-01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10492690","Distributed computing;distributed energy;domestic energy consumption;edge computing;edge intelligence;hybrid renewable energy systems;Internet of Energy;power systems;renewable energy sources;residential energy consumption;sustainable development;systems architecture;trustworthiness;user data privacy","Computer architecture;Cloud computing;Security;Renewable energy sources;Edge computing;Smart grids;Distributed computing;Sustainable development;Systems architecture;Data privacy","","3","","146","IEEE","4 Apr 2024","","","IEEE","IEEE Journals"
"Configurable Harris Hawks Optimisation for Application Placement in Space-Air-Ground Integrated Networks","N. Akhter; R. Mahmud; J. Jin; J. But; I. Ahmad; Y. Xiang","School of Science, Computing and Engineering Technologies, Swinburne University of Technology, Melbourne, VIC, Australia; School of Electrical Engineering, Computing and Mathematical Sciences, Curtin University, Perth, WA, Australia; School of Science, Computing and Engineering Technologies, Swinburne University of Technology, Melbourne, VIC, Australia; School of Science, Computing and Engineering Technologies, Swinburne University of Technology, Melbourne, VIC, Australia; School of Engineering, Edith Cowan University, Perth, WA, Australia; School of Information Technology, Deakin University, Geelong, VIC, Australia",IEEE Transactions on Network and Service Management,"15 Apr 2024","2024","21","2","1724","1736","Space-Air-Ground Integrated Network (SAGIN) has recently emerged as a viable solution for reliable transmission, high data rates, and seamless connectivity with extensive coverage. However, the characteristics of the computation and communication devices located at various levels of SAGIN make application placement within such environments a challenging task. Real-time service expectations and resource requirements of applications further intensify this issue, and push the domain to operate beyond its capacity, resulting in uneven delays and significant overhead. Taking these constraints into account, SAGIN’s application placement problem can be expressed as a multiobjective optimisation problem. This paper aims to solve such a problem using a Dynamic Weight-configurable Harris Hawks Optimisation (DW-HHO) algorithm, considering diverse application contexts such as deadlines, resource usage and the number of application activities. It simultaneously minimises application total service time and host resource overhead with a robust global search. The performance of the proposed solution is compared with benchmark metaheuristic solutions such as PSO, NSGA-II, Greedy and Random. Experimental results demonstrate that DW-HHO outperforms other benchmark metaheuristic solutions in optimising resource utilisation and service delivery time of applications in SAGIN environments. The proposed DW-HHO demonstrates notable improvements over existing methods. Specifically, when evaluating the total service time for PSO, NSGA-II, Greedy, and Random, DW-HHO outperforms these methods by 7.28%, 9.07%, 13.01%, and 14.97%, respectively.","1932-4537","","10.1109/TNSM.2024.3361176","Australian Research Council Linkage Project(grant numbers:LP190100594); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10418264","Application placement;Harris Hawks optimisation (HHO);space-air-ground integrated network (SAGIN)","Space-air-ground integrated networks;Resource management;Computer architecture;Satellites;Delays;Computational modeling;Australia","","3","","31","IEEE","1 Feb 2024","","","IEEE","IEEE Journals"
"Enhancing Rescue Operations With Virtualized Mobile Multimedia Services in Scarce Resource Devices","A. Atutxa; J. Astorga; M. Huarte; E. Jacob; J. Unzilla","Department of Communications Engineering, Faculty of Engineering, University of the Basque Country, Bilbao, Spain; Department of Communications Engineering, Faculty of Engineering, University of the Basque Country, Bilbao, Spain; Department of Communications Engineering, Faculty of Engineering, University of the Basque Country, Bilbao, Spain; Department of Communications Engineering, Faculty of Engineering, University of the Basque Country, Bilbao, Spain; Department of Communications Engineering, Faculty of Engineering, University of the Basque Country, Bilbao, Spain",IEEE Access,"8 Dec 2020","2020","8","","216029","216042","The aim of this article is to present an architecture to support reconfigurable multimedia services for a practical emergency environment use case of rescue operations. Specifically, radio communication and video surveillance services are provided by means of a small device carried by a mobile vehicle. This solution is validated by implementing a digital mobile radio (DMR) standard radio hotspot and a video streaming server in an unmanned aerial vehicle (UAV) that conveys a device with scarce resources in order to minimize power consumption. To achieve a fast and flexible deployment of the envisioned services, a virtualization-based approach is proposed. Namely, a Kubernetes orchestrator is used to manage the life-cycle of services deployed on a small resource device, endowing the architecture with scalability and management flexibility. This article describes the executed performance tests that measured key parameters such as deployment time and recovery time after disconnection. Highly promising results were obtained, showing that the proposed architecture can be deployed in less than 4 minutes and can recover from network disconnections in less than 10 seconds. Thus, the performance, reliability and flexibility of the overall solution are demonstrated.","2169-3536","","10.1109/ACCESS.2020.3041394","Spanish Ministry of Economy, Industry and Competitiveness through the State Secretariat for Research, Development and Innovation under the Adaptive Management of 5G Services to Support Critical Events in Cities (5G-City) Project(grant numbers:TEC2016-76795-C6-5-R); Department of Economic Development and Competitiveness of the Basque Government through the 5G4BRIS Research Project(grant numbers:KK-2020/00031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272962","Application virtualization;emergency services;multimedia communication;radio communication","Streaming media;Containers;Virtualization;Performance evaluation;Time measurement;Reliability;Multimedia communication","","3","","33","CCBY","30 Nov 2020","","","IEEE","IEEE Journals"
"$\mu$μ-DDRL: A QoS-Aware Distributed Deep Reinforcement Learning Technique for Service Offloading in Fog Computing Environments","M. Goudarzi; M. A. Rodriguez; M. Sarvi; R. Buyya","Cloud Computing and Distributed Systems (CLOUDS) Laboratory, School of Computing and Information Systems, University of Melbourne, Parkville, VIC, Australia; School of Computing and Information Systems, University of Melbourne, Parkville, VIC, Australia; Department of Infrastructure Engineering, University of Melbourne, Parkville, VIC, Australia; Cloud Computing and Distributed Systems (CLOUDS) Laboratory, School of Computing and Information Systems, University of Melbourne, Parkville, VIC, Australia",IEEE Transactions on Services Computing,"5 Feb 2024","2024","17","1","47","59","Fog and Edge computing extend cloud services to the proximity of end users, allowing many Internet of Things (IoT) use cases, particularly latency-critical applications. Smart devices, such as traffic and surveillance cameras, often do not have sufficient resources to process computation-intensive and latency-critical services. Hence, the constituent parts of services can be offloaded to nearby Edge/Fog resources for processing and storage. However, making offloading decisions for complex services in highly stochastic and dynamic environments is an important, yet difficult task. Recently, Deep Reinforcement Learning (DRL) has been used in many complex service offloading problems; however, existing techniques are most suitable for centralized environments, and their convergence to the best-suitable solutions is slow. In addition, constituent parts of services often have predefined data dependencies and quality of service constraints, which further intensify the complexity of service offloading. To solve these issues, we propose a distributed DRL technique following the actor-critic architecture based on Asynchronous Proximal Policy Optimization (APPO) to achieve efficient and diverse distributed experience trajectory generation. Also, we employ PPO clipping and V-trace techniques for off-policy correction for faster convergence to the most suitable service offloading solutions. The results obtained demonstrate that our technique converges quickly, offers high scalability and adaptability, and outperforms its counterparts by improving the execution time of heterogeneous services.","1939-1374","","10.1109/TSC.2023.3332308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10316602","Deep Reinforcement Learning (DRL);Fog/Edge Computing;Internet of Things (IoT);QoS- aware service Offloading","Edge computing;Internet of Things;Quality of service;Trajectory;Servers;Stochastic processes;Costs","","3","","48","IEEE","13 Nov 2023","","","IEEE","IEEE Journals"
"Cybertwin Based Cloud Native Networks","Q. Yu; D. Liang; M. Qin; J. Chen; H. Zhou; J. Ren; Y. Li; J. Wu; Y. Gao; W. Zhang","Peng Cheng Laboratory, Shenzhen 518055, China; Peng Cheng Laboratory, Shenzhen 518055, China; Peng Cheng Laboratory, Shenzhen 518055, China; Peng Cheng Laboratory, Shenzhen 518055, China; School of Electronic Science and Engineering, Nanjing University, Nanjing 210008, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu 611730, China; Peng Cheng Laboratory, Shenzhen 518055, China; School of Computer Science, Fudan University, Shanghai 200433, China; Intelligent Networking and Computing Research Center and School of Computer Science, Fudan University, Shanghai 200433, China; School of Electrical Engineering and Telecommunications, University of New South Wales, Sydney 2052, Australia",Journal of Communications and Information Networks,"4 Oct 2023","2023","8","3","187","202","With the emerging applications of the Internet of things, artificial intelligence, and satellite communications, the future network will be featured as the Internet of everything around the globe. The network heterogeneity, applications cloudification, and personalized user services demand a revolutionary change in the network architecture. With the rapid development of cloud native technology, the new network should support heterogeneous networks and personalized quality of services for users. In this paper, we propose a Cybertwin-based cloud native network (CCNN) that merges the radio access network (RAN), the IP bearer network, and the data center network and is based on the cloud native data center network using Kubernetes as a network operating system for unified virtualization of computing, storage, and network resources, unified scheduling and allocation, and unified operation and management. Then, we propose a fully decoupled RAN architecture that can flexibly and efficiently utilize the resource for personlized user services. We also propose a Cybertwin-based management framework built on Kubernetes for integrated networking, computing and storage resource scheduling. Finally, we design an immunology-inspired intrinsic security architecture with zero trust security system and adaptive defense system. The proposed CCNN is a new network architecture expected to address future generation communications and networks challenges.","2509-3312","","10.23919/JCIN.2023.10272347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10272347","6G;network architecture;Cybertwin;cloud native;network security","Cloud computing;5G mobile communication;Wireless fidelity;Internet;Servers;Computer architecture;Network architecture","","3","","","","4 Oct 2023","","","PTP","PTP Journals"
"From Digital Twins to Digital Twin Prototypes: Concepts, Formalization, and Applications","A. Barbie; W. Hasselbring","Software Engineering Group, Kiel University, Kiel, Germany; Software Engineering Group, Kiel University, Kiel, Germany",IEEE Access,"31 May 2024","2024","12","","75337","75365","The transformation to Industry 4.0 also transforms the processes of developing intelligent manufacturing production systems. Digital twins may be employed to advance the development of these new (embedded) software systems. However, there is no consensual definition of what a digital twin is. In this paper, we provide an overview of the current state of the digital twin concept and formalize the digital twin concept using the Object-Z notation. This formalization includes the concepts of physical twins, digital models, digital templates, digital threads, digital shadows, digital twins, and digital twin prototypes. The relationships between all these concepts are visualized as class diagrams using the Unified Modeling Language. Our digital twin prototype approach supports engineers in the development and automated testing of complex embedded software systems. This approach enables engineers to test embedded software systems in a virtual context without the need of a connection to a physical object. In continuous integration/continuous deployment pipelines, such digital twin prototypes can be used for automated integration testing and, thus, allow for an agile verification and validation process. In this paper, we demonstrate and report on the application and implementation of a digital twin using the example of two real-world field studies (ocean observation systems and smart farming). For independent replication and extension of our approach by other researchers, we provide a laboratory study published open source on GitHub.","2169-3536","","10.1109/ACCESS.2024.3406510","Deutsche Forschungsgemeinschaft (DFG) through Open Access Publikationskosten; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10540125","Cyber-physical systems;embedded software systems;digital twin prototypes;automated testing;continuous integration","Digital twins;Unified modeling language;Embedded software;Testing;NASA;Mathematical models;Industrial Internet of Things;Cyber-physical systems;Testing;Automatic testing","","3","","68","CCBY","28 May 2024","","","IEEE","IEEE Journals"
"Enabling Service-Oriented Manufacturing Through Architectures, Models, and Protocols","S. Gaiardelli; S. Spellini; M. Panato; C. Tadiello; M. Lora; D. S. Cheng; F. Fummi","Department of Engineering for Innovation Medicine (DIMI), University of Verona, Verona, Italy; FACTORYAL S.r.l., San Giovanni Lupatoto, Italy; FACTORYAL S.r.l., San Giovanni Lupatoto, Italy; FACTORYAL S.r.l., San Giovanni Lupatoto, Italy; Department of Engineering for Innovation Medicine (DIMI), University of Verona, Verona, Italy; Department of Engineering for Innovation Medicine (DIMI), University of Verona, Verona, Italy; Department of Engineering for Innovation Medicine (DIMI), University of Verona, Verona, Italy",IEEE Access,"21 Jun 2024","2024","12","","85259","85274","Modern production lines are often composed of machinery from different vendors that must be connected with each other to increase the overall interoperability and flexibility. A common problem that arises in such systems is the complexity of the configuration task: they usually require each component to be manually configured. Thus, machinery requires different configuration strategies, negatively impacting scalability and increasing the chance of human errors. Furthermore, each time the manufacturing system has to be updated, the entire procedure must be repeated. This paper proposes a software architecture abstracting the complexity of existing production lines by enabling the service-oriented manufacturing paradigm. Then, it presents a strategy to model manufacturing systems, covering the topology of the production plant, machinery, and production recipes. The paper also proposes a model-based methodology to automatically configure the reference software architecture and hence the machines in the system. The application of the contributions to a fully-fledged production line shows the effectiveness of relying on model-based automatic configuration.","2169-3536","","10.1109/ACCESS.2024.3385634","European Union’s Horizon 2020 Research and Innovation Program through the Marie Skłodowska-Curie(grant numbers:894237); European Union Next-GenerationEU (Piano Nazionale di Ripresa e Resilienza (PNRR)—Missione 4, Componente 2, and Investimento 1.5—D.D. 1058 23/06/2022) through the PNRR Research Activities of the Consortium Interconnected North-Est Innovation Ecosystem (iNEST)(grant numbers:ECS_00000043); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10493040","Smart manufacturing;computer aided manufacturing;flexible manufacturing systems;software architecture;systems modeling","Computer architecture;Manufacturing;Unified modeling language;Automation;Servers;Task analysis;Smart manufacturing;Computer aided manufacturing;Flexible manufacturing systems;Software architecture;Systems engineering and theory","","3","","34","CCBY","8 Apr 2024","","","IEEE","IEEE Journals"
"An Efficient Algorithm for Microservice Placement in Cloud-Edge Collaborative Computing Environment","X. He; H. Xu; X. Xu; Y. Chen; Z. Wang","Faculty of Computing, Harbin Institute of Technology, Harbin, China; Faculty of Computing, Harbin Institute of Technology, Harbin, China; Faculty of Computing, Harbin Institute of Technology, Harbin, China; Faculty of Computing, Harbin Institute of Technology, Harbin, China; Faculty of Computing, Harbin Institute of Technology, Harbin, China",IEEE Transactions on Services Computing,"8 Oct 2024","2024","17","5","1983","1997","Microservices along with cloud-edge computing technologies are widely adopted to take advantage of the abundant computing resources of the cloud and the low latency, high bandwidth capabilities of the edge. However, factors such as frequent user requirement changes have made the current deployment scheme not fully adaptable to new requirements, resulting in an increase in average response time. Therefore, the microservice system needs to adjust the microservice deployment scheme online in response to continuously changing user requirements to reduce the average response time, which is known as the cloud-edge collaborative microservice deployment problem. However, existing methods are not able to meet the efficiency requirements and do not fully consider the complex dependencies of microservices and budget constraints in the cloud. To address this problem, this paper proposes a solution to the cloud-edge collaborative microservice deployment problem by modeling it as an NP-hard integer nonlinear programming problem in a cloud-edge environment consisting of private edge clouds and public cloud. An efficient Two-stage Iterated Greedy Optimization (TIGO) algorithm is also proposed and its convergence is proven. Extensive experimental results show that this approach achieves better average response times in less time compared to existing methods.","1939-1374","","10.1109/TSC.2024.3399650","National Key Research and Development Program of China(grant numbers:2022ZD0115404); National Natural Science Foundation of China(grant numbers:62372140,61832014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10528881","Cloud-Edge computing;microservice system;service deployment;complex dependencies","Microservice architectures;Cloud computing;Time factors;Servers;Costs;Containers;Quality of service","","3","","48","IEEE","10 May 2024","","","IEEE","IEEE Journals"
"A Comparative Study in the Standardization of IoT Devices Using Geospatial Web Standards","D. Marsh-Hunn; S. Trilles; A. González-Pérez; J. Torres-Sospedra; F. Ramos","GeOps Spatial Web, Freiburg, Germany; Institute of New Imaging Technologies, Universitat Jaume I, Castellón de la Plana, Spain; Institute of New Imaging Technologies, Universitat Jaume I, Castellón de la Plana, Spain; UBIK Geospatial Solutions S.L., Castellón de la Plana, Spain; Institute of New Imaging Technologies, Universitat Jaume I, Castellón de la Plana, Spain",IEEE Sensors Journal,"18 Jan 2021","2021","21","4","5512","5528","Although billions of devices are embedded in the World Wide Web through the Internet of Things, there is still a lack of a common, interoperable way to connect them and make them interact seamlessly. IoT has also found its way into the spatial web. Environmental monitoring and sensing platforms connected over the web by wireless sensor networks are now a common way to monitor natural phenomena. This study compares two open Web Standards (OGC's Sensor Observation Service and SensorThings API) from the geospatial point of view. An IoT platform, called SEnviro, is used to integrate and evaluate implementations for each standard and contrast their qualitative and quantitative traits. The results of the study show that the SensorThings API proves to be the adequate Web Standard for IoT applications in terms of interoperability. It outperforms the contesting Web Standard in terms of flexibility and scalability, which strongly impacts on developer and user experience.","1558-1748","","10.1109/JSEN.2020.3031315","Postdoctoral Juan de la Cierva Fellowship Programme of the Spanish Ministry for Science and Innovation(grant numbers:IJC2018-035017-I); Subvenciones para la Realización de Proyectos de I+D+i Desarrollados por Grupos de Investigación Emergentes programme(grant numbers:GV/2020/035); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9224992","Internet of Things;interoperability;geospatial standards;sensors","Standards;Internet of Things;Geospatial analysis;Interoperability;Sensor phenomena and characterization","","3","","30","IEEE","15 Oct 2020","","","IEEE","IEEE Journals"
"Designing a 6G Testbed for Location: Use Cases, Challenges, Enablers and Requirements","E. J. Khatib; C. S. Álvarez-Merino; H. Q. Luo-Chen; R. B. Moreno","E.T.S.I. de Telecomunicación, Telecommunication Research Institute (TELMA), Universidad de Málaga, Málaga, Spain; E.T.S.I. de Telecomunicación, Telecommunication Research Institute (TELMA), Universidad de Málaga, Málaga, Spain; E.T.S.I. de Telecomunicación, Telecommunication Research Institute (TELMA), Universidad de Málaga, Málaga, Spain; E.T.S.I. de Telecomunicación, Telecommunication Research Institute (TELMA), Universidad de Málaga, Málaga, Spain",IEEE Access,"3 Feb 2023","2023","11","","10053","10091","Location will have a central role in Research and Development (R&D) towards 6G networks, both as a service offered by the network (improving the current offering of 5G) and as an input to increasingly location-aware services and network functions. To integrate location into 6G standards, it will be very important to design validation systems such as testbeds, even when the actual technology is not yet commercially available. This paper performs a review of the use cases and their requirements, enabling technologies in 6G, and challenges; and proposes a flexible testbed architecture for performing network location related R&D. This architecture will allow to deploy an evolving infrastructure which will allow early validation of 6G technologies.","2169-3536","","10.1109/ACCESS.2023.3240775","Horizon 2020 Project LOCalization and analytics on-demand embedded in the 5G ecosystem, for Ubiquitous vertical applicationS (LOCUS) from the European Union(grant numbers:871249); Junta de Andalucia(grant numbers:PY18-4647:PENTA); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10032155","Testbed;proof-of-concept;B5G;6G;location;positioning","6G mobile communication;5G mobile communication;Computer architecture;Research and development;Wireless fidelity;Global navigation satellite system","","2","","270","CCBY","30 Jan 2023","","","IEEE","IEEE Journals"
"Digital Thread for Smart Products: A Survey on Technologies, Challenges, and Opportunities in Service-Oriented Supply Chains","D. Bianchini; T. Fapanni; M. Garda; F. Leotta; M. Mecella; A. Rula; E. Sardini","Department of Information Engineering, University of Brescia, Brescia, Italy; Department of Information Engineering, University of Brescia, Brescia, Italy; Department of Information Engineering, University of Brescia, Brescia, Italy; Department of Computer, Control and Management Engineering, University of Rome “La Sapienza,”, Rome, Italy; Department of Computer, Control and Management Engineering, University of Rome “La Sapienza,”, Rome, Italy; Department of Information Engineering, University of Brescia, Brescia, Italy; Department of Information Engineering, University of Brescia, Brescia, Italy",IEEE Access,"17 Sep 2024","2024","12","","125284","125305","In Smart Manufacturing, the recent opportunities provided by the Information and Communication Technologies have paved the way to a seamless connection of the manufactured product throughout its entire lifecycle, leading to the diffusion of the concepts of Smart Product and Digital Thread, which leverage digital technologies to assure a continuous flow of data encompassing the design phase of a product, manufacturing, operation, maintenance and also its eventual disposal or recycling. This compelling need to obtain a unified view of information associated with Smart Products has stimulated the so-called Internet of Services (IoS) paradigm, allowing for the sharing of products data and the execution of functions among various participants in intertwined supply chains. In these contexts, service-oriented architectures are being more and more employed to meet the complex and ever-evolving data analysis requirements, particularly when implementing Digital Thread solutions for Smart Products, where several issues must be considered, ranging from the heterogeneity of (Big) data to data sovereignty and data access policies, as information may cross the borders of multiple actors participating in intertwined supply chains. This survey discusses about the technological solutions and challenges to implement Digital Threads for Smart Products in Smart Manufacturing contexts, providing insights on opportunities for future research directions. In addition, the survey proposes a comprehensive multi-tier service-oriented architectural model to jointly tackle (Big) data heterogeneity, data sovereignty and data access policies issues, as they are only partially addressed by the research efforts examined in the literature review.","2169-3536","","10.1109/ACCESS.2024.3454375","Next-GenerationEU (Italian Piano Nazionale di Ripresa e Resilienza (PNRR)—M4 C2, Invest 1.3—D.D. 1551.11-10-2022)(grant numbers:PE00000004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10664538","Service-oriented architectures;smart products;digital thread;Internet of Services;cyber-physical production network;smart factory","Surveys;Smart manufacturing;Computer architecture;Supply chains;Instruction sets;Distributed databases;Data models;Service-oriented architecture;Product delivery;Cyber-physical systems","","2","","141","CCBYNCND","4 Sep 2024","","","IEEE","IEEE Journals"
"Dependency-Aware Microservice Deployment for Edge Computing: A Deep Reinforcement Learning Approach With Network Representation","C. Wang; H. Yu; X. Li; F. Ma; X. Wang; T. Taleb; V. C. M. Leung","College of Computer Science and Software Engineering, Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen University, Shenzhen, China; ICTFicial Oy, Espoo, Finland; School of Big Data & Software Engineering, Chongqing University, Chongqing, China; Shenzhen Institute, Tsinghua University, Shenzhen, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; Faculty of Electrical Engineering and Information Technology, Ruhr University Bochum, Bochum, Germany; College of Computer Science & Software Engineering, Shenzhen University, Shenzhen, China",IEEE Transactions on Mobile Computing,"6 Nov 2024","2024","23","12","14737","14753","The popularity of microservices in industry has sparked much attention in the research community. Despite significant progress in microservice deployment for resource-intensive services and applications at the network edge, the intricate dependencies among microservices are often overlooked, and some studies underestimate the importance of system context extraction in deployment strategies. This paper addresses these issues by formulating the microservice deployment problem as a max-min problem, considering system cost and quality of service (QoS) jointly. We first study the attention-based microservice representation (AMR) method to achieve effective system context extraction. In this way, the contributions of different computing power providers (users, edge servers, or cloud servers) in the networks can be effectively paid attention to. Subsequently, we propose the attention-modified soft actor-critic (ASAC) algorithm to tackle the microservice deployment problem. ASAC leverages attention mechanisms to enhance decision-making and adapt to changing system dynamics. Our simulation results demonstrate ASAC's effectiveness, prioritizing average system cost and reward compared to the other state-of-the-art algorithms.","1558-0660","","10.1109/TMC.2024.3453069","Science and Technology Innovation Key R&D Program of Chongqing(grant numbers:CSTB2023TIAD-STX0035); National NSFC(grant numbers:62372072,62072332); Chongqing Research Program of Basic Research and Frontier Technology(grant numbers:cstc2022ycjhbgzxm0058); Regional Science and Technology Innovation Cooperation Project of Chengdu City(grant numbers:2023-YF11-00023-HZ); Tianjin Xinchuang Haihe Lab(grant numbers:22HHXCJC00002); European Union's(grant numbers:101139172); AerOS(grant numbers:101069732); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10663201","Attention mechanism;deep reinforcement learning;dependency-aware;edge computing;microservice deployment;network representation","Microservice architectures;Servers;Heuristic algorithms;Decision making;Computer architecture;Cloud computing;Quality of service","","2","","44","IEEE","2 Sep 2024","","","IEEE","IEEE Journals"
"RRFT: A Rank-Based Resource Aware Fault Tolerant Strategy for Cloud Platforms","C. K. Dehury; P. K. Sahoo; B. Veeravalli","Mobile & Cloud Lab, Institute of Computer Science, University of Tartu, Tartu, Estonia; Department of Computer Science and Information Engineering, Chang Gung University, Guishan, Taiwan; Department of Electrical and Computer Engineering, National University of Singapore, Singapore, Singapore",IEEE Transactions on Cloud Computing,"6 Jun 2023","2023","11","2","1257","1272","The applications that are deployed in the cloud to provide services to the users encompass a large number of interconnected dependent cloud components. Multiple identical components are scheduled to run concurrently in order to handle unexpected failures and provide uninterrupted service to the end user, which introduces resource overhead problem for the cloud service provider. Furthermore such resource-intensive fault tolerant strategies bring extra monetary overhead to the cloud service provider and eventually to the cloud users. In order to address these issues, a novel fault tolerant strategy based on the significance level of each component is developed. The communication topology among the application components, their historical performance, failure rate, failure impact on other components, dependencies among them, etc., are used to rank those application components to further decide on the importance of one component over others. Based on the rank, a Markov Decision Process (MDP) model is presented to determine the number of replicas that varies from one component to another. A rigorous performance evaluation is carried out using some of the most common practically useful metrics such as, recovery time upon a fault, average number of components needed, number of parallel components successfully executed, etc., to quote a few, with similar component ranking and fault tolerant strategies. Simulation results demonstrate that the proposed algorithm reduces the required number of virtual and physical machines by approximately 10% and 4.2%, respectively, compared to other similar algorithms.","2168-7161","","10.1109/TCC.2021.3126677","Ministry of Science and Technology, Taiwan(grant numbers:110-2221-E-182-008-MY3); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609649","Cloud computing;component ranking;fault tolerance;Markov decision process","Cloud computing;Fault tolerant systems;Fault tolerance;Servers;Software;Task analysis;Software as a service","","2","","34","IEEE","9 Nov 2021","","","IEEE","IEEE Journals"
"Fenrir: Blockchain-Based Inter-Company App-Store for the Automotive Industry","D. F. Blanco; F. le Mouël; T. Lin","Univ Lyon, INSA Lyon, Inria, CITI, EA3720, Villeurbanne, France; Univ Lyon, INSA Lyon, Inria, CITI, EA3720, Villeurbanne, France; Stellantis, Velizy-Villacoublay, France",IEEE Access,"29 Nov 2022","2022","10","","122933","122953","From a software evolution perspective, more actors are integrating the in-vehicle software development cycle. In this process, software deployment mechanisms must include more complex techniques to meet the software verification and traceability levels required by industry safety and security constraints. In this context, we propose Fenrir, a public inter-automaker blockchain-based application store framework in which each automaker retains software installability control. This application store also aims to ensure traceability and security, while also keeping the solution light in terms of both energy consumption and computing requirements, to be used in constrained environments.We implemented Fenrir in a heterogeneous architecture composed by both on-board (bearing an ARM Cortex-A53 chipset, already deployed in cars) and off-board (Amazon EC2) nodes for a realistic automotive use-case scenario, in which we evaluated the performance and energy consumption. We demonstrate that the overheads added by our solution for an entire software deployment pipeline—comprising both deployment and usage of already deployed software packages—depends mainly on the verification mechanism, whose impact is not significant, i.e., 3.8% for the worst-case scenario and 0.3% for a typical scenario.","2169-3536","","10.1109/ACCESS.2022.3223130","STELLANTIS under the collaborative framework OpenLab VAT@Lyon, involving STELLANTIS and CITI Laboratory (ANRT)(grant numbers:2020/1415); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9954374","Automotive;application store;blockchain;distributed systems;multi-provider;software dependency management;software deployment","Blockchains;Automotive engineering;Computer architecture;Standards;Sensors;Actuators;Security;Software engineering;Distributed processing","","2","","51","CCBY","17 Nov 2022","","","IEEE","IEEE Journals"
"Optimal Resource Allocation Using Genetic Algorithm in Container-Based Heterogeneous Cloud","Q. -H. Chen; C. -Y. Wen","Department of Electrical Engineering, National Chung Hsing University, Taichung, Taiwan; Department of Electrical Engineering, National Chung Hsing University, Taichung, Taiwan",IEEE Access,"17 Jan 2024","2024","12","","7413","7429","This paper tackles the complex problem of optimizing resource configuration for microservice management in heterogeneous cloud environments. To address this challenge, an enhanced framework, the multi-objective microservice allocation (MOMA) algorithm, is developed to formulate the efficient resource management of cloud microservice resources as a constrained optimization problem, guided by resource utilization and network communication overhead, which are two important factors in microservice resource allocation. The proposed framework simplifies the deployment of cloud services and streamlines workload monitoring and analysis within a diverse cloud system. A comprehensive comparison is made between the effectiveness of the proposed algorithm and existing algorithms on real-world datasets, with a focus on resource balancing, network overhead, and network reliability. Experimental results reveal that the proposed algorithm significantly enhances resource utilization, reduces network transmission overhead, and improves reliability.","2169-3536","","10.1109/ACCESS.2024.3351944","Smart Sustainable New Agriculture Research Center (SMARTer), National Science and Technology Council, Taiwan(grant numbers:111-2634-F-005-001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385169","Resource allocation;genetic algorithm;container-based heterogeneous cloud;multi-objective optimization;microservice","Resource management;Microservice architectures;Scheduling;Optimization;Monitoring;Genetic algorithms;Clustering algorithms","","2","","59","CCBY","9 Jan 2024","","","IEEE","IEEE Journals"
"A Hierarchical Vehicular-Based Architecture for Vehicular Networks: A Case Study on Computation Offloading","T. Liu; J. Wang; B. Kim; J. Xie; Z. Han","Nanjing Institute of Technology, Nanjing, China; School of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; InfoTech Labs, Toyota Motor North America, Mountain View, CA, USA; Department of Electrical and Computer Engineering, University of North Carolina at Charlotte, Charlotte, NC~, USA; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA",IEEE Access,"15 Oct 2020","2020","8","","184273","184283","In order to realize an intelligent transportation system (ITS) which will provide smooth urban traffic, autonomous driving, accurate route navigation, etc., enormous computations need to be migrated from cloud centers to edge nodes, especially for the services requiring stringent latency. In addition to base stations and road side units (RSUs), vehicles can be alteratively considered as a kind of computation resources. In this article, a hierarchical vehicular-based architecture which consists of cloud centers and vehicles is investigated. Computation offloading performance in the hierarchical architecture is also studied. In specific, the main components in vehicular networks and their characteristics on communication and computations are presented firstly. Several communication techniques that are essential in enabling computation offloading among these components are then discussed. Secondly, a hierarchical vehicular-based architecture, which integrates the main components, is constructed. Thirdly, a case study on computation offloading in the proposed architecture is conducted. In the concerned scenario, the computation offloading problem is modelled as a multi-dimensional multiple knapsack problem (MMKP). Two algorithms are investigated, among which, the first algorithm is a greedy heuristic method providing a sub-optimal solution with a low computational complexity. The second algorithm is a modified branch and bound (B&B) method, which can obtain the best solution with a high computational complexity. Numerical results are also presented to verify the performance of the two algorithms. It can be demonstrated that the proposed architecture can migrate more computations from cloud centers to vehicular nodes, when the computations require more communication resources.","2169-3536","","10.1109/ACCESS.2020.3029169","National Natural Science Foundation of China(grant numbers:61702258,62002166); Key Projects of Natural Science Research in Colleges and Universities of Jiangsu Province(grant numbers:19KJA410001); Foundation of Jiangsu Advanced Numerical Control Technology Key Laboratory(grant numbers:SYKJ201901,NSF EARS-1839818,CNS-1717454,CNS-1731424,CNS-1702850,CNS-1718666,CNS-1731675,CNS-1910667,CNS-1910891,CNS-2025284); China Postdoctoral Science Foundation(grant numbers:2020M671483); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9214822","Hierarchical vehicular architecture;intelligent transportation system;computation offloading;multi-dimensional multiple knapsack problem;branch and bound algorithm","Computer architecture;Task analysis;Edge computing;Cloud computing;Servers;Electronic mail;Computational modeling","","2","","53","CCBYNCND","6 Oct 2020","","","IEEE","IEEE Journals"
"Integrating Bayesian Optimization and Machine Learning for the Optimal Configuration of Cloud Systems","B. Guindani; D. Ardagna; A. Guglielmi; R. Rocco; G. Palermo","Department of Electronics, Information, and Bioengineering, Politecnico di Milano, Milano, Italy; Department of Electronics, Information, and Bioengineering, Politecnico di Milano, Milano, Italy; Department of Mathematics, Politecnico di Milano, Milano, Italy; Department of Electronics, Information, and Bioengineering, Politecnico di Milano, Milano, Italy; Department of Electronics, Information, and Bioengineering, Politecnico di Milano, Milano, Italy",IEEE Transactions on Cloud Computing,"7 Mar 2024","2024","12","1","277","294","Bayesian Optimization (BO) is an efficient method for finding optimal cloud configurations for several types of applications. On the other hand, Machine Learning (ML) can provide helpful knowledge about the application at hand thanks to its predicting capabilities. This work proposes a general approach based on BO, which integrates elements from ML techniques in multiple ways, to find an optimal configuration of recurring jobs running in public and private cloud environments, possibly subject to black-box constraints, e.g., application execution time or accuracy. We test our approach by considering several use cases, including edge computing, scientific computing, and Big Data applications. Results show that our solution outperforms other state-of-the-art black-box techniques, including classical autotuning and BO- and ML-based algorithms, reducing the number of unfeasible executions and corresponding costs up to 2–4 times.","2168-7161","","10.1109/TCC.2024.3361070","MUR(grant numbers:2023-2027); European Commission(grant numbers:956137); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10418550","Acquisition function;Bayesian optimization;black-box optimization;machine learning","Cloud computing;Optimization;Closed box;Computational modeling;Costs;Software algorithms;Bayes methods","","2","","48","CCBYNCND","1 Feb 2024","","","IEEE","IEEE Journals"
"Real-Time Adaptive Anomaly Detection in Industrial IoT Environments","M. Raeiszadeh; A. Ebrahimzadeh; R. H. Glitho; J. Eker; R. A. F. Mini","Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada; Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada; Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada; Ericsson Research, Lund, Sweden; Ericsson Research, Lund, Sweden",IEEE Transactions on Network and Service Management,"19 Dec 2024","2024","21","6","6839","6856","To ensure reliability and service availability, next-generation networks are expected to rely on automated anomaly detection systems powered by advanced machine learning methods with the capability of handling multi-dimensional data. Such multi-dimensional, heterogeneous data occurs mostly in today’s Industrial Internet of Things (IIoT), where real-time detection of anomalies is critical to prevent impending failures and resolve them in a timely manner. However, existing anomaly detection methods often fall short of effectively coping with the complexity and dynamism of multi-dimensional data streams in IIoT. In this paper, we propose an adaptive method for detecting anomalies in IIoT streaming data utilizing a multi-source prediction model and concept drift adaptation. The proposed anomaly detection algorithm merges a prediction model into a novel drift adaptation method resulting in accurate and efficient anomaly detection that exhibits improved scalability. Our trace-driven evaluations indicate that the proposed method outperforms the state-of-the-art anomaly detection methods by achieving up to an 89.71% accuracy (in terms of Area under the Curve (AUC)) while meeting the given efficiency and scalability requirements.","1932-4537","","10.1109/TNSM.2024.3447532","Ericsson/ENCQOR-5G Senior Industrial Research Chair on Cloud and Edge Computing for 5G and Beyond; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10643234","Anomaly detection;real-time analytics;concept drift;streaming data;Industrial Internet of Things (IIoT)","Anomaly detection;Industrial Internet of Things;Real-time systems;Concept drift;Predictive models;Autoregressive processes;Accuracy","","2","","41","IEEE","21 Aug 2024","","","IEEE","IEEE Journals"
"User Classification and Traffic Steering in O-RAN","R. Ntassah; G. Michele Dell’Area; F. Granelli","DISI, University of Trento, Trento, Italy; Telecom Italia, Turin, Italy; DISI, University of Trento, Trento, Italy",IEEE Open Journal of the Communications Society,"28 Jun 2024","2024","5","","3581","3594","The O-RAN architectural framework enables the application of AI/ML techniques for traffic steering and load balancing. Indeed, an effective steering technique is crucial to avoiding ping-pong and radio link failure. Limited observability and network complexity make it challenging to understand individual user needs. Consequently, traffic steering methods struggle to make optimal decisions, resulting in performance degradation due to unnecessary handovers. Motivated by this, we present an xApp for the RAN intelligence controller (RIC) for user equipment (UE) steering to ensure an even load distribution among cells while maintaining an acceptable throughput level. We propose an ML-aided traffic steering technique. The proposed method comprises three phases: UE classification, downlink (DL) throughput prediction, and a traffic steering (TS) technique. A support vector machine (SVM) is used for UE classification, followed by cell throughput prediction using ensemble Long Short-Term Memory (E-LSTM). The TS algorithm uses the information from the ML models to initiate handovers (HO). The SVM model identifies UEs with low throughput, while the E-LSTM predicts cell DL throughput to provide information about potential target cells for these UEs. Experimental results demonstrate that the proposed method achieves an even load distribution of UEs in 60.25% of the cells with few handovers, while also significantly improving UE throughput.","2644-125X","","10.1109/OJCOMS.2024.3413590","Telecom Italia in the framework of the UniversiTIM Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555357","Bagging;DL throughput;load balancing;LSTM;O-RAN;supervised learning;SVM;traffic steering;xApp","Throughput;Predictive models;Support vector machines;Load modeling;Radio frequency;Long short term memory;Handover","","2","","63","CCBY","12 Jun 2024","","","IEEE","IEEE Journals"
"Edge Intelligence for Rendering Green Camera-Network-as-a-Service","N. A. Singh; A. Roy; S. Misra","Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Advanced Technology Development Centre, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India",IEEE Transactions on Green Communications and Networking,"16 Feb 2022","2022","6","1","365","375","This work proposes the Green Camera-Network-as-a-Service (G-CNaaS) architecture, which provides on-demand camera networks to multiple end-users simultaneously while utilizing minimal energy. G-CNaaS simultaneously reduces the carbon footprint and eliminates the single application-centric approach of traditional camera networks (TCNs) by enabling each camera to participate in multiple Virtual-Camera-Networks (VCNs) and selecting an optimal set of cameras for each VCN. We couple each camera node in every VCNs with a learning model suitable for the requested application. We assign an intelligent edge device to each VCN to analyze time-sensitive events. We introduce the camera selection factor by leveraging the properties of cameras: 1) field-of-view (FoV); 2) angular-distance; 3) observation range; and 4) residual energy to select the optimal camera set. The results of the extensive simulation of the G-CNaaS architecture show that it excels in performance concerning attributes such as the average lifetime, fair distribution of the work among the camera owners, and cost-effectiveness compared to the TCNs. We observe that the expenditure of a user using the TCN varies by 88.7%, while in the case of G-CNaaS, the expenditure varies by 10.28% with the increase in time from 1–60 months. On the other hand, the average energy consumed increases by 59.88% and 99.5% in the presence of 10 and 20 camera sensor owners.","2473-2400","","10.1109/TGCN.2021.3126276","Department of Science and Technology—INSPIRE Fellowship (Sanction letter no. DST/INSPIRE Fellowship/2017/1F170547), Government of India; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9606213","Edge intelligence;green computing;virtual camera network;network services;virtualization;cost effectiveness","Cameras;Sensors;Streaming media;Carbon footprint;Surveillance;Virtualization;Real-time systems","","2","","25","IEEE","8 Nov 2021","","","IEEE","IEEE Journals"
"Resource Choreography in Cyber-Physical-Social Systems: Representation, Modeling and Execution","F. He; L. Shen; X. Peng","Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University, Shanghai, China; Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University, Shanghai, China; Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University, Shanghai, China",IEEE Transactions on Services Computing,"6 Feb 2023","2023","16","1","550","563","Myriad of heterogeneous resources are widely distributed in the cyber, physical and social spaces. These resources are integrated by software to form diverse Cyber-Physical-Social Systems (CPSSs). Among them, the CPSSs in the form of resource choreography is receiving more attention. Traditional software development methods may not be suitable for constructing and executing CPSS applications (CPSS-Apps) with the characteristics of loosely-coupled resource collaboration and spatial-temporal constraints sensitive. In this article we propose a comprehensive framework to support resource choreography from the perspectives of representation, modeling and execution. In the framework, a CPSS-App is represented by an application model conforming to a meta-model. An application model is generated by a multi-scene storyboard modeling tool. The model is further used to customize the capability units acting as the abstract unit of the resources providing the same service. An architecture following the microservice style is applied to achieve the choreography of the capability units by asynchronous message communication while a resource is determined by application-level service discovery. The framework is evaluated through a human experiment. The results show that the application construction and execution by the framework is feasible. The modeling tool is usable and the execution architecture is scalable in different environment settings.","1939-1374","","10.1109/TSC.2021.3138637","National Key R&D Program of China(grant numbers:2018YFB1004803); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664383","Cyber-physical-social;modeling;execution;microservice","Software;Collaboration;Computer architecture;Microservice architectures;Motion pictures;Medical services;Couplings","","2","","60","IEEE","28 Dec 2021","","","IEEE","IEEE Journals"
"DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems","Z. Wang; S. Zhu; J. Li; W. Jiang; K. K. Ramakrishnan; M. Yan; X. Zhang; A. X. Liu","Key Laboratory of Dependable Service Computing in Cyber Physical Society, Ministry of Education, and the School of Big Data and Software Engineering, Chongqing University, Chongqing, China; Ant Group, Hangzhou, China; Ant Group, Hangzhou, China; Ant Group, Hangzhou, China; Department of Computer Science and Engineering, University of California at Riverside, Riverside, CA, USA; School of Big Data and Software Engineering, Chongqing University, Chongqing, China; School of Big Data and Software Engineering, Chongqing University, Chongqing, China; Ant Group, Hangzhou, China",IEEE/ACM Transactions on Networking,"16 Oct 2024","2024","32","5","3961","3976","Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.","1558-2566","","10.1109/TNET.2024.3400953","National Key Research and Development Project(grant numbers:2021YFB1714200); Fundamental Research Funds for the Central Universities(grant numbers:2023CDJKYJH013); Chongqing Technology Innovation and Application Development Project(grant numbers:CSTB2022TIAD-STX0007,CSTB2022TIAD-KPX0067); Postdoctoral Foundation of Chongqing(grant numbers:2020LY13); Research Fund from the Ant Group; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10542703","Microservices autoscaling;cloud systems;horizontal autoscaling;service quality","Microservice architectures;Measurement;Production;Task analysis;Servers;Resource management;Predictive models","","2","","50","IEEE","31 May 2024","","","IEEE","IEEE Journals"
"Internet of Things in Smart Cities: Comprehensive Review, Open Issues, and Challenges","E. H. Houssein; M. A. Othman; W. M. Mohamed; M. Younan","Faculty of Computers and Information, Minia University, Minia, Egypt; Faculty of Computers and Information, Minia University, Minia, Egypt; Faculty of Computers and Information, Minia University, Minia, Egypt; Faculty of Computers and Information, Minia University, Minia, Egypt",IEEE Internet of Things Journal,"25 Oct 2024","2024","11","21","34941","34952","Smart cities rely mainly on the Internet of Things (IoT) to make an urban area smart to offer its citizens a high quality of life with optimal use of resources and preservation of the environment. IoT is the key component that collects raw data on the surrounding environment to be analyzed to extract information that supports decision making. The widespread use of IoT results in the emergence of smart homes, smart energy, smart transportation, and smart healthcare, which build a smart city. On the other hand, challenges, such as heterogeneity, scalability, security, and privacy, hinder the efficient functioning of the IoT in the construction of smart cities. This article presents a comprehensive overview on the concept of IoT moving forward to the concept of smart city, highlighting key elements and characteristics, studying and reviewing state-of-the-art research on this theme. Future directions are discussed to guide researchers, who focus on interoperability between IoT platforms in smart cities and on IoT architectures based on micro-services. Case studies of successful smart cities are presented for gaining learned lessons. The impact of integrating wireless networks (5G and 6G) in the IoT is also clarified in the future direction. The significance of this research is found in its comprehensive examination of various aspects of the smart city instead of concentrating on a singular facet.","2327-4662","","10.1109/JIOT.2024.3449753","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646337","Internet of Things (IoT);smart city;smart energy;smart healthcare;smart transportation","Smart cities;Internet of Things;Reviews;Security;Market research;Sustainable development;Smart transportation","","2","","135","IEEE","26 Aug 2024","","","IEEE","IEEE Journals"
"UMBRELLA: A One-Stop Shop Bridging the Gap From Lab to Real-World IoT Experimentation","I. Mavromatis; Y. Jin; A. Stanoev; A. Portelli; I. Weeks; B. Holden; E. Glasspole; T. Farnham; A. Khan; U. Raza; A. Aijaz; T. Bierton; I. Seto; N. Patel; M. Sooriyabandara","Digital Catapult, London, U.K.; Bristol Research and Innovation Laboratory, Toshiba Europe Ltd., Avon, Bristol, U.K.; Nordic Semiconductor, Avon, Bristol, U.K.; Turtle Beach, Basingstoke, Hampshire, U.K.; Bristol Research and Innovation Laboratory, Toshiba Europe Ltd., Avon, Bristol, U.K.; Q5D Technologies Ltd., Portishead, North Somerset, U.K.; Amiosec Ltd., Tewkesbury, Gloucestershire, U.K.; Bristol Research and Innovation Laboratory, Toshiba Europe Ltd., Avon, Bristol, U.K.; Bristol Research and Innovation Laboratory, Toshiba Europe Ltd., Avon, Bristol, U.K.; Waymap, London, U.K.; Bristol Research and Innovation Laboratory, Toshiba Europe Ltd., Avon, Bristol, U.K.; Bristol Research and Innovation Laboratory, Toshiba Europe Ltd., Avon, Bristol, U.K.; Corporate Research and Development Centre, Toshiba Corporation, Saiwai-ku, Kawasaki-shi, Japan; South Gloucestershire Council, Yate, Gloucester, U.K.; Bristol Research and Innovation Laboratory, Toshiba Europe Ltd., Avon, Bristol, U.K.",IEEE Access,"27 Mar 2024","2024","12","","42181","42213","UMBRELLA (A Living Laboratory: https://www.umbrellaiot.com/) is an open, large-scale IoT ecosystem deployed across South Gloucestershire, UK. It is intended to accelerate innovation across multiple technology domains. UMBRELLA is built to bridge the gap between existing specialised testbeds and address holistically real-world technological challenges in a System-of-Systems (SoS) fashion. UMBRELLA provides open access to real-world devices and infrastructure, enabling researchers and the industry to evaluate solutions for Smart Cities, Robotics, Wireless Communications, Edge Intelligence, and more. Key features include over 200 multi-sensor nodes installed on public infrastructure, a robotics arena with 20 mobile robots, a 5G network-in-a-box solution, and a unified backend platform for management, control and secure user access. The heterogeneity of hardware components, including diverse sensors, communication interfaces, and GPU-enabled edge devices, coupled with tools like digital twins, allows for comprehensive experimentation and benchmarking of innovative solutions unviable in lab environments. This paper provides a comprehensive overview of UMBRELLA’s multi-domain architecture and capabilities, making it an ideal playground for Internet of Things (IoT) and Industrial IoT (IIoT) innovation. It discusses the challenges in designing, developing and operating UMBRELLA as an open, sustainable testbed and shares lessons learned to guide similar future initiatives. With its unique openness, heterogeneity, realism and tools, UMBRELLA aims to continue accelerating cutting-edge technology research, development and translation into real-world progress.","2169-3536","","10.1109/ACCESS.2024.3377662","West of England Local Enterprise Partnership (LEP) Local Growth Fund administered by the West of England Combined Authority (WECA) through UMBRELLA; South Gloucestershire Council; Bristol Research Innovation Laboratory (BRIL), Toshiba Europe Ltd; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10472978","IoT;IIoT;testbed;experimentation;system-of-systems;wireless;robots;smart cities","Robots;Wireless sensor networks;Wireless communication;Sensors;Temperature sensors;Robot sensing systems;Ecosystems;Internet of Things;System of systems;Smart cities","","2","","68","CCBYNCND","18 Mar 2024","","","IEEE","IEEE Journals"
"A Smart CIoT With Secure Healthcare Framework Using Optimized Deep Recuperator Neural Network Long Short-Term Memory","C. Narmatha; S. Manimurugan; P. Karthikeyan","Faculty of Computers and Information Technology, University of Tabuk, Tabuk, Saudi Arabia; Faculty of Computers and Information Technology, University of Tabuk, Tabuk, Saudi Arabia; Department of Computer Science and Information Engineering, National Chung Cheng University, Chiayi, Taiwan",IEEE Internet of Things Journal,"8 Mar 2024","2024","11","6","10551","10562","The healthcare system currently relies on the facility to store and process large amounts of health data, supported by efficient management. The Internet of Things (IoT) has driven the growth of Adroit Healthcare, which has vast data processing capabilities and extensive data collection. The consumer IoT (CIoT), also known as the IoT in the context of individual use cases, is dominated by personal healthcare applications. The remarkable expansion of the CIoT can be attributed to the extensive embrace of wearable technology, facilitating everyday monitoring of vital health indicators like blood pressure, heart rate, and respiration rate. However, the CIoT raises concerns about data security, confidentiality, and customer trust. This study proposes an integrating blockchain and deep learning (DL) approach to promote responsible use of CIoT. The study develops an Intelligent IoT (IIoT) and healthcare diagnostic model, named BT-PWO-DRNN-LSTM, using blockchain technology and Prairie Wolf optimization-based deep recuperator neural network (DRNN) and long short-term memory (LSTM). The BT-PWO-DRNN-LSTM approach includes three key processes: 1) encrypted transactions; 2) cryptographic hash feature; and 3) medical diagnosis. The PWO technique is used for secure communication of medical images, the neighborhood catalogue disposition (NCD) technique is used in the cryptographic hash feature operation (CHFO), and the DL approach is employed as a classification algorithm for diagnosing disorders. The use of PWO methodology for secure healthcare communication and optimal parameter fine-tuning highlights the originality of the study. The BT-PWO-DRNN-LSTM prototype showed positive results with high sensitivity, specificity, and accuracy during the diagnostic process.","2327-4662","","10.1109/JIOT.2023.3326547","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290899","Blockchain technology (BCT);consumer IoT (CIoT);cryptographic hash function;deep learning (DL);deep recuperator neural network (DRNN)-long short-term memory (LSTM);Internet of Things (IoT);Prairie Wolf (PW) optimization","Internet of Things;Medical services;Blockchains;Security;Protocols;Cryptography;Biomedical monitoring","","2","","31","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"AI/ML Enabled Automation System for Software Defined Disaggregated Open Radio Access Networks: Transforming Telecommunication Business","S. Kumar","Institute for Communication Systems, University of Surrey, Guildford, UK",Big Data Mining and Analytics,"22 Apr 2024","2024","7","2","271","293","Open Air Interface (OAI) alliance recently introduced a new disaggregated Open Radio Access Networks (O-RAN) framework for next generation telecommunications and networks. This disaggregated architecture is open, automated, software defined, virtual, and supports the latest advanced technologies like Artificial Intelligence (AI) Machine Learning (AI/ML). This novel intelligent architecture enables programmers to design and customize automated applications according to the business needs and to improve quality of service in fifth generation (5G) and Beyond 5G (B5G). Its disaggregated and multivendor nature gives the opportunity to new startups and small vendors to participate and provide cheap hardware software solutions to keep the market competitive. This paper presents the disaggregated and programmable O-RAN architecture focused on automation, AI/ML services, and applications with Flexible Radio access network Intelligent Controller (FRIC). We schematically demonstrate the reinforcement learning, external applications (xApps), and automation steps to implement this disaggregated O-RAN architecture. The idea of this research paper is to implement an AI/ML enabled automation system for software defined disaggregated O-RAN, which monitors, manages, and performs AI/ML-related services, including the model deployment, optimization, inference, and training.","2097-406X","","10.26599/BDMA.2023.9020033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10506767","Open Radio Access Networks (O-RAN);Flexible Radio access network Intelligent Controller (FRIC);Reinforcement Learning (RL);external Applications (xApps);Artificial Intelligence (AI);Machine Learning (ML);sixth generation (6G)","Training;Automation;5G mobile communication;Computer architecture;Reinforcement learning;Quality of service;Software","","2","","64","","22 Apr 2024","","","TUP","TUP Journals"
"JITA4DS: Disaggregated Execution of Data Science Pipelines Between the Edge and the Data Centre","G. Vargas-Solar; M. S. Hassan; A. Akoglu","French Council of Scientific Research (CNRS)-LIRIS, France; University of Arizona, USA; University of Arizona, USA",Journal of Web Engineering,"22 Sep 2023","2022","21","1","1","26","This paper targets the execution of data science (DS) pipelines supported by data processing, transmission and sharing across several resources executing greedy processes. Current data science pipelines environments provide various infrastructure services with computing resources such as general-purpose processors (GPP), Graphics Processing Units (GPUs), Field Programmable Gate Arrays (FPGAs) and Tensor Processing Unit (TPU) coupled with platform and software services to design, run and maintain DS pipelines. These one-fits-all solutions impose the complete externalization of data pipeline tasks. However, some tasks can be executed in the edge, and the backend can provide just in time resources to ensure ad-hoc and elastic execution environments. This paper introduces an innovative composable “Just in Time Architecture” for configuring DCs for Data Science Pipelines (JITA-4DS) and associated resource management techniques. JITA-4DS is a cross-layer man-agement system that is aware of both the application characteristics and the underlying infrastructures to break the barriers between applications, middleware/operating system, and hardware layers. Vertical integration of these layers is needed for building a customizable Virtual Data Center (VDC) to meet the dynamically changing data science pipelines' requirements such as performance, availability, and energy consumption. Accordingly, the paper shows an experimental simulation devoted to run data science workloads and determine the best strategies for scheduling the allocation of resources implemented by JITA-4DS.","1544-5976","","10.13052/jwe1540-9589.2111","National Science Foundation(grant numbers:NSF CNS-1624668); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10243631","Disaggregated data centers;data science pipelines;edge computing","Data centers;Tensors;Processor scheduling;Pipelines;Data science;Logic gates;Software","","2","","28","","22 Sep 2023","","","River Publishers","River Publishers Journals"
"Multimodal Renewable Energy Hybrid Supply Optimization Model Based on Heterogeneous Cloud Wireless Access","F. Tian; H. Wang; H. Jiang; B. Zhao","College of Information, Shenyang Institute of Engineering, Shenyang, China; College of Information, Shenyang Institute of Engineering, Shenyang, China; School of Renewable Energy, Shenyang Institute of Engineering, Shenyang, China; Innovation and Entrepreneurship Education Center, Shenyang Institute of Engineering, Shenyang, China",IEEE Access,"6 Jun 2024","2024","12","","78286","78303","With the increasing emphasis on environmental issues, the utilization of renewable energy has been recognized as a feasible solution to address the energy crisis and reduce environmental pollution. In view of this, this article proposes a multi-modal renewable energy hybrid power supply optimization model based on heterogeneous cloud wireless access. The model innovatively combines heterogeneous cloud wireless access technology and various intelligent optimization algorithms, including k-clustering algorithm, particle swarm optimization algorithm, and whale optimization algorithm, forming a hybrid optimization algorithm. In order to comprehensively evaluate the actual performance of the model, this study recruited 20 experts to provide detailed ratings on four core dimensions: cost-benefit ratio, reliability, robustness, and user satisfaction. The results showed that the model scored 95.1, 96.4, 95.6, and 96.2 in the four dimensions of cost-benefit ratio, reliability indicators, robustness, and user satisfaction, respectively. This series of significant data not only confirms the theoretical superiority of the model, but also demonstrates its strong potential and practical value in practical applications. In summary, this study provides a promising and innovative solution for the field of renewable energy supply.","2169-3536","","10.1109/ACCESS.2024.3407726","Department of Education of Liaoning Province 2023 Basic Scientific Research Projects of Universities: “Research on Key Technologies for Coordinated Regulation of Multi Load and Clean Energy Based on Deep Learning”(grant numbers:JYTMS20230313); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10542731","K-clustering algorithm;heterogeneous cloud radio access;particle swarm optimization algorithm;WOA algorithm;energy supply optimization","Optimization;Clustering algorithms;Heuristic algorithms;Renewable energy sources;Wireless communication;Cloud computing;Whale optimization algorithms","","2","","30","CCBYNCND","31 May 2024","","","IEEE","IEEE Journals"
"AI-Enhanced Digital Twin Framework for Cyber-Resilient 6G Internet of Vehicles Networks","Y. Yigit; L. A. Maglaras; W. J. Buchanan; B. Canberk; H. Shin; T. Q. Duong","School of Computing, Engineering and The Built Environment, Edinburgh Napier University, Edinburgh, U.K.; School of Computing, Engineering and The Built Environment, Edinburgh Napier University, Edinburgh, U.K.; Blockpass ID Lab, Edinburgh Napier University, Edinburgh, U.K.; School of Computing, Engineering and The Built Environment, Edinburgh Napier University, Edinburgh, U.K.; Department of Electronics and Information Convergence Engineering, Kyung Hee University, Yongin, South Korea; Faculty of Engineering and Applied Science, Memorial University, St. John’s, NL, Canada",IEEE Internet of Things Journal,"7 Nov 2024","2024","11","22","36168","36181","Digital twin technology is crucial to the development of the sixth-generation (6G) Internet of Vehicles (IoV) as it allows the monitoring and assessment of the dynamic and complicated vehicular environment. However, 6G IoV networks have critical challenges in network security and computational efficiency, which need to be addressed. Existing digital twin technologies in 6G IoV networks often suffer from limitations, such as reliance on static models and high computational demands, leading to unstable attack detection and inefficiencies. Their results for attack detection performance metrics, precision, detection rate, and F1-Score are insufficient for 6G IoV. Moreover, these systems concentrate all computational processes within the digital twin’s service layer, leading to inefficiencies. To address these challenges, we introduce a novel artificial intelligence (AI) enhanced digital twin framework designed to significantly improve 6G IoV network security and computational efficiency under dynamic conditions. Our framework employs an advanced feature engineering module that uses feature selection methods and stacked sparse autoencoders (ssAE) to reduce feature dimensions within the cyber twin layer, effectively distributing the overall computational load. It also utilizes an online learning module which enables a network-aware attack detection mechanism for precise attack detection. The proposed solution exhibits a stable performance of around 98% success rate regarding attack detection metrics against two data sets. Specifically, our solution reduces system latency by 12%, energy consumption by 15%, RAM usage by 20%, and improves packet delivery rates by 6.1%. These findings underscore the potential of our framework to enhance the robustness and responsiveness of 6G IoV systems, offering a significant contribution to vehicular network security and management.","2327-4662","","10.1109/JIOT.2024.3455089","Scientific and Technological Research Council of Turkey (TUBITAK) 1515 Frontier Research and Development Laboratories Support Program for BTS Advanced AI Hub: BTS Autonomous Networks and Data Innovation Lab Project(grant numbers:5239903); National Research Foundation of Korea (NRF) Grant funded by the Korean Government (MSIT)(grant numbers:NRF-2022R1A4A3033401); Canada Excellence Research Chair (CERC)(grant numbers:CERC-2022-00109); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10673900","Artificial intelligence (AI);digital twin;Internet of Vehicles (IoV);ITS;security;vehicular ad-hoc network (VANET)","6G mobile communication;Digital twins;Vehicle dynamics;Feature extraction;Artificial intelligence;Measurement;Heuristic algorithms","","2","","33","IEEE","10 Sep 2024","","","IEEE","IEEE Journals"
"Exploring the Synergy of Fog Computing, Blockchain, and Federated Learning for IoT Applications: A Systematic Literature Review","W. V. Solis; J. Marcelo Parra-Ullauri; A. Kertesz","Software Engineering Department, University of Szeged, Szeged, Hungary; Smart Internet Laboratory, University of Bristol, Bristol, U.K.; Software Engineering Department, University of Szeged, Szeged, Hungary",IEEE Access,"17 May 2024","2024","12","","68015","68060","The proliferation of Internet of Things (IoT) applications poses formidable challenges in managing data processing, privacy, and security. In response, technologies such as Fog Computing (FC), Blockchain (BC), and Federated Learning (FL) have emerged as promising solutions. Combining these technologies can broaden their scope, and impose novel challenges. This paper conducts a Systematic Literature Review (SLR) to investigate their integration within the IoT domain, systematically evaluating the current state-of-the-art by analyzing 40 papers against 38 extraction criteria, encompassing technical characteristics specific to FC, BC, FL, or their integration. The findings offer insights into the advantages, challenges, opportunities, and limitations of this integration, addressing data processing, privacy, and security concerns in IoT. By filling a research gap and directly examining FC, BC, and FL interoperability across architectural layers, this study contributes to knowledge expansion in the field. This paper proposes a novel framework for implementing FL and BC within FC environments for IoT applications, alongside a comprehensive synthesis of existing literature, distinguishing it from previous research efforts. Furthermore, it offers valuable insights into the current landscape, identifies research needs, and proposes future research directions. The framework and literature synthesis provided allow readers to access customized information on FC-BC-FL integration, aiding in designing and implementing robust IoT solutions.","2169-3536","","10.1109/ACCESS.2024.3398034","Ministry of Culture and Innovation of Hungary through the National Research, Development and Innovation Fund(grant numbers:TKP2021-NVA); National Project(grant numbers:TKP2021-NVA-09); Connecting Education and Research Communities for an Innovative Resource Aware Society (CERCIRAS) European Cooperation in Science and Technology (COST) Action(grant numbers:CA19135); COST Association, along with the University of Szeged Open Access Fund(grant numbers:6858); U.K. Government (GOV) Department of Science, Innovation, and Technology (DSIT) [Future Open Networks Research Challenge (FONRC)] Project REASON; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10522660","Blockchain;edge computing;federated learning;fog computing;Internet of Things;systematic literature review","Internet of Things;Blockchains;Bibliographies;Edge computing;Systematics;Computer architecture;Federated learning;Federated learning;Edge computing;Reviews;Database systems","","2","","112","CCBYNCND","8 May 2024","","","IEEE","IEEE Journals"
"Dynamic, Context-Aware Cross-Layer Orchestration of Containerized Applications","R. C. Sofia; D. Dykeman; P. Urbanetz; A. Galal; D. A. Dave","fortiss GmbH, Research Institute of the Free State of Bavaria for Software Intensive Systems and Services, Munich, Germany; Zurich Research Laboratory, IBM Research Europe, Zürich, Switzerland; Zurich Research Laboratory, IBM Research Europe, Zürich, Switzerland; fortiss GmbH, Research Institute of the Free State of Bavaria for Software Intensive Systems and Services, Munich, Germany; fortiss GmbH, Research Institute of the Free State of Bavaria for Software Intensive Systems and Services, Munich, Germany",IEEE Access,"4 Sep 2023","2023","11","","93129","93150","Container orchestration handles the semi-automated management of applications across Edge-Cloud, providing features such as autoscaling, high availability, and portability. Having been developed for Cloud-based applications, container orchestration faces challenges in the context of decentralized Edge-Cloud environments, requiring a higher degree of adaptability in the verge of mobility, heterogeneous networks, and constrained devices. In this context, this perspective paper aims at igniting discussion on the aspects that a dynamic orchestration approach should integrate to support an elastic orchestration of containerized applications. The motivation for the provided perspective focuses on proposing directions to better support challenges faced by next-generation IoT services, such as mobility or privacy preservation, advocating the use of context awareness and a cognitive, cross-layer approach to container orchestration to be able to provide adequate support to next-generation services. A proof of concept (available open source software) of the discussed concept has been implemented in a testbed composed of embedded devices.","2169-3536","","10.1109/ACCESS.2023.3307026","Horizon Europe Cognitive Decentralised Edge Cloud Orchestration (CODECO)(grant numbers:101092696); fortiss-IBM Center for Artificial Intelligence (C4AI) EDGE; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10225530","Context-awareness;IoT;edge computing;machine learning;data observability","Containers;Internet of Things;Next generation networking;Cloud computing;Virtual machine monitors;Dynamic scheduling;Delays;Context awareness;Edge computing;Machine learning;Observability","","2","","57","CCBYNCND","21 Aug 2023","","","IEEE","IEEE Journals"
"On-Board Computer for CubeSats: State-of-the-Art and Future Trends","A. Cratere; L. Gagliardi; G. A. Sanca; F. Golmar; F. Dell’Olio","Department of Electrical and Information Engineering, Micro Nano Sensor Group, Polytechnic University of Bari, Bari, Italy; Instituto de Ciencias Físicas, Universidad de San Martín-CONICET, Buenos Aires, Argentina; Instituto de Ciencias Físicas, Universidad de San Martín-CONICET, Buenos Aires, Argentina; Instituto de Ciencias Físicas, Universidad de San Martín-CONICET, Buenos Aires, Argentina; Department of Electrical and Information Engineering, Micro Nano Sensor Group, Polytechnic University of Bari, Bari, Italy",IEEE Access,"24 Jul 2024","2024","12","","99537","99569","Over the past three decades, the acceptance of higher risk thresholds within the space industry has facilitated the widespread integration of commercial off-the-shelf (COTS) components into avionics and payloads, leading to a remarkable transformation in the design of space missions. This transformation has led to the emergence of the New Space Economy and the widespread adoption of lean or small satellites in general, particularly CubeSats. CubeSats are now widely used in commercial, scientific, and research applications due to their versatility, affordability, simplicity of development, and accelerated development timelines. On-board computing plays a crucial role in the design of CubeSat missions, as increasingly high-performance computational requirements are needed to meet the challenges of future missions. This paper systematically reviews the state-of-the-art of CubeSat Command and Data Handling (C&DH) sub-system, covering both hardware components and flight software (FSW) development frameworks. It presents an analysis of the key features and recent developments of on-board computers (OBCs) in commercial and academic institutional projects funded by governments, agencies and public institutions. It further examines the effects of space radiation on avionics components and discusses the main fault-tolerance techniques used in CubeSat platforms. Finally, this paper highlights trends and hazards for future CubeSat avionics and identify potential directions for future developments in high-performance on-board computing. By synthesizing contemporary research and industry insights, this paper aims to shed light on CubeSat OBC design, providing an overview of the existing technology landscape and the challenges to be addressed for next-generation mission needs.","2169-3536","","10.1109/ACCESS.2024.3428388","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10597570","Command and data handling (C&DH);CubeSats;on-board computer (OBC);small satellite avionics","CubeSat;Payloads;Aerospace electronics;Computer architecture;Space vehicles;Distributed databases;On board unit;Small satellites","","2","","235","CCBYNCND","15 Jul 2024","","","IEEE","IEEE Journals"
"BBAD: Blockchain-Backed Assault Detection for Cyber Physical Systems","M. Anwar; N. Tariq; M. Ashraf; S. A. Moqurrab; B. Alabdullah; H. S. Alsagri; A. Almjally","Department of Avionics Engineering, Air University, Islamabad, Pakistan; Department of Avionics Engineering, Air University, Islamabad, Pakistan; School of Electrical Engineering and Computer Science, National University of Sciences & Technology, Islamabad, Pakistan; School of Computing, Gachon University, Sujeong-gu, Seongnam-si, South Korea; Department of Information Systems, College of Computer and Information Sciences, Princess Nourah Bint Abdulrahman University, P. O. Box 88428, Riyadh, Saudi Arabia; Information Systems Department, College of Computer and Information Sciences, Imam Mohammad Ibn Saud Islamic University, Riyadh, Saudi Arabia; Information Technology, College of Computer and Information Sciences, Imam Mohammad Ibn Saud Islamic University, Riyadh, Saudi Arabia",IEEE Access,"29 Jul 2024","2024","12","","101878","101894","Cybersecurity challenges pose a significant threat to Healthcare Cyber Physical Systems (CPS) because they heavily rely on wireless communication. Particularly, jamming attacks can severely disrupt the integrity of these CPS networks. This research introduces a decentralized system to address this issue. Therefore, this paper suggested a system that leverages trust and blockchain technology to detect jamming attacks in healthcare CPS effectively. It proposes a layered model to improve CPS networks’ lifetime and performance. In smart healthcare environments, it ensures secure and reliable communication between sensor nodes, wearable sensors, medical devices, and monitoring systems. Results show that the suggested approach outperforms the baseline model in identifying and minimizing jamming assaults, with an average percentage difference of 15.71% more detection rate, 20.21% less packet loss rates, 16.65% less node-level energy consumption, reduced network latency of 8.29%, and 9.63% more network throughput.","2169-3536","","10.1109/ACCESS.2024.3404656","Princess Nourah Bint Abdulrahman University, Riyadh, Saudi Arabia, through the Researchers Supporting(grant numbers:PNURSP2024R440); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10537615","Smart healthcare;cyber-physical systems;jamming attacks;sensor nodes;trust","Jamming;Medical services;Security;Blockchains;Wireless communication;Reliability;Interference;Smart healthcare;Cyber-physical systems;Trust management","","2","","59","CCBYNCND","23 May 2024","","","IEEE","IEEE Journals"
"Open RAN: A Concise Overview","M. Wani; M. Kretschmer; B. Schröder; A. Grebe; M. Rademacher","Department of Cooperation Systems, Fraunhofer FIT, Sankt Augustin, Germany; Department of Cooperation Systems, Fraunhofer FIT, Sankt Augustin, Germany; Department of Industrial Communication & Data Research, brown-iposs GmbH, Bonn, Germany; Institute on Computer and Communication Technology, Technische Hochschule Köln, Cologne, Germany; Department of Computer Science, Hochschule Bonn-Rhein-Sieg, Sankt Augustin, Germany",IEEE Open Journal of the Communications Society,"8 Jan 2025","2025","6","","13","28","Open RAN has emerged as a transformative approach in the evolution of cellular networks, addressing challenges posed by modern applications and high network density. By leveraging disaggregated, virtualized, and software-based elements interconnected through open standardized interfaces, Open RAN introduces agility, cost-effectiveness, and enhanced competition in the Radio Access Network (RAN) domain. The Open RAN paradigm, driven by the O-RAN Alliance specifications, is set to transform the telecom ecosystem. Despite extensive technical literature, there is a lack of succinct summaries for industry professionals, researchers, and policymakers. This paper addresses this gap by providing a concise, yet comprehensive overview of Open RAN. Compared to previous work, our approach introduces Open RAN by gradually splitting up different components known from previous RAN architectures. We believe that this approach leads to a better understanding for people already familiar with the general concept of mobile communication networks. Building upon this general understanding of Open RAN, we introduce key architectural principles, interfaces, components and use-cases. Moreover, this work investigates potential security implications associated with adopting Open RAN architecture, emphasizing the necessity of robust network protection measures.","2644-125X","","10.1109/OJCOMS.2024.3430823","Federal Ministry for Digital and Transport of the Federal Republic of Germany(grant numbers:165GU054B); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10601697","Open RAN;5G;security;mobile networks","Open RAN;Security;Computer architecture;Hardware;5G mobile communication;Costs;Task analysis","","2","","117","CCBYNCND","18 Jul 2024","","","IEEE","IEEE Journals"
"Phoenix: Transformative Reconfigurability for Edge IoT Devices in Small-Scale IoT Systems","M. Moghaddassian; S. Shafaghi; P. Habibi; A. Leon-Garcia","The Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; The Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; The Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; The Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada",IEEE Access,"15 Dec 2023","2023","11","","137821","137836","Transformative reconfigurability refers to the ability of changing the current software stack of a configurable device by fully replacing its existing one. In the context of IoT systems, such major device reconfigurations can be used to change the role, adapt new functionality, and keep reconfigurable IoT devices compatible with the IoT systems requirements as the ambient technology around them evolve, thus fostering a thriving and continuously-connected IoT environment. In this paper, we introduce Phoenix, an IoT device configuration management system that is designed to automate transformative reconfigurability for edge IoT devices at small scales. Edge IoT devices are typically computationally capable and configurable devices that have enough processing power to run user programs and control sensors and embedded devices in an IoT environment. Enabling transformative reconfigurability for such devices at small scales can increase IoT systems flexibility, efficiency, and adaptability in small IoT environments, for example, agri-farms, smart homes, micro grids, and the like. Phoenix manages the life cycle of edge IoT devices configuration and uses bare-metal provisioning to provide unattended installation of new software stacks that are defined by user intents that instruct the reconfiguration process. We implemented a Phoenix proof-of-concept system and deployed it on the SAVI testbed where we evaluated its performance in reconfiguring a variety of edge IoT devices under different network conditions. Our results indicate that Phoenix can meet the requirements of small-scale heterogeneous IoT systems in various application environments.","2169-3536","","10.1109/ACCESS.2023.3339154","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341245","Bare metal provisioning;infrastructure automation;Internet of Things;reconfigurability","Internet of Things;Engines;Operating systems;Microservice architectures;Metals;Full stack;Fires;Reconfigurable architectures;Edge computing","","2","","66","CCBY","4 Dec 2023","","","IEEE","IEEE Journals"
"Containerized Microservices: A Survey of Resource Management Frameworks","L. M. Al Qassem; T. Stouraitis; E. Damiani; I. M. Elfadel","Center for Secure Cyber Physical Systems and the College of Computing and Mathematical Sciences, Khalifa University, Abu Dhabi, UAE; Center for Secure Cyber Physical Systems and the College of Computing and Mathematical Sciences, Khalifa University, Abu Dhabi, UAE; Center for Secure Cyber Physical Systems and the College of Computing and Mathematical Sciences, Khalifa University, Abu Dhabi, UAE; Center for Secure Cyber Physical Systems and the College of Computing and Mathematical Sciences, Khalifa University, Abu Dhabi, UAE",IEEE Transactions on Network and Service Management,"22 Aug 2024","2024","21","4","3775","3796","The growing adoption of microservice architectures (MSAs) has led to major research and development efforts to address their challenges and improve their performance, reliability, and robustness. Important aspects of MSA that are not sufficiently covered in the open literature include efficient cloud resource allocation and optimal power management. Other aspects of MSA remain widely scattered in the literature, including cost analysis, service level agreements (SLAs), and demand-driven scaling. In this article, we examine recent cloud frameworks for containerized microservices with a focus on efficient resource utilization using auto-scaling. We classify these frameworks on the basis of their resource allocation models and underlying hardware resources. We highlight current MSA trends and identify workload-driven resource sharing within microservice meshes and SLA streamlining as two key areas for future microservice research.","1932-4537","","10.1109/TNSM.2024.3388633","Khalifa University Center of Secure Cyberphysical System; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10499978","Microservices;containers;resource management;container orchestration;machine learning;workload forecasting;reactive allocation;predictive allocation","Microservice architectures;Resource management;Cloud computing;Containers;Service level agreements;Computer architecture;Surveys","","2","","120","IEEE","15 Apr 2024","","","IEEE","IEEE Journals"
"Application-Oriented Cloud Workload Prediction: A Survey and New Perspectives","B. Feng; Z. Ding","Department of Computer Science and Technology, Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, China; Department of Computer Science and Technology, Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, China",Tsinghua Science and Technology,"11 Sep 2024","2025","30","1","34","54","Workload prediction is critical in enabling proactive resource management of cloud applications. Accurate workload prediction is valuable for cloud users and providers as it can effectively guide many practices, such as performance assurance, cost reduction, and energy consumption optimization. However, cloud workload prediction is highly challenging due to the complexity and dynamics of workloads, and various solutions have been proposed to enhance the prediction behavior. This paper aims to provide an in-depth understanding and categorization of existing solutions through extensive literature reviews. Unlike existing surveys, for the first time, we comprehensively sort out and analyze the development landscape of workload prediction from a new perspective, i.e., application-oriented rather than prediction methodologies per se. Specifically, we first introduce the basic features of workload prediction, and then analyze and categorize existing efforts based on two significant characteristics of cloud applications: variability and heterogeneity. Furthermore, we also investigate how workload prediction is applied to resource management. Finally, open research opportunities in workload prediction are highlighted to foster further advancements.","1007-0214","","10.26599/TST.2024.9010024","National Natural Science Foundation of China(grant numbers:62372330); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10676402","cloud computing;workload prediction;resource management;artificial intelligence for IT operations (AIOps)","Surveys;Measurement;Energy consumption;Costs;Reviews;Taxonomy;Predictive models","","1","","117","","11 Sep 2024","","","TUP","TUP Journals"
"A pluggable module for enabling a trusted edge device management system based on microservice","S. -H. Lee; J. -Z. Liu","Department of Intelligent Commerce, National Kaohsiung University of Science and Technology, Kaohsiung, Taiwan; Department of Intelligent Commerce, National Kaohsiung University of Science and Technology, Kaohsiung, Taiwan",Journal of Communications and Networks,"21 Jul 2023","2023","25","3","381","391","In the development of the intelligent Internet of things, edge computing plays a key role. Not only that it has the characteristics of a quick response, but it can also effectively reduce the burden of cloud computing. In addition, it can also extend the application of network edge through the coexistence and collaboration with the cloud system. However, with a large number of edge devices being deployed, the previous remote device management system will face the challenges of resource constraints and software firmware compatibility. In addition, implementing remote fault management, configuration management, accounting management, performance management, and security management through edge device management is a key task. Therefore, this study has designed and implemented a trusted edge device management system based on a microservice and a plug-and-play hardware management device. The running state of the edge device, the functions of remote device control, and system restart are monitored remotely by in- and out-of-band management modes. In addition, in terms of data transmission security, the design concept of a trusted platform module is introduced to realize data encryption and authentication, and ensure security and reliability. In this paper, the operational feasibility of management services based on containerized microservice in edge devices is verified with system benchmarking tools without affecting system performance. According to the experimental results, the proposed architecture in this study can be effective in edge device management.","1976-5541","","10.23919/JCN.2023.000023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10146579","Edge computing;Internet of things;microservice;remote device management","Internet of Things;Microservice architectures;Cloud computing;Hardware;Computer architecture;Protocols;Software","","1","","","","8 Jun 2023","","","KICS","KICS Journals"
"ODTE: A Metric for Digital Twin Entanglement","P. Bellavista; N. Bicocchi; M. Fogli; C. Giannelli; M. Mamei; M. Picone","Department of Computer Science and Engineering (DISI), University of Bologna, Bologna, Italy; Department of Engineering Sciences and Methods, University of Modena and Reggio Emilia, Reggio Emilia, Italy; Department of Mathematics and Computer Science, University of Ferrara, Ferrara, Italy; Department of Mathematics and Computer Science, University of Ferrara, Ferrara, Italy; Department of Engineering Sciences and Methods, University of Modena and Reggio Emilia, Reggio Emilia, Italy; Department of Engineering Sciences and Methods, University of Modena and Reggio Emilia, Reggio Emilia, Italy",IEEE Open Journal of the Communications Society,"26 Apr 2024","2024","5","","2377","2390","Digital Twins (DTs) have recently emerged as a valuable approach for modeling, monitoring, and controlling physical objects in Industrial Internet of Things applications. Measuring the quality of entanglement between the digital and physical counterparts plays a crucial role in the adoption of DTs. In this context, entanglement denotes how well a DT mirrors its counterpart and the extent to which the behavior of the physical counterpart aligns with the commands issued by the DT. In this paper we propose a concise yet expressive and original metric for representing the quality of entanglement, namely Overall Digital Twin Entanglement (ODTE), based on two key factors: timeliness and completeness, i.e., the freshness of the collected data and the ratio between collected and total data, respectively. In addition, the paper describes how we have built our industrial testbed implemented on top of Kubernetes, where we show practical applications of the proposed ODTE metric by highlighting and discussing its benefits in realistic use cases.","2644-125X","","10.1109/OJCOMS.2024.3385659","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10492860","Digital twins;entanglement;industrial IoT","Measurement;Quality of experience;Quality of service;Digital twins;Synchronization;Task analysis;Industrial Internet of Things","","1","","39","CCBYNCND","4 Apr 2024","","","IEEE","IEEE Journals"
"Throughput Maximization for Result Multicasting by Admitting Delay-Aware Tasks in MEC Networks for High-Speed Railways","J. Xu; Z. Wei; X. Yuan; Y. Qiao; Z. Lyu; J. Han","School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; Department of Computer Science and Engineering, University of North Texas, Denton, TX, USA; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China",IEEE Transactions on Vehicular Technology,"19 Jun 2024","2024","73","6","8765","8781","The rapid expansion of high-speed railways (HSRs) and the growing demand for diverse data services during long journeys require efficient computing services. Mobile Edge Computing (MEC) emerged as a promising platform to fulfill this demand. We envision a scenario wherein passengers interact with each other on the same or different trains in real-time by offloading computationally intensive and delay-sensitive tasks to the track-side MEC networks for HSRs and computation results are multicast to the receivers. To improve the quality of data services, we propose a novel approach to optimize network throughput by admitting as many tasks as possible, subject to delay constraints, and multicasting the maximum number of results. The high mobility of trains and the frequent handovers during train-ground communication are factored into our scheme, which presents significant challenges to jointly consider the dynamic multicast grouping and admission/rejection policies for tasks/results. We introduce the multi-group-shared Group Steiner tree (GST) model and propose an efficient heuristic algorithm that reduces the multicast routing problem to finding a GST for each candidate cloudlet. The effectiveness of our proposed algorithm is demonstrated through simulations and the results are promising.","1939-9359","","10.1109/TVT.2024.3357769","Natural Science Foundation of Anhui Province(grant numbers:2108085MF202); National Natural Science Foundation of China(grant numbers:62002097); Fundamental Research Funds for the Central Universities(grant numbers:PA2023GDGP0044); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10412682","Edge computing;high-speed railways;multicasting;group Steiner tree problem;admission control","Cloud computing;Throughput;Delays;Routing;Multicast communication;Multicast algorithms;Edge computing","","1","","60","IEEE","23 Jan 2024","","","IEEE","IEEE Journals"
"A Data Protection Focused Adaptation Engine for Distributed Video Analytics Pipelines","C. Lachner; J. Laufer; S. Dustdar; K. Pohl","Distributed Systems Group, TU Wien, Vienna, Austria; Paluno—The Ruhr Institute for Software Technology, University of Duisburg-Essen, Essen, Germany; Distributed Systems Group, TU Wien, Vienna, Austria; Paluno—The Ruhr Institute for Software Technology, University of Duisburg-Essen, Essen, Germany",IEEE Access,"4 Jul 2022","2022","10","","68669","68685","The design, development, deployment, and operation of a distributed Video Analytics Pipeline (VAP) at the edge of the network is highly complex. In the domain of adaptive systems, several solutions are proposed in literature to optimize either one particular performance aspect of a VAP, e.g., execution time or latency, or focus on minimal energy consumption, or calculate a trade-off including some of those aspects. However, nowadays, most systems utilizing a VAP that records personally identifiable data have to adhere to some form of data protection regulation, such as the GDPR. Still, adaptations to increase data protection requirements are often second to previously mentioned performance or energy consumption characteristics of a VAP. While there is state of the art literature dealing with data protection related adaptations, most of them solely focus on increasing certain security or privacy aspects of a system, leaving previously mentioned performance or energy consumption characteristics out of scope. To the best of our knowledge, there is no solution that covers all of these aspects. In this paper, we present a data protection focused adaptation engine that leverages the application- and infrastructure based adaptation space of a distributed VAP. The engine employs an extended system model and adaptation rules that are based on previous research. It features an optimization algorithm to improve data protection, performance and energy consumption characteristics of a distributed VAP.","2169-3536","","10.1109/ACCESS.2022.3185990","European Union’s Horizon 2020 Research and Innovation Program (FogProtect)(grant numbers:871525); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9805737","Adaptive systems;optimization;video analytics pipelines;data protection;edge computing","Data protection;Energy consumption;Task analysis;Pipelines;Visual analytics;Security;Engines","","1","","39","CCBY","24 Jun 2022","","","IEEE","IEEE Journals"
"Networked Metaverse Systems: Foundations, Gaps, Research Directions","Y. Zhang; D. Kutscher; Y. Cui","The Hong Kong University of Science and Technology, Guangzhou, China; The Hong Kong University of Science and Technology, Guangzhou, China; The Hong Kong University of Science and Technology, Guangzhou, China",IEEE Open Journal of the Communications Society,"10 Sep 2024","2024","5","","5488","5539","This article discusses ‘Metaverse’ from a technical perspective, focusing on networked systems aspects. Based on a technical definition of the ‘Metaverse,’ we examine the current state and challenges in communication and networking within Metaverse systems. We describe the state-of-the-art in different enabling Metaverse technologies and provide a technical analysis of current Metaverse system architectures. We then detail the state-of-the-art and the gaps in four areas: communication performance, mobility, large-scale operation, and end system architecture. Based on our analysis, we formulate a vision for future Metaverse infrastructure, outlining goals, design concepts, and suggested research directions.","2644-125X","","10.1109/OJCOMS.2024.3426098","Guangzhou Municipal Key Laboratory on Future Networked Systems(grant numbers:024A03J0623); National Natural Science Foundation of China(grant numbers:62371412); National Key Research and Development Program of China(grant numbers:2024YFE0200600); Guangdong Basic and Applied Basic Research Natural Science Funding Scheme(grant numbers:2024A1515011184); Guangzhou-HKUST (GZ) Joint Funding Scheme(grant numbers:2024A03J0539); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10592027","Metaverse;networking;Internet;information-centric networking","Metaverse;Surveys;Blockchains;Artificial intelligence;Computer architecture;Games;Focusing","","1","","431","CCBY","10 Jul 2024","","","IEEE","IEEE Journals"
"Energy Consumption in Microservices Architectures: A Systematic Literature Review","G. Araújo; V. Barbosa; L. N. Lima; A. Sabino; C. Brito; I. Fé; P. Rego; E. Choi; D. Min; T. A. Nguyen; F. A. Silva","PASID Laboratory, Federal University of Piauí (UFPI), Picos, Piauí, Brazil; PASID Laboratory, Federal University of Piauí (UFPI), Picos, Piauí, Brazil; PASID Laboratory, Federal University of Piauí (UFPI), Picos, Piauí, Brazil; PASID Laboratory, Federal University of Piauí (UFPI), Picos, Piauí, Brazil; PASID Laboratory, Federal University of Piauí (UFPI), Picos, Piauí, Brazil; PASID Laboratory, Federal University of Piauí (UFPI), Picos, Piauí, Brazil; Federal University of Ceará, Fortaleza, Brazil; School of Software, College of Computer Science, Kookmin University, Seoul, South Korea; Department of Computer Science and Engineering, College of Engineering, Konkuk University, Seoul, South Korea; Konkuk Aerospace Design-Airworthiness Institute (KADA), Konkuk University, Seoul, South Korea; PASID Laboratory, Federal University of Piauí (UFPI), Picos, Piauí, Brazil",IEEE Access,"17 Dec 2024","2024","12","","186710","186729","Cloud computing emerges as a paradigm that facilitates on-demand access to technological resources through the mechanism of service virtualization. This virtualization enables the partitioning of hardware resources among applications that are organized into distinct independent modules. The concept of microservice architecture takes advantage of virtualization capabilities to embrace a software architecture strategy focused on the development of applications as assemblies of several interdependent but loosely coupled modules. Nonetheless, the adoption of microservices architecture is accompanied by substantial energy demands to meet the desired standards of performance and availability. Existing research within the domain of microservices has explored various topics pertinent to energy consumption, including elasticity, reliability, performance, and availability. Yet, the diversity of challenges and solutions presents a complex landscape for identifying prevailing research trends and unaddressed gaps in the context of microservices. This study aims to methodically discern, evaluate, and juxtapose the existing research trends and voids concerning energy consumption within microservices. It elucidates a systematic review on the subject of energy consumption in microservices architectures, offering a compilation of references to facilitate more directed future investigations. The initial selection encompassed 3625 articles, which were subsequently narrowed down through three stages of refinement, resulting in 37 articles chosen for an exhaustive review. These selected studies were cataloged and analyzed based on various criteria, including metrics, evaluation methodologies, and architectural typologies, thus uncovering research gaps and emerging trends related to energy consumption in microservice architectures. Furthermore, this inquiry delineates significant research challenges and prospective directions, structured around the key metrics that underpin the reviewed studies: performance, elasticity, scalability, reliability, sustainability, and availability.","2169-3536","","10.1109/ACCESS.2024.3389064","Basic Science Research Program through the National Research Foundation of Korea (NRF); Ministry of Education(grant numbers:2020R1A6A1A03046811,2021R1A2C2094943); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10500420","Microservices;systematic mapping;energy consumption;cloud computing;container","Microservice architectures;Systematics;Computer architecture;Cloud computing;Reviews;Energy consumption;Security","","1","","66","CCBYNCND","15 Apr 2024","","","IEEE","IEEE Journals"
"Challenges of CPS/IoT Network Architecture in 6G Era","T. Kato; N. Fukumoto; C. Sasaki; A. Tagami; A. Nakao","KDDI Research, Inc., Saitama, Japan; KDDI Research, Inc., Saitama, Japan; KDDI Research, Inc., Saitama, Japan; KDDI Research, Inc., Saitama, Japan; Graduate School of Engineering, The University of Tokyo, Tokyo, Japan",IEEE Access,"7 May 2024","2024","12","","62804","62817","Cyber Physical System (CPS) and Internet of Things (IoT) have attracted attention as a promising technology for the future society. Although the 6th generation mobile system is expected to work as the CPS/IoT platform, the specific technical requirements are not studied thoroughly. In this paper, we take application driven approach focusing on three applications, eHealth, smart manufacturing, and smart city, to identify mandatory technical requirements for the 6G mobile core network architecture to realize CPS/IoT systems. Specifically, this paper gives the following three contributions. First, we investigate several use cases in the applications listed above and list up the requirements for the core network, i.e., the massive connectivity, the flexible SLA, the ambient IoT device accommodation, the wireless sensing function, and the AI-aware data sharing. Second, we identify the issues to satisfy those requirements in the current 5G technologies. Lastly, we pick up the 6G architectures proposed currently and discuss their pros and cons based on the requirements and issues pointed out.","2169-3536","","10.1109/ACCESS.2024.3395363","University of Tokyo; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10516287","6th generation communications system;cyber physical system;Internet of Things","6G mobile communication;Network architecture;Smart cities;Sensors;5G mobile communication;Middleware;Internet of Things;Cyber-physical systems","","1","","71","CCBYNCND","30 Apr 2024","","","IEEE","IEEE Journals"
"Vehicular Fog Resource Allocation Approach for VANETs Based on Deep Adaptive Reinforcement Learning Combined With Heuristic Information","Y. Cheng; A. Vijayaraj; K. Sree Pokkuluri; T. Salehnia; A. Montazerolghaem; R. Rateb","School of Information, Guangdong Polytechnic of Science and Trade, Guangzhou, Guangdong, China; Department of Information Technology, R.M.K. Engineering College, Chennai, Tamil Nadu, India; Department of Computer Science and Engineering, Shri Vishnu Engineering College for Women, Bhimavaram, India; Department of Computer Engineering and Information Technology, Razi University, Kermanshah, Iran; Faculty of Computer Engineering, University of Isfahan, Isfahan, Iran; Department of Computer Science, Faculty of Information Technology, Al-Ahliyya Amman University, Amman, Jordan",IEEE Access,"1 Oct 2024","2024","12","","139056","139075","Intelligent Transport Systems (ITS) are gradually progressing to practical application because of the rapid growth in network and information technology. Currently, the low-latency ITS requirements are hard to achieve in the conventional cloud-based Internet of Vehicles (IoV) infrastructure. In the context of IoV, Vehicular Fog Computing (VFC) has become recognized as an inventive and viable architecture that can effectively decrease the time required for the computation of diverse vehicular application activities. Vehicles receive rapid task execution services from VFC. The benefits of fog computing and vehicular cloud computing are combined in a novel concept called fog-based Vehicular Ad Hoc Networks (VANETs). These networks depend on a movable power source, so they have specific limitations. Cost-effective routing and load distribution in VANETs provide additional difficulties. In this work, a novel method is developed in vehicular applications to solve the difficulty of allocating limited fog resources and minimizing the service latency by using parked vehicles. Here, the improved heuristic algorithm called Revised Fitness-based Binary Battle Royale Optimizer (RF-BinBRO) is proposed to solve the problems of vehicular networks effectively. Additionally, the combination of Deep Adaptive Reinforcement Learning (DARL) and the improved BinBRO algorithm effectively analyzes resource allocation, vehicle parking, and movement status. Here, the parameters are tuned using the RF-BinBRO to achieve better transportation performance. To assess the performance of the proposed algorithm, simulations are carried out. The results defined that the developed VFC resource allocation model attains maximum service satisfaction compared to the traditional methods for resource allocation.","2169-3536","","10.1109/ACCESS.2024.3455168","Razi University of Kermanshah; Al-Ahliyya Amman University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10666661","Vehicular fog resource allocation;vehicular ad hoc networks;revised fitness-based binary battle royale optimizer;deep adaptive reinforcement learning;reward assessment;service satisfaction;service latency","Resource management;Vehicular ad hoc networks;Computational modeling;Cloud computing;Edge computing;Computer architecture;Optimization methods;Reinforcement learning","","1","","37","CCBYNCND","5 Sep 2024","","","IEEE","IEEE Journals"
"A Security Services Management Architecture Toward Resilient 6G Wireless and Computing Ecosystems","E. Rodriguez; X. Masip-Bruin; J. Martrat; R. Diaz; A. Jukan; F. Granelli; P. Trakadas; G. Xilouris","Advanced Network Architectures Laboratory (CRAAX), Universitat Politècnica de Catalunya (UPC), Vilanova i la Geltru, Barcelona, Spain; Advanced Network Architectures Laboratory (CRAAX), Universitat Politècnica de Catalunya (UPC), Vilanova i la Geltru, Barcelona, Spain; Eviden Research and Innovation, Barcelona, Spain; Eviden Research and Innovation, Barcelona, Spain; Department of Communication Networks, Technische Universität Braunschweig, Braunschweig, Germany; CNIT, University of Trento, Trento, Italy; Department of Port Management and Shipping, National and Kapodistrian University of Athens, Athens, Greece; Department of Port Management and Shipping, National and Kapodistrian University of Athens, Athens, Greece",IEEE Access,"22 Jul 2024","2024","12","","98046","98058","The upcoming sixth-generation (6G) networks are expected to be more heterogeneous, scalable, reliable, secure and energy-efficient. 6G services and applications will benefit from the fast data transmission rates, global coverage, precise positioning, and widespread intelligence capabilities provided by 6G technology. In this complex scenario, new attack surfaces and vectors will emerge, due to the distributed, disaggregated, dynamic, programmable and open nature of the entire end-to-end 6G infrastructure along with the fragmentation of data, as well as the need for supporting cross-platforms interoperability, making the application of security and trust fundamentally challenging. To securely manage services in the future 6G mobile networks along with the set of functions ruling its management, conceptual work is needed to defining functional blocks supporting a secure end-to-end 6G systems management. This is especially critical in handling specific functionalities, such as network disaggregation, risk and threats business impact, energy-efficiency, self-configuration or dynamic discovery. This paper positions an innovative 6G security services management architecture, which builds on a set of innovative building blocks, leveraging key technologies, such as AI-assisted models and Digital Twin, while enabling a human-centric approach toward an end-to-end security solution. Two canonical workflows along with two illustrative application scenarios are proposed, Light Rail Transit and Extended Reality, to conceptually highlight the proposed architecture impact and its expected benefits when high bandwidth, ultra-low latency, and secure communications are required.","2169-3536","","10.1109/ACCESS.2024.3427661","European union’s Horizon Europe, under the project HORSE(grant numbers:101096342); Ministerio de Ciencia e Innovación(grant numbers:MCIN/AEI/10.13039/501100011033); ERDF: A way of making Europe(grant numbers:PID2021-124463OB-I00); Catalan Government(grant numbers:2021 SGR 00326); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10597424","6G;communication systems;wireless communication;network architecture;security","6G mobile communication;Security;Prevention and mitigation;Resilience;Privacy;5G mobile communication;Wireless communication;Network architecture","","1","","35","CCBYNCND","15 Jul 2024","","","IEEE","IEEE Journals"
"A Systematic Literature Review of IoT System Architectural Styles and Their Quality Requirements","N. Khezemi; J. Baptiste Minani; F. Sabir; N. Moha; Y. -G. Guéhéneuc; G. El Boussaidi","Département de génie logiciel et des TI, École de technologie supérieure, Montreal, QC, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, QC, Canada; Department of Computer Sciences, University of the Punjab, Lahore, Pakistan; Département de génie logiciel et des TI, École de technologie supérieure, Montreal, QC, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, QC, Canada; Département de génie logiciel et des TI, École de technologie supérieure, Montreal, QC, Canada",IEEE Internet of Things Journal,"20 Nov 2024","2024","11","23","37599","37616","The Internet of Things (IoT) is increasingly prevalent, with systems developed across various domains. Choosing the right IoT architectural style is challenging due to the diversity of devices, dynamic environments, and real-time data needs. This choice significantly impacts system quality, requiring a careful balance of quality requirements and tradeoffs. Previous studies have not adequately identified the most suitable architectural styles for specific IoT quality needs. This study presents a systematic literature review of 103 primary studies (PSs) on IoT system quality requirements and architectural styles, assessing how each architectural style satisfies specific requirements. We followed the preferred reporting items for systematic review and meta-analysis (PRISMA) protocol to report our findings and answer three research questions (RQs). We selected PSs by applying inclusion and exclusion criteria to relevant papers published until the end of 2023. We analyzed data from PSs to understand IoT system quality requirements and architectural styles, assessing their alignment. The research revealed ten essential quality requirements for IoT systems and identified ten distinct architectural styles. Notably, each architectural style varies in its capacity to fulfill specific quality requirements, particularly regarding security, scalability, and performance. SOA, client-server, and REST architectural styles best fulfill many quality requirements. However, various architectural styles, such as Layered, Microservices, and Peer-to-Peer, show limited support for privacy requirements. Our findings can guide IoT systems practitioners in selecting an architectural style that aligns with their desired quality standards. Additionally, we recommend new research opportunities to deepen understanding of key architectural styles based on specific quality requirements.","2327-4662","","10.1109/JIOT.2024.3435496","Natural Sciences and Engineering Research Council of Canada (NSERC); Canada Research Chair program (CRCP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10759538","Architectural style;Internet of Things (IoT);IoT architecture;quality criteria;quality requirements","Privacy;Protocols;Scalability;Microservice architectures;Software;Real-time systems;Peer-to-peer computing;Internet of Things;Security;Standards","","1","","103","IEEE","20 Nov 2024","","","IEEE","IEEE Journals"
"Intelligent Control in 6G Open RAN: Security Risk or Opportunity?","S. Soltani; A. Amanloo; M. Shojafar; R. Tafazolli","5G/6GIC, Institute for Communication Systems, University of Surrey, Guildford, U.K.; 5G/6GIC, Institute for Communication Systems, University of Surrey, Guildford, U.K.; 5G/6GIC, Institute for Communication Systems, University of Surrey, Guildford, U.K.; 5G/6GIC, Institute for Communication Systems, University of Surrey, Guildford, U.K.",IEEE Open Journal of the Communications Society,"29 Jan 2025","2025","6","","840","880","The Open Radio Access Network (Open RAN) framework, emerging as the cornerstone for Artificial Intelligence (AI)-enabled Sixth-Generation (6G) mobile networks, heralds a transformative shift in radio access network architecture. As the adoption of Open RAN accelerates, ensuring its security becomes critical. The RAN Intelligent Controller (RIC) plays a central role in Open RAN by improving network efficiency and flexibility. Nevertheless, it also brings about potential security risks that need careful scrutiny. Therefore, it is imperative to evaluate the current state of RIC security comprehensively. This assessment is essential to gain a profound understanding of the security considerations associated with RIC. This survey combines a comprehensive analysis of RAN security, tracing its evolution from 2G to 5G, with an in-depth exploration of RIC security, marking the first comprehensive examination of its kind in the literature. Real-world security incidents involving RIC are vividly illustrated, providing practical insights. The study evaluates the security implications of the RIC within the 6G Open RAN context, addressing security vulnerabilities, mitigation strategies, and potential enhancements. It aims to guide stakeholders in the telecom industry toward a secure and dependable telecommunications infrastructure. The article serves as a valuable reference, shedding light on the RIC’s crucial role within the broader network infrastructure and emphasizing security’s paramount importance. This survey also explores the promising security opportunities that the RIC presents for enhancing network security and resilience in the context of 6G mobile networks. It outlines open issues, lessons learned, and future research directions in the domain of intelligent control in 6G Open RAN, facilitating a comprehensive understanding of this dynamic landscape.","2644-125X","","10.1109/OJCOMS.2025.3526215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10829659","RAN intelligent controller (RIC) security;open radio access network (Open RAN);6G;RAN security;O-RAN","Security;Open RAN;6G mobile communication;Surveys;Interoperability;Optimization;Protocols;Network security;5G mobile communication;Software","","1","","180","CCBY","6 Jan 2025","","","IEEE","IEEE Journals"
"Learning to Optimize Workflow Scheduling for an Edge–Cloud Computing Environment","K. Zhu; Z. Zhang; S. Zeadally; F. Sun","School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; College of Communication and Information, University of Kentucky, Lexington, KY, USA; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China",IEEE Transactions on Cloud Computing,"5 Sep 2024","2024","12","3","897","912","The widespread deployment of intelligent Internet of Things (IoT) devices brings tighter latency demands on complex workload patterns such as workflows. In such applications, tremendous dataflows are generated and processed in accordance with specific service chains. Edge computing has proven its feasibility in reducing the traffic in the core network and relieving cloud datacenters of fragmented computational demands. However, the efficient scheduling of workflows in hybrid edge–cloud networks is still challenging for the intelligent IoT paradigm. Existing works make dispatching decisions prior to real execution, making it difficult to cope with the dynamicity of the environment. Consequently, the schedulers are affected both by the scheduling strategy and by the mutual impact of dynamic workloads. We design an intelligent workflow scheduler for use in an edge–cloud network where workloads are generated with continuous steady arrivals. We develop new graph neural network (GNN)-based representations for task embedding and we design a proximal policy optimization (PPO)-based online learning scheduler. We further introduce an intrinsic reward to obtain an instantaneous evaluation of the dispatching decision and correct the scheduling policy on-the-fly. Numerical results validate the feasibility of our proposal as it outperforms existing works with an improved quality of service (QoS) level.","2168-7161","","10.1109/TCC.2024.3408006","Fundamental Research Funds for the Central Universities(grant numbers:2022JBZY002); National Natural Science Foundation of China(grant numbers:62173026); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10545338","Workflow scheduling;edge computing;Internet of Things;reinforcement learning","Task analysis;Processor scheduling;Dynamic scheduling;Cloud computing;Internet of Things;Edge computing;Job shop scheduling","","1","","41","IEEE","31 May 2024","","","IEEE","IEEE Journals"
"A Comprehensive Review of AI Techniques for Resource Management in Fog Computing: Trends, Challenges, and Future Directions","D. Alsadie","Department of Computer Science and Artificial Intelligence, College of Computing, Umm Al-Qura University, Makkah, Saudi Arabia",IEEE Access,"29 Aug 2024","2024","12","","118007","118059","Fog computing (FC), extending cloud services to the network edge, has emerged as a key paradigm for low-latency applications like the Internet of Things (IoT). However, efficient resource management, task scheduling, and load balancing pose challenges in fog environments. This review surveys recent research efforts aimed at addressing these challenges and optimizing FC performance. We conducted a systematic analysis of relevant research papers on FC published in reputable academic databases. The review focused on studies published between 2019 and 2024 and emphasized artificial intelligence based studies exploring resource management, task scheduling, and load balancing techniques within the FC domain. The review identifies a diverse range of techniques applied to optimize FC performance. These include machine learning (ML) and deep learning (DL) for resource allocation, heuristic algorithms for task scheduling, and nature-inspired meta-heuristics for load balancing. The review evaluates the strengths and limitations of these approaches, highlighting their impact on metrics like latency, energy consumption, and Quality of Service (QoS). This review demonstrates the significant progress made in optimizing FC through innovative techniques. ML and meta-heuristics have emerged as promising approaches for resource management, task scheduling, and load balancing, respectively. However, challenges persist in areas like real-world implementation complexities and ensuring service quality across geographically distributed fog networks. Future research directions are identified, emphasizing the need for further exploration of these challenges and the integration of emerging technologies like deep reinforcement learning for enhanced FC performance.","2169-3536","","10.1109/ACCESS.2024.3447097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10643044","Fog computing;artificial intelligence;resource management;optimization;challenges","Resource management;Task analysis;Artificial intelligence;Reviews;Load management;Internet of Things;Heuristic algorithms;Edge computing;Optimization methods","","1","","165","CCBYNCND","21 Aug 2024","","","IEEE","IEEE Journals"
"360-ADAPT: An Open-RAN-Based Adaptive Scheme for Quality Enhancement of Opera 360° Content Distribution","A. A. Simiscuka; M. A. Togou; M. Zorrilla; G. -M. Muntean","Insight SFI Centre for Data Analytics and the School of Electronic Engineering, Dublin City University, Dublin 9, Ireland; School of Computing, Dublin City University, Dublin 9, Ireland; Department of Digital Media, Vicomtech, Donostia-San Sebastián, Spain; Insight SFI Centre for Data Analytics and the School of Electronic Engineering, Dublin City University, Dublin 9, Ireland",IEEE Transactions on Green Communications and Networking,"28 Aug 2024","2024","8","3","924","938","There is increasing viewer interest and technological support for streaming immersive clips over the Internet. There are, however, challenges in supporting high quality of viewer experience, mostly due to the large amounts of the data associated with immersive video and spatial audio (Ambisonics). In situations where there are limited network resources, the streamed 360° content needs to be adjusted dynamically to meet the network constraints. Dynamic Adaptive Streaming over HTTP (DASH) adaptation is a key technology for delivering high-quality video over open radio access networks (RANs). DASH allows for efficient adaptation of video streams to the available network conditions. This paper introduces 360-ADAPT, a DASH-based adaptation solution on an Open-RAN architecture for increased quality remote 360° opera experiences. Unlike existing schemes, 360-ADAPT gives precedence to audio over the video when selecting bitrates, increasing the overall quality of the artistic act and improving use of resources and energy. The proposed 360-ADAPT was tested with real opera viewers in the context of an artistic-oriented platform for opera delivery, part of the Horizon2020 TRACTION project. Results indicate that 360-ADAPT achieves higher perceived quality levels than alternative solutions both in QoS and QoE metrics.","2473-2400","","10.1109/TGCN.2024.3418948","European Union Horizon 2020 Research and Innovation, TRACTION Project(grant numbers:870610); Science Foundation Ireland (SFI) through the Frontiers Projects FRADIS(grant numbers:21/FFP-P/10244); INSIGHT SFI Research Centre(grant numbers:12/RC/2289_P2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10571391","Adaptation;QoE;immersive media;360 video;ambisonics;OpenRAN","Streaming media;Media;Bit rate;Quality of experience;Quality of service;Headphones;Ambisonics;Open RAN","","1","","48","CCBY","25 Jun 2024","","","IEEE","IEEE Journals"
"RLPRAF: Reinforcement Learning-Based Proactive Resource Allocation Framework for Resource Provisioning in Cloud Environment","R. Panwar; M. Supriya","Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Bengaluru, India; Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Bengaluru, India",IEEE Access,"16 Jul 2024","2024","12","","95986","96007","Recent developments in cloud technology enable one to dynamically deploy heterogeneous resources as and when needed. This dynamic nature of the incoming workload causes fluctuations in the cloud environment, which is currently addressed using traditional reactive scaling techniques. Simple reactive approaches affect elastic system performance either by over-provisioning resources which significantly increases the cost, or by under-provisioning, which leads to starvation. Hence automated resource provisioning becomes an effective method to deal with such workload fluctuations. The aforementioned problems can also be resolved by using intelligent resource provisioning techniques by dynamically assigning required resources while adapting to the environment. In this paper, a reinforcement learning-based proactive resource allocation framework (RLPRAF) is proposed. This framework simultaneously learns the environment and distributes the resources. The proposed work presents a paradigm for the optimal allocation of resources by merging the notions of automatic computation, linear regression, and reinforcement learning. When tested with real-time workloads, the proposed RLPRAF method surpasses previous auto-scaling algorithms considering CPU usage, response time, and throughput. Finally, a set of tests demonstrate that the suggested strategy lowers overall expense by 30% and SLA violation by 77.7%. Furthermore, it converges at an optimum timing and demonstrates that it is feasible for a wide range of real-world service-based cloud applications.","2169-3536","","10.1109/ACCESS.2024.3421956","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10579971","Resource allocation;resource provisioning;autonomic computing systems;machine learning;reinforcement learning;virtual machines","Cloud computing;Resource management;Dynamic scheduling;Virtual machining;Costs;Computational modeling;Time factors;Autonomous systems","","1","","51","CCBYNCND","2 Jul 2024","","","IEEE","IEEE Journals"
"Model-Driven Dependability and Power Consumption Quantification of Kubernetes-Based Cloud-Fog Continuum","I. Fé; T. A. Nguyen; A. B. Soares; S. Son; E. Choi; D. Min; J. -W. Lee; F. A. Silva","Laboratory of Applied Research to Distributed Systems (PASID), Federal University of Piauí (UFPI), Picos, Piauí, Brazil; Konkuk Aerospace Design-Airworthiness Research Institute (KADA), Konkuk University, Seoul, South Korea; Distributed Systems and Network Computer Laboratory (DisNeL), Federal University of Piauí (UFPI), Teresina, Piauí, Brazil; Electronics and Telecommunications Research Institute (ETRI), Daejeon, South Korea; School of Software, College of Computer Science, Kookmin University, Seoul, South Korea; Department of Computer Science and Engineering, College of Engineering, Konkuk University, Seoul, South Korea; Department of Aerospace Information Engineering, Konkuk University, Seoul, South Korea; Laboratory of Applied Research to Distributed Systems (PASID), Federal University of Piauí (UFPI), Picos, Piauí, Brazil",IEEE Access,"19 Dec 2023","2023","11","","140826","140852","System dependability is pivotal for the reliable execution of designated computing functions. With the emergence of cloud-fog computing and microservices architectures, new challenges and opportunities arise in evaluating system dependability. Enhancing dependability in microservices often involves component replication, potentially increasing energy costs. Thus, discerning optimal redundancy strategies and understanding their energy implications is crucial for both cost efficiency and ecological sustainability. This paper presents a model-driven approach to evaluate the dependability and energy consumption of cloud-fog systems, utilizing Kubernetes, a container application orchestration platform. The developed model considers various determinants affecting system dependability, including hardware and software reliability, resource accessibility, and support personnel availability. Empirical studies validate the model’s effectiveness, demonstrating a 22.33% increase in system availability with only a 1.33% rise in energy consumption. Moreover, this methodology provides a structured framework for understanding cloud-fog system dependability, serves as a reference for comparing dependability across different systems, and aids in resource allocation optimization. This research significantly contributes to the efforts to enhance cloud-fog system dependability.","2169-3536","","10.1109/ACCESS.2023.3340195","Basic Science Research Program through the National Research Foundation of Korea (NRF); Ministry of Education(grant numbers:2020R1A6A1A03046811,2021R1A2C2094943); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10347188","Cloud-fog continuum;dependability;Kubernetes;stochastic modeling","Modeling;Stochastic processes;Power demand;Energy consumption;Edge computing;Cloud computing","","1","","47","CCBYNCND","7 Dec 2023","","","IEEE","IEEE Journals"
"A Big Data Architecture for Digital Twin Creation of Railway Signals Based on Synthetic Data","G. Salierno; L. Leonardi; G. Cabri","Department of Engineering “Enzo Ferrari,”, University of Modena and Reggio Emilia, Modena, Italy; Department of Engineering “Enzo Ferrari,”, University of Modena and Reggio Emilia, Modena, Italy; Department of Physics, Informatics and Mathematics, University of Modena and Reggio Emilia, Modena, Italy",IEEE Open Journal of Intelligent Transportation Systems,"18 Jul 2024","2024","5","","342","359","Industry 5.0 has introduced new possibilities for defining key features of the factories of the future. This trend has transformed traditional industrial production by exploiting Digital Twin (DT) models as virtual representations of physical manufacturing assets. In the railway industry, Digital Twin models offer significant benefits by enabling anticipation of developments in rail systems and subsystems, providing insight into the future performance of physical assets, and allowing testing and prototyping solutions prior to implementation. This paper presents our approach for creating a Digital Twin model in the railway domain. We particularly emphasize the critical role of Big Data in supporting decision-making for railway companies and the importance of data in creating virtual representations of physical objects in railway systems. Our results show that the Digital Twin model of railway switch points, based on synthetic data, accurately represents the behavior of physical railway switches in terms of data points.","2687-7813","","10.1109/OJITS.2024.3412820","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554659","Big data;digital twin;machine learning;synthetic data;railway industry;artificial intelligence","Rail transportation;Switches;Digital twins;Big Data;Data models;Computer architecture;Synthetic data;Machine learning;Artificial intelligence;Computer architecture","","1","","30","CCBY","11 Jun 2024","","","IEEE","IEEE Journals"
"Mapping Technological Trajectories of Edge Computing: A Citation Graph Analysis","E. B. Wagiu; C. -M. Liu; Y. Palopak","Department of Computer Science and Information Engineering, National Taipei University of Technology, Taipei, Taiwan; Department of Computer Science and Information Engineering, National Taipei University of Technology, Taipei, Taiwan; Department of Information Management, National Taiwan University of Science and Technology, Taipei, Taiwan",IEEE Internet of Things Journal,"25 Apr 2024","2024","11","9","16545","16560","The notion of edge computing has recently arisen as a potentially helpful advancement in distributed computing and communication infrastructure. To better understand the evolution and diffusion of technological knowledge in this field, we conducted a citation graph analysis of scholarly publications related to edge computing. Our study aims to identify the critical technological trajectories and research themes that have shaped the development and adoption of edge computing over time. We employ main path analysis (MPA) to construct a citation graph using a comprehensive publications data set from Web of Science (WoS) databases. We applied keyword co-occurrence analysis techniques to identify clusters of research topics and their interconnections. Occurrence cluster analysis results indicate that current research in edge computing is primarily centered around the architecture of edge computing, high-speed networks, mobile edge computing, computing systems, and blockchain and security technologies. We also identified the significant papers and contributors in the edge computing field and their impact on the technological trajectory of edge computing. The results reveal 35 significant papers that shape the emergence of four phases of the technology trajectory, including edge computing transition, architecture, edge future, and edge optimization.","2327-4662","","10.1109/JIOT.2024.3355056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10402108","Edge computing;keyword co-occurrence;main path analysis (MPA);mapping trajectories","Edge computing;Trajectory;Internet of Things;Cloud computing;Systematics;Security;Computer architecture","","1","","84","IEEE","17 Jan 2024","","","IEEE","IEEE Journals"
"A Comprehensive Study of Load Balancing Approaches in Real-Time Multi-Core Systems for Mixed Real-Time Tasks","S. Jadon; P. K. Kannan; U. Kalaria; K. R. Varsha; K. Gupta; P. B. Honnavalli","Department of Computer Science Engineering, PES University, Bengaluru, India; Department of Computer Science Engineering, PES University, Bengaluru, India; Department of Computer Science Engineering, PES University, Bengaluru, India; Department of Computer Science Engineering, PES University, Bengaluru, India; Department of Computer Science Engineering, PES University, Bengaluru, India; Department of Computer Science Engineering, PES University, Bengaluru, India",IEEE Access,"18 Apr 2024","2024","12","","53373","53395","Real-time systems are becoming pervasive with the growing global connectivity and rising consumer demands. The need for real-time processing has become a crucial part of many business applications worldwide. A key factor that determines the time taken for an application to give out the result hinges on its ability to prioritize, manage, and execute real-time workloads. However, there are several difficulties and constraints connected with implementing tasks in a real-time context. This research study primarily focuses on load balancing for mixed real-time tasks on a multi-core system, one of the major challenges for executing real-time workloads. The purpose of load balancing is to distribute the load evenly among the processor(s) and maximize their utility while minimizing overall execution time. The goal of this paper is to present a critical analysis of existing load balancing techniques for both periodic and aperiodic tasks. The paper explores several factors including throughput, performance, migration time, response time, overhead, resource utilization, scalability, fault tolerance, power efficiency, and other variables that play a crucial role in assessing the efficacy of load balancing in real-time systems. The proposed has contributed in four folds. Firstly, the state-of-the-art of various load balancing algorithms are discussed followed by the architecture involved in real-time multi-core systems. Later, different load balancing based scheduling algorithms were compared on the basis of different schemas and metrics for algorithm evaluation is also provided. Finally, the paper also identifies areas that warrant further exploration or investigation, suggesting potential avenues for future research, and highlighting emerging trends or developments that may shape the field.","2169-3536","","10.1109/ACCESS.2024.3388291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10497602","Scheduling algorithms;real-time systems;priority-driven algorithms;EDF (earliest deadline first);RM (rate-monotonic);LBPSA (load balanced partitioning and scheduling algorithm);homogeneous multi-core;heterogeneous multi-core;static algorithms;dynamic algorithms","Task analysis;Real-time systems;Heuristic algorithms;Scheduling;Load management;Partitioning algorithms;Scheduling algorithms","","1","","64","CCBYNCND","12 Apr 2024","","","IEEE","IEEE Journals"
"LoRa Meets IP: A Container-Based Architecture to Virtualize LoRaWAN End Nodes","A. Cilfone; L. Davoli; G. Ferrari","Department of Engineering and Architecture, Internet of Things (IoT) Lab, University of Parma, Parma, Italy; Department of Engineering and Architecture, Internet of Things (IoT) Lab, University of Parma, Parma, Italy; Department of Engineering and Architecture, Internet of Things (IoT) Lab, University of Parma, Parma, Italy",IEEE Transactions on Mobile Computing,"3 Sep 2024","2024","23","10","9191","9207","In this work, a container-based architecture for the integration of Long Range Wide Area Network (LoRaWAN) end nodes—e.g., used to monitor industrial machines or mobile entities in specific environments—with Internet Protocol (IP)-based networks is proposed and its performance is investigated. To this end, we exploit the native service and resource discovery support of the Constrained Application Protocol (CoAP), as well as its light traffic requirements, owing to its use of User Datagram Protocol (UDP) rather than Transmission Control Protocol (TCP). This approach (i) adapts transparently (with no impact) to both private and public LoRaWAN networks, (ii) enables seamless interaction between LoRaWAN-based and CoAP-based nodes, through a logical “virtualization” of LoRaWAN nodes at server side, and (iii) enables routing among LoRaWAN end nodes, overcoming LoRaWAN's absence of inter-node communication and lack of compliance (at the end nodes’ side) with IP. Two virtualization approaches are proposed: (i) virtualization of a single end node (represented as a CoAP server) per container and (ii) virtualization of multiple end nodes (as CoAP servers) per container. Finally, deployments of the proposed virtualization architectures, using both a laptop and an Internet of Things (IoT) device (e.g., a Raspberry Pi), are considered, highlighting how the best solution relies on the use of several containers, with more than one CoAP server per container.","1558-0660","","10.1109/TMC.2024.3359150","European Union's Horizon 2020 research and innovation program ECSEL Joint Undertaking; Airborne Data Collection on Resilient System Architectures(grant numbers:876038); Intelligent Secure Trustable Things; European Union's Horizon Europe Research and Innovation Program Key Digital Technology (KDT) JU(grant numbers:101097267); OPtimization of Electric Vehicle Autonomy; National Research Centre for Agricultural Technologies(grant numbers:CN00000022); National Recovery and Resilience Plan; Italian Ministry of University and Research; European Union – NextGenerationEU; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10415221","Internet of Things;LoRaWAN;virtualization;Constrained Application Protocol (CoAP)","Internet of Things;Low-power wide area networks;Virtualization;Servers;Computer architecture;Mobile computing;Downlink","","1","","57","CCBY","26 Jan 2024","","","IEEE","IEEE Journals"
"Enabling Industrial Internet of Things by Leveraging Distributed Edge-to-Cloud Computing: Challenges and Opportunities","M. N. Jamil; O. Schelén; A. Afif Monrat; K. Andersson","Department of Computer Science, Electrical and Space Engineering, Luleå University of Technology, Skellefteå, Sweden; Department of Computer Science, Electrical and Space Engineering, Luleå University of Technology, Skellefteå, Sweden; Department of Computer Science, Electrical and Space Engineering, Luleå University of Technology, Skellefteå, Sweden; Department of Computer Science, Electrical and Space Engineering, Luleå University of Technology, Skellefteå, Sweden",IEEE Access,"17 Sep 2024","2024","12","","127294","127308","The Industrial Internet of Things (IIoT) promises automation, efficiency, and data-driven decision-making by real-time data collection and analysis. However, traditional IIoT architectures are cloud-centric and, therefore, struggle to handle large volumes of data, edge bandwidth constraints, and data confidentiality. Distributed edge-to-cloud computing emerges as a potential solution, also paving the ground for edge-to-cloud data analytics and distributed Artificial Intelligence (AI) to obtain insights for decision-making and predictive maintenance. Despite the potential, however, there is a lack of comprehensive studies identifying key requirements for distributed edge-to-cloud IIoT and analyzing to what extent emerging IoT platforms meet those requirements. The scope of this article is to survey existing literature to identify key requirements in IIoT from the perspective of distributed edge-to-cloud computing. We provide a comparative analysis of three prominent IoT platforms, namely ThingsBoard, Eclipse Ditto, and Microsoft Azure IoT, and assess how these platforms meet the key IIoT requirements. Finally, we identify open challenges and potential research opportunities based on the insights gained from the analysis of the three IoT platforms, thereby setting the stage for future work.","2169-3536","","10.1109/ACCESS.2024.3454812","Green Transition North project co-funded by European Union(grant numbers:20359796); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10666680","Industrial Internet of Things (IIoT);edge-to-cloud computing;data analytics;IoT platforms","Industrial Internet of Things;Cloud computing;Scalability;Data analysis;Real-time systems;Distributed databases;Big Data","","1","","90","CCBYNCND","5 Sep 2024","","","IEEE","IEEE Journals"
"Demystifying Usability of Open-Source Computational Offloading Simulators: Performance Evaluation Campaign","D. Alekseeva; A. Ometov; E. S. Lohan","Faculty of Information Technology and Communication Sciences, Tampere University, Tampere, Finland; Faculty of Information Technology and Communication Sciences, Tampere University, Tampere, Finland; Faculty of Information Technology and Communication Sciences, Tampere University, Tampere, Finland",IEEE Sensors Journal,"14 Dec 2023","2023","23","24","30522","30534","Along with analysis and practical implementation, simulations play a key role in wireless networks and computational offloading research for several reasons. First, the simulations provide the ability to easily obtain the data for a complex system’s model evaluation. Second, simulated data provide a controlled environment for experimentation, allowing models and algorithms to be tested for robustness and identifying potential limitations before deploying them in real-world applications. Choosing the most appropriate tool for simulation might be challenging and depends on several factors, such as the main purpose, complexity of data, researcher skills, community support, and available budget. As of the time of the present analysis, several system-level open-source tools for modeling computational offloading also cover the systems’ communications side, such as CloudSim, CloudSim Plus, IoTSim-Edge, EdgeCloudSim, iFogSim2, PureEdgeSim, and YAFS. This work presents an evaluation of those based on the unique features and performance results of intensive workload- and delay-tolerant scenarios: XR with an extremely high data rate and workload; remote monitoring with a low data rate with moderate delays and workload requirements; and data streaming as a general human traffic with a relatively high bit rate but moderate workload. The work concludes that CloudSim provides a reliable environment for virtualization on the host resources, while YAFS shows minimal hardware usage, while IoTSim-Edge, PureEdgeSim, and EdgeCloudSim have fewer implemented features.","1558-1748","","10.1109/JSEN.2023.3310669","Doctoral Training Network in ELectronics, Telecommunications and Automation (DELTA); Pekka Ahonen Fund; Doctoral Grant of the Information Technology and Communications Science Faculty at Tampere University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10251116","Cloud computing;edge computing;fog computing;modeling;simulation","Cloud computing;Computational modeling;Edge computing;Data models;Sensors;Servers;Wireless sensor networks","","1","","45","CCBY","13 Sep 2023","","","IEEE","IEEE Journals"
"An Endogenous Security Study of Telematics Box in Intelligent Connected Vehicles","Z. Zhang; Y. Zhang; J. Zhang; J. Xie; S. Liu","Electric and Computer Engineering Department, National Digital Switching System Engineering and Technological Research Center, Zhengzhou, China; Endogenous Security Research Center, Purple Mountain Laboratories, Nanjing, China; Endogenous Security Research Center, Purple Mountain Laboratories, Nanjing, China; Electric and Computer Engineering Department, National Digital Switching System Engineering and Technological Research Center, Zhengzhou, China; Endogenous Security Research Center, Purple Mountain Laboratories, Nanjing, China",IEEE Embedded Systems Letters,"5 Dec 2024","2024","16","4","501","504","Intelligent connected vehicles (ICVs) are the result of technological advancements in the new era, greatly enhancing the driving experience. However, due to the complex nature of the system, the intelligence of devices, and the connectivity of data, a complex physical fusion system has been created. The in-vehicle Telematics Box (T-Box), serving as the central communication and data hub, faces challenges, such as network vulnerabilities, data privacy, and malicious attacks through untrusted software updates. Therefore, an in-vehicle T-Box with high reliability, security, and performance is an urgent product in the era of the Internet of Vehicles. This article investigates the endogenous security of the T-Box in ICV, with a focus on the integration of dynamic heterogeneous redundancy (DHR) architecture. The underlying idea is to transform the original system into multiple heterogeneous systems, where some of them handle the same business functions. By applying a consensus mechanism to detect malicious nodes and dynamically scheduling healthy nodes into the working mode, a self-purifying defense system with intrinsic security is formed. This design approach endows the T-Box with inherent defense capabilities against unknown vulnerabilities. Meanwhile, the in-vehicle T-Box is redesigned in the software and hardware implementation scheme. Experimental results demonstrate that the new design notably enhances and ensures the robust stability and elevated reliability of the in-vehicle T-Box. Evidently, the upgraded T-Box ensures the safe processing of in-vehicle CAN bus data.","1943-0671","","10.1109/LES.2024.3432593","National Key Research and Development Program Project(grant numbers:2022YFB3102800); Major Science and Technology Projects in Henan Province(grant numbers:221100240100); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10606440","Dynamic heterogeneous redundancy (DHR) architecture;endogenous security;reliability;telematics box","Security;Reliability;Hardware;Computer architecture;Operating systems;Numerical models;Virtualization","","1","","14","IEEE","23 Jul 2024","","","IEEE","IEEE Journals"
"Evaluating Performance and Resource Consumption of REST Frameworks and Execution Environments: Insights and Guidelines for Developers and Companies","S. D. Meglio; L. Libero Lucio Starace","Department of Electrical Engineering and Information Technology, Università degli Studi di Napoli Federico II, Naples, Italy; Department of Electrical Engineering and Information Technology, Università degli Studi di Napoli Federico II, Naples, Italy",IEEE Access,"15 Nov 2024","2024","12","","161649","161669","The REST (REpresentational State Transfer) paradigm has become essential for designing distributed applications that leverage the HTTP protocol, enabling efficient data exchange and the development of scalable architectures such as microservices. However, selecting an appropriate framework among the myriad available options, especially given the diversity of emerging execution environments, presents a significant challenge. Often, this decision neglects crucial factors such as performance and energy efficiency, favoring instead developer familiarity and popularity within the industry. To address this, we conducted a comprehensive benchmark study using a prototype REST API application provided by an industry partner, which was implemented multiple times using different REST API frameworks. We evaluated five different REST API frameworks across three popular programming languages, incorporating both traditional and emerging execution environments, resulting in twelve distinct configurations. Our results reveal significant differences in performance and computational resource consumption across different frameworks and execution environments, highlighting the necessity of making informed technology choices based on thorough analysis rather than convenience or familiarity. In addition to our findings, we offer other contributions to the field: an automated pipeline that benchmarks different configurations with various frameworks and execution environments, and a reference benchmark REST API that can be used in other studies. This research provides valuable insights and tools for developers and organizations aiming to select high-performance, resource-efficient technologies that promote environmental sustainability and reduce operational costs.","2169-3536","","10.1109/ACCESS.2024.3489892","Piano Nazionale di Ripresa e Resilienza Ministero dell’Università e della Ricerca(grant numbers:PE0000013-FAIR); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10741246","Execution environments;green computing;performance benchmark;performance testing;REST API;REST API frameworks;sustainable computing;web applications","Java;Benchmark testing;Python;Energy consumption;Energy efficiency;Sports;Software development management;Prototypes;Costs;Codes;Green computing;Green computing;Web services","","","","64","CCBY","1 Nov 2024","","","IEEE","IEEE Journals"
"Modular VNF Components Acceleration With FPGA Overlays","F. Bachini Lopes; A. E. Schaeffer-Filho; G. L. Nazar","Institute of Informatics, Federal University of Rio Grande do Sul, Porto Alegre, Brazil; Institute of Informatics, Federal University of Rio Grande do Sul, Porto Alegre, Brazil; Institute of Informatics, Federal University of Rio Grande do Sul, Porto Alegre, Brazil",IEEE Transactions on Network and Service Management,"10 Mar 2023","2023","20","1","846","857","Network Functions Virtualization (NFV) is a novel paradigm that aims to minimize operational and capital expenditures, by decoupling network functions from dedicated hardware and implementing them as Virtualized Network Functions (VNFs) instead. However, to fulfill such expectations, VNFs must be implemented efficiently, offering high performance and energy efficiency, which is not always feasible on General-Purpose Processors (GPPs). Thus, the use of reconfigurable accelerators, typically based on Field-Programmable Gate Arrays (FPGAs), has been proposed to offer higher efficiency whilst not forsaking the flexibility that is the core of the NFV paradigm. Not all VNFs or even VNF Components (VNFCs), however, are suitable for FPGA acceleration. This leads to new challenges related to identifying those VNFCs that should be deployed in FPGAs, maximizing the reuse of developed FPGA accelerators, and managing this heterogeneous infrastructure. To address these challenges, in this paper we present an enhanced design of VNFAccel, a platform to manage VNFCs in heterogeneous NFV infrastructures. We evaluate the performance and energy efficiency of the implemented functions in comparison to GPP-based solutions, showing that, when properly used, FPGAs can provide relevant benefits while maintaining the flexibility and reuse potential envisioned for NFV.","1932-4537","","10.1109/TNSM.2022.3211448","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) - Finance Code 001; Fundação de Amparo à Pesquisa do Estado do Rio Grande do Sul (FAPERGS)(grant numbers:#21/2551-0002089-0,#19/2551-0001645-0); Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq)(grant numbers:#311276/2021-0,#313124/2020-5); São Paulo Research Foundation (FAPESP)(grant numbers:#2021/06947-6); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910597","FPGA;network function virtualization;hardware acceleration","Field programmable gate arrays;Software;Network function virtualization;Guidelines;Firewalls (computing);Table lookup;Performance evaluation","","","","66","IEEE","4 Oct 2022","","","IEEE","IEEE Journals"
"Reducing Internal Collateral Damage From DDoS Attacks Through Micro-Service Cloud Architecture","A. Kumar; M. Agarwal","Department of Computer Science and Engineering, Indian Institute of Technology Patna, Patna, Bihar, India; Department of Computer Science and Engineering, Indian Institute of Technology Patna, Patna, Bihar, India",IEEE Transactions on Information Forensics and Security,"13 Jan 2025","2025","20","","1081","1091","Mitigating DDoS attacks poses a significant challenge for cyber security teams within victim organizations, as these attacks directly target service availability. Most DDoS mitigation solutions focus address the direct effects of DDoS attacks, such as service unavailability and network congestion, while the indirect effects, including collateral damage to legitimate users, receive substantially less attention in the present state-of-the-art. To address this gap, we propose a novel defense architecture designed to mitigate collateral damage and ensure service availability for legitimate users even under attack conditions. The proposed approach employs containerization, micro-services architecture, and traffic segmentation to enhance system resilience and fortify security. We send requests for two distinct services, namely an HTTP-based service and an SSH service, in order to analyze the collateral damage caused by the DDoS attack. The proposed architecture classifies incoming HTTP traffic into two categories: “benign traffic” and “suspicious traffic,” determined by the number of requests originating from the same source address. We tested this approach in three different scenarios (S-1, S-2, and S-3). Experimental results demonstrate that the proposed architecture effectively isolates suspicious traffic, mitigating its impact on benign services. This ensures the availability of critical services during a DDoS attack while minimizing collateral damage. In scenarios S-1, S-2, and S-3, it maintains service availability at 3%, 67%, and 98%, respectively, highlighting its efficacy in the face of varying levels of DDoS attack intensity. Furthermore, the architecture is extremely effective in reducing the collateral effects on SSH requests during a DDoS attack. In the S-1 scenario, SSH login time was reduced by 25%, 46%, and 27%, respectively. In the S-2 scenario, the reductions were 99%, 53%, and 29%. In the same vein, the system achieved reductions of 4%, 17%, and 99% in the S-3 scenario.","1556-6021","","10.1109/TIFS.2024.3516560","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10810456","Security and protection;cloud computing;DDoS attacks;containerization","Denial-of-service attack;Computer crime;Cloud computing;Prevention and mitigation;Containers;Virtual machines;Computer architecture;Computational modeling;Accuracy;Security","","","","31","IEEE","20 Dec 2024","","","IEEE","IEEE Journals"
"Voice Frequency-Based Gender Classification Using Convolutional Neural Network for Smart Home","N. Nasaruddin; M. A. P. Pratama Tresma; M. Khamdan Muchamad; Z. Fuadi","Department of Electrical and Computer Engineering, Universitas Syiah Kuala, Banda Aceh, Indonesia; Department of Electrical and Computer Engineering, Universitas Syiah Kuala, Banda Aceh, Indonesia; Department of Electrical and Computer Engineering, Universitas Syiah Kuala, Banda Aceh, Indonesia; Department of Mechanical and Industrial Engineering, Universitas Syiah Kuala, Banda Aceh, Indonesia",IEEE Access,"5 Aug 2024","2024","12","","104190","104203","The smart home’s functional requirements should include the capability to differentiate between various user categories, such as gender and voice recognition. The data-driven Internet of Things (IoT) can present challenges for the elderly and people with disabilities, but voice recognition technology could offer an effective solution. In addition, developing an accurate gender prediction model for voice recognition is still challenging due to the large time variation and randomness. Therefore, we propose gender classification and detection models based on voice frequency using Convolutional Neural Networks (CNN) with ResNet50 and ResNet101 architectures to enhance smart home functionality. We also introduce an algorithm for converting voice frequencies into images to speed up the recognition and detection processes. The research method involves converting voice frequencies into images to expedite the recognition and detection processes. The CNN models were trained and tested with various learning rates using audio datasets. Performance was evaluated through simulations that measured training accuracy, validation accuracy, recall, precision, and F1 scores. The simulation results show high training accuracy: ResNet50 achieved 99.67% and ResNet101 achieved 99.82%. The validation accuracy of the models also exceeded the accuracy of traditional CNN models in previous studies. The simulation results based on recall, precision, and F1 score for each proposed model are 99.3%, 100%, and 99.65%, respectively. Finally, we successfully used the ResNet50 model to create a low-latency smart home prototype. Thus, this paper significantly contributes to the practical applications of voice-based gender recognition in smart home environments with high accuracy and efficiency in detection.","2169-3536","","10.1109/ACCESS.2024.3434547","Ministry of Education, Culture, Research, and Technology, Indonesia(grant numbers:590/UN11.2.1/PT.01.03/DPRM/2023); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10613033","Voice frequency;convolutional neural networks (CNN);ResNet50;Resnet101;smart home","Smart homes;Speech recognition;Deep learning;Convolutional neural networks;Accuracy;Residual neural networks;Prototypes","","","","42","CCBYNCND","29 Jul 2024","","","IEEE","IEEE Journals"
"FACT: Facilitating Trustworthy Services in Mobile IoT Systems","P. Dass; S. Misra","Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India",IEEE Systems Journal,"12 Dec 2023","2023","17","4","5511","5518","Mobile Internet of Things (IoT) users possess dynamic service requirements in a real IoT system and always try to get services from trusted entities, which we term as facility nodes. However, the trustworthiness of the facility nodes varies from location to location, depending upon the service quality and user feedback. Therefore, a service provider may have different trust scores for its deployed facility nodes in different locations. It is always crucial for an IoT user to ensure that service has been taken from trustworthy nodes. In this article, we consider an IoT service provisioning architecture, where an IoT user requests services from nearby trusted nodes that are deployed by service providers other than the user's own registered service provider. To serve the requests from these unregistered users, the facility nodes incur additional costs. Furthermore, due to heterogeneous service requirements, IoT users prefer composite microservices from a set of facility nodes. In such cases, it is difficult to design a pricing scheme to incentivize the facility nodes. Therefore, we propose a pricing mechanism based on a two-stage Stackelberg game and analyze the interactions between the IoT users and the facility nodes, while considering the trust scores and service charges. In the game model, the facility nodes act as the leaders, and the IoT users act as the followers. Using backward induction, we analyze the existence and uniqueness of the Stackelberg equilibrium. Finally, the numerical results based on the mobility traces of a real map show that the proposed scheme incentivizes both the IoT users and the facility nodes, while ensuring trusted services.","1937-9234","","10.1109/JSYST.2023.3321665","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10287681","Game theory;mobile users;pricing;service provisioning;trustworthy Internet of Things (IoT)","Internet of Things;Costs;Pricing;Games;Cloud computing;Resource management;Delays","","","","31","IEEE","18 Oct 2023","","","IEEE","IEEE Journals"
"Adaptive Context Monitoring Framework for Enhancing Caching Efficiency in Context Management Platforms","A. Manchanda; P. P. Jayaraman; A. Banerjee; K. Fizza; A. Zaslavsky","SoSCET - Department of Computing Technologies, Swinburne University of Technology, Melbourne, VIC, Australia; SoSCET - Department of Computing Technologies, Swinburne University of Technology, Melbourne, VIC, Australia; SoSCET - Department of Computing Technologies, Swinburne University of Technology, Melbourne, VIC, Australia; SoSCET - Department of Computing Technologies, Swinburne University of Technology, Melbourne, VIC, Australia; School of Information Technology, Deakin University, Melbourne, VIC, Australia",IEEE Access,"1 Nov 2024","2024","12","","157612","157629","As the Internet of Things (IoT) continues to expand, the volume of data generated by IoT devices and the demand for IoT applications is increasing exponentially. These applications critically rely on real-time context reasoned from IoT data for effective decision-making and actuation, thereby making the accessibility of this context crucial. This study introduces a novel adaptive Context Monitoring Framework (CMF) for enhancing context caching efficiency in Context Management Platforms (CMPs) to better support the near real-time needs of IoT applications. Our proposed framework integrates two novel components: the Context Attributes Prioritisation Engine (CAPE), which prioritises and assigns weights to the context, and the Adaptive Context Management Engine (ACME), which dynamically adjusts thresholds for each context based on incoming query volumes and context cache performance. Combined, our hybrid approach ensures timely updates of context within the cache while also serving context in real-time (reducing any query response latency). Our approach is effective for dynamic changes in an IoT environment through the adaptive approach of continuously monitoring and updating the cached context. We implemented the proposed adaptive framework using a CMP namely the context-as-a-service (CoaaS) platform and evaluated it using real-world datasets obtained from a smart city application. A thorough experimental evaluation demonstrated a marked improvement in cache efficiency, achieving a 90% cache hit rate and reducing the cache expiry ratio to 5%.","2169-3536","","10.1109/ACCESS.2024.3486103","Australian Research Council (ARC) Discovery Project(grant numbers:DP200102299); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10734123","CAPE;CoaaS;context freshness;context caching;hybrid approach;IoT","Internet of Things;Monitoring;Real-time systems;Engines;Roads;Delays;Australia;Vehicle dynamics;Time factors;Smart cities","","","","49","CCBY","24 Oct 2024","","","IEEE","IEEE Journals"
"Advancing Interoperable IoT-Based Access Control Systems: A Unified Security Approach in Diverse Environments","M. Penica; M. Bhattacharya; W. O’Brien; S. McGrath; M. Hayes; E. O’Connell","Department of Electronic and Computer Engineering, University of Limerick, Limerick, Ireland; School of Information and Technology, Illinois State University, Normal, IL, USA; Department of Electronic and Computer Engineering, University of Limerick, Limerick, Ireland; Department of Electronic and Computer Engineering, University of Limerick, Limerick, Ireland; Department of Electronic and Computer Engineering, University of Limerick, Limerick, Ireland; Department of Electronic and Computer Engineering, University of Limerick, Limerick, Ireland",IEEE Access,"14 Feb 2025","2025","13","","27767","27782","The convergence of the Internet of Things (IoT) and the construction industry represents a potent opportunity to fundamentally redefine the processes of constructing and maintaining buildings. The integration of IoT technology has the potential to revolutionize building safety, efficiency, and sustainability, offering a promising avenue for innovation and advancement within the industry. In this work, we propose an innovative design and implementation of an interoperable physical access control IoT-based system that can be easily integrated with existing access control security systems and used in various environments such as offices, homes, and public buildings. Based on our developed and implemented physical access control IoT-based system, we identified several challenges in the current state of access control systems. These challenges include difficulties in integrating different security systems, the absence of interoperable standards, and the need for further research to enhance the scalability and performance of such systems.","2169-3536","","10.1109/ACCESS.2025.3538748","Science Foundation Ireland through the CONFIRM Platform(grant numbers:16/RC/3918); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10870056","Internet of Things (IoT);interoperability;physical access control;electronics;smart devices;wireless communication;software design","Access control;Security;Protocols;Interoperability;Hardware;Radiofrequency identification;QR codes;Service-oriented architecture;Buildings;Authentication","","","","33","CCBY","4 Feb 2025","","","IEEE","IEEE Journals"
"EdgeSim++: A Realistic, Versatile, and Easily Customizable Edge Computing Simulator","Q. Lu; G. Li; H. Ye","College of Computer Science and Engineering and the Guangxi Key Laboratory of Embedded Technology and Intelligent System, Guilin University of Technology, Guilin, China; College of Computer Science and Engineering, Guilin University of Technology, Guilin, China; College of Computer Science and Engineering and the Guangxi Key Laboratory of Embedded Technology and Intelligent System, Guilin University of Technology, Guilin, China",IEEE Internet of Things Journal,"25 Oct 2024","2024","11","21","35341","35360","In the edge computing environment, due to factors, such as expensive devices, complex scenarios, and fine control requirements, it is necessary to use simulators to construct heterogeneous devices, simulate task offloading scenarios, and evaluate and optimize decision-making algorithms. However, existing edge computing simulators overly simplify the simulation process, neglecting device interactions and task complexities, making it difficult to finely control the task offloading process and customize complex scenarios. We propose EdgeSim++, a realistic, versatile, and easily customizable edge computing simulator. EdgeSim++ has the capability to generate a batch of heterogeneous devices with various features, supporting the simulation of multilayer cloud-edge-end scenarios and various multiaccess edge computing architectures. It dynamically sets task and device resource attributes, enables devices to interact and schedule behaviors through the network, and facilitates easy customization of both machine learning and nonmachine learning offloading strategies. Additionally, EdgeSim++ supports network topology visualization and result visualization, providing intuitive displays of task transmission processes and offloading results, and real-time monitoring of device resource changes. To test the effectiveness of EdgeSim++, we provided default machine learning and nonmachine learning offloading strategies, detailed simulation steps, demonstrated scenario construction, showcased process control, monitored resource changes, and compared algorithm effects. The results illustrated that our simulator could realistically simulate complex edge computing scenarios, flexibly control decision processes, and customize complex offloading algorithms, exhibiting good scalability and reusability.","2327-4662","","10.1109/JIOT.2024.3434641","National Natural Science Foundation of China(grant numbers:62362018); Guangxi Key Research and Development Program(grant numbers:GUIKEAB23075116); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10612841","Artificial intelligence;cloud computing;edge computing;machine learning;modeling and simulation;resource management;task offloading","Edge computing;Task analysis;Internet of Things;Resource management;Machine learning;Computer architecture;Machine learning algorithms","","","","39","IEEE","29 Jul 2024","","","IEEE","IEEE Journals"
"Simulating Fog of Medical Things: Research Challenges and Opportunities","A. Pati; A. Panigrahi; M. Parhi; B. Kumar Pattanayak; B. Sahu; S. Kant","Department of Computer Science and Engineering, Siksha ‘O’ Anusandhan (Deemed to be University), bhubaneswar, Odisha, India; Department of Computer Science and Engineering, Siksha ‘O’ Anusandhan (Deemed to be University), bhubaneswar, Odisha, India; Center for Data Science, Siksha ‘O’ Anusandhan (Deemed to be University), bhubaneswar, Odisha, India; Department of Computer Science and Engineering, Siksha ‘O’ Anusandhan (Deemed to be University), bhubaneswar, Odisha, India; Department of Information Technology, Vardhaman College of Engineering (Autonomous), Hyderabad, Telangana, India; Department of Management, College of Business and Economics, Bule Hora University, Ethiopia, Africa",IEEE Access,"17 Oct 2024","2024","12","","146527","146550","Technology has a significant impact on medical applications at the current moment. Contemporary computers are capable of processing a lot of patient medical records quickly. Due to recent advancements in the Internet-of-Things (IoT) and medical applications, patient data may be dispersed over several places. Worldwide, the IoT connects numerous devices for e-healthcare systems. The medical data monitoring and tracking field, exercise programs, and remote medical help are expanding within the e-healthcare systems. IoT-based technologies are now being used in e-healthcare systems, which can relieve pressure on e-healthcare systems, lower medical expenses, and speed up computing and processing. In the IoT setting, cloud computing, which contains centralized data centers, was developed to manage more extensive and sophisticated e-healthcare data. The central server governs the data for all IoT devices. Problems with IoT and Cloud integration only include latency, bandwidth overuse, delays in real-time responses, security, privacy, integrity, etc. The ideas of fog computing and edge computing were developed to solve the above-mentioned problems. A thorough literature overview on Fog-based medical applications using IoT is provided in this article, i.e., Fog of Medical Things (FoMT), that explores the simulators that may be employed to create and assess new Fog-related theories as well as the key attributes of Fog computing frameworks. This review also emphasizes the difficulties in the field and some unanswered questions. This study can serve as a crucial road map for the future creation of Fog-based e-healthcare IoT applications.","2169-3536","","10.1109/ACCESS.2024.3468015","Siksha ‘O’ Anusandhan, Bhubaneswar, Odisha, India; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10693431","FoMT;IoT;fog computing;cloud computing;simulation tools;e-healthcare systems","Cloud computing;Edge computing;Internet of Things;Bandwidth;Medical services;Computational modeling;Security;Electronic healthcare","","","","206","CCBYNCND","25 Sep 2024","","","IEEE","IEEE Journals"
"Multi-Access Edge Computing Resource Slice Allocation: A Review","F. Bahramisirat; M. A. Gregory; S. Li","School of Engineering, RMIT University, Melbourne, VIC, Australia; School of Engineering, RMIT University, Melbourne, VIC, Australia; School of Engineering, RMIT University, Melbourne, VIC, Australia",IEEE Access,"17 Dec 2024","2024","12","","188572","188589","The global deployment of fifth generation mobile cellular (5G) and beyond (5GB) communication systems has accelerated the growth of the Internet of Things (IoT) and the Internet of Everything (IoE). New applications and services that add complexity to the network, and require low latency, have necessitated new architectures that facilitate dynamic resource allocation. The network edge is changing, as Multi-Access Edge Computing (MEC) is deployed with new responsive designs that include dynamic resource slice allocation that supports the varying needs of new and innovative applications and services. Augmented reality, extended reality and virtual reality applications are key drivers for the changing network architecture. IoT and IoE data aggregation and analysis at the network edge has required new approaches to designing edge network architectures that achieve seamless access anywhere and anytime, ultra-low latency, edge computing and storage, and ultra-high bandwidth. MEC and Network Slicing (NS) are evolving to provide a key part of the solution to the new challenges. MEC allows operators to connect user devices to edge cloud computing and storage systems with low latency and high bandwidth. However, a practical configuration is required to facilitate cloud-based applications and services at the network edge and to address end-user demands. The MEC resources may be flexibly and dynamincally partitioned into separate slices. This paper will review how the integration of MEC and NS can be utilised for efficient resource allocation in edge networks. This article comprehensively reviewed the architecture of network slicing and edge computing. A review of MEC and NS in support of 5G and 5GB has been included and current challenges identified. This article aims to serve as an insightful guide to inspire further research into 5GB communication systems and edge computing.","2169-3536","","10.1109/ACCESS.2024.3515077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10792437","Network slicing;multi-access edge computing;5G;6G;resource allocation;radio access network","Resource management;5G mobile communication;Network slicing;Ultra reliable low latency communication;Radio access networks;Network function virtualization;Internet of Things;Cloud computing;Surveys;Reviews","","","","101","CCBY","11 Dec 2024","","","IEEE","IEEE Journals"
"From Technical Prerequisites to Improved Care: Distributed Edge AI for Tomographic Imaging","B. Akdemir; H. Faheem Shahid; M. A. K. Brix; J. Lääkkölä; J. Islam; T. Kumar; J. Reponen; M. T. Nieminen; E. Harjula","Centre for Wireless Communications—Networks and Systems (CWC-NS), University of Oulu, Oulu, Finland; Centre for Wireless Communications—Networks and Systems (CWC-NS), University of Oulu, Oulu, Finland; University of Oulu, Oulu, Finland; Centre for Wireless Communications—Networks and Systems (CWC-NS), University of Oulu, Oulu, Finland; Centre for Wireless Communications—Networks and Systems (CWC-NS), University of Oulu, Oulu, Finland; Department of Information and Communications Engineering, Aalto University, Espoo, Finland; University of Oulu, Oulu, Finland; University of Oulu, Oulu, Finland; Centre for Wireless Communications—Networks and Systems (CWC-NS), University of Oulu, Oulu, Finland",IEEE Access,"24 Jan 2025","2025","13","","14317","14343","Recent years have seen a surge in AI-driven medical image processing, leading to significant improvements in diagnostic performance. However, medical imaging technologies tend to create staggering volumes of medical data, necessitating high-performance computing. Cloud systems with robust GPUs and resource capacity are optimal choices for DL-based medical image processing. However, transferring data to the cloud for processing strains communication links, introduces high communication latency, and raises privacy and security concerns. Consequently, despite the undisputed benefits of cloud computing, dedicated standalone local computers are still used for image reconstruction in today’s systems. This localized strategy uses expensive hardware inefficiently and falls short of scalability and maintainability. Edge computing emerges as an innovative concept by bringing cloud processing capabilities closer to data sources. A continuum of computing including local, edge, and cloud tiers would offer a promising solution for medical image processing. According to literature survey, there are no significant works on utilizing edge cloud continuum for CBCT imaging. To fill this gap, we introduce novel 3-TECC architectural concept, specifically designed for CBCT data reconstruction in medical imaging. This article explores the evolving synergy among medical imaging, distributed AI, containerized solutions, and edge-cloud continuum technologies, highlighting their clinical implications and illuminating the potential for transformative patient care. We uncover challenges and opportunities this convergence provides with the CBCT image reconstruction use case, while aligning with regulatory compliance. The proposed 3-TECC architecture advocates a decentralized data processing paradigm, reducing reliance on the centralized approach and emphasizing the role of local-edge computing.","2169-3536","","10.1109/ACCESS.2025.3530297","Business Finland under Tomohead(grant numbers:8095/31/2022); Research Council of Finland DigiHealth(grant numbers:326291); Flagship of Advanced Mathematics for Sensing, Imaging, and Modelling(grant numbers:359186); 6G Flagship Programme(grant numbers:346208); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10843208","CBCT;distributed AI;edge computing;edge cloud continuum;GDPR;medical imaging","Imaging;Medical diagnostic imaging;Cloud computing;Image reconstruction;Image edge detection;Computer architecture;Computed tomography;Edge AI;Hospitals;Technical requirements","","","","162","CCBY","16 Jan 2025","","","IEEE","IEEE Journals"
"A Secure and Efficient BlockChain and Distributed Ledger Technology-Based Optimal Resource Management in Digital Twin Beyond 5G Networks Using Hybrid Energy Valley and Levy Flight Distributer Optimization Algorithm","K. Suresh Kumar; J. A. Alzubi; N. M. Sarhan; E. M. Awwad; V. Kandasamy; G. Ali","Department of Information Technology, Saveetha Engineering College, Chennai, Tamil Nadu, India; Faculty of Engineering, Al-Balqa Applied University, As-Salt, Jordan; Department of Quantitative Analysis, College of Business Administration, King Saud University, Riyadh, Saudi Arabia; Department of Electrical Engineering, College of Engineering, King Saud University, Riyadh, Saudi Arabia; Department of Information Technology, Panimalar Engineering College, Poonamallee, Chennai, Tamil Nadu, India; Department of Information Technology, Muni University, Arua, Uganda",IEEE Access,"14 Aug 2024","2024","12","","110331","110352","This paper aims to establish a virtual object management system, as well as optimal task scheduling using the foundation of Digital Twins (DT), to improve the user’s experience with management and to accomplish the task efficiently. On the other hand, offloading tasks using IoT gadgets to edge computing, fails to speed up control by users. The capabilities of the DT are provided by executing processes such as visualization, virtualization, synchronization, and simulation. The optimal selection of the virtual objects for the DT is done by utilizing the implemented Hybrid Energy Valley with Lévy Flight Distribution Optimization (HEV-LFDO) in order to optimally offload the task by the edge devices. The optimal selection of the virtual objects is done with the aid of the HEV-LFDO in the DT by considering the total cost of executing all tasks using the selected virtual objects and the decision variables to determine whether a virtual object is taken for executing a task or not as the constraint. The data for performing resource management is secured using the blockchain or distributed ledger technology. This accounts for the minimization of the local loss function. Finally, the secured data is considered for optimal resource management tasks. The optimal resource management is done using the same HEV-LFDO. This optimal resource management is carried out by considering the constraints like the cost of assigning a virtual object for the task to the edge device, and the cost of assigning the task to the edge device. These two costs are analyzed by taking the network’s bandwidth, energy consumption, and computational resources into consideration. Experimental verifications are conducted on the executed optimal resource management scheme to prove the ability of the implemented model to be integrated with the edge computing network. The overall processing time as well as the latency are also minimized by executing the optimal resource management scheme.","2169-3536","","10.1109/ACCESS.2024.3435847","King Saud University, Riyadh, Saudi Arabia, through Researchers Supporting Program(grant numbers:RSPD2024R1052); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10614449","Secure and efficient blockchain;distributed ledger technology;optimal resource management;digital twin beyond 5G networks;hybrid energy valley with lévy flight distribution optimization","Resource management;Task analysis;Blockchains;Costs;Optimization;Distributed ledger;5G mobile communication;Security;Digital twins;Object detection;Virtualization","","","","29","CCBYNCND","30 Jul 2024","","","IEEE","IEEE Journals"
"Empowering Healthcare With Cyber-Physical System—A Systematic Literature Review","H. M. Khater; F. Sallabi; M. A. Serhani; E. Barka; K. Shuaib; A. Tariq; M. Khayat","College of Information Technology, United Arab Emirates University, Al Ain, United Arab Emirates; College of Information Technology, United Arab Emirates University, Al Ain, United Arab Emirates; College of Computing and Informatics, University of Sharjah, Sharjah, United Arab Emirates; College of Information Technology, United Arab Emirates University, Al Ain, United Arab Emirates; College of Information Technology, United Arab Emirates University, Al Ain, United Arab Emirates; College of Information Technology, United Arab Emirates University, Al Ain, United Arab Emirates; College of Information Technology, United Arab Emirates University, Al Ain, United Arab Emirates",IEEE Access,"18 Jun 2024","2024","12","","83952","83993","Cyber-physical systems (CPS) are recognized for their intelligence as they seamlessly interact with humans, enhancing the physical world through computation, communication, and control. Over the last few years, the evolution of CPS, including IoT components, has significantly impacted many facets of people’s lifestyles. It has been immersed in a wide range of services and applications in various areas, including manufacturing, healthcare, and energy. However, the interrelationship between the cyber and physical worlds gives rise to a multitude of research problems and challenges. In the healthcare field, for instance, CPS introduces complexities related to interoperability, privacy, data security, and real-time data processing with critical implications for the reliability and safety of medical processes. To address these challenges and harness the full potential of CPS in healthcare, this paper, through a comprehensive literature review, aims to discuss cutting-edge CPS technologies and solutions that hold promise for healthcare applications. To this end, we propose a comprehensive architectural model that can serve as a benchmark for implementing CPS in healthcare applications. This model thoroughly details how services, components, and technologies can be integrated to transform massive raw data collected from the physical world into valuable information for an enhanced decision-making process. Finally, a use case on healthcare CPS is presented, outlining its characteristics, the role that different technologies have played in its development, and the major challenges in implementing such systems successfully. This study provides a cohesive understanding of the role CPS can play in empowering healthcare while offering insights into the challenges, and future research trends.","2169-3536","","10.1109/ACCESS.2024.3407376","United Arab Emirates University(grant numbers:31R227); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10542115","Cyber-physical systems;healthcare;real-time processing;IoT","Medical services;Sensors;Real-time systems;Monitoring;Intelligent sensors;Cloud computing;Cyber-physical systems;Internet of Things","","","","252","CCBYNCND","30 May 2024","","","IEEE","IEEE Journals"
"Optimizing Cloud Performance: A Microservice Scheduling Strategy for Enhanced Fault-Tolerance, Reduced Network Traffic, and Lower Latency","A. Alelyani; A. Datta; G. M. Hassan","Department of Computer Science and Software Engineering, The University of Western Australia, Perth, WA, Australia; Department of Computer Science and Software Engineering, The University of Western Australia, Perth, WA, Australia; Department of Computer Science and Software Engineering, The University of Western Australia, Perth, WA, Australia",IEEE Access,"8 Mar 2024","2024","12","","35135","35153","The emergence of microservice architecture has brought significant advancements in software development, offering improved scalability and availability of applications. Cloud computing benefits from microservice architecture by mitigating the risks of single failures and ensuring compliance with service-level agreements. However, using microservice architecture presents two challenges: 1) managing network traffic, which leads to latency and network congestion; and 2) inefficient resource allocation for microservices. Current approaches have limitations in addressing these challenges. To overcome these limitations, we propose a novel scheduling strategy that schedules microservice replicas using a modified particle swarm optimization algorithm to place them on the most suitable physical machine. Additionally, we balance the load across physical machines in the cluster using a simple round-robin algorithm. Furthermore, our scheduling strategy integrates with Kubernetes to tackle resource allocation and deployment challenges. The proposed strategy has been evaluated by simulating two scenarios using Alibaba and Google datasets. The experimental results demonstrate the effectiveness of our strategy in reducing traffic, balancing load, and utilizing CPU and memory efficiently.","2169-3536","","10.1109/ACCESS.2024.3373316","University of Western Australia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10459012","Microservice;network traffic cost;PSO;resource utilization;Alibaba;VM;container;scheduling strategy","Microservice architectures;Cloud computing;Telecommunication traffic;Resource management;Costs;Containers;Task analysis;Telecommunication network management;Scheduling;Software development management;Performance evaluation;Fault tolerance","","","","54","CCBY","4 Mar 2024","","","IEEE","IEEE Journals"
"A Survey of Industrial AIoT: Opportunities, Challenges, and Directions","K. S. Awaisi; Q. Ye; S. Sampalli","Faculty of Computer Science, Dalhousie University, Halifax, NS, Canada; Faculty of Computer Science, Dalhousie University, Halifax, NS, Canada; Faculty of Computer Science, Dalhousie University, Halifax, NS, Canada",IEEE Access,"19 Jul 2024","2024","12","","96946","96996","Internet of Things (IoT) is an important technology employed in a variety of different applications, such as transportation, healthcare, and manufacturing. In recent years, the number of IoT devices deployed globally has been increasing at a rapid pace and is estimated to reach 20 billion by the end of 2025. In modern industry, IoT plays a pivotal role by monitoring the condition of industrial machines and, consequently, improving the efficiency of industrial processes. To optimize the efficiency of industrial IoT applications, various Artificial Intelligence (AI) techniques have been adopted, leading to a new computing paradigm, namely, Industrial Artificial Intelligence of Things (i.e. Industrial AIoT). In this paper, we describe the challenges to tackle and the opportunities to explore in Industrial AIoT. Specifically, we first review the use of state-of-the-art AI methods in Industrial AIoT applications, with a focus on Deep Learning (DL) and Machine Learning (ML) techniques. Thereafter, we present a series of important applications of Industrial AIoT. The key challenges associated with the implementation of Industrial AIoT applications are also discussed. In addition, the societal and economic impacts of Industrial AIoT are briefly described. Finally, we outline the future research directions in Industrial AIoT, which should be further investigated to fully utilize the potential of this innovative technology.","2169-3536","","10.1109/ACCESS.2024.3426279","Natural Sciences and Engineering Research Council of Canada(grant numbers:RGPIN-2022-04754); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10591982","Internet of Things (IoT);industrial internet of things (IIoT);artificial intelligence (AI);artificial intelligence of things (AIoT);industrial AIoT;machine learning (ML);deep learning (DL)","Artificial intelligence;Industrial Internet of Things;Sensors;Industries;Monitoring;Logic gates;Internet of Things;Machine learning;Deep learning","","","","371","CCBYNCND","10 Jul 2024","","","IEEE","IEEE Journals"
"When Holographic Communication Meets Metaverse: Applications, Challenges, and Future Trends","A. V. Shvetsov; S. Hamood Alsamhi","Department of Road Transport and Car Service Operations, North-Eastern Federal University, Yakutsk, Russia; ICT, Bahrain Polytechnic, Isa Town, Bahrain",IEEE Access,"31 Dec 2024","2024","12","","197488","197515","Holographic communication represents a transformative technology for reshaping the digital interaction landscape by enabling the creation of realistic, immersive, and interactive 3D experiences. This survey overviews holographic communication and its integration with the Metaverse technologies’ concepts, advantages, uses, and many applications. Furthermore, we examine a new paradigm for integrating holographic communication with the Metaverse, emphasizing how holography enhances immersive quality of virtual environments within the Metaverse, making interactions more lifelike and engaging. Extending the integration of holographic communication and Metaverse, we examine this combination’s numerous uses in various applications across various industries, such as education where virtual classrooms and 3D simulations redefine remote learning, a business where virtual meetings and product demonstrations create more impactful customer engagements, entertainment where immersive gaming and 3D broadcasting transform user experiences, healthcare where remote consultations and surgical simulations enhance medical training and accessibility, and remote assistance where real-time holographic support improves technical troubleshooting. In addition, we discuss the challenges and prospects of integration of holographic communication into Metaverse ecosystems, emphasizing key approaches and technical developments emerging technologies such as AI-driven content optimization, advanced coding and compression techniques, and new paradigms like terahertz communication and quantum holography. This survey highlights the revolutionary potential of holographic communication and offers insightful information on how it will influence digital engagement and connectivity in the future, eventually opening up new avenues in the quickly changing Metaverse landscape.","2169-3536","","10.1109/ACCESS.2024.3514576","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10788695","Extended reality;digital interaction;immersive communications;holographic communication;Metaverse;connectivity;virtual environments;interaction;immersive experiences","Metaverse;Surveys;Three-dimensional displays;Medical services;Education;Entertainment industry;Ecosystems;Business;Avatars;6G mobile communication","","","","162","CCBY","11 Dec 2024","","","IEEE","IEEE Journals"
"An Open API Architecture to Discover the Trustworthy Explanation of Cloud AI Services","Z. Wang; Y. Liu; J. Huang","Department of Electrical, Computer Engineering, Concordia University, Montréal, QC, Canada; Department of Electrical, Computer Engineering, Concordia University, Montréal, QC, Canada; Department of Electrical, Computer Engineering, Concordia University, Montréal, QC, Canada",IEEE Transactions on Cloud Computing,"6 Jun 2024","2024","12","2","762","776","This article presents the design of an open-API-based explainable AI (XAI) service to provide feature contribution explanations for cloud AI services. Cloud AI services are widely used to develop domain-specific applications with precise learning metrics. However, the underlying cloud AI services remain opaque on how the model produces the prediction. We argue that XAI operations are accessible as open APIs to enable the consolidation of the XAI operations into the cloud AI services assessment. We propose a design using a microservice architecture that offers feature contribution explanations for cloud AI services without unfolding the network structure of the cloud models. We can also utilize this architecture to evaluate the model performance and XAI consistency metrics showing cloud AI services’ trustworthiness. We collect provenance data from operational pipelines to enable reproducibility within the XAI service. Furthermore, we present the discovery scenarios for the experimental tests regarding model performance and XAI consistency metrics for the leading cloud vision AI services. The results confirm that the architecture, based on open APIs, is cloud-agnostic. Additionally, data augmentations result in measurable improvements in XAI consistency metrics for cloud AI services.","2168-7161","","10.1109/TCC.2024.3398609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10529172","Explainable AI (XAI);microservices;cloud model service;software architecture;software quality","Artificial intelligence;Cloud computing;Computer architecture;Computational modeling;Data models;Measurement;Microservice architectures","","","","56","IEEE","10 May 2024","","","IEEE","IEEE Journals"
"COPA: Experimenter-Level Container Orchestration for Networking Testbeds","H. C. C. de Resende; M. A. K. Schimuneck; C. B. Both; J. A. Wickboldt; J. M. Marquez-Barja","Imec, University of Antwerp, Antwerp, Belgium; Institute of Informatics, Federal University of Rio Grande do Sul (UFRGS), Porto Alegre, Brazil; Applied Computing Graduate Program, University of Vale do Rio dos Sinos (UNISINOS), São Leopoldo, Brazil; Institute of Informatics, Federal University of Rio Grande do Sul (UFRGS), Porto Alegre, Brazil; Imec, University of Antwerp, Antwerp, Belgium",IEEE Access,"18 Nov 2020","2020","8","","201781","201798","As Cloud Computing (CC) branched areas such as Multi-access Edge Computing (MEC) and Fog computing are still on growing research interest. The creation of new tools to improve quality and speed the experimentation in such areas is a general interest. In this article, we propose COPA, an experimenter-level container orchestration tool for networking testbeds. This tool provides a friendly interface for the experimenter test container orchestration algorithms which can start, stop, copy, and even migrate a container from one host to another. COPA also includes network/resources monitoring to feed the experimenter's orchestration algorithm so that it can make decisions based on real-time environment information. Furthermore, the experimenter can automatize the experiment scenario setup and deployment by pre-configuring in COPA. This tool helps the experimenter in testing different scenarios and quickly changing experiment parameters. Considering these features, COPA aims to provide an experimentation architecture to deploy and test container orchestration algorithms. Furthermore, we provide a case study explaining how COPA can be a key tool in the MEC and Network Function Virtualization (NFV) experimentation environments. This tool was already deployed in Federated Union of Telecommunications Research Facilities for an EU-Brazil Open Laboratory (FUTEBOL) testbeds as part of the control framework and was well validated by the project reviewers and partners.","2169-3536","","10.1109/ACCESS.2020.3035619","European Union’s Horizon 2020 through the 5G for Connected and Automated Road Mobility in the European unioN (5 G-CARMEN) Project(grant numbers:825012); Federated Union of Telecommunications Research Facilities for an EU-Brazil Open Laboratory (FUTEBOL) Project(grant numbers:688941); Brazilian Ministry of Science, Technology, Innovation, and Communication (MCTIC) through Rede Nacional de Ensino e Pesquisa (RNP), Centro de Tecnologia da Informação e Comunicação (CTIC), and Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9247110","Testbed;multi-access edge computing;future networks;container;orchestration","Containers;Tools;Monitoring;Virtualization;Computer architecture;Graphical user interfaces;Edge computing","","","","45","CCBY","3 Nov 2020","","","IEEE","IEEE Journals"
"Edge Computing Management With Collaborative Lazy Pulling for Accelerated Container Startup","C. -C. Chen; Y. Chiang; Y. -C. Lee; H. -Y. Wei","Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan",IEEE Transactions on Network and Service Management,"19 Dec 2024","2024","21","6","6437","6450","With the growing demand for latency-sensitive applications in 5G networks, edge computing has emerged as a promising solution. It enables instant response and dynamic resource allocation based on real-time network information by moving resources from the cloud to the network edge. Containers, known for their lightweight nature and ease of deployment, have been recognized as a valuable virtualization technology for service deployment. However, the prolonged startup time of containers can lead to long response time, particularly in edge computing scenarios characterized by long propagation time, frequent deployment, and migration. In this paper, we comprehensively consider image caching, container assignment, and registry selection problem in an edge system. To our best effort, there is no existing work that has taken all the above aspects into account. To address the problem, we propose a novel image caching strategy that employs partial caching, allowing local registries to cache either the least functional or complete version of application images. In addition, a container assignment and registry selection problem is solved by using an edge-based collaborative lazy pulling algorithm. To evaluate the performance of our proposed algorithms, we conduct experiments with real-world app usage data and popular images in a testbed environment. The experimental results demonstrate that our algorithms outperform traditional greedy algorithms in terms of average user response time and cache hit rate.","1932-4537","","10.1109/TNSM.2024.3462408","National Science and Technology Council (NSTC) of Taiwan(grant numbers:113-2628-E-002-032-); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10681577","Edge computing;5G;container;caching;resource allocation","Containers;Image edge detection;Edge computing;Time factors;Delays;Costs;Collaboration","","","","46","IEEE","17 Sep 2024","","","IEEE","IEEE Journals"
"Artificial Intelligence Techniques for Securing Fog Computing Environments: Trends, Challenges, and Future Directions","D. Alsadie","Department of Computer Science and Artificial Intelligence, College of Computing, Umm Al-Qura University, Makkah, Saudi Arabia",IEEE Access,"22 Oct 2024","2024","12","","151598","151648","Fog computing, an extension of cloud computing, enhances capabilities by processing data closer to the source, thereby addressing latency and bandwidth issues inherent in traditional cloud models. However, the integration of Artificial Intelligence (AI) into fog computing introduces challenges, particularly in resource management, security, and privacy. This paper systematically reviews AI applications within fog computing environments, adhering to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology to ensure rigorous analysis. The studies were selected based on predefined inclusion criteria, including research published between 2010 and 2024 in peer-reviewed journals and conference papers, with searches conducted in databases like IEEE Xplore, ACM Digital Library, SpringerLink, and Scopus. The review identifies critical issues such as resource constraints, transparency in AI-driven security systems, and the need for adaptable AI models to address evolving security threats. In response, innovative solutions such as lightweight AI models (e.g., Pruned Neural Networks, Quantized Models, Knowledge Distillation), Explainable AI (XAI) (e.g., Model-Agnostic Methods, Feature Importance Analysis, Rule-Based Approaches), and federated learning are proposed. Additionally, a novel taxonomy is introduced, categorizing AI techniques into resource management, security enhancement, and privacy-preserving methods, offering a structured framework for researchers and practitioners. The paper concludes that effective AI integration in fog computing is essential for developing secure, efficient, and adaptable distributed systems, with significant implications for both academia and industry.","2169-3536","","10.1109/ACCESS.2024.3463791","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10684173","Fog computing;artificial intelligence;resource management;explainable AI;privacy and security","Artificial intelligence;Security;Edge computing;Resource management;Data models;Computational modeling;Authentication;Explainable AI;Privacy","","","","153","CCBYNCND","19 Sep 2024","","","IEEE","IEEE Journals"
"An Evaluation of Unobtrusive Sensing in a Healthcare Case Study","J. E. Rivadeneira; J. M. Fernandes; A. Rodrigues; F. Boavida; J. S. Silva","Centre for Informatics and Systems of the University of Coimbra (CISUC), Coimbra, Portugal; Centre for Informatics and Systems of the University of Coimbra (CISUC), Coimbra, Portugal; Polytechnic University of Coimbra, Coimbra Business School | ISCAC, Bencanta, Coimbra, Portugal; Centre for Informatics and Systems of the University of Coimbra (CISUC), Coimbra, Portugal; Institute for Systems Engineering and Computers at Coimbra (INESC Coimbra), University of Coimbra, Coimbra, Portugal",IEEE Access,"2 Jul 2024","2024","12","","89405","89417","This paper examines the integration of Human-in-the-Loop Cyber-Physical Systems (HiTL-CPS) and Unobtrusive Sensing through a case study named iFriend. Our approach enhances the data acquisition phase of HiTLCPS by integrating unobtrusive sensing techniques to monitor real-time heart rate and breathing rate. This is achieved by leveraging Channel State Information (CSI) of Wi-Fi signals, specifically focusing on its amplitude information. This integration facilitates seamless interaction between humans and the cyber-physical environment. We detail the architecture of the iFriend system, comprising sensors, actuators, and computational units forming a closed-loop control mechanism. The unobtrusive sensing module is specifically designed to capture physiological changes without causing discomfort or interfering with daily activities, making it well-suited for healthcare applications and human-machine interaction. We assess iFriend in an experimental setting, demonstrating its feasibility, with between 80% and 90% of estimates hovering around 2.5 breaths per minute for BR or 10 beats per minute for HR, respectively.","2169-3536","","10.1109/ACCESS.2024.3419555","Fundação para a Ciência e a Tecnologia (FCT), I.P.(grant numbers:UIDB/00326/2020,UIDP/00326/2020,UIDB/00308/2020,DOI 10.54499/UIDB/00308/2020); European Social Fund through the Regional Operational Program Centro 2020; FCT through the Ph.D.(grant numbers:SFRH/BD/147371/2019); Secretaría de Educación Superior, Ciencia, Tecnología e Innovación del Ecuador (SENESCYT) under Ph.D.(grant numbers:CZ02-000903-2018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10571912","CSI;human-in-the-loop;IoT;unobtrusive sensing","Sensors;Biomedical monitoring;Monitoring;Machine learning;Wearable sensors;Medical services;Human in the loop","","","","38","CCBY","26 Jun 2024","","","IEEE","IEEE Journals"
"Multi-Layered Continuous Reasoning for Cloud-IoT Application Management","J. L. Herrera; J. Berrocal; S. Forti; A. Brogi; J. M. Murillo","Dipartimento di Informatica-Scienza e Ingegneria, University of Bologna, Bologna, Italy; Department of Computer Science and Communications Engineering, University of Extremadura, Badajoz, Spain; Department of Computer Science, University of Pisa, Pisa, Italy; Department of Computer Science, University of Pisa, Pisa, Italy; Department of Computer Science and Communications Engineering, University of Extremadura, Badajoz, Spain",IEEE Transactions on Services Computing,"31 Dec 2024","2024","17","6","3985","3998","The advent of the Internet of Things has increased the interest in automating mission-critical processes from domains such as smart cities. These applications’ stringent Quality of Service (QoS) requirements motivate their deployment through the Cloud-IoT Continuum, which requires solving the NP-hard problem of placing the application's services onto the infrastructure's devices. Moreover, as the infrastructure and application change over time, the placement needs to continuously adapt to these changes to maintain an acceptable QoS. While continuous reasoning techniques have enabled the creation of tools for these scenarios, they can have some trouble finding a feasible adaptation for abrupt and sharp changes, requiring non-adaptive techniques in those cases. Furthermore, for scenarios with smoother changes, it would be desirable to have faster algorithms to perform this placement. To explore the trade-off of effectiveness and execution times of different methods while ensuring that an application placement is found, we propose Multi-Layered Continuous Reasoning (MLCR) as an autonomic framework to adapt application placements through multiple continuous reasoning-based methods. We also present an MLCR prototype based on three methods: Faustum, MigDADO, and ConDADO. An evaluation in a realistic use case shows that MLCR is faster than traditional methods for application placement and maintains an acceptable QoS.","1939-1374","","10.1109/TSC.2024.3451239","CAS21/00057 (MCI/AEI/FEDER, UE)(grant numbers:PID2021-124054OB-C31,DIN2020-011586,PDC2022-133465-I00); MCIN/AEI/10.13039/50100011033(grant numbers:TED2021-130913B-I00); European Union NextGenerationEU/PRTR; Department of Economy, Science and Digital Agenda of the Government of Extremadura(grant numbers:GR21133); European Regional Development Fund; European Union; Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU; Telecommunications of the Future; PE00000001 - program “RESTART”; CUP(grant numbers:J33C22 002880001); Energy-aware management of software applications in Cloud-IoT ecosystems(grant numbers:RIC2021_ PON_A18); ESF REACT-EU resources; Italian Ministry of University and Research; PON Ricerca e Innovazione 2014–20; hOlistic Sustainable Management of distributed softWARE systems (OSMWARE)(grant numbers:UNIPI PRA_2022_64); University of Pisa, Italy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10654521","Internet of Things (IoT);fog computing;quality of service (QoS);continuous reasoning;service placement","Microservice architectures;Quality of service;Cognition;Cloud computing;Internet of Things;Costs;Smart cities","","","","45","IEEE","28 Aug 2024","","","IEEE","IEEE Journals"
"Dynamic Swarm Orchestration and Semantics in IoT Edge Devices: A Systematic Literature Review","B. Anuraj; D. Calvaresi; J. -M. Aerts; J. -P. Calbimonte","Institute of Informatics, University of Applied Sciences and Arts Western Switzerland (HES-SO), Sierre, Switzerland; Institute of Informatics, University of Applied Sciences and Arts Western Switzerland (HES-SO), Sierre, Switzerland; Department of Biosystems, Measure, Model & Manage Bioresponses Research Group, KU Leuven, Leuven, Belgium; Institute of Informatics, University of Applied Sciences and Arts Western Switzerland (HES-SO), Sierre, Switzerland",IEEE Access,"29 Aug 2024","2024","12","","116917","116938","In its dynamic evolution, the Internet of Things (IoT) has become increasingly pervasive in our daily lives, ranging from domestic appliances to industrial robots. This integration brings together people, processes, data, and devices, prompting new types of interactions among them. Besides acquiring data, these devices also have actuation and processing capabilities, making them susceptible of becoming autonomous entities with coordination potential. Given the inherent limitations of storage, power, or computation of IoT devices, delegation and cooperation strategies, including intermediary nodes in the network, can significantly optimize the usage of resources. Hence, this type of node can rely on swarm-inspired intelligence to orchestrate edge nodes, possibly with semantics-enabled behaviors. This study proposes a Systematic Literature Review (SLR) investigating different solutions and approaches for the orchestration of edge devices powered by declarative and semantic models of their affordances, goals, and capabilities. The SLR explores different aspects of the literature, including demographics, application domains, goals, requirements, scope, services, frameworks, and technologies, as well as challenges and future directions in the field. The purpose of this SLR is to provide software engineers, researchers, and innovators comprehensive insights into the present status of advancements in this area and a discussion of the unresolved issues and opportunities.","2169-3536","","10.1109/ACCESS.2024.3446876","Horizon Europe Project SmartEdge(grant numbers:101092908); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10643067","Internet of Things;swarm-based orchestration;dynamic orchestration;semantic sensor networks;edge devices;Web of Things","Semantics;Internet of Things;Task analysis;Protocols;Planning;Particle swarm optimization;Ontologies","","","","86","CCBY","21 Aug 2024","","","IEEE","IEEE Journals"
"Efficient task offloading and resource allocation in an intelligent UAV-MEC system","B. Picano; R. Fantacci","University of Florence, 50139 Firenze, Italy; University of Florence, 50139 Firenze, Italy",Journal of Communications and Networks,"8 Jan 2025","2024","26","6","666","678","Nowadays, the functional integration of digital twin (DT) technology and artificial intelligence (AI) methodologies has enabled reliable predictions of many random processes, supporting efficient control and optimization procedures. In line with this trend, this paper explores the joint use of these technologies in an AI-empowered DT framework for an unmanned aerial vehicle-aided multi-access edge computing (UAV-MEC) system. Specifically, this approach defines an intelligent UAV-MEC system capable of significantly improving service quality and deployment flexibility. The focus is on a UAV-MEC network consisting of multiple elementary service areas, where DTs efficiently orchestrate and reduce congestion levels by utilizing UAVs with onboard processing capabilities. A potential architecture for the DTs is outlined, conceptualizing each DT as a collection of basic cyber entities. Additionally, a suitable framework utilizing a matching game approach is proposed to effectively manage task offloading, channel allocation, and the dynamic assignment of UAV support to congested service zones within the same area. Finally, comprehensive simulation results validate the efficacy of the proposed intelligent UAV-MEC system, as indicated by metrics such as task completion delay and accuracy in congestion prediction.","1976-5541","","10.23919/JCN.2024.000050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10759571","Digital twin;intelligence system;machine learning;mobile edge computing;unmanned aerial vehicle","Autonomous aerial vehicles;Resource management;Artificial intelligence;Optimization;Drones;Delays;Decision making;Costs;6G mobile communication;Trajectory","","","","","","20 Nov 2024","","","KICS","KICS Journals"
"Delegating Data Plane With Cloud-Assisted Routing","P. K. Dey; M. Yuksel","Department of Electrical and Computer Engineering, University of Central Florida, Orlando, FL, USA; Department of Electrical and Computer Engineering, University of Central Florida, Orlando, FL, USA",IEEE Transactions on Network and Service Management,"9 Oct 2023","2023","20","3","3190","3204","Data centers are embracing the software-defined networking (SDN) as it is evident that this technology of completely separating the data plane from the control plane gives more flexibility for their internal routing management and provides better Quality-of-Service (QoS) to the users. Implementing a similar solution to serve the purpose of software-defined wide-area networking using public Internet routing is also gaining popularity. Although, instead of having a complete separation, a hybrid approach to keep most of the control plane along with the least of the data plane in the remote and vice-versa for the local platform may be more fitting. To this end, we propose a new hybrid SDN approach, Cloud-Assisted Routing (CAR), that utilizes the high computational services that cloud offers at a lower price by splitting both control and data plane functions between a local router and a remote cloud computing platform. Such delegation of data plane to a third-party authority requires proper control plane management policies and handling or avoiding possible loops and failures. We detail the architectural view of CAR, address its associated challenges, and present prototype-based evaluations of it for reducing routing table sizes.","1932-4537","","10.1109/TNSM.2023.3239802","U.S. National Science Foundation(grant numbers:1814086,1647189); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10025801","Routing scalability;software-defined net-working;cloud-assisted networking","Cloud computing;Routing;Automobiles;Computer architecture;Internet;Scalability;Switches","","","","67","IEEE","25 Jan 2023","","","IEEE","IEEE Journals"
"Integrated Cognitive Symbiotic Computing and Ambient Backscatter Communication Network","C. Ren; Y. Hu; L. Sun; H. Li; C. Sun; H. Zhang; A. Nallanathan; V. C. M. Leung","Beijing Engineering and Technology Research Center for Convergence Networks and Ubiquitous Services, University of Science and Technology Beijing, Beijing, China; Beijing Engineering and Technology Research Center for Convergence Networks and Ubiquitous Services, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Research and Development Center Sony (China) Ltd., Beijing, China; Research and Development Center Sony (China) Ltd., Beijing, China; Beijing Engineering and Technology Research Center for Convergence Networks and Ubiquitous Services, University of Science and Technology Beijing, Beijing, China; School of Electronic Engineering and Computer Science, Queen Mary University of London, London, U.K.; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada",IEEE Transactions on Cognitive Communications and Networking,"7 Oct 2024","2024","10","5","1635","1649","Ambient backscatter communication (AmBC) possesses signal reception and energy-harvesting capabilities, allowing providing wireless cognition through simple energy detection. In typical applications like industrial Internet of Things (IoT), cognitive AmBC (CAmBC) networks are required to offer passive communication, edge computing, and cognition capabilities. However, passive communication relies on the environment and has limited computing power, creating interdependencies among spectrum sensing, networking, and computational cognition. Moreover, the heterogeneous evaluation metrics for communication and computation make unified planning and management challenging. Therefore, this paper proposes the integrated cognitive symbiotic computing-AmBC (CSC-AmBC) based on symbiotic communication and cognitive radio. CSC-AmBC integrates AmBC communication and computational cognition capabilities in a task-oriented manner, sharing proximity and AmBC computing and communication (ACC) resources among primary and secondary tasks. Meta-Link with Tokens and two cognitive ACC reuse models is used to facilitate integration and enhance task execution efficiency, which introduces Places to accommodate the heterogeneous and variable ACC resources. Additionally, the task execution gain metric is introduced to evaluate the multi-task ACC resource utilization. Numerical results validate the cognition networking and the advantage of the proposed task execution gain of CSC-AmBC.","2332-7731","","10.1109/TCCN.2024.3439628","National Natural Science Foundation of China(grant numbers:62201034,62225103,U22B2003,62341103); Beijing Municipal Natural Science Foundation(grant numbers:L212004-03); Beijing Natural Science Foundation(grant numbers:L212004); Xiaomi Fund of Young Scholar, and National Key Laboratory of Wireless Communications Foundation(grant numbers:IFN20230201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10623808","Integrated computing and communication;ambient backscatter communication;cognitive networks","Task analysis;Cognition;Backscatter;Wireless sensor networks;Resource management;Measurement;Symbiosis","","","","34","IEEE","6 Aug 2024","","","IEEE","IEEE Journals"
"Efficient Container Scheduling With Hybrid Deep Learning Model for Improved Service Reliability in Cloud Computing","J. Jeon; S. Park; B. Jeong; Y. -S. Jeong","Department of AI⋅SW Convergence, Dongguk University, Seoul, Republic of Korea; Department of Multimedia Engineering, Dongguk University, Seoul, Republic of Korea; Department of Computer Science and Artificial Intelligence, Dongguk University, Seoul, Republic of Korea; Department of AI⋅SW Convergence, Dongguk University, Seoul, Republic of Korea",IEEE Access,"10 May 2024","2024","12","","65166","65177","In a cloud computing environment, the container scheduling technique ensures reliability for containerized applications by selecting nodes that satisfy various resource requirements and then deploying containers. If the initial resources of a container are over-allocated, resources may be wasted, or other containers that are waiting in a scheduling queue may not be allocated. However, if resources are under-allocated, service disruptions may occur due to node overbooking, and service reliability cannot be ensured. Therefore, in this study, a forecasted resource-evaluating scheduler (FoRES) is proposed as a container scheduling technique that ensures resource efficiency and service reliability. FoRES predicts future CPU and memory usage by using a time-series decomposition-based hybrid forecasting (DeHyFo) model that combines multiple linear regressions with the LightTS model. FoRES then calculates the optimal scheduling decisions that minimize idle resources and node overload by applying an efficient resource utilization (SERU) scoring function to the predicted resource usage. Evaluating the performance of FoRES based on various scenarios improved resource efficiency and service reliability by up to 2.07 and 2.32 times, respectively, compared with existing scheduling techniques, even if the initial resources of the container were inefficiently allocated.","2169-3536","","10.1109/ACCESS.2024.3396652","Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT)(grant numbers:2022-0-00047); Development of microservices development/operation platform technology that supports application service operation intelligence; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10518053","Cloud computing;container scheduling;deep learning;resource efficiency;service reliability;time-series forecasting","Containers;Optimal scheduling;Scheduling;Predictive models;Processor scheduling;Resource management;Measurement;Cloud computing;Deep learning;Time series analysis;Reliability","","","","32","CCBYNCND","3 May 2024","","","IEEE","IEEE Journals"
"Adaptive Security Framework for the Internet of Things: Improving Threat Detection and Energy Optimization in Distributed Environments","W. Villegas-Ch; R. Gutierrez; I. Sánchez-Salazar; A. Mera-Navarrete","Escuela de Ingeniería en Ciberseguridad, FICA, Universidad de Las Américas, Quito, Ecuador; Escuela de Ingeniería en Ciberseguridad, FICA, Universidad de Las Américas, Quito, Ecuador; Escuela de Ingeniería en Ciberseguridad, FICA, Universidad de Las Américas, Quito, Ecuador; Departamento de Sistemas, Universidad Internacional del Ecuador, Quito, Ecuador",IEEE Access,"1 Nov 2024","2024","12","","157924","157944","The increasing use of Internet of Things (IoT) devices in critical sectors has increased exposure to security threats, making protecting these systems a priority challenge. Based on static configurations, traditional security approaches have proven ineffective in the face of the dynamic nature of emerging threats, as they cannot adapt in real time to changes in the environment or new attack vectors. This work proposes an adaptive security framework for Internet of Things (IoT) systems capable of autonomously detecting, mitigating, and adapting to various threats, improving precision and response times, and optimizing energy consumption. The framework was implemented in a distributed Internet of Things environment, using adaptive architectures based on the Robot Operating System (ROS) and microservices orchestration with Kubernetes. The results showed a significant improvement in response time, with a reduction of 44%, reaching an average of 250 milliseconds, compared to 450 milliseconds for static approaches. Furthermore, a 92% precision in threat detection was achieved, reducing false positives to 4% and false negatives to 6%. Power consumption was controlled, reaching a maximum of 160 milliamp-hours after facing multiple threats, confirming the system’s efficiency in resource-limited environments. These results demonstrate that the proposed adaptive framework is a robust and efficient solution for security in Internet of Things environments, overcoming the limitations of traditional approaches and ensuring adequate protection without compromising energy efficiency.","2169-3536","","10.1109/ACCESS.2024.3486983","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10736580","Adaptive security framework;Internet of Things;threat detection;energy efficiency in Internet of Things","Security;Internet of Things;Adaptive systems;Real-time systems;Time factors;Threat assessment;Energy consumption;Energy efficiency;Sensors;Logic gates","","","","44","CCBYNCND","28 Oct 2024","","","IEEE","IEEE Journals"
"Advancing 6G: Survey for Explainable AI on Communications and Network Slicing","H. Sun; Y. Liu; A. Al-Tahmeesschi; A. Nag; M. Soleimanpour; B. Canberk; H. Arslan; H. Ahmadi","Institute for Safe Autonomy and School of Physics, Engineering and Technology, York, U.K.; Institute for Safe Autonomy and School of Physics, Engineering and Technology, York, U.K.; Institute for Safe Autonomy and School of Physics, Engineering and Technology, York, U.K.; School of computer Science, University College Dublin, Dublin 4, Ireland; Texas Biomedical Research Institute, San Antonio, TX, USA; School of Computing, Engineering and The Built Environment, Edinburgh Napier University, Edinburgh, U.K.; School of Engineering and Natural Sciences, Istanbul Medipol University, Istanbul, Türkiye; Institute for Safe Autonomy and School of Physics, Engineering and Technology, York, U.K.",IEEE Open Journal of the Communications Society,"26 Feb 2025","2025","6","","1372","1412","The unprecedented advancement of Artificial Intelligence (AI) has positioned Explainable AI (XAI) as a critical enabler in addressing the complexities of next-generation wireless communications. With the evolution of the 6G networks, characterized by ultra-low latency, massive data rates, and intricate network structures, the need for transparency, interpretability, and fairness in AI-driven decision-making has become more urgent than ever. This survey provides a comprehensive review of the current state and future potential of XAI in communications, with a focus on network slicing, a fundamental technology for resource management in 6G. By systematically categorizing XAI methodologies–ranging from modelagnostic to model-specific approaches, and from pre-model to post-model strategies–this paper identifies their unique advantages, limitations, and applications in wireless communications. Moreover, the survey emphasizes the role of XAI in network slicing for vehicular network, highlighting its ability to enhance transparency and reliability in scenarios requiring real-time decision-making and high-stakes operational environments. Real-world use cases are examined to illustrate how XAI-driven systems can improve resource allocation, facilitate fault diagnosis, and meet regulatory requirements for ethical AI deployment. By addressing the inherent challenges of applying XAI in complex, dynamic networks, this survey offers critical insights into the convergence of XAI and 6G technologies. Future research directions, including scalability, real-time applicability, and interdisciplinary integration, are discussed, establishing a foundation for advancing transparent and trustworthy AI in 6G communications systems.","2644-125X","","10.1109/OJCOMS.2025.3534626","Engineering and Physical Sciences Research Council United Kingdom (EPSRC), Impact Acceleration Accounts (IAA) (Green Secure and Privacy Aware Wireless Networks for Sustainable Future Connected and Autonomous Systems)(grant numbers:EP/X525856/1); EPSRC CHEDDAR: Communications Hub for Empowering Distributed Cloud Computing Applications and Research(grant numbers:EP/Y037421/1,EP/X040518/1); Department of Science, Innovation and Technology, U.K.(grant numbers:Yorkshire Open-RAN (YORAN) TS/X013758/1,RIC Enabled (CF-)mMIMO for HDD (REACH) TS/Y008952/1); Scientific and Technological Research Council of Turkey (TUBITAK) 1515 Frontier RD Laboratories Support Program for BTS Advanced AI Hub: BTS Autonomous Networks and Data Innovation Lab(grant numbers:5239903); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10854503","Sixth Generation (6G);wireless communications;Explainable AI (XAI);Artificial Intelligent (AI);Machine Learning (ML);network slicing;vehicular networks","6G mobile communication;Artificial intelligence;Explainable AI;Surveys;Network slicing;Wireless communication;Decision making;Reviews;Security;Real-time systems","","","","237","CCBY","27 Jan 2025","","","IEEE","IEEE Journals"
"MonitorChain: An Extensible Tool for Real-Time Monitoring of Blockchain-Based Software Applications","A. Araújo; R. Soares; W. Santos","Computing Institute, Federal University of Alagoas, Maceió, Brazil; Computing Institute, Federal University of Alagoas, Maceió, Brazil; Computing Institute, Federal University of Alagoas, Maceió, Brazil",IEEE Access,"18 Oct 2024","2024","12","","147784","147797","Support tools that monitor software application processes are required to identify, measure, and assess their performance and resolve any anomalies that impede their full functionality. Similarly, applications that use blockchain technology require analysis and monitoring tools for the resources and functionality provided by their smart contracts. This paper presents a computational tool that monitors hardware resources and evaluates transaction processing requests in blockchain networks. The solution presented here is called MonitorChain, and it has software components that allow the connection with different blockchain networks, the configuration of the workload, the monitoring of consumed hardware resources, and the evaluation of requests processed over blockchain networks. The effectiveness and adaptability of MonitorChain were validated through extensive evaluations across various blockchain environments, including public networks like Ethereum, Avalanche, and Fantom, as well as a private network using Hyperledger Fabric, with a consistent data load of 10,000 transactions. Additionally, through interviews with experienced software developers, we gathered feedback that led to further refinements and identified opportunities for future enhancements. The results demonstrated MonitorChain’s capability to provide critical insights into blockchain applications’ performance, scalability, and efficiency. MonitorChain advances the field compared to existing solutions by offering enhanced support for diverse networks, customizable workloads, and real-time data visualization, contributing significantly to the blockchain monitoring landscape.","2169-3536","","10.1109/ACCESS.2024.3475876","Alagoas Research Foundation (FAPEAL)(grant numbers:E:60030.0000002551/2022); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10706853","Blockchain technology;software development tools;real-time monitoring tools;smart contracts","Blockchains;Monitoring;Software;Measurement;Scalability;Real-time systems;Distributed ledger;Smart contracts;Security;Software development management;Real-time systems","","","","35","CCBYNCND","7 Oct 2024","","","IEEE","IEEE Journals"
"An Automated Edge Computing Approach for IoT Device Registration and Application Deployment","V. Mafeni; Y. Kim","School of Electronic Engineering, Soongsil University, Seoul, South Korea; School of Electronic Engineering, Soongsil University, Seoul, South Korea",IEEE Systems Journal,"20 Jun 2024","2024","18","2","1447","1458","As the Internet of Things (IoT) evolves rapidly across various industries, the number of IoT protocols and applications is growing with vast number of heterogeneous components and entities. In setups with thousands of IoT devices, manual deployment of applications and device registration become impractical due to their time-consuming and costly nature, as well as the requirement for background knowledge of IoT devices and protocols. Furthermore, IoT devices often have resource constraints that prevent them from running complex software. Therefore, there is a significant need to enhance and optimize edge computing systems for IoT, making them suitable and dynamic for automated IoT device registration and heterogeneous application deployment. In this article, we present an edge-based framework designed to facilitate the automated registration of diverse wireless IoT devices and the deployment of IoT applications. To validate our approach, we use a smart irrigation system enhanced with a containerized machine learning model as a proof of concept. Our evaluation of the implemented prototype demonstrates that our system is scalable and feasible.","1937-9234","","10.1109/JSYST.2024.3392290","Institute of Information and communications Technology Planning and Evaluation(grant numbers:2020-0-00946); Development of Fast and Automatic Service recovery and Transition software in Hybrid Cloud Environment; MSIT, Korea(grant numbers:IITP-2024-RS-2023-00258649); IITP; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10518055","Automation;edge computing;Internet of Things (IoT);service orchestration;wireless sensors","Internet of Things;Edge computing;Cloud computing;Scalability;Wireless communication;Manuals;Wireless sensor networks","","","","51","CCBYNCND","3 May 2024","","","IEEE","IEEE Journals"
"Empowering Beyond 5G Networks: An Experimental Assessment of Zero-Touch Management and Orchestration","S. Barrachina-Muñoz; F. Rezazadeh; L. Blanco; S. Kukliński; E. Zeydan; A. Chawla; L. Zanzi; F. Devoti; V. Vlahodimitropoulou; I. Chochliouros; A. -M. Bosneag; S. Cherrared; L. Vettori; J. Mangues-Bafalluy","Centre Tecnològic de Telecomunicacions de Catalunya (CTTC), Barcelona, Spain; Centre Tecnològic de Telecomunicacions de Catalunya (CTTC), Barcelona, Spain; Centre Tecnològic de Telecomunicacions de Catalunya (CTTC), Barcelona, Spain; Orange Polska, Warsaw, Poland; Centre Tecnològic de Telecomunicacions de Catalunya (CTTC), Barcelona, Spain; Ericsson Ireland, Athlone, Ireland; NEC Laboratories Europe, Heidelberg, Germany; NEC Laboratories Europe, Heidelberg, Germany; Hellenic Telecommunications Organization S.A. (OTE), Athens, Greece; Hellenic Telecommunications Organization S.A. (OTE), Athens, Greece; Ericsson Ireland, Athlone, Ireland; Orange France, Châtillon, France; Centre Tecnològic de Telecomunicacions de Catalunya (CTTC), Barcelona, Spain; Centre Tecnològic de Telecomunicacions de Catalunya (CTTC), Barcelona, Spain",IEEE Access,"11 Dec 2024","2024","12","","182752","182762","Effective zero-touch management and orchestration (ZSM&O) is crucial for scaling network slicing, particularly transitioning toward Beyond 5G (B5G) and 6G networks. This paper empirically validates the network slicing framework developed under the European Union Horizon 2020 MonB5G project. Building on three years of academia-industry collaboration, MonB5G introduces a flexible slicing model featuring umbrella slices that orchestrate modular, specialized slices across multi-domain environments to address next-generation service demands. For the first time, we evaluate its practicality in a 5G cloud-native testbed through a virtual reality (VR) streaming use case, supported by solutions such as federated learning-based CPU forecasting, anomaly detection, and deep reinforcement learning (DRL) for radio access network (RAN) optimization. The paper offers insights from technically demanding experimental tests and highlights challenges and development paths for managing next-generation mobile networks.","2169-3536","","10.1109/ACCESS.2024.3510804","H2020 Distributed management of Network Slices in beyond 5G (MonB5G) Project(grant numbers:871780); Spanish Ministry of Economy and Competitiveness (MINECO)–Program Programa de Universalización de Infraestructuras Digitales para la Cohesión I+D (UNICO I+D)(grant numbers:TSI-063000-2021-54,TSI-063000-2021-55); ‘‘ERDF—A way of making Europe’’ project; Ministerio de Ciencia e Innovación/Agencia Estatal de Investigación (MCIN/AEI)/10.13039/501100011033(grant numbers:PID2021-126431OB-I00,MINECO TSI-064100-2022-16,TSI-064100-2023-26); Generalitat de Catalunya(grant numbers:2021 SGR 00770); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10776956","Network slicing;scalability;network management;orchestration;B5G;testbed","5G mobile communication;6G mobile communication;Scalability;Monitoring;Resource management;Network slicing;Federated learning;Cloud computing;Automation;Anomaly detection","","","","35","CCBY","4 Dec 2024","","","IEEE","IEEE Journals"
"Software Architecture for Agricultural Robots: Systems, Requirements, Challenges, Case Studies, and Future Perspectives","R. Raja","Donders Centre for Cognition, Department of Artificial Intelligence, Radboud University, Nijmegen, The Netherlands",IEEE Transactions on AgriFood Electronics,"10 Apr 2024","2024","2","1","125","137","Designing software architectures for autonomous robots for agricultural contexts is a demanding and difficult job due to the requirement to monitor numerous sensors and actuators, as well as autonomous decision-making in unpredictable, unexpected scenarios. Depending on the essential requirements of a robotic device for agricultural usage, robot software architecture is created differently. Since no single software architecture exists for all applications, extensive knowledge of the various software architectures for robots is needed when creating your own robotic architecture or selecting one from a number of existing architectures. As a result, this article provides a comprehensive history of software architecture and its application in the agricultural domain along with a chronology of how software design has evolved over time. We provide several case studies to understand the importance of application of software architecture in agriculture and food industry and how to choose the best architecture for agricultural tasks. Finally, this article discusses the open obstacles and difficulties that must be addressed in order to ensure more advancements in the development of robot architecture for agricultural applications.","2771-9529","","10.1109/TAFE.2024.3366335","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10459067","Agricultural robot;food processing;food packaging;harvesting robot;robot programming;software architecture","Robot sensing systems;Agricultural robots;Software architecture;Food packaging;Robot programming","","","","66","IEEE","5 Mar 2024","","","IEEE","IEEE Journals"
"Online Layer-Aware Joint Request Scheduling, Container Placement, and Resource Provision in Edge Computing","Z. Li; J. Lou; Z. Tang; J. Guo; T. Wang; W. Jia; W. Zhao","School of Artificial Intelligence, Beijing Normal University, Beijing, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Institute of Artificial Intelligence and Future Networks, Beijing Normal University, Zhuhai, China; Institute of Artificial Intelligence and Future Networks, Beijing Normal University, Zhuhai, China; Institute of Artificial Intelligence and Future Networks, Beijing Normal University, Zhuhai, China; Institute of Artificial Intelligence and Future Networks, Beijing Normal University, Zhuhai, China; Shenzhen University of Advanced Technology, Shenzhen, China",IEEE Transactions on Services Computing,"7 Feb 2025","2025","18","1","328","341","Containers have emerged as a pivotal tool for service deployment in edge computing. Before running the container, an image composed of several layers must exist locally. Recent strategies have utilized layer-sharing in images to reduce deployment delays. However, existing research only focuses on a single aspect of container orchestration, like container placement, neglecting the joint optimization of the entire orchestration process. To fill in such gaps, this article introduces an online strategy that considers layer-aware container orchestration, encompassing request scheduling, container placement, and resource provision. The goal is to reduce costs, adapt to evolving user demands, and adhere to system constraints. We present an online optimization problem that accounts for various real-world factors in orchestration, including container and server expenses. An online algorithm is proposed, integrating a regularization-based approach and stepwise rounding to address this optimization problem efficiently. The regularization approach separates time-dependent container placement and server wake-up costs, requiring only current information and past decisions. The stepwise rounding process generates feasible solutions that meet system constraints, reducing computational costs. Additionally, a competitive ratio proof is provided for the proposed algorithm. Extensive evaluations demonstrate that our approach achieves about 20% performance enhancement compared to baseline algorithms.","1939-1374","","10.1109/TSC.2024.3504237","National Natural Science Foundation of China(grant numbers:62272050,62302048); Guangdong Key Lab of AI and Multi-modal Data Processing, United International College(grant numbers:Zhuhai); Department of Education of Guangdong Province; Institute of Artificial Intelligence and Future Networks; Engineering Center of AI and Future Education; Guangdong Provincial Department of Science and Technology, China; Zhuhai Science-Tech Innovation Bureau(grant numbers:2320004002772); Interdisciplinary Intelligence Super Computer Center of Beijing Normal University at Zhuhai; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10759825","Container placement;request scheduling;resource provision;edge computing","Containers;Costs;Servers;Processor scheduling;Image edge detection;Edge computing;Scheduling;Optimization;Delays;Artificial intelligence","","","","53","IEEE","21 Nov 2024","","","IEEE","IEEE Journals"
"Dynamic Service Placement in Edge Computing: A Comparative Evaluation of Nature-Inspired Algorithms","A. H. Kazmi; A. Staffolani; T. Zhang; C. Cabrera; S. Clarke","School of Computer Science and Statistics, Trinity College Dublin, Dublin 2, Ireland; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; School of Computer Science and Statistics, Trinity College Dublin, Dublin 2, Ireland; Department of Computer Science and Technology, University of Cambridge, Cambridge, U.K.; School of Computer Science and Statistics, Trinity College Dublin, Dublin 2, Ireland",IEEE Access,"6 Jan 2025","2025","13","","2653","2670","Edge computing has emerged as a promising solution for delivering services that demand low latency, high bandwidth, and stringent privacy requirements in numerous data- and compute-intensive applications, such as those in Smart Cities. Heterogeneity in edge computing resources and diverse application requirements demand adaptive optimization techniques, such as service placement, to conform to changing conditions. A service placement model must optimize the selection of edge nodes for deploying and executing services, thereby improving application QoS and maximizing resource utilization. Numerous optimization techniques for adaptive service placement problem have been proposed in the recent past. However, in most cases, the results have been evaluated in limited scenarios. This paper presents a comprehensive comparative study evaluating representative optimization algorithms applied to the problem of dynamic service placement across various application scenarios. The study covers nature-inspired approaches, including both meta-heuristics and reinforcement learning. Our experimental findings offer valuable insights into the strengths and weaknesses of the selected nature-inspired algorithms for service placement optimization, evaluated for applications with different QoS requirements. In our analysis, the Genetic Algorithm shows superior performance in achieving lower average distance and the average number of servers selected. Particle Swarm Optimization excels in minimizing average waiting time and placement decision time. The Artificial Bee Colony maintains low average latency, whereas the RL Proximal Policy Optimization demonstrates superior performance in terms of balancing the utilization of network resources.","2169-3536","","10.1109/ACCESS.2024.3520701","Taighde Éireann - Research Ireland(grant numbers:13/RC/2077_P2); Research Ireland Centre for Future Networks; VMware by Broadcom; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10810420","Edge computing;dynamic service placement;multi-objective optimization;meta-heuristics;nature-inspired algorithms;service offloading;computational offloading;service scheduling","Servers;Cloud computing;Edge computing;Privacy;Quality of service;Microservice architectures;Heuristic algorithms;Metaheuristics;Low latency communication;Computational modeling","","","","76","CCBY","20 Dec 2024","","","IEEE","IEEE Journals"
"Mobility-Aware Dependent Task Offloading in Edge Computing: A Digital Twin-Assisted Reinforcement Learning Approach","X. Chen; J. Cao; Y. Sahni; M. Zhang; Z. Liang; L. Yang","Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong, SAR, China; Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong, SAR, China; Department of Building Environment and Energy Engineering, Hong Kong Polytechnic University, Hung Hom, Hong Kong, SAR, China; Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong, SAR, China; Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong, SAR, China; School of Software Engineering, South China University of Technology, Guangzhou, China",IEEE Transactions on Mobile Computing,"6 Mar 2025","2025","24","4","2979","2994","Collaborative edge computing (CEC) has emerged as a promising paradigm, enabling edge nodes to collaborate and execute tasks from end devices. Task offloading is a fundamental problem in CEC that decides when and where tasks are executed upon the arrival of tasks. However, the mobility of users often results in unstable connections, leading to network failures and resource underutilization. Existing works have not adequately addressed joint mobility-aware dependent task offloading and network flow scheduling, resulting in network congestion and suboptimal performance. To address this, we formulate an online joint mobility-aware dependent task offloading and bandwidth allocation problem, to improve the quality of service by reducing task completion time and energy consumption. We introduce a Mobility-aware Digital Twin-assisted Deep Reinforcement Learning (MDT-DRL) algorithm. Our digital twin model equips the reinforcement learning process by providing future states of mobile users, enabling efficient offloading plans for adapting to the mobile CEC system. Experimental results on real-world and synthetic datasets show that MDT-DRL surpasses state-of-the-art baselines on average task completion time and energy consumption.","1558-0660","","10.1109/TMC.2024.3506221","Research Institute for Artificial Intelligence of Things; Hong Kong Polytechnic University; Hong Kong (HK) Research Grant Council; Theme-based Research Scheme(grant numbers:T43-513/23-N); National Natural Science Foundation of China(grant numbers:CRS_PolyU501/23); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10767295","Collaborative edge computing;deep reinforcement learning;digital twin;mobility;task offloading","Digital twins;Internet of Things;Edge computing;Vehicle dynamics;Dynamic scheduling;Processor scheduling;Real-time systems;Energy consumption;Mobile computing;Job shop scheduling","","","","40","IEEE","25 Nov 2024","","","IEEE","IEEE Journals"
"EMO-TS: An Enhanced Multi-Objective Optimization Algorithm for Energy-Efficient Task Scheduling in Cloud Data Centers","S. Nambi; P. Thanapal","School of Computer Science Engineering and Information Systems, Vellore Institute of Technology, Vellore, India; School of Computer Science Engineering and Information Systems, Vellore Institute of Technology, Vellore, India",IEEE Access,"14 Jan 2025","2025","13","","8187","8200","The rapid expansion of cloud data centers, driven by the increasing demand for diverse user services, has escalated energy consumption and greenhouse gas emissions, posed severe environmental risks, and increased operational costs. Addressing these challenges requires innovative solutions for optimizing resource allocation without compromising service quality. This paper presents the Enhanced Multi-Objective Optimization Algorithm for Task Scheduling (EMO-TS). This novel approach integrates Deep Reinforcement Learning (DRL) and Enhanced Electric Fish Optimization (EEFO) to create an adaptive, dynamic, and energy-efficient scheduling framework. The primary objective of EMO-TS is to significantly reduce the energy consumption of cloud data centers while maintaining high levels of resource utilization, time efficiency, and service quality. Through the hybrid methodology of DRL and EEFO, EMO-TS dynamically adjusts task scheduling based on real-time workloads and operational conditions, effectively minimizing power consumption without sacrificing system performance. Additionally, EMO-TS introduces improvements in makespan and task execution, ensuring timely completion and optimal resource use. A comprehensive set of experiments and simulations demonstrates the practical implications of EMO-TS’s results. EMO-TS outperforms traditional scheduling approaches, reducing energy consumption by up to 25% and decreasing makespan by 15%. These results underscore the algorithm’s potential to address cloud service providers’ economic and environmental concerns, offering a practical solution for green cloud computing initiatives. Furthermore, the integration of renewable energy sources within the EMO-TS framework shows potential for further reducing the carbon footprint of cloud operations, aligning with global sustainability goals.","2169-3536","","10.1109/ACCESS.2025.3527031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10833625","Cloud data centers;deep reinforcement learning;electric fish optimization;energy efficiency;makespan;task scheduling","Cloud computing;Data centers;Processor scheduling;Resource management;Dynamic scheduling;Energy consumption;Heuristic algorithms;Energy efficiency;Real-time systems;Scalability","","","","46","CCBY","8 Jan 2025","","","IEEE","IEEE Journals"
"Adaptive Digital Twin Migration in Vehicular Edge Computing and Networks","F. Mou; J. Lou; Z. Tang; Y. Wu; W. Jia; Y. Zhang; W. Zhao","Hong Kong Baptist University, Kowloon Tong, Hong Kong; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Institute of Artificial Intelligence and Future Networks, Beijing Normal University, Zhuhai, China; State Key Lab of IoT for Smart City, University of Macau, SAR Macau, China; Institute of Artificial Intelligence and Future Networks, Beijing Normal University, Zhuhai, China; Department of Informatics, University of Oslo, Oslo, Norway; Shenzhen University of Advanced Technology, Shenzhen, China",IEEE Transactions on Vehicular Technology,"5 Mar 2025","2025","74","3","4839","4854","The surge in mobile vehicles and data traffic in Vehicular Edge Computing and Networks (VECONs) requires innovative approaches for low latency, stable connectivity, and efficient resource usage in fast-moving vehicles. Existing studies have identified that utilizing digital twins (DTs) can effectively improve service quality in VECONs. However, it still faces substantial challenges posed by large-scale complex DT communications in sustaining real-time collaborative endeavors. In particular, within the dynamic VECONs, the decision regarding DT migration plays a pivotal role in sustaining the quality of services. In this paper, we propose an adaptive DT migration (ADM) algorithm to minimize the overall migration costs when DTs deliver services. Specifically, 1) We formulate ADM as a combinatorial optimization problem in VECONs, comprehensively considering communication latency and migration latency under complex DT communications, vehicular mobilities, and dynamic states of edges; 2) An ADM algorithm based on off-policy actor-critic reinforcement learning is proposed to make migration decisions. Moreover, the ADM agent employs warm-up policies to address exploration challenges in sparse state spaces; 3) Simulations based on real-world, large-scale urban vehicular mobility datasets demonstrate that our method outperforms existing algorithms by approximately 39% on average, and it can achieve results close to the optimal.","1939-9359","","10.1109/TVT.2024.3492349","National Natural Science Foundation of China(grant numbers:62272050,62302048); Science and Technology Development Fund of Macau SAR(grant numbers:0158/2022/A); Guangdong Key Lab of AI and Multi-modal Data Processing, United International College (UIC), Zhuhai(grant numbers:2023-2024); Department of Education of Guangdong Province; Institute of Artificial Intelligence and Future Networks; Engineering Center of AI and Future Education, Guangdong Provincial Department of Science and Technology, China; Zhuhai Science-Tech Innovation Bureau(grant numbers:2320004002772); Interdisciplinary Intelligence Super Computer Center of Beijing Normal University at Zhuhai; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10746337","Digital twin;migration;vehicular edge computing;deep reinforcement learning","Vehicle dynamics;Real-time systems;Heuristic algorithms;Costs;Feature extraction;Edge computing;Approximation algorithms;Adaptive systems;Servers;Training","","","","52","IEEE","7 Nov 2024","","","IEEE","IEEE Journals"
"Fusing Algorithms for Intersection of Computer Science and Art: Innovations in Generative Art and Interactive Digital Installations","J. Xie; M. Yu; G. Liu","School of Arts, Anyang Normal University, Anyang, Henan, China; School of Chinese Language and Literature, Anyang Normal University, Anyang, Henan, China; School of Computer and Information Engineering, Fuyang Normal University, Anhui, Fuyang, China",IEEE Access,"25 Nov 2024","2024","12","","173255","173267","This article investigates the integration of Variational Autoencoders (VAEs) and Particle Swarm Optimization (PSO) in the realm of generative art and interactive digital installations. The study focuses on how these advanced algorithms can enhance artistic expression and interactivity, providing novel approaches for generating and optimizing art. Key innovations include the application of VAEs to create diverse and complex art forms, coupled with PSO to fine-tune these generative processes. The research demonstrates that VAEs significantly improve the aesthetic quality and variety of generated artworks, achieving an average aesthetic score of 8.3 out of 10. Integrating PSO further optimizes these results, enhancing the quality of outputs with a final score of 9.0. The study also reveals that this combination improves user engagement and satisfaction, with interactive installations utilizing VAE + PSO achieving a satisfaction score of 9.0, compared to 7.0 for traditional methods. The findings highlight the transformative impact of these algorithms on art generation, showing that while computational resources and time are higher, the artistic and interactive benefits are substantial. This research underscores the potential of combining deep learning and optimization techniques to push the boundaries of digital creativity and offers new perspectives for artists and designers. The article concludes that the synergy of VAEs and PSO represents a significant advancement in generative art and interactive installations, opening new avenues for future exploration and development in the field.","2169-3536","","10.1109/ACCESS.2024.3488398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10738802","Variational autoencoders (VAEs);particle swarm optimization (PSO);generative art;interactive digital installations;artistic expression;deep learning algorithms;algorithmic creativity","Art;Heuristic algorithms;Sensors;Creativity;Accuracy;Visualization;Software algorithms;Real-time systems;Tuning;Technological innovation;Particle swarm optimization;Deep learning","","","","25","CCBYNCND","30 Oct 2024","","","IEEE","IEEE Journals"
"Architecture Transformation: Integrating Smart Systems for Intelligent Agent-Based Service Management in Smart Organizations","A. Setijadi Prihatmanto; A. Sukoco; A. Budiyono; D. Tresnawati; N. Lestari; H. Teja Sukmana; U. Atiah; S. Wahyuni; W. Ratnawati; K. Abimanyu Munastha","Research Center for Information Communication Technology, Institut Teknologi Bandung, Bandung, Indonesia; Computer Science Faculty, Universitas Bandar Lampung, Bandar Lampung, Indonesia; Indonesia Center for Technology Empowerment, CAP Solutions, Bandung, Indonesia; Institut Teknologi Garut, Garut, Indonesia; Faculty of Engineering, Universitas Sangga Buana, Bandung, Indonesia; Faculty of Science and Technology, UIN Jakarta, Jakarta, Indonesia; Neuro Engineering Study Group, Labuang Baji General Hospital, Makassar, South Sulawesi, Indonesia; Neuro Engineering Study Group, Makassar City General Hospital, Makassar, South Sulawesi, Indonesia; Neuro Engineering Study Group, Tenriawaru General Hospital, Bone, South of Sulawesi, Indonesia; Faculty of Engineering, Universitas Sangga Buana, Bandung, Indonesia",IEEE Access,"17 Oct 2024","2024","12","","146968","146995","In the pursuit of organizational goals, addressing the fundamental needs of employees, such as ensuring the availability of drinking water, plays a pivotal role in fostering a productive, healthy, and value-aligned work environment. This imperative necessitates the implementation of intelligent service management solutions that are both efficient and intelligent. In this research, we propose a visionary architectural transformation that seamlessly integrates intelligent agent-based smart systems within the domain of Smart Organizations. Our novel architectural approach draws inspiration from Activity Theory, strategically orchestrating interactions between employees, company objectives, and tools. Additionally, we employ a meticulously crafted 4-layer organizational system structure to delineate roles, establish rules, foster communities, and optimize task allocation. This forward- looking architecture is further fortified through the integration of cutting-edge technologies, encompassing the Internet of Things (IoT) with precision dispensing scales for real-time predictive capabilities, human-computer interaction (HCI) mobile applications and web services to enhance user-system engagement, and pervasive artificial intelligence (AI) implementations across every layer of the system. The architectural framework is consolidated within a robust Big Data platform, enabling the collection and comprehensive analysis of data on a grand scale. This multifaceted approach seeks to usher in profound and all-encompassing changes in the management of drinking water supply services, encompassing organizational paradigms, technological advancements, and methodological enhancements to enhance efficiency and effectiveness. The focus of this transformation is primarily directed toward companies involved in supply activities, with particular emphasis on drinking water provisioning. Through the development of predictive system prototypes and meticulous performance analysis, our study conclusively demonstrates that the Smart System-based prediction architecture significantly enhances operational performance and elevates the quality of drinking water supply services.","2169-3536","","10.1109/ACCESS.2024.3456845","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10670395","Architectural transformation;smart system;service management;smart organization","Artificial intelligence;Neural engineering;Big Data;Regulation;Real-time systems;Intelligent systems;Organizational aspects;Service-oriented architecture;Internet of Things;Resource management;Human computer interaction;Water resources;Intelligent sensors;Agent-based modeling","","","","94","CCBYNCND","10 Sep 2024","","","IEEE","IEEE Journals"
"DAG-Based Swarm Learning Approach in Healthcare: A Survey","D. Gana; F. Jamil","Department of Computer Science, University of Huddersfield, Huddersfield, U.K.; Department of Computer Science, University of Huddersfield, Huddersfield, U.K.",IEEE Access,"24 Jan 2025","2025","13","","13796","13815","Healthcare systems are advancing at a rapid pace as a result of new technologies to address several issues in the sector such as shortage of skilled health workers and to deal with new diseases like COVID-19. Incorporating technologies like blockchain, federated learning, swarm learning, and Directed Acyclic Graphs is transforming healthcare. This article thoroughly examines recent progress and uses at the intersection of these technologies within the healthcare field. Blockchain’s innovative consensus mechanisms and secure data flow systems offer encouraging solutions to crucial issues in healthcare data management and security. Also, federated learning has been deployed in various ways to tackle healthcare challenges enabling collaborative data analysis while upholding patient confidentiality. Swarm learning algorithms have been notably effective in healthcare, enriching medical diagnostics, disease prognosis, and precision medicine. Solutions based on Directed Acyclic Graphs present scalable and effective alternative to traditional blockchain frameworks, providing improved consensus speed and decreased bottlenecks in transaction processing. These advancements signify a shift in direction towards fully decentralised and secure healthcare systems. This paper highlights the transformative impact of these technologies on medical diagnostics, disease prediction, and precision medicine.","2169-3536","","10.1109/ACCESS.2025.3531216","University of Huddersfield, Huddersfield, U.K; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10844296","Blockchain;directed acyclic graph;federated learning;IoT;privacy;scalability;smart healthcare;swarm learning","Medical services;Blockchains;Diseases;Federated learning;Internet of Things;Security;Accuracy;Surveys;Prediction algorithms;Machine learning algorithms","","","","176","CCBY","17 Jan 2025","","","IEEE","IEEE Journals"
"Modelling IoT Behaviour in Supply Chain Business Processes with BPMN: A Systematic Literature Review","I. Abouzid; Y. K. Bekali; R. Saidi","SI2M Laboratory, National Institute of Statistics and Applied Economics, Rabat, Morocco; LRIT, Faculty of Sciences, Mohammed V University in Rabat, Morocco; SI2M Laboratory, National Institute of Statistics and Applied Economics, Rabat, Morocco",Journal of ICT Standardization,"22 Sep 2023","2022","10","3","439","467","The Internet of Things (IoT) enables to connect physical world to digital processes, allowing real-world data to be fed into business processes. This revolution helps in the making of more informed business decisions as well as the automation and/or improvement of business processes tasks. The successful integration of IoT into business operations is required to realize these benefits. Supporting the modelling of IoT-enhanced business proccesess is the first step toward this goal. Despite the fact that numerous papers studied this topic, it is unclear what the current state of the art is in terms of modelling solutions and gaps. We conduct a systematic literature review in this work to determine how current solutions model IoT into business operations, and whether the standard Business Process Model and Notation (BPMN) has emerged as the de facto standard for business process modelling [20], [26]). The Object Management Group (OMG) developed BPMN, which is now an ISO standard BPMN is already enough for a full modelling of IoT integration, or the extensions are needed. We found and analysed the several existing alternative solutions after reviewing all the literature on this issue. Furthermore, we discuss some key aspects of the planned additions that should be addressed in the near future, such as the absence of standardization.","2246-0853","","10.13052/jicts2245-800X.1035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255414","Systematic literature review;business process modelling;BPMN;IoT;BPMN extensions;process execution;supply chain","Surveys;Analytical models;Systematics;Runtime;Bibliographies;Supply chains;Internet of Things","","","","40","","22 Sep 2023","","","River Publishers","River Publishers Journals"
"A Framework for Dynamically Meeting Performance Objectives on a Service Mesh","F. S. Samani; R. Stadler","Department of Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; Department of Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden",IEEE Transactions on Network and Service Management,"19 Dec 2024","2024","21","6","5992","6007","We present a framework for achieving end-to-end management objectives for multiple services that concurrently execute on a service mesh. We apply reinforcement learning (RL) techniques to train an agent that periodically performs control actions to reallocate resources. We develop and evaluate the framework using a laboratory testbed where we run information and computing services on a service mesh, supported by the Istio and Kubernetes platforms. We investigate different management objectives that include end-to-end delay bounds on service requests, throughput objectives, cost-related objectives, and service differentiation. Our framework supports the design of a control agent for a given management objective. The management objective is defined first and then mapped onto available control actions. Several types of control actions can be executed simultaneously, which allows for efficient resource utilization. Second, the framework separates the learning of the system model and the operating region from the learning of the control policy. By first learning the system model and the operating region from testbed traces, we can instantiate a simulator and train the agent for different management objectives. Third, the use of a simulator shortens the training time by orders of magnitude compared with training the agent on the testbed. We evaluate the learned policies on the testbed and show the effectiveness of our approach in several scenarios. In one scenario, we design a controller that achieves the management objectives with 50% less system resources than Kubernetes HPA autoscaling.","1932-4537","","10.1109/TNSM.2024.3434328","Swedish Governmental Agency for Innovation Systems, VINNOVA, through project ANIARA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10612769","Performance management;adaptive resource allocation;microservice;reinforcement learning;operating region","Microservice architectures;Measurement;Training;Reinforcement learning;Delays;Resource management;Throughput","","","","82","CCBY","26 Jul 2024","","","IEEE","IEEE Journals"
"Cross-Domain Solutions (CDS): A Comprehensive Survey","V. Sundaravarathan; H. Alqalaf; A. Siddiqui; K. Kim; S. Lee; M. Reisslein; A. S. Thyagaturu; N. Ross; J. Howard; S. Tayal","School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA; School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA; College of Computer and Information Sciences, Prince Sultan University, Riyadh, Saudi Arabia; School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA; Hyundai Transys, Mesa, AZ, USA; School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA; Network and Edge Group, Intel Corporation, Chandler, AZ, USA; Network and Edge Group, Intel Corporation, Chandler, AZ, USA; Extreme Scale Computing Group, Intel Corporation, Hillsboro, OR, USA; Extreme Scale Computing Group, Intel Corporation, Hillsboro, OR, USA",IEEE Access,"11 Nov 2024","2024","12","","163551","163620","A domain is commonly defined as a set of system resources, e.g., computers, to which certain users have prescribed access rights as governed by some security policies. The access (viewing) and transfer of data between distinct domains facilitates a wide range of information technology applications. A Cross-Domain Solution (CDS) can provide the security mechanisms that are required to properly restrict the access and exchange of sensitive information between different domains. This article provides a comprehensive up-to-date survey of CDS. This survey is organized according to a CDS taxonomy with access, transfer, and Multi-Level Security (MLS) as the main CDS categories. An access solution helps a client to view the data present in a server (which is in a different domain, other than the client’s domain); thus an access solution effectively provides isolation between domains by allowing data access (without data transfer capabilities) between these domains. A transfer solution secures the transfer of data between domains to avoid leaks of data to unauthorized entities. Thereby, a transfer solution effectively connects domains in either a unidirectional or bidirectional manner. An MLS solution handles data with multiple levels of security (sensitivity of the data), such as top-secret, secret, or unclassified data. Aside from the fundamental conceptual approaches from the existing CDS research, this survey gives a comprehensive overview of the existing commercial CDS products. We identify the limitations of the existing CDS concepts and products and outline directions for future research and development to address these limitations and advance the overall CDS research area.","2169-3536","","10.1109/ACCESS.2024.3483659","Defense Advanced Research Projects Agency (DARPA) to Intel Corp; Department of Defense or the US Government; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10721459","Access;data diode;cross-domain solution (CDS);multi level security (MLS);transfer","Security;Hardware;Wireless communication;Surveys;Privacy;Military computing;Buildings;Wireless sensor networks;Testing","","","","365","CCBY","18 Oct 2024","","","IEEE","IEEE Journals"
"Run-Time Value Chain Analysis and Cost Accounting via Microservices in Agile Manufacturing","S. Javed; C. Paniagua; I. Javed; J. van Deventer; J. Delsing","Department of Computer Science, Electrical and Space Engineering, Lulea University of Technology, Lulea, Sweden; Department of Computer Science, Electrical and Space Engineering, Lulea University of Technology, Lulea, Sweden; Independent Researcher, Dubai, UAE; Department of Computer Science, Electrical and Space Engineering, Lulea University of Technology, Lulea, Sweden; Department of Computer Science, Electrical and Space Engineering, Lulea University of Technology, Lulea, Sweden",IEEE Open Journal of the Industrial Electronics Society,"7 Feb 2025","2025","6","","181","201","The rapid evolution of manufacturing processes driven by Industry 4.0 demands systems capable of quickly adapting to dynamic market conditions and evolving customer needs. Agile manufacturing emphasizes flexibility, adaptability, and real-time responsiveness, posing challenges in run-time value chain analysis (VCA), including cost flows and production times. This article presents a novel two-stage VCA approach using an activity-based costing mechanism via microservices to address these challenges. The VCA system enables real-time cost accounting and decision-making, supporting both pre and postproduction VCA, contrasting with traditional methods that rely on historical data. The first stage involves top–down cost calculations from resources to microservices. In contrast, the second focuses on constructing efficient manufacturing activities based on product requirements, allowing for granular analysis of costs and production times across microservices, activities, broader business processes, and finally, cost objects (e.g., customized products, batches of products, or customer invoices). The approach is validated through a proof-of-concept implementation of the VCA system integrated with the Eclipse Arrowhead framework and simulating Fischertechnik indexed line milling, drilling, and conveying operations. The results demonstrate the effectiveness of the proposed method in providing detailed insights into costs and production times, enhancing the efficiency and competitiveness of agile manufacturers.","2644-1284","","10.1109/OJIES.2025.3532664","European Commission and Arrowhead flexible Production Value Network; Electronic Components and Systems for European Leadership(grant numbers:101111977); AI-REDGIO5.0(grant numbers:101092069); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10849611","Activity-based costing (ABC);agile manufacturing;eclipse arrowhead framework;Industry 4.0 (I4.0);real-time cost accounting and decision-making","Microservice architectures;Agile manufacturing;Costs;Production;Real-time systems;Manufacturing;Cost accounting;Industrial electronics;Decision making;Resource management","","","","46","CCBY","22 Jan 2025","","","IEEE","IEEE Journals"
"AI-Native Multi-Access Future Networks—The REASON Architecture","K. Katsaros; I. Mavromatis; K. Antonakoglou; S. Ghosh; D. Kaleshi; T. Mahmoodi; H. Asgari; A. Karousos; I. Tavakkolnia; H. Safi; H. Hass; C. Vrontos; A. Emami; J. Marcelo Parra-Ullauri; S. Moazzeni; D. Simeonidou","Digital Catapult, London, U.K.; Digital Catapult, London, U.K.; Digital Catapult, London, U.K.; Digital Catapult, London, U.K.; Digital Catapult, London, U.K.; Department of Engineering, King’s College London, Strand Campus, London, U.K.; Thales Research and Technology, Reading, U.K.; Real Wireless, Pulborough, U.K.; Department of Engineering, University of Cambridge, Cambridge, U.K.; Department of Engineering, University of Cambridge, Cambridge, U.K.; Department of Engineering, University of Cambridge, Cambridge, U.K.; Faculty of Engineering, University of Bristol, Bristol, U.K.; Faculty of Engineering, University of Bristol, Bristol, U.K.; Faculty of Engineering, University of Bristol, Bristol, U.K.; Faculty of Engineering, University of Bristol, Bristol, U.K.; Faculty of Engineering, University of Bristol, Bristol, U.K.",IEEE Access,"9 Dec 2024","2024","12","","178586","178622","The development of the sixth generation of communication networks (6G) has been gaining momentum over the past years, with a target of being introduced by 2030. Several initiatives worldwide are developing innovative solutions and setting the direction for the key features of these networks. Some common emerging themes are the tight integration of AI, the convergence of multiple access technologies and sustainable operation, aiming to meet stringent performance and societal requirements. To that end, we are introducing REASON - Realising Enabling Architectures and Solutions for Open Networks. The REASON project aims to address technical challenges in future network deployments, such as E2E service orchestration, sustainability, security and trust management, and policy management, utilising AI-native principles, considering multiple access technologies and cloud-native solutions. This paper presents REASON’s architecture and the identified requirements for future networks. The architecture is meticulously designed for modularity, interoperability, scalability, simplified troubleshooting, flexibility, and enhanced security, taking into consideration current and future standardisation efforts, and the ease of implementation and training. It is structured into four horizontal layers: Physical Infrastructure, Network Service, Knowledge, and End-User Application, complemented by two vertical layers: Management and Orchestration, and E2E Security. This layered approach ensures a robust, adaptable framework to support the diverse and evolving requirements of 6G networks, fostering innovation and facilitating seamless integration of advanced technologies.","2169-3536","","10.1109/ACCESS.2024.3507186","Project REASON, a U.K. Government funded project under the Future Open Networks Research Challenge (FONRC) sponsored by the Department of Science Innovation and Technology (DSIT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10769448","Future networks;6G;AI/ML;open networks;reference architecture;native AI","6G mobile communication;Ethics;Artificial intelligence;5G mobile communication;Real-time systems;Energy efficiency;Sustainable development;Security;Resilience;Interoperability","","","","113","CCBYNCND","27 Nov 2024","","","IEEE","IEEE Journals"
"PEaF-Production Environment Analyzer Framework: Assisting Continuous Deployment of 5G Workloads Using AI/ML","K. Subramaniam; S. Kumar; A. Mishra; A. Bhandari; J. Manja Ppallan; G. Chandrasekaran","Samsung Electronics Research and Development, Bengaluru, India; Samsung Electronics Research and Development, Bengaluru, India; Samsung Electronics Research and Development, Bengaluru, India; Samsung Electronics Research and Development, Bengaluru, India; Samsung Electronics Research and Development, Bengaluru, India; Samsung Electronics Research and Development, Bengaluru, India",IEEE Access,"17 Oct 2024","2024","12","","147012","147022","A Production Environment Analyzer Framework (PEaF) is proposed to address the limitations of the Continuous Deployment (CD) process for 5G workflow lifecycle management. By integrating an AI/ML-based PEaF into the CD pipeline, we aim to ensure reliable deployments. PEaF uses AI/ML techniques to analyze the production environment and predict the health status of the hardware components. It collects raw data, applies K-Means clustering to group similar data points, and assigns scores to each cluster. These scores serve as features for training Support Vector Machine (SVM) and Random Forest (RF) classifiers to classify hardware health status. Experimental results show that PEaF achieves high classification accuracies of 97.26% and 96.44% for SVM and RF, respectively, with clustering. By analyzing the production environment and excluding deteriorating hardware from the CD, service failures are reduced by at least 27.04%. Moreover, PEaF decreases the polling frequency of hardware status by 48.7%, enhancing operational efficiency. Overall, PEaF contributes to advancing Continuous Integration/Continuous Deployment (CI/CD) practices in the 5G ecosystem, ensuring the reliability and stability of the production environment before deploying/upgrading services.","2169-3536","","10.1109/ACCESS.2024.3472498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10703063","5G;AI/ML;CI/CD;classification;feature selection;K-means clustering;random forest;regression;support vector machine","Hardware;5G mobile communication;Pipelines;Clustering methods;Production;Communications technology;DevOps;Random forests;Telecommunication network reliability;Support vector machines;Classification algorithms","","","","16","CCBYNCND","3 Oct 2024","","","IEEE","IEEE Journals"
"Application of Category Theory to Network Service Fault Detection","P. Martinez-Julia; V. P. Kafle; H. Asaeda","Network Architecture Laboratory, Network Research Institute, National Institute of Information and Communications Technology, Tokyo, Japan; Network Architecture Laboratory, Network Research Institute, National Institute of Information and Communications Technology, Tokyo, Japan; Network Architecture Laboratory, Network Research Institute, National Institute of Information and Communications Technology, Tokyo, Japan",IEEE Open Journal of the Communications Society,"30 Jul 2024","2024","5","","4417","4443","Network automation has become crucial in supporting services in 6G networks. This mainly derives from the complexity of the composition of numerous distributed virtual network functions (VNFs) in creating highly flexible virtual network services. Therefore, a network service automation system is a key technology enabler for 6G. However, the added complexity renders network service automation systems particularly sensitive to faults, some of which cause network outages that harm the smooth operation of basic societal services. Current state-of-the-art (SotA) solutions for fault detection can barely detect hidden faults. Herein, we propose a mechanism for automated network service analysis (ANSA), which constructs and analyzes a digital twin of a network service. The digital twin represents the available information about the network service based on category theory. It uses the properties of category theory to perform an analysis through which the faults of the network service are identified. We evaluate a prototype of a network service automation system that incorporates ANSA to demonstrate 1) the benefits of using digital twins for analyzing network services, 2) the benefits of using category theory for constructing digital twins of the network services, and 3) the resulting improvements in fault detection. Overall, ANSA can detect an average of 94% of the faults present in a network service. In comparison, previous SotA solutions can detect only 30%–50% of all faults.","2644-125X","","10.1109/OJCOMS.2024.3425831","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10592034","Network automation;fault detection;category theory","Fault detection;Automation;Proposals;Digital twins;Monitoring;Data models;6G mobile communication","","","","47","CCBY","10 Jul 2024","","","IEEE","IEEE Journals"
"Network Digital Twin Toward Networking, Telecommunications, and Traffic Engineering: A Survey","R. Poorzare; D. N. Kanellopoulos; V. K. Sharma; P. Dalapati; O. P. Waldhorst","Institute of Data-Centric Software Systems (IDSS), Karlsruhe University of Applied Sciences, Karlsruhe, Germany; Department of Mathematics, University of Patras, Patras, Greece; Department of Computer Science and Engineering, The LNM Institute of Information Technology, Jaipur, India; Department of Computer Science and Engineering, The LNM Institute of Information Technology, Jaipur, India; Institute of Data-Centric Software Systems (IDSS), Karlsruhe University of Applied Sciences, Karlsruhe, Germany",IEEE Access,"28 Jan 2025","2025","13","","16489","16538","Network Digital Twin (NDT) is an evolving technology that provides a framework through which a network administrator can have a virtual representation of a computer network. As a result, analysis, monitoring, testing, running new protocols, and more can be performed using the NDT before the final deployment of the developed approach. In this way, the consequences of direct deployment and the negative impact on network operations can be avoided. Telecommunications, along with traffic engineering as one of its critical components, play a prominent role across various networking domains, including Internet service providers, data centers, cellular networks, intelligent transportation systems, and smart cities. In this context, NDT has the potential to serve as a key enabler for optimizing these domains by providing a digital framework, which can facilitate the evaluation and enhancement of different scenarios. Accordingly, this paper presents a comprehensive survey on how NDT can facilitate advancements in network traffic engineering across a wide range of networking domains. First, we start with an in-depth analysis of the evolution of the network digital twin technology and provide a comparison with simulation tools. Next, we examine the role of NDT in various networking and telecommunication domains. We also explore the applicability of NDT technology from a traffic engineering perspective across different network types. Subsequently, we highlight key open research questions and potential future directions that warrant further investigation. Finally, we conclude by outlining the promising future trajectory of NDT within the aforementioned domains.","2169-3536","","10.1109/ACCESS.2025.3531947","Ministry of Science, Research, and the Arts Baden-Württemberg (MWK)(grant numbers:bwNET2.0); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10847826","Digital twin;network digital twin;telecommunications;traffic engineering;traffic optimization","Telecommunications;Data models;Digital twins;Telecommunication traffic;Surveys;Routing;Industrial Internet of Things;Big Data;Testing;Standardization","","","","303","CCBY","20 Jan 2025","","","IEEE","IEEE Journals"
"Enhanced Classification System for Real-Time Embedded Vision Applications","R. Khelifi; B. Nini; M. Berkane","Research Laboratory on Computer Science’s Complex Systems (ReLa(CS)2), University of Oum El Bouaghi, Oum El Bouaghi, Algeria; Research Laboratory on Computer Science’s Complex Systems (ReLa(CS)2), University of Oum El Bouaghi, Oum El Bouaghi, Algeria; Artificial Intelligence and Autonomous Things Laboratory, University of Oum El Bouaghi, Oum El Bouaghi, Algeria",IEEE Access,"8 Nov 2024","2024","12","","162311","162326","Embedded computer vision systems are increasingly being adopted across various domains, playing a pivotal role in enabling advanced technologies such as autonomous vehicles and industrial automation. Their cost-effectiveness, compact size, and portability make them particularly well-suited for diverse implementations and operations. In real-time scenarios, these systems must process visual data with minimal latency, which is crucial for immediate decision-making. However, these solutions continue to face significant challenges related to computational efficiency, memory usage, and accuracy. This research addresses these challenges by enhancing classification methodologies, specifically in Gray Level Co-occurrence Matrix (GLCM) feature extraction and Support Vector Machine (SVM) classifiers. To maintain a high level of accuracy while preserving performance, a smaller feature set is selected following a comprehensive complexity analysis and is further refined through Correlation-based Feature Selection (CFS). The proposed method achieves an overall classification accuracy of 84.76% with a feature set reduced by 79.2%, resulting in a 72.45% decrease in processing time, a 50% reduction in storage requirements, and up to a 77.8% decrease in memory demand during prediction. These improvements demonstrate the effectiveness of the proposed approach in improving the adaptability and capabilities of embedded vision systems (EVS), optimizing their performance under the constraints of real-time limited-resource environments.","2169-3536","","10.1109/ACCESS.2024.3489476","Research Laboratory on Computer Science’s Complex Systems (ReLa(CS)2), University of Oum El Bouaghi, Oum El Bouaghi, Algeria; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10740277","Embedded computer vision;limited resource systems;machine learning;pattern classification;real-time image processing","Accuracy;Support vector machines;Real-time systems;Feature extraction;Memory management;Computer vision;Surveillance;Bandwidth;Wildlife;Machine learning;Image processing","","","","53","CCBYNCND","31 Oct 2024","","","IEEE","IEEE Journals"
"Serverless Microservice Architecture for Cloud-Edge Intelligence in Sensor Networks","D. Loconte; S. Ieva; F. Gramegna; I. Bilenchi; C. Fasciano; A. Pinto; G. Loseto; F. Scioscia; M. Ruta; E. Di Sciascio","Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Department of Engineering, LUM “Giuseppe Degennaro” University, Casamassima, Italy; Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy",IEEE Sensors Journal,"5 Mar 2025","2025","25","5","7875","7885","Machine learning (ML) is increasingly exploited in a wide range of application areas to analyze data streams from large-scale sensor networks, train predictive models, and perform inference. The cloud-edge intelligence (CEI) computing paradigm integrates cloud infrastructures for resource-intensive ML tasks with devices at the border of a local network for distributed data preprocessing, small-scale model training, and prediction tasks. This can achieve a tunable trade-off of ML accuracy with improved data privacy, response latency, and bandwidth usage. Prevalent CEI architectures are based on microservices encapsulated in containers, but serverless computing is emerging as an alternative model. It is based on stateless event-driven functions to facilitate the development and provisioning of application components, increase the infrastructure elasticity, and reduce management effort. This article proposes a novel CEI framework for sensor-based applications, exploiting serverless computing for data management and ML tasks. Small-scale model training occurs at the edge with local data for quick prediction response, while large-scale models are trained in the cloud with the full sensor network data, and then, they are fed back to edge nodes for a progressive accuracy improvement. A fully functional prototype has been built by leveraging open-source software tools, selected devices for field sensing and edge computing (EC), and a commercial cloud platform. Experiments validate the feasibility and sustainability of the proposal, compared with an existing container-oriented microservice architecture.","1558-1748","","10.1109/JSEN.2024.3502254","Spoke 9 of the Italian National Center for High-Performance Computing, Big Data and Quantum Computing; Digital Enterprise Grant; Lutech S.p.A. and European Regional Development Fund for Apulia Region 2014/2020 Operating Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10767204","Cloud-edge intelligence (CEI);machine learning (ML);microservices;sensor networks;serverless computing","Cloud computing;Sensors;Training;Computer architecture;Microservice architectures;Computational modeling;Data models;Accuracy;Internet of Things;Intelligent sensors","","","","41","CCBY","25 Nov 2024","","","IEEE","IEEE Journals"
"Adaptive VNF Placement Considering Overall Latency and 5G Wireless Channel Reliability in Industry 4.0: A Reinforcement Learning Based Approach","N. Saqib; N. F. Abdullah; A. Abu-Samah; H. A. H. Alobaidy; R. Nordin","Department of Electrical, Electronic and Systems Engineering, Wireless Research@UKM, Universiti Kebangsaan Malaysia, Bangi, Malaysia; Department of Electrical, Electronic and Systems Engineering, Wireless Research@UKM, Universiti Kebangsaan Malaysia, Bangi, Malaysia; Department of Electrical, Electronic and Systems Engineering, Wireless Research@UKM, Universiti Kebangsaan Malaysia, Bangi, Malaysia; Department of Electrical, Electronic and Systems Engineering, Wireless Research@UKM, Universiti Kebangsaan Malaysia, Bangi, Malaysia; Department of Engineering, School of Engineering and Technology, Sunway University, Bandar Sunway, Selangor, Malaysia",IEEE Access,"2 Jul 2024","2024","12","","88883","88896","Industry 4.0 incorporates the integration of cloud computing, Industrial Internet of Things (IIoT), and modern communication technologies within the industrial automation systems. Various devices with different network requirements of high reliability and low latency, rely on connectivity. The 5G and Beyond (B5G) software-defined architecture facilitates Network Function Virtualization (NFV), which is an essential solution for fulfilling these stringent demands. NFV allows for the implementation and control of Virtual Network Functions (VNFs) in dynamic network environments. VNF placement optimization has been extensively studied in the 5G perspective outside the industry environment with a focus on minimizing delay and cost, increasing VNF reliability, and increasing resource efficiency. However, the complex dynamics of the wireless channel in industrial environments have a considerable impact on the essential delay factors that are important for optimizing the deployment of VNFs. This study focuses on modeling a Wireless Sensor Network (WSN) based Industry 4.0 factory automation scenario at mmWave band, formulating an optimization problem to minimize overall delay while considering packet loss rate in the 5G industrial wireless channel. The optimization problem is formulated as a Markov Decision Process (MDP) and two Reinforcement Learning (RL) based algorithms AVP-Q and AVP-DQN are proposed for optimizing the VNF placement. The proposed algorithms are extensively evaluated against the Value Iteration algorithm which assumes a completely known MDP model and two other algorithms from the literature. The simulated results show that AVP-DQN outperforms existing algorithms for this scenario by 39% and 22.6% and the achieved performance is only close to that of the Value Iteration algorithm.","2169-3536","","10.1109/ACCESS.2024.3419065","Universiti Kebangsaan Malaysia(grant numbers:DPK-2022-009,GUP-2021-023); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10571938","5G;industry 4.0;deep reinforcement learning;VNF placement;URLLC;mmWave","Wireless sensor networks;5G mobile communication;Wireless communication;Cloud computing;Production facilities;Reliability;Fourth Industrial Revolution;Deep reinforcement learning;Millimeter wave communication","","","","61","CCBYNCND","26 Jun 2024","","","IEEE","IEEE Journals"
"Drone-Based AI System for Wildfire Monitoring and Risk Prediction","C. A. S. Lelis; J. J. Roncal; L. Silveira; R. D. G. De Aquino; C. A. C. Marcondes; J. Marques; D. S. Loubach; F. A. N. Verri; V. V. Curtis; D. G. De Souza","Computer Science Division, Instituto Tecnológico de Aeronáutica (ITA), São José dos Campos, Brazil; Computer Science Division, Instituto Tecnológico de Aeronáutica (ITA), São José dos Campos, Brazil; Computer Science Division, Instituto Tecnológico de Aeronáutica (ITA), São José dos Campos, Brazil; Computer Science Division, Instituto Tecnológico de Aeronáutica (ITA), São José dos Campos, Brazil; Computer Science Division, Instituto Tecnológico de Aeronáutica (ITA), São José dos Campos, Brazil; Computer Science Division, Instituto Tecnológico de Aeronáutica (ITA), São José dos Campos, Brazil; Computer Science Division, Instituto Tecnológico de Aeronáutica (ITA), São José dos Campos, Brazil; Computer Science Division, Instituto Tecnológico de Aeronáutica (ITA), São José dos Campos, Brazil; Computer Science Division, Instituto Tecnológico de Aeronáutica (ITA), São José dos Campos, Brazil; Computer Science Division, Instituto Tecnológico de Aeronáutica (ITA), São José dos Campos, Brazil",IEEE Access,"4 Oct 2024","2024","12","","139865","139882","Wildfires pose a significant threat to ecosystems, human lives, and infrastructure worldwide. Traditional wildfire detection and risk assessment methods often suffer from limitations such as delayed detection and low confidence in certain regions. In this paper, we propose a novel computational system based on Machine Learning for wildfire risk assessment using data collected by drones. The system can integrate various sensors to capture spatiotemporal data on environmental factors such as temperature, humidity, and vegetation. By leveraging high-resolution data collected through autonomous drone missions, our system enhances wildfire risk estimation and enables proactive mission planning. Although the system is mainly designed to address wildfire monitoring using drone-collected data, it can be easily adapted to other environmental monitoring applications and other sources of data. We demonstrate the effectiveness of our approach through a comprehensive evaluation and validation process in both simulated and real-world environments. Our work contributes to advancing wildfire monitoring capabilities, improving early detection, and mitigating the impact of wildfires on communities and the environment.","2169-3536","","10.1109/ACCESS.2024.3462436","Financiadora de Estudos e Projetos (FINEP)(grant numbers:01.22.0615.00); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10681400","Aerial drones;artificial intelligence;environmental monitoring;machine learning;risk assessment;spatiotemporal data;wildfire detection;wildfire risk estimation","Wildfires;Predictive models;Sensors;Normalized difference vegetation index;Measurement;Drones;Artificial intelligence;Environmental monitoring;Machine learning;Risk management;Spatiotemporal phenomena","","","","37","CCBYNCND","17 Sep 2024","","","IEEE","IEEE Journals"
"The Role of Blockchain in Transforming Industries Beyond Finance","C. Vanmathi; A. Farouk; S. M. Alhammad; R. Mangayarkarasi; S. Bhattacharya; M. S. B. Kasyapa","School of Computer Science Engineering and Information Systems, Vellore Institute of Technology, Vellore, Tamil Nadu, India; Department of Computer Science, Faculty of Computers and Artificial Intelligence, Hurghada University, Hurghada, Egypt; Department of Computer Sciences, College of Computer and Information Sciences, Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia; School of Computer Science Engineering and Information Systems, Vellore Institute of Technology, Vellore, Tamil Nadu, India; School of Computer Science Engineering and Information Systems, Vellore Institute of Technology, Vellore, Tamil Nadu, India; School of Computer Science Engineering and Information Systems, Vellore Institute of Technology, Vellore, Tamil Nadu, India",IEEE Access,"18 Oct 2024","2024","12","","148845","148867","Blockchain technology is rapidly transforming how businesses and individuals interact. Its inherent security features, including immutability, tamper-proofing, and verifiable data provenance, are driving its adoption across numerous sectors. The financial sector heavily influences blockchain for its tamper-proof nature, leading to the rise of cryptocurrencies, the technology offers much more. Beyond security, it streamlines processes, reduces costs, and improves customer experience. Smart contracts are one of the predominant key components that automate predetermined operations, further enhancing blockchain’s capabilities. Understanding the importance of blockchain in non-financial applications is crucial for fully exploiting its potential. This analysis explores the fundamentals of blockchain technology and its applications beyond finance. To meet the growing demand across various sectors, blockchain must integrate with other trending technologies such as the Internet of Things (IoT), Artificial Intelligence (AI), and the edge-cloud paradigm. The analysis examines the role of these technologies in facilitating blockchain-enabled applications. Furthermore, the paper researches the specific use cases such as insurance, energy, healthcare, digital voting, supply chain management and government. Concentrating on these sectors elucidates how organisations harness blockchain to tackle intricate challenges, thereby fostering the advancement of sophisticated digital processes and contributing to societal progression.","2169-3536","","10.1109/ACCESS.2024.3468611","Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia, through the Researchers Supporting Project(grant numbers:PNURSP2024R442); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10695085","Blockchain;non-financial applications;energy;insurance;healthcare;decentralization","Blockchains;Medical services;Smart contracts;Supply chains;Supply chain management;Open source software;Finance","","","","180","CCBYNCND","26 Sep 2024","","","IEEE","IEEE Journals"
"GAS: DVFS-Driven Energy Efficiency Approach for Latency-Guaranteed Edge Computing Microservices","Z. Bellal; L. Lahlou; N. Kara; I. El Khayat","Software and Information Technology Engineering Department, École de technologie supérieure, Montréal, QC, Canada; Software and Information Technology Engineering Department, École de technologie supérieure, Montréal, QC, Canada; Software and Information Technology Engineering Department, École de technologie supérieure, Montréal, QC, Canada; Ericsson, Mississauga, ON, Canada",IEEE Transactions on Green Communications and Networking,"13 Feb 2025","2025","9","1","108","124","Edge computing-based microservices (ECM) are pivotal infrastructure components for latency-critical applications such as Virtual Reality/Augmented Reality (VR/AR) and the Internet of Things (IoT). ECM involves strategically deploying microservices at the network’s edge to fulfill the low latency needs of modern applications. However, achieving efficient resource and energy consumption while meeting the latency requirement in the ECM environment remains challenging. Dynamic Voltage and Frequency Scaling (DVFS) is a common technique to address this issue. It adjusts the CPU frequency and voltage to balance energy cost and performance. However, selecting the optimal CPU frequency depends on the nature of the microservice workload (e.g., CPU-bound, memory-bound, or mixed). Moreover, various microservices with different latency requirement can be deployed on the same edge node. This makes the DVFS application extremely challenging, particularly for a chip-wide DVFS implementation for which CPU cores operate at the same frequency and voltage. To this end, we propose GAS, enerGy Aware microServices edge computing framework, which enables CPU frequency scaling to meet diverse microservice latency requirement with the minimum energy cost. Our evaluation indicates that our CPU scaling policy decreases energy consumption by 5% to 23% compared to Linux governors while maintaining latency requirement and significantly contributing to sustainable edge computing.","2473-2400","","10.1109/TGCN.2024.3420957","Natural Sciences and Engineering Research Council and Ericsson Canada(grant numbers:561771); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10578041","Edge computing;microservice;DVFS;energy-efficient;container autoscaling","Microservice architectures;Edge computing;Energy consumption;Energy efficiency;Task analysis;Frequency diversity;Time-frequency analysis","","","","42","IEEE","1 Jul 2024","","","IEEE","IEEE Journals"
"An AI-Assisted Framework for Lifecycle Management of Beyond 5G Services","A. -I. Manolopoulos; V. -M. Alevizaki; M. Anastasopoulos; A. Tzanakaki","Department of Physics, National and Kapodistrian University of Athens, Athens, Greece; Department of Physics, National and Kapodistrian University of Athens, Athens, Greece; Department of Physics, National and Kapodistrian University of Athens, Athens, Greece; Department of Physics, National and Kapodistrian University of Athens, Athens, Greece",IEEE Access,"5 Dec 2024","2024","12","","179449","179463","Future mobile communication networks aim to offer services and applications in the most flexible, adaptable and cost-effective manner. B5G networks aim at a fully softwarized network architecture, where hardware and software programming is used for the design, implementation, deployment, management, monitoring and maintenance of network equipment/components/services. Artificial Intelligence (AI) and Machine Learning (ML) techniques are steadily being integrated into 5G systems, offering intelligent automation, proactive network management, and resource allocation optimization. In this environment, the role of Management and Orchestration (MANO) is vital to ensure efficient infrastructure utilization and fulfillment of heterogeneous service requirements. Despite the development of various tools and platforms to facilitate MANO in 5G systems, in most cases there is still the need of human intervention and manual input for configuring the 5G elements according to service requirements. In this paper, a MANO framework has been developed, that specifically targets the orchestration operations of 5G networks. The proposed framework focuses on the lifecycle management of the 5G components, in order to achieve an operational environment with minimal human intervention or manual configuration (Zero Touch Networks -ZTN). Within this ecosystem, an Analytics & AI/ML Platform has comprehensive monitoring capabilities and influences decisions across various layers or aspects of the infrastructure. This includes optimizing the allocation and orchestration of both networking and edge/cloud computing virtual resources within the infrastructure.","2169-3536","","10.1109/ACCESS.2024.3507359","European Union Projects 5G Trusted and Secure Network Services(grant numbers:101127973); Seamless Integration of Efficient 6G Wireless Technologies for Communication and Sensing(grant numbers:101139282); ECO-eNET(grant numbers:101139133); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10769360","5G;B5G;6G;MANO;slicing;NFV;LSTM;LCM;ZSM","5G mobile communication;Cloud computing;Monitoring;Computer architecture;Quality of service;Resource management;Optimization;Maintenance;Virtualization;Ultra reliable low latency communication","","","","51","CCBY","27 Nov 2024","","","IEEE","IEEE Journals"
"Design of an Optimal Scheduling Control System for Smart Manufacturing Processes in Tobacco Industry","X. Liu; J. Li; H. Wang; W. Jia; J. Yang; Z. Guo","Qingzhou Cigarette Factory, China Tobacco Shandong Industrial Co.Ltd., Qingzhou, China; Qingzhou Cigarette Factory, China Tobacco Shandong Industrial Co.Ltd., Qingzhou, China; Qingzhou Cigarette Factory, China Tobacco Shandong Industrial Co.Ltd., Qingzhou, China; Qingzhou Cigarette Factory, China Tobacco Shandong Industrial Co.Ltd., Qingzhou, China; School of Artificial Intelligence, Chongqing Technology and Business University, Chongqing, China; School of Artificial Intelligence, Chongqing Technology and Business University, Chongqing, China",IEEE Access,"6 Apr 2023","2023","11","","33027","33036","The whole process of tobacco production is composed of many components, in which their operation and administration are currently independent. It is required to deploy smart manufacturing workflow for the whole production process, in order to realize centralized effective global scheduling. This requires an advanced administration control platform that has strong abilities of multisource data integration and automatic decision support. To bridge such research gap, this paper designs an optimal scheduling control system for smart manufacturing processes of tobacco industry. First of all, this work discusses major characteristics of future-generation production control patterns in intelligent tobacco factories (ITF). Then, a five-layer architecture for optimal scheduling control of ITF is proposed, which contains Internet-of-Things layer, centralized control layer, model layer, platform layer and operation layer. In addition, a production scheduling optimization strategy is also developed for the proposed system to serve as the software algorithm that drives the running of whole smart manufacturing processes. Finally, this paper presents a comparative analysis of the proposed system’s transformation in a cigarette factory. Naturally, the effectiveness of the proposed production optimization scheduling strategy is verified through simulation.","2169-3536","","10.1109/ACCESS.2023.3261883","China Tobacco Shandong Industrial Co., Ltd.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10081360","Smart manufacturing;optimal scheduling control;Internet of Things;production decision","Production;Manufacturing;Industries;Production facilities;Centralized control;Data models;Process control","","","","30","CCBY","27 Mar 2023","","","IEEE","IEEE Journals"
"Retrospecting Available CPU Resources: SMT-Aware Scheduling to Prevent SLA Violations in Data Centers","H. Liao; T. -y. Liu; J. Guo; B. Huang; D. Yang; J. Ding","School of Data Science and Engineering, East China Normal University, Shanghai, China; School of Data Science and Engineering, East China Normal University, Shanghai, China; School of Data Science and Engineering, East China Normal University, Shanghai, China; School of Data Science and Engineering, East China Normal University, Shanghai, China; TRE of Alibaba Group, Zhejiang, China; SATG of Intel, Shanghai, China",IEEE Transactions on Parallel and Distributed Systems,"28 Nov 2024","2025","36","1","67","83","The article focuses on an understudied yet fundamental problem: existing methods typically average the utilization of multiple hardware threads to evaluate the available CPU resources. However, the approach could underestimate the actual usage of the underlying physical core for Simultaneous Multi-Threading (SMT) processors, leading to an overestimation of remaining resources. The overestimation propagates from microarchitecture to operating systems and cloud schedulers, which may misguide scheduling decisions, exacerbate CPU overcommitment, and increase Service Level Agreement (SLA) violations. To address the potential overestimation problem, we propose an SMT-aware and purely data-driven approach named Remaining CPU (RCPU) that reserves more CPU resources to restrict CPU overcommitment and prevent SLA violations. RCPU requires only a few modifications to the existing cloud infrastructures and can be scaled up to large data centers. Extensive evaluations in the data center proved that RCPU contributes to a reduction of SLA violations by 18% on average for 98% of all latency-sensitive applications. Under a benchmarking experiment, we prove that RCPU increases the accuracy by 69% in terms of Mean Absolute Error (MAE) compared to the state-of-the-art.","1558-2183","","10.1109/TPDS.2024.3494879","National Natural Science Foundation of China(grant numbers:62272167); Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10748366","Cloud computing;SMT interference;data center;QoS;microarchitecture;latency-sensitive applications","Hardware;Data centers;Cloud computing;Servers;Processor scheduling;Program processors;Monitoring;Message systems;Benchmark testing;Accuracy","","","","102","IEEE","8 Nov 2024","","","IEEE","IEEE Journals"
"Process Manufacturing Intelligence Empowered by Industrial Metaverse: A Survey","W. Luo; K. Huang; X. Liang; H. Ren; N. Zhou; C. Zhang; C. Yang; W. Gui","Department of Strategic and Advanced Interdisciplinary Research, Pengcheng Laboratory, Shenzhen, China; Department of Strategic and Advanced Interdisciplinary Research, Pengcheng Laboratory, Shenzhen, China; Department of Strategic and Advanced Interdisciplinary Research, Pengcheng Laboratory, Shenzhen, China; Department of Strategic and Advanced Interdisciplinary Research, Pengcheng Laboratory, Shenzhen, China; Department of Strategic and Advanced Interdisciplinary Research, Pengcheng Laboratory, Shenzhen, China; Department of Strategic and Advanced Interdisciplinary Research, Pengcheng Laboratory, Shenzhen, China; Department of Strategic and Advanced Interdisciplinary Research, Pengcheng Laboratory, Shenzhen, China; Department of Strategic and Advanced Interdisciplinary Research, Pengcheng Laboratory, Shenzhen, China",IEEE Transactions on Cybernetics,"30 Oct 2024","2024","54","11","6679","6692","The intelligent goal of process manufacturing is to achieve high efficiency and greening of the entire production. Whereas the information system it used is functionally independent, resulting to knowledge gaps between each level. Decision-making still requires lots of knowledge workers making manually. The industrial metaverse is a necessary means to bridge the knowledge gaps by sharing and collaborative decision-making. Considering the safety and stability requirements of the process manufacturing, this article conducts a thorough survey on the process manufacturing intelligence empowered by industrial metaverse. First, it analyzes the current status and challenges of process manufacturing intelligence, and then summarizes the latest developments about key enabling technologies of industrial metaverse, such as interconnection technologies, artificial intelligence, cloud-edge computing, digital twin (DT), immersive interaction, and blockchain technology. On this basis, taking into account the characteristics of process manufacturing, a construction approach and architecture for the process industrial metaverse is proposed: a virtual-real fused industrial metaverse construction method that combines DTs with physical avatar, which can effectively ensure the safety of metaverse’s application in industrial scenarios. Finally, we conducted preliminary exploration and research, to prove the feasibility of proposed method.","2168-2275","","10.1109/TCYB.2024.3420958","Major Key Project of PCL(grant numbers:PCL2023A09,PCL2023A08); National Natural Science Foundation of China(grant numbers:62103208,62203241); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10605094","Industrial metaverse;intelligent manufacturing;process manufacturing;virtual-real fusion","Metaverse;Manufacturing;Decision making;Process control;Optimization;Intelligent manufacturing systems;Process monitoring;Virtual reality;Industrial engineering;Production engineering","","","","95","IEEE","19 Jul 2024","","","IEEE","IEEE Journals"
"Accelerating Digital Twin Development With Generative AI: A Framework for 3D Modeling and Data Integration","S. Gebreab; A. Musamih; K. Salah; R. Jayaraman; D. Boscovic","Department of Computer and Information Engineering, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Management Science and Engineering, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Computer and Information Engineering, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Industrial Engineering, New Mexico State University, Las Cruces, NM, USA; Center for AI and Data Analytics, Blockchain Research Laboratory, Arizona State University, Tempe, AZ, USA",IEEE Access,"17 Dec 2024","2024","12","","185918","185936","Digital twins (DTs) have been introduced as valuable tools for digitally representing physical objects or assets. However, developing comprehensive and accurate DTs remains challenging due to the complexity of adding diverse data sources, creating realistic models, and enabling real-time synchronization. In this paper, we propose a DT framework that uses Generative Artificial Intelligence (GenAI) techniques integrated into the DT development pipeline to address these challenges and accelerate the creation of these virtual representations. We demonstrate how 3D generative models utilizing pre-trained 2D diffusion models, and Large Language Models (LLMs) can automate and accelerate key stages of the DT development process, which include 3D modeling, data acquisition and integration, as well as simulation and monitoring. By providing a use-case scenario of a smart medical cooler box, we demonstrate the effectiveness of the proposed framework, highlighting the potential of GenAI to reduce manual effort and streamline the integration of DT components. In particular, we illustrate how it can accelerate the creation of 3D models for DTs from 2D images by using 2D-to-3D generative models. Additionally, we show the use of LLM-based agents in automating the integration of data sources with a DT and connecting physical devices with their virtual counterparts. Challenges related to computational scalability, data privacy, and model hallucinations are highlighted, which need to be addressed for the widespread adoption of GenAI in DT development.","2169-3536","","10.1109/ACCESS.2024.3514175","Khalifa University of Science and Technology(grant numbers:RIG-2023-049); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10786996","Generative AI;large language models;3D generative models;diffusion models;digital twin","Three-dimensional displays;Solid modeling;Data models;Rendering (computer graphics);Monitoring;Computational modeling;Adaptation models;Accuracy;Surface treatment;Soft sensors","","","","55","CCBY","9 Dec 2024","","","IEEE","IEEE Journals"
"5G Networks Security Mitigation Model: An ANN-ISM Hybrid Approach","R. A. Khan; H. U. Khan; H. S. Alwageed; H. A. Hashimi; I. Keshta","Department of Computer Science and IT, Software Engineering Research Group, University of Malakand, Malakand, Paksitan; Department of Accounting and Information Systems, College of Business and Economics, Qatar University, Doha, Qatar; College of Computer and Information Sciences, Jouf University, Sakaka, Saudi Arabia; Software Engineering Department, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Computer Science and Information Systems Department, College of Applied Sciences, Almaarefa University, Riyadh, Saudi Arabia",IEEE Open Journal of the Communications Society,"31 Jan 2025","2025","6","","881","925","The advent of Fifth-Generation (5G) networks has introduced significant security challenges due to increased complexity and diverse use cases. Conventional threat models may fall short of addressing these emerging threats effectively. This paper presents a new security mitigation model using artificial neural network (ANN) with interpretive structure modeling (ISM) to improve the 5G network security system. The main goal of this study is to develop a 5G network security mitigation model (5GN-SMM) that leverages the predictive capabilities of ANN and the analysis of ISM to identify and mitigate security threats by providing practices in 5G networks. This model aims to improve the accuracy and effectiveness of security measures by integrating advanced computational practices with systematic modeling. Initially, a systematic evaluation of existing 5G network security threats was conducted to identify gaps and incorporate best practices into the proposed model. In the second phase, an empirical survey was conducted to identify and validate the systematic literature review (SLR) findings. In the third phase, we employed a hybrid approach integrating ANN for real-time threat detection and risk assessment and utilizing ISM to analyze the relationships between security threats and vulnerabilities, creating a structured framework for understanding their interdependencies. A case study was conducted in the last stage to test and evaluate 5GN-SMM. The given article illustrates that the proposed hybrid model of ANN-ISM shows a better understanding and management of the security threats than the conventional techniques. The component of the ANN then comes up with the potential of the security breach with improved accuracy, and the ISM framework helps in understanding the relationship and the priorities of the threats. We identified 15 security threats and 144 practices in 5G networks through SLR and empirical surveys. The identified security threats were then analyzed and categorized into 15 process areas and five levels of 5GN-SMM. The proposed model includes state-of-the-art machine learning with traditional information security paradigms to offer an integrated solution to the emerging complex security issues related to 5G. This approach enhances the capacity to detect threats and contributes to good policy enforcement and other risk-related activities to enhance safer 5G networks.","2644-125X","","10.1109/OJCOMS.2025.3529717","Qatar National Library; Qatar University(grant numbers:IRCC-2021-010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10841375","5G networks;security threats and practices;systematic literature review;survey and case study;artificial neural networks (ANN);interpretive structure modeling (ISM);threat prediction and assessment","5G mobile communication;Security;Network security;Computational modeling;Artificial neural networks;Prevention and mitigation;Accuracy;Telecommunication traffic;Systematic literature review;Privacy","","","","87","CCBY","14 Jan 2025","","","IEEE","IEEE Journals"
"Advancing Manufacturing Through Artificial Intelligence: Current Landscape, Perspectives, Best Practices, Challenges, and Future Direction","R. Rakholia; A. L. Suárez-Cetrulo; M. Singh; R. Simón Carbajo","Ireland’s Centre for Artificial Intelligence (CeADAR), School of Computer Science, University College Dublin (UCD), Dublin 4, Ireland; Ireland’s Centre for Artificial Intelligence (CeADAR), School of Computer Science, University College Dublin (UCD), Dublin 4, Ireland; Ireland’s Centre for Artificial Intelligence (CeADAR), School of Computer Science, University College Dublin (UCD), Dublin 4, Ireland; Ireland’s Centre for Artificial Intelligence (CeADAR), School of Computer Science, University College Dublin (UCD), Dublin 4, Ireland",IEEE Access,"24 Sep 2024","2024","12","","131621","131637","The industrial sector is currently undergoing a transformative era of intelligent automation driven by Artificial Intelligence (AI) capabilities. This synergy greatly enhances efficiency and seamlessly enables data-driven decision-making processes. These advantages enable more efficient resource allocation and enhance production planning precision. This paper aims to provide state-of-the-art and ongoing developments in the AI landscape within the manufacturing industry. In addition, the review explores the key areas where AI is being applied in manufacturing, such as predictive maintenance, quality control, process optimization, supply chain management, robotics and automation, and intelligent decision support systems. The review also encompasses an exploration of the challenges encountered by the manufacturing sector, alongside an investigation into the potential of AI to mitigate these challenges. Furthermore, this work thoroughly reviews recent AI advancements, including explainable AI, human-robot collaboration, edge computing, and the Internet of Things (IoT) integration. The review concludes by providing recommendations, highlighting best practices, and providing insights into potential collaborative opportunities.","2169-3536","","10.1109/ACCESS.2024.3458830","PeRCEPTION Project, which focuses on the predictive development of functional foods using data analytics and machine learning; Enterprise Ireland under the Disruptive Technologies Innovation Fund (DTIF)(grant numbers:DT2020214); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10677409","Smart manufacturing;Industry 4.0;artificial intelligence;automation;machine learning;quality control;Internet of Things;robotics","Artificial intelligence;Manufacturing;Industries;Machine learning algorithms;Reviews;Automation;Production;Smart manufacturing;Fourth Industrial Revolution;Quality control;Internet of Things;Best practices","","","","110","CCBY","11 Sep 2024","","","IEEE","IEEE Journals"
"A Lean Simulation Framework for Stress Testing IoT Cloud Systems","J. Li; B. Moeini; S. Nejati; M. Sabetzadeh; M. McCallen","University of Ottawa, Ottawa, Canada; University of Ottawa, Ottawa, Canada; University of Ottawa, Ottawa, Canada; University of Ottawa, Ottawa, Canada; Cheetah Networks, Ottawa, Canada",IEEE Transactions on Software Engineering,"17 Jul 2024","2024","50","7","1827","1851","The Internet of Things (IoT) connects a plethora of smart devices globally across various applications like smart cities, autonomous vehicles, and health monitoring. Simulation plays a key role in the testing of IoT systems, noting that field testing of a complete IoT product may be infeasible or prohibitively expensive. This paper addresses a specific yet important need in simulation-based testing for IoT: Stress testing of cloud systems that are increasingly employed in IoT applications. Existing stress testing solutions for IoT demand significant computational resources, making them ill-suited and costly. We propose a lean simulation framework designed for IoT cloud stress testing. The framework enables efficient simulation of a large array of IoT and edge devices that communicate with the cloud. To facilitate simulation construction for practitioners, we develop a domain-specific language (DSL), named IoTECS, for generating simulators from model-based specifications. We provide the syntax and semantics of IoTECS and implement IoTECS using Xtext and Xtend. We assess simulators generated from IoTECS specifications for stress testing two real-world systems: a cloud-based IoT monitoring system developed by our industry partner and an IoT-connected vehicle system. Our empirical results indicate that simulators created using IoTECS: (1) achieve best performance when configured with Docker containerization; (2) effectively assess the service capacity of our case-study systems, and (3) outperform industrial stress-testing baseline tools, JMeter and Locust, by a factor of 3.5 in terms of the number of IoT and edge devices they can simulate using identical hardware resources. To gain initial insights about the usefulness of IoTECS in practice, we interviewed two engineers from our industry partner who have firsthand experience with IoTECS. Feedback from these interviews suggests that IoTECS is effective in stress testing IoT cloud systems, saving significant time and effort.","1939-3520","","10.1109/TSE.2024.3402157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10535730","Simulation-based testing;stress testing;IoT cloud;model-driven engineering;Xtext","Internet of Things;Testing;Cloud computing;Stress;Containers;Industries;Performance evaluation","","","","76","IEEE","21 May 2024","","","IEEE","IEEE Journals"
"Security, Privacy, and Trust for Open Radio Access Networks in 6G","P. Porambage; M. Christopoulou; B. Han; M. Asif Habibi; H. Bogucka; P. Kryszkiewicz","VTT Technical Research Centre of Finland, Espoo, Finland; National Centre of Scientific Research “Demokritos,”, Athens, Greece; University of Kaiserslautern (RPTU), Kaiserslautern, Germany; University of Kaiserslautern (RPTU), Kaiserslautern, Germany; Poznan University of Technology, Poznań, Poland; Poznan University of Technology, Poznań, Poland",IEEE Open Journal of the Communications Society,"8 Jan 2025","2025","6","","332","361","The programmability of radio access networks was already introduced in the fifth generation of mobile networks by leading the pathway towards the Open RAN. To cater the stringent requirements emerging with the novel services and use cases, Open RAN is becoming integral to 6G as well. The most highlighted characteristics of Open RAN such as disaggregated architecture, cloudification of network functions, open interfaces, and intelligent network management are also associated with many challenges related to the security of the end-to-end telco networks. Most existing research works and specifications focus on security in Open RAN in 5G. In this paper, we discuss the role of Open RAN in 6G focusing on three aspects such as security, privacy, and trust, while highlighting their emerging trends and relevance to 6G. Our contributions include presenting a holistic view of Open RAN security within the 6G ecosystem, with insights into use case and stakeholder perspectives and standardization efforts. This includes a concise update on Open RAN security in the 5G era and their applicability to 6G exploration. Moreover, we identify the key research endeavors focused on privacy and trust in Open RAN, with an emphasis on their development toward 6G. Finally, we discuss the possibilities of incorporating energy awareness, quantum-safe solutions, and other emerging technological enablers with Open RAN security.","2644-125X","","10.1109/OJCOMS.2024.3519725","XcARet Project funded by the Research Council of Finland(grant numbers:355114); German Federal Ministry for Education and Research through the Open6GHub Project(grant numbers:16KISK004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10807044","Open radio access network;security;privacy;trust;6G","Open RAN;6G mobile communication;Security;5G mobile communication;Privacy;Computer architecture;Surveys;Ecosystems;Reviews;3GPP","","","","172","CCBY","18 Dec 2024","","","IEEE","IEEE Journals"
"IEEE Access Special Section Editorial: Edge Computing and Networking for Ubiquitous AI","V. C. M. Leung; X. Wang; A. Jamalipour; X. Chen; S. Bouzefrane","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Intelligent Computing, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; CEDRIC Lab, Conservatoire National des Arts et Métiers, Paris, France",IEEE Access,"29 Jun 2021","2021","9","","90933","90936","With its rapid development recently, edge computing with processing, storage, and networking capabilities has become an important solution to break through the bottleneck of emerging technology development by virtue of its advantages in reducing data transmission, decreasing service latency, and easing cloud computing pressure. Among several application scenarios such as network optimization, intelligent manufacturing, and real-time video analytics, edge computing can work with artificial intelligence (AI) synergistically. Therefore, many researchers are investigating edge computing with AI from two perspectives. One is that the emergence of AI solves the optimization problem of edge computing. For example, when network devices need to process some complex and fuzzy information, the powerful learning and reasoning ability of AI can help to extract valuable information from the massive data and realize intelligent management. Another is how edge computing supports AI in a networking environment. For example, AI training and inference can be efficiently enabled by a multitude of computing resources from edge computing. Therefore, edge computing and AI are mutually beneficial in networking.","2169-3536","","10.1109/ACCESS.2021.3090143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9467255","","","","3","","0","CCBY","29 Jun 2021","","","IEEE","IEEE Journals"
"Physiological Electrosignal Asynchronous Acquisition Technology: Insight and Perspectives","B. Hu; L. Zhu; Q. Dong; K. Qian; H. Cai; F. Tian","Ministry of Education Key Laboratory of Brain Health Intelligent Evaluation and Intervention, School of Medical Technology, Beijing Institute of Technology, Beijing, China; School of Medical Technology, Beijing Institute of Technology, Beijing, China; School of Medical Technology, Beijing Institute of Technology, Beijing, China; School of Medical Technology, Beijing Institute of Technology, Beijing, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China; School of Medical Technology, Beijing Institute of Technology, Beijing, China",IEEE Transactions on Computational Social Systems,"25 Jan 2024","2024","11","1","5","24","With great pride and enthusiasm, we present the inaugural edition of IEEE Transactions on Computational Social Systems (TCSS) for 2024. Reflecting on the year gone by, 2023 stands as a hallmark of academic excellence and prolific output, wherein our journal has successfully disseminated a substantial volume of scholarly work—301 articles encompassing approximately 3600 pages, distributed across six distinct issues.","2329-924X","","10.1109/TCSS.2024.3350958","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10414277","","","","2","","31","IEEE","25 Jan 2024","","","IEEE","IEEE Journals"
"IEEE Access Special Section Editorial: Data Mining for Internet of Things","C. -W. Tsai; M. -Y. Chen; F. Piccialli; T. Qiu; J. J. Jung; P. C. K. Hung; S. Zeadally","Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan; Department of Engineering Science, National Cheng Kung University, Tainan, Taiwan; Department of Mathematics and Applications “R. Caccioppoli”, University of Naples Federico II (UNINA), Naples, Italy; School of Computer Science and Technology, Tianjin University, Tianjin, China; Department of Computer Engineering, Chung-Ang University, Seoul, South Korea; Faculty of Business and Information Technology, University of Ontario Institute of Technology, Oshawa, ON, Canada; College of Communication and Information, University of Kentucky, Lexington, KY, USA",IEEE Access,"29 Jun 2021","2021","9","","90418","90427","It is an irrefutable fact that the Internet of Things (IoT) will eventually change our daily lives because its applications and relevant technologies have been or will be penetrating our daily lives. Also, the IoT is aimed to connect all the things (e.g., devices and systems) together via the Internet, thus making it easy to collect the data of users or environments and to find out useful information from the gathered data by using data mining technologies. As a consequence, how intelligent systems are developed for the IoT has become a critical research topic today. This means that artificial intelligence (AI) technologies (e.g., supervised learning, unsupervised learning, and semi-supervised learning) were used in the development of intelligent systems for analyzing the data captured from IoT devices or making decisions for IoT systems. It can be easily seen that AI can make an IoT system more intelligent and thus more accurate. For example, various sensors can be used for a smart home system to pinpoint the location and analyze the behavior of a human; however, with AI technologies, a more accurate prediction can be provided on the two pieces of information of a human. One of the most important uses for AI technologies is to make IoT systems more intelligent in order to provide a more convenient environment for users; thus, how to use existing AI technologies or develop new AI technologies to construct a better IoT system has attracted the attention of researchers from different disciplines in recent years. That is why, besides using existing supervised, unsupervised, semi-supervised learning algorithms, data mining algorithms, and machine learning algorithms, several recent studies have also attempted to develop new intelligent methods for the devices or systems for the IoT. All these approaches for making an IoT system more intelligent can also be found in the articles of this Special Section.","2169-3536","","10.1109/ACCESS.2021.3090137","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9467257","","","","2","","0","CCBY","29 Jun 2021","","","IEEE","IEEE Journals"
"Guest Editorial Special Issue on Smart IoT System: Opportunities by Linking Cloud, Edge, and AI","W. Yang; L. T. Yang; A. T. Chronopoulos","Department of Computer Science and Technology, Hunan University, Changsha, China; Department of Computer Science, St. Francis Xavier University, Antigonish, Canada; Department of Computer Science, University of Texas at San Antonio, San Antonio, TX, USA",IEEE Internet of Things Journal,"5 Aug 2021","2021","8","16","12478","12480","Recently, the Internet of Things (IoT) technologies have made their entrances into many fields, such as smart city, healthcare, intelligent transportation, forest protection, and environmental monitoring.","2327-4662","","10.1109/JIOT.2021.3092440","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9507561","","Special issues and sections;Artificial intelligence;Internet of Things;Smart devices","","","","0","IEEE","5 Aug 2021","","","IEEE","IEEE Journals"
"2019 Index IEEE Transactions on Parallel and Distributed Systems Vol. 30","",,IEEE Transactions on Parallel and Distributed Systems,"20 Dec 2019","2020","31","2","1","33","Presents the 2019 subject/author index for this publication.","1558-2183","","10.1109/TPDS.2019.2954077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8938150","","","","","","","IEEE","20 Dec 2019","","","IEEE","IEEE Journals"
"Scanning the Issue","A. Eskandarian","Dean, College of Engineering and William H. Goodwin Jr. Endowed Chair, Virginia Commonwealth University, Richmond, VA, USA",IEEE Transactions on Intelligent Transportation Systems,"30 Aug 2023","2023","24","9","8899","8918","Edge Intelligence in Intelligent Transportation Systems: A Survey T. Gong, L. Zhu, F. R. Yu, and T. Tang Edge intelligence (EI) is becoming one of the research hotspots among researchers, which is believed to help empower intelligent transportation systems (ITS). ITS generates a large amount of data at the network edge by millions of devices and sensors. Data-driven artificial intelligence (AI) is at the core of ITS development. By pushing the AI frontier to the network edge, EI enables ITS AI applications to have lower latency, higher security, less pressure on the backbone network, and better use of edge big data. This article surveys edge intelligence in intelligent transportation systems. The authors first introduce the challenges ITS faces and explain the motivation for using EI in ITS. They then explore the framework of using EI in ITS, including the EI-based ITS architecture, the data gathering and communication methods, the data processing and service delivery, and the performance indexes. The enabling technologies, such as AI models, the Internet of Things, and edge computing technologies used in EI-based ITS, are reviewed intensively. They discuss edge intelligence applications and research fields in ITS in depth. Typical application scenarios, such as autonomous driving, vehicular edge computing, intelligent vehicular transportation system, unmanned aerial vehicle (UAV) in ITS environment, and rail transportation control and management, are explored. The general platforms of EI, the EI training and inference in ITS, as well as the benchmark datasets, are introduced. Finally, they discuss some of the challenges and future directions of using EI in ITS.","1558-0016","","10.1109/TITS.2023.3299370","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10235285","","","","","","0","IEEE","30 Aug 2023","","","IEEE","IEEE Journals"
"2018 IndexIEEE Internet of Things JournalVol. 5","",,IEEE Internet of Things Journal,"21 Jan 2019","2018","5","6","5348","5413","Presents the 2018 subject/author index for this publication.","2327-4662","","10.1109/JIOT.2019.2894196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8620612","","","","","","","IEEE","21 Jan 2019","","","IEEE","IEEE Journals"
